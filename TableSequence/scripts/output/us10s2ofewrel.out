Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:21<04:55, 21.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:38<04:05, 18.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:55<03:33, 17.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:11<03:08, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:27<02:49, 16.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:43<02:28, 16.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:01<02:17, 17.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:18<01:59, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:35<01:41, 16.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:51<01:22, 16.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:08<01:07, 16.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:25<00:50, 16.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:41<00:33, 16.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:58<00:16, 16.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:14<00:00, 16.52s/it]Generating: 100%|██████████| 15/15 [04:14<00:00, 16.97s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 565, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : follows .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 558, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : owned by . Context : Later in the year , the department built a high - speed commuter railway crossing the Bordeaux River into New York City from Union Station in the Empire State Building . Head Entity : Union Station , Tail Entity : Department of Transportation .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 526, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 579, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : owned by .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7981770833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 227, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 272, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 473, 'raw': 672}
{'target': 600, 'success': 497, 'raw': 704}
{'target': 600, 'success': 520, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 587, 'raw': 832}
{'target': 600, 'success': 607, 'raw': 864}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7025462962962963, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 494, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 593, 'raw': 800}
{'target': 600, 'success': 618, 'raw': 832}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7427884615384616, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : location of formation . Context : Later in the year ( 1190 ) , Pheidole and his friends made a series of expeditions to the Old Town of Antwerp , which was a medieval church , to find remnants of the Church of St John the Evangelists , or John XII , with the exception of the dome . Head Entity : Pheidole , Tail Entity : New Town of Antwerp .\n']
['Relation : location of formation . Context : Later in the year ( 1190 ) , Pheidole and his friends made a series of expeditions to the Old Town of Antwerp , which was a medieval church , to find remnants of the Church of St John the Evangelists , or John XII , with the exception of the dome . Head Entity : Pheidole , Tail Entity : New Town of Antwerp .\n', "Relation : location of formation . Context : After the death of Emperor Nefertiti ( 9 January 1789 - 7 February 1819 ) , St Peter was succeeded as Archbishop by the emperor , St Peter 's son , Emperor Frederick II ( 18 November 1861 - 12 December 1955 ) . Head Entity : Emperor Frederick II , Tail Entity : Emperor St Peter .\n"]
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 576, 'raw': 736}
{'target': 600, 'success': 600, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.78125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : place served by transport hub . Context : The city of Stuttgart is a medieval city located in the eastern part of Germany . Head Entity : Struttgart , Tail Entity : Struttgart .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : record label . Context : Later in 2008 , the band became a major draw for their debut solo album entitled " The Way I Am " . Head Entity : The Way I Am , Tail Entity : The Bizarre Bizarre .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8033854166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 189, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 399, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 470, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 517, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 583, 'raw': 800}
{'target': 600, 'success': 600, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7211538461538461, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 14638
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14738, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.69s/it]Extractor Estimating: 2it [00:18,  8.05s/it]Extractor Estimating: 3it [00:19,  4.68s/it]Extractor Estimating: 4it [00:20,  3.08s/it]Extractor Estimating: 5it [00:20,  2.20s/it]Extractor Estimating: 6it [00:21,  1.67s/it]Extractor Estimating: 7it [00:22,  1.33s/it]Extractor Estimating: 8it [00:22,  1.11s/it]Extractor Estimating: 9it [00:23,  1.06it/s]Extractor Estimating: 10it [00:24,  1.14it/s]Extractor Estimating: 11it [00:24,  1.20it/s]Extractor Estimating: 12it [00:26,  1.04it/s]Extractor Estimating: 13it [00:26,  1.18it/s]Extractor Estimating: 14it [00:27,  1.25it/s]Extractor Estimating: 15it [00:28,  1.34it/s]Extractor Estimating: 16it [00:28,  1.41it/s]Extractor Estimating: 17it [00:29,  1.39it/s]Extractor Estimating: 18it [00:30,  1.44it/s]Extractor Estimating: 19it [00:30,  1.47it/s]Extractor Estimating: 20it [00:31,  1.45it/s]Extractor Estimating: 21it [00:31,  1.53it/s]Extractor Estimating: 22it [00:35,  1.54s/it]Extractor Estimating: 23it [00:36,  1.29s/it]Extractor Estimating: 24it [00:36,  1.09s/it]Extractor Estimating: 25it [00:37,  1.01it/s]Extractor Estimating: 26it [00:38,  1.12it/s]Extractor Estimating: 27it [00:38,  1.21it/s]Extractor Estimating: 28it [00:39,  1.27it/s]Extractor Estimating: 29it [00:40,  1.32it/s]Extractor Estimating: 30it [00:41,  1.38it/s]Extractor Estimating: 31it [00:41,  1.40it/s]Extractor Estimating: 32it [00:42,  1.43it/s]Extractor Estimating: 33it [00:43,  1.46it/s]Extractor Estimating: 34it [00:43,  1.45it/s]Extractor Estimating: 35it [00:44,  1.47it/s]Extractor Estimating: 36it [00:45,  1.45it/s]Extractor Estimating: 37it [00:45,  1.43it/s]Extractor Estimating: 38it [00:46,  1.41it/s]Extractor Estimating: 39it [00:47,  1.44it/s]Extractor Estimating: 40it [00:47,  1.43it/s]Extractor Estimating: 41it [00:48,  1.47it/s]Extractor Estimating: 42it [00:49,  1.43it/s]Extractor Estimating: 43it [00:49,  1.48it/s]Extractor Estimating: 44it [00:50,  1.45it/s]Extractor Estimating: 45it [00:51,  1.48it/s]Extractor Estimating: 46it [00:51,  1.48it/s]Extractor Estimating: 47it [00:52,  1.51it/s]Extractor Estimating: 48it [00:53,  1.50it/s]Extractor Estimating: 49it [00:54,  1.46it/s]Extractor Estimating: 50it [00:54,  1.49it/s]Extractor Estimating: 51it [00:55,  1.54it/s]Extractor Estimating: 52it [00:55,  1.57it/s]Extractor Estimating: 53it [00:56,  1.55it/s]Extractor Estimating: 54it [00:57,  1.57it/s]Extractor Estimating: 55it [00:57,  1.57it/s]Extractor Estimating: 56it [00:58,  1.57it/s]Extractor Estimating: 57it [00:59,  1.55it/s]Extractor Estimating: 58it [00:59,  1.54it/s]Extractor Estimating: 59it [01:00,  1.57it/s]Extractor Estimating: 60it [01:00,  1.57it/s]Extractor Estimating: 61it [01:01,  1.57it/s]Extractor Estimating: 62it [01:02,  1.57it/s]Extractor Estimating: 63it [01:02,  1.55it/s]Extractor Estimating: 64it [01:03,  1.55it/s]Extractor Estimating: 65it [01:04,  1.56it/s]Extractor Estimating: 66it [01:04,  1.58it/s]Extractor Estimating: 67it [01:05,  1.62it/s]Extractor Estimating: 68it [01:05,  1.62it/s]Extractor Estimating: 69it [01:06,  1.66it/s]Extractor Estimating: 70it [01:07,  1.57it/s]Extractor Estimating: 71it [01:07,  1.58it/s]Extractor Estimating: 72it [01:08,  1.58it/s]Extractor Estimating: 73it [01:09,  1.59it/s]Extractor Estimating: 74it [01:09,  1.59it/s]Extractor Estimating: 75it [01:10,  1.63it/s]Extractor Estimating: 76it [01:10,  1.61it/s]Extractor Estimating: 77it [01:11,  1.63it/s]Extractor Estimating: 78it [01:12,  1.64it/s]Extractor Estimating: 79it [01:12,  1.68it/s]Extractor Estimating: 80it [01:13,  1.65it/s]Extractor Estimating: 81it [01:13,  1.66it/s]Extractor Estimating: 82it [01:14,  1.68it/s]Extractor Estimating: 83it [01:15,  1.64it/s]Extractor Estimating: 84it [01:15,  1.62it/s]Extractor Estimating: 85it [01:16,  1.55it/s]Extractor Estimating: 86it [01:17,  1.55it/s]Extractor Estimating: 87it [01:17,  1.47it/s]Extractor Estimating: 88it [01:18,  1.52it/s]Extractor Estimating: 89it [01:19,  1.56it/s]Extractor Estimating: 90it [01:19,  1.58it/s]Extractor Estimating: 91it [01:20,  1.59it/s]Extractor Estimating: 92it [01:20,  1.61it/s]Extractor Estimating: 93it [01:21,  1.68it/s]Extractor Estimating: 94it [01:22,  1.70it/s]Extractor Estimating: 95it [01:22,  1.69it/s]Extractor Estimating: 96it [01:23,  1.69it/s]Extractor Estimating: 97it [01:23,  1.68it/s]Extractor Estimating: 98it [01:24,  1.61it/s]Extractor Estimating: 99it [01:25,  1.58it/s]Extractor Estimating: 100it [01:25,  1.66it/s]Extractor Estimating: 101it [01:26,  1.66it/s]Extractor Estimating: 102it [01:26,  1.68it/s]Extractor Estimating: 103it [01:27,  1.64it/s]Extractor Estimating: 104it [01:28,  1.48it/s]Extractor Estimating: 105it [01:29,  1.42it/s]Extractor Estimating: 106it [01:29,  1.42it/s]Extractor Estimating: 107it [01:30,  1.47it/s]Extractor Estimating: 108it [01:31,  1.51it/s]Extractor Estimating: 109it [01:31,  1.53it/s]Extractor Estimating: 110it [01:32,  1.52it/s]Extractor Estimating: 111it [01:33,  1.50it/s]Extractor Estimating: 112it [01:33,  1.46it/s]Extractor Estimating: 113it [01:34,  1.54it/s]Extractor Estimating: 114it [01:35,  1.56it/s]Extractor Estimating: 115it [01:35,  1.54it/s]Extractor Estimating: 116it [01:36,  1.60it/s]Extractor Estimating: 117it [01:36,  1.59it/s]Extractor Estimating: 118it [01:37,  1.61it/s]Extractor Estimating: 119it [01:38,  1.55it/s]Extractor Estimating: 120it [01:38,  1.56it/s]Extractor Estimating: 121it [01:39,  1.57it/s]Extractor Estimating: 122it [01:40,  1.57it/s]Extractor Estimating: 123it [01:40,  1.53it/s]Extractor Estimating: 124it [01:41,  1.54it/s]Extractor Estimating: 125it [01:42,  1.56it/s]Extractor Estimating: 126it [01:42,  1.55it/s]Extractor Estimating: 127it [01:43,  1.50it/s]Extractor Estimating: 128it [01:44,  1.53it/s]Extractor Estimating: 129it [01:44,  1.47it/s]Extractor Estimating: 130it [01:45,  1.53it/s]Extractor Estimating: 131it [01:46,  1.55it/s]Extractor Estimating: 132it [01:46,  1.51it/s]Extractor Estimating: 133it [01:47,  1.52it/s]Extractor Estimating: 134it [01:48,  1.54it/s]Extractor Estimating: 135it [01:48,  1.54it/s]Extractor Estimating: 136it [01:49,  1.55it/s]Extractor Estimating: 137it [01:49,  1.56it/s]Extractor Estimating: 138it [01:50,  1.61it/s]Extractor Estimating: 139it [01:51,  1.59it/s]Extractor Estimating: 140it [01:51,  1.63it/s]Extractor Estimating: 141it [01:52,  1.61it/s]Extractor Estimating: 142it [01:53,  1.53it/s]Extractor Estimating: 143it [01:53,  1.50it/s]Extractor Estimating: 144it [01:54,  1.49it/s]Extractor Estimating: 145it [01:55,  1.51it/s]Extractor Estimating: 146it [01:55,  1.52it/s]Extractor Estimating: 147it [01:56,  1.48it/s]Extractor Estimating: 148it [01:57,  1.43it/s]Extractor Estimating: 149it [01:57,  1.42it/s]Extractor Estimating: 150it [01:58,  1.45it/s]Extractor Estimating: 151it [01:59,  1.46it/s]Extractor Estimating: 152it [01:59,  1.45it/s]Extractor Estimating: 153it [02:00,  1.47it/s]Extractor Estimating: 154it [02:01,  1.47it/s]Extractor Estimating: 155it [02:01,  1.48it/s]Extractor Estimating: 156it [02:02,  1.46it/s]Extractor Estimating: 157it [02:03,  1.43it/s]Extractor Estimating: 158it [02:04,  1.43it/s]Extractor Estimating: 159it [02:04,  1.44it/s]Extractor Estimating: 160it [02:05,  1.40it/s]Extractor Estimating: 161it [02:06,  1.44it/s]Extractor Estimating: 162it [02:06,  1.41it/s]Extractor Estimating: 163it [02:07,  1.37it/s]Extractor Estimating: 164it [02:08,  1.36it/s]Extractor Estimating: 165it [02:09,  1.37it/s]Extractor Estimating: 166it [02:09,  1.40it/s]Extractor Estimating: 167it [02:10,  1.39it/s]Extractor Estimating: 168it [02:11,  1.42it/s]Extractor Estimating: 169it [02:12,  1.40it/s]Extractor Estimating: 170it [02:12,  1.48it/s]Extractor Estimating: 171it [02:13,  1.56it/s]Extractor Estimating: 172it [02:13,  1.47it/s]Extractor Estimating: 173it [02:14,  1.46it/s]Extractor Estimating: 174it [02:15,  1.46it/s]Extractor Estimating: 175it [02:16,  1.46it/s]Extractor Estimating: 176it [02:16,  1.51it/s]Extractor Estimating: 177it [02:17,  1.46it/s]Extractor Estimating: 178it [02:17,  1.52it/s]Extractor Estimating: 179it [02:18,  1.41it/s]Extractor Estimating: 180it [02:19,  1.49it/s]Extractor Estimating: 181it [02:19,  1.56it/s]Extractor Estimating: 182it [02:20,  1.52it/s]Extractor Estimating: 183it [02:21,  1.53it/s]Extractor Estimating: 184it [02:21,  1.55it/s]Extractor Estimating: 185it [02:22,  1.56it/s]Extractor Estimating: 186it [02:23,  1.53it/s]Extractor Estimating: 187it [02:23,  1.49it/s]Extractor Estimating: 188it [02:24,  1.52it/s]Extractor Estimating: 189it [02:25,  1.53it/s]Extractor Estimating: 190it [02:25,  1.55it/s]Extractor Estimating: 191it [02:26,  1.56it/s]Extractor Estimating: 192it [02:27,  1.54it/s]Extractor Estimating: 193it [02:27,  1.54it/s]Extractor Estimating: 194it [02:28,  1.50it/s]Extractor Estimating: 195it [02:29,  1.51it/s]Extractor Estimating: 196it [02:29,  1.51it/s]Extractor Estimating: 197it [02:30,  1.49it/s]Extractor Estimating: 198it [02:31,  1.59it/s]Extractor Estimating: 199it [02:31,  1.54it/s]Extractor Estimating: 200it [02:32,  1.44it/s]Extractor Estimating: 201it [02:33,  1.45it/s]Extractor Estimating: 202it [02:33,  1.51it/s]Extractor Estimating: 203it [02:34,  1.51it/s]Extractor Estimating: 204it [02:35,  1.48it/s]Extractor Estimating: 205it [02:35,  1.44it/s]Extractor Estimating: 206it [02:36,  1.51it/s]Extractor Estimating: 207it [02:37,  1.49it/s]Extractor Estimating: 208it [02:37,  1.46it/s]Extractor Estimating: 209it [02:38,  1.46it/s]Extractor Estimating: 210it [02:39,  1.51it/s]Extractor Estimating: 211it [02:39,  1.51it/s]Extractor Estimating: 212it [02:40,  1.48it/s]Extractor Estimating: 213it [02:41,  1.47it/s]Extractor Estimating: 214it [02:41,  1.52it/s]Extractor Estimating: 215it [02:42,  1.49it/s]Extractor Estimating: 216it [02:43,  1.53it/s]Extractor Estimating: 217it [02:43,  1.50it/s]Extractor Estimating: 218it [02:44,  1.52it/s]Extractor Estimating: 219it [02:45,  1.49it/s]Extractor Estimating: 220it [02:45,  1.44it/s]Extractor Estimating: 221it [02:46,  1.41it/s]Extractor Estimating: 222it [02:47,  1.41it/s]Extractor Estimating: 223it [02:48,  1.43it/s]Extractor Estimating: 224it [02:48,  1.42it/s]Extractor Estimating: 225it [02:49,  1.44it/s]Extractor Estimating: 226it [02:50,  1.50it/s]Extractor Estimating: 227it [02:50,  1.51it/s]Extractor Estimating: 228it [02:51,  1.52it/s]Extractor Estimating: 229it [02:52,  1.53it/s]Extractor Estimating: 230it [02:52,  1.56it/s]Extractor Estimating: 231it [02:53,  1.50it/s]Extractor Estimating: 232it [02:53,  1.54it/s]Extractor Estimating: 233it [02:54,  1.56it/s]Extractor Estimating: 234it [02:55,  1.56it/s]Extractor Estimating: 235it [02:55,  1.57it/s]Extractor Estimating: 236it [02:56,  1.56it/s]Extractor Estimating: 237it [02:57,  1.47it/s]Extractor Estimating: 238it [02:58,  1.42it/s]Extractor Estimating: 239it [02:58,  1.48it/s]Extractor Estimating: 240it [02:59,  1.55it/s]Extractor Estimating: 241it [02:59,  1.54it/s]Extractor Estimating: 242it [03:00,  1.56it/s]Extractor Estimating: 243it [03:01,  1.59it/s]Extractor Estimating: 244it [03:01,  1.57it/s]Extractor Estimating: 245it [03:02,  1.58it/s]Extractor Estimating: 246it [03:03,  1.56it/s]Extractor Estimating: 247it [03:03,  1.52it/s]Extractor Estimating: 248it [03:04,  1.57it/s]Extractor Estimating: 249it [03:05,  1.55it/s]Extractor Estimating: 250it [03:05,  1.57it/s]Extractor Estimating: 251it [03:06,  1.60it/s]Extractor Estimating: 252it [03:07,  1.47it/s]Extractor Estimating: 253it [03:07,  1.46it/s]Extractor Estimating: 254it [03:08,  1.37it/s]Extractor Estimating: 255it [03:09,  1.31it/s]Extractor Estimating: 256it [03:09,  1.43it/s]Extractor Estimating: 257it [03:10,  1.50it/s]Extractor Estimating: 258it [03:11,  1.51it/s]Extractor Estimating: 259it [03:11,  1.51it/s]Extractor Estimating: 260it [03:12,  1.52it/s]Extractor Estimating: 261it [03:13,  1.43it/s]Extractor Estimating: 262it [03:13,  1.43it/s]Extractor Estimating: 263it [03:14,  1.51it/s]Extractor Estimating: 264it [03:15,  1.54it/s]Extractor Estimating: 265it [03:15,  1.52it/s]Extractor Estimating: 266it [03:16,  1.49it/s]Extractor Estimating: 267it [03:17,  1.50it/s]Extractor Estimating: 268it [03:17,  1.48it/s]Extractor Estimating: 269it [03:18,  1.50it/s]Extractor Estimating: 270it [03:19,  1.55it/s]Extractor Estimating: 271it [03:19,  1.55it/s]Extractor Estimating: 272it [03:20,  1.58it/s]Extractor Estimating: 273it [03:21,  1.58it/s]Extractor Estimating: 274it [03:21,  1.58it/s]Extractor Estimating: 275it [03:22,  1.54it/s]Extractor Estimating: 276it [03:22,  1.56it/s]Extractor Estimating: 277it [03:23,  1.56it/s]Extractor Estimating: 278it [03:24,  1.60it/s]Extractor Estimating: 279it [03:24,  1.61it/s]Extractor Estimating: 280it [03:25,  1.61it/s]Extractor Estimating: 281it [03:26,  1.65it/s]Extractor Estimating: 282it [03:26,  1.61it/s]Extractor Estimating: 283it [03:27,  1.55it/s]Extractor Estimating: 284it [03:27,  1.58it/s]Extractor Estimating: 285it [03:28,  1.58it/s]Extractor Estimating: 286it [03:29,  1.55it/s]Extractor Estimating: 287it [03:29,  1.58it/s]Extractor Estimating: 288it [03:30,  1.50it/s]Extractor Estimating: 289it [03:31,  1.55it/s]Extractor Estimating: 290it [03:31,  1.58it/s]Extractor Estimating: 291it [03:32,  1.64it/s]Extractor Estimating: 292it [03:32,  1.68it/s]Extractor Estimating: 293it [03:33,  1.59it/s]Extractor Estimating: 294it [03:34,  1.60it/s]Extractor Estimating: 295it [03:34,  1.61it/s]Extractor Estimating: 296it [03:35,  1.59it/s]Extractor Estimating: 297it [03:36,  1.59it/s]Extractor Estimating: 298it [03:36,  1.58it/s]Extractor Estimating: 299it [03:37,  1.59it/s]Extractor Estimating: 300it [03:38,  1.63it/s]Extractor Estimating: 301it [03:38,  1.59it/s]Extractor Estimating: 302it [03:39,  1.54it/s]Extractor Estimating: 303it [03:40,  1.51it/s]Extractor Estimating: 304it [03:40,  1.49it/s]Extractor Estimating: 305it [03:41,  1.51it/s]Extractor Estimating: 306it [03:42,  1.53it/s]Extractor Estimating: 307it [03:42,  1.58it/s]Extractor Estimating: 308it [03:43,  1.59it/s]Extractor Estimating: 309it [03:43,  1.63it/s]Extractor Estimating: 310it [03:44,  1.60it/s]Extractor Estimating: 311it [03:45,  1.62it/s]Extractor Estimating: 312it [03:45,  1.60it/s]Extractor Estimating: 313it [03:46,  1.59it/s]Extractor Estimating: 314it [03:47,  1.57it/s]Extractor Estimating: 315it [03:47,  1.55it/s]Extractor Estimating: 316it [03:48,  1.59it/s]Extractor Estimating: 317it [03:48,  1.55it/s]Extractor Estimating: 318it [03:49,  1.50it/s]Extractor Estimating: 319it [03:50,  1.49it/s]Extractor Estimating: 320it [03:51,  1.50it/s]Extractor Estimating: 321it [03:51,  1.46it/s]Extractor Estimating: 322it [03:52,  1.40it/s]Extractor Estimating: 323it [03:53,  1.45it/s]Extractor Estimating: 324it [03:53,  1.49it/s]Extractor Estimating: 325it [03:54,  1.50it/s]Extractor Estimating: 326it [03:55,  1.52it/s]Extractor Estimating: 327it [03:55,  1.52it/s]Extractor Estimating: 328it [03:56,  1.50it/s]Extractor Estimating: 329it [03:57,  1.40it/s]Extractor Estimating: 330it [03:57,  1.43it/s]Extractor Estimating: 331it [03:58,  1.48it/s]Extractor Estimating: 332it [03:59,  1.50it/s]Extractor Estimating: 333it [03:59,  1.58it/s]Extractor Estimating: 334it [04:00,  1.59it/s]Extractor Estimating: 335it [04:00,  1.61it/s]Extractor Estimating: 336it [04:01,  1.57it/s]Extractor Estimating: 337it [04:02,  1.51it/s]Extractor Estimating: 338it [04:03,  1.49it/s]Extractor Estimating: 339it [04:03,  1.53it/s]Extractor Estimating: 340it [04:04,  1.60it/s]Extractor Estimating: 341it [04:04,  1.58it/s]Extractor Estimating: 342it [04:05,  1.56it/s]Extractor Estimating: 343it [04:06,  1.57it/s]Extractor Estimating: 344it [04:06,  1.56it/s]Extractor Estimating: 345it [04:07,  1.54it/s]Extractor Estimating: 346it [04:08,  1.55it/s]Extractor Estimating: 347it [04:08,  1.51it/s]Extractor Estimating: 348it [04:09,  1.54it/s]Extractor Estimating: 349it [04:10,  1.54it/s]Extractor Estimating: 350it [04:10,  1.57it/s]Extractor Estimating: 351it [04:11,  1.56it/s]Extractor Estimating: 352it [04:12,  1.54it/s]Extractor Estimating: 353it [04:12,  1.50it/s]Extractor Estimating: 354it [04:13,  1.50it/s]Extractor Estimating: 355it [04:13,  1.58it/s]Extractor Estimating: 356it [04:14,  1.51it/s]Extractor Estimating: 357it [04:15,  1.48it/s]Extractor Estimating: 358it [04:15,  1.53it/s]Extractor Estimating: 359it [04:16,  1.54it/s]Extractor Estimating: 360it [04:17,  1.53it/s]Extractor Estimating: 361it [04:17,  1.49it/s]Extractor Estimating: 362it [04:18,  1.45it/s]Extractor Estimating: 363it [04:19,  1.43it/s]Extractor Estimating: 364it [04:20,  1.46it/s]Extractor Estimating: 365it [04:20,  1.45it/s]Extractor Estimating: 366it [04:21,  1.47it/s]Extractor Estimating: 367it [04:22,  1.49it/s]Extractor Estimating: 368it [04:22,  1.48it/s]Extractor Estimating: 369it [04:23,  1.48it/s]Extractor Estimating: 370it [04:24,  1.50it/s]Extractor Estimating: 371it [04:24,  1.49it/s]Extractor Estimating: 372it [04:25,  1.47it/s]Extractor Estimating: 373it [04:26,  1.49it/s]Extractor Estimating: 374it [04:26,  1.51it/s]Extractor Estimating: 375it [04:27,  1.63it/s]Extractor Estimating: 375it [04:27,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 1553 mean pseudo reward: 0.9298647667242588
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 14722
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14822, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14822, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 35, avg_time 1.298, loss:532.1708
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 5, avg_time 1.012, loss:435.8678
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 40, avg_time 1.009, loss:371.6860
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 10, avg_time 1.006, loss:336.5379
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 45, avg_time 1.003, loss:299.1279
>> valid entity prec:0.5574, rec:0.5827, f1:0.5698
>> valid relation prec:0.4089, rec:0.1787, f1:0.2488
>> valid relation with NER prec:0.4089, rec:0.1787, f1:0.2488
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 15, avg_time 2.215, loss:263.5548
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 50, avg_time 1.009, loss:251.3553
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 20, avg_time 1.002, loss:224.0389
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 55, avg_time 1.006, loss:231.5531
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 25, avg_time 0.995, loss:203.4986
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5440, rec:0.5104, f1:0.5266
>> valid relation prec:0.3611, rec:0.1590, f1:0.2208
>> valid relation with NER prec:0.3611, rec:0.1590, f1:0.2208
g_step 1100, step 60, avg_time 2.217, loss:192.1331
g_step 1200, step 30, avg_time 1.007, loss:168.4541
g_step 1300, step 65, avg_time 1.008, loss:158.2929
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:57:44 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:57:44 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-57-44_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:57:45 - WARNING - datasets.builder -   Using custom data configuration default-347ab0179b8a9abf
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-347ab0179b8a9abf/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:57:46,767 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:57:46,768 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:57:46,768 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:57:46,769 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:57:46,784 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:57:46,795 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:57:46,795 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:57:46,795 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:57:46,795 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:57:46,795 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:57:46,795 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:57:46,915 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:57:50,143 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:57:50,191 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-347ab0179b8a9abf/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 17:57:50 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1457e5fc4830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  1.53ba/s]100%|██████████| 2/2 [00:00<00:00,  2.94ba/s]100%|██████████| 2/2 [00:00<00:00,  2.58ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.23ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.92ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.18ba/s]100%|██████████| 4/4 [00:00<00:00,  5.30ba/s]100%|██████████| 4/4 [00:00<00:00,  4.66ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  9.17ba/s]100%|██████████| 2/2 [00:00<00:00, 12.56ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.33ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.03ba/s]100%|██████████| 4/4 [00:00<00:00, 11.49ba/s]
[INFO|trainer.py:414] 2023-08-28 17:57:52,906 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:57:52,915 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:57:52,915 >>   Num examples = 1553
[INFO|trainer.py:1149] 2023-08-28 17:57:52,915 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:57:52,915 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:57:52,915 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:57:52,915 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:57:52,915 >>   Total optimization steps = 120
  0%|          | 0/120 [00:00<?, ?it/s]  1%|          | 1/120 [00:00<00:35,  3.32it/s]  2%|▏         | 2/120 [00:00<00:34,  3.40it/s]  2%|▎         | 3/120 [00:00<00:34,  3.42it/s]  3%|▎         | 4/120 [00:01<00:33,  3.43it/s]  4%|▍         | 5/120 [00:01<00:33,  3.43it/s]  5%|▌         | 6/120 [00:01<00:33,  3.44it/s]  6%|▌         | 7/120 [00:02<00:32,  3.44it/s]  7%|▋         | 8/120 [00:02<00:32,  3.44it/s]  8%|▊         | 9/120 [00:02<00:32,  3.44it/s]  8%|▊         | 10/120 [00:02<00:31,  3.44it/s]  9%|▉         | 11/120 [00:03<00:31,  3.44it/s] 10%|█         | 12/120 [00:03<00:31,  3.44it/s] 11%|█         | 13/120 [00:03<00:35,  2.99it/s] 12%|█▏        | 14/120 [00:04<00:34,  3.11it/s] 12%|█▎        | 15/120 [00:04<00:32,  3.21it/s] 13%|█▎        | 16/120 [00:04<00:31,  3.27it/s] 14%|█▍        | 17/120 [00:05<00:30,  3.32it/s] 15%|█▌        | 18/120 [00:05<00:30,  3.36it/s] 16%|█▌        | 19/120 [00:05<00:29,  3.38it/s] 17%|█▋        | 20/120 [00:05<00:29,  3.40it/s] 18%|█▊        | 21/120 [00:06<00:28,  3.41it/s] 18%|█▊        | 22/120 [00:06<00:28,  3.42it/s] 19%|█▉        | 23/120 [00:06<00:28,  3.43it/s] 20%|██        | 24/120 [00:07<00:27,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 17:58:00,077 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:58:00,077 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:58:00,077 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.96it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.61it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.00it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.15it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.91it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.65it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.42it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.32it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.39it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.47it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.32it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.14it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.13it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.13it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.16it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.10it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.05it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.18it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.24it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.28it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.22it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.20it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.13it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.08it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.16it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.10it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.08it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.25it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.18it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.23it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.10it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.15it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.05it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.08it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.18it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.06it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.31it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.20it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.13it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.15it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.15it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.06it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.06it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.10it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.10it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.21it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.20it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.19it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.19it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.07it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.07it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.97it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.06it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.14it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.12it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.23it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.13it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.02it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.99it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.91it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.05it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.16it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.18it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.04it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.08it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.13it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.97it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.04it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.11it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.03it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.08it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.15it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.15it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 34.63it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 37.18it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 39.16it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 40.72it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 41.66it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 42.55it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.11it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.27it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.25it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.11it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.24it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.69it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.00it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.11it/s][A                                                
                                                 [A 20%|██        | 24/120 [00:17<00:27,  3.44it/s]
100%|██████████| 437/437 [00:09<00:00, 44.11it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:58:10,135 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24
[INFO|configuration_utils.py:351] 2023-08-28 17:58:10,158 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:58:16,466 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:58:16,511 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:58:16,520 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24/special_tokens_map.json
 21%|██        | 25/120 [00:31<11:57,  7.55s/it] 22%|██▏       | 26/120 [00:31<08:25,  5.38s/it] 22%|██▎       | 27/120 [00:32<05:58,  3.85s/it] 23%|██▎       | 28/120 [00:32<04:16,  2.78s/it] 24%|██▍       | 29/120 [00:32<03:05,  2.04s/it] 25%|██▌       | 30/120 [00:33<02:16,  1.51s/it] 26%|██▌       | 31/120 [00:33<01:42,  1.15s/it] 27%|██▋       | 32/120 [00:33<01:18,  1.12it/s] 28%|██▊       | 33/120 [00:33<01:01,  1.40it/s] 28%|██▊       | 34/120 [00:34<00:50,  1.70it/s] 29%|██▉       | 35/120 [00:34<00:42,  2.01it/s] 30%|███       | 36/120 [00:34<00:36,  2.29it/s] 31%|███       | 37/120 [00:35<00:32,  2.55it/s] 32%|███▏      | 38/120 [00:35<00:30,  2.72it/s] 32%|███▎      | 39/120 [00:35<00:27,  2.90it/s] 33%|███▎      | 40/120 [00:36<00:26,  3.04it/s] 34%|███▍      | 41/120 [00:36<00:25,  3.16it/s] 35%|███▌      | 42/120 [00:36<00:24,  3.24it/s] 36%|███▌      | 43/120 [00:36<00:23,  3.30it/s] 37%|███▋      | 44/120 [00:37<00:23,  3.30it/s] 38%|███▊      | 45/120 [00:37<00:22,  3.32it/s] 38%|███▊      | 46/120 [00:37<00:22,  3.36it/s] 39%|███▉      | 47/120 [00:38<00:21,  3.38it/s] 40%|████      | 48/120 [00:38<00:21,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 17:58:31,334 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:58:31,334 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:58:31,334 >>   Batch size = 8
{'eval_loss': 1.0437448024749756, 'eval_runtime': 10.0366, 'eval_samples_per_second': 348.026, 'eval_steps_per_second': 43.541, 'epoch': 0.98}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.05it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.86it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.09it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.51it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.89it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.65it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.52it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.21it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.36it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.33it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.34it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.28it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.16it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.94it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.17it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.04it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.12it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.11it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.23it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.33it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.23it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.18it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.05it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.05it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.07it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.17it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.12it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.17it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.25it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.16it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.07it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.11it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.05it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.04it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.18it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.11it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.18it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.09it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.26it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.12it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.14it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.13it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.03it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.03it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.08it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.10it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.01it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.10it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.15it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.15it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.08it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.04it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.06it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.06it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.06it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.00it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.14it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.15it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.10it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.08it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.00it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.11it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.08it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.07it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.08it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.01it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.03it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.08it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.08it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.97it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.01it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.12it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.12it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.01it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.99it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.95it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.96it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.03it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.00it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.97it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.11it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.07it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.98it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.98it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.00it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.96it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.07it/s][A                                                
                                                 [A 40%|████      | 48/120 [00:48<00:21,  3.40it/s]
100%|██████████| 437/437 [00:09<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:58:41,282 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48
[INFO|configuration_utils.py:351] 2023-08-28 17:58:41,305 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:58:45,286 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:58:45,313 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:58:45,326 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48/special_tokens_map.json
 41%|████      | 49/120 [01:02<08:48,  7.45s/it] 42%|████▏     | 50/120 [01:02<06:11,  5.30s/it] 42%|████▎     | 51/120 [01:03<04:22,  3.80s/it] 43%|████▎     | 52/120 [01:03<03:06,  2.75s/it] 44%|████▍     | 53/120 [01:03<02:14,  2.01s/it] 45%|████▌     | 54/120 [01:03<01:38,  1.50s/it] 46%|████▌     | 55/120 [01:04<01:13,  1.14s/it] 47%|████▋     | 56/120 [01:04<00:56,  1.13it/s] 48%|████▊     | 57/120 [01:04<00:44,  1.42it/s] 48%|████▊     | 58/120 [01:05<00:36,  1.72it/s] 49%|████▉     | 59/120 [01:05<00:30,  2.02it/s] 50%|█████     | 60/120 [01:05<00:26,  2.30it/s] 51%|█████     | 61/120 [01:06<00:23,  2.54it/s] 52%|█████▏    | 62/120 [01:06<00:21,  2.75it/s] 52%|█████▎    | 63/120 [01:06<00:19,  2.91it/s] 53%|█████▎    | 64/120 [01:06<00:18,  3.05it/s] 54%|█████▍    | 65/120 [01:07<00:17,  3.15it/s] 55%|█████▌    | 66/120 [01:07<00:16,  3.22it/s] 56%|█████▌    | 67/120 [01:07<00:16,  3.27it/s] 57%|█████▋    | 68/120 [01:08<00:15,  3.31it/s] 57%|█████▊    | 69/120 [01:08<00:15,  3.34it/s] 58%|█████▊    | 70/120 [01:08<00:14,  3.35it/s] 59%|█████▉    | 71/120 [01:08<00:14,  3.36it/s] 60%|██████    | 72/120 [01:09<00:14,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 17:59:02,252 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:59:02,252 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:59:02,252 >>   Batch size = 8
{'eval_loss': 1.0495083332061768, 'eval_runtime': 9.9396, 'eval_samples_per_second': 351.422, 'eval_steps_per_second': 43.965, 'epoch': 1.98}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.47it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.32it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.92it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.22it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.59it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.41it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.45it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.25it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.34it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.45it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.34it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.28it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.05it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.92it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.01it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.07it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.08it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.16it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.15it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.27it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.13it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.02it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.01it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.02it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.05it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.14it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.22it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.20it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.14it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.07it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.11it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.13it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.98it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.96it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.16it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.14it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.14it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.06it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.04it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.12it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.11it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.99it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.93it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.06it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.08it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.09it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.15it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.01it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.05it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.03it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 41.52it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 42.32it/s][A
 61%|██████    | 267/437 [00:06<00:03, 42.95it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.49it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.73it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.81it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.83it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.85it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.53it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.70it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.82it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.00it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.21it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.22it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.27it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.11it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.88it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.76it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.74it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.90it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.99it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.14it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.35it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.20it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.33it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.00it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.81it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.00it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.05it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.25it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.43it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.48it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.30it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.20it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.07it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.09it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.13it/s][A                                                
                                                 [A 60%|██████    | 72/120 [01:19<00:14,  3.36it/s]
100%|██████████| 437/437 [00:09<00:00, 44.13it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:59:12,418 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72
[INFO|configuration_utils.py:351] 2023-08-28 17:59:12,615 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:59:21,089 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:59:21,470 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:59:21,740 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72/special_tokens_map.json
 61%|██████    | 73/120 [01:44<08:29, 10.85s/it] 62%|██████▏   | 74/120 [01:45<05:53,  7.68s/it] 62%|██████▎   | 75/120 [01:45<04:06,  5.47s/it] 63%|██████▎   | 76/120 [01:45<02:52,  3.92s/it] 64%|██████▍   | 77/120 [01:45<02:01,  2.83s/it] 65%|██████▌   | 78/120 [01:46<01:26,  2.07s/it] 66%|██████▌   | 79/120 [01:46<01:02,  1.54s/it] 67%|██████▋   | 80/120 [01:46<00:46,  1.16s/it] 68%|██████▊   | 81/120 [01:47<00:35,  1.11it/s] 68%|██████▊   | 82/120 [01:47<00:27,  1.39it/s] 69%|██████▉   | 83/120 [01:47<00:21,  1.69it/s] 70%|███████   | 84/120 [01:48<00:18,  1.99it/s] 71%|███████   | 85/120 [01:48<00:15,  2.27it/s] 72%|███████▏  | 86/120 [01:48<00:13,  2.52it/s] 72%|███████▎  | 87/120 [01:48<00:12,  2.73it/s] 73%|███████▎  | 88/120 [01:49<00:11,  2.91it/s] 74%|███████▍  | 89/120 [01:49<00:10,  3.04it/s] 75%|███████▌  | 90/120 [01:49<00:09,  3.14it/s] 76%|███████▌  | 91/120 [01:50<00:09,  3.21it/s] 77%|███████▋  | 92/120 [01:50<00:08,  3.27it/s] 78%|███████▊  | 93/120 [01:50<00:08,  3.31it/s] 78%|███████▊  | 94/120 [01:50<00:07,  3.33it/s] 79%|███████▉  | 95/120 [01:51<00:07,  3.35it/s] 80%|████████  | 96/120 [01:51<00:07,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 17:59:44,495 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:59:44,495 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:59:44,495 >>   Batch size = 8
{'eval_loss': 1.0510469675064087, 'eval_runtime': 9.9639, 'eval_samples_per_second': 350.566, 'eval_steps_per_second': 43.858, 'epoch': 2.98}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.90it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.47it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.96it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.28it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.77it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.70it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.49it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.39it/s][A
 11%|█         | 47/437 [00:01<00:13, 29.32it/s][A
 12%|█▏        | 52/437 [00:01<00:11, 32.97it/s][A
 13%|█▎        | 57/437 [00:01<00:10, 35.85it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 38.12it/s][A
 15%|█▌        | 67/437 [00:01<00:09, 39.95it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 41.35it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 42.22it/s][A
 19%|█▉        | 82/437 [00:02<00:08, 42.99it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 43.08it/s][A
 21%|██        | 92/437 [00:02<00:08, 42.96it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.15it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.49it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.75it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.09it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.76it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.89it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 44.10it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.99it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.85it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.77it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.84it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.05it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.28it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.25it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.43it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.28it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.19it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.02it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.96it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.04it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.08it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.12it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.39it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.36it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.28it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.16it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.00it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.84it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.92it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.17it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.20it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.24it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.37it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.26it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.12it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.02it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.93it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.95it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.17it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.49it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.76it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 43.84it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.99it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.91it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.91it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.98it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.97it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.93it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.09it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.08it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.25it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.26it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.13it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.07it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.94it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.04it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.12it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.11it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.20it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.16it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.21it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.10it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.91it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.89it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.01it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.04it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.15it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.24it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.28it/s][A                                                
                                                 [A 80%|████████  | 96/120 [02:01<00:07,  3.36it/s]
100%|██████████| 437/437 [00:10<00:00, 44.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:59:54,781 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-28 17:59:54,866 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:00:00,181 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:00:00,226 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:00:00,244 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96/special_tokens_map.json
 81%|████████  | 97/120 [02:18<03:13,  8.42s/it] 82%|████████▏ | 98/120 [02:19<02:11,  5.98s/it] 82%|████████▎ | 99/120 [02:19<01:29,  4.28s/it] 83%|████████▎ | 100/120 [02:19<01:01,  3.08s/it] 84%|████████▍ | 101/120 [02:20<00:42,  2.25s/it] 85%|████████▌ | 102/120 [02:20<00:29,  1.66s/it] 86%|████████▌ | 103/120 [02:20<00:21,  1.25s/it] 87%|████████▋ | 104/120 [02:20<00:15,  1.04it/s] 88%|████████▊ | 105/120 [02:21<00:11,  1.31it/s] 88%|████████▊ | 106/120 [02:21<00:08,  1.61it/s] 89%|████████▉ | 107/120 [02:21<00:06,  1.91it/s] 90%|█████████ | 108/120 [02:22<00:05,  2.21it/s] 91%|█████████ | 109/120 [02:22<00:04,  2.29it/s] 92%|█████████▏| 110/120 [02:22<00:03,  2.55it/s] 92%|█████████▎| 111/120 [02:23<00:03,  2.77it/s] 93%|█████████▎| 112/120 [02:23<00:02,  2.94it/s] 94%|█████████▍| 113/120 [02:23<00:02,  3.08it/s] 95%|█████████▌| 114/120 [02:23<00:01,  3.18it/s] 96%|█████████▌| 115/120 [02:24<00:01,  3.26it/s] 97%|█████████▋| 116/120 [02:24<00:01,  3.31it/s] 98%|█████████▊| 117/120 [02:24<00:00,  3.35it/s] 98%|█████████▊| 118/120 [02:25<00:00,  3.38it/s] 99%|█████████▉| 119/120 [02:25<00:00,  3.38it/s]100%|██████████| 120/120 [02:25<00:00,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 18:00:18,645 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:00:18,645 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:00:18,645 >>   Batch size = 8
{'eval_loss': 1.052690863609314, 'eval_runtime': 10.1177, 'eval_samples_per_second': 345.236, 'eval_steps_per_second': 43.192, 'epoch': 3.98}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.40it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.02it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.12it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.22it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.92it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.55it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.49it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.36it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.36it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.48it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.31it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.21it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.22it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.15it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.14it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.08it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.18it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.36it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.34it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.18it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.19it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.14it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.07it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.13it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.09it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.01it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.26it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.18it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.23it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 42.90it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.37it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.62it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.82it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.94it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.05it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.04it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.17it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.00it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.90it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.16it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.15it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.18it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.24it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.24it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.18it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.08it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.08it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.10it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.16it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.14it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.22it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.18it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.25it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.05it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.00it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.08it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.04it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.08it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.14it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.22it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.22it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.21it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.10it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.01it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.06it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.16it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.23it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.14it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.02it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.23it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.18it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.04it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.07it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.06it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.11it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.11it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.16it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.08it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.08it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.12it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.07it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.03it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.09it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 41.10it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 42.06it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 42.78it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.29it/s][A                                                 
                                                 [A100%|██████████| 120/120 [02:35<00:00,  3.40it/s]
100%|██████████| 437/437 [00:09<00:00, 43.29it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:00:28,598 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 18:00:28,628 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:00:32,919 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:00:33,030 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:00:33,045 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:00:41,012 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:00:41,018 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24 (score: 1.0437448024749756).
                                                 100%|██████████| 120/120 [02:51<00:00,  3.40it/s]100%|██████████| 120/120 [02:51<00:00,  1.43s/it]
[INFO|trainer.py:1894] 2023-08-28 18:00:44,845 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 18:00:44,863 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:00:48,991 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:00:49,011 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:00:49,020 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:00:49,234 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:49,234 >>   epoch                    =       4.98
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:49,234 >>   train_loss               =     0.6137
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:49,234 >>   train_runtime            = 0:02:51.91
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:49,234 >>   train_samples            =       1553
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:49,234 >>   train_samples_per_second =     45.166
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:49,234 >>   train_steps_per_second   =      0.698
{'eval_loss': 1.0518944263458252, 'eval_runtime': 9.9273, 'eval_samples_per_second': 351.859, 'eval_steps_per_second': 44.02, 'epoch': 4.98}
{'train_runtime': 171.9197, 'train_samples_per_second': 45.166, 'train_steps_per_second': 0.698, 'train_loss': 0.6137179692586263, 'epoch': 4.98}
08/28/2023 18:00:49 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:00:49,285 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:00:49,285 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 18:00:49,285 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.53it/s]  3%|▎         | 12/437 [00:00<00:08, 48.43it/s]  4%|▍         | 17/437 [00:00<00:08, 47.14it/s]  5%|▌         | 22/437 [00:00<00:08, 46.44it/s]  6%|▌         | 27/437 [00:00<00:08, 45.78it/s]  7%|▋         | 32/437 [00:00<00:08, 45.55it/s]  8%|▊         | 37/437 [00:00<00:08, 45.30it/s] 10%|▉         | 42/437 [00:00<00:08, 44.80it/s] 11%|█         | 47/437 [00:01<00:08, 44.22it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.02it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.15it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.18it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.38it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.56it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.76it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.61it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.23it/s] 21%|██        | 92/437 [00:02<00:07, 44.00it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.86it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.94it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.25it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.27it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.54it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.58it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.50it/s] 30%|███       | 132/437 [00:02<00:06, 44.36it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.17it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.02it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.10it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.17it/s] 36%|███▌      | 157/437 [00:03<00:06, 40.35it/s] 37%|███▋      | 162/437 [00:03<00:06, 41.62it/s] 38%|███▊      | 167/437 [00:03<00:06, 42.48it/s] 39%|███▉      | 172/437 [00:03<00:06, 43.24it/s] 41%|████      | 177/437 [00:03<00:05, 43.64it/s] 42%|████▏     | 182/437 [00:04<00:05, 43.71it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.88it/s] 44%|████▍     | 192/437 [00:04<00:05, 43.82it/s] 45%|████▌     | 197/437 [00:04<00:05, 43.63it/s] 46%|████▌     | 202/437 [00:04<00:05, 43.82it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.09it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.27it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.48it/s] 51%|█████     | 222/437 [00:05<00:04, 44.36it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.34it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.34it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.05it/s] 55%|█████▌    | 242/437 [00:05<00:04, 43.95it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.00it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.19it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.44it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.55it/s] 61%|██████    | 267/437 [00:06<00:03, 44.23it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.50it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.24it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.02it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.04it/s] 67%|██████▋   | 292/437 [00:06<00:03, 40.23it/s] 68%|██████▊   | 297/437 [00:06<00:03, 41.50it/s] 69%|██████▉   | 302/437 [00:06<00:03, 42.42it/s] 70%|███████   | 307/437 [00:06<00:03, 43.13it/s] 71%|███████▏  | 312/437 [00:07<00:02, 43.68it/s] 73%|███████▎  | 317/437 [00:07<00:02, 43.86it/s] 74%|███████▎  | 322/437 [00:07<00:02, 43.84it/s] 75%|███████▍  | 327/437 [00:07<00:02, 43.80it/s] 76%|███████▌  | 332/437 [00:07<00:02, 43.53it/s] 77%|███████▋  | 337/437 [00:07<00:02, 43.69it/s] 78%|███████▊  | 342/437 [00:07<00:02, 43.87it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.07it/s] 81%|████████  | 352/437 [00:07<00:01, 44.36it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.44it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.55it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.29it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.00it/s] 86%|████████▋ | 377/437 [00:08<00:01, 43.86it/s] 87%|████████▋ | 382/437 [00:08<00:01, 43.90it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.07it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.29it/s] 91%|█████████ | 397/437 [00:09<00:00, 44.37it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.54it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.50it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.29it/s] 95%|█████████▌| 417/437 [00:09<00:00, 43.98it/s] 97%|█████████▋| 422/437 [00:09<00:00, 43.89it/s] 98%|█████████▊| 427/437 [00:09<00:00, 38.10it/s] 99%|█████████▉| 432/437 [00:09<00:00, 39.82it/s]100%|██████████| 437/437 [00:09<00:00, 41.10it/s]100%|██████████| 437/437 [00:09<00:00, 43.80it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:00:59,280 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:59,280 >>   epoch                   =       4.98
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:59,280 >>   eval_loss               =     1.0437
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:59,280 >>   eval_runtime            = 0:00:09.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:59,280 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:59,280 >>   eval_samples_per_second =    349.476
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:59,280 >>   eval_steps_per_second   =     43.722
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:00:59,280 >>   perplexity              =     2.8398
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:05,071 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:05,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:05,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:05,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:05,079 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:01:05,781 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:01:05,783 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:01:06,116 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:01:07,148 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:01:07,149 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:08,871 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:08,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:08,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:08,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:01:08,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:01:09,208 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:01:09,209 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:01:09,489 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:01:09,636 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:01:09,636 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-96
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-120
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-24
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-72
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/checkpoint-48
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.51it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:06,  1.55it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:09,  1.61it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.58it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.62it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:20,  1.62it/s]Extractor Predicting: 33it [00:21,  1.59it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:25,  1.59it/s]Extractor Predicting: 40it [00:25,  1.53it/s]Extractor Predicting: 41it [00:26,  1.59it/s]Extractor Predicting: 42it [00:26,  1.61it/s]Extractor Predicting: 43it [00:27,  1.58it/s]Extractor Predicting: 44it [00:28,  1.62it/s]Extractor Predicting: 45it [00:28,  1.58it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:30,  1.58it/s]Extractor Predicting: 48it [00:30,  1.62it/s]Extractor Predicting: 49it [00:31,  1.65it/s]Extractor Predicting: 50it [00:31,  1.60it/s]Extractor Predicting: 51it [00:32,  1.60it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:33,  1.65it/s]Extractor Predicting: 54it [00:34,  1.65it/s]Extractor Predicting: 55it [00:34,  1.64it/s]Extractor Predicting: 56it [00:35,  1.61it/s]Extractor Predicting: 57it [00:36,  1.63it/s]Extractor Predicting: 58it [00:36,  1.61it/s]Extractor Predicting: 59it [00:37,  1.60it/s]Extractor Predicting: 60it [00:38,  1.56it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:40,  1.57it/s]Extractor Predicting: 64it [00:40,  1.56it/s]Extractor Predicting: 65it [00:41,  1.56it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:42,  1.54it/s]Extractor Predicting: 68it [00:43,  1.50it/s]Extractor Predicting: 69it [00:44,  1.53it/s]Extractor Predicting: 70it [00:44,  1.54it/s]Extractor Predicting: 71it [00:45,  1.56it/s]Extractor Predicting: 72it [00:45,  1.59it/s]Extractor Predicting: 73it [00:46,  1.57it/s]Extractor Predicting: 74it [00:47,  1.55it/s]Extractor Predicting: 75it [00:47,  1.59it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:49,  1.55it/s]Extractor Predicting: 78it [00:49,  1.41it/s]Extractor Predicting: 79it [00:50,  1.44it/s]Extractor Predicting: 80it [00:51,  1.47it/s]Extractor Predicting: 81it [00:51,  1.48it/s]Extractor Predicting: 82it [00:52,  1.50it/s]Extractor Predicting: 83it [00:53,  1.54it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:54,  1.55it/s]Extractor Predicting: 86it [00:55,  1.49it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:56,  1.52it/s]Extractor Predicting: 89it [00:57,  1.53it/s]Extractor Predicting: 90it [00:57,  1.49it/s]Extractor Predicting: 91it [00:58,  1.50it/s]Extractor Predicting: 92it [00:59,  1.52it/s]Extractor Predicting: 93it [00:59,  1.42it/s]Extractor Predicting: 94it [01:00,  1.47it/s]Extractor Predicting: 95it [01:01,  1.53it/s]Extractor Predicting: 96it [01:01,  1.56it/s]Extractor Predicting: 97it [01:02,  1.54it/s]Extractor Predicting: 98it [01:03,  1.50it/s]Extractor Predicting: 99it [01:03,  1.51it/s]Extractor Predicting: 100it [01:04,  1.55it/s]Extractor Predicting: 101it [01:05,  1.54it/s]Extractor Predicting: 102it [01:05,  1.54it/s]Extractor Predicting: 103it [01:06,  1.52it/s]Extractor Predicting: 104it [01:07,  1.53it/s]Extractor Predicting: 105it [01:07,  1.55it/s]Extractor Predicting: 106it [01:08,  1.53it/s]Extractor Predicting: 107it [01:08,  1.54it/s]Extractor Predicting: 108it [01:09,  1.52it/s]Extractor Predicting: 109it [01:10,  1.53it/s]Extractor Predicting: 110it [01:10,  1.51it/s]Extractor Predicting: 111it [01:11,  1.51it/s]Extractor Predicting: 112it [01:12,  1.51it/s]Extractor Predicting: 113it [01:13,  1.49it/s]Extractor Predicting: 114it [01:13,  1.49it/s]Extractor Predicting: 115it [01:14,  1.43it/s]Extractor Predicting: 116it [01:15,  1.46it/s]Extractor Predicting: 117it [01:15,  1.46it/s]Extractor Predicting: 118it [01:16,  1.47it/s]Extractor Predicting: 119it [01:17,  1.48it/s]Extractor Predicting: 120it [01:17,  1.49it/s]Extractor Predicting: 121it [01:18,  1.48it/s]Extractor Predicting: 122it [01:19,  1.48it/s]Extractor Predicting: 123it [01:19,  1.54it/s]Extractor Predicting: 124it [01:20,  1.54it/s]Extractor Predicting: 125it [01:21,  1.49it/s]Extractor Predicting: 126it [01:21,  1.50it/s]Extractor Predicting: 127it [01:22,  1.50it/s]Extractor Predicting: 128it [01:23,  1.51it/s]Extractor Predicting: 129it [01:23,  1.50it/s]Extractor Predicting: 130it [01:24,  1.49it/s]Extractor Predicting: 131it [01:25,  1.46it/s]Extractor Predicting: 132it [01:25,  1.45it/s]Extractor Predicting: 133it [01:26,  1.45it/s]Extractor Predicting: 134it [01:27,  1.44it/s]Extractor Predicting: 135it [01:27,  1.46it/s]Extractor Predicting: 136it [01:28,  1.79it/s]Extractor Predicting: 136it [01:28,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:46,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:46,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:46,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:46,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:46,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:02:47,127 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:02:47,128 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:02:47,413 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:02:48,457 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:02:48,457 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:50,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:50,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:50,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:50,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:02:50,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:02:50,945 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:02:50,946 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:02:51,321 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:02:51,485 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:02:51,485 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.49159021406727826,
  "recall": 0.18408245061551676,
  "score": 0.2678608623203499,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.79it/s]Extractor Predicting: 3it [00:01,  1.72it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:18,  1.66it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:19,  1.73it/s]Extractor Predicting: 34it [00:20,  1.72it/s]Extractor Predicting: 35it [00:21,  1.68it/s]Extractor Predicting: 36it [00:21,  1.65it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.70it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:25,  1.72it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:27,  1.68it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:30,  1.74it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:31,  1.70it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.69it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.63it/s]Extractor Predicting: 58it [00:34,  1.65it/s]Extractor Predicting: 59it [00:35,  1.63it/s]Extractor Predicting: 60it [00:36,  1.63it/s]Extractor Predicting: 61it [00:36,  1.65it/s]Extractor Predicting: 62it [00:37,  1.64it/s]Extractor Predicting: 63it [00:37,  1.61it/s]Extractor Predicting: 64it [00:38,  1.68it/s]Extractor Predicting: 65it [00:39,  1.66it/s]Extractor Predicting: 66it [00:39,  1.65it/s]Extractor Predicting: 67it [00:40,  1.65it/s]Extractor Predicting: 68it [00:40,  1.70it/s]Extractor Predicting: 69it [00:41,  1.71it/s]Extractor Predicting: 70it [00:42,  1.70it/s]Extractor Predicting: 71it [00:42,  1.70it/s]Extractor Predicting: 72it [00:43,  1.66it/s]Extractor Predicting: 73it [00:43,  1.68it/s]Extractor Predicting: 74it [00:44,  1.64it/s]Extractor Predicting: 75it [00:45,  1.63it/s]Extractor Predicting: 76it [00:45,  1.65it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:47,  1.57it/s]Extractor Predicting: 79it [00:47,  1.59it/s]Extractor Predicting: 80it [00:48,  1.63it/s]Extractor Predicting: 81it [00:49,  1.49it/s]Extractor Predicting: 82it [00:49,  1.55it/s]Extractor Predicting: 83it [00:50,  1.52it/s]Extractor Predicting: 84it [00:50,  1.55it/s]Extractor Predicting: 85it [00:51,  1.55it/s]Extractor Predicting: 86it [00:52,  1.54it/s]Extractor Predicting: 87it [00:52,  1.61it/s]Extractor Predicting: 88it [00:53,  1.55it/s]Extractor Predicting: 89it [00:54,  1.55it/s]Extractor Predicting: 90it [00:54,  1.53it/s]Extractor Predicting: 91it [00:55,  1.55it/s]Extractor Predicting: 92it [00:56,  1.53it/s]Extractor Predicting: 93it [00:56,  1.50it/s]Extractor Predicting: 94it [00:57,  1.52it/s]Extractor Predicting: 95it [00:58,  1.55it/s]Extractor Predicting: 96it [00:58,  1.55it/s]Extractor Predicting: 97it [00:59,  1.53it/s]Extractor Predicting: 98it [01:00,  1.53it/s]Extractor Predicting: 99it [01:00,  1.51it/s]Extractor Predicting: 100it [01:01,  1.51it/s]Extractor Predicting: 101it [01:02,  1.51it/s]Extractor Predicting: 102it [01:02,  1.50it/s]Extractor Predicting: 103it [01:03,  1.54it/s]Extractor Predicting: 104it [01:03,  1.54it/s]Extractor Predicting: 105it [01:04,  1.54it/s]Extractor Predicting: 106it [01:05,  1.51it/s]Extractor Predicting: 107it [01:05,  1.51it/s]Extractor Predicting: 108it [01:06,  1.49it/s]Extractor Predicting: 109it [01:07,  1.49it/s]Extractor Predicting: 110it [01:08,  1.49it/s]Extractor Predicting: 111it [01:08,  1.51it/s]Extractor Predicting: 112it [01:09,  1.50it/s]Extractor Predicting: 113it [01:09,  1.50it/s]Extractor Predicting: 114it [01:10,  1.50it/s]Extractor Predicting: 115it [01:11,  1.49it/s]Extractor Predicting: 116it [01:12,  1.50it/s]Extractor Predicting: 117it [01:12,  1.51it/s]Extractor Predicting: 118it [01:13,  1.52it/s]Extractor Predicting: 119it [01:13,  1.53it/s]Extractor Predicting: 120it [01:14,  1.57it/s]Extractor Predicting: 121it [01:15,  1.56it/s]Extractor Predicting: 122it [01:15,  1.55it/s]Extractor Predicting: 123it [01:16,  1.56it/s]Extractor Predicting: 124it [01:17,  1.55it/s]Extractor Predicting: 125it [01:17,  1.56it/s]Extractor Predicting: 126it [01:18,  1.60it/s]Extractor Predicting: 127it [01:18,  1.61it/s]Extractor Predicting: 128it [01:19,  1.63it/s]Extractor Predicting: 129it [01:20,  1.61it/s]Extractor Predicting: 130it [01:20,  1.56it/s]Extractor Predicting: 131it [01:21,  1.57it/s]Extractor Predicting: 132it [01:22,  1.58it/s]Extractor Predicting: 133it [01:22,  1.61it/s]Extractor Predicting: 134it [01:23,  1.63it/s]Extractor Predicting: 135it [01:24,  1.58it/s]Extractor Predicting: 136it [01:24,  1.61it/s]Extractor Predicting: 137it [01:25,  1.63it/s]Extractor Predicting: 138it [01:25,  1.61it/s]Extractor Predicting: 139it [01:26,  1.62it/s]Extractor Predicting: 140it [01:27,  1.61it/s]Extractor Predicting: 141it [01:27,  1.55it/s]Extractor Predicting: 142it [01:28,  1.55it/s]Extractor Predicting: 143it [01:29,  1.55it/s]Extractor Predicting: 144it [01:29,  1.55it/s]Extractor Predicting: 145it [01:30,  1.53it/s]Extractor Predicting: 146it [01:30,  1.58it/s]Extractor Predicting: 147it [01:31,  1.59it/s]Extractor Predicting: 148it [01:32,  1.58it/s]Extractor Predicting: 149it [01:32,  1.58it/s]Extractor Predicting: 150it [01:33,  1.57it/s]Extractor Predicting: 151it [01:34,  1.53it/s]Extractor Predicting: 152it [01:34,  1.54it/s]Extractor Predicting: 153it [01:35,  1.54it/s]Extractor Predicting: 154it [01:36,  1.58it/s]Extractor Predicting: 155it [01:36,  1.54it/s]Extractor Predicting: 156it [01:37,  1.54it/s]Extractor Predicting: 157it [01:38,  1.52it/s]Extractor Predicting: 158it [01:38,  1.46it/s]Extractor Predicting: 159it [01:39,  1.47it/s]Extractor Predicting: 160it [01:40,  1.48it/s]Extractor Predicting: 161it [01:40,  1.50it/s]Extractor Predicting: 162it [01:41,  1.50it/s]Extractor Predicting: 163it [01:42,  1.49it/s]Extractor Predicting: 164it [01:42,  1.52it/s]Extractor Predicting: 165it [01:43,  1.37it/s]Extractor Predicting: 166it [01:44,  1.42it/s]Extractor Predicting: 167it [01:45,  1.43it/s]Extractor Predicting: 168it [01:45,  1.45it/s]Extractor Predicting: 169it [01:46,  1.47it/s]Extractor Predicting: 170it [01:47,  1.47it/s]Extractor Predicting: 171it [01:47,  1.49it/s]Extractor Predicting: 172it [01:48,  1.50it/s]Extractor Predicting: 173it [01:49,  1.49it/s]Extractor Predicting: 174it [01:49,  1.50it/s]Extractor Predicting: 175it [01:50,  1.55it/s]Extractor Predicting: 176it [01:50,  1.54it/s]Extractor Predicting: 177it [01:51,  1.56it/s]Extractor Predicting: 178it [01:52,  1.57it/s]Extractor Predicting: 179it [01:52,  1.59it/s]Extractor Predicting: 180it [01:53,  1.58it/s]Extractor Predicting: 181it [01:54,  1.60it/s]Extractor Predicting: 182it [01:54,  1.62it/s]Extractor Predicting: 183it [01:55,  1.53it/s]Extractor Predicting: 184it [01:55,  1.60it/s]Extractor Predicting: 185it [01:56,  1.59it/s]Extractor Predicting: 186it [01:57,  1.61it/s]Extractor Predicting: 187it [01:57,  1.61it/s]Extractor Predicting: 188it [01:58,  1.57it/s]Extractor Predicting: 189it [01:59,  1.56it/s]Extractor Predicting: 190it [01:59,  1.57it/s]Extractor Predicting: 191it [02:00,  1.55it/s]Extractor Predicting: 192it [02:01,  1.56it/s]Extractor Predicting: 193it [02:01,  1.50it/s]Extractor Predicting: 194it [02:02,  1.53it/s]Extractor Predicting: 195it [02:03,  1.50it/s]Extractor Predicting: 196it [02:03,  1.47it/s]Extractor Predicting: 197it [02:04,  1.51it/s]Extractor Predicting: 198it [02:05,  1.51it/s]Extractor Predicting: 199it [02:05,  1.55it/s]Extractor Predicting: 200it [02:06,  1.51it/s]Extractor Predicting: 201it [02:07,  1.54it/s]Extractor Predicting: 202it [02:07,  1.56it/s]Extractor Predicting: 203it [02:08,  1.57it/s]Extractor Predicting: 204it [02:08,  1.60it/s]Extractor Predicting: 205it [02:09,  1.58it/s]Extractor Predicting: 206it [02:10,  1.62it/s]Extractor Predicting: 207it [02:10,  1.59it/s]Extractor Predicting: 208it [02:11,  1.58it/s]Extractor Predicting: 209it [02:12,  1.58it/s]Extractor Predicting: 210it [02:12,  1.52it/s]Extractor Predicting: 211it [02:13,  1.53it/s]Extractor Predicting: 212it [02:14,  1.55it/s]Extractor Predicting: 213it [02:14,  1.58it/s]Extractor Predicting: 214it [02:15,  1.57it/s]Extractor Predicting: 215it [02:15,  1.57it/s]Extractor Predicting: 216it [02:16,  1.57it/s]Extractor Predicting: 217it [02:17,  1.55it/s]Extractor Predicting: 218it [02:17,  1.57it/s]Extractor Predicting: 219it [02:18,  1.54it/s]Extractor Predicting: 220it [02:19,  1.56it/s]Extractor Predicting: 221it [02:19,  1.58it/s]Extractor Predicting: 222it [02:20,  1.58it/s]Extractor Predicting: 223it [02:21,  1.54it/s]Extractor Predicting: 224it [02:21,  1.59it/s]Extractor Predicting: 225it [02:22,  1.57it/s]Extractor Predicting: 226it [02:22,  1.56it/s]Extractor Predicting: 227it [02:23,  1.57it/s]Extractor Predicting: 228it [02:24,  1.59it/s]Extractor Predicting: 229it [02:24,  1.59it/s]Extractor Predicting: 230it [02:25,  1.54it/s]Extractor Predicting: 231it [02:26,  1.57it/s]Extractor Predicting: 232it [02:26,  1.57it/s]Extractor Predicting: 233it [02:27,  1.59it/s]Extractor Predicting: 234it [02:28,  1.57it/s]Extractor Predicting: 235it [02:28,  1.53it/s]Extractor Predicting: 236it [02:29,  1.54it/s]Extractor Predicting: 237it [02:29,  1.55it/s]Extractor Predicting: 238it [02:30,  1.55it/s]Extractor Predicting: 239it [02:31,  1.53it/s]Extractor Predicting: 240it [02:31,  1.54it/s]Extractor Predicting: 241it [02:32,  1.55it/s]Extractor Predicting: 242it [02:33,  1.55it/s]Extractor Predicting: 243it [02:33,  1.57it/s]Extractor Predicting: 244it [02:34,  1.55it/s]Extractor Predicting: 245it [02:35,  1.56it/s]Extractor Predicting: 246it [02:35,  1.57it/s]Extractor Predicting: 247it [02:36,  1.58it/s]Extractor Predicting: 248it [02:37,  1.57it/s]Extractor Predicting: 249it [02:37,  1.59it/s]Extractor Predicting: 250it [02:38,  1.59it/s]Extractor Predicting: 251it [02:38,  1.62it/s]Extractor Predicting: 252it [02:39,  1.60it/s]Extractor Predicting: 253it [02:40,  1.58it/s]Extractor Predicting: 254it [02:40,  1.58it/s]Extractor Predicting: 255it [02:41,  1.58it/s]Extractor Predicting: 256it [02:42,  1.54it/s]Extractor Predicting: 257it [02:42,  1.55it/s]Extractor Predicting: 258it [02:43,  1.55it/s]Extractor Predicting: 259it [02:43,  1.60it/s]Extractor Predicting: 260it [02:44,  1.58it/s]Extractor Predicting: 261it [02:45,  1.44it/s]Extractor Predicting: 262it [02:46,  1.48it/s]Extractor Predicting: 263it [02:46,  1.49it/s]Extractor Predicting: 264it [02:47,  1.57it/s]Extractor Predicting: 265it [02:47,  1.58it/s]Extractor Predicting: 266it [02:48,  1.61it/s]Extractor Predicting: 267it [02:49,  1.60it/s]Extractor Predicting: 268it [02:49,  1.58it/s]Extractor Predicting: 269it [02:50,  1.57it/s]Extractor Predicting: 270it [02:51,  1.55it/s]Extractor Predicting: 271it [02:51,  1.54it/s]Extractor Predicting: 272it [02:52,  1.55it/s]Extractor Predicting: 273it [02:53,  1.54it/s]Extractor Predicting: 274it [02:53,  1.52it/s]Extractor Predicting: 275it [02:54,  1.50it/s]Extractor Predicting: 276it [02:55,  1.52it/s]Extractor Predicting: 277it [02:55,  1.52it/s]Extractor Predicting: 278it [02:56,  1.55it/s]Extractor Predicting: 279it [02:56,  1.55it/s]Extractor Predicting: 280it [02:57,  1.55it/s]Extractor Predicting: 281it [02:58,  1.57it/s]Extractor Predicting: 282it [02:58,  1.56it/s]Extractor Predicting: 283it [02:59,  1.54it/s]Extractor Predicting: 284it [03:00,  1.55it/s]Extractor Predicting: 285it [03:00,  1.55it/s]Extractor Predicting: 286it [03:01,  1.59it/s]Extractor Predicting: 287it [03:02,  1.55it/s]Extractor Predicting: 288it [03:02,  1.76it/s]Extractor Predicting: 288it [03:02,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,208 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,220 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,220 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,220 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:02,220 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:06:02,506 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:06:02,506 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:06:02,773 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:06:03,876 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:06:03,876 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:05,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:05,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:05,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:05,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:06:05,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:06:06,091 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:06:06,092 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:06:06,354 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:06:06,510 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:06:06,510 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.38179148311306904,
  "recall": 0.15072463768115943,
  "score": 0.21612635078969245,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:02,  1.31it/s]Extractor Predicting: 3it [00:02,  1.33it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:06:09,393 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:06:09,394 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:06:09,407 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:06:09,408 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:06:09,415 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:06:16,056 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:06:16,063 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:06:16,098 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:06:16,099 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:06:16,105 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:16,109 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:16,109 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:16,109 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:16,109 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:16,109 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:06:16,110 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.05,
  "score": 0.09090909090909091,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:06:16,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:16,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:17,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:18,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:18,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:19,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:20,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:20,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:21,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:22,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:22,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:23,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:24,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:24,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:25,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:26,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:26,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:27,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:28,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:28,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:29,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:12, 13.78s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:30,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:30,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:31,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:31,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:32,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:33,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:33,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:34,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:34,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:35,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:36,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:36,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:37,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:38,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:38,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:39,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:39,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:40,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:41,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:41,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:42,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:42,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:55, 13.49s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:43,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:44,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:44,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:45,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:45,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:46,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:47,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:47,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:48,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:48,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:49,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:50,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:50,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:51,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:52,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:52,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:53,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:54,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:54,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:55,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:56,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:56,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:57,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:58,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:51, 14.29s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:58,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:59,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:59,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:00,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:01,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:01,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:02,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:02,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:03,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:04,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:04,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:05,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:05,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:06,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:07,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:08,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:08,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:09,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:09,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:10,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:11,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:11,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:34, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:12,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:12,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:13,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:14,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:14,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:15,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:15,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:16,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:17,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:17,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:18,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:18,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:19,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:20,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:20,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:21,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:22,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:23,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:23,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:24,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:24,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:09<02:17, 13.74s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:25,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:26,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:26,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:27,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:28,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:28,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:29,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:30,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:30,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:31,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:32,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:33,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:33,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:34,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:34,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:35,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:36,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:36,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:37,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:37,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:38,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:22<02:03, 13.74s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:39,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:39,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:40,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:41,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:41,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:42,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:43,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:43,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:44,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:45,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:45,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:46,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:47,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:47,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:48,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:48,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:49,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:50,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:50,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:51,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:52,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:52,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:53,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:37<01:52, 14.11s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:54,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:54,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:55,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:55,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:56,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:57,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:57,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:58,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:58,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:59,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:00,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:00,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:01,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:02,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:02,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:03,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:04,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:04,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:05,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:06,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:06,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:07,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:51<01:38, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:08,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:08,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:09,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:10,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:11,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:11,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:12,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:13,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:13,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:14,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:16,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:17,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:17,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:18,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:19,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:19,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:20,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:21,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:22,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:06<01:25, 14.29s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:22,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:23,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:24,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:24,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:25,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:26,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:26,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:27,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:28,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:28,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:29,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:30,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:30,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:31,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:31,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:32,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:32,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:33,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:34,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:35,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:35,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:19<01:10, 14.02s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:36,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:36,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:37,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:38,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:38,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:39,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:40,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:40,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:41,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:41,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:42,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:42,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:43,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:44,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:44,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:45,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:46,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:46,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:47,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:48,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:48,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:49,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:49,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:34<00:56, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:50,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:51,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:51,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:52,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:52,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:53,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:54,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:54,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:55,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:55,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:56,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:56,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:57,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:58,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:58,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:59,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:59,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:00,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:01,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:01,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:02,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:03,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:47<00:41, 13.95s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:04,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:04,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:05,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:06,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:06,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:07,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:07,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:08,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:08,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:09,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:10,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:10,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:11,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:11,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:12,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:13,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:13,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:14,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:14,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:15,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:16,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:00<00:27, 13.52s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:16,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:17,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:17,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:18,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:19,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:19,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:20,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:20,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:21,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:22,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:22,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:23,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:23,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:24,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:25,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:26,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:26,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:27,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:28,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:28,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:29,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:29,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:30,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:31,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:15<00:13, 13.99s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:31,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:32,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:33,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:33,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:34,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:35,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:36,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:36,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:37,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:37,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:38,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:39,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:39,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:40,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:40,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:41,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:42,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:42,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:43,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:44,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:44,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:29<00:00, 13.96s/it]Generating: 100%|██████████| 15/15 [03:29<00:00, 13.95s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:51,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:51,463 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:51,464 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:51,464 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:51,464 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:09:51,752 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:09:51,753 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:09:52,111 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:09:53,207 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:09:53,207 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:54,522 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:54,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:54,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:54,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:09:54,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:09:54,831 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:09:54,832 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:09:55,080 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:09:55,224 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:09:55,224 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : follows . Context : Later in the year , the band formed with drummer Joe Piscopo and vocalist Dave Winthrop , who had also toured with the band . Head Entity : Dave Winthrop , Tail Entity : following .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 558, 'raw': 704}
{'target': 600, 'success': 581, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7903645833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : after a work by . Context : Later in the year , the band formed with drummer Joe Piscopo and vocalist Dave Winthrop , who had also toured with the band . Head Entity : Dave Winthrop , Tail Entity : The band .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year , the United States decided to open the U.S. Embassy in London to American citizens . Head Entity : United States Embassy , Tail Entity : United States .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 589, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : winner .', 'success_rate': 0.8033854166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : work location .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 10738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.44it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:02,  1.48it/s]Extractor Estimating: 4it [00:02,  1.52it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.65it/s]Extractor Estimating: 7it [00:04,  1.59it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.61it/s]Extractor Estimating: 11it [00:07,  1.60it/s]Extractor Estimating: 12it [00:07,  1.62it/s]Extractor Estimating: 13it [00:08,  1.63it/s]Extractor Estimating: 14it [00:08,  1.55it/s]Extractor Estimating: 15it [00:09,  1.62it/s]Extractor Estimating: 16it [00:10,  1.55it/s]Extractor Estimating: 17it [00:10,  1.59it/s]Extractor Estimating: 18it [00:11,  1.55it/s]Extractor Estimating: 19it [00:12,  1.59it/s]Extractor Estimating: 20it [00:12,  1.53it/s]Extractor Estimating: 21it [00:13,  1.47it/s]Extractor Estimating: 22it [00:14,  1.46it/s]Extractor Estimating: 23it [00:14,  1.51it/s]Extractor Estimating: 24it [00:15,  1.43it/s]Extractor Estimating: 25it [00:16,  1.48it/s]Extractor Estimating: 26it [00:16,  1.51it/s]Extractor Estimating: 27it [00:17,  1.53it/s]Extractor Estimating: 28it [00:18,  1.50it/s]Extractor Estimating: 29it [00:18,  1.53it/s]Extractor Estimating: 30it [00:19,  1.57it/s]Extractor Estimating: 31it [00:20,  1.59it/s]Extractor Estimating: 32it [00:20,  1.57it/s]Extractor Estimating: 33it [00:21,  1.60it/s]Extractor Estimating: 34it [00:21,  1.63it/s]Extractor Estimating: 35it [00:22,  1.59it/s]Extractor Estimating: 36it [00:23,  1.63it/s]Extractor Estimating: 37it [00:23,  1.60it/s]Extractor Estimating: 38it [00:24,  1.57it/s]Extractor Estimating: 39it [00:24,  1.62it/s]Extractor Estimating: 40it [00:25,  1.63it/s]Extractor Estimating: 41it [00:26,  1.66it/s]Extractor Estimating: 42it [00:26,  1.67it/s]Extractor Estimating: 43it [00:27,  1.71it/s]Extractor Estimating: 44it [00:27,  1.68it/s]Extractor Estimating: 45it [00:28,  1.75it/s]Extractor Estimating: 46it [00:29,  1.65it/s]Extractor Estimating: 47it [00:29,  1.60it/s]Extractor Estimating: 48it [00:30,  1.57it/s]Extractor Estimating: 49it [00:31,  1.60it/s]Extractor Estimating: 50it [00:31,  1.70it/s]Extractor Estimating: 51it [00:32,  1.53it/s]Extractor Estimating: 52it [00:32,  1.62it/s]Extractor Estimating: 53it [00:33,  1.59it/s]Extractor Estimating: 54it [00:34,  1.58it/s]Extractor Estimating: 55it [00:34,  1.61it/s]Extractor Estimating: 56it [00:35,  1.60it/s]Extractor Estimating: 57it [00:36,  1.60it/s]Extractor Estimating: 58it [00:36,  1.65it/s]Extractor Estimating: 59it [00:37,  1.70it/s]Extractor Estimating: 60it [00:37,  1.61it/s]Extractor Estimating: 61it [00:38,  1.65it/s]Extractor Estimating: 62it [00:39,  1.62it/s]Extractor Estimating: 63it [00:39,  1.67it/s]Extractor Estimating: 64it [00:40,  1.65it/s]Extractor Estimating: 65it [00:40,  1.66it/s]Extractor Estimating: 66it [00:41,  1.70it/s]Extractor Estimating: 67it [00:42,  1.69it/s]Extractor Estimating: 68it [00:42,  1.69it/s]Extractor Estimating: 69it [00:43,  1.61it/s]Extractor Estimating: 70it [00:44,  1.50it/s]Extractor Estimating: 71it [00:44,  1.53it/s]Extractor Estimating: 72it [00:45,  1.58it/s]Extractor Estimating: 73it [00:45,  1.63it/s]Extractor Estimating: 74it [00:46,  1.51it/s]Extractor Estimating: 75it [00:47,  1.53it/s]Extractor Estimating: 76it [00:47,  1.57it/s]Extractor Estimating: 77it [00:48,  1.62it/s]Extractor Estimating: 78it [00:49,  1.63it/s]Extractor Estimating: 79it [00:49,  1.62it/s]Extractor Estimating: 80it [00:50,  1.63it/s]Extractor Estimating: 81it [00:50,  1.64it/s]Extractor Estimating: 82it [00:51,  1.66it/s]Extractor Estimating: 83it [00:51,  1.71it/s]Extractor Estimating: 84it [00:52,  1.67it/s]Extractor Estimating: 85it [00:53,  1.68it/s]Extractor Estimating: 86it [00:53,  1.65it/s]Extractor Estimating: 87it [00:54,  1.71it/s]Extractor Estimating: 88it [00:54,  1.74it/s]Extractor Estimating: 89it [00:55,  1.73it/s]Extractor Estimating: 90it [00:56,  1.69it/s]Extractor Estimating: 91it [00:56,  1.70it/s]Extractor Estimating: 92it [00:57,  1.42it/s]Extractor Estimating: 93it [00:58,  1.52it/s]Extractor Estimating: 94it [00:58,  1.57it/s]Extractor Estimating: 95it [00:59,  1.58it/s]Extractor Estimating: 96it [01:00,  1.56it/s]Extractor Estimating: 97it [01:00,  1.59it/s]Extractor Estimating: 98it [01:01,  1.62it/s]Extractor Estimating: 99it [01:01,  1.63it/s]Extractor Estimating: 100it [01:02,  1.71it/s]Extractor Estimating: 101it [01:03,  1.67it/s]Extractor Estimating: 102it [01:03,  1.60it/s]Extractor Estimating: 103it [01:04,  1.67it/s]Extractor Estimating: 104it [01:04,  1.70it/s]Extractor Estimating: 105it [01:05,  1.69it/s]Extractor Estimating: 106it [01:06,  1.59it/s]Extractor Estimating: 107it [01:06,  1.62it/s]Extractor Estimating: 108it [01:07,  1.64it/s]Extractor Estimating: 109it [01:07,  1.64it/s]Extractor Estimating: 110it [01:08,  1.61it/s]Extractor Estimating: 111it [01:09,  1.64it/s]Extractor Estimating: 112it [01:09,  1.60it/s]Extractor Estimating: 113it [01:10,  1.65it/s]Extractor Estimating: 114it [01:10,  1.66it/s]Extractor Estimating: 115it [01:11,  1.62it/s]Extractor Estimating: 116it [01:12,  1.64it/s]Extractor Estimating: 117it [01:12,  1.64it/s]Extractor Estimating: 118it [01:13,  1.55it/s]Extractor Estimating: 119it [01:14,  1.59it/s]Extractor Estimating: 120it [01:15,  1.32it/s]Extractor Estimating: 121it [01:15,  1.40it/s]Extractor Estimating: 122it [01:16,  1.42it/s]Extractor Estimating: 123it [01:17,  1.43it/s]Extractor Estimating: 124it [01:17,  1.51it/s]Extractor Estimating: 125it [01:18,  1.57it/s]Extractor Estimating: 126it [01:19,  1.53it/s]Extractor Estimating: 127it [01:19,  1.51it/s]Extractor Estimating: 128it [01:20,  1.60it/s]Extractor Estimating: 129it [01:21,  1.51it/s]Extractor Estimating: 130it [01:21,  1.54it/s]Extractor Estimating: 131it [01:22,  1.58it/s]Extractor Estimating: 132it [01:22,  1.57it/s]Extractor Estimating: 133it [01:23,  1.60it/s]Extractor Estimating: 134it [01:24,  1.61it/s]Extractor Estimating: 135it [01:24,  1.66it/s]Extractor Estimating: 136it [01:25,  1.62it/s]Extractor Estimating: 137it [01:25,  1.57it/s]Extractor Estimating: 138it [01:26,  1.61it/s]Extractor Estimating: 139it [01:27,  1.62it/s]Extractor Estimating: 140it [01:27,  1.60it/s]Extractor Estimating: 141it [01:28,  1.61it/s]Extractor Estimating: 142it [01:29,  1.63it/s]Extractor Estimating: 143it [01:29,  1.63it/s]Extractor Estimating: 144it [01:30,  1.63it/s]Extractor Estimating: 145it [01:30,  1.67it/s]Extractor Estimating: 146it [01:31,  1.62it/s]Extractor Estimating: 147it [01:32,  1.64it/s]Extractor Estimating: 148it [01:32,  1.69it/s]Extractor Estimating: 149it [01:33,  1.64it/s]Extractor Estimating: 150it [01:33,  1.62it/s]Extractor Estimating: 151it [01:34,  1.62it/s]Extractor Estimating: 152it [01:35,  1.61it/s]Extractor Estimating: 153it [01:35,  1.58it/s]Extractor Estimating: 154it [01:36,  1.61it/s]Extractor Estimating: 155it [01:37,  1.60it/s]Extractor Estimating: 156it [01:37,  1.53it/s]Extractor Estimating: 157it [01:38,  1.56it/s]Extractor Estimating: 158it [01:39,  1.56it/s]Extractor Estimating: 159it [01:39,  1.56it/s]Extractor Estimating: 160it [01:40,  1.55it/s]Extractor Estimating: 161it [01:40,  1.55it/s]Extractor Estimating: 162it [01:41,  1.48it/s]Extractor Estimating: 163it [01:42,  1.46it/s]Extractor Estimating: 164it [01:42,  1.53it/s]Extractor Estimating: 165it [01:43,  1.51it/s]Extractor Estimating: 166it [01:44,  1.56it/s]Extractor Estimating: 167it [01:44,  1.59it/s]Extractor Estimating: 168it [01:45,  1.52it/s]Extractor Estimating: 169it [01:46,  1.55it/s]Extractor Estimating: 170it [01:46,  1.50it/s]Extractor Estimating: 171it [01:47,  1.43it/s]Extractor Estimating: 172it [01:48,  1.52it/s]Extractor Estimating: 173it [01:48,  1.52it/s]Extractor Estimating: 174it [01:49,  1.50it/s]Extractor Estimating: 175it [01:50,  1.53it/s]Extractor Estimating: 176it [01:50,  1.56it/s]Extractor Estimating: 177it [01:51,  1.60it/s]Extractor Estimating: 178it [01:52,  1.61it/s]Extractor Estimating: 179it [01:52,  1.66it/s]Extractor Estimating: 180it [01:53,  1.64it/s]Extractor Estimating: 181it [01:54,  1.48it/s]Extractor Estimating: 182it [01:54,  1.52it/s]Extractor Estimating: 183it [01:55,  1.58it/s]Extractor Estimating: 184it [01:55,  1.61it/s]Extractor Estimating: 185it [01:56,  1.62it/s]Extractor Estimating: 186it [01:57,  1.59it/s]Extractor Estimating: 187it [01:57,  1.62it/s]Extractor Estimating: 188it [01:58,  1.66it/s]Extractor Estimating: 189it [01:58,  1.68it/s]Extractor Estimating: 190it [01:59,  1.70it/s]Extractor Estimating: 191it [02:00,  1.72it/s]Extractor Estimating: 192it [02:00,  1.61it/s]Extractor Estimating: 193it [02:01,  1.55it/s]Extractor Estimating: 194it [02:02,  1.57it/s]Extractor Estimating: 195it [02:02,  1.62it/s]Extractor Estimating: 196it [02:03,  1.66it/s]Extractor Estimating: 197it [02:03,  1.64it/s]Extractor Estimating: 198it [02:04,  1.63it/s]Extractor Estimating: 199it [02:04,  1.67it/s]Extractor Estimating: 200it [02:05,  1.63it/s]Extractor Estimating: 201it [02:06,  1.60it/s]Extractor Estimating: 202it [02:06,  1.59it/s]Extractor Estimating: 203it [02:07,  1.60it/s]Extractor Estimating: 204it [02:08,  1.57it/s]Extractor Estimating: 205it [02:08,  1.55it/s]Extractor Estimating: 206it [02:09,  1.59it/s]Extractor Estimating: 207it [02:10,  1.59it/s]Extractor Estimating: 208it [02:10,  1.56it/s]Extractor Estimating: 209it [02:11,  1.60it/s]Extractor Estimating: 210it [02:12,  1.58it/s]Extractor Estimating: 211it [02:12,  1.59it/s]Extractor Estimating: 212it [02:13,  1.53it/s]Extractor Estimating: 213it [02:13,  1.54it/s]Extractor Estimating: 214it [02:14,  1.46it/s]Extractor Estimating: 215it [02:15,  1.46it/s]Extractor Estimating: 216it [02:16,  1.51it/s]Extractor Estimating: 217it [02:16,  1.53it/s]Extractor Estimating: 218it [02:17,  1.57it/s]Extractor Estimating: 219it [02:17,  1.57it/s]Extractor Estimating: 220it [02:18,  1.57it/s]Extractor Estimating: 221it [02:19,  1.55it/s]Extractor Estimating: 222it [02:19,  1.52it/s]Extractor Estimating: 223it [02:20,  1.57it/s]Extractor Estimating: 224it [02:21,  1.55it/s]Extractor Estimating: 225it [02:21,  1.60it/s]Extractor Estimating: 226it [02:22,  1.66it/s]Extractor Estimating: 227it [02:22,  1.67it/s]Extractor Estimating: 228it [02:23,  1.66it/s]Extractor Estimating: 229it [02:24,  1.71it/s]Extractor Estimating: 230it [02:24,  1.78it/s]Extractor Estimating: 231it [02:25,  1.73it/s]Extractor Estimating: 232it [02:25,  1.70it/s]Extractor Estimating: 233it [02:26,  1.75it/s]Extractor Estimating: 234it [02:26,  1.70it/s]Extractor Estimating: 235it [02:27,  1.61it/s]Extractor Estimating: 236it [02:28,  1.62it/s]Extractor Estimating: 237it [02:28,  1.63it/s]Extractor Estimating: 238it [02:29,  1.66it/s]Extractor Estimating: 239it [02:29,  1.69it/s]Extractor Estimating: 240it [02:30,  1.70it/s]Extractor Estimating: 241it [02:31,  1.70it/s]Extractor Estimating: 242it [02:31,  1.71it/s]Extractor Estimating: 243it [02:32,  1.70it/s]Extractor Estimating: 244it [02:32,  1.73it/s]Extractor Estimating: 245it [02:33,  1.68it/s]Extractor Estimating: 246it [02:34,  1.71it/s]Extractor Estimating: 247it [02:34,  1.71it/s]Extractor Estimating: 248it [02:35,  1.75it/s]Extractor Estimating: 249it [02:35,  1.57it/s]Extractor Estimating: 250it [02:36,  1.64it/s]Extractor Estimating: 251it [02:37,  1.62it/s]Extractor Estimating: 252it [02:37,  1.59it/s]Extractor Estimating: 253it [02:38,  1.58it/s]Extractor Estimating: 254it [02:39,  1.60it/s]Extractor Estimating: 255it [02:39,  1.66it/s]Extractor Estimating: 256it [02:40,  1.61it/s]Extractor Estimating: 257it [02:40,  1.60it/s]Extractor Estimating: 258it [02:41,  1.63it/s]Extractor Estimating: 259it [02:42,  1.66it/s]Extractor Estimating: 260it [02:42,  1.68it/s]Extractor Estimating: 261it [02:43,  1.68it/s]Extractor Estimating: 262it [02:43,  1.64it/s]Extractor Estimating: 263it [02:44,  1.68it/s]Extractor Estimating: 264it [02:45,  1.65it/s]Extractor Estimating: 265it [02:45,  1.63it/s]Extractor Estimating: 266it [02:46,  1.63it/s]Extractor Estimating: 267it [02:46,  1.64it/s]Extractor Estimating: 268it [02:47,  1.62it/s]Extractor Estimating: 269it [02:48,  1.67it/s]Extractor Estimating: 270it [02:48,  1.70it/s]Extractor Estimating: 271it [02:49,  1.70it/s]Extractor Estimating: 272it [02:49,  1.62it/s]Extractor Estimating: 273it [02:50,  1.64it/s]Extractor Estimating: 274it [02:51,  1.67it/s]Extractor Estimating: 275it [02:51,  1.70it/s]Extractor Estimating: 276it [02:52,  1.69it/s]Extractor Estimating: 277it [02:52,  1.69it/s]Extractor Estimating: 278it [02:53,  1.68it/s]Extractor Estimating: 279it [02:54,  1.72it/s]Extractor Estimating: 280it [02:54,  1.75it/s]Extractor Estimating: 281it [02:55,  1.81it/s]Extractor Estimating: 282it [02:55,  1.82it/s]Extractor Estimating: 283it [02:56,  1.79it/s]Extractor Estimating: 284it [02:56,  1.80it/s]Extractor Estimating: 285it [02:57,  1.76it/s]Extractor Estimating: 286it [02:57,  1.70it/s]Extractor Estimating: 287it [02:58,  1.75it/s]Extractor Estimating: 288it [02:59,  1.80it/s]Extractor Estimating: 289it [02:59,  1.73it/s]Extractor Estimating: 290it [03:00,  1.75it/s]Extractor Estimating: 291it [03:00,  1.76it/s]Extractor Estimating: 292it [03:01,  1.73it/s]Extractor Estimating: 293it [03:01,  1.73it/s]Extractor Estimating: 294it [03:02,  1.73it/s]Extractor Estimating: 295it [03:03,  1.72it/s]Extractor Estimating: 296it [03:03,  1.71it/s]Extractor Estimating: 297it [03:04,  1.65it/s]Extractor Estimating: 298it [03:04,  1.71it/s]Extractor Estimating: 299it [03:05,  1.76it/s]Extractor Estimating: 300it [03:05,  1.76it/s]Extractor Estimating: 301it [03:06,  1.66it/s]Extractor Estimating: 302it [03:07,  1.62it/s]Extractor Estimating: 303it [03:07,  1.64it/s]Extractor Estimating: 304it [03:08,  1.63it/s]Extractor Estimating: 305it [03:09,  1.68it/s]Extractor Estimating: 306it [03:09,  1.71it/s]Extractor Estimating: 307it [03:10,  1.69it/s]Extractor Estimating: 308it [03:10,  1.74it/s]Extractor Estimating: 309it [03:11,  1.74it/s]Extractor Estimating: 310it [03:12,  1.69it/s]Extractor Estimating: 311it [03:12,  1.73it/s]Extractor Estimating: 312it [03:13,  1.59it/s]Extractor Estimating: 313it [03:13,  1.60it/s]Extractor Estimating: 314it [03:14,  1.63it/s]Extractor Estimating: 315it [03:15,  1.58it/s]Extractor Estimating: 316it [03:15,  1.51it/s]Extractor Estimating: 317it [03:16,  1.54it/s]Extractor Estimating: 318it [03:17,  1.53it/s]Extractor Estimating: 319it [03:17,  1.60it/s]Extractor Estimating: 320it [03:18,  1.59it/s]Extractor Estimating: 321it [03:18,  1.63it/s]Extractor Estimating: 322it [03:19,  1.60it/s]Extractor Estimating: 323it [03:20,  1.62it/s]Extractor Estimating: 324it [03:20,  1.62it/s]Extractor Estimating: 325it [03:21,  1.63it/s]Extractor Estimating: 326it [03:22,  1.46it/s]Extractor Estimating: 327it [03:22,  1.52it/s]Extractor Estimating: 328it [03:23,  1.57it/s]Extractor Estimating: 329it [03:24,  1.57it/s]Extractor Estimating: 330it [03:24,  1.61it/s]Extractor Estimating: 331it [03:25,  1.62it/s]Extractor Estimating: 332it [03:25,  1.59it/s]Extractor Estimating: 333it [03:26,  1.58it/s]Extractor Estimating: 334it [03:27,  1.64it/s]Extractor Estimating: 335it [03:27,  1.70it/s]Extractor Estimating: 336it [03:28,  1.65it/s]Extractor Estimating: 337it [03:28,  1.66it/s]Extractor Estimating: 338it [03:29,  1.69it/s]Extractor Estimating: 339it [03:30,  1.65it/s]Extractor Estimating: 340it [03:30,  1.65it/s]Extractor Estimating: 341it [03:31,  1.71it/s]Extractor Estimating: 342it [03:31,  1.65it/s]Extractor Estimating: 343it [03:32,  1.64it/s]Extractor Estimating: 344it [03:33,  1.58it/s]Extractor Estimating: 345it [03:33,  1.63it/s]Extractor Estimating: 346it [03:34,  1.59it/s]Extractor Estimating: 347it [03:35,  1.64it/s]Extractor Estimating: 348it [03:35,  1.68it/s]Extractor Estimating: 349it [03:36,  1.71it/s]Extractor Estimating: 350it [03:37,  1.50it/s]Extractor Estimating: 351it [03:37,  1.51it/s]Extractor Estimating: 352it [03:38,  1.56it/s]Extractor Estimating: 353it [03:38,  1.58it/s]Extractor Estimating: 354it [03:39,  1.55it/s]Extractor Estimating: 355it [03:40,  1.55it/s]Extractor Estimating: 356it [03:40,  1.54it/s]Extractor Estimating: 357it [03:41,  1.51it/s]Extractor Estimating: 358it [03:42,  1.52it/s]Extractor Estimating: 359it [03:42,  1.54it/s]Extractor Estimating: 360it [03:43,  1.59it/s]Extractor Estimating: 361it [03:44,  1.59it/s]Extractor Estimating: 362it [03:44,  1.56it/s]Extractor Estimating: 363it [03:45,  1.58it/s]Extractor Estimating: 364it [03:45,  1.55it/s]Extractor Estimating: 365it [03:46,  1.56it/s]Extractor Estimating: 366it [03:47,  1.63it/s]Extractor Estimating: 367it [03:47,  1.57it/s]Extractor Estimating: 368it [03:48,  1.60it/s]Extractor Estimating: 369it [03:49,  1.55it/s]Extractor Estimating: 370it [03:49,  1.58it/s]Extractor Estimating: 371it [03:50,  1.57it/s]Extractor Estimating: 372it [03:51,  1.57it/s]Extractor Estimating: 373it [03:51,  1.56it/s]Extractor Estimating: 374it [03:52,  1.56it/s]Extractor Estimating: 375it [03:52,  1.75it/s]Extractor Estimating: 375it [03:52,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:59,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:59,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:59,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:59,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:13:59,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:13:59,444 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:13:59,445 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:13:59,727 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:14:00,855 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:14:00,855 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:02,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:02,161 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:02,161 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:02,161 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:14:02,161 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:14:02,479 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:14:02,480 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:14:02,774 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:14:02,959 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:14:02,959 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:05:44,535 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:05:44,613 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 2999 mean pseudo reward: 0.9591868095511003
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 15421
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15521, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15521, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.040, loss:502.3751
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 75, avg_time 0.997, loss:453.9007
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 1.003, loss:447.4888
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 0.996, loss:423.0700
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 0.996, loss:411.0086
>> valid entity prec:0.5396, rec:0.6262, f1:0.5797
>> valid relation prec:0.3906, rec:0.1928, f1:0.2582
>> valid relation with NER prec:0.3906, rec:0.1928, f1:0.2582
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.008, loss:385.6191
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 75, avg_time 1.004, loss:363.6710
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 0.997, loss:377.0811
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 25, avg_time 0.994, loss:342.9099
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 125, avg_time 0.998, loss:350.2074
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5590, rec:0.5300, f1:0.5441
>> valid relation prec:0.3113, rec:0.1621, f1:0.2132
>> valid relation with NER prec:0.3113, rec:0.1621, f1:0.2132
g_step 1100, step 100, avg_time 0.996, loss:329.9157
g_step 1200, step 75, avg_time 1.010, loss:317.4299
g_step 1300, step 50, avg_time 0.984, loss:302.1117
g_step 1400, step 25, avg_time 1.000, loss:288.8935
g_step 1500, step 125, avg_time 0.999, loss:283.1537
>> valid entity prec:0.5769, rec:0.5353, f1:0.5553
>> valid relation prec:0.3279, rec:0.1667, f1:0.2210
>> valid relation with NER prec:0.3279, rec:0.1667, f1:0.2210
g_step 1600, step 100, avg_time 0.998, loss:267.3321
g_step 1700, step 75, avg_time 0.995, loss:243.4774
g_step 1800, step 50, avg_time 0.992, loss:239.0178
g_step 1900, step 25, avg_time 0.989, loss:241.8639
g_step 2000, step 125, avg_time 1.007, loss:210.8309
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5689, rec:0.5569, f1:0.5628
>> valid relation prec:0.2917, rec:0.1707, f1:0.2154
>> valid relation with NER prec:0.2917, rec:0.1707, f1:0.2154
g_step 2100, step 100, avg_time 0.999, loss:202.4575
g_step 2200, step 75, avg_time 0.984, loss:187.3523
g_step 2300, step 50, avg_time 1.003, loss:183.1738
g_step 2400, step 25, avg_time 0.991, loss:180.6077
g_step 2500, step 125, avg_time 0.994, loss:201.3663
>> valid entity prec:0.5312, rec:0.6072, f1:0.5666
>> valid relation prec:0.2659, rec:0.1954, f1:0.2252
>> valid relation with NER prec:0.2659, rec:0.1954, f1:0.2252
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:05:44 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:05:44 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-05-44_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:05:45 - WARNING - datasets.builder -   Using custom data configuration default-09396bdc7be90782
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-09396bdc7be90782/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:05:46,069 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:05:46,070 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:05:46,071 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:05:46,072 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:05:46,087 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:05:46,102 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:05:46,102 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:05:46,102 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:05:46,102 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:05:46,102 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:05:46,102 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:05:46,250 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:05:49,402 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:05:49,405 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-09396bdc7be90782/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  3.13ba/s] 67%|██████▋   | 2/3 [00:00<00:00,  3.97ba/s]100%|██████████| 3/3 [00:00<00:00,  4.37ba/s]100%|██████████| 3/3 [00:00<00:00,  4.13ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.96ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.28ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.39ba/s]100%|██████████| 4/4 [00:00<00:00,  5.47ba/s]100%|██████████| 4/4 [00:00<00:00,  4.95ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  6.00ba/s]100%|██████████| 3/3 [00:00<00:00,  9.18ba/s]100%|██████████| 3/3 [00:00<00:00,  8.71ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.24ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.10ba/s]100%|██████████| 4/4 [00:00<00:00, 11.45ba/s]
[INFO|trainer.py:414] 2023-08-28 19:05:52,374 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:05:52,389 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:05:52,389 >>   Num examples = 3000
[INFO|trainer.py:1149] 2023-08-28 19:05:52,389 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:05:52,389 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:05:52,389 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:05:52,389 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:05:52,389 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:00<01:09,  3.36it/s]  1%|          | 2/235 [00:00<01:08,  3.43it/s]  1%|▏         | 3/235 [00:00<01:07,  3.44it/s]  2%|▏         | 4/235 [00:01<01:06,  3.45it/s]  2%|▏         | 5/235 [00:01<01:06,  3.46it/s]  3%|▎         | 6/235 [00:01<01:06,  3.46it/s]  3%|▎         | 7/235 [00:02<01:05,  3.46it/s]  3%|▎         | 8/235 [00:02<01:05,  3.46it/s]  4%|▍         | 9/235 [00:02<01:05,  3.47it/s]  4%|▍         | 10/235 [00:02<01:04,  3.47it/s]  5%|▍         | 11/235 [00:03<01:04,  3.46it/s]  5%|▌         | 12/235 [00:03<01:05,  3.39it/s]  6%|▌         | 13/235 [00:03<01:05,  3.41it/s]  6%|▌         | 14/235 [00:04<01:04,  3.43it/s]  6%|▋         | 15/235 [00:04<01:03,  3.44it/s]  7%|▋         | 16/235 [00:04<01:03,  3.45it/s]  7%|▋         | 17/235 [00:04<01:03,  3.45it/s]  8%|▊         | 18/235 [00:05<01:02,  3.45it/s]  8%|▊         | 19/235 [00:05<01:02,  3.46it/s]  9%|▊         | 20/235 [00:05<01:02,  3.46it/s]  9%|▉         | 21/235 [00:06<01:01,  3.46it/s]  9%|▉         | 22/235 [00:06<01:01,  3.46it/s] 10%|▉         | 23/235 [00:06<01:01,  3.46it/s] 10%|█         | 24/235 [00:06<01:01,  3.46it/s] 11%|█         | 25/235 [00:07<01:00,  3.46it/s] 11%|█         | 26/235 [00:07<01:00,  3.45it/s] 11%|█▏        | 27/235 [00:07<01:00,  3.46it/s] 12%|█▏        | 28/235 [00:08<00:59,  3.46it/s] 12%|█▏        | 29/235 [00:08<00:59,  3.46it/s] 13%|█▎        | 30/235 [00:08<00:59,  3.46it/s] 13%|█▎        | 31/235 [00:08<00:59,  3.46it/s] 14%|█▎        | 32/235 [00:09<00:58,  3.45it/s] 14%|█▍        | 33/235 [00:09<00:58,  3.45it/s] 14%|█▍        | 34/235 [00:09<00:58,  3.45it/s] 15%|█▍        | 35/235 [00:10<00:57,  3.46it/s] 15%|█▌        | 36/235 [00:10<00:57,  3.45it/s] 16%|█▌        | 37/235 [00:10<00:57,  3.46it/s] 16%|█▌        | 38/235 [00:11<00:57,  3.46it/s] 17%|█▋        | 39/235 [00:11<00:56,  3.46it/s] 17%|█▋        | 40/235 [00:11<00:56,  3.45it/s] 17%|█▋        | 41/235 [00:11<00:56,  3.45it/s] 18%|█▊        | 42/235 [00:12<00:55,  3.45it/s] 18%|█▊        | 43/235 [00:12<00:55,  3.45it/s] 19%|█▊        | 44/235 [00:12<00:55,  3.46it/s] 19%|█▉        | 45/235 [00:13<00:55,  3.45it/s] 20%|█▉        | 46/235 [00:13<00:54,  3.46it/s] 20%|██        | 47/235 [00:13<00:52,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 19:06:05,975 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:06:05,975 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:06:05,975 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.17it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.10it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.35it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.33it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.97it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.75it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.67it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.53it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.37it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.56it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.57it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.40it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.23it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.34it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.24it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.19it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.36it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.42it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.48it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.44it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.23it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.21it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.16it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.17it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.14it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.32it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.44it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.44it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.30it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.25it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.25it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.29it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.15it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.24it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.29it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.41it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.38it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.26it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.17it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.26it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.16it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.04it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.20it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.18it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.40it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.28it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.23it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.27it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.23it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.24it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.19it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.22it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.34it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.39it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.30it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.23it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.22it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.24it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.20it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.18it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.26it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.24it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.37it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.34it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.25it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.20it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.23it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.19it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.17it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.22it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.27it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.21it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.27it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.17it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.18it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.22it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.18it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.22it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.24it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.28it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.19it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.17it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.14it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.15it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.18it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.25it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.21it/s][A                                                
                                                 [A 20%|██        | 47/235 [00:23<00:52,  3.58it/s]
100%|██████████| 437/437 [00:09<00:00, 44.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:06:15,916 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-28 19:06:16,040 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:06:19,116 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:06:19,141 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:06:19,155 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [00:34<20:27,  6.56s/it] 21%|██        | 49/235 [00:35<14:30,  4.68s/it] 21%|██▏       | 50/235 [00:35<10:22,  3.37s/it] 22%|██▏       | 51/235 [00:35<07:29,  2.44s/it] 22%|██▏       | 52/235 [00:35<05:29,  1.80s/it] 23%|██▎       | 53/235 [00:36<04:05,  1.35s/it] 23%|██▎       | 54/235 [00:36<03:06,  1.03s/it] 23%|██▎       | 55/235 [00:36<02:25,  1.23it/s] 24%|██▍       | 56/235 [00:37<01:57,  1.53it/s] 24%|██▍       | 57/235 [00:37<01:37,  1.83it/s] 25%|██▍       | 58/235 [00:37<01:23,  2.13it/s] 25%|██▌       | 59/235 [00:38<01:13,  2.41it/s] 26%|██▌       | 60/235 [00:38<01:06,  2.65it/s] 26%|██▌       | 61/235 [00:38<01:01,  2.84it/s] 26%|██▋       | 62/235 [00:38<00:57,  3.00it/s] 27%|██▋       | 63/235 [00:39<00:55,  3.12it/s] 27%|██▋       | 64/235 [00:39<00:53,  3.22it/s] 28%|██▊       | 65/235 [00:39<00:51,  3.28it/s] 28%|██▊       | 66/235 [00:40<00:50,  3.33it/s] 29%|██▊       | 67/235 [00:40<00:49,  3.37it/s] 29%|██▉       | 68/235 [00:40<00:49,  3.39it/s] 29%|██▉       | 69/235 [00:40<00:48,  3.41it/s] 30%|██▉       | 70/235 [00:41<00:48,  3.42it/s] 30%|███       | 71/235 [00:41<00:47,  3.43it/s] 31%|███       | 72/235 [00:41<00:47,  3.43it/s] 31%|███       | 73/235 [00:42<00:47,  3.44it/s] 31%|███▏      | 74/235 [00:42<00:46,  3.44it/s] 32%|███▏      | 75/235 [00:42<00:46,  3.44it/s] 32%|███▏      | 76/235 [00:42<00:46,  3.45it/s] 33%|███▎      | 77/235 [00:43<00:45,  3.45it/s] 33%|███▎      | 78/235 [00:43<00:45,  3.45it/s] 34%|███▎      | 79/235 [00:43<00:45,  3.45it/s] 34%|███▍      | 80/235 [00:44<00:44,  3.45it/s] 34%|███▍      | 81/235 [00:44<00:44,  3.45it/s] 35%|███▍      | 82/235 [00:44<00:44,  3.45it/s] 35%|███▌      | 83/235 [00:44<00:44,  3.45it/s] 36%|███▌      | 84/235 [00:45<00:43,  3.45it/s] 36%|███▌      | 85/235 [00:45<00:43,  3.45it/s] 37%|███▋      | 86/235 [00:45<00:43,  3.45it/s] 37%|███▋      | 87/235 [00:46<00:42,  3.45it/s] 37%|███▋      | 88/235 [00:46<00:42,  3.45it/s] 38%|███▊      | 89/235 [00:46<00:42,  3.43it/s] 38%|███▊      | 90/235 [00:47<00:42,  3.42it/s] 39%|███▊      | 91/235 [00:47<00:42,  3.41it/s] 39%|███▉      | 92/235 [00:47<00:41,  3.41it/s] 40%|███▉      | 93/235 [00:47<00:41,  3.41it/s] 40%|████      | 94/235 [00:48<00:39,  3.53it/s][INFO|trainer.py:2140] 2023-08-28 19:06:40,559 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:06:40,559 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:06:40,559 >>   Batch size = 8
{'eval_loss': 1.080496072769165, 'eval_runtime': 9.8685, 'eval_samples_per_second': 353.955, 'eval_steps_per_second': 44.282, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.35it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.82it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.17it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.28it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.77it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.40it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.27it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.17it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.25it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.31it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.42it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.34it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.08it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.99it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.96it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.95it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.90it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.67it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.98it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.11it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.05it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.92it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.89it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.88it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.86it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.82it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.82it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.15it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.34it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.24it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.25it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.15it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.91it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.95it/s][A
 41%|████      | 177/437 [00:04<00:06, 42.95it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.20it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.36it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.38it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.21it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.20it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.08it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.03it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.00it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.90it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.07it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.17it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.21it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.20it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.08it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.09it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.00it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.95it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.95it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.08it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.16it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.10it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.20it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.05it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.97it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.79it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.92it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.93it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.10it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.12it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.14it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.10it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.09it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.02it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.90it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.91it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.03it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 42.96it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.36it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.68it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.78it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.79it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.83it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.90it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.88it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.81it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.95it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.12it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.19it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.96it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.04it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.03it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.94it/s][A                                                
                                                 [A 40%|████      | 94/235 [00:58<00:39,  3.53it/s]
100%|██████████| 437/437 [00:09<00:00, 43.94it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:06:50,515 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 19:06:50,563 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:06:55,190 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:06:55,208 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:06:55,223 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [01:13<18:23,  7.88s/it] 41%|████      | 96/235 [01:14<12:59,  5.61s/it] 41%|████▏     | 97/235 [01:14<09:14,  4.02s/it] 42%|████▏     | 98/235 [01:14<06:37,  2.90s/it] 42%|████▏     | 99/235 [01:14<04:48,  2.12s/it] 43%|████▎     | 100/235 [01:15<03:32,  1.57s/it] 43%|████▎     | 101/235 [01:15<02:39,  1.19s/it] 43%|████▎     | 102/235 [01:15<02:02,  1.09it/s] 44%|████▍     | 103/235 [01:16<01:36,  1.37it/s] 44%|████▍     | 104/235 [01:16<01:18,  1.67it/s] 45%|████▍     | 105/235 [01:16<01:06,  1.97it/s] 45%|████▌     | 106/235 [01:17<00:57,  2.25it/s] 46%|████▌     | 107/235 [01:17<00:51,  2.50it/s] 46%|████▌     | 108/235 [01:17<00:46,  2.71it/s] 46%|████▋     | 109/235 [01:17<00:43,  2.89it/s] 47%|████▋     | 110/235 [01:18<00:41,  3.02it/s] 47%|████▋     | 111/235 [01:18<00:39,  3.13it/s] 48%|████▊     | 112/235 [01:18<00:38,  3.21it/s] 48%|████▊     | 113/235 [01:19<00:37,  3.26it/s] 49%|████▊     | 114/235 [01:19<00:36,  3.30it/s] 49%|████▉     | 115/235 [01:19<00:36,  3.33it/s] 49%|████▉     | 116/235 [01:19<00:35,  3.35it/s] 50%|████▉     | 117/235 [01:20<00:35,  3.36it/s] 50%|█████     | 118/235 [01:20<00:34,  3.35it/s] 51%|█████     | 119/235 [01:20<00:34,  3.36it/s] 51%|█████     | 120/235 [01:21<00:34,  3.37it/s] 51%|█████▏    | 121/235 [01:21<00:33,  3.38it/s] 52%|█████▏    | 122/235 [01:21<00:33,  3.38it/s] 52%|█████▏    | 123/235 [01:22<00:33,  3.39it/s] 53%|█████▎    | 124/235 [01:22<00:32,  3.39it/s] 53%|█████▎    | 125/235 [01:22<00:32,  3.39it/s] 54%|█████▎    | 126/235 [01:22<00:32,  3.39it/s] 54%|█████▍    | 127/235 [01:23<00:31,  3.40it/s] 54%|█████▍    | 128/235 [01:23<00:31,  3.41it/s] 55%|█████▍    | 129/235 [01:23<00:31,  3.36it/s] 55%|█████▌    | 130/235 [01:24<00:31,  3.38it/s] 56%|█████▌    | 131/235 [01:24<00:30,  3.40it/s] 56%|█████▌    | 132/235 [01:24<00:30,  3.41it/s] 57%|█████▋    | 133/235 [01:24<00:29,  3.42it/s] 57%|█████▋    | 134/235 [01:25<00:29,  3.43it/s] 57%|█████▋    | 135/235 [01:25<00:29,  3.43it/s] 58%|█████▊    | 136/235 [01:25<00:28,  3.44it/s] 58%|█████▊    | 137/235 [01:26<00:28,  3.44it/s] 59%|█████▊    | 138/235 [01:26<00:28,  3.44it/s] 59%|█████▉    | 139/235 [01:26<00:27,  3.44it/s] 60%|█████▉    | 140/235 [01:27<00:28,  3.38it/s] 60%|██████    | 141/235 [01:27<00:26,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 19:07:19,684 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:07:19,684 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:07:19,684 >>   Batch size = 8
{'eval_loss': 1.0763835906982422, 'eval_runtime': 9.9279, 'eval_samples_per_second': 351.836, 'eval_steps_per_second': 44.017, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.86it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.17it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.08it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.14it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.80it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.30it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.09it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.20it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.28it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.42it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.28it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.25it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.12it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.04it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.85it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.63it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.86it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.06it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.18it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.22it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.18it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.12it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.96it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.82it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.84it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.96it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.18it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.20it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.14it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.17it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.12it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.93it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.84it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.87it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.95it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.13it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.16it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.16it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.16it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.10it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.88it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.89it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.91it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.98it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.10it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.18it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.13it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.11it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.02it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.93it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.88it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.98it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.08it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.15it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.13it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.06it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.06it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.96it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.97it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.88it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.97it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.13it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.03it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.03it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.10it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.03it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.95it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.96it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.87it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.07it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.05it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.05it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.00it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.05it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.05it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.93it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.94it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.97it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.07it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.08it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.04it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.05it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.00it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.04it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.88it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.87it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.03it/s][A                                                 
                                                 [A 60%|██████    | 141/235 [01:37<00:26,  3.52it/s]
100%|██████████| 437/437 [00:09<00:00, 44.03it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:07:29,621 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-28 19:07:29,646 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:07:32,278 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:07:32,486 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:07:32,533 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [01:48<10:22,  6.70s/it] 61%|██████    | 143/235 [01:49<07:20,  4.79s/it] 61%|██████▏   | 144/235 [01:49<05:12,  3.44s/it] 62%|██████▏   | 145/235 [01:49<03:44,  2.50s/it] 62%|██████▏   | 146/235 [01:50<02:43,  1.84s/it] 63%|██████▎   | 147/235 [01:50<02:00,  1.37s/it] 63%|██████▎   | 148/235 [01:50<01:31,  1.05s/it] 63%|██████▎   | 149/235 [01:51<01:10,  1.22it/s] 64%|██████▍   | 150/235 [01:51<00:56,  1.51it/s] 64%|██████▍   | 151/235 [01:51<00:46,  1.81it/s] 65%|██████▍   | 152/235 [01:51<00:39,  2.10it/s] 65%|██████▌   | 153/235 [01:52<00:38,  2.16it/s] 66%|██████▌   | 154/235 [01:52<00:33,  2.41it/s] 66%|██████▌   | 155/235 [01:52<00:30,  2.64it/s] 66%|██████▋   | 156/235 [01:53<00:27,  2.83it/s] 67%|██████▋   | 157/235 [01:53<00:26,  2.99it/s] 67%|██████▋   | 158/235 [01:53<00:24,  3.11it/s] 68%|██████▊   | 159/235 [01:54<00:23,  3.20it/s] 68%|██████▊   | 160/235 [01:54<00:22,  3.27it/s] 69%|██████▊   | 161/235 [01:54<00:22,  3.32it/s] 69%|██████▉   | 162/235 [01:54<00:21,  3.36it/s] 69%|██████▉   | 163/235 [01:55<00:21,  3.38it/s] 70%|██████▉   | 164/235 [01:55<00:20,  3.40it/s] 70%|███████   | 165/235 [01:55<00:20,  3.40it/s] 71%|███████   | 166/235 [01:56<00:20,  3.42it/s] 71%|███████   | 167/235 [01:56<00:19,  3.43it/s] 71%|███████▏  | 168/235 [01:56<00:19,  3.43it/s] 72%|███████▏  | 169/235 [01:57<00:19,  3.43it/s] 72%|███████▏  | 170/235 [01:57<00:18,  3.43it/s] 73%|███████▎  | 171/235 [01:57<00:18,  3.44it/s] 73%|███████▎  | 172/235 [01:57<00:18,  3.44it/s] 74%|███████▎  | 173/235 [01:58<00:18,  3.44it/s] 74%|███████▍  | 174/235 [01:58<00:17,  3.44it/s] 74%|███████▍  | 175/235 [01:58<00:17,  3.44it/s] 75%|███████▍  | 176/235 [01:59<00:17,  3.40it/s] 75%|███████▌  | 177/235 [01:59<00:16,  3.42it/s] 76%|███████▌  | 178/235 [01:59<00:16,  3.42it/s] 76%|███████▌  | 179/235 [01:59<00:16,  3.43it/s] 77%|███████▋  | 180/235 [02:00<00:16,  3.43it/s] 77%|███████▋  | 181/235 [02:00<00:15,  3.44it/s] 77%|███████▋  | 182/235 [02:00<00:15,  3.44it/s] 78%|███████▊  | 183/235 [02:01<00:15,  3.44it/s] 78%|███████▊  | 184/235 [02:01<00:14,  3.44it/s] 79%|███████▊  | 185/235 [02:01<00:14,  3.44it/s] 79%|███████▉  | 186/235 [02:01<00:14,  3.44it/s] 80%|███████▉  | 187/235 [02:02<00:14,  3.40it/s] 80%|████████  | 188/235 [02:02<00:13,  3.53it/s][INFO|trainer.py:2140] 2023-08-28 19:07:54,936 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:07:54,936 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:07:54,936 >>   Batch size = 8
{'eval_loss': 1.0865780115127563, 'eval_runtime': 9.9197, 'eval_samples_per_second': 352.126, 'eval_steps_per_second': 44.054, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.47it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.08it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.20it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.21it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.79it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.59it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.50it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.38it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.60it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.66it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.48it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.19it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.96it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.98it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.09it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.09it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.26it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.38it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.58it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.40it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.18it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.08it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.95it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.99it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.22it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.30it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.42it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.47it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.42it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.13it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.15it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.12it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.02it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.15it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.32it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.33it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.41it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.24it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.23it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.13it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.10it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.12it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.25it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.35it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.18it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.32it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.28it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.18it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.01it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 41.49it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 42.38it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.02it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.50it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.87it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.05it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.17it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.01it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.70it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.78it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.88it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.24it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.39it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.41it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.38it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.35it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.06it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.88it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.85it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.05it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.18it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.28it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.42it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.50it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.34it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.11it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.01it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.01it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.03it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.19it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.29it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.46it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.35it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.21it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.10it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.98it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.02it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.03it/s][A                                                 
                                                 [A 80%|████████  | 188/235 [02:12<00:13,  3.53it/s]
100%|██████████| 437/437 [00:09<00:00, 44.03it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:08:04,905 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 19:08:04,928 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:08:10,431 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:08:10,463 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:08:10,581 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [02:27<05:55,  7.72s/it] 81%|████████  | 190/235 [02:27<04:07,  5.50s/it] 81%|████████▏ | 191/235 [02:28<02:53,  3.94s/it] 82%|████████▏ | 192/235 [02:28<02:02,  2.84s/it] 82%|████████▏ | 193/235 [02:28<01:27,  2.08s/it] 83%|████████▎ | 194/235 [02:29<01:03,  1.54s/it] 83%|████████▎ | 195/235 [02:29<00:46,  1.17s/it] 83%|████████▎ | 196/235 [02:29<00:35,  1.10it/s] 84%|████████▍ | 197/235 [02:29<00:27,  1.38it/s] 84%|████████▍ | 198/235 [02:30<00:21,  1.68it/s] 85%|████████▍ | 199/235 [02:30<00:18,  1.98it/s] 85%|████████▌ | 200/235 [02:30<00:15,  2.27it/s] 86%|████████▌ | 201/235 [02:31<00:13,  2.52it/s] 86%|████████▌ | 202/235 [02:31<00:12,  2.73it/s] 86%|████████▋ | 203/235 [02:31<00:11,  2.90it/s] 87%|████████▋ | 204/235 [02:32<00:10,  3.03it/s] 87%|████████▋ | 205/235 [02:32<00:09,  3.14it/s] 88%|████████▊ | 206/235 [02:32<00:09,  3.21it/s] 88%|████████▊ | 207/235 [02:32<00:08,  3.26it/s] 89%|████████▊ | 208/235 [02:33<00:08,  3.30it/s] 89%|████████▉ | 209/235 [02:33<00:07,  3.33it/s] 89%|████████▉ | 210/235 [02:33<00:07,  3.35it/s] 90%|████████▉ | 211/235 [02:34<00:07,  3.29it/s] 90%|█████████ | 212/235 [02:34<00:06,  3.33it/s] 91%|█████████ | 213/235 [02:34<00:06,  3.35it/s] 91%|█████████ | 214/235 [02:35<00:06,  3.36it/s] 91%|█████████▏| 215/235 [02:35<00:05,  3.37it/s] 92%|█████████▏| 216/235 [02:35<00:05,  3.38it/s] 92%|█████████▏| 217/235 [02:35<00:05,  3.39it/s] 93%|█████████▎| 218/235 [02:36<00:04,  3.41it/s] 93%|█████████▎| 219/235 [02:36<00:04,  3.42it/s] 94%|█████████▎| 220/235 [02:36<00:04,  3.42it/s] 94%|█████████▍| 221/235 [02:37<00:04,  3.43it/s] 94%|█████████▍| 222/235 [02:37<00:03,  3.43it/s] 95%|█████████▍| 223/235 [02:37<00:03,  3.44it/s] 95%|█████████▌| 224/235 [02:37<00:03,  3.44it/s] 96%|█████████▌| 225/235 [02:38<00:02,  3.44it/s] 96%|█████████▌| 226/235 [02:38<00:02,  3.44it/s] 97%|█████████▋| 227/235 [02:38<00:02,  3.44it/s] 97%|█████████▋| 228/235 [02:39<00:02,  3.44it/s] 97%|█████████▋| 229/235 [02:39<00:01,  3.44it/s] 98%|█████████▊| 230/235 [02:39<00:01,  3.44it/s] 98%|█████████▊| 231/235 [02:39<00:01,  3.44it/s] 99%|█████████▊| 232/235 [02:40<00:00,  3.44it/s] 99%|█████████▉| 233/235 [02:40<00:00,  3.44it/s]100%|█████████▉| 234/235 [02:40<00:00,  3.44it/s]100%|██████████| 235/235 [02:41<00:00,  3.56it/s][INFO|trainer.py:2140] 2023-08-28 19:08:33,482 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:08:33,483 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:08:33,483 >>   Batch size = 8
{'eval_loss': 1.0892560482025146, 'eval_runtime': 9.9025, 'eval_samples_per_second': 352.739, 'eval_steps_per_second': 44.13, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.19it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.89it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.14it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.42it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.01it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.48it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.52it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.49it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.35it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.42it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.48it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.42it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.28it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.16it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.17it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.23it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.26it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.18it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.31it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.41it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.34it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.19it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.07it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.11it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.20it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.13it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.11it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.33it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.42it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.18it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.00it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.08it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.19it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.24it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.19it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.17it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.35it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.33it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.22it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.15it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.10it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.20it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.21it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.21it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.22it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.29it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.36it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.19it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.07it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.11it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.25it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.17it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.20it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.30it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.24it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.16it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.15it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.10it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.05it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.24it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.21it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.15it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.26it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.21it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.18it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.10it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.12it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.12it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.09it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.08it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.18it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.19it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.23it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.09it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.11it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.21it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.13it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.10it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.18it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.19it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.22it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.18it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.09it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.14it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.12it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.16it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.18it/s][A                                                 
                                                 [A100%|██████████| 235/235 [02:50<00:00,  3.56it/s]
100%|██████████| 437/437 [00:09<00:00, 44.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:08:43,388 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-28 19:08:43,422 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:08:45,924 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:08:46,044 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:08:46,269 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:08:53,082 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:08:53,087 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94 (score: 1.0763835906982422).
                                                 100%|██████████| 235/235 [03:06<00:00,  3.56it/s]100%|██████████| 235/235 [03:06<00:00,  1.26it/s]
[INFO|trainer.py:1894] 2023-08-28 19:08:58,982 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 19:08:59,010 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:09:03,403 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:09:03,510 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:09:03,716 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:09:04,021 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:04,022 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:04,022 >>   train_loss               =      0.475
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:04,022 >>   train_runtime            = 0:03:06.58
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:04,022 >>   train_samples            =       3000
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:04,022 >>   train_samples_per_second =     80.394
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:04,022 >>   train_steps_per_second   =      1.259
{'eval_loss': 1.093895673751831, 'eval_runtime': 9.8828, 'eval_samples_per_second': 353.444, 'eval_steps_per_second': 44.218, 'epoch': 5.0}
{'train_runtime': 186.582, 'train_samples_per_second': 80.394, 'train_steps_per_second': 1.259, 'train_loss': 0.4750022401200964, 'epoch': 5.0}
08/28/2023 19:09:04 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:09:04,074 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:09:04,074 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:09:04,074 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.92it/s]  3%|▎         | 12/437 [00:00<00:08, 48.56it/s]  4%|▍         | 17/437 [00:00<00:08, 46.93it/s]  5%|▌         | 22/437 [00:00<00:08, 46.24it/s]  6%|▌         | 27/437 [00:00<00:08, 45.74it/s]  7%|▋         | 32/437 [00:00<00:08, 45.56it/s]  8%|▊         | 37/437 [00:00<00:08, 45.34it/s] 10%|▉         | 42/437 [00:00<00:08, 44.77it/s] 11%|█         | 47/437 [00:01<00:08, 44.21it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.73it/s] 13%|█▎        | 57/437 [00:01<00:08, 43.76it/s] 14%|█▍        | 62/437 [00:01<00:08, 43.95it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.18it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.52it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.57it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.59it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.31it/s] 21%|██        | 92/437 [00:02<00:07, 43.88it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.59it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.74it/s] 24%|██▍       | 107/437 [00:02<00:07, 43.96it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.23it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.35it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.55it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.56it/s] 30%|███       | 132/437 [00:02<00:06, 44.37it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.10it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.78it/s] 34%|███▎      | 147/437 [00:03<00:06, 43.75it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.04it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.31it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.46it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.54it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.57it/s] 41%|████      | 177/437 [00:03<00:05, 44.31it/s] 42%|████▏     | 182/437 [00:04<00:05, 43.91it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.83it/s] 44%|████▍     | 192/437 [00:04<00:05, 43.86it/s] 45%|████▌     | 197/437 [00:04<00:05, 43.96it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.28it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.45it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.60it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.45it/s] 51%|█████     | 222/437 [00:04<00:04, 44.21it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.02it/s] 53%|█████▎    | 232/437 [00:05<00:04, 43.81it/s] 54%|█████▍    | 237/437 [00:05<00:04, 43.92it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.02it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.23it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.50it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.48it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.41it/s] 61%|██████    | 267/437 [00:06<00:03, 44.17it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.01it/s] 63%|██████▎   | 277/437 [00:06<00:03, 43.97it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.05it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.08it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.29it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.38it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.42it/s] 70%|███████   | 307/437 [00:06<00:02, 44.38it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.11it/s] 73%|███████▎  | 317/437 [00:07<00:02, 43.98it/s] 74%|███████▎  | 322/437 [00:07<00:02, 43.98it/s] 75%|███████▍  | 327/437 [00:07<00:02, 43.97it/s] 76%|███████▌  | 332/437 [00:07<00:02, 40.65it/s] 77%|███████▋  | 337/437 [00:07<00:02, 41.81it/s] 78%|███████▊  | 342/437 [00:07<00:02, 42.62it/s] 79%|███████▉  | 347/437 [00:07<00:02, 43.22it/s] 81%|████████  | 352/437 [00:07<00:01, 43.59it/s] 82%|████████▏ | 357/437 [00:08<00:01, 43.69it/s] 83%|████████▎ | 362/437 [00:08<00:01, 43.77it/s] 84%|████████▍ | 367/437 [00:08<00:01, 43.81it/s] 85%|████████▌ | 372/437 [00:08<00:01, 43.56it/s] 86%|████████▋ | 377/437 [00:08<00:01, 43.67it/s] 87%|████████▋ | 382/437 [00:08<00:01, 43.94it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.17it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.15it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.33it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.23it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.12it/s] 94%|█████████▍| 412/437 [00:09<00:00, 43.94it/s] 95%|█████████▌| 417/437 [00:09<00:00, 43.89it/s] 97%|█████████▋| 422/437 [00:09<00:00, 43.94it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.03it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.23it/s]100%|██████████| 437/437 [00:09<00:00, 44.36it/s]100%|██████████| 437/437 [00:09<00:00, 44.16it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:09:13,988 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:13,988 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:13,988 >>   eval_loss               =     1.0764
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:13,988 >>   eval_runtime            = 0:00:09.91
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:13,988 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:13,988 >>   eval_samples_per_second =    352.354
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:13,988 >>   eval_steps_per_second   =     44.082
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:09:13,988 >>   perplexity              =      2.934
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:19,795 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:19,800 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:19,800 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:19,800 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:19,801 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:09:20,225 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:09:20,226 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:09:20,482 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:09:21,617 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:09:21,617 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:23,345 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:23,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:23,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:23,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:09:23,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:09:24,108 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:09:24,109 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:09:24,773 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:09:24,969 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:09:24,969 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-47
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-141
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-94
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/checkpoint-235
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.36it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:06,  1.53it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:08,  1.56it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:09,  1.59it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:11,  1.57it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.58it/s]Extractor Predicting: 23it [00:15,  1.59it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:16,  1.60it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.60it/s]Extractor Predicting: 31it [00:20,  1.61it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.58it/s]Extractor Predicting: 34it [00:21,  1.56it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:25,  1.59it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.60it/s]Extractor Predicting: 42it [00:27,  1.61it/s]Extractor Predicting: 43it [00:27,  1.58it/s]Extractor Predicting: 44it [00:28,  1.62it/s]Extractor Predicting: 45it [00:28,  1.59it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:30,  1.57it/s]Extractor Predicting: 48it [00:30,  1.63it/s]Extractor Predicting: 49it [00:31,  1.65it/s]Extractor Predicting: 50it [00:32,  1.60it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.61it/s]Extractor Predicting: 53it [00:34,  1.52it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:35,  1.55it/s]Extractor Predicting: 57it [00:36,  1.59it/s]Extractor Predicting: 58it [00:37,  1.57it/s]Extractor Predicting: 59it [00:37,  1.56it/s]Extractor Predicting: 60it [00:38,  1.54it/s]Extractor Predicting: 61it [00:39,  1.56it/s]Extractor Predicting: 62it [00:39,  1.55it/s]Extractor Predicting: 63it [00:40,  1.55it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:41,  1.54it/s]Extractor Predicting: 66it [00:42,  1.54it/s]Extractor Predicting: 67it [00:43,  1.52it/s]Extractor Predicting: 68it [00:43,  1.49it/s]Extractor Predicting: 69it [00:44,  1.51it/s]Extractor Predicting: 70it [00:45,  1.51it/s]Extractor Predicting: 71it [00:45,  1.53it/s]Extractor Predicting: 72it [00:46,  1.56it/s]Extractor Predicting: 73it [00:46,  1.55it/s]Extractor Predicting: 74it [00:47,  1.50it/s]Extractor Predicting: 75it [00:48,  1.55it/s]Extractor Predicting: 76it [00:48,  1.53it/s]Extractor Predicting: 77it [00:49,  1.53it/s]Extractor Predicting: 78it [00:50,  1.51it/s]Extractor Predicting: 79it [00:50,  1.52it/s]Extractor Predicting: 80it [00:51,  1.52it/s]Extractor Predicting: 81it [00:52,  1.51it/s]Extractor Predicting: 82it [00:52,  1.52it/s]Extractor Predicting: 83it [00:53,  1.56it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:54,  1.54it/s]Extractor Predicting: 86it [00:55,  1.49it/s]Extractor Predicting: 87it [00:56,  1.54it/s]Extractor Predicting: 88it [00:56,  1.53it/s]Extractor Predicting: 89it [00:57,  1.52it/s]Extractor Predicting: 90it [00:58,  1.49it/s]Extractor Predicting: 91it [00:58,  1.49it/s]Extractor Predicting: 92it [00:59,  1.51it/s]Extractor Predicting: 93it [01:00,  1.50it/s]Extractor Predicting: 94it [01:00,  1.53it/s]Extractor Predicting: 95it [01:01,  1.57it/s]Extractor Predicting: 96it [01:01,  1.58it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:03,  1.56it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.57it/s]Extractor Predicting: 101it [01:05,  1.56it/s]Extractor Predicting: 102it [01:05,  1.55it/s]Extractor Predicting: 103it [01:06,  1.54it/s]Extractor Predicting: 104it [01:07,  1.48it/s]Extractor Predicting: 105it [01:07,  1.50it/s]Extractor Predicting: 106it [01:08,  1.50it/s]Extractor Predicting: 107it [01:09,  1.51it/s]Extractor Predicting: 108it [01:09,  1.51it/s]Extractor Predicting: 109it [01:10,  1.52it/s]Extractor Predicting: 110it [01:11,  1.51it/s]Extractor Predicting: 111it [01:11,  1.51it/s]Extractor Predicting: 112it [01:12,  1.52it/s]Extractor Predicting: 113it [01:13,  1.50it/s]Extractor Predicting: 114it [01:13,  1.49it/s]Extractor Predicting: 115it [01:14,  1.48it/s]Extractor Predicting: 116it [01:15,  1.49it/s]Extractor Predicting: 117it [01:15,  1.48it/s]Extractor Predicting: 118it [01:16,  1.49it/s]Extractor Predicting: 119it [01:17,  1.49it/s]Extractor Predicting: 120it [01:17,  1.51it/s]Extractor Predicting: 121it [01:18,  1.49it/s]Extractor Predicting: 122it [01:19,  1.49it/s]Extractor Predicting: 123it [01:19,  1.54it/s]Extractor Predicting: 124it [01:20,  1.53it/s]Extractor Predicting: 125it [01:21,  1.51it/s]Extractor Predicting: 126it [01:21,  1.43it/s]Extractor Predicting: 127it [01:22,  1.45it/s]Extractor Predicting: 128it [01:23,  1.47it/s]Extractor Predicting: 129it [01:23,  1.47it/s]Extractor Predicting: 130it [01:24,  1.48it/s]Extractor Predicting: 131it [01:25,  1.45it/s]Extractor Predicting: 132it [01:26,  1.44it/s]Extractor Predicting: 133it [01:26,  1.45it/s]Extractor Predicting: 134it [01:27,  1.43it/s]Extractor Predicting: 135it [01:28,  1.45it/s]Extractor Predicting: 136it [01:28,  1.78it/s]Extractor Predicting: 136it [01:28,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:03,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:03,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:03,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:03,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:03,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:11:04,762 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:11:04,763 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:11:05,677 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:11:06,936 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:11:06,936 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:10,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:10,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:10,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:10,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:11:10,659 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:11:11,743 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:11:11,744 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:11:12,490 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:11:13,344 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:11:13,344 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4881118881118881,
  "recall": 0.1998282278843401,
  "score": 0.2835669307332927,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.66it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:17,  1.68it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.65it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:20,  1.74it/s]Extractor Predicting: 34it [00:20,  1.72it/s]Extractor Predicting: 35it [00:21,  1.68it/s]Extractor Predicting: 36it [00:21,  1.65it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.70it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:26,  1.72it/s]Extractor Predicting: 44it [00:26,  1.70it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:27,  1.69it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:29,  1.69it/s]Extractor Predicting: 49it [00:29,  1.70it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:30,  1.75it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:31,  1.71it/s]Extractor Predicting: 54it [00:32,  1.52it/s]Extractor Predicting: 55it [00:33,  1.58it/s]Extractor Predicting: 56it [00:34,  1.57it/s]Extractor Predicting: 57it [00:34,  1.59it/s]Extractor Predicting: 58it [00:35,  1.62it/s]Extractor Predicting: 59it [00:35,  1.61it/s]Extractor Predicting: 60it [00:36,  1.61it/s]Extractor Predicting: 61it [00:37,  1.62it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:38,  1.60it/s]Extractor Predicting: 64it [00:38,  1.66it/s]Extractor Predicting: 65it [00:39,  1.63it/s]Extractor Predicting: 66it [00:40,  1.62it/s]Extractor Predicting: 67it [00:40,  1.64it/s]Extractor Predicting: 68it [00:41,  1.69it/s]Extractor Predicting: 69it [00:41,  1.71it/s]Extractor Predicting: 70it [00:42,  1.69it/s]Extractor Predicting: 71it [00:43,  1.67it/s]Extractor Predicting: 72it [00:43,  1.64it/s]Extractor Predicting: 73it [00:44,  1.67it/s]Extractor Predicting: 74it [00:44,  1.63it/s]Extractor Predicting: 75it [00:45,  1.63it/s]Extractor Predicting: 76it [00:46,  1.63it/s]Extractor Predicting: 77it [00:46,  1.62it/s]Extractor Predicting: 78it [00:47,  1.63it/s]Extractor Predicting: 79it [00:48,  1.63it/s]Extractor Predicting: 80it [00:48,  1.65it/s]Extractor Predicting: 81it [00:49,  1.65it/s]Extractor Predicting: 82it [00:49,  1.67it/s]Extractor Predicting: 83it [00:50,  1.62it/s]Extractor Predicting: 84it [00:51,  1.63it/s]Extractor Predicting: 85it [00:51,  1.59it/s]Extractor Predicting: 86it [00:52,  1.57it/s]Extractor Predicting: 87it [00:52,  1.62it/s]Extractor Predicting: 88it [00:53,  1.58it/s]Extractor Predicting: 89it [00:54,  1.57it/s]Extractor Predicting: 90it [00:54,  1.53it/s]Extractor Predicting: 91it [00:55,  1.54it/s]Extractor Predicting: 92it [00:56,  1.53it/s]Extractor Predicting: 93it [00:56,  1.52it/s]Extractor Predicting: 94it [00:57,  1.53it/s]Extractor Predicting: 95it [00:58,  1.55it/s]Extractor Predicting: 96it [00:58,  1.55it/s]Extractor Predicting: 97it [00:59,  1.53it/s]Extractor Predicting: 98it [01:00,  1.53it/s]Extractor Predicting: 99it [01:00,  1.51it/s]Extractor Predicting: 100it [01:01,  1.50it/s]Extractor Predicting: 101it [01:02,  1.50it/s]Extractor Predicting: 102it [01:02,  1.48it/s]Extractor Predicting: 103it [01:03,  1.52it/s]Extractor Predicting: 104it [01:04,  1.52it/s]Extractor Predicting: 105it [01:04,  1.51it/s]Extractor Predicting: 106it [01:05,  1.48it/s]Extractor Predicting: 107it [01:06,  1.49it/s]Extractor Predicting: 108it [01:06,  1.46it/s]Extractor Predicting: 109it [01:07,  1.47it/s]Extractor Predicting: 110it [01:08,  1.41it/s]Extractor Predicting: 111it [01:08,  1.45it/s]Extractor Predicting: 112it [01:09,  1.45it/s]Extractor Predicting: 113it [01:10,  1.46it/s]Extractor Predicting: 114it [01:11,  1.47it/s]Extractor Predicting: 115it [01:11,  1.38it/s]Extractor Predicting: 116it [01:12,  1.42it/s]Extractor Predicting: 117it [01:13,  1.45it/s]Extractor Predicting: 118it [01:13,  1.48it/s]Extractor Predicting: 119it [01:14,  1.51it/s]Extractor Predicting: 120it [01:15,  1.54it/s]Extractor Predicting: 121it [01:15,  1.54it/s]Extractor Predicting: 122it [01:16,  1.54it/s]Extractor Predicting: 123it [01:17,  1.55it/s]Extractor Predicting: 124it [01:17,  1.54it/s]Extractor Predicting: 125it [01:18,  1.53it/s]Extractor Predicting: 126it [01:18,  1.58it/s]Extractor Predicting: 127it [01:19,  1.59it/s]Extractor Predicting: 128it [01:20,  1.62it/s]Extractor Predicting: 129it [01:20,  1.60it/s]Extractor Predicting: 130it [01:21,  1.56it/s]Extractor Predicting: 131it [01:22,  1.58it/s]Extractor Predicting: 132it [01:22,  1.58it/s]Extractor Predicting: 133it [01:23,  1.61it/s]Extractor Predicting: 134it [01:23,  1.64it/s]Extractor Predicting: 135it [01:24,  1.61it/s]Extractor Predicting: 136it [01:25,  1.63it/s]Extractor Predicting: 137it [01:25,  1.62it/s]Extractor Predicting: 138it [01:26,  1.60it/s]Extractor Predicting: 139it [01:26,  1.61it/s]Extractor Predicting: 140it [01:27,  1.61it/s]Extractor Predicting: 141it [01:28,  1.56it/s]Extractor Predicting: 142it [01:28,  1.55it/s]Extractor Predicting: 143it [01:29,  1.56it/s]Extractor Predicting: 144it [01:30,  1.55it/s]Extractor Predicting: 145it [01:31,  1.40it/s]Extractor Predicting: 146it [01:31,  1.47it/s]Extractor Predicting: 147it [01:32,  1.46it/s]Extractor Predicting: 148it [01:33,  1.48it/s]Extractor Predicting: 149it [01:33,  1.51it/s]Extractor Predicting: 150it [01:34,  1.52it/s]Extractor Predicting: 151it [01:35,  1.49it/s]Extractor Predicting: 152it [01:35,  1.50it/s]Extractor Predicting: 153it [01:36,  1.51it/s]Extractor Predicting: 154it [01:36,  1.56it/s]Extractor Predicting: 155it [01:37,  1.53it/s]Extractor Predicting: 156it [01:38,  1.54it/s]Extractor Predicting: 157it [01:38,  1.51it/s]Extractor Predicting: 158it [01:39,  1.53it/s]Extractor Predicting: 159it [01:40,  1.53it/s]Extractor Predicting: 160it [01:40,  1.51it/s]Extractor Predicting: 161it [01:41,  1.53it/s]Extractor Predicting: 162it [01:42,  1.51it/s]Extractor Predicting: 163it [01:42,  1.51it/s]Extractor Predicting: 164it [01:43,  1.53it/s]Extractor Predicting: 165it [01:44,  1.52it/s]Extractor Predicting: 166it [01:44,  1.53it/s]Extractor Predicting: 167it [01:45,  1.51it/s]Extractor Predicting: 168it [01:46,  1.51it/s]Extractor Predicting: 169it [01:46,  1.51it/s]Extractor Predicting: 170it [01:47,  1.50it/s]Extractor Predicting: 171it [01:48,  1.51it/s]Extractor Predicting: 172it [01:48,  1.50it/s]Extractor Predicting: 173it [01:49,  1.50it/s]Extractor Predicting: 174it [01:50,  1.50it/s]Extractor Predicting: 175it [01:50,  1.56it/s]Extractor Predicting: 176it [01:51,  1.54it/s]Extractor Predicting: 177it [01:52,  1.57it/s]Extractor Predicting: 178it [01:52,  1.57it/s]Extractor Predicting: 179it [01:53,  1.58it/s]Extractor Predicting: 180it [01:53,  1.57it/s]Extractor Predicting: 181it [01:54,  1.59it/s]Extractor Predicting: 182it [01:55,  1.61it/s]Extractor Predicting: 183it [01:55,  1.55it/s]Extractor Predicting: 184it [01:56,  1.61it/s]Extractor Predicting: 185it [01:57,  1.59it/s]Extractor Predicting: 186it [01:57,  1.61it/s]Extractor Predicting: 187it [01:58,  1.61it/s]Extractor Predicting: 188it [01:58,  1.57it/s]Extractor Predicting: 189it [01:59,  1.53it/s]Extractor Predicting: 190it [02:00,  1.54it/s]Extractor Predicting: 191it [02:00,  1.53it/s]Extractor Predicting: 192it [02:01,  1.54it/s]Extractor Predicting: 193it [02:02,  1.58it/s]Extractor Predicting: 194it [02:02,  1.57it/s]Extractor Predicting: 195it [02:03,  1.55it/s]Extractor Predicting: 196it [02:04,  1.51it/s]Extractor Predicting: 197it [02:04,  1.54it/s]Extractor Predicting: 198it [02:05,  1.53it/s]Extractor Predicting: 199it [02:06,  1.55it/s]Extractor Predicting: 200it [02:06,  1.54it/s]Extractor Predicting: 201it [02:07,  1.57it/s]Extractor Predicting: 202it [02:08,  1.58it/s]Extractor Predicting: 203it [02:08,  1.58it/s]Extractor Predicting: 204it [02:09,  1.61it/s]Extractor Predicting: 205it [02:09,  1.59it/s]Extractor Predicting: 206it [02:10,  1.63it/s]Extractor Predicting: 207it [02:11,  1.60it/s]Extractor Predicting: 208it [02:11,  1.58it/s]Extractor Predicting: 209it [02:12,  1.59it/s]Extractor Predicting: 210it [02:13,  1.59it/s]Extractor Predicting: 211it [02:13,  1.58it/s]Extractor Predicting: 212it [02:14,  1.59it/s]Extractor Predicting: 213it [02:14,  1.60it/s]Extractor Predicting: 214it [02:15,  1.58it/s]Extractor Predicting: 215it [02:16,  1.58it/s]Extractor Predicting: 216it [02:16,  1.58it/s]Extractor Predicting: 217it [02:17,  1.56it/s]Extractor Predicting: 218it [02:18,  1.58it/s]Extractor Predicting: 219it [02:18,  1.53it/s]Extractor Predicting: 220it [02:19,  1.56it/s]Extractor Predicting: 221it [02:20,  1.59it/s]Extractor Predicting: 222it [02:20,  1.59it/s]Extractor Predicting: 223it [02:21,  1.55it/s]Extractor Predicting: 224it [02:21,  1.60it/s]Extractor Predicting: 225it [02:22,  1.58it/s]Extractor Predicting: 226it [02:23,  1.56it/s]Extractor Predicting: 227it [02:23,  1.55it/s]Extractor Predicting: 228it [02:24,  1.57it/s]Extractor Predicting: 229it [02:25,  1.57it/s]Extractor Predicting: 230it [02:25,  1.59it/s]Extractor Predicting: 231it [02:26,  1.60it/s]Extractor Predicting: 232it [02:27,  1.57it/s]Extractor Predicting: 233it [02:27,  1.43it/s]Extractor Predicting: 234it [02:28,  1.46it/s]Extractor Predicting: 235it [02:29,  1.47it/s]Extractor Predicting: 236it [02:29,  1.50it/s]Extractor Predicting: 237it [02:30,  1.51it/s]Extractor Predicting: 238it [02:31,  1.52it/s]Extractor Predicting: 239it [02:31,  1.52it/s]Extractor Predicting: 240it [02:32,  1.53it/s]Extractor Predicting: 241it [02:33,  1.53it/s]Extractor Predicting: 242it [02:33,  1.54it/s]Extractor Predicting: 243it [02:34,  1.55it/s]Extractor Predicting: 244it [02:35,  1.53it/s]Extractor Predicting: 245it [02:35,  1.55it/s]Extractor Predicting: 246it [02:36,  1.55it/s]Extractor Predicting: 247it [02:36,  1.57it/s]Extractor Predicting: 248it [02:37,  1.56it/s]Extractor Predicting: 249it [02:38,  1.59it/s]Extractor Predicting: 250it [02:38,  1.59it/s]Extractor Predicting: 251it [02:39,  1.62it/s]Extractor Predicting: 252it [02:40,  1.58it/s]Extractor Predicting: 253it [02:40,  1.58it/s]Extractor Predicting: 254it [02:41,  1.58it/s]Extractor Predicting: 255it [02:41,  1.58it/s]Extractor Predicting: 256it [02:42,  1.54it/s]Extractor Predicting: 257it [02:43,  1.52it/s]Extractor Predicting: 258it [02:43,  1.53it/s]Extractor Predicting: 259it [02:44,  1.59it/s]Extractor Predicting: 260it [02:45,  1.56it/s]Extractor Predicting: 261it [02:45,  1.59it/s]Extractor Predicting: 262it [02:46,  1.57it/s]Extractor Predicting: 263it [02:47,  1.57it/s]Extractor Predicting: 264it [02:47,  1.63it/s]Extractor Predicting: 265it [02:48,  1.63it/s]Extractor Predicting: 266it [02:48,  1.64it/s]Extractor Predicting: 267it [02:49,  1.62it/s]Extractor Predicting: 268it [02:50,  1.61it/s]Extractor Predicting: 269it [02:50,  1.60it/s]Extractor Predicting: 270it [02:51,  1.56it/s]Extractor Predicting: 271it [02:52,  1.55it/s]Extractor Predicting: 272it [02:52,  1.56it/s]Extractor Predicting: 273it [02:53,  1.55it/s]Extractor Predicting: 274it [02:54,  1.53it/s]Extractor Predicting: 275it [02:54,  1.48it/s]Extractor Predicting: 276it [02:55,  1.51it/s]Extractor Predicting: 277it [02:56,  1.52it/s]Extractor Predicting: 278it [02:56,  1.56it/s]Extractor Predicting: 279it [02:57,  1.55it/s]Extractor Predicting: 280it [02:58,  1.52it/s]Extractor Predicting: 281it [02:58,  1.55it/s]Extractor Predicting: 282it [02:59,  1.55it/s]Extractor Predicting: 283it [02:59,  1.54it/s]Extractor Predicting: 284it [03:00,  1.55it/s]Extractor Predicting: 285it [03:01,  1.52it/s]Extractor Predicting: 286it [03:01,  1.57it/s]Extractor Predicting: 287it [03:02,  1.54it/s]Extractor Predicting: 288it [03:02,  1.76it/s]Extractor Predicting: 288it [03:02,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:28,322 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:28,327 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:28,327 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:28,327 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:28,327 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:14:28,653 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:14:28,655 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:14:29,052 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:14:30,187 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:14:30,187 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:32,665 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:32,715 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:32,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:32,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:14:32,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:14:33,406 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:14:33,407 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:14:33,696 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:14:33,905 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:14:33,905 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35003190810465856,
  "recall": 0.1589855072463768,
  "score": 0.21865656766992225,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:01,  2.28it/s]Extractor Predicting: 3it [00:01,  1.99it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:14:36,075 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:14:36,076 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:14:36,084 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:14:36,085 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:14:36,090 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:14:41,351 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:14:41,351 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:14:41,364 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:14:41,365 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:14:41,372 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:14:41,378 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:14:41,378 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:14:41,378 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:14:41,378 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:14:41,378 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:14:41,378 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.07,
  "score": 0.12280701754385964,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:14:41,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:42,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:42,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:43,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:44,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:44,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:45,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:46,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:46,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:47,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:48,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:48,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:49,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:50,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:50,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:51,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:52,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:52,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:53,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:54,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:54,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:12, 13.76s/it][WARNING|generation_utils.py:914] 2023-08-28 19:14:55,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:56,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:56,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:57,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:57,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:58,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:14:59,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:00,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:00,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:01,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:02,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:02,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:03,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:03,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:04,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:05,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:05,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:06,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:07,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:07,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:08,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:56, 13.59s/it][WARNING|generation_utils.py:914] 2023-08-28 19:15:08,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:09,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:10,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:10,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:11,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:11,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:12,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:12,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:13,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:14,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:14,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:15,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:16,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:16,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:17,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:18,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:18,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:19,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:20,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:20,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:21,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:21,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:41<02:44, 13.68s/it][WARNING|generation_utils.py:914] 2023-08-28 19:15:22,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:23,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:23,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:24,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:25,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:25,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:26,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:27,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:27,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:28,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:28,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:29,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:29,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:30,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:31,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:31,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:32,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:33,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:33,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:34,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:35,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:54<02:27, 13.44s/it][WARNING|generation_utils.py:914] 2023-08-28 19:15:35,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:36,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:36,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:37,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:38,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:38,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:39,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:40,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:40,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:41,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:42,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:43,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:43,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:44,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:44,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:45,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:46,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:46,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:47,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:48,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:07<02:14, 13.43s/it][WARNING|generation_utils.py:914] 2023-08-28 19:15:49,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:49,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:50,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:51,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:51,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:52,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:52,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:53,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:54,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:54,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:55,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:56,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:56,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:57,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:58,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:58,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:15:59,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:00,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:01,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:01,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:20<02:00, 13.36s/it][WARNING|generation_utils.py:914] 2023-08-28 19:16:02,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:02,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:03,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:04,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:04,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:05,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:06,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:06,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:07,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:08,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:09,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:09,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:10,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:11,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:12,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:12,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:13,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:14,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:15,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:16,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:16,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:35<01:51, 13.94s/it][WARNING|generation_utils.py:914] 2023-08-28 19:16:17,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:18,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:18,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:19,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:20,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:20,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:21,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:22,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:22,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:23,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:24,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:24,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:25,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:26,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:26,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:27,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:28,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:28,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:29,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:30,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:30,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:49<01:37, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-28 19:16:31,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:31,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:32,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:33,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:34,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:35,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:35,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:36,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:37,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:37,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:38,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:39,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:39,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:40,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:41,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:41,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:42,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:43,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:43,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:44,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:03<01:23, 13.86s/it][WARNING|generation_utils.py:914] 2023-08-28 19:16:45,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:45,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:46,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:47,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:47,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:48,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:49,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:49,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:50,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:50,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:51,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:52,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:52,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:53,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:54,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:54,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:55,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:55,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:56,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:56,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:16<01:07, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-28 19:16:57,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:58,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:59,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:16:59,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:00,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:01,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:01,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:02,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:03,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:03,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:04,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:04,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:05,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:06,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:06,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:07,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:08,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:08,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:09,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:10,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:10,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:29<00:54, 13.54s/it][WARNING|generation_utils.py:914] 2023-08-28 19:17:11,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:11,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:12,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:13,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:13,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:14,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:15,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:15,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:16,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:16,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:17,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:18,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:18,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:19,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:19,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:20,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:21,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:21,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:22,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:22,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:23,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:42<00:40, 13.34s/it][WARNING|generation_utils.py:914] 2023-08-28 19:17:24,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:24,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:25,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:26,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:26,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:27,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:27,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:28,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:28,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:29,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:30,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:30,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:31,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:31,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:33,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:33,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:34,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:34,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:35,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:36,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:55<00:26, 13.06s/it][WARNING|generation_utils.py:914] 2023-08-28 19:17:36,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:37,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:38,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:38,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:39,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:39,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:40,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:41,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:41,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:42,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:42,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:43,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:44,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:44,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:45,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:46,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:46,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:47,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:47,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:48,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:49,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:49,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:08<00:13, 13.26s/it][WARNING|generation_utils.py:914] 2023-08-28 19:17:50,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:50,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:51,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:52,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:52,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:53,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:53,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:54,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:55,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:56,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:56,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:57,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:57,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:58,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:59,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:17:59,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:18:00,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:18:01,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:18:01,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:18:02,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:21<00:00, 13.06s/it]Generating: 100%|██████████| 15/15 [03:21<00:00, 13.42s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:10,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:10,146 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:10,146 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:10,146 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:10,146 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:18:10,847 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:18:10,849 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:18:11,440 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:18:12,512 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:18:12,512 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:15,437 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:15,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:15,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:15,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:18:15,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:18:16,074 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:18:16,076 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:18:16,632 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:18:16,803 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:18:16,803 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9211309523809523, 'errors': {''}}
['Relation : member of political party . Context : On 31 March 2014 , the party announced that it would be given an independent leadership at the end of 2015 , led by Prime Minister Theresa May . Head Entity : Theresa May , Tail Entity : Conservative .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8664772727272727, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.95, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.953125, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8877840909090909, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.95625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 9344
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9444, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.41it/s]Extractor Estimating: 2it [00:01,  1.30it/s]Extractor Estimating: 3it [00:02,  1.36it/s]Extractor Estimating: 4it [00:02,  1.41it/s]Extractor Estimating: 5it [00:03,  1.48it/s]Extractor Estimating: 6it [00:04,  1.41it/s]Extractor Estimating: 7it [00:05,  1.40it/s]Extractor Estimating: 8it [00:05,  1.42it/s]Extractor Estimating: 9it [00:06,  1.43it/s]Extractor Estimating: 10it [00:07,  1.43it/s]Extractor Estimating: 11it [00:07,  1.42it/s]Extractor Estimating: 12it [00:08,  1.45it/s]Extractor Estimating: 13it [00:09,  1.43it/s]Extractor Estimating: 14it [00:09,  1.47it/s]Extractor Estimating: 15it [00:10,  1.49it/s]Extractor Estimating: 16it [00:11,  1.51it/s]Extractor Estimating: 17it [00:11,  1.54it/s]Extractor Estimating: 18it [00:12,  1.52it/s]Extractor Estimating: 19it [00:13,  1.48it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:14,  1.49it/s]Extractor Estimating: 22it [00:15,  1.55it/s]Extractor Estimating: 23it [00:15,  1.51it/s]Extractor Estimating: 24it [00:16,  1.50it/s]Extractor Estimating: 25it [00:17,  1.47it/s]Extractor Estimating: 26it [00:17,  1.51it/s]Extractor Estimating: 27it [00:18,  1.59it/s]Extractor Estimating: 28it [00:18,  1.60it/s]Extractor Estimating: 29it [00:19,  1.54it/s]Extractor Estimating: 30it [00:20,  1.57it/s]Extractor Estimating: 31it [00:20,  1.60it/s]Extractor Estimating: 32it [00:21,  1.46it/s]Extractor Estimating: 33it [00:22,  1.53it/s]Extractor Estimating: 34it [00:22,  1.54it/s]Extractor Estimating: 35it [00:23,  1.60it/s]Extractor Estimating: 36it [00:24,  1.47it/s]Extractor Estimating: 37it [00:24,  1.47it/s]Extractor Estimating: 38it [00:25,  1.56it/s]Extractor Estimating: 39it [00:26,  1.58it/s]Extractor Estimating: 40it [00:26,  1.61it/s]Extractor Estimating: 41it [00:27,  1.58it/s]Extractor Estimating: 42it [00:27,  1.57it/s]Extractor Estimating: 43it [00:28,  1.59it/s]Extractor Estimating: 44it [00:29,  1.59it/s]Extractor Estimating: 45it [00:29,  1.56it/s]Extractor Estimating: 46it [00:30,  1.60it/s]Extractor Estimating: 47it [00:31,  1.58it/s]Extractor Estimating: 48it [00:31,  1.64it/s]Extractor Estimating: 49it [00:32,  1.61it/s]Extractor Estimating: 50it [00:32,  1.62it/s]Extractor Estimating: 51it [00:33,  1.64it/s]Extractor Estimating: 52it [00:34,  1.57it/s]Extractor Estimating: 53it [00:34,  1.54it/s]Extractor Estimating: 54it [00:35,  1.56it/s]Extractor Estimating: 55it [00:36,  1.58it/s]Extractor Estimating: 56it [00:36,  1.61it/s]Extractor Estimating: 57it [00:37,  1.61it/s]Extractor Estimating: 58it [00:37,  1.63it/s]Extractor Estimating: 59it [00:38,  1.65it/s]Extractor Estimating: 60it [00:39,  1.66it/s]Extractor Estimating: 61it [00:39,  1.66it/s]Extractor Estimating: 62it [00:40,  1.62it/s]Extractor Estimating: 63it [00:41,  1.62it/s]Extractor Estimating: 64it [00:41,  1.60it/s]Extractor Estimating: 65it [00:42,  1.57it/s]Extractor Estimating: 66it [00:42,  1.56it/s]Extractor Estimating: 67it [00:43,  1.55it/s]Extractor Estimating: 68it [00:44,  1.59it/s]Extractor Estimating: 69it [00:44,  1.63it/s]Extractor Estimating: 70it [00:45,  1.62it/s]Extractor Estimating: 71it [00:46,  1.59it/s]Extractor Estimating: 72it [00:46,  1.62it/s]Extractor Estimating: 73it [00:47,  1.57it/s]Extractor Estimating: 74it [00:47,  1.58it/s]Extractor Estimating: 75it [00:48,  1.55it/s]Extractor Estimating: 76it [00:49,  1.63it/s]Extractor Estimating: 77it [00:49,  1.61it/s]Extractor Estimating: 78it [00:50,  1.62it/s]Extractor Estimating: 79it [00:51,  1.62it/s]Extractor Estimating: 80it [00:51,  1.62it/s]Extractor Estimating: 81it [00:52,  1.68it/s]Extractor Estimating: 82it [00:52,  1.60it/s]Extractor Estimating: 83it [00:53,  1.63it/s]Extractor Estimating: 84it [00:54,  1.68it/s]Extractor Estimating: 85it [00:54,  1.53it/s]Extractor Estimating: 86it [00:55,  1.52it/s]Extractor Estimating: 87it [00:56,  1.54it/s]Extractor Estimating: 88it [00:56,  1.65it/s]Extractor Estimating: 89it [00:57,  1.70it/s]Extractor Estimating: 90it [00:57,  1.73it/s]Extractor Estimating: 91it [00:58,  1.75it/s]Extractor Estimating: 92it [00:58,  1.73it/s]Extractor Estimating: 93it [00:59,  1.80it/s]Extractor Estimating: 94it [01:00,  1.68it/s]Extractor Estimating: 95it [01:00,  1.63it/s]Extractor Estimating: 96it [01:01,  1.61it/s]Extractor Estimating: 97it [01:02,  1.59it/s]Extractor Estimating: 98it [01:02,  1.67it/s]Extractor Estimating: 99it [01:03,  1.63it/s]Extractor Estimating: 100it [01:03,  1.63it/s]Extractor Estimating: 101it [01:04,  1.63it/s]Extractor Estimating: 102it [01:05,  1.60it/s]Extractor Estimating: 103it [01:05,  1.57it/s]Extractor Estimating: 104it [01:06,  1.50it/s]Extractor Estimating: 105it [01:07,  1.48it/s]Extractor Estimating: 106it [01:07,  1.49it/s]Extractor Estimating: 107it [01:08,  1.47it/s]Extractor Estimating: 108it [01:09,  1.49it/s]Extractor Estimating: 109it [01:09,  1.52it/s]Extractor Estimating: 110it [01:10,  1.55it/s]Extractor Estimating: 111it [01:11,  1.48it/s]Extractor Estimating: 112it [01:11,  1.49it/s]Extractor Estimating: 113it [01:12,  1.53it/s]Extractor Estimating: 114it [01:13,  1.59it/s]Extractor Estimating: 115it [01:13,  1.64it/s]Extractor Estimating: 116it [01:14,  1.64it/s]Extractor Estimating: 117it [01:14,  1.62it/s]Extractor Estimating: 118it [01:15,  1.61it/s]Extractor Estimating: 119it [01:16,  1.60it/s]Extractor Estimating: 120it [01:16,  1.59it/s]Extractor Estimating: 121it [01:17,  1.66it/s]Extractor Estimating: 122it [01:17,  1.67it/s]Extractor Estimating: 123it [01:18,  1.56it/s]Extractor Estimating: 124it [01:19,  1.62it/s]Extractor Estimating: 125it [01:19,  1.67it/s]Extractor Estimating: 126it [01:20,  1.56it/s]Extractor Estimating: 127it [01:21,  1.58it/s]Extractor Estimating: 128it [01:21,  1.57it/s]Extractor Estimating: 129it [01:22,  1.54it/s]Extractor Estimating: 130it [01:23,  1.57it/s]Extractor Estimating: 131it [01:23,  1.59it/s]Extractor Estimating: 132it [01:24,  1.63it/s]Extractor Estimating: 133it [01:24,  1.60it/s]Extractor Estimating: 134it [01:25,  1.59it/s]Extractor Estimating: 135it [01:26,  1.56it/s]Extractor Estimating: 136it [01:26,  1.57it/s]Extractor Estimating: 137it [01:27,  1.55it/s]Extractor Estimating: 138it [01:28,  1.60it/s]Extractor Estimating: 139it [01:28,  1.56it/s]Extractor Estimating: 140it [01:29,  1.59it/s]Extractor Estimating: 141it [01:30,  1.55it/s]Extractor Estimating: 142it [01:30,  1.59it/s]Extractor Estimating: 143it [01:31,  1.55it/s]Extractor Estimating: 144it [01:31,  1.58it/s]Extractor Estimating: 145it [01:32,  1.53it/s]Extractor Estimating: 146it [01:33,  1.52it/s]Extractor Estimating: 147it [01:33,  1.56it/s]Extractor Estimating: 148it [01:34,  1.61it/s]Extractor Estimating: 149it [01:35,  1.56it/s]Extractor Estimating: 150it [01:35,  1.56it/s]Extractor Estimating: 151it [01:36,  1.54it/s]Extractor Estimating: 152it [01:37,  1.39it/s]Extractor Estimating: 153it [01:37,  1.44it/s]Extractor Estimating: 154it [01:38,  1.45it/s]Extractor Estimating: 155it [01:39,  1.39it/s]Extractor Estimating: 156it [01:40,  1.44it/s]Extractor Estimating: 157it [01:40,  1.50it/s]Extractor Estimating: 158it [01:41,  1.50it/s]Extractor Estimating: 159it [01:42,  1.48it/s]Extractor Estimating: 160it [01:42,  1.49it/s]Extractor Estimating: 161it [01:43,  1.50it/s]Extractor Estimating: 162it [01:43,  1.52it/s]Extractor Estimating: 163it [01:44,  1.48it/s]Extractor Estimating: 164it [01:45,  1.52it/s]Extractor Estimating: 165it [01:46,  1.47it/s]Extractor Estimating: 166it [01:46,  1.44it/s]Extractor Estimating: 167it [01:47,  1.47it/s]Extractor Estimating: 168it [01:48,  1.42it/s]Extractor Estimating: 169it [01:48,  1.42it/s]Extractor Estimating: 170it [01:49,  1.46it/s]Extractor Estimating: 171it [01:50,  1.46it/s]Extractor Estimating: 172it [01:50,  1.47it/s]Extractor Estimating: 173it [01:51,  1.51it/s]Extractor Estimating: 174it [01:52,  1.50it/s]Extractor Estimating: 175it [01:52,  1.51it/s]Extractor Estimating: 176it [01:53,  1.51it/s]Extractor Estimating: 177it [01:54,  1.52it/s]Extractor Estimating: 178it [01:54,  1.50it/s]Extractor Estimating: 179it [01:55,  1.53it/s]Extractor Estimating: 180it [01:56,  1.52it/s]Extractor Estimating: 181it [01:56,  1.50it/s]Extractor Estimating: 182it [01:57,  1.52it/s]Extractor Estimating: 183it [01:58,  1.50it/s]Extractor Estimating: 184it [01:58,  1.51it/s]Extractor Estimating: 185it [01:59,  1.46it/s]Extractor Estimating: 186it [02:00,  1.52it/s]Extractor Estimating: 187it [02:00,  1.60it/s]Extractor Estimating: 188it [02:01,  1.57it/s]Extractor Estimating: 189it [02:01,  1.56it/s]Extractor Estimating: 190it [02:02,  1.61it/s]Extractor Estimating: 191it [02:03,  1.58it/s]Extractor Estimating: 192it [02:03,  1.55it/s]Extractor Estimating: 193it [02:04,  1.55it/s]Extractor Estimating: 194it [02:05,  1.57it/s]Extractor Estimating: 195it [02:05,  1.58it/s]Extractor Estimating: 196it [02:06,  1.59it/s]Extractor Estimating: 197it [02:07,  1.59it/s]Extractor Estimating: 198it [02:07,  1.57it/s]Extractor Estimating: 199it [02:08,  1.60it/s]Extractor Estimating: 200it [02:08,  1.62it/s]Extractor Estimating: 201it [02:09,  1.56it/s]Extractor Estimating: 202it [02:10,  1.54it/s]Extractor Estimating: 203it [02:10,  1.53it/s]Extractor Estimating: 204it [02:11,  1.54it/s]Extractor Estimating: 205it [02:12,  1.51it/s]Extractor Estimating: 206it [02:12,  1.51it/s]Extractor Estimating: 207it [02:13,  1.53it/s]Extractor Estimating: 208it [02:14,  1.52it/s]Extractor Estimating: 209it [02:14,  1.53it/s]Extractor Estimating: 210it [02:15,  1.53it/s]Extractor Estimating: 211it [02:16,  1.47it/s]Extractor Estimating: 212it [02:16,  1.52it/s]Extractor Estimating: 213it [02:17,  1.50it/s]Extractor Estimating: 214it [02:18,  1.50it/s]Extractor Estimating: 215it [02:18,  1.49it/s]Extractor Estimating: 216it [02:19,  1.49it/s]Extractor Estimating: 217it [02:20,  1.51it/s]Extractor Estimating: 218it [02:20,  1.53it/s]Extractor Estimating: 219it [02:21,  1.51it/s]Extractor Estimating: 220it [02:22,  1.50it/s]Extractor Estimating: 221it [02:22,  1.51it/s]Extractor Estimating: 222it [02:23,  1.57it/s]Extractor Estimating: 223it [02:24,  1.37it/s]Extractor Estimating: 224it [02:25,  1.39it/s]Extractor Estimating: 225it [02:25,  1.44it/s]Extractor Estimating: 226it [02:26,  1.51it/s]Extractor Estimating: 227it [02:26,  1.56it/s]Extractor Estimating: 228it [02:27,  1.62it/s]Extractor Estimating: 229it [02:27,  1.68it/s]Extractor Estimating: 230it [02:28,  1.65it/s]Extractor Estimating: 231it [02:29,  1.63it/s]Extractor Estimating: 232it [02:29,  1.69it/s]Extractor Estimating: 233it [02:30,  1.71it/s]Extractor Estimating: 234it [02:30,  1.68it/s]Extractor Estimating: 235it [02:31,  1.71it/s]Extractor Estimating: 236it [02:32,  1.72it/s]Extractor Estimating: 237it [02:32,  1.77it/s]Extractor Estimating: 238it [02:33,  1.80it/s]Extractor Estimating: 239it [02:33,  1.84it/s]Extractor Estimating: 240it [02:34,  1.75it/s]Extractor Estimating: 241it [02:34,  1.76it/s]Extractor Estimating: 242it [02:35,  1.81it/s]Extractor Estimating: 243it [02:35,  1.78it/s]Extractor Estimating: 244it [02:36,  1.78it/s]Extractor Estimating: 245it [02:37,  1.76it/s]Extractor Estimating: 246it [02:37,  1.78it/s]Extractor Estimating: 247it [02:38,  1.76it/s]Extractor Estimating: 248it [02:38,  1.78it/s]Extractor Estimating: 249it [02:39,  1.85it/s]Extractor Estimating: 250it [02:39,  1.80it/s]Extractor Estimating: 251it [02:40,  1.61it/s]Extractor Estimating: 252it [02:41,  1.57it/s]Extractor Estimating: 253it [02:41,  1.59it/s]Extractor Estimating: 254it [02:42,  1.55it/s]Extractor Estimating: 255it [02:43,  1.58it/s]Extractor Estimating: 256it [02:43,  1.62it/s]Extractor Estimating: 257it [02:44,  1.59it/s]Extractor Estimating: 258it [02:45,  1.58it/s]Extractor Estimating: 259it [02:45,  1.61it/s]Extractor Estimating: 260it [02:46,  1.63it/s]Extractor Estimating: 261it [02:46,  1.58it/s]Extractor Estimating: 262it [02:47,  1.59it/s]Extractor Estimating: 263it [02:48,  1.46it/s]Extractor Estimating: 264it [02:48,  1.51it/s]Extractor Estimating: 265it [02:49,  1.57it/s]Extractor Estimating: 266it [02:50,  1.61it/s]Extractor Estimating: 267it [02:50,  1.57it/s]Extractor Estimating: 268it [02:51,  1.58it/s]Extractor Estimating: 269it [02:52,  1.59it/s]Extractor Estimating: 270it [02:52,  1.61it/s]Extractor Estimating: 271it [02:53,  1.61it/s]Extractor Estimating: 272it [02:54,  1.53it/s]Extractor Estimating: 273it [02:54,  1.58it/s]Extractor Estimating: 274it [02:55,  1.59it/s]Extractor Estimating: 275it [02:55,  1.61it/s]Extractor Estimating: 276it [02:56,  1.60it/s]Extractor Estimating: 277it [02:57,  1.59it/s]Extractor Estimating: 278it [02:57,  1.65it/s]Extractor Estimating: 279it [02:58,  1.65it/s]Extractor Estimating: 280it [02:58,  1.67it/s]Extractor Estimating: 281it [02:59,  1.63it/s]Extractor Estimating: 282it [03:00,  1.67it/s]Extractor Estimating: 283it [03:00,  1.69it/s]Extractor Estimating: 284it [03:01,  1.72it/s]Extractor Estimating: 285it [03:01,  1.75it/s]Extractor Estimating: 286it [03:02,  1.71it/s]Extractor Estimating: 287it [03:02,  1.69it/s]Extractor Estimating: 288it [03:03,  1.75it/s]Extractor Estimating: 289it [03:04,  1.70it/s]Extractor Estimating: 290it [03:04,  1.68it/s]Extractor Estimating: 291it [03:05,  1.71it/s]Extractor Estimating: 292it [03:05,  1.72it/s]Extractor Estimating: 293it [03:06,  1.70it/s]Extractor Estimating: 294it [03:07,  1.70it/s]Extractor Estimating: 295it [03:07,  1.69it/s]Extractor Estimating: 296it [03:08,  1.63it/s]Extractor Estimating: 297it [03:08,  1.61it/s]Extractor Estimating: 298it [03:09,  1.66it/s]Extractor Estimating: 299it [03:10,  1.69it/s]Extractor Estimating: 300it [03:10,  1.69it/s]Extractor Estimating: 301it [03:11,  1.61it/s]Extractor Estimating: 302it [03:11,  1.61it/s]Extractor Estimating: 303it [03:12,  1.62it/s]Extractor Estimating: 304it [03:13,  1.64it/s]Extractor Estimating: 305it [03:13,  1.66it/s]Extractor Estimating: 306it [03:14,  1.70it/s]Extractor Estimating: 307it [03:14,  1.69it/s]Extractor Estimating: 308it [03:15,  1.66it/s]Extractor Estimating: 309it [03:16,  1.67it/s]Extractor Estimating: 310it [03:16,  1.64it/s]Extractor Estimating: 311it [03:17,  1.50it/s]Extractor Estimating: 312it [03:18,  1.55it/s]Extractor Estimating: 313it [03:18,  1.55it/s]Extractor Estimating: 314it [03:19,  1.57it/s]Extractor Estimating: 315it [03:20,  1.60it/s]Extractor Estimating: 316it [03:20,  1.56it/s]Extractor Estimating: 317it [03:21,  1.60it/s]Extractor Estimating: 318it [03:21,  1.59it/s]Extractor Estimating: 319it [03:22,  1.59it/s]Extractor Estimating: 320it [03:23,  1.64it/s]Extractor Estimating: 321it [03:23,  1.65it/s]Extractor Estimating: 322it [03:24,  1.65it/s]Extractor Estimating: 323it [03:24,  1.62it/s]Extractor Estimating: 324it [03:25,  1.55it/s]Extractor Estimating: 325it [03:26,  1.54it/s]Extractor Estimating: 326it [03:27,  1.48it/s]Extractor Estimating: 327it [03:27,  1.55it/s]Extractor Estimating: 328it [03:28,  1.56it/s]Extractor Estimating: 329it [03:28,  1.58it/s]Extractor Estimating: 330it [03:29,  1.60it/s]Extractor Estimating: 331it [03:30,  1.52it/s]Extractor Estimating: 332it [03:30,  1.56it/s]Extractor Estimating: 333it [03:31,  1.61it/s]Extractor Estimating: 334it [03:32,  1.61it/s]Extractor Estimating: 335it [03:32,  1.59it/s]Extractor Estimating: 336it [03:33,  1.61it/s]Extractor Estimating: 337it [03:33,  1.61it/s]Extractor Estimating: 338it [03:34,  1.56it/s]Extractor Estimating: 339it [03:35,  1.55it/s]Extractor Estimating: 340it [03:35,  1.55it/s]Extractor Estimating: 341it [03:36,  1.55it/s]Extractor Estimating: 342it [03:37,  1.59it/s]Extractor Estimating: 343it [03:37,  1.60it/s]Extractor Estimating: 344it [03:38,  1.55it/s]Extractor Estimating: 345it [03:39,  1.54it/s]Extractor Estimating: 346it [03:39,  1.62it/s]Extractor Estimating: 347it [03:40,  1.66it/s]Extractor Estimating: 348it [03:40,  1.65it/s]Extractor Estimating: 349it [03:41,  1.64it/s]Extractor Estimating: 350it [03:42,  1.60it/s]Extractor Estimating: 351it [03:42,  1.58it/s]Extractor Estimating: 352it [03:43,  1.56it/s]Extractor Estimating: 353it [03:44,  1.59it/s]Extractor Estimating: 354it [03:44,  1.59it/s]Extractor Estimating: 355it [03:45,  1.58it/s]Extractor Estimating: 356it [03:45,  1.59it/s]Extractor Estimating: 357it [03:46,  1.56it/s]Extractor Estimating: 358it [03:47,  1.58it/s]Extractor Estimating: 359it [03:47,  1.53it/s]Extractor Estimating: 360it [03:48,  1.49it/s]Extractor Estimating: 361it [03:49,  1.50it/s]Extractor Estimating: 362it [03:49,  1.51it/s]Extractor Estimating: 363it [03:50,  1.48it/s]Extractor Estimating: 364it [03:51,  1.50it/s]Extractor Estimating: 365it [03:51,  1.50it/s]Extractor Estimating: 366it [03:52,  1.48it/s]Extractor Estimating: 367it [03:53,  1.48it/s]Extractor Estimating: 368it [03:53,  1.50it/s]Extractor Estimating: 369it [03:54,  1.53it/s]Extractor Estimating: 370it [03:55,  1.43it/s]Extractor Estimating: 371it [03:56,  1.46it/s]Extractor Estimating: 372it [03:56,  1.45it/s]Extractor Estimating: 373it [03:57,  1.46it/s]Extractor Estimating: 374it [03:58,  1.51it/s]Extractor Estimating: 375it [03:58,  1.83it/s]Extractor Estimating: 375it [03:58,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:28,683 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:28,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:28,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:28,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:28,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:22:29,314 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:22:29,315 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:22:29,916 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:22:30,982 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:22:30,982 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:33,863 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:33,872 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:33,872 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:33,872 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:22:33,872 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:22:34,565 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:22:34,566 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:22:35,161 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:22:35,331 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:22:35,331 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 20:42:07,224 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 20:42:07,287 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 4499 mean pseudo reward: 0.9602907364212594
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 15877
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15977, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15977, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.044, loss:499.9958
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 12, avg_time 1.047, loss:467.6216
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 112, avg_time 1.059, loss:459.1693
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 24, avg_time 1.038, loss:444.8122
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 124, avg_time 1.055, loss:429.4927
>> valid entity prec:0.5528, rec:0.5324, f1:0.5424
>> valid relation prec:0.3191, rec:0.1538, f1:0.2076
>> valid relation with NER prec:0.3191, rec:0.1538, f1:0.2076
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 36, avg_time 2.236, loss:456.9936
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 136, avg_time 1.052, loss:405.8150
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 48, avg_time 1.047, loss:428.2302
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 148, avg_time 1.044, loss:428.8557
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 60, avg_time 1.060, loss:431.2522
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5701, rec:0.5523, f1:0.5611
>> valid relation prec:0.3930, rec:0.1767, f1:0.2438
>> valid relation with NER prec:0.3930, rec:0.1767, f1:0.2438
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 160, avg_time 2.251, loss:410.7321
g_step 1200, step 72, avg_time 1.052, loss:393.7683
g_step 1300, step 172, avg_time 1.041, loss:394.5749
g_step 1400, step 84, avg_time 1.039, loss:347.8922
g_step 1500, step 184, avg_time 1.049, loss:367.6065
>> valid entity prec:0.5760, rec:0.4659, f1:0.5151
>> valid relation prec:0.3521, rec:0.1504, f1:0.2108
>> valid relation with NER prec:0.3521, rec:0.1504, f1:0.2108
g_step 1600, step 96, avg_time 2.237, loss:334.1830
g_step 1700, step 8, avg_time 1.045, loss:339.7594
g_step 1800, step 108, avg_time 1.039, loss:310.1715
g_step 1900, step 20, avg_time 1.051, loss:322.4795
g_step 2000, step 120, avg_time 1.043, loss:306.3828
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5447, rec:0.5704, f1:0.5572
>> valid relation prec:0.3296, rec:0.1621, f1:0.2174
>> valid relation with NER prec:0.3296, rec:0.1621, f1:0.2174
g_step 2100, step 32, avg_time 2.297, loss:284.5212
g_step 2200, step 132, avg_time 1.033, loss:277.9020
g_step 2300, step 44, avg_time 1.042, loss:267.6566
g_step 2400, step 144, avg_time 1.037, loss:269.3205
g_step 2500, step 56, avg_time 1.038, loss:252.9661
>> valid entity prec:0.5428, rec:0.5871, f1:0.5641
>> valid relation prec:0.2814, rec:0.1919, f1:0.2282
>> valid relation with NER prec:0.2814, rec:0.1919, f1:0.2282
new max entity f1 on valid!
g_step 2600, step 156, avg_time 2.248, loss:258.0660
g_step 2700, step 68, avg_time 1.049, loss:236.7712
g_step 2800, step 168, avg_time 1.032, loss:244.0152
g_step 2900, step 80, avg_time 1.038, loss:240.8114
g_step 3000, step 180, avg_time 1.049, loss:228.4851
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5680, rec:0.5261, f1:0.5462
>> valid relation prec:0.2971, rec:0.1693, f1:0.2157
>> valid relation with NER prec:0.2971, rec:0.1693, f1:0.2157
g_step 3100, step 92, avg_time 2.230, loss:207.3004
g_step 3200, step 4, avg_time 1.050, loss:214.0970
g_step 3300, step 104, avg_time 1.035, loss:203.1503
g_step 3400, step 16, avg_time 1.033, loss:195.0504
g_step 3500, step 116, avg_time 1.043, loss:179.8033
>> valid entity prec:0.5553, rec:0.5157, f1:0.5348
>> valid relation prec:0.2993, rec:0.1469, f1:0.1971
>> valid relation with NER prec:0.2993, rec:0.1469, f1:0.1971
g_step 3600, step 28, avg_time 2.246, loss:185.8822
g_step 3700, step 128, avg_time 1.038, loss:176.5504
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:42:07 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:42:07 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-42-07_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:42:08 - WARNING - datasets.builder -   Using custom data configuration default-095f5d36a02ba9fd
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-095f5d36a02ba9fd/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:42:08,837 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:42:08,838 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:42:08,838 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:42:08,839 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:42:08,848 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:42:08,853 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:42:08,853 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:42:08,853 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:42:08,853 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:42:08,853 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:42:08,854 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:42:09,160 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:42:12,308 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:42:12,319 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-095f5d36a02ba9fd/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.16ba/s] 40%|████      | 2/5 [00:00<00:00,  3.96ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.33ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.76ba/s]100%|██████████| 5/5 [00:01<00:00,  4.80ba/s]100%|██████████| 5/5 [00:01<00:00,  4.32ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.14ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.38ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42ba/s]100%|██████████| 4/4 [00:00<00:00,  5.52ba/s]100%|██████████| 4/4 [00:00<00:00,  5.02ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.39ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.61ba/s]100%|██████████| 5/5 [00:00<00:00, 12.33ba/s]100%|██████████| 5/5 [00:00<00:00, 11.79ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.90ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.34ba/s]100%|██████████| 4/4 [00:00<00:00, 11.73ba/s]
[INFO|trainer.py:414] 2023-08-28 20:42:16,247 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:42:16,408 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:42:16,408 >>   Num examples = 4500
[INFO|trainer.py:1149] 2023-08-28 20:42:16,409 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:42:16,409 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:42:16,409 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:42:16,409 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:42:16,409 >>   Total optimization steps = 350
  0%|          | 0/350 [00:00<?, ?it/s]  0%|          | 1/350 [00:00<01:46,  3.29it/s]  1%|          | 2/350 [00:00<01:42,  3.38it/s]  1%|          | 3/350 [00:00<01:41,  3.41it/s]  1%|          | 4/350 [00:01<01:40,  3.43it/s]  1%|▏         | 5/350 [00:01<01:41,  3.41it/s]  2%|▏         | 6/350 [00:01<01:40,  3.41it/s]  2%|▏         | 7/350 [00:02<01:40,  3.41it/s]  2%|▏         | 8/350 [00:02<01:40,  3.40it/s]  3%|▎         | 9/350 [00:02<01:40,  3.38it/s]  3%|▎         | 10/350 [00:02<01:40,  3.39it/s]  3%|▎         | 11/350 [00:03<01:40,  3.39it/s]  3%|▎         | 12/350 [00:03<01:39,  3.39it/s]  4%|▎         | 13/350 [00:03<01:39,  3.39it/s]  4%|▍         | 14/350 [00:04<01:39,  3.39it/s]  4%|▍         | 15/350 [00:04<01:38,  3.39it/s]  5%|▍         | 16/350 [00:04<01:38,  3.39it/s]  5%|▍         | 17/350 [00:05<01:38,  3.40it/s]  5%|▌         | 18/350 [00:05<01:37,  3.39it/s]  5%|▌         | 19/350 [00:05<01:37,  3.39it/s]  6%|▌         | 20/350 [00:05<01:38,  3.34it/s]  6%|▌         | 21/350 [00:06<01:38,  3.35it/s]  6%|▋         | 22/350 [00:06<01:37,  3.37it/s]  7%|▋         | 23/350 [00:06<01:36,  3.37it/s]  7%|▋         | 24/350 [00:07<01:36,  3.38it/s]  7%|▋         | 25/350 [00:07<01:35,  3.39it/s]  7%|▋         | 26/350 [00:07<01:35,  3.39it/s]  8%|▊         | 27/350 [00:07<01:35,  3.40it/s]  8%|▊         | 28/350 [00:08<01:34,  3.41it/s]  8%|▊         | 29/350 [00:08<01:33,  3.42it/s]  9%|▊         | 30/350 [00:08<01:33,  3.42it/s]  9%|▉         | 31/350 [00:09<01:33,  3.40it/s]  9%|▉         | 32/350 [00:09<01:33,  3.42it/s]  9%|▉         | 33/350 [00:09<01:32,  3.42it/s] 10%|▉         | 34/350 [00:10<01:32,  3.43it/s] 10%|█         | 35/350 [00:10<01:31,  3.43it/s] 10%|█         | 36/350 [00:10<01:31,  3.43it/s] 11%|█         | 37/350 [00:10<01:31,  3.43it/s] 11%|█         | 38/350 [00:11<01:30,  3.44it/s] 11%|█         | 39/350 [00:11<01:30,  3.44it/s] 11%|█▏        | 40/350 [00:11<01:30,  3.44it/s] 12%|█▏        | 41/350 [00:12<01:29,  3.44it/s] 12%|█▏        | 42/350 [00:12<01:29,  3.43it/s] 12%|█▏        | 43/350 [00:12<01:29,  3.44it/s] 13%|█▎        | 44/350 [00:12<01:29,  3.44it/s] 13%|█▎        | 45/350 [00:13<01:28,  3.44it/s] 13%|█▎        | 46/350 [00:13<01:28,  3.44it/s] 13%|█▎        | 47/350 [00:13<01:28,  3.44it/s] 14%|█▎        | 48/350 [00:14<01:27,  3.44it/s] 14%|█▍        | 49/350 [00:14<01:27,  3.44it/s] 14%|█▍        | 50/350 [00:14<01:27,  3.44it/s] 15%|█▍        | 51/350 [00:14<01:26,  3.44it/s] 15%|█▍        | 52/350 [00:15<01:26,  3.44it/s] 15%|█▌        | 53/350 [00:15<01:27,  3.41it/s] 15%|█▌        | 54/350 [00:15<01:26,  3.42it/s] 16%|█▌        | 55/350 [00:16<01:26,  3.43it/s] 16%|█▌        | 56/350 [00:16<01:25,  3.43it/s] 16%|█▋        | 57/350 [00:16<01:25,  3.43it/s] 17%|█▋        | 58/350 [00:16<01:25,  3.43it/s] 17%|█▋        | 59/350 [00:17<01:24,  3.44it/s] 17%|█▋        | 60/350 [00:17<01:24,  3.43it/s] 17%|█▋        | 61/350 [00:17<01:24,  3.43it/s] 18%|█▊        | 62/350 [00:18<01:23,  3.43it/s] 18%|█▊        | 63/350 [00:18<01:23,  3.43it/s] 18%|█▊        | 64/350 [00:18<01:25,  3.34it/s] 19%|█▊        | 65/350 [00:19<01:24,  3.37it/s] 19%|█▉        | 66/350 [00:19<01:23,  3.39it/s] 19%|█▉        | 67/350 [00:19<01:23,  3.40it/s] 19%|█▉        | 68/350 [00:19<01:22,  3.41it/s] 20%|█▉        | 69/350 [00:20<01:22,  3.42it/s] 20%|██        | 70/350 [00:20<01:21,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 20:42:36,972 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:42:36,972 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 20:42:36,972 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.44it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.49it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.93it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.20it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.70it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.54it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.16it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.96it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.02it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.26it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.23it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.12it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.93it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.90it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.93it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.91it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.77it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.99it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.08it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.20it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.08it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.03it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.07it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.89it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.77it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.97it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.99it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.15it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.20it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.06it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.07it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.00it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.91it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.73it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.97it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.03it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.17it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.10it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.06it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.99it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.95it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.80it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.79it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.82it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.07it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.04it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.09it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.09it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.06it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.88it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.98it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.83it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.90it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.05it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.05it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.09it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.09it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.98it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.87it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.81it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.89it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.97it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.85it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.99it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.01it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.10it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.92it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.87it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.85it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.92it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.01it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.00it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.97it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.06it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.97it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.90it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.91it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.88it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.90it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.94it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.01it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.02it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.07it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.99it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.92it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.88it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.95it/s][A                                                
                                                 [A 20%|██        | 70/350 [00:30<01:21,  3.42it/s]
100%|██████████| 437/437 [00:09<00:00, 43.95it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:42:46,963 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70
[INFO|configuration_utils.py:351] 2023-08-28 20:42:47,038 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:42:50,909 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:42:50,929 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:42:50,949 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70/special_tokens_map.json
 20%|██        | 71/350 [00:43<32:19,  6.95s/it] 21%|██        | 72/350 [00:43<22:58,  4.96s/it] 21%|██        | 73/350 [00:43<16:25,  3.56s/it] 21%|██        | 74/350 [00:43<11:51,  2.58s/it] 21%|██▏       | 75/350 [00:44<08:40,  1.89s/it] 22%|██▏       | 76/350 [00:44<06:27,  1.41s/it] 22%|██▏       | 77/350 [00:44<04:54,  1.08s/it] 22%|██▏       | 78/350 [00:45<03:49,  1.19it/s] 23%|██▎       | 79/350 [00:45<03:03,  1.47it/s] 23%|██▎       | 80/350 [00:45<02:32,  1.77it/s] 23%|██▎       | 81/350 [00:45<02:09,  2.07it/s] 23%|██▎       | 82/350 [00:46<01:54,  2.34it/s] 24%|██▎       | 83/350 [00:46<01:43,  2.58it/s] 24%|██▍       | 84/350 [00:46<01:35,  2.78it/s] 24%|██▍       | 85/350 [00:47<01:30,  2.94it/s] 25%|██▍       | 86/350 [00:47<01:26,  3.06it/s] 25%|██▍       | 87/350 [00:47<01:23,  3.15it/s] 25%|██▌       | 88/350 [00:48<01:21,  3.22it/s] 25%|██▌       | 89/350 [00:48<01:19,  3.27it/s] 26%|██▌       | 90/350 [00:48<01:18,  3.30it/s] 26%|██▌       | 91/350 [00:48<01:17,  3.33it/s] 26%|██▋       | 92/350 [00:49<01:17,  3.31it/s] 27%|██▋       | 93/350 [00:49<01:17,  3.34it/s] 27%|██▋       | 94/350 [00:49<01:17,  3.29it/s] 27%|██▋       | 95/350 [00:50<01:16,  3.32it/s] 27%|██▋       | 96/350 [00:50<01:16,  3.34it/s] 28%|██▊       | 97/350 [00:50<01:15,  3.36it/s] 28%|██▊       | 98/350 [00:51<01:22,  3.07it/s] 28%|██▊       | 99/350 [00:51<01:19,  3.16it/s] 29%|██▊       | 100/350 [00:51<01:17,  3.23it/s] 29%|██▉       | 101/350 [00:51<01:16,  3.28it/s] 29%|██▉       | 102/350 [00:52<01:14,  3.31it/s] 29%|██▉       | 103/350 [00:52<01:14,  3.33it/s] 30%|██▉       | 104/350 [00:52<01:13,  3.33it/s] 30%|███       | 105/350 [00:53<01:13,  3.35it/s] 30%|███       | 106/350 [00:53<01:12,  3.36it/s] 31%|███       | 107/350 [00:53<01:12,  3.37it/s] 31%|███       | 108/350 [00:54<01:11,  3.37it/s] 31%|███       | 109/350 [00:54<01:11,  3.38it/s] 31%|███▏      | 110/350 [00:54<01:10,  3.38it/s] 32%|███▏      | 111/350 [00:54<01:10,  3.38it/s] 32%|███▏      | 112/350 [00:55<01:10,  3.38it/s] 32%|███▏      | 113/350 [00:55<01:10,  3.38it/s] 33%|███▎      | 114/350 [00:55<01:09,  3.39it/s] 33%|███▎      | 115/350 [00:56<01:09,  3.37it/s] 33%|███▎      | 116/350 [00:56<01:09,  3.37it/s] 33%|███▎      | 117/350 [00:56<01:08,  3.38it/s] 34%|███▎      | 118/350 [00:57<01:08,  3.38it/s] 34%|███▍      | 119/350 [00:57<01:08,  3.38it/s] 34%|███▍      | 120/350 [00:57<01:07,  3.39it/s] 35%|███▍      | 121/350 [00:57<01:07,  3.38it/s] 35%|███▍      | 122/350 [00:58<01:07,  3.38it/s] 35%|███▌      | 123/350 [00:58<01:07,  3.39it/s] 35%|███▌      | 124/350 [00:58<01:06,  3.39it/s] 36%|███▌      | 125/350 [00:59<01:06,  3.38it/s] 36%|███▌      | 126/350 [00:59<01:06,  3.37it/s] 36%|███▋      | 127/350 [00:59<01:06,  3.37it/s] 37%|███▋      | 128/350 [00:59<01:05,  3.38it/s] 37%|███▋      | 129/350 [01:00<01:05,  3.38it/s] 37%|███▋      | 130/350 [01:00<01:05,  3.38it/s] 37%|███▋      | 131/350 [01:00<01:04,  3.38it/s] 38%|███▊      | 132/350 [01:01<01:04,  3.38it/s] 38%|███▊      | 133/350 [01:01<01:04,  3.38it/s] 38%|███▊      | 134/350 [01:01<01:03,  3.38it/s] 39%|███▊      | 135/350 [01:02<01:08,  3.15it/s] 39%|███▉      | 136/350 [01:02<01:06,  3.21it/s] 39%|███▉      | 137/350 [01:02<01:05,  3.27it/s] 39%|███▉      | 138/350 [01:03<01:04,  3.30it/s] 40%|███▉      | 139/350 [01:03<01:03,  3.32it/s] 40%|████      | 140/350 [01:03<01:02,  3.34it/s][INFO|trainer.py:2140] 2023-08-28 20:43:20,055 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:43:20,055 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 20:43:20,055 >>   Batch size = 8
{'eval_loss': 1.1066620349884033, 'eval_runtime': 9.9787, 'eval_samples_per_second': 350.046, 'eval_steps_per_second': 43.793, 'epoch': 0.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.90it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.41it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.88it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.26it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.74it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.55it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.44it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.25it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.41it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.48it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.34it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.23it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.13it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.96it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.06it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.09it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.07it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.24it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.38it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.33it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.23it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.14it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.15it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.14it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.91it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.15it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.21it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.26it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.31it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.22it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.11it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.09it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.11it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.04it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.16it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.21it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.09it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.23it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.22it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.13it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.11it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.10it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.08it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.12it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.17it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.21it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.29it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.10it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.20it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.14it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.11it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.07it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.23it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.18it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.31it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.28it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.21it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.12it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.16it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.12it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.06it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.09it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.08it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.17it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.21it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.22it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.10it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.13it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.05it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.02it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.12it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.17it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.24it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.26it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.15it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.23it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.05it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.16it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.01it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.09it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.17it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.14it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.26it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.25it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.16it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.17it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.10it/s][A                                                 
                                                 [A 40%|████      | 140/350 [01:13<01:02,  3.34it/s]
100%|██████████| 437/437 [00:09<00:00, 44.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:43:30,057 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140
[INFO|configuration_utils.py:351] 2023-08-28 20:43:30,123 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:43:34,396 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:43:34,411 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:43:34,420 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140/special_tokens_map.json
 40%|████      | 141/350 [01:27<25:57,  7.45s/it] 41%|████      | 142/350 [01:28<18:23,  5.31s/it] 41%|████      | 143/350 [01:28<13:07,  3.80s/it] 41%|████      | 144/350 [01:28<09:26,  2.75s/it] 41%|████▏     | 145/350 [01:28<06:52,  2.01s/it] 42%|████▏     | 146/350 [01:29<05:05,  1.50s/it] 42%|████▏     | 147/350 [01:29<03:50,  1.14s/it] 42%|████▏     | 148/350 [01:29<02:58,  1.13it/s] 43%|████▎     | 149/350 [01:30<02:22,  1.41it/s] 43%|████▎     | 150/350 [01:30<01:56,  1.71it/s] 43%|████▎     | 151/350 [01:30<01:38,  2.01it/s] 43%|████▎     | 152/350 [01:30<01:26,  2.29it/s] 44%|████▎     | 153/350 [01:31<01:17,  2.53it/s] 44%|████▍     | 154/350 [01:31<01:11,  2.74it/s] 44%|████▍     | 155/350 [01:31<01:07,  2.91it/s] 45%|████▍     | 156/350 [01:32<01:03,  3.04it/s] 45%|████▍     | 157/350 [01:32<01:01,  3.14it/s] 45%|████▌     | 158/350 [01:32<00:59,  3.21it/s] 45%|████▌     | 159/350 [01:33<00:58,  3.26it/s] 46%|████▌     | 160/350 [01:33<00:57,  3.30it/s] 46%|████▌     | 161/350 [01:33<00:56,  3.32it/s] 46%|████▋     | 162/350 [01:33<00:56,  3.34it/s] 47%|████▋     | 163/350 [01:34<00:55,  3.36it/s] 47%|████▋     | 164/350 [01:34<00:55,  3.37it/s] 47%|████▋     | 165/350 [01:34<00:54,  3.37it/s] 47%|████▋     | 166/350 [01:35<00:54,  3.38it/s] 48%|████▊     | 167/350 [01:35<00:54,  3.39it/s] 48%|████▊     | 168/350 [01:35<00:54,  3.31it/s] 48%|████▊     | 169/350 [01:36<00:54,  3.35it/s] 49%|████▊     | 170/350 [01:36<00:53,  3.37it/s] 49%|████▉     | 171/350 [01:36<00:52,  3.39it/s] 49%|████▉     | 172/350 [01:36<00:52,  3.41it/s] 49%|████▉     | 173/350 [01:37<00:51,  3.41it/s] 50%|████▉     | 174/350 [01:37<00:51,  3.42it/s] 50%|█████     | 175/350 [01:37<00:51,  3.43it/s] 50%|█████     | 176/350 [01:38<00:50,  3.43it/s] 51%|█████     | 177/350 [01:38<00:50,  3.43it/s] 51%|█████     | 178/350 [01:38<00:50,  3.43it/s] 51%|█████     | 179/350 [01:38<00:50,  3.40it/s] 51%|█████▏    | 180/350 [01:39<00:49,  3.41it/s] 52%|█████▏    | 181/350 [01:39<00:49,  3.42it/s] 52%|█████▏    | 182/350 [01:39<00:49,  3.42it/s] 52%|█████▏    | 183/350 [01:40<00:48,  3.43it/s] 53%|█████▎    | 184/350 [01:40<00:48,  3.43it/s] 53%|█████▎    | 185/350 [01:40<00:48,  3.43it/s] 53%|█████▎    | 186/350 [01:40<00:47,  3.43it/s] 53%|█████▎    | 187/350 [01:41<00:47,  3.43it/s] 54%|█████▎    | 188/350 [01:41<00:47,  3.43it/s] 54%|█████▍    | 189/350 [01:41<00:46,  3.43it/s] 54%|█████▍    | 190/350 [01:42<00:46,  3.41it/s] 55%|█████▍    | 191/350 [01:42<00:46,  3.42it/s] 55%|█████▍    | 192/350 [01:42<00:46,  3.42it/s] 55%|█████▌    | 193/350 [01:43<00:45,  3.42it/s] 55%|█████▌    | 194/350 [01:43<00:45,  3.43it/s] 56%|█████▌    | 195/350 [01:43<00:45,  3.43it/s] 56%|█████▌    | 196/350 [01:43<00:44,  3.43it/s] 56%|█████▋    | 197/350 [01:44<00:44,  3.43it/s] 57%|█████▋    | 198/350 [01:44<00:44,  3.43it/s] 57%|█████▋    | 199/350 [01:44<00:43,  3.43it/s] 57%|█████▋    | 200/350 [01:45<00:43,  3.43it/s] 57%|█████▋    | 201/350 [01:45<00:44,  3.38it/s] 58%|█████▊    | 202/350 [01:45<00:43,  3.40it/s] 58%|█████▊    | 203/350 [01:45<00:43,  3.41it/s] 58%|█████▊    | 204/350 [01:46<00:42,  3.41it/s] 59%|█████▊    | 205/350 [01:46<00:42,  3.42it/s] 59%|█████▉    | 206/350 [01:46<00:42,  3.42it/s] 59%|█████▉    | 207/350 [01:47<00:41,  3.42it/s] 59%|█████▉    | 208/350 [01:47<00:41,  3.43it/s] 60%|█████▉    | 209/350 [01:47<00:41,  3.43it/s] 60%|██████    | 210/350 [01:47<00:40,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 20:44:04,446 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:44:04,446 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 20:44:04,446 >>   Batch size = 8
{'eval_loss': 1.1211495399475098, 'eval_runtime': 9.9378, 'eval_samples_per_second': 351.487, 'eval_steps_per_second': 43.974, 'epoch': 1.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 59.08it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.46it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.25it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.41it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.68it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.45it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.28it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.17it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.19it/s][A
 12%|█▏        | 52/437 [00:01<00:09, 41.60it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 42.65it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.02it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.43it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.53it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.66it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.73it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.88it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.80it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.65it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.81it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.96it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.99it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.98it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.97it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.02it/s][A
 30%|███       | 132/437 [00:03<00:07, 39.84it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 41.31it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 42.16it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 42.89it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.28it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.64it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.69it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.85it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.59it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.37it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.46it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.74it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.99it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.03it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.03it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.13it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.04it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.78it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.57it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.62it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.79it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.92it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.03it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.16it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.13it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.96it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.76it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.70it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.24it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.51it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.68it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.88it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.04it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.99it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.88it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.80it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.61it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.70it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.82it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.91it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.15it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.15it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.95it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.81it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.72it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.67it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.69it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.88it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.90it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.16it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.17it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.02it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.86it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.76it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.69it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.75it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.87it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.98it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.08it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.11it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.88it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.86it/s][A                                                 
                                                 [A 60%|██████    | 210/350 [01:58<00:40,  3.43it/s]
100%|██████████| 437/437 [00:09<00:00, 43.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:44:14,546 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210
[INFO|configuration_utils.py:351] 2023-08-28 20:44:14,571 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:44:19,696 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:44:19,706 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:44:19,719 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210/special_tokens_map.json
 60%|██████    | 211/350 [02:13<18:14,  7.88s/it] 61%|██████    | 212/350 [02:13<12:53,  5.60s/it] 61%|██████    | 213/350 [02:14<09:09,  4.01s/it] 61%|██████    | 214/350 [02:14<06:33,  2.90s/it] 61%|██████▏   | 215/350 [02:14<04:45,  2.12s/it] 62%|██████▏   | 216/350 [02:15<03:30,  1.57s/it] 62%|██████▏   | 217/350 [02:15<02:37,  1.19s/it] 62%|██████▏   | 218/350 [02:15<02:01,  1.09it/s] 63%|██████▎   | 219/350 [02:15<01:35,  1.37it/s] 63%|██████▎   | 220/350 [02:16<01:18,  1.66it/s] 63%|██████▎   | 221/350 [02:16<01:05,  1.96it/s] 63%|██████▎   | 222/350 [02:16<00:56,  2.25it/s] 64%|██████▎   | 223/350 [02:17<00:53,  2.35it/s] 64%|██████▍   | 224/350 [02:17<00:48,  2.59it/s] 64%|██████▍   | 225/350 [02:17<00:44,  2.79it/s] 65%|██████▍   | 226/350 [02:18<00:42,  2.94it/s] 65%|██████▍   | 227/350 [02:18<00:40,  3.06it/s] 65%|██████▌   | 228/350 [02:18<00:38,  3.16it/s] 65%|██████▌   | 229/350 [02:18<00:37,  3.23it/s] 66%|██████▌   | 230/350 [02:19<00:36,  3.27it/s] 66%|██████▌   | 231/350 [02:19<00:35,  3.31it/s] 66%|██████▋   | 232/350 [02:19<00:35,  3.33it/s] 67%|██████▋   | 233/350 [02:20<00:35,  3.28it/s] 67%|██████▋   | 234/350 [02:20<00:35,  3.31it/s] 67%|██████▋   | 235/350 [02:20<00:34,  3.33it/s] 67%|██████▋   | 236/350 [02:21<00:34,  3.35it/s] 68%|██████▊   | 237/350 [02:21<00:33,  3.36it/s] 68%|██████▊   | 238/350 [02:21<00:33,  3.37it/s] 68%|██████▊   | 239/350 [02:21<00:32,  3.37it/s] 69%|██████▊   | 240/350 [02:22<00:32,  3.38it/s] 69%|██████▉   | 241/350 [02:22<00:32,  3.38it/s] 69%|██████▉   | 242/350 [02:22<00:31,  3.38it/s] 69%|██████▉   | 243/350 [02:23<00:31,  3.38it/s] 70%|██████▉   | 244/350 [02:23<00:32,  3.25it/s] 70%|███████   | 245/350 [02:23<00:31,  3.29it/s] 70%|███████   | 246/350 [02:24<00:31,  3.32it/s] 71%|███████   | 247/350 [02:24<00:30,  3.34it/s] 71%|███████   | 248/350 [02:24<00:30,  3.35it/s] 71%|███████   | 249/350 [02:24<00:30,  3.36it/s] 71%|███████▏  | 250/350 [02:25<00:29,  3.37it/s] 72%|███████▏  | 251/350 [02:25<00:29,  3.37it/s] 72%|███████▏  | 252/350 [02:25<00:29,  3.38it/s] 72%|███████▏  | 253/350 [02:26<00:28,  3.38it/s] 73%|███████▎  | 254/350 [02:26<00:28,  3.38it/s] 73%|███████▎  | 255/350 [02:26<00:28,  3.36it/s] 73%|███████▎  | 256/350 [02:27<00:27,  3.37it/s] 73%|███████▎  | 257/350 [02:27<00:27,  3.37it/s] 74%|███████▎  | 258/350 [02:27<00:27,  3.38it/s] 74%|███████▍  | 259/350 [02:27<00:26,  3.38it/s] 74%|███████▍  | 260/350 [02:28<00:26,  3.38it/s] 75%|███████▍  | 261/350 [02:28<00:26,  3.39it/s] 75%|███████▍  | 262/350 [02:28<00:26,  3.38it/s] 75%|███████▌  | 263/350 [02:29<00:25,  3.38it/s] 75%|███████▌  | 264/350 [02:29<00:25,  3.38it/s] 76%|███████▌  | 265/350 [02:29<00:25,  3.39it/s] 76%|███████▌  | 266/350 [02:29<00:25,  3.29it/s] 76%|███████▋  | 267/350 [02:30<00:25,  3.32it/s] 77%|███████▋  | 268/350 [02:30<00:24,  3.34it/s] 77%|███████▋  | 269/350 [02:30<00:24,  3.35it/s] 77%|███████▋  | 270/350 [02:31<00:23,  3.36it/s] 77%|███████▋  | 271/350 [02:31<00:23,  3.37it/s] 78%|███████▊  | 272/350 [02:31<00:23,  3.37it/s] 78%|███████▊  | 273/350 [02:32<00:22,  3.38it/s] 78%|███████▊  | 274/350 [02:32<00:22,  3.38it/s] 79%|███████▊  | 275/350 [02:32<00:22,  3.38it/s] 79%|███████▉  | 276/350 [02:32<00:21,  3.38it/s] 79%|███████▉  | 277/350 [02:33<00:22,  3.31it/s] 79%|███████▉  | 278/350 [02:33<00:21,  3.33it/s] 80%|███████▉  | 279/350 [02:33<00:21,  3.34it/s] 80%|████████  | 280/350 [02:34<00:20,  3.35it/s][INFO|trainer.py:2140] 2023-08-28 20:44:50,605 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:44:50,605 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 20:44:50,606 >>   Batch size = 8
{'eval_loss': 1.1365408897399902, 'eval_runtime': 10.0502, 'eval_samples_per_second': 347.554, 'eval_steps_per_second': 43.482, 'epoch': 2.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.82it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.29it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.83it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.13it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.36it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.31it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.19it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.96it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.22it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.04it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.17it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.23it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.11it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.88it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.88it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.91it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.96it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.98it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.15it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.11it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.01it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.89it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.82it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.84it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.96it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.91it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.00it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.09it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.06it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.97it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.86it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.87it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.87it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.97it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.94it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.02it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.14it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.97it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.93it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.90it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.85it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.95it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.00it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.88it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.96it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.01it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.99it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.98it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.83it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.00it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.92it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.96it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.93it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.00it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.02it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.03it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.90it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.93it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.96it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.90it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.03it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.86it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.97it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.05it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.96it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.81it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.90it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.85it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.00it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.90it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.91it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.91it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.99it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.90it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.86it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.94it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 41.60it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 42.53it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 42.92it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.26it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.48it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.69it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.68it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.67it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.48it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.74it/s][A                                                 
                                                 [A 80%|████████  | 280/350 [02:44<00:20,  3.35it/s]
100%|██████████| 437/437 [00:09<00:00, 43.74it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:45:00,686 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280
[INFO|configuration_utils.py:351] 2023-08-28 20:45:00,730 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:45:06,221 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:45:06,238 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:45:06,250 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280/special_tokens_map.json
 80%|████████  | 281/350 [02:59<09:06,  7.92s/it] 81%|████████  | 282/350 [03:00<06:23,  5.64s/it] 81%|████████  | 283/350 [03:00<04:30,  4.03s/it] 81%|████████  | 284/350 [03:00<03:12,  2.91s/it] 81%|████████▏ | 285/350 [03:01<02:18,  2.13s/it] 82%|████████▏ | 286/350 [03:01<01:40,  1.58s/it] 82%|████████▏ | 287/350 [03:01<01:15,  1.19s/it] 82%|████████▏ | 288/350 [03:01<00:57,  1.08it/s] 83%|████████▎ | 289/350 [03:02<00:44,  1.36it/s] 83%|████████▎ | 290/350 [03:02<00:36,  1.66it/s] 83%|████████▎ | 291/350 [03:02<00:30,  1.96it/s] 83%|████████▎ | 292/350 [03:03<00:25,  2.24it/s] 84%|████████▎ | 293/350 [03:03<00:23,  2.47it/s] 84%|████████▍ | 294/350 [03:03<00:20,  2.69it/s] 84%|████████▍ | 295/350 [03:04<00:19,  2.87it/s] 85%|████████▍ | 296/350 [03:04<00:17,  3.01it/s] 85%|████████▍ | 297/350 [03:04<00:17,  3.11it/s] 85%|████████▌ | 298/350 [03:04<00:16,  3.19it/s] 85%|████████▌ | 299/350 [03:05<00:15,  3.25it/s] 86%|████████▌ | 300/350 [03:05<00:15,  3.29it/s] 86%|████████▌ | 301/350 [03:05<00:14,  3.32it/s] 86%|████████▋ | 302/350 [03:06<00:14,  3.34it/s] 87%|████████▋ | 303/350 [03:06<00:14,  3.36it/s] 87%|████████▋ | 304/350 [03:06<00:13,  3.36it/s] 87%|████████▋ | 305/350 [03:06<00:13,  3.37it/s] 87%|████████▋ | 306/350 [03:07<00:13,  3.38it/s] 88%|████████▊ | 307/350 [03:07<00:12,  3.38it/s] 88%|████████▊ | 308/350 [03:07<00:12,  3.34it/s] 88%|████████▊ | 309/350 [03:08<00:12,  3.35it/s] 89%|████████▊ | 310/350 [03:08<00:11,  3.36it/s] 89%|████████▉ | 311/350 [03:08<00:11,  3.37it/s] 89%|████████▉ | 312/350 [03:09<00:11,  3.37it/s] 89%|████████▉ | 313/350 [03:09<00:10,  3.38it/s] 90%|████████▉ | 314/350 [03:09<00:10,  3.38it/s] 90%|█████████ | 315/350 [03:09<00:10,  3.38it/s] 90%|█████████ | 316/350 [03:10<00:10,  3.39it/s] 91%|█████████ | 317/350 [03:10<00:09,  3.39it/s] 91%|█████████ | 318/350 [03:10<00:09,  3.39it/s] 91%|█████████ | 319/350 [03:11<00:09,  3.36it/s] 91%|█████████▏| 320/350 [03:11<00:08,  3.37it/s] 92%|█████████▏| 321/350 [03:11<00:08,  3.37it/s] 92%|█████████▏| 322/350 [03:11<00:08,  3.38it/s] 92%|█████████▏| 323/350 [03:12<00:07,  3.38it/s] 93%|█████████▎| 324/350 [03:12<00:07,  3.38it/s] 93%|█████████▎| 325/350 [03:12<00:07,  3.38it/s] 93%|█████████▎| 326/350 [03:13<00:07,  3.39it/s] 93%|█████████▎| 327/350 [03:13<00:06,  3.39it/s] 94%|█████████▎| 328/350 [03:13<00:06,  3.39it/s] 94%|█████████▍| 329/350 [03:14<00:06,  3.39it/s] 94%|█████████▍| 330/350 [03:14<00:05,  3.37it/s] 95%|█████████▍| 331/350 [03:14<00:05,  3.37it/s] 95%|█████████▍| 332/350 [03:14<00:05,  3.38it/s] 95%|█████████▌| 333/350 [03:15<00:05,  3.38it/s] 95%|█████████▌| 334/350 [03:15<00:04,  3.38it/s] 96%|█████████▌| 335/350 [03:15<00:04,  3.38it/s] 96%|█████████▌| 336/350 [03:16<00:04,  3.38it/s] 96%|█████████▋| 337/350 [03:16<00:03,  3.39it/s] 97%|█████████▋| 338/350 [03:16<00:03,  3.39it/s] 97%|█████████▋| 339/350 [03:17<00:03,  3.38it/s] 97%|█████████▋| 340/350 [03:17<00:02,  3.38it/s] 97%|█████████▋| 341/350 [03:17<00:02,  3.14it/s] 98%|█████████▊| 342/350 [03:17<00:02,  3.21it/s] 98%|█████████▊| 343/350 [03:18<00:02,  3.26it/s] 98%|█████████▊| 344/350 [03:18<00:01,  3.29it/s] 99%|█████████▊| 345/350 [03:18<00:01,  3.33it/s] 99%|█████████▉| 346/350 [03:19<00:01,  3.34it/s] 99%|█████████▉| 347/350 [03:19<00:00,  3.35it/s] 99%|█████████▉| 348/350 [03:19<00:00,  3.36it/s]100%|█████████▉| 349/350 [03:20<00:00,  3.37it/s]100%|██████████| 350/350 [03:20<00:00,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 20:45:36,839 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:45:36,839 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 20:45:36,839 >>   Batch size = 8
{'eval_loss': 1.1467061042785645, 'eval_runtime': 10.005, 'eval_samples_per_second': 349.127, 'eval_steps_per_second': 43.678, 'epoch': 3.99}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.55it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.46it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.95it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.93it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.43it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.10it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.84it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.22it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.61it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.63it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.75it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.02it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.91it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.30it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.28it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.17it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.89it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.50it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.60it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.76it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.91it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.15it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.25it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.21it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.12it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.73it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.65it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.66it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.83it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.92it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.10it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.25it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.20it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.04it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.75it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.57it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.62it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.75it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.85it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.08it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.18it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.15it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.04it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.71it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.70it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.68it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.86it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.01it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.15it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.13it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.19it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.95it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.88it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 42.81it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.18it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.41it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.72it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.92it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.01it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.97it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.94it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.72it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.77it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.77it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.81it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.05it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.16it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.02it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.07it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.94it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.90it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.85it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.79it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.89it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.06it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.03it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.98it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.95it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.89it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.86it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.72it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.87it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.02it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.99it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.03it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.00it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.94it/s][A                                                 
                                                 [A100%|██████████| 350/350 [03:30<00:00,  3.38it/s]
100%|██████████| 437/437 [00:09<00:00, 43.94it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:45:46,877 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350
[INFO|configuration_utils.py:351] 2023-08-28 20:45:46,920 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:45:51,540 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:45:51,559 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:45:51,571 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 20:46:01,000 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 20:46:01,002 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70 (score: 1.1066620349884033).
                                                 100%|██████████| 350/350 [03:50<00:00,  3.38it/s]100%|██████████| 350/350 [03:50<00:00,  1.52it/s]
[INFO|trainer.py:1894] 2023-08-28 20:46:06,822 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 20:46:06,856 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:46:13,258 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:46:13,294 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:46:13,322 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:46:13,523 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:13,523 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:13,524 >>   train_loss               =     0.4381
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:13,524 >>   train_runtime            = 0:03:50.37
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:13,524 >>   train_samples            =       4500
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:13,524 >>   train_samples_per_second =     97.666
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:13,524 >>   train_steps_per_second   =      1.519
{'eval_loss': 1.152918815612793, 'eval_runtime': 9.9463, 'eval_samples_per_second': 351.185, 'eval_steps_per_second': 43.936, 'epoch': 4.99}
{'train_runtime': 230.3776, 'train_samples_per_second': 97.666, 'train_steps_per_second': 1.519, 'train_loss': 0.4381075177873884, 'epoch': 4.99}
08/28/2023 20:46:13 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 20:46:13,565 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:46:13,565 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 20:46:13,565 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.08it/s]  3%|▎         | 12/437 [00:00<00:08, 48.56it/s]  4%|▍         | 17/437 [00:00<00:08, 47.16it/s]  5%|▌         | 22/437 [00:00<00:08, 46.28it/s]  6%|▌         | 27/437 [00:00<00:08, 45.80it/s]  7%|▋         | 32/437 [00:00<00:08, 45.53it/s]  8%|▊         | 37/437 [00:00<00:08, 45.40it/s] 10%|▉         | 42/437 [00:00<00:08, 44.87it/s] 11%|█         | 47/437 [00:01<00:08, 44.27it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.09it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.21it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.29it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.51it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.59it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.68it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.66it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.33it/s] 21%|██        | 92/437 [00:02<00:07, 44.08it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.90it/s] 23%|██▎       | 102/437 [00:02<00:10, 31.60it/s] 24%|██▍       | 107/437 [00:02<00:09, 34.87it/s] 26%|██▌       | 112/437 [00:02<00:08, 37.43it/s] 27%|██▋       | 117/437 [00:02<00:08, 39.37it/s] 28%|██▊       | 122/437 [00:02<00:07, 40.89it/s] 29%|██▉       | 127/437 [00:02<00:07, 42.02it/s] 30%|███       | 132/437 [00:03<00:07, 42.86it/s] 31%|███▏      | 137/437 [00:03<00:06, 43.43it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.32it/s] 34%|███▎      | 147/437 [00:03<00:06, 43.29it/s] 35%|███▍      | 152/437 [00:03<00:06, 43.34it/s] 36%|███▌      | 157/437 [00:03<00:06, 43.67it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.10it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.33it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.56it/s] 41%|████      | 177/437 [00:04<00:05, 44.60it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.47it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.12it/s] 44%|████▍     | 192/437 [00:04<00:05, 43.76it/s] 45%|████▌     | 197/437 [00:04<00:05, 43.92it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.07it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.33it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.57it/s] 50%|████▉     | 217/437 [00:05<00:04, 44.60it/s] 51%|█████     | 222/437 [00:05<00:04, 44.48it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.29it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.00it/s] 54%|█████▍    | 237/437 [00:05<00:04, 43.82it/s] 55%|█████▌    | 242/437 [00:05<00:04, 43.84it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.05it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.32it/s] 59%|█████▉    | 257/437 [00:05<00:04, 42.29it/s] 60%|█████▉    | 262/437 [00:06<00:04, 43.00it/s] 61%|██████    | 267/437 [00:06<00:03, 43.54it/s] 62%|██████▏   | 272/437 [00:06<00:03, 43.76it/s] 63%|██████▎   | 277/437 [00:06<00:03, 43.75it/s] 65%|██████▍   | 282/437 [00:06<00:03, 43.72it/s] 66%|██████▌   | 287/437 [00:06<00:03, 43.79it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.01it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.01it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.11it/s] 70%|███████   | 307/437 [00:07<00:02, 44.34it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.53it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.39it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.18it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.10it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.19it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.18it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.13it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.08it/s] 81%|████████  | 352/437 [00:08<00:01, 44.32it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.42it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.36it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.13it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.16it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.13it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.20it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.11it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.18it/s] 91%|█████████ | 397/437 [00:09<00:00, 44.36it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.45it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.32it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.21it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.17it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.17it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.16it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.13it/s]100%|██████████| 437/437 [00:09<00:00, 44.24it/s]100%|██████████| 437/437 [00:10<00:00, 43.67it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:46:23,588 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:23,588 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:23,588 >>   eval_loss               =     1.1067
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:23,588 >>   eval_runtime            = 0:00:10.02
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:23,588 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:23,588 >>   eval_samples_per_second =    348.484
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:23,588 >>   eval_steps_per_second   =     43.598
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:46:23,588 >>   perplexity              =     3.0242
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:31,690 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:31,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:31,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:31,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:31,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:46:32,459 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:46:32,460 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:46:33,038 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:46:34,083 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:46:34,083 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:37,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:37,040 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:37,040 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:37,040 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:46:37,040 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:46:37,725 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:46:37,726 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:46:38,361 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:46:38,527 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:46:38,527 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-140
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-350
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-210
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-280
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/checkpoint-70
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.29it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:02,  1.38it/s]Extractor Predicting: 5it [00:03,  1.43it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:06,  1.53it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:08,  1.56it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:09,  1.59it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:11,  1.59it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.58it/s]Extractor Predicting: 23it [00:15,  1.59it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.60it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.62it/s]Extractor Predicting: 31it [00:20,  1.62it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:21,  1.59it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.48it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.58it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:28,  1.60it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:30,  1.62it/s]Extractor Predicting: 49it [00:31,  1.64it/s]Extractor Predicting: 50it [00:32,  1.60it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.61it/s]Extractor Predicting: 53it [00:34,  1.64it/s]Extractor Predicting: 54it [00:34,  1.64it/s]Extractor Predicting: 55it [00:35,  1.64it/s]Extractor Predicting: 56it [00:35,  1.60it/s]Extractor Predicting: 57it [00:36,  1.62it/s]Extractor Predicting: 58it [00:37,  1.60it/s]Extractor Predicting: 59it [00:37,  1.58it/s]Extractor Predicting: 60it [00:38,  1.55it/s]Extractor Predicting: 61it [00:39,  1.57it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:40,  1.57it/s]Extractor Predicting: 64it [00:40,  1.57it/s]Extractor Predicting: 65it [00:41,  1.56it/s]Extractor Predicting: 66it [00:42,  1.54it/s]Extractor Predicting: 67it [00:42,  1.53it/s]Extractor Predicting: 68it [00:43,  1.51it/s]Extractor Predicting: 69it [00:44,  1.53it/s]Extractor Predicting: 70it [00:44,  1.54it/s]Extractor Predicting: 71it [00:45,  1.56it/s]Extractor Predicting: 72it [00:46,  1.60it/s]Extractor Predicting: 73it [00:46,  1.58it/s]Extractor Predicting: 74it [00:47,  1.56it/s]Extractor Predicting: 75it [00:48,  1.59it/s]Extractor Predicting: 76it [00:48,  1.57it/s]Extractor Predicting: 77it [00:49,  1.56it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:50,  1.54it/s]Extractor Predicting: 80it [00:51,  1.52it/s]Extractor Predicting: 81it [00:52,  1.52it/s]Extractor Predicting: 82it [00:52,  1.53it/s]Extractor Predicting: 83it [00:53,  1.57it/s]Extractor Predicting: 84it [00:53,  1.58it/s]Extractor Predicting: 85it [00:54,  1.55it/s]Extractor Predicting: 86it [00:55,  1.50it/s]Extractor Predicting: 87it [00:55,  1.55it/s]Extractor Predicting: 88it [00:56,  1.55it/s]Extractor Predicting: 89it [00:57,  1.54it/s]Extractor Predicting: 90it [00:57,  1.46it/s]Extractor Predicting: 91it [00:58,  1.47it/s]Extractor Predicting: 92it [00:59,  1.50it/s]Extractor Predicting: 93it [00:59,  1.51it/s]Extractor Predicting: 94it [01:00,  1.54it/s]Extractor Predicting: 95it [01:01,  1.56it/s]Extractor Predicting: 96it [01:01,  1.58it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:03,  1.56it/s]Extractor Predicting: 99it [01:03,  1.55it/s]Extractor Predicting: 100it [01:04,  1.53it/s]Extractor Predicting: 101it [01:05,  1.54it/s]Extractor Predicting: 102it [01:05,  1.53it/s]Extractor Predicting: 103it [01:06,  1.53it/s]Extractor Predicting: 104it [01:06,  1.54it/s]Extractor Predicting: 105it [01:07,  1.55it/s]Extractor Predicting: 106it [01:08,  1.53it/s]Extractor Predicting: 107it [01:08,  1.54it/s]Extractor Predicting: 108it [01:09,  1.53it/s]Extractor Predicting: 109it [01:10,  1.54it/s]Extractor Predicting: 110it [01:10,  1.53it/s]Extractor Predicting: 111it [01:11,  1.53it/s]Extractor Predicting: 112it [01:12,  1.53it/s]Extractor Predicting: 113it [01:12,  1.51it/s]Extractor Predicting: 114it [01:13,  1.50it/s]Extractor Predicting: 115it [01:14,  1.36it/s]Extractor Predicting: 116it [01:15,  1.40it/s]Extractor Predicting: 117it [01:15,  1.42it/s]Extractor Predicting: 118it [01:16,  1.44it/s]Extractor Predicting: 119it [01:17,  1.45it/s]Extractor Predicting: 120it [01:17,  1.48it/s]Extractor Predicting: 121it [01:18,  1.46it/s]Extractor Predicting: 122it [01:19,  1.46it/s]Extractor Predicting: 123it [01:19,  1.52it/s]Extractor Predicting: 124it [01:20,  1.52it/s]Extractor Predicting: 125it [01:21,  1.49it/s]Extractor Predicting: 126it [01:21,  1.49it/s]Extractor Predicting: 127it [01:22,  1.49it/s]Extractor Predicting: 128it [01:23,  1.50it/s]Extractor Predicting: 129it [01:23,  1.49it/s]Extractor Predicting: 130it [01:24,  1.48it/s]Extractor Predicting: 131it [01:25,  1.45it/s]Extractor Predicting: 132it [01:25,  1.44it/s]Extractor Predicting: 133it [01:26,  1.44it/s]Extractor Predicting: 134it [01:27,  1.43it/s]Extractor Predicting: 135it [01:28,  1.45it/s]Extractor Predicting: 136it [01:28,  1.77it/s]Extractor Predicting: 136it [01:28,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:15,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:15,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:15,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:15,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:15,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:48:15,967 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:48:15,968 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:48:16,547 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:48:17,626 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:48:17,626 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:20,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:20,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:20,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:20,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:48:20,581 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:48:21,240 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:48:21,241 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:48:21,836 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:48:22,007 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:48:22,007 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4957983193277311,
  "recall": 0.18580017177211566,
  "score": 0.2703040399833403,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.78it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.56it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.65it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:17,  1.69it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:20,  1.73it/s]Extractor Predicting: 34it [00:20,  1.60it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:22,  1.58it/s]Extractor Predicting: 38it [00:23,  1.57it/s]Extractor Predicting: 39it [00:23,  1.61it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:25,  1.66it/s]Extractor Predicting: 43it [00:26,  1.69it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:27,  1.65it/s]Extractor Predicting: 46it [00:28,  1.66it/s]Extractor Predicting: 47it [00:28,  1.68it/s]Extractor Predicting: 48it [00:29,  1.68it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.69it/s]Extractor Predicting: 51it [00:30,  1.74it/s]Extractor Predicting: 52it [00:31,  1.73it/s]Extractor Predicting: 53it [00:32,  1.69it/s]Extractor Predicting: 54it [00:32,  1.66it/s]Extractor Predicting: 55it [00:33,  1.68it/s]Extractor Predicting: 56it [00:34,  1.64it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:35,  1.63it/s]Extractor Predicting: 60it [00:36,  1.63it/s]Extractor Predicting: 61it [00:37,  1.64it/s]Extractor Predicting: 62it [00:37,  1.64it/s]Extractor Predicting: 63it [00:38,  1.61it/s]Extractor Predicting: 64it [00:38,  1.68it/s]Extractor Predicting: 65it [00:39,  1.65it/s]Extractor Predicting: 66it [00:40,  1.63it/s]Extractor Predicting: 67it [00:40,  1.65it/s]Extractor Predicting: 68it [00:41,  1.70it/s]Extractor Predicting: 69it [00:41,  1.72it/s]Extractor Predicting: 70it [00:42,  1.70it/s]Extractor Predicting: 71it [00:43,  1.69it/s]Extractor Predicting: 72it [00:43,  1.66it/s]Extractor Predicting: 73it [00:44,  1.68it/s]Extractor Predicting: 74it [00:44,  1.64it/s]Extractor Predicting: 75it [00:45,  1.64it/s]Extractor Predicting: 76it [00:46,  1.65it/s]Extractor Predicting: 77it [00:46,  1.64it/s]Extractor Predicting: 78it [00:47,  1.65it/s]Extractor Predicting: 79it [00:47,  1.65it/s]Extractor Predicting: 80it [00:48,  1.67it/s]Extractor Predicting: 81it [00:49,  1.65it/s]Extractor Predicting: 82it [00:49,  1.67it/s]Extractor Predicting: 83it [00:50,  1.63it/s]Extractor Predicting: 84it [00:50,  1.63it/s]Extractor Predicting: 85it [00:51,  1.61it/s]Extractor Predicting: 86it [00:52,  1.56it/s]Extractor Predicting: 87it [00:52,  1.62it/s]Extractor Predicting: 88it [00:53,  1.59it/s]Extractor Predicting: 89it [00:54,  1.57it/s]Extractor Predicting: 90it [00:54,  1.55it/s]Extractor Predicting: 91it [00:55,  1.52it/s]Extractor Predicting: 92it [00:56,  1.52it/s]Extractor Predicting: 93it [00:56,  1.51it/s]Extractor Predicting: 94it [00:57,  1.52it/s]Extractor Predicting: 95it [00:58,  1.55it/s]Extractor Predicting: 96it [00:58,  1.54it/s]Extractor Predicting: 97it [00:59,  1.52it/s]Extractor Predicting: 98it [01:00,  1.53it/s]Extractor Predicting: 99it [01:00,  1.51it/s]Extractor Predicting: 100it [01:01,  1.51it/s]Extractor Predicting: 101it [01:02,  1.49it/s]Extractor Predicting: 102it [01:02,  1.48it/s]Extractor Predicting: 103it [01:03,  1.53it/s]Extractor Predicting: 104it [01:04,  1.53it/s]Extractor Predicting: 105it [01:04,  1.55it/s]Extractor Predicting: 106it [01:05,  1.50it/s]Extractor Predicting: 107it [01:06,  1.51it/s]Extractor Predicting: 108it [01:06,  1.49it/s]Extractor Predicting: 109it [01:07,  1.49it/s]Extractor Predicting: 110it [01:08,  1.49it/s]Extractor Predicting: 111it [01:08,  1.51it/s]Extractor Predicting: 112it [01:09,  1.50it/s]Extractor Predicting: 113it [01:10,  1.48it/s]Extractor Predicting: 114it [01:10,  1.49it/s]Extractor Predicting: 115it [01:11,  1.48it/s]Extractor Predicting: 116it [01:12,  1.49it/s]Extractor Predicting: 117it [01:12,  1.50it/s]Extractor Predicting: 118it [01:13,  1.51it/s]Extractor Predicting: 119it [01:14,  1.53it/s]Extractor Predicting: 120it [01:14,  1.58it/s]Extractor Predicting: 121it [01:15,  1.57it/s]Extractor Predicting: 122it [01:15,  1.56it/s]Extractor Predicting: 123it [01:16,  1.56it/s]Extractor Predicting: 124it [01:17,  1.55it/s]Extractor Predicting: 125it [01:17,  1.58it/s]Extractor Predicting: 126it [01:18,  1.45it/s]Extractor Predicting: 127it [01:19,  1.49it/s]Extractor Predicting: 128it [01:19,  1.54it/s]Extractor Predicting: 129it [01:20,  1.54it/s]Extractor Predicting: 130it [01:21,  1.52it/s]Extractor Predicting: 131it [01:21,  1.54it/s]Extractor Predicting: 132it [01:22,  1.55it/s]Extractor Predicting: 133it [01:23,  1.58it/s]Extractor Predicting: 134it [01:23,  1.61it/s]Extractor Predicting: 135it [01:24,  1.59it/s]Extractor Predicting: 136it [01:24,  1.61it/s]Extractor Predicting: 137it [01:25,  1.62it/s]Extractor Predicting: 138it [01:26,  1.59it/s]Extractor Predicting: 139it [01:26,  1.60it/s]Extractor Predicting: 140it [01:27,  1.60it/s]Extractor Predicting: 141it [01:28,  1.55it/s]Extractor Predicting: 142it [01:28,  1.54it/s]Extractor Predicting: 143it [01:29,  1.55it/s]Extractor Predicting: 144it [01:30,  1.55it/s]Extractor Predicting: 145it [01:30,  1.54it/s]Extractor Predicting: 146it [01:31,  1.58it/s]Extractor Predicting: 147it [01:31,  1.59it/s]Extractor Predicting: 148it [01:32,  1.57it/s]Extractor Predicting: 149it [01:33,  1.57it/s]Extractor Predicting: 150it [01:33,  1.56it/s]Extractor Predicting: 151it [01:34,  1.52it/s]Extractor Predicting: 152it [01:35,  1.54it/s]Extractor Predicting: 153it [01:35,  1.55it/s]Extractor Predicting: 154it [01:36,  1.58it/s]Extractor Predicting: 155it [01:37,  1.55it/s]Extractor Predicting: 156it [01:37,  1.54it/s]Extractor Predicting: 157it [01:38,  1.52it/s]Extractor Predicting: 158it [01:39,  1.54it/s]Extractor Predicting: 159it [01:39,  1.53it/s]Extractor Predicting: 160it [01:40,  1.52it/s]Extractor Predicting: 161it [01:41,  1.52it/s]Extractor Predicting: 162it [01:41,  1.52it/s]Extractor Predicting: 163it [01:42,  1.52it/s]Extractor Predicting: 164it [01:43,  1.53it/s]Extractor Predicting: 165it [01:43,  1.53it/s]Extractor Predicting: 166it [01:44,  1.53it/s]Extractor Predicting: 167it [01:45,  1.51it/s]Extractor Predicting: 168it [01:45,  1.51it/s]Extractor Predicting: 169it [01:46,  1.51it/s]Extractor Predicting: 170it [01:47,  1.51it/s]Extractor Predicting: 171it [01:47,  1.51it/s]Extractor Predicting: 172it [01:48,  1.51it/s]Extractor Predicting: 173it [01:49,  1.50it/s]Extractor Predicting: 174it [01:49,  1.50it/s]Extractor Predicting: 175it [01:50,  1.55it/s]Extractor Predicting: 176it [01:50,  1.53it/s]Extractor Predicting: 177it [01:51,  1.56it/s]Extractor Predicting: 178it [01:52,  1.57it/s]Extractor Predicting: 179it [01:52,  1.60it/s]Extractor Predicting: 180it [01:53,  1.58it/s]Extractor Predicting: 181it [01:54,  1.60it/s]Extractor Predicting: 182it [01:54,  1.62it/s]Extractor Predicting: 183it [01:55,  1.56it/s]Extractor Predicting: 184it [01:55,  1.63it/s]Extractor Predicting: 185it [01:56,  1.61it/s]Extractor Predicting: 186it [01:57,  1.62it/s]Extractor Predicting: 187it [01:57,  1.61it/s]Extractor Predicting: 188it [01:58,  1.57it/s]Extractor Predicting: 189it [01:59,  1.57it/s]Extractor Predicting: 190it [01:59,  1.57it/s]Extractor Predicting: 191it [02:00,  1.55it/s]Extractor Predicting: 192it [02:01,  1.55it/s]Extractor Predicting: 193it [02:01,  1.59it/s]Extractor Predicting: 194it [02:02,  1.59it/s]Extractor Predicting: 195it [02:02,  1.56it/s]Extractor Predicting: 196it [02:03,  1.52it/s]Extractor Predicting: 197it [02:04,  1.55it/s]Extractor Predicting: 198it [02:04,  1.54it/s]Extractor Predicting: 199it [02:05,  1.57it/s]Extractor Predicting: 200it [02:06,  1.56it/s]Extractor Predicting: 201it [02:06,  1.58it/s]Extractor Predicting: 202it [02:07,  1.59it/s]Extractor Predicting: 203it [02:08,  1.59it/s]Extractor Predicting: 204it [02:08,  1.45it/s]Extractor Predicting: 205it [02:09,  1.48it/s]Extractor Predicting: 206it [02:10,  1.53it/s]Extractor Predicting: 207it [02:10,  1.52it/s]Extractor Predicting: 208it [02:11,  1.53it/s]Extractor Predicting: 209it [02:12,  1.54it/s]Extractor Predicting: 210it [02:12,  1.55it/s]Extractor Predicting: 211it [02:13,  1.54it/s]Extractor Predicting: 212it [02:13,  1.56it/s]Extractor Predicting: 213it [02:14,  1.58it/s]Extractor Predicting: 214it [02:15,  1.56it/s]Extractor Predicting: 215it [02:15,  1.57it/s]Extractor Predicting: 216it [02:16,  1.57it/s]Extractor Predicting: 217it [02:17,  1.55it/s]Extractor Predicting: 218it [02:17,  1.57it/s]Extractor Predicting: 219it [02:18,  1.54it/s]Extractor Predicting: 220it [02:19,  1.56it/s]Extractor Predicting: 221it [02:19,  1.60it/s]Extractor Predicting: 222it [02:20,  1.59it/s]Extractor Predicting: 223it [02:20,  1.55it/s]Extractor Predicting: 224it [02:21,  1.60it/s]Extractor Predicting: 225it [02:22,  1.58it/s]Extractor Predicting: 226it [02:22,  1.57it/s]Extractor Predicting: 227it [02:23,  1.58it/s]Extractor Predicting: 228it [02:24,  1.60it/s]Extractor Predicting: 229it [02:24,  1.59it/s]Extractor Predicting: 230it [02:25,  1.60it/s]Extractor Predicting: 231it [02:25,  1.62it/s]Extractor Predicting: 232it [02:26,  1.60it/s]Extractor Predicting: 233it [02:27,  1.61it/s]Extractor Predicting: 234it [02:27,  1.58it/s]Extractor Predicting: 235it [02:28,  1.56it/s]Extractor Predicting: 236it [02:29,  1.56it/s]Extractor Predicting: 237it [02:29,  1.56it/s]Extractor Predicting: 238it [02:30,  1.56it/s]Extractor Predicting: 239it [02:31,  1.54it/s]Extractor Predicting: 240it [02:31,  1.55it/s]Extractor Predicting: 241it [02:32,  1.55it/s]Extractor Predicting: 242it [02:33,  1.55it/s]Extractor Predicting: 243it [02:33,  1.56it/s]Extractor Predicting: 244it [02:34,  1.54it/s]Extractor Predicting: 245it [02:34,  1.56it/s]Extractor Predicting: 246it [02:35,  1.56it/s]Extractor Predicting: 247it [02:36,  1.58it/s]Extractor Predicting: 248it [02:36,  1.57it/s]Extractor Predicting: 249it [02:37,  1.59it/s]Extractor Predicting: 250it [02:38,  1.59it/s]Extractor Predicting: 251it [02:38,  1.63it/s]Extractor Predicting: 252it [02:39,  1.59it/s]Extractor Predicting: 253it [02:39,  1.59it/s]Extractor Predicting: 254it [02:40,  1.59it/s]Extractor Predicting: 255it [02:41,  1.58it/s]Extractor Predicting: 256it [02:41,  1.54it/s]Extractor Predicting: 257it [02:42,  1.54it/s]Extractor Predicting: 258it [02:43,  1.54it/s]Extractor Predicting: 259it [02:43,  1.59it/s]Extractor Predicting: 260it [02:44,  1.58it/s]Extractor Predicting: 261it [02:45,  1.60it/s]Extractor Predicting: 262it [02:45,  1.57it/s]Extractor Predicting: 263it [02:46,  1.57it/s]Extractor Predicting: 264it [02:46,  1.63it/s]Extractor Predicting: 265it [02:47,  1.63it/s]Extractor Predicting: 266it [02:48,  1.64it/s]Extractor Predicting: 267it [02:48,  1.60it/s]Extractor Predicting: 268it [02:49,  1.59it/s]Extractor Predicting: 269it [02:50,  1.59it/s]Extractor Predicting: 270it [02:50,  1.56it/s]Extractor Predicting: 271it [02:51,  1.54it/s]Extractor Predicting: 272it [02:52,  1.56it/s]Extractor Predicting: 273it [02:52,  1.56it/s]Extractor Predicting: 274it [02:53,  1.54it/s]Extractor Predicting: 275it [02:54,  1.51it/s]Extractor Predicting: 276it [02:54,  1.53it/s]Extractor Predicting: 277it [02:55,  1.52it/s]Extractor Predicting: 278it [02:55,  1.57it/s]Extractor Predicting: 279it [02:56,  1.56it/s]Extractor Predicting: 280it [02:57,  1.56it/s]Extractor Predicting: 281it [02:57,  1.57it/s]Extractor Predicting: 282it [02:58,  1.57it/s]Extractor Predicting: 283it [02:59,  1.55it/s]Extractor Predicting: 284it [02:59,  1.56it/s]Extractor Predicting: 285it [03:00,  1.55it/s]Extractor Predicting: 286it [03:01,  1.60it/s]Extractor Predicting: 287it [03:01,  1.56it/s]Extractor Predicting: 288it [03:02,  1.77it/s]Extractor Predicting: 288it [03:02,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:31,876 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:31,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:31,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:31,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:31,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:51:32,595 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:51:32,603 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:51:32,869 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:51:33,928 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:51:33,928 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:35,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:35,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:35,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:35,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:51:35,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:51:35,728 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:51:35,730 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:51:35,995 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:51:36,154 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:51:36,154 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3869639794168096,
  "recall": 0.1634782608695652,
  "score": 0.22985226693835967,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:01,  2.34it/s]Extractor Predicting: 3it [00:01,  2.01it/s]
[INFO|configuration_utils.py:515] 2023-08-28 20:51:38,083 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:51:38,084 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:51:38,089 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:51:38,089 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 20:51:38,091 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:51:44,679 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 20:51:44,684 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 20:51:44,694 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:51:44,695 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:51:44,704 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:44,710 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:44,710 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:44,710 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:44,710 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:44,710 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:44,710 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5454545454545454,
  "recall": 0.06,
  "score": 0.10810810810810809,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 20:51:44,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:45,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:46,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:46,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:47,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:48,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:48,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:49,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:49,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:50,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:51,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:52,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:52,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:53,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:53,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:54,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:55,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:55,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:56,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:57,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<02:59, 12.82s/it][WARNING|generation_utils.py:914] 2023-08-28 20:51:57,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:58,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:59,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:59,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:00,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:00,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:01,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:02,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:02,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:03,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:03,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:04,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:05,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:05,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:06,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:06,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:07,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:08,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:08,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:09,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:09,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:10,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:50, 13.12s/it][WARNING|generation_utils.py:914] 2023-08-28 20:52:11,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:11,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:12,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:12,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:13,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:14,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:14,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:15,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:16,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:16,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:17,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:18,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:18,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:19,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:19,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:20,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:21,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:21,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:22,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:22,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:23,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:37, 13.12s/it][WARNING|generation_utils.py:914] 2023-08-28 20:52:24,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:24,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:25,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:26,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:26,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:27,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:28,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:29,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:29,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:30,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:31,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:32,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:32,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:33,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:33,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:34,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:35,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:35,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:36,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:37,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:38,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:53<02:30, 13.66s/it][WARNING|generation_utils.py:914] 2023-08-28 20:52:38,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:39,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:39,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:40,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:41,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:42,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:42,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:43,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:43,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:44,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:45,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:46,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:46,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:47,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:48,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:48,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:49,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:50,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:50,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:51,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:07<02:15, 13.53s/it][WARNING|generation_utils.py:914] 2023-08-28 20:52:52,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:52,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:53,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:53,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:54,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:54,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:55,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:56,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:56,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:57,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:57,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:58,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:59,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:59,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:00,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:01,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:01,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:02,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:03,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:04,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:19<01:59, 13.29s/it][WARNING|generation_utils.py:914] 2023-08-28 20:53:04,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:05,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:06,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:07,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:08,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:08,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:09,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:10,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:10,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:11,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:12,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:13,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:13,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:14,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:15,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:15,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:16,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:17,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:17,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:18,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:19,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:35<01:53, 14.19s/it][WARNING|generation_utils.py:914] 2023-08-28 20:53:20,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:21,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:22,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:22,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:23,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:24,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:24,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:25,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:26,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:26,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:27,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:27,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:28,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:29,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:29,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:30,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:31,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:31,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:32,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:33,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:33,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:49<01:38, 14.04s/it][WARNING|generation_utils.py:914] 2023-08-28 20:53:34,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:35,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:36,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:36,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:37,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:38,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:38,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:39,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:40,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:41,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:41,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:42,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:43,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:43,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:44,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:45,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:45,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:46,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:47,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:47,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:48,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:04<01:24, 14.17s/it][WARNING|generation_utils.py:914] 2023-08-28 20:53:49,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:49,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:50,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:51,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:51,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:52,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:52,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:53,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:54,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:54,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:55,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:55,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:56,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:56,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:57,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:58,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:58,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:59,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:59,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:00,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:16<01:07, 13.52s/it][WARNING|generation_utils.py:914] 2023-08-28 20:54:01,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:01,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:02,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:03,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:03,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:04,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:04,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:05,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:06,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:07,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:08,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:08,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:09,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:10,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:10,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:11,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:11,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:12,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:13,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:13,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:14,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:30<00:54, 13.69s/it][WARNING|generation_utils.py:914] 2023-08-28 20:54:15,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:15,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:16,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:17,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:17,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:18,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:19,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:19,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:20,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:20,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:21,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:22,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:22,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:23,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:23,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:24,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:25,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:25,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:26,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:27,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:42<00:39, 13.30s/it][WARNING|generation_utils.py:914] 2023-08-28 20:54:27,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:28,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:28,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:29,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:30,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:30,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:31,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:31,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:32,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:33,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:33,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:34,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:34,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:35,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:35,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:36,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:37,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:37,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:38,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:38,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:54<00:25, 12.80s/it][WARNING|generation_utils.py:914] 2023-08-28 20:54:39,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:39,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:40,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:41,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:41,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:42,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:42,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:43,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:44,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:44,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:45,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:45,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:46,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:47,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:47,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:48,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:48,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:49,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:50,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:50,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:51,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:06<00:12, 12.72s/it][WARNING|generation_utils.py:914] 2023-08-28 20:54:51,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:52,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:52,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:53,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:54,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:54,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:55,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:56,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:56,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:57,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:58,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:58,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:59,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:59,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:00,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:01,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:01,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:02,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:03,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:55:03,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:19<00:00, 12.67s/it]Generating: 100%|██████████| 15/15 [03:19<00:00, 13.29s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:11,442 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:11,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:11,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:11,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:11,453 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:55:12,167 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:55:12,167 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:55:12,787 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:55:13,879 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:55:13,879 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:16,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:16,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:16,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:16,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:16,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:55:17,412 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:55:17,413 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:55:17,966 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:55:18,166 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:55:18,166 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.96875, 'errors': {''}}
['Relation : instrument . Context : On 31 March 2014 , the band announced that they would be releasing their fourth studio album " On the Beach " . Head Entity : On the Beach , Tail Entity : guitar .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.90625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9609375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.959375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 630, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9578125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.9241071428571429, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.9515625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 8722
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8822, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.34it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.50it/s]Extractor Estimating: 4it [00:02,  1.55it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.53it/s]Extractor Estimating: 7it [00:04,  1.51it/s]Extractor Estimating: 8it [00:05,  1.45it/s]Extractor Estimating: 9it [00:06,  1.50it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.40it/s]Extractor Estimating: 12it [00:08,  1.43it/s]Extractor Estimating: 13it [00:08,  1.47it/s]Extractor Estimating: 14it [00:09,  1.39it/s]Extractor Estimating: 15it [00:10,  1.42it/s]Extractor Estimating: 16it [00:10,  1.41it/s]Extractor Estimating: 17it [00:11,  1.43it/s]Extractor Estimating: 18it [00:12,  1.41it/s]Extractor Estimating: 19it [00:13,  1.43it/s]Extractor Estimating: 20it [00:13,  1.43it/s]Extractor Estimating: 21it [00:14,  1.44it/s]Extractor Estimating: 22it [00:15,  1.46it/s]Extractor Estimating: 23it [00:15,  1.45it/s]Extractor Estimating: 24it [00:16,  1.53it/s]Extractor Estimating: 25it [00:17,  1.45it/s]Extractor Estimating: 26it [00:17,  1.50it/s]Extractor Estimating: 27it [00:18,  1.49it/s]Extractor Estimating: 28it [00:19,  1.53it/s]Extractor Estimating: 29it [00:19,  1.55it/s]Extractor Estimating: 30it [00:20,  1.51it/s]Extractor Estimating: 31it [00:20,  1.58it/s]Extractor Estimating: 32it [00:21,  1.58it/s]Extractor Estimating: 33it [00:22,  1.55it/s]Extractor Estimating: 34it [00:22,  1.54it/s]Extractor Estimating: 35it [00:23,  1.59it/s]Extractor Estimating: 36it [00:24,  1.60it/s]Extractor Estimating: 37it [00:24,  1.61it/s]Extractor Estimating: 38it [00:25,  1.60it/s]Extractor Estimating: 39it [00:26,  1.56it/s]Extractor Estimating: 40it [00:26,  1.56it/s]Extractor Estimating: 41it [00:27,  1.57it/s]Extractor Estimating: 42it [00:28,  1.53it/s]Extractor Estimating: 43it [00:28,  1.58it/s]Extractor Estimating: 44it [00:29,  1.56it/s]Extractor Estimating: 45it [00:29,  1.56it/s]Extractor Estimating: 46it [00:30,  1.58it/s]Extractor Estimating: 47it [00:31,  1.60it/s]Extractor Estimating: 48it [00:31,  1.56it/s]Extractor Estimating: 49it [00:32,  1.60it/s]Extractor Estimating: 50it [00:33,  1.57it/s]Extractor Estimating: 51it [00:33,  1.56it/s]Extractor Estimating: 52it [00:34,  1.54it/s]Extractor Estimating: 53it [00:35,  1.55it/s]Extractor Estimating: 54it [00:35,  1.51it/s]Extractor Estimating: 55it [00:36,  1.55it/s]Extractor Estimating: 56it [00:36,  1.58it/s]Extractor Estimating: 57it [00:37,  1.57it/s]Extractor Estimating: 58it [00:38,  1.59it/s]Extractor Estimating: 59it [00:38,  1.53it/s]Extractor Estimating: 60it [00:39,  1.56it/s]Extractor Estimating: 61it [00:40,  1.57it/s]Extractor Estimating: 62it [00:40,  1.57it/s]Extractor Estimating: 63it [00:41,  1.57it/s]Extractor Estimating: 64it [00:42,  1.54it/s]Extractor Estimating: 65it [00:42,  1.57it/s]Extractor Estimating: 66it [00:43,  1.58it/s]Extractor Estimating: 67it [00:43,  1.60it/s]Extractor Estimating: 68it [00:44,  1.56it/s]Extractor Estimating: 69it [00:45,  1.56it/s]Extractor Estimating: 70it [00:45,  1.55it/s]Extractor Estimating: 71it [00:46,  1.53it/s]Extractor Estimating: 72it [00:47,  1.49it/s]Extractor Estimating: 73it [00:47,  1.49it/s]Extractor Estimating: 74it [00:48,  1.46it/s]Extractor Estimating: 75it [00:49,  1.52it/s]Extractor Estimating: 76it [00:49,  1.53it/s]Extractor Estimating: 77it [00:50,  1.53it/s]Extractor Estimating: 78it [00:51,  1.56it/s]Extractor Estimating: 79it [00:51,  1.63it/s]Extractor Estimating: 80it [00:52,  1.61it/s]Extractor Estimating: 81it [00:52,  1.60it/s]Extractor Estimating: 82it [00:53,  1.59it/s]Extractor Estimating: 83it [00:54,  1.63it/s]Extractor Estimating: 84it [00:54,  1.66it/s]Extractor Estimating: 85it [00:55,  1.61it/s]Extractor Estimating: 86it [00:55,  1.66it/s]Extractor Estimating: 87it [00:56,  1.55it/s]Extractor Estimating: 88it [00:57,  1.60it/s]Extractor Estimating: 89it [00:57,  1.65it/s]Extractor Estimating: 90it [00:58,  1.70it/s]Extractor Estimating: 91it [00:59,  1.65it/s]Extractor Estimating: 92it [00:59,  1.63it/s]Extractor Estimating: 93it [01:00,  1.69it/s]Extractor Estimating: 94it [01:00,  1.67it/s]Extractor Estimating: 95it [01:01,  1.67it/s]Extractor Estimating: 96it [01:02,  1.65it/s]Extractor Estimating: 97it [01:02,  1.57it/s]Extractor Estimating: 98it [01:03,  1.66it/s]Extractor Estimating: 99it [01:04,  1.59it/s]Extractor Estimating: 100it [01:04,  1.67it/s]Extractor Estimating: 101it [01:05,  1.66it/s]Extractor Estimating: 102it [01:05,  1.64it/s]Extractor Estimating: 103it [01:06,  1.67it/s]Extractor Estimating: 104it [01:06,  1.64it/s]Extractor Estimating: 105it [01:07,  1.65it/s]Extractor Estimating: 106it [01:08,  1.51it/s]Extractor Estimating: 107it [01:09,  1.46it/s]Extractor Estimating: 108it [01:09,  1.55it/s]Extractor Estimating: 109it [01:10,  1.58it/s]Extractor Estimating: 110it [01:10,  1.65it/s]Extractor Estimating: 111it [01:11,  1.59it/s]Extractor Estimating: 112it [01:12,  1.59it/s]Extractor Estimating: 113it [01:12,  1.57it/s]Extractor Estimating: 114it [01:13,  1.34it/s]Extractor Estimating: 115it [01:14,  1.37it/s]Extractor Estimating: 116it [01:15,  1.40it/s]Extractor Estimating: 117it [01:15,  1.49it/s]Extractor Estimating: 118it [01:16,  1.51it/s]Extractor Estimating: 119it [01:16,  1.53it/s]Extractor Estimating: 120it [01:17,  1.56it/s]Extractor Estimating: 121it [01:18,  1.56it/s]Extractor Estimating: 122it [01:18,  1.57it/s]Extractor Estimating: 123it [01:19,  1.50it/s]Extractor Estimating: 124it [01:20,  1.53it/s]Extractor Estimating: 125it [01:20,  1.51it/s]Extractor Estimating: 126it [01:21,  1.55it/s]Extractor Estimating: 127it [01:22,  1.59it/s]Extractor Estimating: 128it [01:22,  1.56it/s]Extractor Estimating: 129it [01:23,  1.58it/s]Extractor Estimating: 130it [01:23,  1.59it/s]Extractor Estimating: 131it [01:24,  1.59it/s]Extractor Estimating: 132it [01:25,  1.62it/s]Extractor Estimating: 133it [01:25,  1.62it/s]Extractor Estimating: 134it [01:26,  1.66it/s]Extractor Estimating: 135it [01:27,  1.62it/s]Extractor Estimating: 136it [01:27,  1.59it/s]Extractor Estimating: 137it [01:28,  1.62it/s]Extractor Estimating: 138it [01:28,  1.57it/s]Extractor Estimating: 139it [01:29,  1.55it/s]Extractor Estimating: 140it [01:30,  1.52it/s]Extractor Estimating: 141it [01:30,  1.58it/s]Extractor Estimating: 142it [01:31,  1.58it/s]Extractor Estimating: 143it [01:32,  1.60it/s]Extractor Estimating: 144it [01:32,  1.58it/s]Extractor Estimating: 145it [01:33,  1.62it/s]Extractor Estimating: 146it [01:33,  1.65it/s]Extractor Estimating: 147it [01:34,  1.60it/s]Extractor Estimating: 148it [01:35,  1.62it/s]Extractor Estimating: 149it [01:35,  1.63it/s]Extractor Estimating: 150it [01:36,  1.60it/s]Extractor Estimating: 151it [01:37,  1.59it/s]Extractor Estimating: 152it [01:37,  1.53it/s]Extractor Estimating: 153it [01:38,  1.52it/s]Extractor Estimating: 154it [01:39,  1.44it/s]Extractor Estimating: 155it [01:39,  1.43it/s]Extractor Estimating: 156it [01:40,  1.47it/s]Extractor Estimating: 157it [01:41,  1.40it/s]Extractor Estimating: 158it [01:42,  1.37it/s]Extractor Estimating: 159it [01:42,  1.43it/s]Extractor Estimating: 160it [01:43,  1.44it/s]Extractor Estimating: 161it [01:44,  1.43it/s]Extractor Estimating: 162it [01:44,  1.42it/s]Extractor Estimating: 163it [01:45,  1.41it/s]Extractor Estimating: 164it [01:46,  1.43it/s]Extractor Estimating: 165it [01:46,  1.45it/s]Extractor Estimating: 166it [01:47,  1.49it/s]Extractor Estimating: 167it [01:48,  1.45it/s]Extractor Estimating: 168it [01:49,  1.44it/s]Extractor Estimating: 169it [01:49,  1.49it/s]Extractor Estimating: 170it [01:50,  1.46it/s]Extractor Estimating: 171it [01:51,  1.49it/s]Extractor Estimating: 172it [01:51,  1.48it/s]Extractor Estimating: 173it [01:52,  1.44it/s]Extractor Estimating: 174it [01:53,  1.41it/s]Extractor Estimating: 175it [01:53,  1.41it/s]Extractor Estimating: 176it [01:54,  1.44it/s]Extractor Estimating: 177it [01:55,  1.44it/s]Extractor Estimating: 178it [01:55,  1.46it/s]Extractor Estimating: 179it [01:56,  1.54it/s]Extractor Estimating: 180it [01:57,  1.50it/s]Extractor Estimating: 181it [01:57,  1.50it/s]Extractor Estimating: 182it [01:58,  1.47it/s]Extractor Estimating: 183it [01:59,  1.40it/s]Extractor Estimating: 184it [02:00,  1.44it/s]Extractor Estimating: 185it [02:00,  1.44it/s]Extractor Estimating: 186it [02:01,  1.48it/s]Extractor Estimating: 187it [02:02,  1.45it/s]Extractor Estimating: 188it [02:02,  1.44it/s]Extractor Estimating: 189it [02:03,  1.44it/s]Extractor Estimating: 190it [02:04,  1.45it/s]Extractor Estimating: 191it [02:04,  1.43it/s]Extractor Estimating: 192it [02:05,  1.44it/s]Extractor Estimating: 193it [02:06,  1.48it/s]Extractor Estimating: 194it [02:06,  1.51it/s]Extractor Estimating: 195it [02:07,  1.50it/s]Extractor Estimating: 196it [02:08,  1.47it/s]Extractor Estimating: 197it [02:08,  1.46it/s]Extractor Estimating: 198it [02:09,  1.46it/s]Extractor Estimating: 199it [02:10,  1.46it/s]Extractor Estimating: 200it [02:11,  1.40it/s]Extractor Estimating: 201it [02:11,  1.41it/s]Extractor Estimating: 202it [02:12,  1.39it/s]Extractor Estimating: 203it [02:13,  1.45it/s]Extractor Estimating: 204it [02:13,  1.46it/s]Extractor Estimating: 205it [02:14,  1.47it/s]Extractor Estimating: 206it [02:15,  1.50it/s]Extractor Estimating: 207it [02:15,  1.50it/s]Extractor Estimating: 208it [02:16,  1.50it/s]Extractor Estimating: 209it [02:17,  1.37it/s]Extractor Estimating: 210it [02:17,  1.43it/s]Extractor Estimating: 211it [02:18,  1.44it/s]Extractor Estimating: 212it [02:19,  1.48it/s]Extractor Estimating: 213it [02:19,  1.52it/s]Extractor Estimating: 214it [02:20,  1.51it/s]Extractor Estimating: 215it [02:21,  1.46it/s]Extractor Estimating: 216it [02:21,  1.49it/s]Extractor Estimating: 217it [02:22,  1.47it/s]Extractor Estimating: 218it [02:23,  1.53it/s]Extractor Estimating: 219it [02:23,  1.57it/s]Extractor Estimating: 220it [02:24,  1.54it/s]Extractor Estimating: 221it [02:25,  1.50it/s]Extractor Estimating: 222it [02:25,  1.49it/s]Extractor Estimating: 223it [02:26,  1.52it/s]Extractor Estimating: 224it [02:27,  1.50it/s]Extractor Estimating: 225it [02:27,  1.53it/s]Extractor Estimating: 226it [02:28,  1.60it/s]Extractor Estimating: 227it [02:28,  1.61it/s]Extractor Estimating: 228it [02:29,  1.66it/s]Extractor Estimating: 229it [02:30,  1.72it/s]Extractor Estimating: 230it [02:30,  1.77it/s]Extractor Estimating: 231it [02:31,  1.78it/s]Extractor Estimating: 232it [02:31,  1.76it/s]Extractor Estimating: 233it [02:32,  1.76it/s]Extractor Estimating: 234it [02:32,  1.72it/s]Extractor Estimating: 235it [02:33,  1.72it/s]Extractor Estimating: 236it [02:34,  1.73it/s]Extractor Estimating: 237it [02:34,  1.75it/s]Extractor Estimating: 238it [02:35,  1.79it/s]Extractor Estimating: 239it [02:35,  1.86it/s]Extractor Estimating: 240it [02:36,  1.84it/s]Extractor Estimating: 241it [02:36,  1.79it/s]Extractor Estimating: 242it [02:37,  1.75it/s]Extractor Estimating: 243it [02:37,  1.74it/s]Extractor Estimating: 244it [02:38,  1.74it/s]Extractor Estimating: 245it [02:39,  1.79it/s]Extractor Estimating: 246it [02:39,  1.77it/s]Extractor Estimating: 247it [02:40,  1.76it/s]Extractor Estimating: 248it [02:40,  1.73it/s]Extractor Estimating: 249it [02:41,  1.77it/s]Extractor Estimating: 250it [02:41,  1.80it/s]Extractor Estimating: 251it [02:42,  1.70it/s]Extractor Estimating: 252it [02:43,  1.65it/s]Extractor Estimating: 253it [02:43,  1.57it/s]Extractor Estimating: 254it [02:44,  1.40it/s]Extractor Estimating: 255it [02:45,  1.45it/s]Extractor Estimating: 256it [02:46,  1.46it/s]Extractor Estimating: 257it [02:46,  1.46it/s]Extractor Estimating: 258it [02:47,  1.47it/s]Extractor Estimating: 259it [02:48,  1.50it/s]Extractor Estimating: 260it [02:48,  1.55it/s]Extractor Estimating: 261it [02:49,  1.55it/s]Extractor Estimating: 262it [02:49,  1.58it/s]Extractor Estimating: 263it [02:50,  1.61it/s]Extractor Estimating: 264it [02:51,  1.59it/s]Extractor Estimating: 265it [02:51,  1.58it/s]Extractor Estimating: 266it [02:52,  1.60it/s]Extractor Estimating: 267it [02:53,  1.59it/s]Extractor Estimating: 268it [02:53,  1.61it/s]Extractor Estimating: 269it [02:54,  1.59it/s]Extractor Estimating: 270it [02:54,  1.60it/s]Extractor Estimating: 271it [02:55,  1.57it/s]Extractor Estimating: 272it [02:56,  1.54it/s]Extractor Estimating: 273it [02:56,  1.55it/s]Extractor Estimating: 274it [02:57,  1.58it/s]Extractor Estimating: 275it [02:58,  1.64it/s]Extractor Estimating: 276it [02:58,  1.65it/s]Extractor Estimating: 277it [02:59,  1.70it/s]Extractor Estimating: 278it [02:59,  1.70it/s]Extractor Estimating: 279it [03:00,  1.70it/s]Extractor Estimating: 280it [03:00,  1.75it/s]Extractor Estimating: 281it [03:01,  1.79it/s]Extractor Estimating: 282it [03:02,  1.78it/s]Extractor Estimating: 283it [03:02,  1.68it/s]Extractor Estimating: 284it [03:03,  1.73it/s]Extractor Estimating: 285it [03:03,  1.75it/s]Extractor Estimating: 286it [03:04,  1.74it/s]Extractor Estimating: 287it [03:04,  1.76it/s]Extractor Estimating: 288it [03:05,  1.70it/s]Extractor Estimating: 289it [03:06,  1.76it/s]Extractor Estimating: 290it [03:06,  1.70it/s]Extractor Estimating: 291it [03:07,  1.67it/s]Extractor Estimating: 292it [03:07,  1.72it/s]Extractor Estimating: 293it [03:08,  1.74it/s]Extractor Estimating: 294it [03:09,  1.74it/s]Extractor Estimating: 295it [03:09,  1.74it/s]Extractor Estimating: 296it [03:10,  1.77it/s]Extractor Estimating: 297it [03:10,  1.76it/s]Extractor Estimating: 298it [03:11,  1.74it/s]Extractor Estimating: 299it [03:11,  1.70it/s]Extractor Estimating: 300it [03:12,  1.72it/s]Extractor Estimating: 301it [03:13,  1.62it/s]Extractor Estimating: 302it [03:13,  1.65it/s]Extractor Estimating: 303it [03:14,  1.64it/s]Extractor Estimating: 304it [03:15,  1.58it/s]Extractor Estimating: 305it [03:15,  1.58it/s]Extractor Estimating: 306it [03:16,  1.60it/s]Extractor Estimating: 307it [03:16,  1.59it/s]Extractor Estimating: 308it [03:17,  1.57it/s]Extractor Estimating: 309it [03:18,  1.57it/s]Extractor Estimating: 310it [03:18,  1.58it/s]Extractor Estimating: 311it [03:19,  1.58it/s]Extractor Estimating: 312it [03:20,  1.57it/s]Extractor Estimating: 313it [03:20,  1.61it/s]Extractor Estimating: 314it [03:21,  1.62it/s]Extractor Estimating: 315it [03:21,  1.61it/s]Extractor Estimating: 316it [03:22,  1.65it/s]Extractor Estimating: 317it [03:23,  1.56it/s]Extractor Estimating: 318it [03:23,  1.55it/s]Extractor Estimating: 319it [03:24,  1.57it/s]Extractor Estimating: 320it [03:25,  1.57it/s]Extractor Estimating: 321it [03:25,  1.58it/s]Extractor Estimating: 322it [03:26,  1.56it/s]Extractor Estimating: 323it [03:27,  1.59it/s]Extractor Estimating: 324it [03:27,  1.63it/s]Extractor Estimating: 325it [03:28,  1.61it/s]Extractor Estimating: 326it [03:28,  1.56it/s]Extractor Estimating: 327it [03:29,  1.56it/s]Extractor Estimating: 328it [03:30,  1.57it/s]Extractor Estimating: 329it [03:30,  1.59it/s]Extractor Estimating: 330it [03:31,  1.62it/s]Extractor Estimating: 331it [03:32,  1.59it/s]Extractor Estimating: 332it [03:32,  1.56it/s]Extractor Estimating: 333it [03:33,  1.58it/s]Extractor Estimating: 334it [03:33,  1.61it/s]Extractor Estimating: 335it [03:34,  1.60it/s]Extractor Estimating: 336it [03:35,  1.64it/s]Extractor Estimating: 337it [03:35,  1.60it/s]Extractor Estimating: 338it [03:36,  1.59it/s]Extractor Estimating: 339it [03:37,  1.57it/s]Extractor Estimating: 340it [03:37,  1.57it/s]Extractor Estimating: 341it [03:38,  1.59it/s]Extractor Estimating: 342it [03:38,  1.61it/s]Extractor Estimating: 343it [03:39,  1.61it/s]Extractor Estimating: 344it [03:40,  1.61it/s]Extractor Estimating: 345it [03:40,  1.60it/s]Extractor Estimating: 346it [03:41,  1.56it/s]Extractor Estimating: 347it [03:42,  1.60it/s]Extractor Estimating: 348it [03:42,  1.61it/s]Extractor Estimating: 349it [03:43,  1.64it/s]Extractor Estimating: 350it [03:43,  1.61it/s]Extractor Estimating: 351it [03:44,  1.63it/s]Extractor Estimating: 352it [03:45,  1.64it/s]Extractor Estimating: 353it [03:45,  1.60it/s]Extractor Estimating: 354it [03:46,  1.55it/s]Extractor Estimating: 355it [03:47,  1.49it/s]Extractor Estimating: 356it [03:47,  1.50it/s]Extractor Estimating: 357it [03:48,  1.49it/s]Extractor Estimating: 358it [03:49,  1.50it/s]Extractor Estimating: 359it [03:49,  1.51it/s]Extractor Estimating: 360it [03:50,  1.53it/s]Extractor Estimating: 361it [03:51,  1.53it/s]Extractor Estimating: 362it [03:51,  1.52it/s]Extractor Estimating: 363it [03:52,  1.50it/s]Extractor Estimating: 364it [03:53,  1.46it/s]Extractor Estimating: 365it [03:53,  1.47it/s]Extractor Estimating: 366it [03:54,  1.49it/s]Extractor Estimating: 367it [03:55,  1.52it/s]Extractor Estimating: 368it [03:55,  1.51it/s]Extractor Estimating: 369it [03:56,  1.50it/s]Extractor Estimating: 370it [03:57,  1.53it/s]Extractor Estimating: 371it [03:57,  1.51it/s]Extractor Estimating: 372it [03:58,  1.49it/s]Extractor Estimating: 373it [03:59,  1.52it/s]Extractor Estimating: 374it [03:59,  1.54it/s]Extractor Estimating: 375it [04:00,  1.62it/s]Extractor Estimating: 375it [04:00,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:30,583 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:30,605 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:30,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:30,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:30,606 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:59:31,107 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:59:31,109 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:59:31,382 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:59:32,492 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:59:32,492 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:33,851 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:33,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:33,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:33,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:59:33,858 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:59:34,224 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:59:34,225 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:59:34,476 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:59:34,666 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:59:34,667 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 22:48:42,433 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 22:48:42,466 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 5997 mean pseudo reward: 0.9591119778072708
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 16326
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16426, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16426, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.068, loss:544.6579
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.104, loss:508.2402
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 1.072, loss:466.1929
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 150, avg_time 1.063, loss:456.0677
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 250, avg_time 1.076, loss:484.8616
>> valid entity prec:0.5477, rec:0.5908, f1:0.5684
>> valid relation prec:0.3622, rec:0.1893, f1:0.2487
>> valid relation with NER prec:0.3622, rec:0.1893, f1:0.2487
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.072, loss:445.6583
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 1.054, loss:428.4100
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 1.080, loss:460.3751
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 1.074, loss:422.2509
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 1.071, loss:464.9761
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5217, rec:0.6353, f1:0.5729
>> valid relation prec:0.3231, rec:0.1954, f1:0.2435
>> valid relation with NER prec:0.3231, rec:0.1954, f1:0.2435
new max entity f1 on valid!
g_step 1100, step 100, avg_time 1.069, loss:425.0486
g_step 1200, step 200, avg_time 1.080, loss:420.5181
g_step 1300, step 50, avg_time 1.053, loss:444.0446
g_step 1400, step 150, avg_time 1.066, loss:397.1590
g_step 1500, step 250, avg_time 1.079, loss:393.2477
>> valid entity prec:0.5941, rec:0.4735, f1:0.5270
>> valid relation prec:0.3847, rec:0.1538, f1:0.2198
>> valid relation with NER prec:0.3847, rec:0.1538, f1:0.2198
g_step 1600, step 100, avg_time 1.078, loss:366.0790
g_step 1700, step 200, avg_time 1.068, loss:387.8916
g_step 1800, step 50, avg_time 1.085, loss:359.7083
g_step 1900, step 150, avg_time 1.057, loss:377.5086
g_step 2000, step 250, avg_time 1.069, loss:363.2527
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5248, rec:0.5809, f1:0.5514
>> valid relation prec:0.2989, rec:0.1641, f1:0.2119
>> valid relation with NER prec:0.2989, rec:0.1641, f1:0.2119
g_step 2100, step 100, avg_time 1.074, loss:329.1736
g_step 2200, step 200, avg_time 1.078, loss:338.5031
g_step 2300, step 50, avg_time 1.055, loss:329.4581
g_step 2400, step 150, avg_time 1.067, loss:330.4768
g_step 2500, step 250, avg_time 1.071, loss:343.8765
>> valid entity prec:0.5890, rec:0.5014, f1:0.5417
>> valid relation prec:0.3088, rec:0.1573, f1:0.2084
>> valid relation with NER prec:0.3088, rec:0.1573, f1:0.2084
g_step 2600, step 100, avg_time 1.077, loss:279.7737
g_step 2700, step 200, avg_time 1.057, loss:313.8624
g_step 2800, step 50, avg_time 1.081, loss:294.7870
g_step 2900, step 150, avg_time 1.060, loss:296.2864
g_step 3000, step 250, avg_time 1.078, loss:308.3525
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5395, rec:0.5803, f1:0.5591
>> valid relation prec:0.2984, rec:0.1928, f1:0.2342
>> valid relation with NER prec:0.2984, rec:0.1928, f1:0.2342
g_step 3100, step 100, avg_time 1.068, loss:267.4677
g_step 3200, step 200, avg_time 1.070, loss:271.9765
g_step 3300, step 50, avg_time 1.074, loss:265.7480
g_step 3400, step 150, avg_time 1.059, loss:263.4459
g_step 3500, step 250, avg_time 1.054, loss:263.8612
>> valid entity prec:0.5406, rec:0.5829, f1:0.5609
>> valid relation prec:0.2769, rec:0.1733, f1:0.2132
>> valid relation with NER prec:0.2769, rec:0.1733, f1:0.2132
g_step 3600, step 100, avg_time 1.059, loss:237.6293
g_step 3700, step 200, avg_time 1.070, loss:251.1644
g_step 3800, step 50, avg_time 1.053, loss:229.2272
g_step 3900, step 150, avg_time 1.059, loss:247.7487
g_step 4000, step 250, avg_time 1.064, loss:239.4914
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5677, rec:0.5576, f1:0.5626
>> valid relation prec:0.3368, rec:0.1848, f1:0.2386
>> valid relation with NER prec:0.3368, rec:0.1848, f1:0.2386
g_step 4100, step 100, avg_time 1.058, loss:217.7236
g_step 4200, step 200, avg_time 1.075, loss:216.4654
g_step 4300, step 50, avg_time 1.065, loss:209.3099
g_step 4400, step 150, avg_time 1.064, loss:209.5525
g_step 4500, step 250, avg_time 1.061, loss:235.0052
>> valid entity prec:0.5566, rec:0.5494, f1:0.5530
>> valid relation prec:0.2922, rec:0.1670, f1:0.2125
>> valid relation with NER prec:0.2922, rec:0.1670, f1:0.2125
g_step 4600, step 100, avg_time 1.084, loss:204.9373
g_step 4700, step 200, avg_time 1.049, loss:220.5627
g_step 4800, step 50, avg_time 1.067, loss:196.3920
g_step 4900, step 150, avg_time 1.049, loss:193.2836
g_step 5000, step 250, avg_time 1.067, loss:202.6723
learning rate was adjusted to 0.0008
>> valid entity prec:0.5572, rec:0.5428, f1:0.5499
>> valid relation prec:0.2876, rec:0.1681, f1:0.2122
>> valid relation with NER prec:0.2876, rec:0.1681, f1:0.2122
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 22:48:42 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 22:48:42 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_22-48-42_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 22:48:44 - WARNING - datasets.builder -   Using custom data configuration default-d7561098c2007c36
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d7561098c2007c36/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 22:48:44,590 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:48:44,591 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:48:44,591 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:48:44,592 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:48:44,606 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:48:44,614 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:48:44,614 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:48:44,614 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:48:44,614 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:48:44,614 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:48:44,614 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 22:48:44,778 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:48:48,030 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 22:48:48,039 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d7561098c2007c36/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.39ba/s] 33%|███▎      | 2/6 [00:00<00:00,  4.12ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.43ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.57ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.68ba/s]100%|██████████| 6/6 [00:01<00:00,  4.76ba/s]100%|██████████| 6/6 [00:01<00:00,  4.54ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.01ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.33ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.40ba/s]100%|██████████| 4/4 [00:00<00:00,  5.52ba/s]100%|██████████| 4/4 [00:00<00:00,  4.99ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:00,  9.35ba/s] 50%|█████     | 3/6 [00:00<00:00, 10.46ba/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.56ba/s]100%|██████████| 6/6 [00:00<00:00, 10.45ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.94ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.21ba/s]100%|██████████| 4/4 [00:00<00:00, 11.62ba/s]
[INFO|trainer.py:414] 2023-08-28 22:48:51,680 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 22:48:51,707 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 22:48:51,707 >>   Num examples = 6000
[INFO|trainer.py:1149] 2023-08-28 22:48:51,707 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 22:48:51,707 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 22:48:51,707 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 22:48:51,708 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 22:48:51,708 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:00<03:35,  2.18it/s]  0%|          | 2/470 [00:00<02:54,  2.69it/s]  1%|          | 3/470 [00:01<02:44,  2.85it/s]  1%|          | 4/470 [00:01<02:34,  3.02it/s]  1%|          | 5/470 [00:01<02:26,  3.17it/s]  1%|▏         | 6/470 [00:01<02:23,  3.24it/s]  1%|▏         | 7/470 [00:02<02:21,  3.27it/s]  2%|▏         | 8/470 [00:02<02:22,  3.23it/s]  2%|▏         | 9/470 [00:02<02:20,  3.29it/s]  2%|▏         | 10/470 [00:03<02:17,  3.34it/s]  2%|▏         | 11/470 [00:03<02:15,  3.38it/s]  3%|▎         | 12/470 [00:03<02:14,  3.40it/s]  3%|▎         | 13/470 [00:04<02:14,  3.39it/s]  3%|▎         | 14/470 [00:04<02:13,  3.41it/s]  3%|▎         | 15/470 [00:04<02:12,  3.43it/s]  3%|▎         | 16/470 [00:04<02:12,  3.44it/s]  4%|▎         | 17/470 [00:05<02:11,  3.45it/s]  4%|▍         | 18/470 [00:05<02:11,  3.45it/s]  4%|▍         | 19/470 [00:05<02:10,  3.45it/s]  4%|▍         | 20/470 [00:06<02:12,  3.41it/s]  4%|▍         | 21/470 [00:06<02:11,  3.42it/s]  5%|▍         | 22/470 [00:06<02:10,  3.43it/s]  5%|▍         | 23/470 [00:06<02:09,  3.44it/s]  5%|▌         | 24/470 [00:07<02:09,  3.45it/s]  5%|▌         | 25/470 [00:07<02:08,  3.45it/s]  6%|▌         | 26/470 [00:07<02:08,  3.46it/s]  6%|▌         | 27/470 [00:08<02:08,  3.46it/s]  6%|▌         | 28/470 [00:08<02:07,  3.46it/s]  6%|▌         | 29/470 [00:08<02:07,  3.46it/s]  6%|▋         | 30/470 [00:08<02:07,  3.46it/s]  7%|▋         | 31/470 [00:09<02:08,  3.42it/s]  7%|▋         | 32/470 [00:09<02:08,  3.41it/s]  7%|▋         | 33/470 [00:09<02:07,  3.43it/s]  7%|▋         | 34/470 [00:10<02:06,  3.44it/s]  7%|▋         | 35/470 [00:10<02:06,  3.44it/s]  8%|▊         | 36/470 [00:10<02:05,  3.45it/s]  8%|▊         | 37/470 [00:11<02:05,  3.45it/s]  8%|▊         | 38/470 [00:11<02:05,  3.45it/s]  8%|▊         | 39/470 [00:11<02:04,  3.45it/s]  9%|▊         | 40/470 [00:11<02:04,  3.45it/s]  9%|▊         | 41/470 [00:12<02:04,  3.45it/s]  9%|▉         | 42/470 [00:12<02:04,  3.44it/s]  9%|▉         | 43/470 [00:12<02:03,  3.45it/s]  9%|▉         | 44/470 [00:13<02:03,  3.45it/s] 10%|▉         | 45/470 [00:13<02:03,  3.45it/s] 10%|▉         | 46/470 [00:13<02:02,  3.45it/s] 10%|█         | 47/470 [00:13<02:02,  3.46it/s] 10%|█         | 48/470 [00:14<02:02,  3.46it/s] 10%|█         | 49/470 [00:14<02:01,  3.46it/s] 11%|█         | 50/470 [00:14<02:01,  3.46it/s] 11%|█         | 51/470 [00:15<02:01,  3.46it/s] 11%|█         | 52/470 [00:15<02:00,  3.46it/s] 11%|█▏        | 53/470 [00:15<02:01,  3.44it/s] 11%|█▏        | 54/470 [00:15<02:00,  3.44it/s] 12%|█▏        | 55/470 [00:16<02:00,  3.45it/s] 12%|█▏        | 56/470 [00:16<02:00,  3.45it/s] 12%|█▏        | 57/470 [00:16<01:59,  3.45it/s] 12%|█▏        | 58/470 [00:17<01:59,  3.45it/s] 13%|█▎        | 59/470 [00:17<01:59,  3.45it/s] 13%|█▎        | 60/470 [00:17<01:58,  3.45it/s] 13%|█▎        | 61/470 [00:17<01:58,  3.45it/s] 13%|█▎        | 62/470 [00:18<01:58,  3.45it/s] 13%|█▎        | 63/470 [00:18<01:57,  3.45it/s] 14%|█▎        | 64/470 [00:18<01:59,  3.41it/s] 14%|█▍        | 65/470 [00:19<01:58,  3.43it/s] 14%|█▍        | 66/470 [00:19<01:57,  3.43it/s] 14%|█▍        | 67/470 [00:19<01:57,  3.44it/s] 14%|█▍        | 68/470 [00:20<01:56,  3.44it/s] 15%|█▍        | 69/470 [00:20<01:56,  3.45it/s] 15%|█▍        | 70/470 [00:20<01:56,  3.44it/s] 15%|█▌        | 71/470 [00:20<01:55,  3.45it/s] 15%|█▌        | 72/470 [00:21<01:55,  3.45it/s] 16%|█▌        | 73/470 [00:21<01:55,  3.45it/s] 16%|█▌        | 74/470 [00:21<01:54,  3.45it/s] 16%|█▌        | 75/470 [00:22<01:55,  3.43it/s] 16%|█▌        | 76/470 [00:22<01:54,  3.44it/s] 16%|█▋        | 77/470 [00:22<01:54,  3.44it/s] 17%|█▋        | 78/470 [00:22<01:53,  3.44it/s] 17%|█▋        | 79/470 [00:23<01:53,  3.44it/s] 17%|█▋        | 80/470 [00:23<01:53,  3.44it/s] 17%|█▋        | 81/470 [00:23<01:53,  3.44it/s] 17%|█▋        | 82/470 [00:24<01:52,  3.44it/s] 18%|█▊        | 83/470 [00:24<01:52,  3.45it/s] 18%|█▊        | 84/470 [00:24<01:52,  3.45it/s] 18%|█▊        | 85/470 [00:24<01:51,  3.44it/s] 18%|█▊        | 86/470 [00:25<01:51,  3.45it/s] 19%|█▊        | 87/470 [00:25<01:51,  3.44it/s] 19%|█▊        | 88/470 [00:25<01:50,  3.45it/s] 19%|█▉        | 89/470 [00:26<01:50,  3.44it/s] 19%|█▉        | 90/470 [00:26<01:50,  3.45it/s] 19%|█▉        | 91/470 [00:26<01:54,  3.31it/s] 20%|█▉        | 92/470 [00:27<01:52,  3.35it/s] 20%|█▉        | 93/470 [00:27<01:51,  3.38it/s] 20%|██        | 94/470 [00:27<01:43,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 22:49:19,250 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:49:19,250 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:49:19,250 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.18it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.77it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.36it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.40it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.86it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.60it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.44it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.46it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.56it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.44it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.36it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.40it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.29it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.14it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.11it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.13it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.33it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.35it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.34it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.23it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.24it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.26it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.13it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.09it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.10it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.26it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.35it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.39it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.31it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.28it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.20it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.09it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.03it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.10it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.20it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.38it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.35it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.29it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.06it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.24it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.14it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.10it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.16it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.20it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.31it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.32it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.34it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.14it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.17it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.17it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.15it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.16it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.28it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.28it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.35it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.21it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.09it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.19it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.23it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.17it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.28it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.27it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.29it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.27it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.20it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.12it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.14it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.12it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.16it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.25it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.27it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.22it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.25it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.21it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.03it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.19it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.21it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.24it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.19it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.24it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.21it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.11it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.00it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.12it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.14it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.25it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.20it/s][A
                                                 [A                                                
100%|██████████| 437/437 [00:09<00:00, 44.20it/s][A 20%|██        | 94/470 [00:37<01:43,  3.63it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:49:29,175 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 22:49:29,217 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:49:33,531 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:49:33,687 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:49:33,719 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [00:49<41:55,  6.71s/it] 20%|██        | 96/470 [00:49<29:49,  4.78s/it] 21%|██        | 97/470 [00:49<21:22,  3.44s/it] 21%|██        | 98/470 [00:50<15:27,  2.49s/it] 21%|██        | 99/470 [00:50<11:20,  1.83s/it] 21%|██▏       | 100/470 [00:50<08:27,  1.37s/it] 21%|██▏       | 101/470 [00:51<06:38,  1.08s/it] 22%|██▏       | 102/470 [00:51<05:10,  1.19it/s] 22%|██▏       | 103/470 [00:51<04:09,  1.47it/s] 22%|██▏       | 104/470 [00:52<03:26,  1.77it/s] 22%|██▏       | 105/470 [00:52<02:56,  2.07it/s] 23%|██▎       | 106/470 [00:52<02:36,  2.33it/s] 23%|██▎       | 107/470 [00:52<02:20,  2.58it/s] 23%|██▎       | 108/470 [00:53<02:09,  2.79it/s] 23%|██▎       | 109/470 [00:53<02:02,  2.96it/s] 23%|██▎       | 110/470 [00:53<01:56,  3.09it/s] 24%|██▎       | 111/470 [00:54<01:52,  3.18it/s] 24%|██▍       | 112/470 [00:54<01:49,  3.26it/s] 24%|██▍       | 113/470 [00:54<01:47,  3.31it/s] 24%|██▍       | 114/470 [00:54<01:46,  3.35it/s] 24%|██▍       | 115/470 [00:55<01:45,  3.37it/s] 25%|██▍       | 116/470 [00:55<01:44,  3.40it/s] 25%|██▍       | 117/470 [00:55<01:43,  3.41it/s] 25%|██▌       | 118/470 [00:56<01:43,  3.42it/s] 25%|██▌       | 119/470 [00:56<01:42,  3.42it/s] 26%|██▌       | 120/470 [00:56<01:42,  3.43it/s] 26%|██▌       | 121/470 [00:56<01:41,  3.43it/s] 26%|██▌       | 122/470 [00:57<01:42,  3.41it/s] 26%|██▌       | 123/470 [00:57<01:41,  3.42it/s] 26%|██▋       | 124/470 [00:57<01:40,  3.43it/s] 27%|██▋       | 125/470 [00:58<01:40,  3.43it/s] 27%|██▋       | 126/470 [00:58<01:40,  3.43it/s] 27%|██▋       | 127/470 [00:58<01:39,  3.44it/s] 27%|██▋       | 128/470 [00:59<01:39,  3.44it/s] 27%|██▋       | 129/470 [00:59<01:39,  3.44it/s] 28%|██▊       | 130/470 [00:59<01:38,  3.44it/s] 28%|██▊       | 131/470 [00:59<01:38,  3.44it/s] 28%|██▊       | 132/470 [01:00<01:38,  3.44it/s] 28%|██▊       | 133/470 [01:00<01:38,  3.41it/s] 29%|██▊       | 134/470 [01:00<01:38,  3.43it/s] 29%|██▊       | 135/470 [01:01<01:37,  3.43it/s] 29%|██▉       | 136/470 [01:01<01:37,  3.43it/s] 29%|██▉       | 137/470 [01:01<01:36,  3.44it/s] 29%|██▉       | 138/470 [01:01<01:36,  3.44it/s] 30%|██▉       | 139/470 [01:02<01:36,  3.44it/s] 30%|██▉       | 140/470 [01:02<01:36,  3.42it/s] 30%|███       | 141/470 [01:02<01:36,  3.41it/s] 30%|███       | 142/470 [01:03<01:36,  3.41it/s] 30%|███       | 143/470 [01:03<01:36,  3.41it/s] 31%|███       | 144/470 [01:03<01:36,  3.39it/s] 31%|███       | 145/470 [01:03<01:35,  3.39it/s] 31%|███       | 146/470 [01:04<01:35,  3.39it/s] 31%|███▏      | 147/470 [01:04<01:35,  3.40it/s] 31%|███▏      | 148/470 [01:04<01:34,  3.39it/s] 32%|███▏      | 149/470 [01:05<01:34,  3.39it/s] 32%|███▏      | 150/470 [01:05<01:34,  3.39it/s] 32%|███▏      | 151/470 [01:05<01:33,  3.40it/s] 32%|███▏      | 152/470 [01:06<01:33,  3.39it/s] 33%|███▎      | 153/470 [01:06<01:33,  3.39it/s] 33%|███▎      | 154/470 [01:06<01:32,  3.40it/s] 33%|███▎      | 155/470 [01:06<01:33,  3.38it/s] 33%|███▎      | 156/470 [01:07<01:32,  3.39it/s] 33%|███▎      | 157/470 [01:07<01:32,  3.39it/s] 34%|███▎      | 158/470 [01:07<01:31,  3.39it/s] 34%|███▍      | 159/470 [01:08<01:31,  3.39it/s] 34%|███▍      | 160/470 [01:08<01:31,  3.39it/s] 34%|███▍      | 161/470 [01:08<01:31,  3.38it/s] 34%|███▍      | 162/470 [01:08<01:30,  3.39it/s] 35%|███▍      | 163/470 [01:09<01:30,  3.39it/s] 35%|███▍      | 164/470 [01:09<01:30,  3.39it/s] 35%|███▌      | 165/470 [01:09<01:29,  3.39it/s] 35%|███▌      | 166/470 [01:10<01:29,  3.38it/s] 36%|███▌      | 167/470 [01:10<01:29,  3.39it/s] 36%|███▌      | 168/470 [01:10<01:29,  3.39it/s] 36%|███▌      | 169/470 [01:11<01:28,  3.39it/s] 36%|███▌      | 170/470 [01:11<01:28,  3.39it/s] 36%|███▋      | 171/470 [01:11<01:28,  3.39it/s] 37%|███▋      | 172/470 [01:11<01:27,  3.39it/s] 37%|███▋      | 173/470 [01:12<01:27,  3.39it/s] 37%|███▋      | 174/470 [01:12<01:27,  3.38it/s] 37%|███▋      | 175/470 [01:12<01:27,  3.38it/s] 37%|███▋      | 176/470 [01:13<01:26,  3.38it/s] 38%|███▊      | 177/470 [01:13<01:27,  3.35it/s] 38%|███▊      | 178/470 [01:13<01:26,  3.36it/s] 38%|███▊      | 179/470 [01:14<01:26,  3.37it/s] 38%|███▊      | 180/470 [01:14<01:25,  3.38it/s] 39%|███▊      | 181/470 [01:14<01:25,  3.38it/s] 39%|███▊      | 182/470 [01:14<01:25,  3.39it/s] 39%|███▉      | 183/470 [01:15<01:24,  3.38it/s] 39%|███▉      | 184/470 [01:15<01:24,  3.39it/s] 39%|███▉      | 185/470 [01:15<01:24,  3.39it/s] 40%|███▉      | 186/470 [01:16<01:23,  3.39it/s] 40%|███▉      | 187/470 [01:16<01:23,  3.38it/s] 40%|████      | 188/470 [01:16<01:18,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 22:50:08,331 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:50:08,332 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:50:08,332 >>   Batch size = 8
{'eval_loss': 1.13685142993927, 'eval_runtime': 9.8792, 'eval_samples_per_second': 353.571, 'eval_steps_per_second': 44.234, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.40it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.05it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.24it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.24it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.85it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.34it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.21it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.12it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.05it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.24it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.28it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.26it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.13it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.04it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.02it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.02it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.81it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.89it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.08it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.14it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.17it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.03it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.92it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.39it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 40.45it/s][A
 30%|███       | 132/437 [00:03<00:07, 41.70it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 42.38it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 42.96it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.33it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.43it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.59it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.64it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.35it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.56it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.72it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.87it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.06it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.23it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.17it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.06it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.76it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.68it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.88it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.98it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.15it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.21it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.28it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.11it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.91it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.87it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.70it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 42.97it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.60it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.73it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.96it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.98it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.97it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.82it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.74it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.67it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.76it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.02it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.18it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.24it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.17it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.09it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.93it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.75it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.63it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.81it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.99it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.11it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.16it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.20it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.11it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.93it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.72it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.79it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.80it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.03it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.22it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.30it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.23it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.02it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.86it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.81it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.81it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.81it/s][A 40%|████      | 188/470 [01:26<01:18,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:50:18,330 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 22:50:18,361 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:50:21,750 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:50:21,820 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:50:21,839 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:38<31:35,  6.74s/it] 40%|████      | 190/470 [01:38<22:29,  4.82s/it] 41%|████      | 191/470 [01:39<16:06,  3.46s/it] 41%|████      | 192/470 [01:39<11:38,  2.51s/it] 41%|████      | 193/470 [01:39<08:31,  1.85s/it] 41%|████▏     | 194/470 [01:39<06:21,  1.38s/it] 41%|████▏     | 195/470 [01:40<04:50,  1.06s/it] 42%|████▏     | 196/470 [01:40<03:46,  1.21it/s] 42%|████▏     | 197/470 [01:40<03:02,  1.50it/s] 42%|████▏     | 198/470 [01:41<02:31,  1.80it/s] 42%|████▏     | 199/470 [01:41<02:09,  2.09it/s] 43%|████▎     | 200/470 [01:41<01:54,  2.36it/s] 43%|████▎     | 201/470 [01:42<01:43,  2.59it/s] 43%|████▎     | 202/470 [01:42<01:36,  2.78it/s] 43%|████▎     | 203/470 [01:42<01:30,  2.94it/s] 43%|████▎     | 204/470 [01:42<01:26,  3.06it/s] 44%|████▎     | 205/470 [01:43<01:24,  3.15it/s] 44%|████▍     | 206/470 [01:43<01:21,  3.22it/s] 44%|████▍     | 207/470 [01:43<01:20,  3.27it/s] 44%|████▍     | 208/470 [01:44<01:19,  3.30it/s] 44%|████▍     | 209/470 [01:44<01:18,  3.33it/s] 45%|████▍     | 210/470 [01:44<01:17,  3.35it/s] 45%|████▍     | 211/470 [01:44<01:17,  3.36it/s] 45%|████▌     | 212/470 [01:45<01:17,  3.32it/s] 45%|████▌     | 213/470 [01:45<01:16,  3.34it/s] 46%|████▌     | 214/470 [01:45<01:16,  3.36it/s] 46%|████▌     | 215/470 [01:46<01:15,  3.37it/s] 46%|████▌     | 216/470 [01:46<01:15,  3.38it/s] 46%|████▌     | 217/470 [01:46<01:14,  3.38it/s] 46%|████▋     | 218/470 [01:47<01:14,  3.39it/s] 47%|████▋     | 219/470 [01:47<01:14,  3.39it/s] 47%|████▋     | 220/470 [01:47<01:13,  3.39it/s] 47%|████▋     | 221/470 [01:47<01:13,  3.39it/s] 47%|████▋     | 222/470 [01:48<01:13,  3.39it/s] 47%|████▋     | 223/470 [01:48<01:13,  3.37it/s] 48%|████▊     | 224/470 [01:48<01:14,  3.28it/s] 48%|████▊     | 225/470 [01:49<01:13,  3.32it/s] 48%|████▊     | 226/470 [01:49<01:13,  3.34it/s] 48%|████▊     | 227/470 [01:49<01:12,  3.35it/s] 49%|████▊     | 228/470 [01:50<01:11,  3.37it/s] 49%|████▊     | 229/470 [01:50<01:11,  3.38it/s] 49%|████▉     | 230/470 [01:50<01:10,  3.38it/s] 49%|████▉     | 231/470 [01:50<01:10,  3.38it/s] 49%|████▉     | 232/470 [01:51<01:12,  3.28it/s] 50%|████▉     | 233/470 [01:51<01:11,  3.30it/s] 50%|████▉     | 234/470 [01:51<01:10,  3.33it/s] 50%|█████     | 235/470 [01:52<01:10,  3.35it/s] 50%|█████     | 236/470 [01:52<01:09,  3.36it/s] 50%|█████     | 237/470 [01:52<01:09,  3.37it/s] 51%|█████     | 238/470 [01:53<01:08,  3.37it/s] 51%|█████     | 239/470 [01:53<01:08,  3.38it/s] 51%|█████     | 240/470 [01:53<01:07,  3.38it/s] 51%|█████▏    | 241/470 [01:53<01:07,  3.39it/s] 51%|█████▏    | 242/470 [01:54<01:07,  3.38it/s] 52%|█████▏    | 243/470 [01:54<01:07,  3.38it/s] 52%|█████▏    | 244/470 [01:54<01:07,  3.34it/s] 52%|█████▏    | 245/470 [01:55<01:07,  3.35it/s] 52%|█████▏    | 246/470 [01:55<01:06,  3.36it/s] 53%|█████▎    | 247/470 [01:55<01:07,  3.32it/s] 53%|█████▎    | 248/470 [01:56<01:06,  3.34it/s] 53%|█████▎    | 249/470 [01:56<01:05,  3.36it/s] 53%|█████▎    | 250/470 [01:56<01:05,  3.37it/s] 53%|█████▎    | 251/470 [01:56<01:04,  3.37it/s] 54%|█████▎    | 252/470 [01:57<01:04,  3.38it/s] 54%|█████▍    | 253/470 [01:57<01:04,  3.38it/s] 54%|█████▍    | 254/470 [01:57<01:03,  3.38it/s] 54%|█████▍    | 255/470 [01:58<01:03,  3.38it/s] 54%|█████▍    | 256/470 [01:58<01:03,  3.39it/s] 55%|█████▍    | 257/470 [01:58<01:02,  3.39it/s] 55%|█████▍    | 258/470 [01:58<01:03,  3.35it/s] 55%|█████▌    | 259/470 [01:59<01:02,  3.36it/s] 55%|█████▌    | 260/470 [01:59<01:02,  3.37it/s] 56%|█████▌    | 261/470 [01:59<01:01,  3.38it/s] 56%|█████▌    | 262/470 [02:00<01:01,  3.38it/s] 56%|█████▌    | 263/470 [02:00<01:01,  3.38it/s] 56%|█████▌    | 264/470 [02:00<01:00,  3.38it/s] 56%|█████▋    | 265/470 [02:01<01:00,  3.39it/s] 57%|█████▋    | 266/470 [02:01<01:00,  3.39it/s] 57%|█████▋    | 267/470 [02:01<00:59,  3.39it/s] 57%|█████▋    | 268/470 [02:01<00:59,  3.39it/s] 57%|█████▋    | 269/470 [02:02<01:00,  3.33it/s] 57%|█████▋    | 270/470 [02:02<00:59,  3.35it/s] 58%|█████▊    | 271/470 [02:02<00:59,  3.36it/s] 58%|█████▊    | 272/470 [02:03<00:58,  3.37it/s] 58%|█████▊    | 273/470 [02:03<00:58,  3.38it/s] 58%|█████▊    | 274/470 [02:03<00:57,  3.38it/s] 59%|█████▊    | 275/470 [02:04<00:57,  3.38it/s] 59%|█████▊    | 276/470 [02:04<00:57,  3.39it/s] 59%|█████▉    | 277/470 [02:04<00:56,  3.39it/s] 59%|█████▉    | 278/470 [02:04<00:56,  3.39it/s] 59%|█████▉    | 279/470 [02:05<00:56,  3.39it/s] 60%|█████▉    | 280/470 [02:05<00:56,  3.35it/s] 60%|█████▉    | 281/470 [02:05<00:56,  3.36it/s] 60%|██████    | 282/470 [02:06<00:52,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 22:50:57,725 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:50:57,725 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:50:57,725 >>   Batch size = 8
{'eval_loss': 1.1578086614608765, 'eval_runtime': 9.9686, 'eval_samples_per_second': 350.401, 'eval_steps_per_second': 43.838, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.98it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.72it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.95it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.98it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.65it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.22it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.25it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.11it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.91it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.16it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.30it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.15it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.05it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.04it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.88it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.93it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.91it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.99it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.08it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.12it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.05it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.02it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.93it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.93it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.86it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.88it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.04it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.22it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.15it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.15it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.98it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.96it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.95it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.90it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.88it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.10it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.19it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.15it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.07it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.91it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.99it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.89it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.87it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.84it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.06it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.10it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.10it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.09it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.94it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.82it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.91it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.97it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.90it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.01it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.93it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.00it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.87it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.99it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.95it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.87it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.93it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.91it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.01it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.14it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.98it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.96it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.01it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.89it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.88it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.81it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.02it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.01it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.07it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.04it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.02it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.90it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.83it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.83it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.86it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.98it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.97it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.03it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.90it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.01it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.99it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.89it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.79it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.79it/s][A 60%|██████    | 282/470 [02:15<00:52,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:51:07,684 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-28 22:51:07,711 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:51:11,613 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:51:11,696 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:51:11,742 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [02:28<22:04,  7.08s/it] 60%|██████    | 284/470 [02:29<15:38,  5.05s/it] 61%|██████    | 285/470 [02:29<11:10,  3.62s/it] 61%|██████    | 286/470 [02:29<08:03,  2.63s/it] 61%|██████    | 287/470 [02:30<05:52,  1.93s/it] 61%|██████▏   | 288/470 [02:30<04:21,  1.44s/it] 61%|██████▏   | 289/470 [02:30<03:17,  1.09s/it] 62%|██████▏   | 290/470 [02:31<02:33,  1.17it/s] 62%|██████▏   | 291/470 [02:31<02:02,  1.46it/s] 62%|██████▏   | 292/470 [02:31<01:40,  1.77it/s] 62%|██████▏   | 293/470 [02:31<01:25,  2.07it/s] 63%|██████▎   | 294/470 [02:32<01:14,  2.35it/s] 63%|██████▎   | 295/470 [02:32<01:07,  2.60it/s] 63%|██████▎   | 296/470 [02:32<01:02,  2.79it/s] 63%|██████▎   | 297/470 [02:33<00:58,  2.96it/s] 63%|██████▎   | 298/470 [02:33<00:55,  3.09it/s] 64%|██████▎   | 299/470 [02:33<00:53,  3.18it/s] 64%|██████▍   | 300/470 [02:33<00:52,  3.26it/s] 64%|██████▍   | 301/470 [02:34<00:51,  3.31it/s] 64%|██████▍   | 302/470 [02:34<00:50,  3.35it/s] 64%|██████▍   | 303/470 [02:34<00:49,  3.38it/s] 65%|██████▍   | 304/470 [02:35<00:48,  3.40it/s] 65%|██████▍   | 305/470 [02:35<00:48,  3.41it/s] 65%|██████▌   | 306/470 [02:35<00:47,  3.42it/s] 65%|██████▌   | 307/470 [02:35<00:47,  3.42it/s] 66%|██████▌   | 308/470 [02:36<00:47,  3.43it/s] 66%|██████▌   | 309/470 [02:36<00:46,  3.43it/s] 66%|██████▌   | 310/470 [02:36<00:46,  3.43it/s] 66%|██████▌   | 311/470 [02:37<00:46,  3.44it/s] 66%|██████▋   | 312/470 [02:37<00:45,  3.44it/s] 67%|██████▋   | 313/470 [02:37<00:45,  3.44it/s] 67%|██████▋   | 314/470 [02:38<00:45,  3.44it/s] 67%|██████▋   | 315/470 [02:38<00:45,  3.44it/s] 67%|██████▋   | 316/470 [02:38<00:44,  3.44it/s] 67%|██████▋   | 317/470 [02:38<00:44,  3.44it/s] 68%|██████▊   | 318/470 [02:39<00:44,  3.40it/s] 68%|██████▊   | 319/470 [02:39<00:44,  3.41it/s] 68%|██████▊   | 320/470 [02:39<00:43,  3.43it/s] 68%|██████▊   | 321/470 [02:40<00:43,  3.42it/s] 69%|██████▊   | 322/470 [02:40<00:43,  3.43it/s] 69%|██████▊   | 323/470 [02:40<00:42,  3.43it/s] 69%|██████▉   | 324/470 [02:40<00:42,  3.43it/s] 69%|██████▉   | 325/470 [02:41<00:42,  3.43it/s] 69%|██████▉   | 326/470 [02:41<00:41,  3.44it/s] 70%|██████▉   | 327/470 [02:41<00:41,  3.43it/s] 70%|██████▉   | 328/470 [02:42<00:41,  3.43it/s] 70%|███████   | 329/470 [02:42<00:41,  3.41it/s] 70%|███████   | 330/470 [02:42<00:40,  3.42it/s] 70%|███████   | 331/470 [02:42<00:40,  3.42it/s] 71%|███████   | 332/470 [02:43<00:40,  3.43it/s] 71%|███████   | 333/470 [02:43<00:39,  3.43it/s] 71%|███████   | 334/470 [02:43<00:39,  3.43it/s] 71%|███████▏  | 335/470 [02:44<00:39,  3.43it/s] 71%|███████▏  | 336/470 [02:44<00:39,  3.43it/s] 72%|███████▏  | 337/470 [02:44<00:38,  3.43it/s] 72%|███████▏  | 338/470 [02:45<00:38,  3.43it/s] 72%|███████▏  | 339/470 [02:45<00:38,  3.43it/s] 72%|███████▏  | 340/470 [02:45<00:37,  3.42it/s] 73%|███████▎  | 341/470 [02:45<00:37,  3.43it/s] 73%|███████▎  | 342/470 [02:46<00:37,  3.43it/s] 73%|███████▎  | 343/470 [02:46<00:37,  3.43it/s] 73%|███████▎  | 344/470 [02:46<00:36,  3.42it/s] 73%|███████▎  | 345/470 [02:47<00:36,  3.41it/s] 74%|███████▎  | 346/470 [02:47<00:36,  3.40it/s] 74%|███████▍  | 347/470 [02:47<00:36,  3.40it/s] 74%|███████▍  | 348/470 [02:47<00:35,  3.40it/s] 74%|███████▍  | 349/470 [02:48<00:35,  3.39it/s] 74%|███████▍  | 350/470 [02:48<00:35,  3.39it/s] 75%|███████▍  | 351/470 [02:48<00:35,  3.34it/s] 75%|███████▍  | 352/470 [02:49<00:35,  3.36it/s] 75%|███████▌  | 353/470 [02:49<00:34,  3.37it/s] 75%|███████▌  | 354/470 [02:49<00:34,  3.37it/s] 76%|███████▌  | 355/470 [02:50<00:34,  3.38it/s] 76%|███████▌  | 356/470 [02:50<00:33,  3.38it/s] 76%|███████▌  | 357/470 [02:50<00:33,  3.38it/s] 76%|███████▌  | 358/470 [02:50<00:33,  3.38it/s] 76%|███████▋  | 359/470 [02:51<00:35,  3.14it/s] 77%|███████▋  | 360/470 [02:51<00:34,  3.21it/s] 77%|███████▋  | 361/470 [02:51<00:33,  3.24it/s] 77%|███████▋  | 362/470 [02:52<00:32,  3.28it/s] 77%|███████▋  | 363/470 [02:52<00:32,  3.31it/s] 77%|███████▋  | 364/470 [02:52<00:31,  3.34it/s] 78%|███████▊  | 365/470 [02:53<00:31,  3.35it/s] 78%|███████▊  | 366/470 [02:53<00:30,  3.36it/s] 78%|███████▊  | 367/470 [02:53<00:30,  3.38it/s] 78%|███████▊  | 368/470 [02:53<00:30,  3.40it/s] 79%|███████▊  | 369/470 [02:54<00:29,  3.41it/s] 79%|███████▊  | 370/470 [02:54<00:29,  3.42it/s] 79%|███████▉  | 371/470 [02:54<00:28,  3.42it/s] 79%|███████▉  | 372/470 [02:55<00:28,  3.41it/s] 79%|███████▉  | 373/470 [02:55<00:28,  3.42it/s] 80%|███████▉  | 374/470 [02:55<00:28,  3.43it/s] 80%|███████▉  | 375/470 [02:55<00:27,  3.43it/s] 80%|████████  | 376/470 [02:56<00:25,  3.66it/s][INFO|trainer.py:2140] 2023-08-28 22:51:47,936 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:51:47,936 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:51:47,936 >>   Batch size = 8
{'eval_loss': 1.165108561515808, 'eval_runtime': 9.934, 'eval_samples_per_second': 351.621, 'eval_steps_per_second': 43.99, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.92it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.52it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.90it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.21it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.75it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.42it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.23it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.12it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.14it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.15it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.07it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.09it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.07it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.09it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.95it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.96it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.88it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.95it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.02it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.99it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.02it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.02it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.98it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.92it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.83it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.95it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.03it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.95it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.94it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.99it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.04it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.86it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.86it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.82it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.01it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.09it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.06it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.93it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.95it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.98it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.93it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.93it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.93it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.95it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.03it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.94it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.09it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.00it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.03it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.89it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.85it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.89it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.05it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.98it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.89it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.03it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.08it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.01it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.85it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.95it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.93it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.93it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.89it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.02it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.98it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.04it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.89it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.88it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.93it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.09it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.98it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.93it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.00it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.92it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.01it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.87it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.99it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.96it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.93it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.00it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.04it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.03it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.92it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.84it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.01it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.00it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.00it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.00it/s][A 80%|████████  | 376/470 [03:06<00:25,  3.66it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:51:57,893 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-28 22:51:57,933 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:52:01,017 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:52:01,044 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:52:01,057 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [03:16<09:40,  6.25s/it] 80%|████████  | 378/470 [03:16<06:50,  4.46s/it] 81%|████████  | 379/470 [03:17<04:52,  3.21s/it] 81%|████████  | 380/470 [03:17<03:30,  2.34s/it] 81%|████████  | 381/470 [03:17<02:33,  1.72s/it] 81%|████████▏ | 382/470 [03:17<01:53,  1.30s/it] 81%|████████▏ | 383/470 [03:18<01:26,  1.01it/s] 82%|████████▏ | 384/470 [03:18<01:07,  1.27it/s] 82%|████████▏ | 385/470 [03:18<00:54,  1.57it/s] 82%|████████▏ | 386/470 [03:19<00:44,  1.87it/s] 82%|████████▏ | 387/470 [03:19<00:38,  2.16it/s] 83%|████████▎ | 388/470 [03:19<00:33,  2.43it/s] 83%|████████▎ | 389/470 [03:19<00:30,  2.64it/s] 83%|████████▎ | 390/470 [03:20<00:28,  2.83it/s] 83%|████████▎ | 391/470 [03:20<00:26,  2.98it/s] 83%|████████▎ | 392/470 [03:20<00:25,  3.09it/s] 84%|████████▎ | 393/470 [03:21<00:24,  3.17it/s] 84%|████████▍ | 394/470 [03:21<00:23,  3.24it/s] 84%|████████▍ | 395/470 [03:21<00:22,  3.28it/s] 84%|████████▍ | 396/470 [03:22<00:22,  3.31it/s] 84%|████████▍ | 397/470 [03:22<00:21,  3.33it/s] 85%|████████▍ | 398/470 [03:22<00:21,  3.35it/s] 85%|████████▍ | 399/470 [03:22<00:21,  3.36it/s] 85%|████████▌ | 400/470 [03:23<00:20,  3.34it/s] 85%|████████▌ | 401/470 [03:23<00:20,  3.36it/s] 86%|████████▌ | 402/470 [03:23<00:20,  3.36it/s] 86%|████████▌ | 403/470 [03:24<00:19,  3.37it/s] 86%|████████▌ | 404/470 [03:24<00:19,  3.38it/s] 86%|████████▌ | 405/470 [03:24<00:19,  3.38it/s] 86%|████████▋ | 406/470 [03:24<00:18,  3.39it/s] 87%|████████▋ | 407/470 [03:25<00:18,  3.39it/s] 87%|████████▋ | 408/470 [03:25<00:18,  3.39it/s] 87%|████████▋ | 409/470 [03:25<00:17,  3.39it/s] 87%|████████▋ | 410/470 [03:26<00:17,  3.41it/s] 87%|████████▋ | 411/470 [03:26<00:17,  3.35it/s] 88%|████████▊ | 412/470 [03:26<00:17,  3.37it/s] 88%|████████▊ | 413/470 [03:27<00:16,  3.39it/s] 88%|████████▊ | 414/470 [03:27<00:16,  3.41it/s] 88%|████████▊ | 415/470 [03:27<00:16,  3.41it/s] 89%|████████▊ | 416/470 [03:27<00:15,  3.42it/s] 89%|████████▊ | 417/470 [03:28<00:15,  3.43it/s] 89%|████████▉ | 418/470 [03:28<00:15,  3.43it/s] 89%|████████▉ | 419/470 [03:28<00:14,  3.43it/s] 89%|████████▉ | 420/470 [03:29<00:14,  3.43it/s] 90%|████████▉ | 421/470 [03:29<00:14,  3.43it/s] 90%|████████▉ | 422/470 [03:29<00:13,  3.43it/s] 90%|█████████ | 423/470 [03:29<00:13,  3.43it/s] 90%|█████████ | 424/470 [03:30<00:13,  3.44it/s] 90%|█████████ | 425/470 [03:30<00:13,  3.43it/s] 91%|█████████ | 426/470 [03:30<00:12,  3.44it/s] 91%|█████████ | 427/470 [03:31<00:12,  3.42it/s] 91%|█████████ | 428/470 [03:31<00:12,  3.43it/s] 91%|█████████▏| 429/470 [03:31<00:11,  3.43it/s] 91%|█████████▏| 430/470 [03:31<00:11,  3.43it/s] 92%|█████████▏| 431/470 [03:32<00:11,  3.43it/s] 92%|█████████▏| 432/470 [03:32<00:11,  3.43it/s] 92%|█████████▏| 433/470 [03:32<00:10,  3.43it/s] 92%|█████████▏| 434/470 [03:33<00:10,  3.43it/s] 93%|█████████▎| 435/470 [03:33<00:10,  3.43it/s] 93%|█████████▎| 436/470 [03:33<00:09,  3.44it/s] 93%|█████████▎| 437/470 [03:34<00:09,  3.44it/s] 93%|█████████▎| 438/470 [03:34<00:09,  3.39it/s] 93%|█████████▎| 439/470 [03:34<00:09,  3.41it/s] 94%|█████████▎| 440/470 [03:34<00:08,  3.41it/s] 94%|█████████▍| 441/470 [03:35<00:08,  3.42it/s] 94%|█████████▍| 442/470 [03:35<00:08,  3.43it/s] 94%|█████████▍| 443/470 [03:35<00:07,  3.43it/s] 94%|█████████▍| 444/470 [03:36<00:07,  3.43it/s] 95%|█████████▍| 445/470 [03:36<00:07,  3.43it/s] 95%|█████████▍| 446/470 [03:36<00:06,  3.43it/s] 95%|█████████▌| 447/470 [03:36<00:06,  3.43it/s] 95%|█████████▌| 448/470 [03:37<00:06,  3.43it/s] 96%|█████████▌| 449/470 [03:37<00:06,  3.37it/s] 96%|█████████▌| 450/470 [03:37<00:05,  3.39it/s] 96%|█████████▌| 451/470 [03:38<00:05,  3.40it/s] 96%|█████████▌| 452/470 [03:38<00:05,  3.41it/s] 96%|█████████▋| 453/470 [03:38<00:04,  3.42it/s] 97%|█████████▋| 454/470 [03:39<00:04,  3.42it/s] 97%|█████████▋| 455/470 [03:39<00:04,  3.42it/s] 97%|█████████▋| 456/470 [03:39<00:04,  3.43it/s] 97%|█████████▋| 457/470 [03:39<00:03,  3.43it/s] 97%|█████████▋| 458/470 [03:40<00:03,  3.43it/s] 98%|█████████▊| 459/470 [03:40<00:03,  3.43it/s] 98%|█████████▊| 460/470 [03:40<00:02,  3.41it/s] 98%|█████████▊| 461/470 [03:41<00:02,  3.41it/s] 98%|█████████▊| 462/470 [03:41<00:02,  3.42it/s] 99%|█████████▊| 463/470 [03:41<00:02,  3.42it/s] 99%|█████████▊| 464/470 [03:41<00:01,  3.43it/s] 99%|█████████▉| 465/470 [03:42<00:01,  3.43it/s] 99%|█████████▉| 466/470 [03:42<00:01,  3.43it/s] 99%|█████████▉| 467/470 [03:42<00:00,  3.43it/s]100%|█████████▉| 468/470 [03:43<00:00,  3.43it/s]100%|█████████▉| 469/470 [03:43<00:00,  3.43it/s]100%|██████████| 470/470 [03:43<00:00,  3.67it/s][INFO|trainer.py:2140] 2023-08-28 22:52:35,354 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:52:35,354 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:52:35,354 >>   Batch size = 8
{'eval_loss': 1.1756352186203003, 'eval_runtime': 9.9338, 'eval_samples_per_second': 351.626, 'eval_steps_per_second': 43.991, 'epoch': 4.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.90it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.31it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.57it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.44it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.87it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.57it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.37it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.02it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.97it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.14it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.12it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.08it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.00it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.08it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.91it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.01it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.79it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.86it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.98it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.12it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.02it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.05it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.00it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.87it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.91it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.96it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 40.73it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 41.89it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 42.59it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.19it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.43it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.47it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.55it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.55it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.50it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.63it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.75it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.88it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.15it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.19it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.99it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.98it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.85it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.81it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.77it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.95it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.14it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.11it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.10it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.06it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.96it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.91it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.86it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.82it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.01it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.13it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.01it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.11it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.90it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.82it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.80it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.75it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.83it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.01it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.02it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.73it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.98it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.96it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.90it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.83it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.88it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.98it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.04it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.96it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.10it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.98it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.93it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.95it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.79it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.96it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.94it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.09it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.11it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.06it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.08it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.00it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.98it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.98it/s][A100%|██████████| 470/470 [03:53<00:00,  3.67it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:52:45,342 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-28 22:52:45,383 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:52:48,051 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:52:48,075 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:52:48,091 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 22:52:54,543 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 22:52:54,546 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94 (score: 1.13685142993927).
                                                 100%|██████████| 470/470 [04:06<00:00,  3.67it/s]100%|██████████| 470/470 [04:06<00:00,  1.91it/s]
[INFO|trainer.py:1894] 2023-08-28 22:52:57,789 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 22:52:57,810 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:53:00,591 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:53:00,615 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:53:00,639 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:53:01,040 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:01,043 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:01,043 >>   train_loss               =     0.4212
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:01,043 >>   train_runtime            = 0:04:06.07
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:01,043 >>   train_samples            =       6000
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:01,043 >>   train_samples_per_second =    121.914
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:01,043 >>   train_steps_per_second   =       1.91
{'eval_loss': 1.1821391582489014, 'eval_runtime': 9.96, 'eval_samples_per_second': 350.705, 'eval_steps_per_second': 43.876, 'epoch': 5.0}
{'train_runtime': 246.0753, 'train_samples_per_second': 121.914, 'train_steps_per_second': 1.91, 'train_loss': 0.4212317284117354, 'epoch': 5.0}
08/28/2023 22:53:01 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 22:53:01,104 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:53:01,104 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:53:01,104 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 56.08it/s]  3%|▎         | 12/437 [00:00<00:08, 48.84it/s]  4%|▍         | 17/437 [00:00<00:08, 47.03it/s]  5%|▌         | 22/437 [00:00<00:08, 46.17it/s]  6%|▌         | 27/437 [00:00<00:08, 45.79it/s]  7%|▋         | 32/437 [00:00<00:08, 45.50it/s]  8%|▊         | 37/437 [00:00<00:08, 45.40it/s] 10%|▉         | 42/437 [00:00<00:08, 44.88it/s] 11%|█         | 47/437 [00:01<00:08, 44.29it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.97it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.14it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.22it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.30it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.55it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.63it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.64it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.42it/s] 21%|██        | 92/437 [00:02<00:07, 44.06it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.97it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.09it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.10it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.25it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.13it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.33it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.40it/s] 30%|███       | 132/437 [00:02<00:06, 44.22it/s] 31%|███▏      | 137/437 [00:03<00:06, 43.83it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.83it/s] 34%|███▎      | 147/437 [00:03<00:06, 43.88it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.01it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.16it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.40it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.48it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.47it/s] 41%|████      | 177/437 [00:03<00:05, 44.27it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.07it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.91it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.00it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.12it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.26it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.40it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.44it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.34it/s] 51%|█████     | 222/437 [00:04<00:04, 44.33it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.09it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.05it/s] 54%|█████▍    | 237/437 [00:05<00:04, 43.96it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.02it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.37it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.46it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.47it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.21it/s] 61%|██████    | 267/437 [00:06<00:03, 44.23it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.04it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.01it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.12it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.15it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.33it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.44it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.39it/s] 70%|███████   | 307/437 [00:06<00:02, 44.33it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.17it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.00it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.06it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.14it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.30it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.38it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.37it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.33it/s] 81%|████████  | 352/437 [00:07<00:01, 44.34it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.05it/s] 83%|████████▎ | 362/437 [00:08<00:01, 43.98it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.00it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.16it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.29it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.41it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.38it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.35it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.16it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.11it/s] 93%|█████████▎| 407/437 [00:09<00:00, 43.98it/s] 94%|█████████▍| 412/437 [00:09<00:00, 43.94it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.07it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.21it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.22it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.37it/s]100%|██████████| 437/437 [00:09<00:00, 44.40it/s]100%|██████████| 437/437 [00:09<00:00, 44.34it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:53:10,979 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:10,979 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:10,979 >>   eval_loss               =     1.1369
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:10,979 >>   eval_runtime            = 0:00:09.87
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:10,979 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:10,979 >>   eval_samples_per_second =    353.736
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:10,979 >>   eval_steps_per_second   =     44.255
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:53:10,979 >>   perplexity              =     3.1169
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:18,669 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:18,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:18,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:18,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:18,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:53:19,439 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:53:19,440 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:53:20,082 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:53:21,143 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:53:21,143 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:28,013 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:28,034 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:28,035 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:28,035 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:28,035 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:53:29,260 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:53:29,261 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:53:30,231 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:53:30,393 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:53:30,393 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-376
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-282
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-470
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.50it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:06,  1.54it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:09,  1.59it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:11,  1.59it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.58it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.59it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.62it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.60it/s]Extractor Predicting: 33it [00:21,  1.57it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.47it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.60it/s]Extractor Predicting: 43it [00:27,  1.58it/s]Extractor Predicting: 44it [00:28,  1.61it/s]Extractor Predicting: 45it [00:28,  1.58it/s]Extractor Predicting: 46it [00:29,  1.57it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:30,  1.63it/s]Extractor Predicting: 49it [00:31,  1.65it/s]Extractor Predicting: 50it [00:32,  1.61it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:33,  1.65it/s]Extractor Predicting: 54it [00:34,  1.65it/s]Extractor Predicting: 55it [00:35,  1.64it/s]Extractor Predicting: 56it [00:35,  1.61it/s]Extractor Predicting: 57it [00:36,  1.63it/s]Extractor Predicting: 58it [00:37,  1.60it/s]Extractor Predicting: 59it [00:37,  1.58it/s]Extractor Predicting: 60it [00:38,  1.54it/s]Extractor Predicting: 61it [00:39,  1.55it/s]Extractor Predicting: 62it [00:39,  1.55it/s]Extractor Predicting: 63it [00:40,  1.55it/s]Extractor Predicting: 64it [00:40,  1.56it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:42,  1.53it/s]Extractor Predicting: 68it [00:43,  1.50it/s]Extractor Predicting: 69it [00:44,  1.52it/s]Extractor Predicting: 70it [00:44,  1.53it/s]Extractor Predicting: 71it [00:45,  1.54it/s]Extractor Predicting: 72it [00:46,  1.58it/s]Extractor Predicting: 73it [00:46,  1.56it/s]Extractor Predicting: 74it [00:47,  1.53it/s]Extractor Predicting: 75it [00:48,  1.56it/s]Extractor Predicting: 76it [00:48,  1.53it/s]Extractor Predicting: 77it [00:49,  1.53it/s]Extractor Predicting: 78it [00:50,  1.51it/s]Extractor Predicting: 79it [00:50,  1.52it/s]Extractor Predicting: 80it [00:51,  1.52it/s]Extractor Predicting: 81it [00:52,  1.51it/s]Extractor Predicting: 82it [00:52,  1.52it/s]Extractor Predicting: 83it [00:53,  1.56it/s]Extractor Predicting: 84it [00:53,  1.56it/s]Extractor Predicting: 85it [00:54,  1.55it/s]Extractor Predicting: 86it [00:55,  1.49it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:56,  1.53it/s]Extractor Predicting: 89it [00:57,  1.50it/s]Extractor Predicting: 90it [00:58,  1.47it/s]Extractor Predicting: 91it [00:58,  1.48it/s]Extractor Predicting: 92it [00:59,  1.50it/s]Extractor Predicting: 93it [01:00,  1.49it/s]Extractor Predicting: 94it [01:00,  1.52it/s]Extractor Predicting: 95it [01:01,  1.57it/s]Extractor Predicting: 96it [01:01,  1.58it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:03,  1.56it/s]Extractor Predicting: 99it [01:03,  1.54it/s]Extractor Predicting: 100it [01:04,  1.57it/s]Extractor Predicting: 101it [01:05,  1.56it/s]Extractor Predicting: 102it [01:05,  1.55it/s]Extractor Predicting: 103it [01:06,  1.55it/s]Extractor Predicting: 104it [01:07,  1.55it/s]Extractor Predicting: 105it [01:07,  1.56it/s]Extractor Predicting: 106it [01:08,  1.54it/s]Extractor Predicting: 107it [01:08,  1.54it/s]Extractor Predicting: 108it [01:09,  1.54it/s]Extractor Predicting: 109it [01:10,  1.54it/s]Extractor Predicting: 110it [01:10,  1.53it/s]Extractor Predicting: 111it [01:11,  1.53it/s]Extractor Predicting: 112it [01:12,  1.53it/s]Extractor Predicting: 113it [01:12,  1.51it/s]Extractor Predicting: 114it [01:13,  1.50it/s]Extractor Predicting: 115it [01:14,  1.48it/s]Extractor Predicting: 116it [01:14,  1.50it/s]Extractor Predicting: 117it [01:15,  1.37it/s]Extractor Predicting: 118it [01:16,  1.40it/s]Extractor Predicting: 119it [01:17,  1.43it/s]Extractor Predicting: 120it [01:17,  1.46it/s]Extractor Predicting: 121it [01:18,  1.46it/s]Extractor Predicting: 122it [01:19,  1.46it/s]Extractor Predicting: 123it [01:19,  1.52it/s]Extractor Predicting: 124it [01:20,  1.51it/s]Extractor Predicting: 125it [01:21,  1.48it/s]Extractor Predicting: 126it [01:21,  1.49it/s]Extractor Predicting: 127it [01:22,  1.50it/s]Extractor Predicting: 128it [01:23,  1.50it/s]Extractor Predicting: 129it [01:23,  1.49it/s]Extractor Predicting: 130it [01:24,  1.48it/s]Extractor Predicting: 131it [01:25,  1.45it/s]Extractor Predicting: 132it [01:25,  1.44it/s]Extractor Predicting: 133it [01:26,  1.45it/s]Extractor Predicting: 134it [01:27,  1.43it/s]Extractor Predicting: 135it [01:28,  1.45it/s]Extractor Predicting: 136it [01:28,  1.77it/s]Extractor Predicting: 136it [01:28,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:06,974 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:06,979 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:06,979 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:06,979 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:06,979 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:55:07,287 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:55:07,288 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:55:07,961 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:55:08,993 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:55:08,994 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:10,800 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:10,806 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:10,806 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:10,806 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:55:10,806 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:55:11,528 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:55:11,529 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:55:11,798 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:55:11,950 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:55:11,951 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.43285446595877575,
  "recall": 0.19839679358717435,
  "score": 0.27208480565371024,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.67it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:06,  1.57it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:13,  1.53it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:16,  1.62it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:20,  1.71it/s]Extractor Predicting: 34it [00:20,  1.70it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:22,  1.64it/s]Extractor Predicting: 37it [00:22,  1.62it/s]Extractor Predicting: 38it [00:23,  1.59it/s]Extractor Predicting: 39it [00:24,  1.62it/s]Extractor Predicting: 40it [00:24,  1.68it/s]Extractor Predicting: 41it [00:25,  1.67it/s]Extractor Predicting: 42it [00:25,  1.67it/s]Extractor Predicting: 43it [00:26,  1.70it/s]Extractor Predicting: 44it [00:27,  1.68it/s]Extractor Predicting: 45it [00:27,  1.65it/s]Extractor Predicting: 46it [00:28,  1.67it/s]Extractor Predicting: 47it [00:28,  1.68it/s]Extractor Predicting: 48it [00:29,  1.67it/s]Extractor Predicting: 49it [00:30,  1.68it/s]Extractor Predicting: 50it [00:30,  1.69it/s]Extractor Predicting: 51it [00:31,  1.72it/s]Extractor Predicting: 52it [00:31,  1.73it/s]Extractor Predicting: 53it [00:32,  1.68it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:33,  1.67it/s]Extractor Predicting: 56it [00:34,  1.63it/s]Extractor Predicting: 57it [00:34,  1.63it/s]Extractor Predicting: 58it [00:35,  1.65it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:36,  1.62it/s]Extractor Predicting: 61it [00:37,  1.64it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:38,  1.60it/s]Extractor Predicting: 64it [00:39,  1.66it/s]Extractor Predicting: 65it [00:39,  1.65it/s]Extractor Predicting: 66it [00:40,  1.63it/s]Extractor Predicting: 67it [00:40,  1.64it/s]Extractor Predicting: 68it [00:41,  1.68it/s]Extractor Predicting: 69it [00:42,  1.70it/s]Extractor Predicting: 70it [00:42,  1.68it/s]Extractor Predicting: 71it [00:43,  1.68it/s]Extractor Predicting: 72it [00:43,  1.65it/s]Extractor Predicting: 73it [00:44,  1.67it/s]Extractor Predicting: 74it [00:45,  1.63it/s]Extractor Predicting: 75it [00:45,  1.63it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:46,  1.63it/s]Extractor Predicting: 78it [00:47,  1.63it/s]Extractor Predicting: 79it [00:48,  1.63it/s]Extractor Predicting: 80it [00:48,  1.65it/s]Extractor Predicting: 81it [00:49,  1.65it/s]Extractor Predicting: 82it [00:49,  1.66it/s]Extractor Predicting: 83it [00:50,  1.61it/s]Extractor Predicting: 84it [00:51,  1.62it/s]Extractor Predicting: 85it [00:51,  1.59it/s]Extractor Predicting: 86it [00:52,  1.57it/s]Extractor Predicting: 87it [00:53,  1.62it/s]Extractor Predicting: 88it [00:53,  1.58it/s]Extractor Predicting: 89it [00:54,  1.56it/s]Extractor Predicting: 90it [00:55,  1.54it/s]Extractor Predicting: 91it [00:55,  1.54it/s]Extractor Predicting: 92it [00:56,  1.53it/s]Extractor Predicting: 93it [00:57,  1.51it/s]Extractor Predicting: 94it [00:57,  1.53it/s]Extractor Predicting: 95it [00:58,  1.56it/s]Extractor Predicting: 96it [00:59,  1.56it/s]Extractor Predicting: 97it [00:59,  1.54it/s]Extractor Predicting: 98it [01:00,  1.53it/s]Extractor Predicting: 99it [01:01,  1.52it/s]Extractor Predicting: 100it [01:01,  1.51it/s]Extractor Predicting: 101it [01:02,  1.51it/s]Extractor Predicting: 102it [01:03,  1.50it/s]Extractor Predicting: 103it [01:03,  1.54it/s]Extractor Predicting: 104it [01:04,  1.53it/s]Extractor Predicting: 105it [01:04,  1.55it/s]Extractor Predicting: 106it [01:05,  1.51it/s]Extractor Predicting: 107it [01:06,  1.51it/s]Extractor Predicting: 108it [01:07,  1.49it/s]Extractor Predicting: 109it [01:07,  1.49it/s]Extractor Predicting: 110it [01:08,  1.49it/s]Extractor Predicting: 111it [01:08,  1.51it/s]Extractor Predicting: 112it [01:09,  1.50it/s]Extractor Predicting: 113it [01:10,  1.49it/s]Extractor Predicting: 114it [01:11,  1.49it/s]Extractor Predicting: 115it [01:11,  1.49it/s]Extractor Predicting: 116it [01:12,  1.50it/s]Extractor Predicting: 117it [01:13,  1.51it/s]Extractor Predicting: 118it [01:13,  1.52it/s]Extractor Predicting: 119it [01:14,  1.54it/s]Extractor Predicting: 120it [01:15,  1.44it/s]Extractor Predicting: 121it [01:15,  1.47it/s]Extractor Predicting: 122it [01:16,  1.48it/s]Extractor Predicting: 123it [01:17,  1.50it/s]Extractor Predicting: 124it [01:17,  1.50it/s]Extractor Predicting: 125it [01:18,  1.53it/s]Extractor Predicting: 126it [01:18,  1.57it/s]Extractor Predicting: 127it [01:19,  1.58it/s]Extractor Predicting: 128it [01:20,  1.61it/s]Extractor Predicting: 129it [01:20,  1.59it/s]Extractor Predicting: 130it [01:21,  1.55it/s]Extractor Predicting: 131it [01:22,  1.56it/s]Extractor Predicting: 132it [01:22,  1.57it/s]Extractor Predicting: 133it [01:23,  1.59it/s]Extractor Predicting: 134it [01:23,  1.62it/s]Extractor Predicting: 135it [01:24,  1.59it/s]Extractor Predicting: 136it [01:25,  1.61it/s]Extractor Predicting: 137it [01:25,  1.63it/s]Extractor Predicting: 138it [01:26,  1.60it/s]Extractor Predicting: 139it [01:27,  1.61it/s]Extractor Predicting: 140it [01:27,  1.61it/s]Extractor Predicting: 141it [01:28,  1.55it/s]Extractor Predicting: 142it [01:29,  1.54it/s]Extractor Predicting: 143it [01:29,  1.55it/s]Extractor Predicting: 144it [01:30,  1.55it/s]Extractor Predicting: 145it [01:30,  1.53it/s]Extractor Predicting: 146it [01:31,  1.56it/s]Extractor Predicting: 147it [01:32,  1.57it/s]Extractor Predicting: 148it [01:32,  1.56it/s]Extractor Predicting: 149it [01:33,  1.56it/s]Extractor Predicting: 150it [01:34,  1.56it/s]Extractor Predicting: 151it [01:34,  1.51it/s]Extractor Predicting: 152it [01:35,  1.52it/s]Extractor Predicting: 153it [01:36,  1.53it/s]Extractor Predicting: 154it [01:36,  1.57it/s]Extractor Predicting: 155it [01:37,  1.53it/s]Extractor Predicting: 156it [01:38,  1.53it/s]Extractor Predicting: 157it [01:38,  1.51it/s]Extractor Predicting: 158it [01:39,  1.53it/s]Extractor Predicting: 159it [01:40,  1.51it/s]Extractor Predicting: 160it [01:40,  1.50it/s]Extractor Predicting: 161it [01:41,  1.51it/s]Extractor Predicting: 162it [01:42,  1.51it/s]Extractor Predicting: 163it [01:42,  1.51it/s]Extractor Predicting: 164it [01:43,  1.52it/s]Extractor Predicting: 165it [01:44,  1.52it/s]Extractor Predicting: 166it [01:44,  1.53it/s]Extractor Predicting: 167it [01:45,  1.51it/s]Extractor Predicting: 168it [01:46,  1.49it/s]Extractor Predicting: 169it [01:46,  1.50it/s]Extractor Predicting: 170it [01:47,  1.49it/s]Extractor Predicting: 171it [01:48,  1.50it/s]Extractor Predicting: 172it [01:48,  1.50it/s]Extractor Predicting: 173it [01:49,  1.50it/s]Extractor Predicting: 174it [01:50,  1.50it/s]Extractor Predicting: 175it [01:50,  1.55it/s]Extractor Predicting: 176it [01:51,  1.54it/s]Extractor Predicting: 177it [01:51,  1.57it/s]Extractor Predicting: 178it [01:52,  1.56it/s]Extractor Predicting: 179it [01:53,  1.59it/s]Extractor Predicting: 180it [01:53,  1.58it/s]Extractor Predicting: 181it [01:54,  1.59it/s]Extractor Predicting: 182it [01:55,  1.61it/s]Extractor Predicting: 183it [01:55,  1.55it/s]Extractor Predicting: 184it [01:56,  1.62it/s]Extractor Predicting: 185it [01:56,  1.60it/s]Extractor Predicting: 186it [01:57,  1.61it/s]Extractor Predicting: 187it [01:58,  1.62it/s]Extractor Predicting: 188it [01:58,  1.56it/s]Extractor Predicting: 189it [01:59,  1.56it/s]Extractor Predicting: 190it [02:00,  1.57it/s]Extractor Predicting: 191it [02:00,  1.55it/s]Extractor Predicting: 192it [02:01,  1.56it/s]Extractor Predicting: 193it [02:02,  1.58it/s]Extractor Predicting: 194it [02:02,  1.58it/s]Extractor Predicting: 195it [02:03,  1.56it/s]Extractor Predicting: 196it [02:04,  1.52it/s]Extractor Predicting: 197it [02:04,  1.55it/s]Extractor Predicting: 198it [02:05,  1.53it/s]Extractor Predicting: 199it [02:05,  1.56it/s]Extractor Predicting: 200it [02:06,  1.55it/s]Extractor Predicting: 201it [02:07,  1.58it/s]Extractor Predicting: 202it [02:07,  1.58it/s]Extractor Predicting: 203it [02:08,  1.58it/s]Extractor Predicting: 204it [02:09,  1.61it/s]Extractor Predicting: 205it [02:09,  1.59it/s]Extractor Predicting: 206it [02:10,  1.62it/s]Extractor Predicting: 207it [02:10,  1.59it/s]Extractor Predicting: 208it [02:11,  1.57it/s]Extractor Predicting: 209it [02:12,  1.59it/s]Extractor Predicting: 210it [02:12,  1.59it/s]Extractor Predicting: 211it [02:13,  1.58it/s]Extractor Predicting: 212it [02:14,  1.59it/s]Extractor Predicting: 213it [02:14,  1.61it/s]Extractor Predicting: 214it [02:15,  1.59it/s]Extractor Predicting: 215it [02:15,  1.60it/s]Extractor Predicting: 216it [02:16,  1.59it/s]Extractor Predicting: 217it [02:17,  1.57it/s]Extractor Predicting: 218it [02:17,  1.59it/s]Extractor Predicting: 219it [02:18,  1.56it/s]Extractor Predicting: 220it [02:19,  1.58it/s]Extractor Predicting: 221it [02:19,  1.60it/s]Extractor Predicting: 222it [02:20,  1.60it/s]Extractor Predicting: 223it [02:21,  1.40it/s]Extractor Predicting: 224it [02:21,  1.48it/s]Extractor Predicting: 225it [02:22,  1.49it/s]Extractor Predicting: 226it [02:23,  1.50it/s]Extractor Predicting: 227it [02:23,  1.52it/s]Extractor Predicting: 228it [02:24,  1.55it/s]Extractor Predicting: 229it [02:25,  1.56it/s]Extractor Predicting: 230it [02:25,  1.58it/s]Extractor Predicting: 231it [02:26,  1.59it/s]Extractor Predicting: 232it [02:26,  1.58it/s]Extractor Predicting: 233it [02:27,  1.59it/s]Extractor Predicting: 234it [02:28,  1.58it/s]Extractor Predicting: 235it [02:28,  1.55it/s]Extractor Predicting: 236it [02:29,  1.55it/s]Extractor Predicting: 237it [02:30,  1.55it/s]Extractor Predicting: 238it [02:30,  1.55it/s]Extractor Predicting: 239it [02:31,  1.53it/s]Extractor Predicting: 240it [02:32,  1.54it/s]Extractor Predicting: 241it [02:32,  1.54it/s]Extractor Predicting: 242it [02:33,  1.54it/s]Extractor Predicting: 243it [02:34,  1.55it/s]Extractor Predicting: 244it [02:34,  1.53it/s]Extractor Predicting: 245it [02:35,  1.55it/s]Extractor Predicting: 246it [02:36,  1.55it/s]Extractor Predicting: 247it [02:36,  1.56it/s]Extractor Predicting: 248it [02:37,  1.55it/s]Extractor Predicting: 249it [02:37,  1.58it/s]Extractor Predicting: 250it [02:38,  1.57it/s]Extractor Predicting: 251it [02:39,  1.60it/s]Extractor Predicting: 252it [02:39,  1.58it/s]Extractor Predicting: 253it [02:40,  1.57it/s]Extractor Predicting: 254it [02:41,  1.57it/s]Extractor Predicting: 255it [02:41,  1.57it/s]Extractor Predicting: 256it [02:42,  1.53it/s]Extractor Predicting: 257it [02:43,  1.54it/s]Extractor Predicting: 258it [02:43,  1.54it/s]Extractor Predicting: 259it [02:44,  1.59it/s]Extractor Predicting: 260it [02:44,  1.57it/s]Extractor Predicting: 261it [02:45,  1.59it/s]Extractor Predicting: 262it [02:46,  1.57it/s]Extractor Predicting: 263it [02:46,  1.56it/s]Extractor Predicting: 264it [02:47,  1.61it/s]Extractor Predicting: 265it [02:48,  1.61it/s]Extractor Predicting: 266it [02:48,  1.62it/s]Extractor Predicting: 267it [02:49,  1.61it/s]Extractor Predicting: 268it [02:49,  1.59it/s]Extractor Predicting: 269it [02:50,  1.57it/s]Extractor Predicting: 270it [02:51,  1.55it/s]Extractor Predicting: 271it [02:51,  1.53it/s]Extractor Predicting: 272it [02:52,  1.54it/s]Extractor Predicting: 273it [02:53,  1.53it/s]Extractor Predicting: 274it [02:53,  1.50it/s]Extractor Predicting: 275it [02:54,  1.48it/s]Extractor Predicting: 276it [02:55,  1.51it/s]Extractor Predicting: 277it [02:55,  1.51it/s]Extractor Predicting: 278it [02:56,  1.55it/s]Extractor Predicting: 279it [02:57,  1.54it/s]Extractor Predicting: 280it [02:57,  1.54it/s]Extractor Predicting: 281it [02:58,  1.57it/s]Extractor Predicting: 282it [02:59,  1.56it/s]Extractor Predicting: 283it [02:59,  1.54it/s]Extractor Predicting: 284it [03:00,  1.54it/s]Extractor Predicting: 285it [03:01,  1.54it/s]Extractor Predicting: 286it [03:01,  1.58it/s]Extractor Predicting: 287it [03:02,  1.54it/s]Extractor Predicting: 288it [03:02,  1.76it/s]Extractor Predicting: 288it [03:02,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:21,596 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:21,603 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:21,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:21,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:21,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:58:22,361 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:58:22,363 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:58:22,647 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:58:23,699 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:58:23,699 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:25,096 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:25,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:25,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:25,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:25,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:58:25,460 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:58:25,461 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:58:25,727 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:58:25,891 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:58:25,891 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3422539399345822,
  "recall": 0.16681159420289854,
  "score": 0.22430088668030787,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:01,  2.34it/s]Extractor Predicting: 3it [00:01,  2.01it/s]
[INFO|configuration_utils.py:515] 2023-08-28 22:58:27,808 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:58:27,809 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:58:27,813 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:58:27,814 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 22:58:27,819 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:58:32,273 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 22:58:32,277 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 22:58:32,292 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:58:32,293 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:58:32,300 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:58:32,305 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:58:32,305 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:58:32,305 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:58:32,305 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:58:32,305 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:58:32,306 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5263157894736842,
  "recall": 0.1,
  "score": 0.16806722689075632,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 22:58:32,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:33,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:33,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:34,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:35,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:35,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:36,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:36,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:37,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:38,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:38,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:39,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:40,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:40,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:41,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:42,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:42,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:43,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:44,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:44,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<03:00, 12.89s/it][WARNING|generation_utils.py:914] 2023-08-28 22:58:45,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:46,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:46,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:47,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:47,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:48,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:49,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:49,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:50,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:51,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:51,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:52,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:52,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:53,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:54,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:54,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:55,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:55,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:56,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:57,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:57,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:58,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:53, 13.36s/it][WARNING|generation_utils.py:914] 2023-08-28 22:58:59,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:58:59,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:00,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:01,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:01,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:02,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:02,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:03,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:04,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:04,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:05,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:05,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:06,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:07,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:07,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:08,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:08,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:09,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:10,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:10,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:11,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:37, 13.11s/it][WARNING|generation_utils.py:914] 2023-08-28 22:59:11,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:12,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:13,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:13,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:14,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:15,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:15,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:16,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:16,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:17,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:18,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:18,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:19,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:19,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:20,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:21,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:21,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:22,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:23,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:23,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:24,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:52<02:24, 13.14s/it][WARNING|generation_utils.py:914] 2023-08-28 22:59:25,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:25,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:26,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:27,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:27,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:28,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:29,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:30,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:30,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:31,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:32,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:32,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:33,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:33,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:34,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:35,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:35,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:36,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:37,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:37,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:05<02:11, 13.12s/it][WARNING|generation_utils.py:914] 2023-08-28 22:59:38,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:38,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:39,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:40,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:41,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:41,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:42,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:43,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:43,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:44,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:44,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:45,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:45,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:46,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:47,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:47,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:48,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:49,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:49,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:50,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:18<01:57, 13.04s/it][WARNING|generation_utils.py:914] 2023-08-28 22:59:51,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:51,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:52,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:52,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:53,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:54,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:54,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:55,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:56,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:56,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:57,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:58,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:58,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:59:59,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:00,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:00,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:01,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:02,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:02,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:03,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:31<01:43, 12.98s/it][WARNING|generation_utils.py:914] 2023-08-28 23:00:03,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:04,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:05,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:05,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:06,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:06,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:07,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:08,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:08,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:09,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:09,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:10,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:11,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:11,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:12,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:13,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:14,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:14,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:15,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:15,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:16,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:44<01:31, 13.02s/it][WARNING|generation_utils.py:914] 2023-08-28 23:00:17,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:17,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:18,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:18,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:19,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:20,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:21,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:21,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:22,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:23,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:23,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:24,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:24,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:25,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:26,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:26,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:27,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:28,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:28,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:29,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:57<01:18, 13.06s/it][WARNING|generation_utils.py:914] 2023-08-28 23:00:30,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:30,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:31,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:31,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:32,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:33,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:34,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:34,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:35,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:35,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:36,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:37,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:37,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:38,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:38,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:39,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:39,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:40,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:41,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:41,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:09<01:03, 12.75s/it][WARNING|generation_utils.py:914] 2023-08-28 23:00:42,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:42,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:43,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:44,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:44,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:45,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:45,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:46,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:47,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:47,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:48,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:48,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:49,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:50,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:50,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:51,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:51,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:52,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:53,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:54,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:55,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:23<00:51, 12.97s/it][WARNING|generation_utils.py:914] 2023-08-28 23:00:55,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:56,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:56,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:57,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:58,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:58,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:59,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:00:59,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:00,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:01,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:01,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:02,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:03,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:03,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:04,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:04,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:05,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:06,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:06,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:07,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:07,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:36<00:38, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-28 23:01:08,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:09,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:09,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:10,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:11,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:11,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:12,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:12,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:13,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:13,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:14,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:15,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:15,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:16,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:16,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:17,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:18,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:18,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:19,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:19,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:47<00:25, 12.59s/it][WARNING|generation_utils.py:914] 2023-08-28 23:01:20,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:20,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:21,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:22,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:22,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:23,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:23,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:24,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:24,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:25,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:26,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:26,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:27,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:27,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:28,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:29,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:29,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:30,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:30,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:31,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:31,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:59<00:12, 12.44s/it][WARNING|generation_utils.py:914] 2023-08-28 23:01:32,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:33,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:33,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:34,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:34,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:35,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:36,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:37,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:37,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:38,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:39,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:39,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:40,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:41,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:41,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:42,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:42,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:43,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:44,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:01:44,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:12<00:00, 12.57s/it]Generating: 100%|██████████| 15/15 [03:12<00:00, 12.85s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:51,146 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:51,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:51,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:51,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:51,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:01:52,226 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:01:52,227 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:01:52,890 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:01:53,954 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:01:53,954 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:59,032 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:59,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:59,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:59,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:01:59,043 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:02:00,312 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:02:00,313 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:02:01,036 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:02:01,221 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:02:01,221 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 410, 'raw': 416}
{'target': 600, 'success': 442, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 562, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 623, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9734375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9640625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 7741
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7841, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.43it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.40it/s]Extractor Estimating: 5it [00:03,  1.34it/s]Extractor Estimating: 6it [00:04,  1.38it/s]Extractor Estimating: 7it [00:04,  1.47it/s]Extractor Estimating: 8it [00:05,  1.44it/s]Extractor Estimating: 9it [00:06,  1.47it/s]Extractor Estimating: 10it [00:06,  1.44it/s]Extractor Estimating: 11it [00:07,  1.42it/s]Extractor Estimating: 12it [00:08,  1.44it/s]Extractor Estimating: 13it [00:09,  1.43it/s]Extractor Estimating: 14it [00:09,  1.48it/s]Extractor Estimating: 15it [00:10,  1.47it/s]Extractor Estimating: 16it [00:11,  1.46it/s]Extractor Estimating: 17it [00:11,  1.45it/s]Extractor Estimating: 18it [00:12,  1.47it/s]Extractor Estimating: 19it [00:13,  1.40it/s]Extractor Estimating: 20it [00:13,  1.40it/s]Extractor Estimating: 21it [00:14,  1.43it/s]Extractor Estimating: 22it [00:15,  1.44it/s]Extractor Estimating: 23it [00:15,  1.47it/s]Extractor Estimating: 24it [00:16,  1.50it/s]Extractor Estimating: 25it [00:17,  1.49it/s]Extractor Estimating: 26it [00:17,  1.47it/s]Extractor Estimating: 27it [00:18,  1.52it/s]Extractor Estimating: 28it [00:19,  1.52it/s]Extractor Estimating: 29it [00:19,  1.50it/s]Extractor Estimating: 30it [00:20,  1.54it/s]Extractor Estimating: 31it [00:21,  1.54it/s]Extractor Estimating: 32it [00:21,  1.52it/s]Extractor Estimating: 33it [00:22,  1.56it/s]Extractor Estimating: 34it [00:23,  1.58it/s]Extractor Estimating: 35it [00:23,  1.61it/s]Extractor Estimating: 36it [00:24,  1.58it/s]Extractor Estimating: 37it [00:24,  1.59it/s]Extractor Estimating: 38it [00:25,  1.59it/s]Extractor Estimating: 39it [00:26,  1.55it/s]Extractor Estimating: 40it [00:26,  1.58it/s]Extractor Estimating: 41it [00:27,  1.52it/s]Extractor Estimating: 42it [00:28,  1.52it/s]Extractor Estimating: 43it [00:28,  1.51it/s]Extractor Estimating: 44it [00:29,  1.55it/s]Extractor Estimating: 45it [00:30,  1.58it/s]Extractor Estimating: 46it [00:30,  1.57it/s]Extractor Estimating: 47it [00:31,  1.58it/s]Extractor Estimating: 48it [00:32,  1.52it/s]Extractor Estimating: 49it [00:32,  1.55it/s]Extractor Estimating: 50it [00:33,  1.55it/s]Extractor Estimating: 51it [00:34,  1.51it/s]Extractor Estimating: 52it [00:34,  1.52it/s]Extractor Estimating: 53it [00:35,  1.55it/s]Extractor Estimating: 54it [00:35,  1.58it/s]Extractor Estimating: 55it [00:36,  1.63it/s]Extractor Estimating: 56it [00:37,  1.66it/s]Extractor Estimating: 57it [00:37,  1.61it/s]Extractor Estimating: 58it [00:38,  1.60it/s]Extractor Estimating: 59it [00:39,  1.57it/s]Extractor Estimating: 60it [00:39,  1.50it/s]Extractor Estimating: 61it [00:40,  1.53it/s]Extractor Estimating: 62it [00:41,  1.50it/s]Extractor Estimating: 63it [00:41,  1.55it/s]Extractor Estimating: 64it [00:42,  1.57it/s]Extractor Estimating: 65it [00:43,  1.51it/s]Extractor Estimating: 66it [00:43,  1.52it/s]Extractor Estimating: 67it [00:44,  1.49it/s]Extractor Estimating: 68it [00:45,  1.48it/s]Extractor Estimating: 69it [00:45,  1.52it/s]Extractor Estimating: 70it [00:46,  1.56it/s]Extractor Estimating: 71it [00:46,  1.59it/s]Extractor Estimating: 72it [00:47,  1.58it/s]Extractor Estimating: 73it [00:48,  1.54it/s]Extractor Estimating: 74it [00:48,  1.57it/s]Extractor Estimating: 75it [00:49,  1.54it/s]Extractor Estimating: 76it [00:50,  1.60it/s]Extractor Estimating: 77it [00:50,  1.58it/s]Extractor Estimating: 78it [00:51,  1.61it/s]Extractor Estimating: 79it [00:51,  1.63it/s]Extractor Estimating: 80it [00:52,  1.64it/s]Extractor Estimating: 81it [00:53,  1.66it/s]Extractor Estimating: 82it [00:53,  1.67it/s]Extractor Estimating: 83it [00:54,  1.66it/s]Extractor Estimating: 84it [00:54,  1.73it/s]Extractor Estimating: 85it [00:55,  1.76it/s]Extractor Estimating: 86it [00:56,  1.68it/s]Extractor Estimating: 87it [00:56,  1.71it/s]Extractor Estimating: 88it [00:57,  1.73it/s]Extractor Estimating: 89it [00:57,  1.73it/s]Extractor Estimating: 90it [00:58,  1.70it/s]Extractor Estimating: 91it [00:58,  1.70it/s]Extractor Estimating: 92it [00:59,  1.73it/s]Extractor Estimating: 93it [01:00,  1.70it/s]Extractor Estimating: 94it [01:00,  1.65it/s]Extractor Estimating: 95it [01:01,  1.54it/s]Extractor Estimating: 96it [01:02,  1.61it/s]Extractor Estimating: 97it [01:02,  1.68it/s]Extractor Estimating: 98it [01:03,  1.64it/s]Extractor Estimating: 99it [01:03,  1.66it/s]Extractor Estimating: 100it [01:04,  1.63it/s]Extractor Estimating: 101it [01:05,  1.63it/s]Extractor Estimating: 102it [01:05,  1.65it/s]Extractor Estimating: 103it [01:06,  1.64it/s]Extractor Estimating: 104it [01:06,  1.67it/s]Extractor Estimating: 105it [01:07,  1.65it/s]Extractor Estimating: 106it [01:08,  1.59it/s]Extractor Estimating: 107it [01:08,  1.63it/s]Extractor Estimating: 108it [01:09,  1.60it/s]Extractor Estimating: 109it [01:09,  1.63it/s]Extractor Estimating: 110it [01:10,  1.58it/s]Extractor Estimating: 111it [01:11,  1.60it/s]Extractor Estimating: 112it [01:11,  1.64it/s]Extractor Estimating: 113it [01:12,  1.60it/s]Extractor Estimating: 114it [01:13,  1.61it/s]Extractor Estimating: 115it [01:13,  1.60it/s]Extractor Estimating: 116it [01:14,  1.61it/s]Extractor Estimating: 117it [01:15,  1.56it/s]Extractor Estimating: 118it [01:15,  1.55it/s]Extractor Estimating: 119it [01:16,  1.54it/s]Extractor Estimating: 120it [01:16,  1.57it/s]Extractor Estimating: 121it [01:17,  1.62it/s]Extractor Estimating: 122it [01:18,  1.62it/s]Extractor Estimating: 123it [01:18,  1.65it/s]Extractor Estimating: 124it [01:19,  1.65it/s]Extractor Estimating: 125it [01:19,  1.65it/s]Extractor Estimating: 126it [01:20,  1.48it/s]Extractor Estimating: 127it [01:21,  1.48it/s]Extractor Estimating: 128it [01:22,  1.51it/s]Extractor Estimating: 129it [01:22,  1.53it/s]Extractor Estimating: 130it [01:23,  1.49it/s]Extractor Estimating: 131it [01:24,  1.46it/s]Extractor Estimating: 132it [01:24,  1.47it/s]Extractor Estimating: 133it [01:25,  1.51it/s]Extractor Estimating: 134it [01:26,  1.55it/s]Extractor Estimating: 135it [01:26,  1.57it/s]Extractor Estimating: 136it [01:27,  1.58it/s]Extractor Estimating: 137it [01:27,  1.63it/s]Extractor Estimating: 138it [01:28,  1.61it/s]Extractor Estimating: 139it [01:29,  1.62it/s]Extractor Estimating: 140it [01:29,  1.59it/s]Extractor Estimating: 141it [01:30,  1.56it/s]Extractor Estimating: 142it [01:31,  1.55it/s]Extractor Estimating: 143it [01:31,  1.52it/s]Extractor Estimating: 144it [01:32,  1.53it/s]Extractor Estimating: 145it [01:33,  1.49it/s]Extractor Estimating: 146it [01:33,  1.54it/s]Extractor Estimating: 147it [01:34,  1.55it/s]Extractor Estimating: 148it [01:35,  1.55it/s]Extractor Estimating: 149it [01:35,  1.52it/s]Extractor Estimating: 150it [01:36,  1.48it/s]Extractor Estimating: 151it [01:37,  1.50it/s]Extractor Estimating: 152it [01:37,  1.50it/s]Extractor Estimating: 153it [01:38,  1.50it/s]Extractor Estimating: 154it [01:39,  1.51it/s]Extractor Estimating: 155it [01:39,  1.46it/s]Extractor Estimating: 156it [01:40,  1.43it/s]Extractor Estimating: 157it [01:41,  1.44it/s]Extractor Estimating: 158it [01:41,  1.50it/s]Extractor Estimating: 159it [01:42,  1.53it/s]Extractor Estimating: 160it [01:43,  1.42it/s]Extractor Estimating: 161it [01:43,  1.50it/s]Extractor Estimating: 162it [01:44,  1.50it/s]Extractor Estimating: 163it [01:45,  1.49it/s]Extractor Estimating: 164it [01:45,  1.46it/s]Extractor Estimating: 165it [01:46,  1.47it/s]Extractor Estimating: 166it [01:47,  1.46it/s]Extractor Estimating: 167it [01:47,  1.47it/s]Extractor Estimating: 168it [01:48,  1.43it/s]Extractor Estimating: 169it [01:49,  1.43it/s]Extractor Estimating: 170it [01:50,  1.42it/s]Extractor Estimating: 171it [01:50,  1.41it/s]Extractor Estimating: 172it [01:51,  1.39it/s]Extractor Estimating: 173it [01:52,  1.45it/s]Extractor Estimating: 174it [01:52,  1.50it/s]Extractor Estimating: 175it [01:53,  1.47it/s]Extractor Estimating: 176it [01:54,  1.52it/s]Extractor Estimating: 177it [01:54,  1.53it/s]Extractor Estimating: 178it [01:55,  1.57it/s]Extractor Estimating: 179it [01:55,  1.61it/s]Extractor Estimating: 180it [01:56,  1.57it/s]Extractor Estimating: 181it [01:57,  1.53it/s]Extractor Estimating: 182it [01:57,  1.56it/s]Extractor Estimating: 183it [01:58,  1.59it/s]Extractor Estimating: 184it [01:59,  1.59it/s]Extractor Estimating: 185it [01:59,  1.56it/s]Extractor Estimating: 186it [02:00,  1.58it/s]Extractor Estimating: 187it [02:01,  1.59it/s]Extractor Estimating: 188it [02:01,  1.61it/s]Extractor Estimating: 189it [02:02,  1.60it/s]Extractor Estimating: 190it [02:02,  1.58it/s]Extractor Estimating: 191it [02:03,  1.57it/s]Extractor Estimating: 192it [02:04,  1.52it/s]Extractor Estimating: 193it [02:04,  1.52it/s]Extractor Estimating: 194it [02:05,  1.48it/s]Extractor Estimating: 195it [02:06,  1.50it/s]Extractor Estimating: 196it [02:06,  1.48it/s]Extractor Estimating: 197it [02:07,  1.47it/s]Extractor Estimating: 198it [02:08,  1.34it/s]Extractor Estimating: 199it [02:09,  1.39it/s]Extractor Estimating: 200it [02:09,  1.44it/s]Extractor Estimating: 201it [02:10,  1.39it/s]Extractor Estimating: 202it [02:11,  1.45it/s]Extractor Estimating: 203it [02:11,  1.46it/s]Extractor Estimating: 204it [02:12,  1.46it/s]Extractor Estimating: 205it [02:13,  1.49it/s]Extractor Estimating: 206it [02:13,  1.53it/s]Extractor Estimating: 207it [02:14,  1.47it/s]Extractor Estimating: 208it [02:15,  1.49it/s]Extractor Estimating: 209it [02:15,  1.49it/s]Extractor Estimating: 210it [02:16,  1.48it/s]Extractor Estimating: 211it [02:17,  1.45it/s]Extractor Estimating: 212it [02:17,  1.49it/s]Extractor Estimating: 213it [02:18,  1.50it/s]Extractor Estimating: 214it [02:19,  1.51it/s]Extractor Estimating: 215it [02:19,  1.54it/s]Extractor Estimating: 216it [02:20,  1.52it/s]Extractor Estimating: 217it [02:21,  1.50it/s]Extractor Estimating: 218it [02:21,  1.52it/s]Extractor Estimating: 219it [02:22,  1.54it/s]Extractor Estimating: 220it [02:23,  1.48it/s]Extractor Estimating: 221it [02:24,  1.43it/s]Extractor Estimating: 222it [02:24,  1.46it/s]Extractor Estimating: 223it [02:25,  1.47it/s]Extractor Estimating: 224it [02:25,  1.49it/s]Extractor Estimating: 225it [02:26,  1.50it/s]Extractor Estimating: 226it [02:27,  1.61it/s]Extractor Estimating: 227it [02:27,  1.63it/s]Extractor Estimating: 228it [02:28,  1.67it/s]Extractor Estimating: 229it [02:28,  1.66it/s]Extractor Estimating: 230it [02:29,  1.70it/s]Extractor Estimating: 231it [02:30,  1.66it/s]Extractor Estimating: 232it [02:30,  1.69it/s]Extractor Estimating: 233it [02:31,  1.68it/s]Extractor Estimating: 234it [02:31,  1.67it/s]Extractor Estimating: 235it [02:32,  1.70it/s]Extractor Estimating: 236it [02:33,  1.73it/s]Extractor Estimating: 237it [02:33,  1.73it/s]Extractor Estimating: 238it [02:34,  1.74it/s]Extractor Estimating: 239it [02:34,  1.79it/s]Extractor Estimating: 240it [02:35,  1.78it/s]Extractor Estimating: 241it [02:35,  1.75it/s]Extractor Estimating: 242it [02:36,  1.82it/s]Extractor Estimating: 243it [02:36,  1.80it/s]Extractor Estimating: 244it [02:37,  1.79it/s]Extractor Estimating: 245it [02:38,  1.83it/s]Extractor Estimating: 246it [02:38,  1.79it/s]Extractor Estimating: 247it [02:39,  1.80it/s]Extractor Estimating: 248it [02:39,  1.80it/s]Extractor Estimating: 249it [02:40,  1.77it/s]Extractor Estimating: 250it [02:40,  1.73it/s]Extractor Estimating: 251it [02:41,  1.70it/s]Extractor Estimating: 252it [02:42,  1.73it/s]Extractor Estimating: 253it [02:42,  1.76it/s]Extractor Estimating: 254it [02:43,  1.73it/s]Extractor Estimating: 255it [02:43,  1.76it/s]Extractor Estimating: 256it [02:44,  1.72it/s]Extractor Estimating: 257it [02:45,  1.66it/s]Extractor Estimating: 258it [02:45,  1.67it/s]Extractor Estimating: 259it [02:46,  1.71it/s]Extractor Estimating: 260it [02:46,  1.70it/s]Extractor Estimating: 261it [02:47,  1.63it/s]Extractor Estimating: 262it [02:48,  1.60it/s]Extractor Estimating: 263it [02:48,  1.66it/s]Extractor Estimating: 264it [02:49,  1.70it/s]Extractor Estimating: 265it [02:49,  1.71it/s]Extractor Estimating: 266it [02:50,  1.65it/s]Extractor Estimating: 267it [02:51,  1.61it/s]Extractor Estimating: 268it [02:51,  1.64it/s]Extractor Estimating: 269it [02:52,  1.66it/s]Extractor Estimating: 270it [02:52,  1.65it/s]Extractor Estimating: 271it [02:53,  1.61it/s]Extractor Estimating: 272it [02:54,  1.63it/s]Extractor Estimating: 273it [02:54,  1.61it/s]Extractor Estimating: 274it [02:55,  1.62it/s]Extractor Estimating: 275it [02:56,  1.58it/s]Extractor Estimating: 276it [02:56,  1.47it/s]Extractor Estimating: 277it [02:57,  1.46it/s]Extractor Estimating: 278it [02:58,  1.52it/s]Extractor Estimating: 279it [02:58,  1.60it/s]Extractor Estimating: 280it [02:59,  1.65it/s]Extractor Estimating: 281it [02:59,  1.70it/s]Extractor Estimating: 282it [03:00,  1.72it/s]Extractor Estimating: 283it [03:00,  1.72it/s]Extractor Estimating: 284it [03:01,  1.66it/s]Extractor Estimating: 285it [03:02,  1.65it/s]Extractor Estimating: 286it [03:02,  1.67it/s]Extractor Estimating: 287it [03:03,  1.71it/s]Extractor Estimating: 288it [03:03,  1.71it/s]Extractor Estimating: 289it [03:04,  1.73it/s]Extractor Estimating: 290it [03:04,  1.77it/s]Extractor Estimating: 291it [03:05,  1.73it/s]Extractor Estimating: 292it [03:06,  1.67it/s]Extractor Estimating: 293it [03:06,  1.72it/s]Extractor Estimating: 294it [03:07,  1.72it/s]Extractor Estimating: 295it [03:07,  1.73it/s]Extractor Estimating: 296it [03:08,  1.76it/s]Extractor Estimating: 297it [03:09,  1.79it/s]Extractor Estimating: 298it [03:09,  1.72it/s]Extractor Estimating: 299it [03:10,  1.72it/s]Extractor Estimating: 300it [03:10,  1.67it/s]Extractor Estimating: 301it [03:11,  1.56it/s]Extractor Estimating: 302it [03:12,  1.52it/s]Extractor Estimating: 303it [03:12,  1.52it/s]Extractor Estimating: 304it [03:13,  1.58it/s]Extractor Estimating: 305it [03:14,  1.56it/s]Extractor Estimating: 306it [03:14,  1.55it/s]Extractor Estimating: 307it [03:15,  1.53it/s]Extractor Estimating: 308it [03:16,  1.57it/s]Extractor Estimating: 309it [03:16,  1.62it/s]Extractor Estimating: 310it [03:17,  1.57it/s]Extractor Estimating: 311it [03:17,  1.62it/s]Extractor Estimating: 312it [03:18,  1.61it/s]Extractor Estimating: 313it [03:19,  1.56it/s]Extractor Estimating: 314it [03:19,  1.56it/s]Extractor Estimating: 315it [03:20,  1.57it/s]Extractor Estimating: 316it [03:21,  1.58it/s]Extractor Estimating: 317it [03:21,  1.55it/s]Extractor Estimating: 318it [03:22,  1.57it/s]Extractor Estimating: 319it [03:23,  1.56it/s]Extractor Estimating: 320it [03:23,  1.61it/s]Extractor Estimating: 321it [03:24,  1.55it/s]Extractor Estimating: 322it [03:25,  1.56it/s]Extractor Estimating: 323it [03:25,  1.54it/s]Extractor Estimating: 324it [03:26,  1.58it/s]Extractor Estimating: 325it [03:26,  1.56it/s]Extractor Estimating: 326it [03:27,  1.61it/s]Extractor Estimating: 327it [03:28,  1.63it/s]Extractor Estimating: 328it [03:28,  1.66it/s]Extractor Estimating: 329it [03:29,  1.72it/s]Extractor Estimating: 330it [03:29,  1.67it/s]Extractor Estimating: 331it [03:30,  1.64it/s]Extractor Estimating: 332it [03:31,  1.72it/s]Extractor Estimating: 333it [03:31,  1.74it/s]Extractor Estimating: 334it [03:32,  1.76it/s]Extractor Estimating: 335it [03:32,  1.79it/s]Extractor Estimating: 336it [03:33,  1.81it/s]Extractor Estimating: 337it [03:33,  1.78it/s]Extractor Estimating: 338it [03:34,  1.77it/s]Extractor Estimating: 339it [03:34,  1.76it/s]Extractor Estimating: 340it [03:35,  1.70it/s]Extractor Estimating: 341it [03:36,  1.63it/s]Extractor Estimating: 342it [03:36,  1.60it/s]Extractor Estimating: 343it [03:37,  1.63it/s]Extractor Estimating: 344it [03:38,  1.65it/s]Extractor Estimating: 345it [03:38,  1.65it/s]Extractor Estimating: 346it [03:39,  1.69it/s]Extractor Estimating: 347it [03:39,  1.72it/s]Extractor Estimating: 348it [03:40,  1.52it/s]Extractor Estimating: 349it [03:41,  1.55it/s]Extractor Estimating: 350it [03:41,  1.58it/s]Extractor Estimating: 351it [03:42,  1.57it/s]Extractor Estimating: 352it [03:43,  1.59it/s]Extractor Estimating: 353it [03:43,  1.56it/s]Extractor Estimating: 354it [03:44,  1.57it/s]Extractor Estimating: 355it [03:45,  1.55it/s]Extractor Estimating: 356it [03:45,  1.56it/s]Extractor Estimating: 357it [03:46,  1.54it/s]Extractor Estimating: 358it [03:47,  1.52it/s]Extractor Estimating: 359it [03:47,  1.51it/s]Extractor Estimating: 360it [03:48,  1.41it/s]Extractor Estimating: 361it [03:49,  1.43it/s]Extractor Estimating: 362it [03:49,  1.46it/s]Extractor Estimating: 363it [03:50,  1.43it/s]Extractor Estimating: 364it [03:51,  1.46it/s]Extractor Estimating: 365it [03:51,  1.48it/s]Extractor Estimating: 366it [03:52,  1.49it/s]Extractor Estimating: 367it [03:53,  1.54it/s]Extractor Estimating: 368it [03:53,  1.54it/s]Extractor Estimating: 369it [03:54,  1.53it/s]Extractor Estimating: 370it [03:55,  1.50it/s]Extractor Estimating: 371it [03:55,  1.47it/s]Extractor Estimating: 372it [03:56,  1.48it/s]Extractor Estimating: 373it [03:57,  1.52it/s]Extractor Estimating: 374it [03:57,  1.52it/s]Extractor Estimating: 375it [03:58,  1.47it/s]Extractor Estimating: 375it [03:58,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:13,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:13,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:13,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:13,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:13,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:06:15,017 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:06:15,018 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:06:15,609 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:06:16,697 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:06:16,697 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:20,996 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:21,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:21,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:21,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:06:21,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:06:21,686 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:06:21,687 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:06:22,323 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:06:22,490 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:06:22,490 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:22:36,077 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:22:36,100 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7497 mean pseudo reward: 0.9529731662071647
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 16342
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16442, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16442, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.058, loss:599.0530
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.069, loss:572.0394
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.064, loss:575.0658
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.073, loss:535.2774
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.057, loss:520.3489
>> valid entity prec:0.5458, rec:0.6122, f1:0.5771
>> valid relation prec:0.4009, rec:0.1598, f1:0.2285
>> valid relation with NER prec:0.4009, rec:0.1598, f1:0.2285
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.303, loss:591.9113
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.074, loss:524.0456
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.083, loss:524.9063
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.075, loss:582.5194
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.088, loss:514.1892
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5759, rec:0.5754, f1:0.5756
>> valid relation prec:0.3891, rec:0.1518, f1:0.2184
>> valid relation with NER prec:0.3891, rec:0.1518, f1:0.2184
g_step 1100, step 161, avg_time 2.300, loss:520.2345
g_step 1200, step 261, avg_time 1.071, loss:557.5461
g_step 1300, step 48, avg_time 1.075, loss:517.5576
g_step 1400, step 148, avg_time 1.088, loss:503.7219
g_step 1500, step 248, avg_time 1.068, loss:496.3655
>> valid entity prec:0.5686, rec:0.5425, f1:0.5552
>> valid relation prec:0.3319, rec:0.1530, f1:0.2094
>> valid relation with NER prec:0.3319, rec:0.1530, f1:0.2094
g_step 1600, step 35, avg_time 2.284, loss:492.5242
g_step 1700, step 135, avg_time 1.077, loss:485.5218
g_step 1800, step 235, avg_time 1.085, loss:469.3306
g_step 1900, step 22, avg_time 1.062, loss:473.7976
g_step 2000, step 122, avg_time 1.081, loss:438.2820
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5501, rec:0.5703, f1:0.5600
>> valid relation prec:0.3354, rec:0.1418, f1:0.1993
>> valid relation with NER prec:0.3354, rec:0.1418, f1:0.1993
g_step 2100, step 222, avg_time 2.293, loss:450.4351
g_step 2200, step 9, avg_time 1.066, loss:469.7100
g_step 2300, step 109, avg_time 1.084, loss:398.7563
g_step 2400, step 209, avg_time 1.083, loss:451.1482
g_step 2500, step 309, avg_time 1.067, loss:449.7785
>> valid entity prec:0.5554, rec:0.5961, f1:0.5750
>> valid relation prec:0.3547, rec:0.1936, f1:0.2505
>> valid relation with NER prec:0.3547, rec:0.1936, f1:0.2505
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.300, loss:395.7165
g_step 2700, step 196, avg_time 1.074, loss:413.4401
g_step 2800, step 296, avg_time 1.077, loss:419.2766
g_step 2900, step 83, avg_time 1.067, loss:382.1528
g_step 3000, step 183, avg_time 1.070, loss:363.6667
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5625, rec:0.5425, f1:0.5523
>> valid relation prec:0.3293, rec:0.1570, f1:0.2126
>> valid relation with NER prec:0.3293, rec:0.1570, f1:0.2126
g_step 3100, step 283, avg_time 2.304, loss:408.6411
g_step 3200, step 70, avg_time 1.082, loss:379.7543
g_step 3300, step 170, avg_time 1.054, loss:363.7108
g_step 3400, step 270, avg_time 1.080, loss:376.3298
g_step 3500, step 57, avg_time 1.081, loss:362.8600
>> valid entity prec:0.5375, rec:0.5760, f1:0.5561
>> valid relation prec:0.2730, rec:0.1624, f1:0.2037
>> valid relation with NER prec:0.2730, rec:0.1624, f1:0.2037
g_step 3600, step 157, avg_time 2.293, loss:345.9887
g_step 3700, step 257, avg_time 1.081, loss:369.7900
g_step 3800, step 44, avg_time 1.063, loss:350.0079
g_step 3900, step 144, avg_time 1.090, loss:326.7824
g_step 4000, step 244, avg_time 1.083, loss:350.7165
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5160, rec:0.5835, f1:0.5477
>> valid relation prec:0.2764, rec:0.1702, f1:0.2106
>> valid relation with NER prec:0.2764, rec:0.1702, f1:0.2106
g_step 4100, step 31, avg_time 2.281, loss:342.9091
g_step 4200, step 131, avg_time 1.074, loss:301.8595
g_step 4300, step 231, avg_time 1.095, loss:312.2794
g_step 4400, step 18, avg_time 1.089, loss:332.5439
g_step 4500, step 118, avg_time 1.063, loss:306.7567
>> valid entity prec:0.5993, rec:0.4951, f1:0.5422
>> valid relation prec:0.3491, rec:0.1610, f1:0.2203
>> valid relation with NER prec:0.3491, rec:0.1610, f1:0.2203
g_step 4600, step 218, avg_time 2.285, loss:311.8308
g_step 4700, step 5, avg_time 1.058, loss:315.2136
g_step 4800, step 105, avg_time 1.074, loss:289.4580
g_step 4900, step 205, avg_time 1.068, loss:299.6801
g_step 5000, step 305, avg_time 1.069, loss:309.3643
learning rate was adjusted to 0.0008
>> valid entity prec:0.5614, rec:0.5362, f1:0.5485
>> valid relation prec:0.3077, rec:0.1653, f1:0.2151
>> valid relation with NER prec:0.3077, rec:0.1653, f1:0.2151
g_step 5100, step 92, avg_time 2.270, loss:293.3339
g_step 5200, step 192, avg_time 1.079, loss:277.5938
g_step 5300, step 292, avg_time 1.057, loss:284.8968
g_step 5400, step 79, avg_time 1.069, loss:267.3550
g_step 5500, step 179, avg_time 1.070, loss:263.7155
>> valid entity prec:0.5470, rec:0.5996, f1:0.5721
>> valid relation prec:0.2781, rec:0.1859, f1:0.2228
>> valid relation with NER prec:0.2781, rec:0.1859, f1:0.2228
g_step 5600, step 279, avg_time 2.274, loss:284.6986
g_step 5700, step 66, avg_time 1.069, loss:263.2802
g_step 5800, step 166, avg_time 1.055, loss:260.2491
g_step 5900, step 266, avg_time 1.080, loss:279.2536
g_step 6000, step 53, avg_time 1.056, loss:250.0123
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5440, rec:0.5669, f1:0.5552
>> valid relation prec:0.2856, rec:0.1659, f1:0.2099
>> valid relation with NER prec:0.2856, rec:0.1659, f1:0.2099
g_step 6100, step 153, avg_time 2.287, loss:249.2357
g_step 6200, step 253, avg_time 1.035, loss:238.7957
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:22:36 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:22:36 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-22-36_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:22:37 - WARNING - datasets.builder -   Using custom data configuration default-c7fd2b13b081f140
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c7fd2b13b081f140/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  2.76 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:22:37,894 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:22:37,895 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:22:37,896 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:22:37,897 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:22:37,907 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:22:37,913 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:22:37,913 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:22:37,913 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:22:37,913 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:22:37,913 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:22:37,913 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:22:38,157 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:22:41,291 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:22:41,293 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c7fd2b13b081f140/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.03ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.90ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.31ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.54ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.86ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.17ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.39ba/s]100%|██████████| 8/8 [00:01<00:00,  5.30ba/s]100%|██████████| 8/8 [00:01<00:00,  4.50ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.22ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.45ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.51ba/s]100%|██████████| 4/4 [00:00<00:00,  5.61ba/s]100%|██████████| 4/4 [00:00<00:00,  5.11ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.79ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.69ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 11.09ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.13ba/s]100%|██████████| 8/8 [00:00<00:00, 11.68ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.48ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.89ba/s]100%|██████████| 4/4 [00:00<00:00, 12.35ba/s]
[INFO|trainer.py:414] 2023-08-29 01:22:45,276 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:22:45,298 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:22:45,298 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 01:22:45,298 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:22:45,298 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:22:45,298 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:22:45,298 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:22:45,298 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<04:22,  2.22it/s]  0%|          | 2/585 [00:00<03:27,  2.81it/s]  1%|          | 3/585 [00:01<03:09,  3.07it/s]  1%|          | 4/585 [00:01<03:00,  3.22it/s]  1%|          | 5/585 [00:01<02:55,  3.30it/s]  1%|          | 6/585 [00:01<02:52,  3.35it/s]  1%|          | 7/585 [00:02<02:51,  3.37it/s]  1%|▏         | 8/585 [00:02<02:50,  3.38it/s]  2%|▏         | 9/585 [00:02<02:50,  3.39it/s]  2%|▏         | 10/585 [00:03<02:49,  3.39it/s]  2%|▏         | 11/585 [00:03<02:48,  3.40it/s]  2%|▏         | 12/585 [00:03<02:48,  3.40it/s]  2%|▏         | 13/585 [00:03<02:48,  3.40it/s]  2%|▏         | 14/585 [00:04<02:47,  3.40it/s]  3%|▎         | 15/585 [00:04<02:47,  3.41it/s]  3%|▎         | 16/585 [00:04<02:46,  3.41it/s]  3%|▎         | 17/585 [00:05<02:46,  3.41it/s]  3%|▎         | 18/585 [00:05<02:46,  3.41it/s]  3%|▎         | 19/585 [00:05<02:46,  3.40it/s]  3%|▎         | 20/585 [00:06<02:45,  3.40it/s]  4%|▎         | 21/585 [00:06<02:45,  3.40it/s]  4%|▍         | 22/585 [00:06<02:45,  3.40it/s]  4%|▍         | 23/585 [00:06<02:44,  3.41it/s]  4%|▍         | 24/585 [00:07<02:44,  3.41it/s]  4%|▍         | 25/585 [00:07<02:44,  3.40it/s]  4%|▍         | 26/585 [00:07<02:44,  3.40it/s]  5%|▍         | 27/585 [00:08<02:43,  3.41it/s]  5%|▍         | 28/585 [00:08<02:43,  3.40it/s]  5%|▍         | 29/585 [00:08<02:43,  3.40it/s]  5%|▌         | 30/585 [00:08<02:44,  3.38it/s]  5%|▌         | 31/585 [00:09<02:43,  3.39it/s]  5%|▌         | 32/585 [00:09<02:42,  3.39it/s]  6%|▌         | 33/585 [00:09<02:42,  3.40it/s]  6%|▌         | 34/585 [00:10<02:42,  3.40it/s]  6%|▌         | 35/585 [00:10<02:41,  3.40it/s]  6%|▌         | 36/585 [00:10<02:41,  3.40it/s]  6%|▋         | 37/585 [00:11<02:40,  3.40it/s]  6%|▋         | 38/585 [00:11<02:40,  3.40it/s]  7%|▋         | 39/585 [00:11<02:40,  3.41it/s]  7%|▋         | 40/585 [00:11<02:40,  3.40it/s]  7%|▋         | 41/585 [00:12<02:40,  3.40it/s]  7%|▋         | 42/585 [00:12<02:39,  3.40it/s]  7%|▋         | 43/585 [00:12<02:39,  3.40it/s]  8%|▊         | 44/585 [00:13<02:39,  3.40it/s]  8%|▊         | 45/585 [00:13<02:38,  3.40it/s]  8%|▊         | 46/585 [00:13<02:38,  3.40it/s]  8%|▊         | 47/585 [00:13<02:38,  3.40it/s]  8%|▊         | 48/585 [00:14<02:37,  3.40it/s]  8%|▊         | 49/585 [00:14<02:37,  3.40it/s]  9%|▊         | 50/585 [00:14<02:37,  3.41it/s]  9%|▊         | 51/585 [00:15<02:36,  3.40it/s]  9%|▉         | 52/585 [00:15<02:37,  3.38it/s]  9%|▉         | 53/585 [00:15<02:37,  3.38it/s]  9%|▉         | 54/585 [00:16<02:36,  3.39it/s]  9%|▉         | 55/585 [00:16<02:36,  3.40it/s] 10%|▉         | 56/585 [00:16<02:35,  3.39it/s] 10%|▉         | 57/585 [00:16<02:35,  3.40it/s] 10%|▉         | 58/585 [00:17<02:34,  3.41it/s] 10%|█         | 59/585 [00:17<02:33,  3.42it/s] 10%|█         | 60/585 [00:17<02:33,  3.43it/s] 10%|█         | 61/585 [00:18<02:32,  3.44it/s] 11%|█         | 62/585 [00:18<02:31,  3.44it/s] 11%|█         | 63/585 [00:18<02:32,  3.42it/s] 11%|█         | 64/585 [00:18<02:31,  3.43it/s] 11%|█         | 65/585 [00:19<02:31,  3.44it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.44it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 68/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 69/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 72/585 [00:21<02:28,  3.44it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 75/585 [00:22<02:28,  3.44it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.44it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.45it/s] 14%|█▎        | 79/585 [00:23<02:27,  3.43it/s] 14%|█▎        | 80/585 [00:23<02:27,  3.42it/s] 14%|█▍        | 81/585 [00:23<02:27,  3.41it/s] 14%|█▍        | 82/585 [00:24<02:27,  3.41it/s] 14%|█▍        | 83/585 [00:24<02:27,  3.40it/s] 14%|█▍        | 84/585 [00:24<02:27,  3.39it/s] 15%|█▍        | 85/585 [00:25<02:27,  3.39it/s] 15%|█▍        | 86/585 [00:25<02:26,  3.40it/s] 15%|█▍        | 87/585 [00:25<02:26,  3.39it/s] 15%|█▌        | 88/585 [00:25<02:26,  3.39it/s] 15%|█▌        | 89/585 [00:26<02:26,  3.39it/s] 15%|█▌        | 90/585 [00:26<02:25,  3.39it/s] 16%|█▌        | 91/585 [00:26<02:25,  3.40it/s] 16%|█▌        | 92/585 [00:27<02:25,  3.39it/s] 16%|█▌        | 93/585 [00:27<02:24,  3.39it/s] 16%|█▌        | 94/585 [00:27<02:24,  3.39it/s] 16%|█▌        | 95/585 [00:28<02:24,  3.38it/s] 16%|█▋        | 96/585 [00:28<02:24,  3.39it/s] 17%|█▋        | 97/585 [00:28<02:24,  3.39it/s] 17%|█▋        | 98/585 [00:28<02:23,  3.39it/s] 17%|█▋        | 99/585 [00:29<02:23,  3.39it/s] 17%|█▋        | 100/585 [00:29<02:22,  3.39it/s] 17%|█▋        | 101/585 [00:29<02:22,  3.39it/s] 17%|█▋        | 102/585 [00:30<02:22,  3.39it/s] 18%|█▊        | 103/585 [00:30<02:22,  3.39it/s] 18%|█▊        | 104/585 [00:30<02:21,  3.39it/s] 18%|█▊        | 105/585 [00:30<02:21,  3.39it/s] 18%|█▊        | 106/585 [00:31<02:21,  3.39it/s] 18%|█▊        | 107/585 [00:31<02:20,  3.39it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.39it/s] 19%|█▊        | 109/585 [00:32<02:20,  3.39it/s] 19%|█▉        | 110/585 [00:32<02:20,  3.39it/s] 19%|█▉        | 111/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 112/585 [00:33<02:19,  3.39it/s] 19%|█▉        | 113/585 [00:33<02:19,  3.39it/s] 19%|█▉        | 114/585 [00:33<02:18,  3.39it/s] 20%|█▉        | 115/585 [00:33<02:18,  3.39it/s] 20%|█▉        | 116/585 [00:34<02:18,  3.39it/s] 20%|██        | 117/585 [00:34<02:18,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 01:23:19,830 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:23:19,830 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:23:19,830 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.08it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.53it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.98it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.21it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.67it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.39it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.16it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.07it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.02it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.22it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.32it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.27it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.15it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.04it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.87it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.85it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.97it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.86it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.13it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.25it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.27it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.15it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.11it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.96it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.90it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.89it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.11it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.25it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.24it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.11it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.10it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.06it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.92it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.88it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.01it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.09it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.19it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.20it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.18it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.02it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.10it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.01it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.00it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.97it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.10it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.09it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.19it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.13it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.03it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.07it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.01it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.04it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.12it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.05it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.09it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.09it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.08it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.05it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.06it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.75it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.17it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.12it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.07it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.11it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.02it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.08it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.01it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.04it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.11it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.07it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.10it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.13it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.15it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.06it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.98it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.96it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.07it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.01it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.10it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.07it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.12it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.13it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.01it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.97it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.03it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.91it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.04it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.04it/s][A 20%|██        | 117/585 [00:44<02:18,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:23:29,777 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 01:23:29,796 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:23:31,439 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:23:31,452 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:23:31,475 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:50<37:53,  4.87s/it] 20%|██        | 119/585 [00:50<27:09,  3.50s/it] 21%|██        | 120/585 [00:50<19:38,  2.53s/it] 21%|██        | 121/585 [00:50<14:26,  1.87s/it] 21%|██        | 122/585 [00:51<10:45,  1.39s/it] 21%|██        | 123/585 [00:51<08:11,  1.06s/it] 21%|██        | 124/585 [00:51<06:23,  1.20it/s] 21%|██▏       | 125/585 [00:52<05:07,  1.50it/s] 22%|██▏       | 126/585 [00:52<04:14,  1.80it/s] 22%|██▏       | 127/585 [00:52<03:37,  2.10it/s] 22%|██▏       | 128/585 [00:52<03:11,  2.38it/s] 22%|██▏       | 129/585 [00:53<02:53,  2.62it/s] 22%|██▏       | 130/585 [00:53<02:40,  2.83it/s] 22%|██▏       | 131/585 [00:53<02:31,  2.99it/s] 23%|██▎       | 132/585 [00:54<02:25,  3.11it/s] 23%|██▎       | 133/585 [00:54<02:20,  3.21it/s] 23%|██▎       | 134/585 [00:54<02:17,  3.28it/s] 23%|██▎       | 135/585 [00:54<02:15,  3.33it/s] 23%|██▎       | 136/585 [00:55<02:13,  3.35it/s] 23%|██▎       | 137/585 [00:55<02:12,  3.38it/s] 24%|██▎       | 138/585 [00:55<02:11,  3.40it/s] 24%|██▍       | 139/585 [00:56<02:10,  3.41it/s] 24%|██▍       | 140/585 [00:56<02:10,  3.42it/s] 24%|██▍       | 141/585 [00:56<02:09,  3.43it/s] 24%|██▍       | 142/585 [00:57<02:09,  3.43it/s] 24%|██▍       | 143/585 [00:57<02:08,  3.44it/s] 25%|██▍       | 144/585 [00:57<02:08,  3.44it/s] 25%|██▍       | 145/585 [00:57<02:07,  3.44it/s] 25%|██▍       | 146/585 [00:58<02:07,  3.44it/s] 25%|██▌       | 147/585 [00:58<02:08,  3.42it/s] 25%|██▌       | 148/585 [00:58<02:07,  3.43it/s] 25%|██▌       | 149/585 [00:59<02:06,  3.44it/s] 26%|██▌       | 150/585 [00:59<02:06,  3.44it/s] 26%|██▌       | 151/585 [00:59<02:06,  3.44it/s] 26%|██▌       | 152/585 [00:59<02:05,  3.45it/s] 26%|██▌       | 153/585 [01:00<02:05,  3.44it/s] 26%|██▋       | 154/585 [01:00<02:05,  3.44it/s] 26%|██▋       | 155/585 [01:00<02:04,  3.45it/s] 27%|██▋       | 156/585 [01:01<02:04,  3.45it/s] 27%|██▋       | 157/585 [01:01<02:04,  3.45it/s] 27%|██▋       | 158/585 [01:01<02:04,  3.44it/s] 27%|██▋       | 159/585 [01:01<02:03,  3.44it/s] 27%|██▋       | 160/585 [01:02<02:03,  3.44it/s] 28%|██▊       | 161/585 [01:02<02:03,  3.44it/s] 28%|██▊       | 162/585 [01:02<02:02,  3.44it/s] 28%|██▊       | 163/585 [01:03<02:02,  3.44it/s] 28%|██▊       | 164/585 [01:03<02:02,  3.44it/s] 28%|██▊       | 165/585 [01:03<02:01,  3.45it/s] 28%|██▊       | 166/585 [01:03<02:01,  3.44it/s] 29%|██▊       | 167/585 [01:04<02:01,  3.44it/s] 29%|██▊       | 168/585 [01:04<02:01,  3.44it/s] 29%|██▉       | 169/585 [01:04<02:02,  3.38it/s] 29%|██▉       | 170/585 [01:05<02:01,  3.40it/s] 29%|██▉       | 171/585 [01:05<02:01,  3.42it/s] 29%|██▉       | 172/585 [01:05<02:00,  3.42it/s] 30%|██▉       | 173/585 [01:06<02:00,  3.43it/s] 30%|██▉       | 174/585 [01:06<01:59,  3.43it/s] 30%|██▉       | 175/585 [01:06<01:59,  3.44it/s] 30%|███       | 176/585 [01:06<01:58,  3.44it/s] 30%|███       | 177/585 [01:07<01:58,  3.44it/s] 30%|███       | 178/585 [01:07<01:58,  3.44it/s] 31%|███       | 179/585 [01:07<01:57,  3.44it/s] 31%|███       | 180/585 [01:08<01:58,  3.43it/s] 31%|███       | 181/585 [01:08<01:57,  3.44it/s] 31%|███       | 182/585 [01:08<01:57,  3.44it/s] 31%|███▏      | 183/585 [01:08<01:56,  3.44it/s] 31%|███▏      | 184/585 [01:09<01:56,  3.44it/s] 32%|███▏      | 185/585 [01:09<01:56,  3.44it/s] 32%|███▏      | 186/585 [01:09<01:55,  3.44it/s] 32%|███▏      | 187/585 [01:10<01:55,  3.44it/s] 32%|███▏      | 188/585 [01:10<01:55,  3.44it/s] 32%|███▏      | 189/585 [01:10<01:54,  3.44it/s] 32%|███▏      | 190/585 [01:10<01:54,  3.44it/s] 33%|███▎      | 191/585 [01:11<01:55,  3.40it/s] 33%|███▎      | 192/585 [01:11<01:55,  3.42it/s] 33%|███▎      | 193/585 [01:11<01:54,  3.42it/s] 33%|███▎      | 194/585 [01:12<01:54,  3.43it/s] 33%|███▎      | 195/585 [01:12<01:53,  3.43it/s] 34%|███▎      | 196/585 [01:12<01:53,  3.43it/s] 34%|███▎      | 197/585 [01:13<01:52,  3.43it/s] 34%|███▍      | 198/585 [01:13<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:13<01:52,  3.44it/s] 34%|███▍      | 200/585 [01:13<01:52,  3.44it/s] 34%|███▍      | 201/585 [01:14<01:51,  3.44it/s] 35%|███▍      | 202/585 [01:14<01:52,  3.42it/s] 35%|███▍      | 203/585 [01:14<01:51,  3.42it/s] 35%|███▍      | 204/585 [01:15<01:51,  3.43it/s] 35%|███▌      | 205/585 [01:15<01:50,  3.43it/s] 35%|███▌      | 206/585 [01:15<01:50,  3.43it/s] 35%|███▌      | 207/585 [01:15<01:50,  3.43it/s] 36%|███▌      | 208/585 [01:16<01:49,  3.43it/s] 36%|███▌      | 209/585 [01:16<01:49,  3.44it/s] 36%|███▌      | 210/585 [01:16<01:49,  3.44it/s] 36%|███▌      | 211/585 [01:17<01:48,  3.44it/s] 36%|███▌      | 212/585 [01:17<01:48,  3.44it/s] 36%|███▋      | 213/585 [01:17<01:48,  3.43it/s] 37%|███▋      | 214/585 [01:17<01:48,  3.43it/s] 37%|███▋      | 215/585 [01:18<01:47,  3.44it/s] 37%|███▋      | 216/585 [01:18<01:47,  3.44it/s] 37%|███▋      | 217/585 [01:18<01:46,  3.44it/s] 37%|███▋      | 218/585 [01:19<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:19<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:19<01:46,  3.44it/s] 38%|███▊      | 221/585 [01:20<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:20<01:45,  3.44it/s] 38%|███▊      | 223/585 [01:20<01:45,  3.44it/s] 38%|███▊      | 224/585 [01:20<01:45,  3.43it/s] 38%|███▊      | 225/585 [01:21<01:44,  3.43it/s] 39%|███▊      | 226/585 [01:21<01:44,  3.43it/s] 39%|███▉      | 227/585 [01:21<01:44,  3.43it/s] 39%|███▉      | 228/585 [01:22<01:44,  3.43it/s] 39%|███▉      | 229/585 [01:22<01:43,  3.43it/s] 39%|███▉      | 230/585 [01:22<01:43,  3.44it/s] 39%|███▉      | 231/585 [01:22<01:43,  3.43it/s] 40%|███▉      | 232/585 [01:23<01:42,  3.43it/s] 40%|███▉      | 233/585 [01:23<01:42,  3.43it/s] 40%|████      | 234/585 [01:23<01:42,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 01:24:09,140 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:24:09,140 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:24:09,140 >>   Batch size = 8
{'eval_loss': 1.1672658920288086, 'eval_runtime': 9.9306, 'eval_samples_per_second': 351.74, 'eval_steps_per_second': 44.005, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.64it/s][A
  3%|▎         | 12/437 [00:00<00:09, 47.17it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.56it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.95it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.52it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.30it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.24it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.13it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.27it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.30it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.08it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.07it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.89it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.94it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.97it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.88it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 42.94it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.33it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.70it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.77it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.67it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.78it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.82it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.88it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.81it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.94it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.98it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.05it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.09it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.96it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.93it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.89it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.91it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.96it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.09it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.95it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.04it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.95it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.00it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.90it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.92it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.91it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.91it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.11it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.12it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.97it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.02it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.99it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.84it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.92it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.93it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.05it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.15it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.03it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.09it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.91it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.94it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.88it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.92it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.03it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.08it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.95it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.10it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.02it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.03it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.96it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.81it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.99it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.03it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.08it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.07it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.08it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.00it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.94it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.93it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.88it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.97it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.99it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.05it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.95it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.05it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.03it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.90it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.89it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.91it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.97it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.03it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.03it/s][A 40%|████      | 234/585 [01:33<01:42,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:24:19,127 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 01:24:19,153 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:24:20,990 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:24:21,007 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:24:21,041 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:39<29:13,  5.01s/it] 40%|████      | 236/585 [01:40<20:54,  3.60s/it] 41%|████      | 237/585 [01:40<15:06,  2.60s/it] 41%|████      | 238/585 [01:40<11:03,  1.91s/it] 41%|████      | 239/585 [01:40<08:13,  1.43s/it] 41%|████      | 240/585 [01:41<06:14,  1.09s/it] 41%|████      | 241/585 [01:41<04:52,  1.18it/s] 41%|████▏     | 242/585 [01:41<03:54,  1.47it/s] 42%|████▏     | 243/585 [01:42<03:13,  1.77it/s] 42%|████▏     | 244/585 [01:42<02:45,  2.06it/s] 42%|████▏     | 245/585 [01:42<02:25,  2.34it/s] 42%|████▏     | 246/585 [01:43<02:11,  2.58it/s] 42%|████▏     | 247/585 [01:43<02:01,  2.77it/s] 42%|████▏     | 248/585 [01:43<01:54,  2.93it/s] 43%|████▎     | 249/585 [01:43<01:49,  3.06it/s] 43%|████▎     | 250/585 [01:44<01:46,  3.15it/s] 43%|████▎     | 251/585 [01:44<01:43,  3.23it/s] 43%|████▎     | 252/585 [01:44<01:41,  3.27it/s] 43%|████▎     | 253/585 [01:45<01:40,  3.31it/s] 43%|████▎     | 254/585 [01:45<01:39,  3.33it/s] 44%|████▎     | 255/585 [01:45<01:38,  3.35it/s] 44%|████▍     | 256/585 [01:46<01:37,  3.36it/s] 44%|████▍     | 257/585 [01:46<01:37,  3.37it/s] 44%|████▍     | 258/585 [01:46<01:36,  3.37it/s] 44%|████▍     | 259/585 [01:46<01:36,  3.38it/s] 44%|████▍     | 260/585 [01:47<01:36,  3.38it/s] 45%|████▍     | 261/585 [01:47<01:35,  3.38it/s] 45%|████▍     | 262/585 [01:47<01:35,  3.39it/s] 45%|████▍     | 263/585 [01:48<01:34,  3.39it/s] 45%|████▌     | 264/585 [01:48<01:34,  3.39it/s] 45%|████▌     | 265/585 [01:48<01:34,  3.40it/s] 45%|████▌     | 266/585 [01:48<01:33,  3.40it/s] 46%|████▌     | 267/585 [01:49<01:33,  3.40it/s] 46%|████▌     | 268/585 [01:49<01:33,  3.39it/s] 46%|████▌     | 269/585 [01:49<01:33,  3.38it/s] 46%|████▌     | 270/585 [01:50<01:33,  3.38it/s] 46%|████▋     | 271/585 [01:50<01:32,  3.38it/s] 46%|████▋     | 272/585 [01:50<01:32,  3.39it/s] 47%|████▋     | 273/585 [01:51<01:33,  3.32it/s] 47%|████▋     | 274/585 [01:51<01:33,  3.34it/s] 47%|████▋     | 275/585 [01:51<01:32,  3.36it/s] 47%|████▋     | 276/585 [01:51<01:31,  3.37it/s] 47%|████▋     | 277/585 [01:52<01:31,  3.38it/s] 48%|████▊     | 278/585 [01:52<01:30,  3.38it/s] 48%|████▊     | 279/585 [01:52<01:30,  3.39it/s] 48%|████▊     | 280/585 [01:53<01:30,  3.38it/s] 48%|████▊     | 281/585 [01:53<01:29,  3.39it/s] 48%|████▊     | 282/585 [01:53<01:29,  3.39it/s] 48%|████▊     | 283/585 [01:53<01:29,  3.39it/s] 49%|████▊     | 284/585 [01:54<01:28,  3.39it/s] 49%|████▊     | 285/585 [01:54<01:28,  3.39it/s] 49%|████▉     | 286/585 [01:54<01:28,  3.39it/s] 49%|████▉     | 287/585 [01:55<01:27,  3.39it/s] 49%|████▉     | 288/585 [01:55<01:27,  3.39it/s] 49%|████▉     | 289/585 [01:55<01:27,  3.39it/s] 50%|████▉     | 290/585 [01:56<01:27,  3.39it/s] 50%|████▉     | 291/585 [01:56<01:26,  3.39it/s] 50%|████▉     | 292/585 [01:56<01:26,  3.39it/s] 50%|█████     | 293/585 [01:56<01:26,  3.37it/s] 50%|█████     | 294/585 [01:57<01:26,  3.38it/s] 50%|█████     | 295/585 [01:57<01:25,  3.38it/s] 51%|█████     | 296/585 [01:57<01:25,  3.38it/s] 51%|█████     | 297/585 [01:58<01:25,  3.38it/s] 51%|█████     | 298/585 [01:58<01:24,  3.39it/s] 51%|█████     | 299/585 [01:58<01:24,  3.39it/s] 51%|█████▏    | 300/585 [01:59<01:24,  3.39it/s] 51%|█████▏    | 301/585 [01:59<01:23,  3.39it/s] 52%|█████▏    | 302/585 [01:59<01:23,  3.39it/s] 52%|█████▏    | 303/585 [01:59<01:23,  3.39it/s] 52%|█████▏    | 304/585 [02:00<01:23,  3.38it/s] 52%|█████▏    | 305/585 [02:00<01:22,  3.38it/s] 52%|█████▏    | 306/585 [02:00<01:22,  3.39it/s] 52%|█████▏    | 307/585 [02:01<01:22,  3.39it/s] 53%|█████▎    | 308/585 [02:01<01:21,  3.39it/s] 53%|█████▎    | 309/585 [02:01<01:21,  3.39it/s] 53%|█████▎    | 310/585 [02:01<01:21,  3.39it/s] 53%|█████▎    | 311/585 [02:02<01:20,  3.39it/s] 53%|█████▎    | 312/585 [02:02<01:20,  3.39it/s] 54%|█████▎    | 313/585 [02:02<01:20,  3.39it/s] 54%|█████▎    | 314/585 [02:03<01:20,  3.39it/s] 54%|█████▍    | 315/585 [02:03<01:19,  3.38it/s] 54%|█████▍    | 316/585 [02:03<01:19,  3.38it/s] 54%|█████▍    | 317/585 [02:04<01:19,  3.39it/s] 54%|█████▍    | 318/585 [02:04<01:18,  3.39it/s] 55%|█████▍    | 319/585 [02:04<01:18,  3.39it/s] 55%|█████▍    | 320/585 [02:04<01:18,  3.39it/s] 55%|█████▍    | 321/585 [02:05<01:17,  3.39it/s] 55%|█████▌    | 322/585 [02:05<01:17,  3.39it/s] 55%|█████▌    | 323/585 [02:05<01:17,  3.39it/s] 55%|█████▌    | 324/585 [02:06<01:17,  3.39it/s] 56%|█████▌    | 325/585 [02:06<01:16,  3.38it/s] 56%|█████▌    | 326/585 [02:06<01:17,  3.35it/s] 56%|█████▌    | 327/585 [02:06<01:16,  3.37it/s] 56%|█████▌    | 328/585 [02:07<01:15,  3.39it/s] 56%|█████▌    | 329/585 [02:07<01:15,  3.41it/s] 56%|█████▋    | 330/585 [02:07<01:14,  3.41it/s] 57%|█████▋    | 331/585 [02:08<01:14,  3.42it/s] 57%|█████▋    | 332/585 [02:08<01:13,  3.43it/s] 57%|█████▋    | 333/585 [02:08<01:13,  3.43it/s] 57%|█████▋    | 334/585 [02:09<01:13,  3.43it/s] 57%|█████▋    | 335/585 [02:09<01:12,  3.44it/s] 57%|█████▋    | 336/585 [02:09<01:12,  3.44it/s] 58%|█████▊    | 337/585 [02:09<01:12,  3.41it/s] 58%|█████▊    | 338/585 [02:10<01:12,  3.42it/s] 58%|█████▊    | 339/585 [02:10<01:11,  3.43it/s] 58%|█████▊    | 340/585 [02:10<01:11,  3.43it/s] 58%|█████▊    | 341/585 [02:11<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:11<01:10,  3.43it/s] 59%|█████▊    | 343/585 [02:11<01:10,  3.43it/s] 59%|█████▉    | 344/585 [02:11<01:10,  3.43it/s] 59%|█████▉    | 345/585 [02:12<01:09,  3.43it/s] 59%|█████▉    | 346/585 [02:12<01:09,  3.43it/s] 59%|█████▉    | 347/585 [02:12<01:09,  3.43it/s] 59%|█████▉    | 348/585 [02:13<01:09,  3.43it/s] 60%|█████▉    | 349/585 [02:13<01:08,  3.43it/s] 60%|█████▉    | 350/585 [02:13<01:08,  3.43it/s] 60%|██████    | 351/585 [02:13<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 01:24:59,319 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:24:59,319 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:24:59,319 >>   Batch size = 8
{'eval_loss': 1.1768293380737305, 'eval_runtime': 9.9596, 'eval_samples_per_second': 350.717, 'eval_steps_per_second': 43.877, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.26it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.75it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.00it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.30it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.61it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.28it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.05it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.00it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.01it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.15it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.20it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.12it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.12it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.01it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.89it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.81it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.77it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.90it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.00it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.12it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.12it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.10it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.03it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.90it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.86it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.84it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.95it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.00it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.04it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.13it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.09it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.96it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.88it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.92it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.84it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.00it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.98it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.05it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.10it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.01it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.92it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.90it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.92it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.71it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.06it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.07it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.12it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.01it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.97it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.96it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.92it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.84it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.89it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.04it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.07it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.15it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.04it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.87it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.90it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.87it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.86it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.98it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.91it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.93it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.15it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.05it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.00it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.91it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.97it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.85it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.08it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.13it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.10it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.08it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.08it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.98it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.97it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.85it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.88it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.92it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.07it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.10it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.06it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.04it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.06it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.90it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.85it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.85it/s][A 60%|██████    | 351/585 [02:23<01:08,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:25:09,290 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 01:25:09,319 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:25:10,801 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:25:10,818 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:25:10,831 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:29<18:44,  4.83s/it] 60%|██████    | 353/585 [02:29<13:24,  3.47s/it] 61%|██████    | 354/585 [02:29<09:40,  2.52s/it] 61%|██████    | 355/585 [02:30<07:05,  1.85s/it] 61%|██████    | 356/585 [02:30<05:16,  1.38s/it] 61%|██████    | 357/585 [02:30<04:00,  1.06s/it] 61%|██████    | 358/585 [02:31<03:07,  1.21it/s] 61%|██████▏   | 359/585 [02:31<02:30,  1.50it/s] 62%|██████▏   | 360/585 [02:31<02:04,  1.80it/s] 62%|██████▏   | 361/585 [02:32<01:46,  2.10it/s] 62%|██████▏   | 362/585 [02:32<01:34,  2.37it/s] 62%|██████▏   | 363/585 [02:32<01:25,  2.60it/s] 62%|██████▏   | 364/585 [02:32<01:19,  2.79it/s] 62%|██████▏   | 365/585 [02:33<01:14,  2.95it/s] 63%|██████▎   | 366/585 [02:33<01:11,  3.07it/s] 63%|██████▎   | 367/585 [02:33<01:08,  3.16it/s] 63%|██████▎   | 368/585 [02:34<01:07,  3.23it/s] 63%|██████▎   | 369/585 [02:34<01:05,  3.28it/s] 63%|██████▎   | 370/585 [02:34<01:04,  3.31it/s] 63%|██████▎   | 371/585 [02:34<01:04,  3.34it/s] 64%|██████▎   | 372/585 [02:35<01:03,  3.36it/s] 64%|██████▍   | 373/585 [02:35<01:02,  3.37it/s] 64%|██████▍   | 374/585 [02:35<01:02,  3.38it/s] 64%|██████▍   | 375/585 [02:36<01:02,  3.37it/s] 64%|██████▍   | 376/585 [02:36<01:01,  3.38it/s] 64%|██████▍   | 377/585 [02:36<01:01,  3.38it/s] 65%|██████▍   | 378/585 [02:37<01:01,  3.38it/s] 65%|██████▍   | 379/585 [02:37<01:00,  3.39it/s] 65%|██████▍   | 380/585 [02:37<01:00,  3.39it/s] 65%|██████▌   | 381/585 [02:37<01:00,  3.39it/s] 65%|██████▌   | 382/585 [02:38<00:59,  3.39it/s] 65%|██████▌   | 383/585 [02:38<00:59,  3.39it/s] 66%|██████▌   | 384/585 [02:38<00:59,  3.39it/s] 66%|██████▌   | 385/585 [02:39<00:58,  3.39it/s] 66%|██████▌   | 386/585 [02:39<00:58,  3.38it/s] 66%|██████▌   | 387/585 [02:39<00:58,  3.39it/s] 66%|██████▋   | 388/585 [02:40<00:58,  3.39it/s] 66%|██████▋   | 389/585 [02:40<00:57,  3.39it/s] 67%|██████▋   | 390/585 [02:40<00:57,  3.39it/s] 67%|██████▋   | 391/585 [02:40<00:57,  3.39it/s] 67%|██████▋   | 392/585 [02:41<00:56,  3.39it/s] 67%|██████▋   | 393/585 [02:41<00:56,  3.39it/s] 67%|██████▋   | 394/585 [02:41<00:56,  3.39it/s] 68%|██████▊   | 395/585 [02:42<00:56,  3.39it/s] 68%|██████▊   | 396/585 [02:42<00:55,  3.39it/s] 68%|██████▊   | 397/585 [02:42<00:55,  3.38it/s] 68%|██████▊   | 398/585 [02:42<00:55,  3.38it/s] 68%|██████▊   | 399/585 [02:43<00:54,  3.39it/s] 68%|██████▊   | 400/585 [02:43<00:54,  3.40it/s] 69%|██████▊   | 401/585 [02:43<00:53,  3.41it/s] 69%|██████▊   | 402/585 [02:44<00:53,  3.42it/s] 69%|██████▉   | 403/585 [02:44<00:53,  3.43it/s] 69%|██████▉   | 404/585 [02:44<00:52,  3.43it/s] 69%|██████▉   | 405/585 [02:44<00:52,  3.44it/s] 69%|██████▉   | 406/585 [02:45<00:52,  3.43it/s] 70%|██████▉   | 407/585 [02:45<00:51,  3.44it/s] 70%|██████▉   | 408/585 [02:45<00:51,  3.42it/s] 70%|██████▉   | 409/585 [02:46<00:51,  3.43it/s] 70%|███████   | 410/585 [02:46<00:50,  3.43it/s] 70%|███████   | 411/585 [02:46<00:50,  3.44it/s] 70%|███████   | 412/585 [02:47<00:50,  3.44it/s] 71%|███████   | 413/585 [02:47<00:50,  3.44it/s] 71%|███████   | 414/585 [02:47<00:49,  3.44it/s] 71%|███████   | 415/585 [02:47<00:49,  3.44it/s] 71%|███████   | 416/585 [02:48<00:49,  3.44it/s] 71%|███████▏  | 417/585 [02:48<00:48,  3.44it/s] 71%|███████▏  | 418/585 [02:48<00:48,  3.44it/s] 72%|███████▏  | 419/585 [02:49<00:48,  3.43it/s] 72%|███████▏  | 420/585 [02:49<00:48,  3.43it/s] 72%|███████▏  | 421/585 [02:49<00:47,  3.43it/s] 72%|███████▏  | 422/585 [02:49<00:47,  3.42it/s] 72%|███████▏  | 423/585 [02:50<00:47,  3.43it/s] 72%|███████▏  | 424/585 [02:50<00:46,  3.43it/s] 73%|███████▎  | 425/585 [02:50<00:46,  3.43it/s] 73%|███████▎  | 426/585 [02:51<00:47,  3.35it/s] 73%|███████▎  | 427/585 [02:51<00:46,  3.37it/s] 73%|███████▎  | 428/585 [02:51<00:46,  3.40it/s] 73%|███████▎  | 429/585 [02:52<00:45,  3.41it/s] 74%|███████▎  | 430/585 [02:52<00:45,  3.41it/s] 74%|███████▎  | 431/585 [02:52<00:45,  3.41it/s] 74%|███████▍  | 432/585 [02:52<00:44,  3.42it/s] 74%|███████▍  | 433/585 [02:53<00:44,  3.43it/s] 74%|███████▍  | 434/585 [02:53<00:44,  3.43it/s] 74%|███████▍  | 435/585 [02:53<00:43,  3.43it/s] 75%|███████▍  | 436/585 [02:54<00:43,  3.44it/s] 75%|███████▍  | 437/585 [02:54<00:43,  3.44it/s] 75%|███████▍  | 438/585 [02:54<00:42,  3.44it/s] 75%|███████▌  | 439/585 [02:54<00:42,  3.44it/s] 75%|███████▌  | 440/585 [02:55<00:42,  3.44it/s] 75%|███████▌  | 441/585 [02:55<00:41,  3.44it/s] 76%|███████▌  | 442/585 [02:55<00:41,  3.44it/s] 76%|███████▌  | 443/585 [02:56<00:41,  3.44it/s] 76%|███████▌  | 444/585 [02:56<00:40,  3.44it/s] 76%|███████▌  | 445/585 [02:56<00:40,  3.44it/s] 76%|███████▌  | 446/585 [02:56<00:40,  3.44it/s] 76%|███████▋  | 447/585 [02:57<00:40,  3.44it/s] 77%|███████▋  | 448/585 [02:57<00:39,  3.44it/s] 77%|███████▋  | 449/585 [02:57<00:39,  3.44it/s] 77%|███████▋  | 450/585 [02:58<00:39,  3.44it/s] 77%|███████▋  | 451/585 [02:58<00:39,  3.43it/s] 77%|███████▋  | 452/585 [02:58<00:38,  3.43it/s] 77%|███████▋  | 453/585 [02:58<00:38,  3.44it/s] 78%|███████▊  | 454/585 [02:59<00:38,  3.44it/s] 78%|███████▊  | 455/585 [02:59<00:37,  3.44it/s] 78%|███████▊  | 456/585 [02:59<00:37,  3.44it/s] 78%|███████▊  | 457/585 [03:00<00:37,  3.44it/s] 78%|███████▊  | 458/585 [03:00<00:36,  3.44it/s] 78%|███████▊  | 459/585 [03:00<00:36,  3.44it/s] 79%|███████▊  | 460/585 [03:01<00:36,  3.44it/s] 79%|███████▉  | 461/585 [03:01<00:36,  3.44it/s] 79%|███████▉  | 462/585 [03:01<00:35,  3.43it/s] 79%|███████▉  | 463/585 [03:01<00:35,  3.43it/s] 79%|███████▉  | 464/585 [03:02<00:35,  3.43it/s] 79%|███████▉  | 465/585 [03:02<00:34,  3.44it/s] 80%|███████▉  | 466/585 [03:02<00:34,  3.44it/s] 80%|███████▉  | 467/585 [03:03<00:34,  3.44it/s] 80%|████████  | 468/585 [03:03<00:34,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 01:25:48,686 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:25:48,686 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:25:48,686 >>   Batch size = 8
{'eval_loss': 1.195220708847046, 'eval_runtime': 9.9496, 'eval_samples_per_second': 351.068, 'eval_steps_per_second': 43.921, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 54.83it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.46it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.82it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.07it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.71it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.34it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.27it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.18it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.21it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.17it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.07it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.05it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.91it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.94it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.92it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.91it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.97it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.81it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.06it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.09it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.99it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.88it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.93it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.00it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.91it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.93it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.90it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.95it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.10it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.94it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.94it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.02it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.04it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.99it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.90it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.93it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.01it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.01it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.92it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.99it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.96it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.01it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.88it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.94it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.04it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.97it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.97it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.98it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.01it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.06it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.93it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.90it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.03it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.02it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.76it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.05it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.03it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.97it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.09it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.87it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.91it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.08it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.11it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.02it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.01it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.99it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.01it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.86it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.93it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.89it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.99it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.05it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.06it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.04it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.07it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.84it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.92it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.01it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.02it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.95it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.02it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.04it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.08it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.02it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.97it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.91it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.85it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.85it/s][A 80%|████████  | 468/585 [03:13<00:34,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:25:58,653 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 01:25:58,678 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:26:00,584 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:26:00,599 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:26:00,608 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:19<09:30,  4.92s/it] 80%|████████  | 470/585 [03:19<06:46,  3.53s/it] 81%|████████  | 471/585 [03:19<04:51,  2.56s/it] 81%|████████  | 472/585 [03:19<03:32,  1.88s/it] 81%|████████  | 473/585 [03:20<02:37,  1.40s/it] 81%|████████  | 474/585 [03:20<01:58,  1.07s/it] 81%|████████  | 475/585 [03:20<01:32,  1.19it/s] 81%|████████▏ | 476/585 [03:21<01:13,  1.48it/s] 82%|████████▏ | 477/585 [03:21<01:00,  1.79it/s] 82%|████████▏ | 478/585 [03:21<00:51,  2.09it/s] 82%|████████▏ | 479/585 [03:22<00:44,  2.37it/s] 82%|████████▏ | 480/585 [03:22<00:40,  2.61it/s] 82%|████████▏ | 481/585 [03:22<00:37,  2.81it/s] 82%|████████▏ | 482/585 [03:22<00:34,  2.97it/s] 83%|████████▎ | 483/585 [03:23<00:32,  3.10it/s] 83%|████████▎ | 484/585 [03:23<00:31,  3.20it/s] 83%|████████▎ | 485/585 [03:23<00:30,  3.27it/s] 83%|████████▎ | 486/585 [03:24<00:29,  3.32it/s] 83%|████████▎ | 487/585 [03:24<00:29,  3.36it/s] 83%|████████▎ | 488/585 [03:24<00:28,  3.38it/s] 84%|████████▎ | 489/585 [03:24<00:28,  3.40it/s] 84%|████████▍ | 490/585 [03:25<00:27,  3.42it/s] 84%|████████▍ | 491/585 [03:25<00:27,  3.42it/s] 84%|████████▍ | 492/585 [03:25<00:27,  3.41it/s] 84%|████████▍ | 493/585 [03:26<00:26,  3.42it/s] 84%|████████▍ | 494/585 [03:26<00:26,  3.42it/s] 85%|████████▍ | 495/585 [03:26<00:26,  3.43it/s] 85%|████████▍ | 496/585 [03:26<00:25,  3.43it/s] 85%|████████▍ | 497/585 [03:27<00:25,  3.43it/s] 85%|████████▌ | 498/585 [03:27<00:25,  3.44it/s] 85%|████████▌ | 499/585 [03:27<00:24,  3.44it/s] 85%|████████▌ | 500/585 [03:28<00:24,  3.44it/s]                                                  85%|████████▌ | 500/585 [03:28<00:24,  3.44it/s] 86%|████████▌ | 501/585 [03:28<00:24,  3.39it/s] 86%|████████▌ | 502/585 [03:28<00:24,  3.41it/s] 86%|████████▌ | 503/585 [03:28<00:24,  3.41it/s] 86%|████████▌ | 504/585 [03:29<00:23,  3.42it/s] 86%|████████▋ | 505/585 [03:29<00:23,  3.43it/s] 86%|████████▋ | 506/585 [03:29<00:22,  3.44it/s] 87%|████████▋ | 507/585 [03:30<00:22,  3.44it/s] 87%|████████▋ | 508/585 [03:30<00:22,  3.44it/s] 87%|████████▋ | 509/585 [03:30<00:22,  3.44it/s] 87%|████████▋ | 510/585 [03:31<00:21,  3.45it/s] 87%|████████▋ | 511/585 [03:31<00:21,  3.45it/s] 88%|████████▊ | 512/585 [03:31<00:21,  3.44it/s] 88%|████████▊ | 513/585 [03:31<00:20,  3.45it/s] 88%|████████▊ | 514/585 [03:32<00:20,  3.43it/s] 88%|████████▊ | 515/585 [03:32<00:20,  3.44it/s] 88%|████████▊ | 516/585 [03:32<00:20,  3.44it/s] 88%|████████▊ | 517/585 [03:33<00:19,  3.44it/s] 89%|████████▊ | 518/585 [03:33<00:19,  3.44it/s] 89%|████████▊ | 519/585 [03:33<00:19,  3.44it/s] 89%|████████▉ | 520/585 [03:33<00:18,  3.45it/s] 89%|████████▉ | 521/585 [03:34<00:18,  3.45it/s] 89%|████████▉ | 522/585 [03:34<00:18,  3.44it/s] 89%|████████▉ | 523/585 [03:34<00:17,  3.45it/s] 90%|████████▉ | 524/585 [03:35<00:17,  3.45it/s] 90%|████████▉ | 525/585 [03:35<00:17,  3.43it/s] 90%|████████▉ | 526/585 [03:35<00:17,  3.43it/s] 90%|█████████ | 527/585 [03:35<00:16,  3.44it/s] 90%|█████████ | 528/585 [03:36<00:16,  3.44it/s] 90%|█████████ | 529/585 [03:36<00:16,  3.44it/s] 91%|█████████ | 530/585 [03:36<00:15,  3.44it/s] 91%|█████████ | 531/585 [03:37<00:15,  3.44it/s] 91%|█████████ | 532/585 [03:37<00:15,  3.44it/s] 91%|█████████ | 533/585 [03:37<00:15,  3.44it/s] 91%|█████████▏| 534/585 [03:38<00:14,  3.44it/s] 91%|█████████▏| 535/585 [03:38<00:14,  3.45it/s] 92%|█████████▏| 536/585 [03:38<00:14,  3.44it/s] 92%|█████████▏| 537/585 [03:38<00:13,  3.44it/s] 92%|█████████▏| 538/585 [03:39<00:13,  3.44it/s] 92%|█████████▏| 539/585 [03:39<00:13,  3.44it/s] 92%|█████████▏| 540/585 [03:39<00:13,  3.45it/s] 92%|█████████▏| 541/585 [03:40<00:12,  3.45it/s] 93%|█████████▎| 542/585 [03:40<00:12,  3.45it/s] 93%|█████████▎| 543/585 [03:40<00:12,  3.44it/s] 93%|█████████▎| 544/585 [03:40<00:11,  3.45it/s] 93%|█████████▎| 545/585 [03:41<00:11,  3.45it/s] 93%|█████████▎| 546/585 [03:41<00:11,  3.45it/s] 94%|█████████▎| 547/585 [03:41<00:11,  3.43it/s] 94%|█████████▎| 548/585 [03:42<00:10,  3.43it/s] 94%|█████████▍| 549/585 [03:42<00:10,  3.44it/s] 94%|█████████▍| 550/585 [03:42<00:10,  3.44it/s] 94%|█████████▍| 551/585 [03:42<00:09,  3.43it/s] 94%|█████████▍| 552/585 [03:43<00:09,  3.44it/s] 95%|█████████▍| 553/585 [03:43<00:09,  3.44it/s] 95%|█████████▍| 554/585 [03:43<00:09,  3.44it/s] 95%|█████████▍| 555/585 [03:44<00:08,  3.44it/s] 95%|█████████▌| 556/585 [03:44<00:08,  3.44it/s] 95%|█████████▌| 557/585 [03:44<00:08,  3.44it/s] 95%|█████████▌| 558/585 [03:44<00:07,  3.43it/s] 96%|█████████▌| 559/585 [03:45<00:07,  3.43it/s] 96%|█████████▌| 560/585 [03:45<00:07,  3.44it/s] 96%|█████████▌| 561/585 [03:45<00:06,  3.44it/s] 96%|█████████▌| 562/585 [03:46<00:06,  3.44it/s] 96%|█████████▌| 563/585 [03:46<00:06,  3.44it/s] 96%|█████████▋| 564/585 [03:46<00:06,  3.44it/s] 97%|█████████▋| 565/585 [03:47<00:05,  3.44it/s] 97%|█████████▋| 566/585 [03:47<00:05,  3.44it/s] 97%|█████████▋| 567/585 [03:47<00:05,  3.44it/s] 97%|█████████▋| 568/585 [03:47<00:04,  3.44it/s] 97%|█████████▋| 569/585 [03:48<00:04,  3.42it/s] 97%|█████████▋| 570/585 [03:48<00:04,  3.42it/s] 98%|█████████▊| 571/585 [03:48<00:04,  3.43it/s] 98%|█████████▊| 572/585 [03:49<00:03,  3.43it/s] 98%|█████████▊| 573/585 [03:49<00:03,  3.44it/s] 98%|█████████▊| 574/585 [03:49<00:03,  3.44it/s] 98%|█████████▊| 575/585 [03:49<00:02,  3.44it/s] 98%|█████████▊| 576/585 [03:50<00:02,  3.43it/s] 99%|█████████▊| 577/585 [03:50<00:02,  3.43it/s] 99%|█████████▉| 578/585 [03:50<00:02,  3.43it/s] 99%|█████████▉| 579/585 [03:51<00:01,  3.44it/s] 99%|█████████▉| 580/585 [03:51<00:01,  3.36it/s] 99%|█████████▉| 581/585 [03:51<00:01,  3.38it/s] 99%|█████████▉| 582/585 [03:51<00:00,  3.40it/s]100%|█████████▉| 583/585 [03:52<00:00,  3.41it/s]100%|█████████▉| 584/585 [03:52<00:00,  3.42it/s]100%|██████████| 585/585 [03:52<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 01:26:38,160 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:26:38,160 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:26:38,160 >>   Batch size = 8
{'eval_loss': 1.2057173252105713, 'eval_runtime': 9.9518, 'eval_samples_per_second': 350.992, 'eval_steps_per_second': 43.912, 'epoch': 4.0}
{'loss': 0.4083, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.00it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.48it/s][A
  4%|▍         | 17/437 [00:00<00:09, 45.82it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.15it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.65it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.52it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.33it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.21it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.31it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.30it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.11it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.02it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.94it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.94it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.02it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.02it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.04it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.97it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.12it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.07it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.02it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.92it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.96it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.04it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.05it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.01it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.00it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.08it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.05it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.87it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.00it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.00it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.04it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.01it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.03it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.03it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.98it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.04it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.03it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.00it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.91it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.89it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.99it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.06it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.07it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.98it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.08it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.05it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.09it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.91it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.85it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.05it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.07it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.98it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.03it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.05it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.99it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.04it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.99it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.99it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.01it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.90it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.02it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.00it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.95it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.01it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.02it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.95it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.94it/s][A
 81%|████████  | 352/437 [00:07<00:01, 43.99it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.02it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.07it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.05it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.03it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.02it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.96it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.95it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 43.98it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 43.94it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.06it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.06it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.07it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.05it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.99it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.88it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.96it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.93it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 43.93it/s][A100%|██████████| 585/585 [04:02<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:26:48,109 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 01:26:48,129 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:26:49,860 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:26:49,873 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:26:49,887 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:26:53,188 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:26:53,190 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117 (score: 1.1672658920288086).
                                                 100%|██████████| 585/585 [04:10<00:00,  3.43it/s]100%|██████████| 585/585 [04:10<00:00,  2.34it/s]
[INFO|trainer.py:1894] 2023-08-29 01:26:55,380 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 01:26:55,404 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:26:57,693 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:26:57,738 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:26:57,785 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:26:58,059 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:26:58,060 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:26:58,060 >>   train_loss               =     0.4051
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:26:58,060 >>   train_runtime            = 0:04:10.07
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:26:58,060 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:26:58,060 >>   train_samples_per_second =    149.953
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:26:58,060 >>   train_steps_per_second   =      2.339
{'eval_loss': 1.2144839763641357, 'eval_runtime': 9.9259, 'eval_samples_per_second': 351.907, 'eval_steps_per_second': 44.026, 'epoch': 5.0}
{'train_runtime': 250.0778, 'train_samples_per_second': 149.953, 'train_steps_per_second': 2.339, 'train_loss': 0.4051273378551516, 'epoch': 5.0}
08/29/2023 01:26:58 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:26:58,149 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:26:58,149 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 01:26:58,149 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.41it/s]  3%|▎         | 12/437 [00:00<00:08, 48.52it/s]  4%|▍         | 17/437 [00:00<00:08, 47.17it/s]  5%|▌         | 22/437 [00:00<00:08, 46.34it/s]  6%|▌         | 27/437 [00:00<00:08, 45.91it/s]  7%|▋         | 32/437 [00:00<00:08, 45.45it/s]  8%|▊         | 37/437 [00:00<00:08, 45.11it/s] 10%|▉         | 42/437 [00:00<00:08, 44.63it/s] 11%|█         | 47/437 [00:01<00:08, 44.06it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.91it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.14it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.40it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.48it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.56it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.62it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.39it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.15it/s] 21%|██        | 92/437 [00:02<00:07, 43.72it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.87it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.16it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.34it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.44it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.54it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.49it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.30it/s] 30%|███       | 132/437 [00:02<00:06, 44.11it/s] 31%|███▏      | 137/437 [00:03<00:06, 43.84it/s] 32%|███▏      | 142/437 [00:03<00:06, 43.90it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.13it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.31it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.38it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.52it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.51it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.29it/s] 41%|████      | 177/437 [00:03<00:05, 44.15it/s] 42%|████▏     | 182/437 [00:04<00:05, 43.94it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.97it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.03it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.22it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.35it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.49it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.30it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.26it/s] 51%|█████     | 222/437 [00:04<00:04, 44.24it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.03it/s] 53%|█████▎    | 232/437 [00:05<00:04, 43.92it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.07it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.28it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.41it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.44it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.28it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.31it/s] 61%|██████    | 267/437 [00:06<00:03, 44.09it/s] 62%|██████▏   | 272/437 [00:06<00:03, 43.99it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.04it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.05it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.29it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.48it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.44it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.26it/s] 70%|███████   | 307/437 [00:06<00:02, 44.26it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.03it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.07it/s] 74%|███████▎  | 322/437 [00:07<00:02, 43.96it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.09it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.26it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.42it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.39it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.24it/s] 81%|████████  | 352/437 [00:07<00:01, 44.14it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.18it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.04it/s] 84%|████████▍ | 367/437 [00:08<00:01, 43.98it/s] 85%|████████▌ | 372/437 [00:08<00:01, 43.99it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.24it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.35it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.18it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.23it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.17it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.14it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.10it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.06it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.05it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.24it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.26it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.34it/s]100%|██████████| 437/437 [00:09<00:00, 44.23it/s]100%|██████████| 437/437 [00:09<00:00, 44.33it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:27:08,025 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:27:08,025 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:27:08,025 >>   eval_loss               =     1.1673
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:27:08,025 >>   eval_runtime            = 0:00:09.87
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:27:08,025 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:27:08,025 >>   eval_samples_per_second =    353.704
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:27:08,025 >>   eval_steps_per_second   =     44.251
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:27:08,025 >>   perplexity              =     3.2132
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:13,933 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:13,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:13,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:13,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:13,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:27:14,239 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:27:14,240 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:27:14,500 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:27:15,575 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:27:15,577 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:18,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:18,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:18,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:18,495 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:27:18,495 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:27:18,833 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:27:18,837 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:27:19,097 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:27:19,258 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:27:19,258 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.52it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:06,  1.56it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:09,  1.64it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:14,  1.61it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:17,  1.55it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:19,  1.64it/s]Extractor Predicting: 31it [00:19,  1.64it/s]Extractor Predicting: 32it [00:20,  1.64it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.56it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.60it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:24,  1.62it/s]Extractor Predicting: 40it [00:25,  1.59it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:26,  1.64it/s]Extractor Predicting: 43it [00:27,  1.60it/s]Extractor Predicting: 44it [00:27,  1.64it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:29,  1.60it/s]Extractor Predicting: 47it [00:29,  1.59it/s]Extractor Predicting: 48it [00:30,  1.65it/s]Extractor Predicting: 49it [00:30,  1.66it/s]Extractor Predicting: 50it [00:31,  1.62it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:32,  1.62it/s]Extractor Predicting: 53it [00:33,  1.65it/s]Extractor Predicting: 54it [00:33,  1.66it/s]Extractor Predicting: 55it [00:34,  1.66it/s]Extractor Predicting: 56it [00:35,  1.63it/s]Extractor Predicting: 57it [00:35,  1.64it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:37,  1.61it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:38,  1.58it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:40,  1.58it/s]Extractor Predicting: 66it [00:41,  1.58it/s]Extractor Predicting: 67it [00:42,  1.56it/s]Extractor Predicting: 68it [00:42,  1.53it/s]Extractor Predicting: 69it [00:43,  1.56it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:44,  1.59it/s]Extractor Predicting: 72it [00:45,  1.62it/s]Extractor Predicting: 73it [00:45,  1.60it/s]Extractor Predicting: 74it [00:46,  1.58it/s]Extractor Predicting: 75it [00:47,  1.61it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:49,  1.56it/s]Extractor Predicting: 79it [00:49,  1.57it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.55it/s]Extractor Predicting: 82it [00:51,  1.56it/s]Extractor Predicting: 83it [00:52,  1.60it/s]Extractor Predicting: 84it [00:52,  1.62it/s]Extractor Predicting: 85it [00:53,  1.60it/s]Extractor Predicting: 86it [00:54,  1.54it/s]Extractor Predicting: 87it [00:54,  1.59it/s]Extractor Predicting: 88it [00:55,  1.59it/s]Extractor Predicting: 89it [00:56,  1.58it/s]Extractor Predicting: 90it [00:56,  1.54it/s]Extractor Predicting: 91it [00:57,  1.53it/s]Extractor Predicting: 92it [00:58,  1.55it/s]Extractor Predicting: 93it [00:58,  1.55it/s]Extractor Predicting: 94it [00:59,  1.58it/s]Extractor Predicting: 95it [00:59,  1.62it/s]Extractor Predicting: 96it [01:00,  1.62it/s]Extractor Predicting: 97it [01:01,  1.59it/s]Extractor Predicting: 98it [01:01,  1.59it/s]Extractor Predicting: 99it [01:02,  1.58it/s]Extractor Predicting: 100it [01:02,  1.61it/s]Extractor Predicting: 101it [01:03,  1.60it/s]Extractor Predicting: 102it [01:04,  1.45it/s]Extractor Predicting: 103it [01:05,  1.48it/s]Extractor Predicting: 104it [01:05,  1.50it/s]Extractor Predicting: 105it [01:06,  1.53it/s]Extractor Predicting: 106it [01:07,  1.51it/s]Extractor Predicting: 107it [01:07,  1.53it/s]Extractor Predicting: 108it [01:08,  1.53it/s]Extractor Predicting: 109it [01:08,  1.54it/s]Extractor Predicting: 110it [01:09,  1.53it/s]Extractor Predicting: 111it [01:10,  1.53it/s]Extractor Predicting: 112it [01:10,  1.53it/s]Extractor Predicting: 113it [01:11,  1.51it/s]Extractor Predicting: 114it [01:12,  1.50it/s]Extractor Predicting: 115it [01:12,  1.49it/s]Extractor Predicting: 116it [01:13,  1.50it/s]Extractor Predicting: 117it [01:14,  1.49it/s]Extractor Predicting: 118it [01:14,  1.50it/s]Extractor Predicting: 119it [01:15,  1.50it/s]Extractor Predicting: 120it [01:16,  1.51it/s]Extractor Predicting: 121it [01:16,  1.49it/s]Extractor Predicting: 122it [01:17,  1.49it/s]Extractor Predicting: 123it [01:18,  1.55it/s]Extractor Predicting: 124it [01:18,  1.54it/s]Extractor Predicting: 125it [01:19,  1.51it/s]Extractor Predicting: 126it [01:20,  1.52it/s]Extractor Predicting: 127it [01:20,  1.53it/s]Extractor Predicting: 128it [01:21,  1.53it/s]Extractor Predicting: 129it [01:22,  1.51it/s]Extractor Predicting: 130it [01:22,  1.51it/s]Extractor Predicting: 131it [01:23,  1.48it/s]Extractor Predicting: 132it [01:24,  1.47it/s]Extractor Predicting: 133it [01:24,  1.47it/s]Extractor Predicting: 134it [01:25,  1.46it/s]Extractor Predicting: 135it [01:26,  1.48it/s]Extractor Predicting: 136it [01:26,  1.81it/s]Extractor Predicting: 136it [01:26,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:54,166 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:54,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:54,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:54,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:54,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:28:54,805 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:28:54,806 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:28:55,364 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:28:56,403 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:28:56,403 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:59,379 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:59,382 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:59,382 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:59,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:28:59,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:29:00,017 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:29:00,018 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:29:00,590 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:29:00,749 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:29:00,750 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4114521841794569,
  "recall": 0.19954194102490697,
  "score": 0.26874879506458454,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.68it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.67it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:16,  1.71it/s]Extractor Predicting: 29it [00:17,  1.68it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.73it/s]Extractor Predicting: 33it [00:19,  1.75it/s]Extractor Predicting: 34it [00:20,  1.73it/s]Extractor Predicting: 35it [00:21,  1.70it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:22,  1.62it/s]Extractor Predicting: 39it [00:23,  1.65it/s]Extractor Predicting: 40it [00:24,  1.70it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.70it/s]Extractor Predicting: 43it [00:25,  1.72it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:27,  1.68it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:28,  1.70it/s]Extractor Predicting: 49it [00:29,  1.71it/s]Extractor Predicting: 50it [00:29,  1.73it/s]Extractor Predicting: 51it [00:30,  1.77it/s]Extractor Predicting: 52it [00:31,  1.77it/s]Extractor Predicting: 53it [00:31,  1.73it/s]Extractor Predicting: 54it [00:32,  1.70it/s]Extractor Predicting: 55it [00:32,  1.72it/s]Extractor Predicting: 56it [00:33,  1.68it/s]Extractor Predicting: 57it [00:34,  1.68it/s]Extractor Predicting: 58it [00:34,  1.70it/s]Extractor Predicting: 59it [00:35,  1.67it/s]Extractor Predicting: 60it [00:35,  1.66it/s]Extractor Predicting: 61it [00:36,  1.68it/s]Extractor Predicting: 62it [00:37,  1.68it/s]Extractor Predicting: 63it [00:37,  1.64it/s]Extractor Predicting: 64it [00:38,  1.71it/s]Extractor Predicting: 65it [00:38,  1.69it/s]Extractor Predicting: 66it [00:39,  1.68it/s]Extractor Predicting: 67it [00:40,  1.69it/s]Extractor Predicting: 68it [00:40,  1.74it/s]Extractor Predicting: 69it [00:41,  1.75it/s]Extractor Predicting: 70it [00:41,  1.74it/s]Extractor Predicting: 71it [00:42,  1.74it/s]Extractor Predicting: 72it [00:42,  1.70it/s]Extractor Predicting: 73it [00:43,  1.72it/s]Extractor Predicting: 74it [00:44,  1.68it/s]Extractor Predicting: 75it [00:44,  1.68it/s]Extractor Predicting: 76it [00:45,  1.69it/s]Extractor Predicting: 77it [00:45,  1.67it/s]Extractor Predicting: 78it [00:46,  1.68it/s]Extractor Predicting: 79it [00:47,  1.68it/s]Extractor Predicting: 80it [00:47,  1.70it/s]Extractor Predicting: 81it [00:48,  1.69it/s]Extractor Predicting: 82it [00:48,  1.71it/s]Extractor Predicting: 83it [00:49,  1.66it/s]Extractor Predicting: 84it [00:50,  1.65it/s]Extractor Predicting: 85it [00:50,  1.63it/s]Extractor Predicting: 86it [00:51,  1.61it/s]Extractor Predicting: 87it [00:51,  1.66it/s]Extractor Predicting: 88it [00:52,  1.62it/s]Extractor Predicting: 89it [00:53,  1.60it/s]Extractor Predicting: 90it [00:53,  1.58it/s]Extractor Predicting: 91it [00:54,  1.59it/s]Extractor Predicting: 92it [00:55,  1.57it/s]Extractor Predicting: 93it [00:55,  1.56it/s]Extractor Predicting: 94it [00:56,  1.57it/s]Extractor Predicting: 95it [00:56,  1.60it/s]Extractor Predicting: 96it [00:57,  1.59it/s]Extractor Predicting: 97it [00:58,  1.57it/s]Extractor Predicting: 98it [00:58,  1.57it/s]Extractor Predicting: 99it [00:59,  1.55it/s]Extractor Predicting: 100it [01:00,  1.40it/s]Extractor Predicting: 101it [01:01,  1.43it/s]Extractor Predicting: 102it [01:01,  1.45it/s]Extractor Predicting: 103it [01:02,  1.50it/s]Extractor Predicting: 104it [01:03,  1.52it/s]Extractor Predicting: 105it [01:03,  1.54it/s]Extractor Predicting: 106it [01:04,  1.51it/s]Extractor Predicting: 107it [01:05,  1.51it/s]Extractor Predicting: 108it [01:05,  1.49it/s]Extractor Predicting: 109it [01:06,  1.50it/s]Extractor Predicting: 110it [01:07,  1.50it/s]Extractor Predicting: 111it [01:07,  1.52it/s]Extractor Predicting: 112it [01:08,  1.51it/s]Extractor Predicting: 113it [01:09,  1.51it/s]Extractor Predicting: 114it [01:09,  1.50it/s]Extractor Predicting: 115it [01:10,  1.48it/s]Extractor Predicting: 116it [01:11,  1.49it/s]Extractor Predicting: 117it [01:11,  1.50it/s]Extractor Predicting: 118it [01:12,  1.52it/s]Extractor Predicting: 119it [01:12,  1.54it/s]Extractor Predicting: 120it [01:13,  1.59it/s]Extractor Predicting: 121it [01:14,  1.58it/s]Extractor Predicting: 122it [01:14,  1.57it/s]Extractor Predicting: 123it [01:15,  1.57it/s]Extractor Predicting: 124it [01:16,  1.55it/s]Extractor Predicting: 125it [01:16,  1.58it/s]Extractor Predicting: 126it [01:17,  1.62it/s]Extractor Predicting: 127it [01:17,  1.62it/s]Extractor Predicting: 128it [01:18,  1.64it/s]Extractor Predicting: 129it [01:19,  1.61it/s]Extractor Predicting: 130it [01:19,  1.56it/s]Extractor Predicting: 131it [01:20,  1.58it/s]Extractor Predicting: 132it [01:21,  1.58it/s]Extractor Predicting: 133it [01:21,  1.62it/s]Extractor Predicting: 134it [01:22,  1.64it/s]Extractor Predicting: 135it [01:22,  1.62it/s]Extractor Predicting: 136it [01:23,  1.64it/s]Extractor Predicting: 137it [01:24,  1.66it/s]Extractor Predicting: 138it [01:24,  1.63it/s]Extractor Predicting: 139it [01:25,  1.63it/s]Extractor Predicting: 140it [01:25,  1.63it/s]Extractor Predicting: 141it [01:26,  1.58it/s]Extractor Predicting: 142it [01:27,  1.58it/s]Extractor Predicting: 143it [01:27,  1.59it/s]Extractor Predicting: 144it [01:28,  1.58it/s]Extractor Predicting: 145it [01:29,  1.57it/s]Extractor Predicting: 146it [01:29,  1.61it/s]Extractor Predicting: 147it [01:30,  1.63it/s]Extractor Predicting: 148it [01:31,  1.61it/s]Extractor Predicting: 149it [01:31,  1.61it/s]Extractor Predicting: 150it [01:32,  1.60it/s]Extractor Predicting: 151it [01:32,  1.56it/s]Extractor Predicting: 152it [01:33,  1.57it/s]Extractor Predicting: 153it [01:34,  1.57it/s]Extractor Predicting: 154it [01:34,  1.61it/s]Extractor Predicting: 155it [01:35,  1.58it/s]Extractor Predicting: 156it [01:36,  1.58it/s]Extractor Predicting: 157it [01:36,  1.55it/s]Extractor Predicting: 158it [01:37,  1.57it/s]Extractor Predicting: 159it [01:38,  1.55it/s]Extractor Predicting: 160it [01:38,  1.54it/s]Extractor Predicting: 161it [01:39,  1.56it/s]Extractor Predicting: 162it [01:39,  1.55it/s]Extractor Predicting: 163it [01:40,  1.55it/s]Extractor Predicting: 164it [01:41,  1.56it/s]Extractor Predicting: 165it [01:41,  1.56it/s]Extractor Predicting: 166it [01:42,  1.57it/s]Extractor Predicting: 167it [01:43,  1.55it/s]Extractor Predicting: 168it [01:43,  1.55it/s]Extractor Predicting: 169it [01:44,  1.55it/s]Extractor Predicting: 170it [01:45,  1.54it/s]Extractor Predicting: 171it [01:45,  1.55it/s]Extractor Predicting: 172it [01:46,  1.54it/s]Extractor Predicting: 173it [01:47,  1.54it/s]Extractor Predicting: 174it [01:47,  1.54it/s]Extractor Predicting: 175it [01:48,  1.59it/s]Extractor Predicting: 176it [01:48,  1.58it/s]Extractor Predicting: 177it [01:49,  1.60it/s]Extractor Predicting: 178it [01:50,  1.61it/s]Extractor Predicting: 179it [01:50,  1.63it/s]Extractor Predicting: 180it [01:51,  1.45it/s]Extractor Predicting: 181it [01:52,  1.50it/s]Extractor Predicting: 182it [01:52,  1.55it/s]Extractor Predicting: 183it [01:53,  1.52it/s]Extractor Predicting: 184it [01:54,  1.60it/s]Extractor Predicting: 185it [01:54,  1.60it/s]Extractor Predicting: 186it [01:55,  1.61it/s]Extractor Predicting: 187it [01:55,  1.62it/s]Extractor Predicting: 188it [01:56,  1.58it/s]Extractor Predicting: 189it [01:57,  1.58it/s]Extractor Predicting: 190it [01:57,  1.58it/s]Extractor Predicting: 191it [01:58,  1.56it/s]Extractor Predicting: 192it [01:59,  1.56it/s]Extractor Predicting: 193it [01:59,  1.60it/s]Extractor Predicting: 194it [02:00,  1.60it/s]Extractor Predicting: 195it [02:01,  1.57it/s]Extractor Predicting: 196it [02:01,  1.53it/s]Extractor Predicting: 197it [02:02,  1.56it/s]Extractor Predicting: 198it [02:02,  1.55it/s]Extractor Predicting: 199it [02:03,  1.58it/s]Extractor Predicting: 200it [02:04,  1.57it/s]Extractor Predicting: 201it [02:04,  1.57it/s]Extractor Predicting: 202it [02:05,  1.58it/s]Extractor Predicting: 203it [02:06,  1.59it/s]Extractor Predicting: 204it [02:06,  1.62it/s]Extractor Predicting: 205it [02:07,  1.60it/s]Extractor Predicting: 206it [02:07,  1.63it/s]Extractor Predicting: 207it [02:08,  1.60it/s]Extractor Predicting: 208it [02:09,  1.59it/s]Extractor Predicting: 209it [02:09,  1.60it/s]Extractor Predicting: 210it [02:10,  1.61it/s]Extractor Predicting: 211it [02:11,  1.59it/s]Extractor Predicting: 212it [02:11,  1.61it/s]Extractor Predicting: 213it [02:12,  1.62it/s]Extractor Predicting: 214it [02:12,  1.61it/s]Extractor Predicting: 215it [02:13,  1.61it/s]Extractor Predicting: 216it [02:14,  1.61it/s]Extractor Predicting: 217it [02:14,  1.59it/s]Extractor Predicting: 218it [02:15,  1.61it/s]Extractor Predicting: 219it [02:16,  1.57it/s]Extractor Predicting: 220it [02:16,  1.60it/s]Extractor Predicting: 221it [02:17,  1.63it/s]Extractor Predicting: 222it [02:17,  1.63it/s]Extractor Predicting: 223it [02:18,  1.59it/s]Extractor Predicting: 224it [02:19,  1.64it/s]Extractor Predicting: 225it [02:19,  1.61it/s]Extractor Predicting: 226it [02:20,  1.60it/s]Extractor Predicting: 227it [02:21,  1.61it/s]Extractor Predicting: 228it [02:21,  1.63it/s]Extractor Predicting: 229it [02:22,  1.62it/s]Extractor Predicting: 230it [02:22,  1.64it/s]Extractor Predicting: 231it [02:23,  1.65it/s]Extractor Predicting: 232it [02:24,  1.64it/s]Extractor Predicting: 233it [02:24,  1.65it/s]Extractor Predicting: 234it [02:25,  1.63it/s]Extractor Predicting: 235it [02:25,  1.60it/s]Extractor Predicting: 236it [02:26,  1.60it/s]Extractor Predicting: 237it [02:27,  1.60it/s]Extractor Predicting: 238it [02:27,  1.60it/s]Extractor Predicting: 239it [02:28,  1.58it/s]Extractor Predicting: 240it [02:29,  1.58it/s]Extractor Predicting: 241it [02:29,  1.59it/s]Extractor Predicting: 242it [02:30,  1.59it/s]Extractor Predicting: 243it [02:30,  1.60it/s]Extractor Predicting: 244it [02:31,  1.57it/s]Extractor Predicting: 245it [02:32,  1.59it/s]Extractor Predicting: 246it [02:32,  1.59it/s]Extractor Predicting: 247it [02:33,  1.60it/s]Extractor Predicting: 248it [02:34,  1.60it/s]Extractor Predicting: 249it [02:34,  1.61it/s]Extractor Predicting: 250it [02:35,  1.61it/s]Extractor Predicting: 251it [02:35,  1.65it/s]Extractor Predicting: 252it [02:36,  1.63it/s]Extractor Predicting: 253it [02:37,  1.62it/s]Extractor Predicting: 254it [02:37,  1.62it/s]Extractor Predicting: 255it [02:38,  1.61it/s]Extractor Predicting: 256it [02:39,  1.57it/s]Extractor Predicting: 257it [02:39,  1.59it/s]Extractor Predicting: 258it [02:40,  1.59it/s]Extractor Predicting: 259it [02:40,  1.63it/s]Extractor Predicting: 260it [02:41,  1.62it/s]Extractor Predicting: 261it [02:42,  1.64it/s]Extractor Predicting: 262it [02:42,  1.63it/s]Extractor Predicting: 263it [02:43,  1.62it/s]Extractor Predicting: 264it [02:44,  1.51it/s]Extractor Predicting: 265it [02:44,  1.54it/s]Extractor Predicting: 266it [02:45,  1.58it/s]Extractor Predicting: 267it [02:45,  1.59it/s]Extractor Predicting: 268it [02:46,  1.59it/s]Extractor Predicting: 269it [02:47,  1.59it/s]Extractor Predicting: 270it [02:47,  1.57it/s]Extractor Predicting: 271it [02:48,  1.55it/s]Extractor Predicting: 272it [02:49,  1.57it/s]Extractor Predicting: 273it [02:49,  1.55it/s]Extractor Predicting: 274it [02:50,  1.54it/s]Extractor Predicting: 275it [02:51,  1.51it/s]Extractor Predicting: 276it [02:51,  1.54it/s]Extractor Predicting: 277it [02:52,  1.54it/s]Extractor Predicting: 278it [02:53,  1.58it/s]Extractor Predicting: 279it [02:53,  1.57it/s]Extractor Predicting: 280it [02:54,  1.57it/s]Extractor Predicting: 281it [02:54,  1.59it/s]Extractor Predicting: 282it [02:55,  1.58it/s]Extractor Predicting: 283it [02:56,  1.57it/s]Extractor Predicting: 284it [02:56,  1.57it/s]Extractor Predicting: 285it [02:57,  1.56it/s]Extractor Predicting: 286it [02:58,  1.61it/s]Extractor Predicting: 287it [02:58,  1.56it/s]Extractor Predicting: 288it [02:59,  1.78it/s]Extractor Predicting: 288it [02:59,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:07,874 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:07,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:07,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:07,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:07,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:32:08,487 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:32:08,488 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:32:09,090 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:32:10,101 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:32:10,102 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:12,957 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:12,960 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:12,960 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:12,961 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:32:12,961 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:32:13,611 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:32:13,612 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:32:14,180 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:32:14,343 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:32:14,343 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.29692832764505117,
  "recall": 0.13869565217391305,
  "score": 0.18907438506371632,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  2.39it/s]Extractor Predicting: 3it [00:01,  2.06it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5384615384615384,
  "recall": 0.07,
  "score": 0.12389380530973453,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
