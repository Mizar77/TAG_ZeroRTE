Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:17<04:01, 17.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:37, 16.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:49<03:17, 16.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:08<03:11, 17.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:25<02:52, 17.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:42<02:33, 17.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:00<02:20, 17.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:20<02:06, 18.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:36<01:45, 17.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:52<01:24, 16.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:08<01:06, 16.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:28<00:53, 17.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:46<00:35, 17.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:03<00:17, 17.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:21<00:00, 17.72s/it]Generating: 100%|██████████| 15/15 [04:21<00:00, 17.44s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : characters . Context : Later in Life , he played the title character , a young princess of the family at the end of the third season of HBO s Game of Thrones . Head Entity : Game of Thrones , Tail Entity : Princess of the family .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.75375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.7525, 'errors': {'', "('pharaoh Amenhotep II', 'made from material', '', 'He is best remembered for the painting of the pharaoh Amenhotep II , composed by Alfred Wohl in 1827 for a French conservatory .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : cast member .', 'success_rate': 0.7838541666666666, 'errors': {'', "('Billboard Pop Hits', 'cast member', '', 'On October 25 , 1978 , the band released the track The Lads , the debut single by the British band The Riveters ( now defunct ) , on Columbia Records , along with several appearances on the Billboard Pop Hits chart for June .')"}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 433, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 506, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : follows .', 'success_rate': 0.7283653846153846, 'errors': {'', "('Nigeria', 'follows', '', 'In 2013 , a second poll showed Zaire had a 7 % approval rating , while in 2014 , Nigeria had a 24 % approval rating .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 230, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 281, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 366, 'raw': 576}
{'target': 600, 'success': 391, 'raw': 608}
{'target': 600, 'success': 410, 'raw': 640}
{'target': 600, 'success': 431, 'raw': 672}
{'target': 600, 'success': 452, 'raw': 704}
{'target': 600, 'success': 470, 'raw': 736}
{'target': 600, 'success': 490, 'raw': 768}
{'target': 600, 'success': 511, 'raw': 800}
{'target': 600, 'success': 531, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 590, 'raw': 928}
{'target': 600, 'success': 608, 'raw': 960}
{'prompt': 'Relation : league .', 'success_rate': 0.6333333333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.765, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in Life , he studied with a group of Latin philologists and other thinkers at the Renaissance in Italy , in Naples from 1635 to 1645 , and from 1654 back to 1701 . Head Entity : Naples , Tail Entity : Rome .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 175, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 218, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 262, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 305, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 341, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 412, 'raw': 608}
{'target': 600, 'success': 437, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 501, 'raw': 736}
{'target': 600, 'success': 524, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 564, 'raw': 832}
{'target': 600, 'success': 584, 'raw': 864}
{'target': 600, 'success': 603, 'raw': 896}
{'prompt': 'Relation : mother .', 'success_rate': 0.6729910714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : residence .', 'success_rate': 0.7283653846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.7439903846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in 1453 the head of the province of Ghent and of neighboring districts at Neuromö , in Burgundesland , were also under the Ottoman Empire . Head Entity : Burgundesland , Tail Entity : Ottomans .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 564, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 15273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.81s/it]Extractor Estimating: 2it [00:19,  8.09s/it]Extractor Estimating: 3it [00:19,  4.68s/it]Extractor Estimating: 4it [00:20,  3.09s/it]Extractor Estimating: 5it [00:20,  2.20s/it]Extractor Estimating: 6it [00:23,  2.19s/it]Extractor Estimating: 7it [00:23,  1.67s/it]Extractor Estimating: 8it [00:24,  1.53s/it]Extractor Estimating: 9it [00:25,  1.28s/it]Extractor Estimating: 10it [00:26,  1.09s/it]Extractor Estimating: 11it [00:27,  1.01it/s]Extractor Estimating: 12it [00:27,  1.14it/s]Extractor Estimating: 13it [00:28,  1.24it/s]Extractor Estimating: 14it [00:29,  1.28it/s]Extractor Estimating: 15it [00:29,  1.35it/s]Extractor Estimating: 16it [00:30,  1.30it/s]Extractor Estimating: 17it [00:31,  1.36it/s]Extractor Estimating: 18it [00:31,  1.44it/s]Extractor Estimating: 19it [00:32,  1.40it/s]Extractor Estimating: 20it [00:33,  1.47it/s]Extractor Estimating: 21it [00:33,  1.50it/s]Extractor Estimating: 22it [00:34,  1.58it/s]Extractor Estimating: 23it [00:35,  1.53it/s]Extractor Estimating: 24it [00:36,  1.38it/s]Extractor Estimating: 25it [00:36,  1.43it/s]Extractor Estimating: 26it [00:37,  1.48it/s]Extractor Estimating: 27it [00:37,  1.51it/s]Extractor Estimating: 28it [00:38,  1.57it/s]Extractor Estimating: 29it [00:39,  1.57it/s]Extractor Estimating: 30it [00:39,  1.60it/s]Extractor Estimating: 31it [00:40,  1.57it/s]Extractor Estimating: 32it [00:41,  1.48it/s]Extractor Estimating: 33it [00:41,  1.44it/s]Extractor Estimating: 34it [00:42,  1.45it/s]Extractor Estimating: 35it [00:43,  1.48it/s]Extractor Estimating: 36it [00:43,  1.48it/s]Extractor Estimating: 37it [00:44,  1.51it/s]Extractor Estimating: 38it [00:45,  1.53it/s]Extractor Estimating: 39it [00:46,  1.40it/s]Extractor Estimating: 40it [00:46,  1.43it/s]Extractor Estimating: 41it [00:47,  1.48it/s]Extractor Estimating: 42it [00:48,  1.36it/s]Extractor Estimating: 43it [00:50,  1.11s/it]Extractor Estimating: 44it [00:50,  1.02it/s]Extractor Estimating: 45it [00:51,  1.02it/s]Extractor Estimating: 46it [00:52,  1.15it/s]Extractor Estimating: 47it [00:53,  1.24it/s]Extractor Estimating: 48it [00:53,  1.32it/s]Extractor Estimating: 49it [00:54,  1.36it/s]Extractor Estimating: 50it [00:55,  1.37it/s]Extractor Estimating: 51it [00:55,  1.40it/s]Extractor Estimating: 52it [00:56,  1.48it/s]Extractor Estimating: 53it [00:57,  1.51it/s]Extractor Estimating: 54it [00:57,  1.51it/s]Extractor Estimating: 55it [00:58,  1.50it/s]Extractor Estimating: 56it [00:58,  1.53it/s]Extractor Estimating: 57it [00:59,  1.51it/s]Extractor Estimating: 58it [01:00,  1.53it/s]Extractor Estimating: 59it [01:00,  1.54it/s]Extractor Estimating: 60it [01:01,  1.45it/s]Extractor Estimating: 61it [01:02,  1.48it/s]Extractor Estimating: 62it [01:02,  1.52it/s]Extractor Estimating: 63it [01:03,  1.35it/s]Extractor Estimating: 64it [01:04,  1.43it/s]Extractor Estimating: 65it [01:05,  1.48it/s]Extractor Estimating: 66it [01:05,  1.50it/s]Extractor Estimating: 67it [01:06,  1.53it/s]Extractor Estimating: 68it [01:07,  1.44it/s]Extractor Estimating: 69it [01:07,  1.49it/s]Extractor Estimating: 70it [01:08,  1.52it/s]Extractor Estimating: 71it [01:09,  1.37it/s]Extractor Estimating: 72it [01:09,  1.44it/s]Extractor Estimating: 73it [01:10,  1.50it/s]Extractor Estimating: 74it [01:11,  1.51it/s]Extractor Estimating: 75it [01:11,  1.52it/s]Extractor Estimating: 76it [01:12,  1.36it/s]Extractor Estimating: 77it [01:13,  1.43it/s]Extractor Estimating: 78it [01:14,  1.47it/s]Extractor Estimating: 79it [01:15,  1.23it/s]Extractor Estimating: 80it [01:15,  1.26it/s]Extractor Estimating: 81it [01:16,  1.34it/s]Extractor Estimating: 82it [01:17,  1.35it/s]Extractor Estimating: 83it [01:17,  1.34it/s]Extractor Estimating: 84it [01:18,  1.39it/s]Extractor Estimating: 85it [01:19,  1.34it/s]Extractor Estimating: 86it [01:20,  1.43it/s]Extractor Estimating: 87it [01:20,  1.48it/s]Extractor Estimating: 88it [01:21,  1.53it/s]Extractor Estimating: 89it [01:21,  1.53it/s]Extractor Estimating: 90it [01:22,  1.51it/s]Extractor Estimating: 91it [01:23,  1.51it/s]Extractor Estimating: 92it [01:23,  1.51it/s]Extractor Estimating: 93it [01:24,  1.51it/s]Extractor Estimating: 94it [01:25,  1.52it/s]Extractor Estimating: 95it [01:25,  1.51it/s]Extractor Estimating: 96it [01:26,  1.53it/s]Extractor Estimating: 97it [01:27,  1.54it/s]Extractor Estimating: 98it [01:27,  1.45it/s]Extractor Estimating: 99it [01:28,  1.41it/s]Extractor Estimating: 100it [01:29,  1.47it/s]Extractor Estimating: 101it [01:30,  1.45it/s]Extractor Estimating: 102it [01:30,  1.48it/s]Extractor Estimating: 103it [01:31,  1.53it/s]Extractor Estimating: 104it [01:31,  1.54it/s]Extractor Estimating: 105it [01:32,  1.54it/s]Extractor Estimating: 106it [01:33,  1.51it/s]Extractor Estimating: 107it [01:33,  1.54it/s]Extractor Estimating: 108it [01:34,  1.58it/s]Extractor Estimating: 109it [01:35,  1.60it/s]Extractor Estimating: 110it [01:35,  1.56it/s]Extractor Estimating: 111it [01:36,  1.59it/s]Extractor Estimating: 112it [01:37,  1.58it/s]Extractor Estimating: 113it [01:37,  1.60it/s]Extractor Estimating: 114it [01:38,  1.60it/s]Extractor Estimating: 115it [01:39,  1.36it/s]Extractor Estimating: 116it [01:39,  1.38it/s]Extractor Estimating: 117it [01:41,  1.03it/s]Extractor Estimating: 118it [01:42,  1.13it/s]Extractor Estimating: 119it [01:42,  1.24it/s]Extractor Estimating: 120it [01:43,  1.29it/s]Extractor Estimating: 121it [01:44,  1.23it/s]Extractor Estimating: 122it [01:45,  1.31it/s]Extractor Estimating: 123it [01:45,  1.40it/s]Extractor Estimating: 124it [01:46,  1.32it/s]Extractor Estimating: 125it [01:47,  1.08it/s]Extractor Estimating: 126it [01:48,  1.18it/s]Extractor Estimating: 127it [01:49,  1.26it/s]Extractor Estimating: 128it [01:49,  1.31it/s]Extractor Estimating: 129it [01:50,  1.42it/s]Extractor Estimating: 130it [01:51,  1.43it/s]Extractor Estimating: 131it [01:51,  1.51it/s]Extractor Estimating: 132it [01:52,  1.50it/s]Extractor Estimating: 133it [01:52,  1.52it/s]Extractor Estimating: 134it [01:53,  1.52it/s]Extractor Estimating: 135it [01:54,  1.51it/s]Extractor Estimating: 136it [01:55,  1.43it/s]Extractor Estimating: 137it [01:55,  1.43it/s]Extractor Estimating: 138it [01:56,  1.48it/s]Extractor Estimating: 139it [01:57,  1.42it/s]Extractor Estimating: 140it [01:57,  1.48it/s]Extractor Estimating: 141it [01:58,  1.50it/s]Extractor Estimating: 142it [01:59,  1.50it/s]Extractor Estimating: 143it [01:59,  1.55it/s]Extractor Estimating: 144it [02:00,  1.59it/s]Extractor Estimating: 145it [02:01,  1.53it/s]Extractor Estimating: 146it [02:01,  1.48it/s]Extractor Estimating: 147it [02:02,  1.53it/s]Extractor Estimating: 148it [02:02,  1.58it/s]Extractor Estimating: 149it [02:03,  1.56it/s]Extractor Estimating: 150it [02:04,  1.55it/s]Extractor Estimating: 151it [02:05,  1.36it/s]Extractor Estimating: 152it [02:05,  1.40it/s]Extractor Estimating: 153it [02:06,  1.42it/s]Extractor Estimating: 154it [02:07,  1.47it/s]Extractor Estimating: 155it [02:07,  1.50it/s]Extractor Estimating: 156it [02:09,  1.11it/s]Extractor Estimating: 157it [02:09,  1.19it/s]Extractor Estimating: 158it [02:10,  1.28it/s]Extractor Estimating: 159it [02:11,  1.34it/s]Extractor Estimating: 160it [02:12,  1.05it/s]Extractor Estimating: 161it [02:13,  1.16it/s]Extractor Estimating: 162it [02:13,  1.27it/s]Extractor Estimating: 163it [02:14,  1.32it/s]Extractor Estimating: 164it [02:15,  1.27it/s]Extractor Estimating: 165it [02:16,  1.27it/s]Extractor Estimating: 166it [02:16,  1.33it/s]Extractor Estimating: 167it [02:18,  1.18it/s]Extractor Estimating: 168it [02:18,  1.19it/s]Extractor Estimating: 169it [02:19,  1.25it/s]Extractor Estimating: 170it [02:20,  1.34it/s]Extractor Estimating: 171it [02:20,  1.32it/s]Extractor Estimating: 172it [02:21,  1.39it/s]Extractor Estimating: 173it [02:22,  1.44it/s]Extractor Estimating: 174it [02:23,  1.35it/s]Extractor Estimating: 175it [02:23,  1.38it/s]Extractor Estimating: 176it [02:24,  1.46it/s]Extractor Estimating: 177it [02:25,  1.44it/s]Extractor Estimating: 178it [02:25,  1.50it/s]Extractor Estimating: 179it [02:26,  1.44it/s]Extractor Estimating: 180it [02:27,  1.48it/s]Extractor Estimating: 181it [02:27,  1.51it/s]Extractor Estimating: 182it [02:28,  1.21it/s]Extractor Estimating: 183it [02:29,  1.31it/s]Extractor Estimating: 184it [02:30,  1.37it/s]Extractor Estimating: 185it [02:30,  1.42it/s]Extractor Estimating: 186it [02:31,  1.35it/s]Extractor Estimating: 187it [02:32,  1.44it/s]Extractor Estimating: 188it [02:32,  1.51it/s]Extractor Estimating: 189it [02:33,  1.53it/s]Extractor Estimating: 190it [02:34,  1.52it/s]Extractor Estimating: 191it [02:34,  1.55it/s]Extractor Estimating: 192it [02:35,  1.52it/s]Extractor Estimating: 193it [02:35,  1.59it/s]Extractor Estimating: 194it [02:37,  1.08it/s]Extractor Estimating: 195it [02:38,  1.19it/s]Extractor Estimating: 196it [02:38,  1.28it/s]Extractor Estimating: 197it [02:39,  1.39it/s]Extractor Estimating: 198it [02:40,  1.38it/s]Extractor Estimating: 199it [02:40,  1.43it/s]Extractor Estimating: 200it [02:41,  1.48it/s]Extractor Estimating: 201it [02:42,  1.50it/s]Extractor Estimating: 202it [02:42,  1.57it/s]Extractor Estimating: 203it [02:43,  1.54it/s]Extractor Estimating: 204it [02:43,  1.59it/s]Extractor Estimating: 205it [02:44,  1.64it/s]Extractor Estimating: 206it [02:45,  1.64it/s]Extractor Estimating: 207it [02:45,  1.64it/s]Extractor Estimating: 208it [02:46,  1.50it/s]Extractor Estimating: 209it [02:47,  1.52it/s]Extractor Estimating: 210it [02:47,  1.52it/s]Extractor Estimating: 211it [02:48,  1.50it/s]Extractor Estimating: 212it [02:49,  1.57it/s]Extractor Estimating: 213it [02:49,  1.48it/s]Extractor Estimating: 214it [02:50,  1.57it/s]Extractor Estimating: 215it [02:50,  1.62it/s]Extractor Estimating: 216it [02:51,  1.62it/s]Extractor Estimating: 217it [02:52,  1.68it/s]Extractor Estimating: 218it [02:52,  1.54it/s]Extractor Estimating: 219it [02:53,  1.58it/s]Extractor Estimating: 220it [02:54,  1.39it/s]Extractor Estimating: 221it [02:54,  1.49it/s]Extractor Estimating: 222it [02:55,  1.55it/s]Extractor Estimating: 223it [02:56,  1.57it/s]Extractor Estimating: 224it [02:56,  1.60it/s]Extractor Estimating: 225it [02:57,  1.60it/s]Extractor Estimating: 226it [02:57,  1.65it/s]Extractor Estimating: 227it [02:58,  1.70it/s]Extractor Estimating: 228it [02:59,  1.73it/s]Extractor Estimating: 229it [02:59,  1.64it/s]Extractor Estimating: 230it [03:00,  1.70it/s]Extractor Estimating: 231it [03:00,  1.71it/s]Extractor Estimating: 232it [03:01,  1.67it/s]Extractor Estimating: 233it [03:02,  1.68it/s]Extractor Estimating: 234it [03:02,  1.61it/s]Extractor Estimating: 235it [03:03,  1.62it/s]Extractor Estimating: 236it [03:03,  1.64it/s]Extractor Estimating: 237it [03:04,  1.66it/s]Extractor Estimating: 238it [03:05,  1.67it/s]Extractor Estimating: 239it [03:05,  1.50it/s]Extractor Estimating: 240it [03:06,  1.57it/s]Extractor Estimating: 241it [03:07,  1.65it/s]Extractor Estimating: 242it [03:07,  1.67it/s]Extractor Estimating: 243it [03:08,  1.60it/s]Extractor Estimating: 244it [03:08,  1.60it/s]Extractor Estimating: 245it [03:09,  1.64it/s]Extractor Estimating: 246it [03:10,  1.28it/s]Extractor Estimating: 247it [03:11,  1.35it/s]Extractor Estimating: 248it [03:11,  1.43it/s]Extractor Estimating: 249it [03:12,  1.52it/s]Extractor Estimating: 250it [03:13,  1.45it/s]Extractor Estimating: 251it [03:13,  1.49it/s]Extractor Estimating: 252it [03:14,  1.53it/s]Extractor Estimating: 253it [03:15,  1.53it/s]Extractor Estimating: 254it [03:15,  1.51it/s]Extractor Estimating: 255it [03:16,  1.51it/s]Extractor Estimating: 256it [03:17,  1.53it/s]Extractor Estimating: 257it [03:17,  1.51it/s]Extractor Estimating: 258it [03:18,  1.53it/s]Extractor Estimating: 259it [03:19,  1.38it/s]Extractor Estimating: 260it [03:19,  1.43it/s]Extractor Estimating: 261it [03:20,  1.47it/s]Extractor Estimating: 262it [03:21,  1.50it/s]Extractor Estimating: 263it [03:21,  1.52it/s]Extractor Estimating: 264it [03:22,  1.55it/s]Extractor Estimating: 265it [03:23,  1.58it/s]Extractor Estimating: 266it [03:23,  1.59it/s]Extractor Estimating: 267it [03:24,  1.50it/s]Extractor Estimating: 268it [03:25,  1.50it/s]Extractor Estimating: 269it [03:25,  1.55it/s]Extractor Estimating: 270it [03:26,  1.46it/s]Extractor Estimating: 271it [03:27,  1.52it/s]Extractor Estimating: 272it [03:27,  1.56it/s]Extractor Estimating: 273it [03:28,  1.53it/s]Extractor Estimating: 274it [03:29,  1.55it/s]Extractor Estimating: 275it [03:30,  1.23it/s]Extractor Estimating: 276it [03:30,  1.29it/s]Extractor Estimating: 277it [03:31,  1.31it/s]Extractor Estimating: 278it [03:32,  1.34it/s]Extractor Estimating: 279it [03:33,  1.38it/s]Extractor Estimating: 280it [03:33,  1.45it/s]Extractor Estimating: 281it [03:34,  1.39it/s]Extractor Estimating: 282it [03:35,  1.42it/s]Extractor Estimating: 283it [03:35,  1.44it/s]Extractor Estimating: 284it [03:36,  1.41it/s]Extractor Estimating: 285it [03:37,  1.48it/s]Extractor Estimating: 286it [03:37,  1.51it/s]Extractor Estimating: 287it [03:38,  1.46it/s]Extractor Estimating: 288it [03:39,  1.49it/s]Extractor Estimating: 289it [03:39,  1.38it/s]Extractor Estimating: 290it [03:40,  1.42it/s]Extractor Estimating: 291it [03:41,  1.45it/s]Extractor Estimating: 292it [03:42,  1.32it/s]Extractor Estimating: 293it [03:42,  1.42it/s]Extractor Estimating: 294it [03:43,  1.44it/s]Extractor Estimating: 295it [03:44,  1.44it/s]Extractor Estimating: 296it [03:44,  1.43it/s]Extractor Estimating: 297it [03:45,  1.45it/s]Extractor Estimating: 298it [03:46,  1.45it/s]Extractor Estimating: 299it [03:46,  1.46it/s]Extractor Estimating: 300it [03:47,  1.49it/s]Extractor Estimating: 301it [03:48,  1.48it/s]Extractor Estimating: 302it [03:48,  1.53it/s]Extractor Estimating: 303it [03:49,  1.47it/s]Extractor Estimating: 304it [03:50,  1.34it/s]Extractor Estimating: 305it [03:51,  1.44it/s]Extractor Estimating: 306it [03:51,  1.50it/s]Extractor Estimating: 307it [03:52,  1.20it/s]Extractor Estimating: 308it [03:53,  1.28it/s]Extractor Estimating: 309it [03:54,  1.36it/s]Extractor Estimating: 310it [03:54,  1.43it/s]Extractor Estimating: 311it [03:55,  1.34it/s]Extractor Estimating: 312it [03:56,  1.38it/s]Extractor Estimating: 313it [03:56,  1.42it/s]Extractor Estimating: 314it [03:57,  1.49it/s]Extractor Estimating: 315it [03:58,  1.54it/s]Extractor Estimating: 316it [03:58,  1.46it/s]Extractor Estimating: 317it [03:59,  1.47it/s]Extractor Estimating: 318it [04:00,  1.49it/s]Extractor Estimating: 319it [04:01,  1.34it/s]Extractor Estimating: 320it [04:01,  1.40it/s]Extractor Estimating: 321it [04:02,  1.41it/s]Extractor Estimating: 322it [04:03,  1.34it/s]Extractor Estimating: 323it [04:03,  1.42it/s]Extractor Estimating: 324it [04:04,  1.47it/s]Extractor Estimating: 325it [04:05,  1.53it/s]Extractor Estimating: 326it [04:05,  1.56it/s]Extractor Estimating: 327it [04:06,  1.57it/s]Extractor Estimating: 328it [04:06,  1.64it/s]Extractor Estimating: 329it [04:07,  1.73it/s]Extractor Estimating: 330it [04:08,  1.71it/s]Extractor Estimating: 331it [04:08,  1.70it/s]Extractor Estimating: 332it [04:09,  1.74it/s]Extractor Estimating: 333it [04:09,  1.62it/s]Extractor Estimating: 334it [04:10,  1.66it/s]Extractor Estimating: 335it [04:11,  1.66it/s]Extractor Estimating: 336it [04:11,  1.69it/s]Extractor Estimating: 337it [04:12,  1.65it/s]Extractor Estimating: 338it [04:13,  1.53it/s]Extractor Estimating: 339it [04:13,  1.52it/s]Extractor Estimating: 340it [04:14,  1.50it/s]Extractor Estimating: 341it [04:14,  1.54it/s]Extractor Estimating: 342it [04:15,  1.59it/s]Extractor Estimating: 343it [04:16,  1.53it/s]Extractor Estimating: 344it [04:16,  1.57it/s]Extractor Estimating: 345it [04:17,  1.67it/s]Extractor Estimating: 346it [04:17,  1.66it/s]Extractor Estimating: 347it [04:18,  1.65it/s]Extractor Estimating: 348it [04:19,  1.65it/s]Extractor Estimating: 349it [04:19,  1.62it/s]Extractor Estimating: 350it [04:20,  1.65it/s]Extractor Estimating: 351it [04:21,  1.66it/s]Extractor Estimating: 352it [04:21,  1.51it/s]Extractor Estimating: 353it [04:22,  1.55it/s]Extractor Estimating: 354it [04:23,  1.57it/s]Extractor Estimating: 355it [04:23,  1.50it/s]Extractor Estimating: 356it [04:24,  1.49it/s]Extractor Estimating: 357it [04:25,  1.46it/s]Extractor Estimating: 358it [04:25,  1.48it/s]Extractor Estimating: 359it [04:26,  1.55it/s]Extractor Estimating: 360it [04:27,  1.49it/s]Extractor Estimating: 361it [04:27,  1.56it/s]Extractor Estimating: 362it [04:28,  1.57it/s]Extractor Estimating: 363it [04:29,  1.16it/s]Extractor Estimating: 364it [04:30,  1.29it/s]Extractor Estimating: 365it [04:30,  1.39it/s]Extractor Estimating: 366it [04:31,  1.44it/s]Extractor Estimating: 367it [04:32,  1.35it/s]Extractor Estimating: 368it [04:33,  1.39it/s]Extractor Estimating: 369it [04:33,  1.46it/s]Extractor Estimating: 370it [04:34,  1.52it/s]Extractor Estimating: 371it [04:34,  1.54it/s]Extractor Estimating: 372it [04:35,  1.53it/s]Extractor Estimating: 373it [04:36,  1.49it/s]Extractor Estimating: 374it [04:36,  1.48it/s]Extractor Estimating: 375it [04:37,  1.43it/s]Extractor Estimating: 375it [04:37,  1.35it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 1520 mean pseudo reward: 0.9183873265478993
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 15172
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15272, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15272, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 36, avg_time 1.366, loss:427.7644
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 8, avg_time 1.022, loss:343.5615
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 44, avg_time 1.014, loss:323.3088
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 16, avg_time 1.013, loss:291.9210
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 52, avg_time 1.011, loss:262.9069
>> valid entity prec:0.4741, rec:0.4282, f1:0.4500
>> valid relation prec:0.0329, rec:0.0095, f1:0.0147
>> valid relation with NER prec:0.0329, rec:0.0095, f1:0.0147
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 24, avg_time 2.465, loss:246.3416
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 60, avg_time 1.021, loss:232.8320
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 32, avg_time 1.015, loss:214.3734
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 4, avg_time 1.019, loss:204.4430
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 40, avg_time 1.024, loss:196.4575
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4809, rec:0.3466, f1:0.4028
>> valid relation prec:0.0377, rec:0.0111, f1:0.0171
>> valid relation with NER prec:0.0377, rec:0.0111, f1:0.0171
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 12, avg_time 2.432, loss:195.8513
g_step 1200, step 48, avg_time 1.020, loss:174.6570
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:35:08 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:35:08 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-35-08_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:35:09 - WARNING - datasets.builder -   Using custom data configuration default-594b98a0dba7cedc
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-594b98a0dba7cedc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:35:16,913 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:35:17,051 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:35:17,051 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:35:17,052 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:35:17,367 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:17,496 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:17,496 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:17,496 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:17,496 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:17,496 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:35:17,496 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:35:18,047 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:35:21,907 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:35:22,723 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-594b98a0dba7cedc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 19:35:22 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1509a5d9d9e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  1.65ba/s]100%|██████████| 2/2 [00:00<00:00,  3.17ba/s]100%|██████████| 2/2 [00:00<00:00,  2.79ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.09ba/s] 40%|████      | 2/5 [00:00<00:00,  3.10ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.67ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.38ba/s]100%|██████████| 5/5 [00:01<00:00,  3.80ba/s]
  0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  1.73ba/s]100%|██████████| 2/2 [00:00<00:00,  3.14ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.30ba/s] 40%|████      | 2/5 [00:00<00:00,  3.71ba/s] 60%|██████    | 3/5 [00:00<00:00,  5.04ba/s]100%|██████████| 5/5 [00:00<00:00,  8.14ba/s]100%|██████████| 5/5 [00:00<00:00,  6.01ba/s]
[INFO|trainer.py:414] 2023-08-28 19:35:34,394 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:35:34,404 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:35:34,404 >>   Num examples = 1520
[INFO|trainer.py:1149] 2023-08-28 19:35:34,404 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:35:34,404 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:35:34,404 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:35:34,404 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:35:34,404 >>   Total optimization steps = 120
  0%|          | 0/120 [00:00<?, ?it/s]  1%|          | 1/120 [00:00<01:04,  1.84it/s]  2%|▏         | 2/120 [00:00<00:46,  2.53it/s]  2%|▎         | 3/120 [00:01<00:40,  2.87it/s]  3%|▎         | 4/120 [00:01<00:37,  3.06it/s]  4%|▍         | 5/120 [00:01<00:41,  2.76it/s]  5%|▌         | 6/120 [00:02<00:38,  2.95it/s]  6%|▌         | 7/120 [00:02<00:36,  3.09it/s]  7%|▋         | 8/120 [00:02<00:35,  3.19it/s]  8%|▊         | 9/120 [00:03<00:34,  3.25it/s]  8%|▊         | 10/120 [00:03<00:33,  3.30it/s]  9%|▉         | 11/120 [00:03<00:32,  3.34it/s] 10%|█         | 12/120 [00:03<00:32,  3.36it/s] 11%|█         | 13/120 [00:04<00:31,  3.38it/s] 12%|█▏        | 14/120 [00:04<00:31,  3.39it/s] 12%|█▎        | 15/120 [00:04<00:33,  3.11it/s] 13%|█▎        | 16/120 [00:05<00:32,  3.19it/s] 14%|█▍        | 17/120 [00:05<00:31,  3.25it/s] 15%|█▌        | 18/120 [00:05<00:30,  3.31it/s] 16%|█▌        | 19/120 [00:06<00:30,  3.34it/s] 17%|█▋        | 20/120 [00:06<00:29,  3.36it/s] 18%|█▊        | 21/120 [00:06<00:29,  3.38it/s] 18%|█▊        | 22/120 [00:06<00:28,  3.39it/s] 19%|█▉        | 23/120 [00:07<00:28,  3.40it/s] 20%|██        | 24/120 [00:07<00:26,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 19:35:41,844 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:35:41,844 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 19:35:41,844 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.75it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.51it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.57it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.77it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.01it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.73it/s][A
  7%|▋         | 38/543 [00:00<00:11, 45.40it/s][A
  8%|▊         | 43/543 [00:01<00:16, 30.55it/s][A
  9%|▉         | 48/543 [00:01<00:14, 34.06it/s][A
 10%|▉         | 53/543 [00:01<00:13, 36.84it/s][A
 11%|█         | 58/543 [00:01<00:12, 39.07it/s][A
 12%|█▏        | 63/543 [00:01<00:11, 40.72it/s][A
 13%|█▎        | 68/543 [00:01<00:11, 42.01it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 42.95it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 43.59it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 43.70it/s][A
 16%|█▌        | 88/543 [00:02<00:10, 43.95it/s][A
 17%|█▋        | 93/543 [00:02<00:10, 44.26it/s][A
 18%|█▊        | 98/543 [00:02<00:10, 44.49it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 44.77it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 45.02it/s][A
 21%|██        | 113/543 [00:02<00:09, 45.27it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 45.37it/s][A
 23%|██▎       | 123/543 [00:02<00:09, 45.22it/s][A
 24%|██▎       | 128/543 [00:02<00:09, 45.02it/s][A
 24%|██▍       | 133/543 [00:03<00:09, 44.82it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 44.84it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 44.81it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 45.02it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 45.20it/s][A
 29%|██▉       | 158/543 [00:03<00:10, 35.96it/s][A
 30%|███       | 163/543 [00:03<00:09, 38.45it/s][A
 31%|███       | 168/543 [00:04<00:10, 35.32it/s][A
 32%|███▏      | 173/543 [00:04<00:09, 37.88it/s][A
 33%|███▎      | 178/543 [00:04<00:09, 39.84it/s][A
 34%|███▎      | 183/543 [00:04<00:08, 41.39it/s][A
 35%|███▍      | 188/543 [00:04<00:08, 42.62it/s][A
 36%|███▌      | 193/543 [00:04<00:08, 43.43it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 43.87it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 44.23it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 44.19it/s][A
 39%|███▉      | 213/543 [00:05<00:07, 44.23it/s][A
 40%|████      | 218/543 [00:05<00:07, 44.46it/s][A
 41%|████      | 223/543 [00:05<00:07, 44.70it/s][A
 42%|████▏     | 228/543 [00:05<00:07, 44.94it/s][A
 43%|████▎     | 233/543 [00:05<00:06, 45.23it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 45.37it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 45.40it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 45.25it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 45.07it/s][A
 48%|████▊     | 258/543 [00:06<00:06, 44.85it/s][A
 48%|████▊     | 263/543 [00:06<00:06, 44.95it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 45.05it/s][A
 50%|█████     | 273/543 [00:06<00:05, 45.27it/s][A
 51%|█████     | 278/543 [00:06<00:05, 45.43it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 45.53it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 45.51it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 45.36it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 45.14it/s][A
 56%|█████▌    | 303/543 [00:07<00:06, 39.72it/s][A
 57%|█████▋    | 308/543 [00:08<00:30,  7.70it/s][A
 58%|█████▊    | 313/543 [00:09<00:22, 10.26it/s][A
 59%|█████▊    | 318/543 [00:09<00:16, 13.38it/s][A
 59%|█████▉    | 323/543 [00:09<00:12, 16.99it/s][A
 60%|██████    | 328/543 [00:09<00:10, 20.94it/s][A
 61%|██████▏   | 333/543 [00:09<00:08, 25.01it/s][A
 62%|██████▏   | 338/543 [00:09<00:07, 28.94it/s][A
 63%|██████▎   | 343/543 [00:09<00:06, 32.50it/s][A
 64%|██████▍   | 348/543 [00:09<00:05, 35.45it/s][A
 65%|██████▌   | 353/543 [00:10<00:05, 37.60it/s][A
 66%|██████▌   | 358/543 [00:10<00:06, 30.23it/s][A
 67%|██████▋   | 363/543 [00:10<00:05, 33.66it/s][A
 68%|██████▊   | 368/543 [00:10<00:04, 36.53it/s][A
 69%|██████▊   | 373/543 [00:10<00:04, 38.84it/s][A
 70%|██████▉   | 378/543 [00:10<00:04, 40.66it/s][A
 71%|███████   | 383/543 [00:10<00:03, 42.06it/s][A
 71%|███████▏  | 388/543 [00:10<00:03, 43.08it/s][A
 72%|███████▏  | 393/543 [00:10<00:03, 43.66it/s][A
 73%|███████▎  | 398/543 [00:11<00:03, 43.78it/s][A
 74%|███████▍  | 403/543 [00:11<00:03, 44.05it/s][A
 75%|███████▌  | 408/543 [00:11<00:03, 44.41it/s][A
 76%|███████▌  | 413/543 [00:11<00:02, 44.74it/s][A
 77%|███████▋  | 418/543 [00:11<00:02, 44.99it/s][A
 78%|███████▊  | 423/543 [00:11<00:02, 45.26it/s][A
 79%|███████▉  | 428/543 [00:11<00:02, 45.30it/s][A
 80%|███████▉  | 433/543 [00:11<00:02, 45.23it/s][A
 81%|████████  | 438/543 [00:11<00:02, 45.10it/s][A
 82%|████████▏ | 443/543 [00:12<00:02, 44.84it/s][A
 83%|████████▎ | 448/543 [00:12<00:02, 44.87it/s][A
 83%|████████▎ | 453/543 [00:12<00:02, 44.94it/s][A
 84%|████████▍ | 458/543 [00:12<00:01, 45.00it/s][A
 85%|████████▌ | 463/543 [00:12<00:01, 45.18it/s][A
 86%|████████▌ | 468/543 [00:12<00:01, 45.34it/s][A
 87%|████████▋ | 473/543 [00:12<00:01, 45.43it/s][A
 88%|████████▊ | 478/543 [00:12<00:01, 45.39it/s][A
 89%|████████▉ | 483/543 [00:13<00:01, 45.19it/s][A
 90%|████████▉ | 488/543 [00:13<00:02, 23.81it/s][A
 91%|█████████ | 493/543 [00:13<00:01, 27.83it/s][A
 92%|█████████▏| 498/543 [00:13<00:01, 31.54it/s][A
 93%|█████████▎| 503/543 [00:13<00:01, 34.80it/s][A
 94%|█████████▎| 508/543 [00:13<00:00, 37.40it/s][A
 94%|█████████▍| 513/543 [00:13<00:00, 39.54it/s][A
 95%|█████████▌| 518/543 [00:14<00:00, 41.16it/s][A
 96%|█████████▋| 523/543 [00:14<00:00, 42.31it/s][A
 97%|█████████▋| 528/543 [00:14<00:00, 42.78it/s][A
 98%|█████████▊| 533/543 [00:14<00:00, 43.19it/s][A
 99%|█████████▉| 538/543 [00:14<00:00, 43.66it/s][A
100%|██████████| 543/543 [00:14<00:00, 44.08it/s][A                                                
                                                 [A 20%|██        | 24/120 [00:22<00:26,  3.64it/s]
100%|██████████| 543/543 [00:14<00:00, 44.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:35:57,495 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24
[INFO|configuration_utils.py:351] 2023-08-28 19:35:58,775 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:36:16,944 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:36:17,985 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:36:18,133 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24/special_tokens_map.json
 21%|██        | 25/120 [01:06<28:21, 17.91s/it] 22%|██▏       | 26/120 [01:06<19:47, 12.63s/it] 22%|██▎       | 27/120 [01:07<14:04,  9.08s/it] 23%|██▎       | 28/120 [01:07<09:52,  6.44s/it] 24%|██▍       | 29/120 [01:08<06:58,  4.60s/it] 25%|██▌       | 30/120 [01:08<04:57,  3.30s/it] 26%|██▌       | 31/120 [01:08<03:33,  2.40s/it] 27%|██▋       | 32/120 [01:09<02:35,  1.77s/it] 28%|██▊       | 33/120 [01:09<01:55,  1.32s/it] 28%|██▊       | 34/120 [01:09<01:27,  1.01s/it] 29%|██▉       | 35/120 [01:09<01:07,  1.26it/s] 30%|███       | 36/120 [01:10<00:58,  1.43it/s] 31%|███       | 37/120 [01:10<00:47,  1.73it/s] 32%|███▏      | 38/120 [01:10<00:40,  2.04it/s] 32%|███▎      | 39/120 [01:11<00:34,  2.33it/s] 33%|███▎      | 40/120 [01:11<00:30,  2.58it/s] 34%|███▍      | 41/120 [01:11<00:28,  2.79it/s] 35%|███▌      | 42/120 [01:12<00:26,  2.97it/s] 36%|███▌      | 43/120 [01:12<00:24,  3.10it/s] 37%|███▋      | 44/120 [01:12<00:23,  3.20it/s] 38%|███▊      | 45/120 [01:12<00:22,  3.27it/s] 38%|███▊      | 46/120 [01:13<00:23,  3.15it/s] 39%|███▉      | 47/120 [01:13<00:22,  3.24it/s] 40%|████      | 48/120 [01:13<00:20,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 19:36:48,257 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:36:48,257 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 19:36:48,257 >>   Batch size = 8
{'eval_loss': 1.0283375978469849, 'eval_runtime': 14.5958, 'eval_samples_per_second': 297.482, 'eval_steps_per_second': 37.202, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.15it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.23it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.57it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.54it/s][A
  5%|▍         | 27/543 [00:00<00:11, 45.90it/s][A
  6%|▌         | 32/543 [00:00<00:13, 39.26it/s][A
  7%|▋         | 37/543 [00:03<01:36,  5.25it/s][A
  7%|▋         | 40/543 [00:03<01:27,  5.76it/s][A
  8%|▊         | 45/543 [00:03<01:00,  8.20it/s][A
  9%|▉         | 50/543 [00:03<00:44, 11.20it/s][A
 10%|█         | 55/543 [00:04<00:33, 14.75it/s][A
 11%|█         | 60/543 [00:04<00:25, 18.71it/s][A
 12%|█▏        | 65/543 [00:04<00:20, 22.89it/s][A
 13%|█▎        | 70/543 [00:04<00:17, 26.96it/s][A
 14%|█▍        | 75/543 [00:04<00:15, 30.81it/s][A
 15%|█▍        | 80/543 [00:04<00:13, 34.04it/s][A
 16%|█▌        | 85/543 [00:04<00:12, 36.53it/s][A
 17%|█▋        | 90/543 [00:04<00:11, 38.64it/s][A
 17%|█▋        | 95/543 [00:04<00:11, 40.44it/s][A
 18%|█▊        | 100/543 [00:05<00:10, 41.93it/s][A
 19%|█▉        | 105/543 [00:05<00:10, 42.99it/s][A
 20%|██        | 110/543 [00:05<00:09, 43.77it/s][A
 21%|██        | 115/543 [00:05<00:09, 44.20it/s][A
 22%|██▏       | 120/543 [00:05<00:09, 44.60it/s][A
 23%|██▎       | 125/543 [00:05<00:09, 44.62it/s][A
 24%|██▍       | 130/543 [00:05<00:09, 44.52it/s][A
 25%|██▍       | 135/543 [00:05<00:09, 44.57it/s][A
 26%|██▌       | 140/543 [00:05<00:09, 44.64it/s][A
 27%|██▋       | 145/543 [00:06<00:08, 44.98it/s][A
 28%|██▊       | 150/543 [00:06<00:08, 45.19it/s][A
 29%|██▊       | 155/543 [00:06<00:08, 45.25it/s][A
 29%|██▉       | 160/543 [00:06<00:08, 45.37it/s][A
 30%|███       | 165/543 [00:06<00:08, 45.36it/s][A
 31%|███▏      | 170/543 [00:06<00:09, 38.58it/s][A
 32%|███▏      | 175/543 [00:06<00:09, 40.53it/s][A
 33%|███▎      | 180/543 [00:06<00:08, 41.92it/s][A
 34%|███▍      | 185/543 [00:07<00:08, 43.04it/s][A
 35%|███▍      | 190/543 [00:07<00:08, 43.79it/s][A
 36%|███▌      | 195/543 [00:07<00:07, 44.32it/s][A
 37%|███▋      | 200/543 [00:07<00:07, 44.63it/s][A
 38%|███▊      | 205/543 [00:07<00:07, 44.87it/s][A
 39%|███▊      | 210/543 [00:07<00:07, 44.59it/s][A
 40%|███▉      | 215/543 [00:07<00:07, 44.35it/s][A
 41%|████      | 220/543 [00:07<00:07, 44.49it/s][A
 41%|████▏     | 225/543 [00:07<00:07, 44.67it/s][A
 42%|████▏     | 230/543 [00:08<00:06, 44.98it/s][A
 43%|████▎     | 235/543 [00:08<00:06, 45.12it/s][A
 44%|████▍     | 240/543 [00:08<00:06, 45.35it/s][A
 45%|████▌     | 245/543 [00:08<00:06, 45.38it/s][A
 46%|████▌     | 250/543 [00:08<00:06, 45.30it/s][A
 47%|████▋     | 255/543 [00:08<00:06, 45.00it/s][A
 48%|████▊     | 260/543 [00:08<00:06, 44.86it/s][A
 49%|████▉     | 265/543 [00:08<00:06, 44.81it/s][A
 50%|████▉     | 270/543 [00:08<00:06, 44.88it/s][A
 51%|█████     | 275/543 [00:09<00:05, 44.86it/s][A
 52%|█████▏    | 280/543 [00:09<00:05, 45.11it/s][A
 52%|█████▏    | 285/543 [00:09<00:05, 45.28it/s][A
 53%|█████▎    | 290/543 [00:09<00:05, 45.33it/s][A
 54%|█████▍    | 295/543 [00:09<00:05, 45.42it/s][A
 55%|█████▌    | 300/543 [00:09<00:05, 45.27it/s][A
 56%|█████▌    | 305/543 [00:10<00:10, 22.62it/s][A
 57%|█████▋    | 310/543 [00:10<00:08, 26.67it/s][A
 58%|█████▊    | 315/543 [00:10<00:07, 30.46it/s][A
 59%|█████▉    | 320/543 [00:10<00:06, 33.81it/s][A
 60%|█████▉    | 325/543 [00:10<00:05, 36.70it/s][A
 61%|██████    | 330/543 [00:10<00:05, 38.96it/s][A
 62%|██████▏   | 335/543 [00:10<00:05, 40.76it/s][A
 63%|██████▎   | 340/543 [00:10<00:04, 42.07it/s][A
 64%|██████▎   | 345/543 [00:10<00:04, 42.59it/s][A
 64%|██████▍   | 350/543 [00:11<00:04, 43.20it/s][A
 65%|██████▌   | 355/543 [00:11<00:04, 43.74it/s][A
 66%|██████▋   | 360/543 [00:11<00:04, 44.32it/s][A
 67%|██████▋   | 365/543 [00:11<00:03, 44.65it/s][A
 68%|██████▊   | 370/543 [00:11<00:03, 44.94it/s][A
 69%|██████▉   | 375/543 [00:11<00:03, 45.11it/s][A
 70%|██████▉   | 380/543 [00:11<00:03, 45.17it/s][A
 71%|███████   | 385/543 [00:11<00:03, 44.96it/s][A
 72%|███████▏  | 390/543 [00:11<00:03, 44.73it/s][A
 73%|███████▎  | 395/543 [00:12<00:03, 44.53it/s][A
 74%|███████▎  | 400/543 [00:12<00:03, 44.58it/s][A
 75%|███████▍  | 405/543 [00:12<00:03, 44.82it/s][A
 76%|███████▌  | 410/543 [00:12<00:02, 45.04it/s][A
 76%|███████▋  | 415/543 [00:12<00:02, 45.37it/s][A
 77%|███████▋  | 420/543 [00:12<00:02, 45.50it/s][A
 78%|███████▊  | 425/543 [00:12<00:04, 26.74it/s][A
 79%|███████▉  | 430/543 [00:13<00:03, 30.58it/s][A
 80%|████████  | 435/543 [00:13<00:03, 33.88it/s][A
 81%|████████  | 440/543 [00:13<00:02, 36.73it/s][A
 82%|████████▏ | 445/543 [00:13<00:02, 39.04it/s][A
 83%|████████▎ | 450/543 [00:13<00:02, 40.86it/s][A
 84%|████████▍ | 455/543 [00:13<00:02, 42.18it/s][A
 85%|████████▍ | 460/543 [00:13<00:01, 43.11it/s][A
 86%|████████▌ | 465/543 [00:13<00:01, 43.37it/s][A
 87%|████████▋ | 470/543 [00:13<00:01, 43.72it/s][A
 87%|████████▋ | 475/543 [00:14<00:01, 44.10it/s][A
 88%|████████▊ | 480/543 [00:14<00:01, 44.48it/s][A
 89%|████████▉ | 485/543 [00:14<00:01, 44.85it/s][A
 90%|█████████ | 490/543 [00:14<00:01, 45.09it/s][A
 91%|█████████ | 495/543 [00:14<00:01, 45.29it/s][A
 92%|█████████▏| 500/543 [00:14<00:00, 45.46it/s][A
 93%|█████████▎| 505/543 [00:14<00:00, 45.36it/s][A
 94%|█████████▍| 510/543 [00:14<00:00, 45.06it/s][A
 95%|█████████▍| 515/543 [00:14<00:00, 44.95it/s][A
 96%|█████████▌| 520/543 [00:15<00:00, 44.95it/s][A
 97%|█████████▋| 525/543 [00:15<00:00, 45.10it/s][A
 98%|█████████▊| 530/543 [00:15<00:00, 45.19it/s][A
 99%|█████████▊| 535/543 [00:15<00:00, 45.41it/s][A
 99%|█████████▉| 540/543 [00:15<00:00, 45.50it/s][A                                                
                                                 [A 40%|████      | 48/120 [01:29<00:20,  3.52it/s]
100%|██████████| 543/543 [00:15<00:00, 45.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:37:04,121 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48
[INFO|configuration_utils.py:351] 2023-08-28 19:37:04,693 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:37:21,303 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:37:22,410 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:37:22,970 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48/special_tokens_map.json
 41%|████      | 49/120 [02:21<24:08, 20.41s/it] 42%|████▏     | 50/120 [02:21<16:48, 14.40s/it] 42%|████▎     | 51/120 [02:21<11:41, 10.17s/it] 43%|████▎     | 52/120 [02:22<08:09,  7.21s/it] 44%|████▍     | 53/120 [02:22<05:43,  5.13s/it] 45%|████▌     | 54/120 [02:22<04:02,  3.68s/it] 46%|████▌     | 55/120 [02:23<02:53,  2.66s/it] 47%|████▋     | 56/120 [02:23<02:04,  1.95s/it] 48%|████▊     | 57/120 [02:23<01:31,  1.45s/it] 48%|████▊     | 58/120 [02:23<01:08,  1.11s/it] 49%|████▉     | 59/120 [02:24<00:52,  1.16it/s] 50%|█████     | 60/120 [02:24<00:44,  1.34it/s] 51%|█████     | 61/120 [02:24<00:36,  1.64it/s] 52%|█████▏    | 62/120 [02:25<00:29,  1.94it/s] 52%|█████▎    | 63/120 [02:25<00:25,  2.23it/s] 53%|█████▎    | 64/120 [02:25<00:22,  2.49it/s] 54%|█████▍    | 65/120 [02:26<00:20,  2.71it/s] 55%|█████▌    | 66/120 [02:26<00:18,  2.89it/s] 56%|█████▌    | 67/120 [02:26<00:17,  3.04it/s] 57%|█████▋    | 68/120 [02:27<00:16,  3.14it/s] 57%|█████▊    | 69/120 [02:27<00:15,  3.22it/s] 58%|█████▊    | 70/120 [02:27<00:16,  3.04it/s] 59%|█████▉    | 71/120 [02:27<00:15,  3.15it/s] 60%|██████    | 72/120 [02:28<00:13,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 19:38:02,631 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:38:02,631 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 19:38:02,631 >>   Batch size = 8
{'eval_loss': 1.0222392082214355, 'eval_runtime': 15.5876, 'eval_samples_per_second': 278.555, 'eval_steps_per_second': 34.835, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.99it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.72it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.51it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.57it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.04it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.63it/s][A
  7%|▋         | 38/543 [00:00<00:11, 45.28it/s][A
  8%|▊         | 43/543 [00:00<00:11, 45.13it/s][A
  9%|▉         | 48/543 [00:01<00:10, 45.17it/s][A
 10%|▉         | 53/543 [00:01<00:10, 45.33it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.38it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.51it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.60it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 45.59it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 45.32it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 45.17it/s][A
 16%|█▌        | 88/543 [00:01<00:10, 44.96it/s][A
 17%|█▋        | 93/543 [00:02<00:09, 45.00it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 45.16it/s][A
 19%|█▉        | 103/543 [00:02<00:16, 26.93it/s][A
 20%|█▉        | 108/543 [00:02<00:14, 30.77it/s][A
 21%|██        | 113/543 [00:02<00:12, 34.12it/s][A
 22%|██▏       | 118/543 [00:02<00:11, 36.89it/s][A
 23%|██▎       | 123/543 [00:02<00:10, 39.12it/s][A
 24%|██▎       | 128/543 [00:03<00:10, 40.87it/s][A
 24%|██▍       | 133/543 [00:03<00:09, 42.18it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 43.13it/s][A
 26%|██▋       | 143/543 [00:03<00:09, 43.42it/s][A
 27%|██▋       | 148/543 [00:03<00:09, 43.72it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 44.16it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 44.47it/s][A
 30%|███       | 163/543 [00:03<00:08, 44.84it/s][A
 31%|███       | 168/543 [00:03<00:08, 45.00it/s][A
 32%|███▏      | 173/543 [00:04<00:08, 45.19it/s][A
 33%|███▎      | 178/543 [00:04<00:08, 45.41it/s][A
 34%|███▎      | 183/543 [00:04<00:07, 45.32it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 45.09it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 44.92it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 44.93it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 45.09it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 45.31it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 45.28it/s][A
 40%|████      | 218/543 [00:05<00:07, 45.42it/s][A
 41%|████      | 223/543 [00:06<00:07, 45.50it/s][A
 42%|████▏     | 228/543 [00:06<00:23, 13.68it/s][A
 43%|████▎     | 233/543 [00:06<00:17, 17.33it/s][A
 44%|████▍     | 238/543 [00:06<00:14, 21.30it/s][A
 45%|████▍     | 243/543 [00:06<00:11, 25.35it/s][A
 46%|████▌     | 248/543 [00:06<00:10, 29.26it/s][A
 47%|████▋     | 253/543 [00:06<00:08, 32.76it/s][A
 48%|████▊     | 258/543 [00:06<00:07, 35.72it/s][A
 48%|████▊     | 263/543 [00:06<00:07, 38.23it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 39.81it/s][A
 50%|█████     | 273/543 [00:07<00:06, 41.02it/s][A
 51%|█████     | 278/543 [00:07<00:06, 42.18it/s][A
 52%|█████▏    | 283/543 [00:07<00:06, 43.14it/s][A
 53%|█████▎    | 288/543 [00:07<00:05, 43.82it/s][A
 54%|█████▍    | 293/543 [00:07<00:05, 44.37it/s][A
 55%|█████▍    | 298/543 [00:07<00:05, 44.80it/s][A
 56%|█████▌    | 303/543 [00:07<00:05, 45.10it/s][A
 57%|█████▋    | 308/543 [00:07<00:05, 45.16it/s][A
 58%|█████▊    | 313/543 [00:07<00:05, 44.87it/s][A
 59%|█████▊    | 318/543 [00:08<00:05, 44.70it/s][A
 59%|█████▉    | 323/543 [00:08<00:04, 44.75it/s][A
 60%|██████    | 328/543 [00:08<00:07, 28.01it/s][A
 61%|██████▏   | 333/543 [00:08<00:06, 31.71it/s][A
 62%|██████▏   | 338/543 [00:08<00:05, 34.93it/s][A
 63%|██████▎   | 343/543 [00:08<00:05, 37.56it/s][A
 64%|██████▍   | 348/543 [00:08<00:04, 39.65it/s][A
 65%|██████▌   | 353/543 [00:09<00:04, 41.25it/s][A
 66%|██████▌   | 358/543 [00:09<00:04, 42.53it/s][A
 67%|██████▋   | 363/543 [00:09<00:04, 43.28it/s][A
 68%|██████▊   | 368/543 [00:09<00:04, 43.54it/s][A
 69%|██████▊   | 373/543 [00:09<00:03, 43.77it/s][A
 70%|██████▉   | 378/543 [00:09<00:03, 44.21it/s][A
 71%|███████   | 383/543 [00:09<00:03, 44.57it/s][A
 71%|███████▏  | 388/543 [00:09<00:03, 44.71it/s][A
 72%|███████▏  | 393/543 [00:09<00:03, 45.07it/s][A
 73%|███████▎  | 398/543 [00:10<00:03, 45.20it/s][A
 74%|███████▍  | 403/543 [00:10<00:03, 45.30it/s][A
 75%|███████▌  | 408/543 [00:10<00:02, 45.39it/s][A
 76%|███████▌  | 413/543 [00:10<00:02, 45.22it/s][A
 77%|███████▋  | 418/543 [00:10<00:02, 45.05it/s][A
 78%|███████▊  | 423/543 [00:11<00:02, 44.99it/s][A
 79%|███████▉  | 428/543 [00:11<00:09, 11.79it/s][A
 80%|███████▉  | 433/543 [00:11<00:07, 15.17it/s][A
 81%|████████  | 438/543 [00:12<00:05, 18.97it/s][A
 82%|████████▏ | 443/543 [00:12<00:04, 23.02it/s][A
 83%|████████▎ | 448/543 [00:12<00:03, 27.01it/s][A
 83%|████████▎ | 453/543 [00:12<00:02, 30.79it/s][A
 84%|████████▍ | 458/543 [00:12<00:02, 34.14it/s][A
 85%|████████▌ | 463/543 [00:12<00:02, 36.93it/s][A
 86%|████████▌ | 468/543 [00:12<00:01, 38.81it/s][A
 87%|████████▋ | 473/543 [00:12<00:01, 40.20it/s][A
 88%|████████▊ | 478/543 [00:12<00:01, 41.52it/s][A
 89%|████████▉ | 483/543 [00:13<00:01, 42.62it/s][A
 90%|████████▉ | 488/543 [00:13<00:01, 43.44it/s][A
 91%|█████████ | 493/543 [00:13<00:01, 44.08it/s][A
 92%|█████████▏| 498/543 [00:13<00:01, 44.54it/s][A
 93%|█████████▎| 503/543 [00:13<00:00, 44.92it/s][A
 94%|█████████▎| 508/543 [00:13<00:00, 45.15it/s][A
 94%|█████████▍| 513/543 [00:14<00:00, 45.19it/s][A
 95%|█████████▌| 518/543 [00:14<00:01, 23.71it/s][A
 96%|█████████▋| 523/543 [00:14<00:00, 27.72it/s][A
 97%|█████████▋| 528/543 [00:14<00:00, 31.48it/s][A
 98%|█████████▊| 533/543 [00:14<00:00, 34.78it/s][A
 99%|█████████▉| 538/543 [00:14<00:00, 37.55it/s][A
100%|██████████| 543/543 [00:14<00:00, 39.73it/s][A                                                
                                                 [A 60%|██████    | 72/120 [02:42<00:13,  3.43it/s]
100%|██████████| 543/543 [00:14<00:00, 39.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:38:17,595 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72
[INFO|configuration_utils.py:351] 2023-08-28 19:38:19,020 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:38:38,490 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:38:40,275 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:38:40,906 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72/special_tokens_map.json
 61%|██████    | 73/120 [03:32<15:18, 19.54s/it] 62%|██████▏   | 74/120 [03:33<10:37, 13.86s/it] 62%|██████▎   | 75/120 [03:33<07:20,  9.79s/it] 63%|██████▎   | 76/120 [03:33<05:05,  6.94s/it] 64%|██████▍   | 77/120 [03:34<03:32,  4.95s/it] 65%|██████▌   | 78/120 [03:34<02:29,  3.55s/it] 66%|██████▌   | 79/120 [03:34<01:45,  2.57s/it] 67%|██████▋   | 80/120 [03:35<01:15,  1.89s/it] 68%|██████▊   | 81/120 [03:35<00:54,  1.41s/it] 68%|██████▊   | 82/120 [03:35<00:40,  1.07s/it] 69%|██████▉   | 83/120 [03:35<00:31,  1.19it/s] 70%|███████   | 84/120 [03:36<00:26,  1.37it/s] 71%|███████   | 85/120 [03:36<00:20,  1.67it/s] 72%|███████▏  | 86/120 [03:36<00:17,  1.97it/s] 72%|███████▎  | 87/120 [03:37<00:14,  2.26it/s] 73%|███████▎  | 88/120 [03:37<00:12,  2.51it/s] 74%|███████▍  | 89/120 [03:37<00:11,  2.73it/s] 75%|███████▌  | 90/120 [03:38<00:10,  2.91it/s] 76%|███████▌  | 91/120 [03:38<00:09,  3.04it/s] 77%|███████▋  | 92/120 [03:38<00:08,  3.15it/s] 78%|███████▊  | 93/120 [03:39<00:08,  3.23it/s] 78%|███████▊  | 94/120 [03:39<00:10,  2.45it/s] 79%|███████▉  | 95/120 [03:39<00:09,  2.68it/s] 80%|████████  | 96/120 [03:40<00:07,  3.03it/s][INFO|trainer.py:2140] 2023-08-28 19:39:14,592 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:39:14,592 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 19:39:14,592 >>   Batch size = 8
{'eval_loss': 1.0373544692993164, 'eval_runtime': 14.6794, 'eval_samples_per_second': 295.789, 'eval_steps_per_second': 36.991, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.71it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.60it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.70it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.69it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.14it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.60it/s][A
  7%|▋         | 38/543 [00:00<00:11, 45.40it/s][A
  8%|▊         | 43/543 [00:00<00:11, 45.11it/s][A
  9%|▉         | 48/543 [00:01<00:10, 45.24it/s][A
 10%|▉         | 53/543 [00:01<00:10, 45.31it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.47it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.48it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.58it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 45.35it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 45.28it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 45.02it/s][A
 16%|█▌        | 88/543 [00:02<00:10, 44.94it/s][A
 17%|█▋        | 93/543 [00:02<00:27, 16.60it/s][A
 18%|█▊        | 98/543 [00:02<00:21, 20.51it/s][A
 19%|█▉        | 103/543 [00:02<00:17, 24.58it/s][A
 20%|█▉        | 108/543 [00:02<00:15, 28.57it/s][A
 21%|██        | 113/543 [00:03<00:13, 32.19it/s][A
 22%|██▏       | 118/543 [00:03<00:12, 35.34it/s][A
 23%|██▎       | 123/543 [00:03<00:11, 37.92it/s][A
 24%|██▎       | 128/543 [00:03<00:10, 39.91it/s][A
 24%|██▍       | 133/543 [00:03<00:09, 41.09it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 41.92it/s][A
 26%|██▋       | 143/543 [00:03<00:09, 42.76it/s][A
 27%|██▋       | 148/543 [00:03<00:09, 43.50it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 44.03it/s][A
 29%|██▉       | 158/543 [00:04<00:08, 44.57it/s][A
 30%|███       | 163/543 [00:04<00:08, 44.91it/s][A
 31%|███       | 168/543 [00:04<00:08, 45.12it/s][A
 32%|███▏      | 173/543 [00:04<00:08, 45.29it/s][A
 33%|███▎      | 178/543 [00:04<00:08, 45.05it/s][A
 34%|███▎      | 183/543 [00:04<00:08, 44.87it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 44.83it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 44.95it/s][A
 36%|███▋      | 198/543 [00:05<00:07, 45.02it/s][A
 37%|███▋      | 203/543 [00:05<00:08, 40.56it/s][A
 38%|███▊      | 208/543 [00:05<00:07, 41.99it/s][A
 39%|███▉      | 213/543 [00:06<00:21, 15.55it/s][A
 40%|████      | 218/543 [00:06<00:16, 19.41it/s][A
 41%|████      | 223/543 [00:06<00:13, 23.47it/s][A
 42%|████▏     | 228/543 [00:06<00:11, 27.50it/s][A
 43%|████▎     | 233/543 [00:06<00:09, 31.22it/s][A
 44%|████▍     | 238/543 [00:06<00:08, 34.47it/s][A
 45%|████▍     | 243/543 [00:06<00:08, 37.25it/s][A
 46%|████▌     | 248/543 [00:06<00:07, 39.37it/s][A
 47%|████▋     | 253/543 [00:06<00:07, 40.64it/s][A
 48%|████▊     | 258/543 [00:07<00:06, 41.61it/s][A
 48%|████▊     | 263/543 [00:07<00:06, 42.57it/s][A
 49%|████▉     | 268/543 [00:07<00:06, 43.41it/s][A
 50%|█████     | 273/543 [00:07<00:06, 43.95it/s][A
 51%|█████     | 278/543 [00:07<00:05, 44.44it/s][A
 52%|█████▏    | 283/543 [00:07<00:05, 44.65it/s][A
 53%|█████▎    | 288/543 [00:07<00:05, 44.86it/s][A
 54%|█████▍    | 293/543 [00:07<00:05, 44.93it/s][A
 55%|█████▍    | 298/543 [00:07<00:05, 44.74it/s][A
 56%|█████▌    | 303/543 [00:08<00:05, 44.58it/s][A
 57%|█████▋    | 308/543 [00:08<00:05, 44.59it/s][A
 58%|█████▊    | 313/543 [00:08<00:05, 44.67it/s][A
 59%|█████▊    | 318/543 [00:08<00:07, 28.21it/s][A
 59%|█████▉    | 323/543 [00:08<00:06, 31.89it/s][A
 60%|██████    | 328/543 [00:08<00:06, 35.08it/s][A
 61%|██████▏   | 333/543 [00:08<00:05, 37.65it/s][A
 62%|██████▏   | 338/543 [00:09<00:05, 39.72it/s][A
 63%|██████▎   | 343/543 [00:09<00:04, 41.33it/s][A
 64%|██████▍   | 348/543 [00:09<00:04, 42.54it/s][A
 65%|██████▌   | 353/543 [00:09<00:04, 43.43it/s][A
 66%|██████▌   | 358/543 [00:09<00:04, 43.60it/s][A
 67%|██████▋   | 363/543 [00:09<00:04, 43.75it/s][A
 68%|██████▊   | 368/543 [00:09<00:03, 44.24it/s][A
 69%|██████▊   | 373/543 [00:09<00:03, 44.46it/s][A
 70%|██████▉   | 378/543 [00:09<00:03, 44.85it/s][A
 71%|███████   | 383/543 [00:10<00:03, 45.09it/s][A
 71%|███████▏  | 388/543 [00:10<00:03, 45.34it/s][A
 72%|███████▏  | 393/543 [00:10<00:03, 45.48it/s][A
 73%|███████▎  | 398/543 [00:10<00:03, 45.53it/s][A
 74%|███████▍  | 403/543 [00:10<00:03, 45.21it/s][A
 75%|███████▌  | 408/543 [00:10<00:02, 45.02it/s][A
 76%|███████▌  | 413/543 [00:10<00:02, 45.04it/s][A
 77%|███████▋  | 418/543 [00:10<00:02, 45.14it/s][A
 78%|███████▊  | 423/543 [00:10<00:02, 45.19it/s][A
 79%|███████▉  | 428/543 [00:11<00:02, 45.38it/s][A
 80%|███████▉  | 433/543 [00:11<00:02, 45.51it/s][A
 81%|████████  | 438/543 [00:11<00:02, 45.61it/s][A
 82%|████████▏ | 443/543 [00:12<00:02, 45.50it/s][A
 83%|████████▎ | 448/543 [00:12<00:06, 14.36it/s][A
 83%|████████▎ | 453/543 [00:12<00:04, 18.06it/s][A
 84%|████████▍ | 458/543 [00:12<00:03, 22.10it/s][A
 85%|████████▌ | 463/543 [00:12<00:03, 26.19it/s][A
 86%|████████▌ | 468/543 [00:12<00:02, 30.07it/s][A
 87%|████████▋ | 473/543 [00:12<00:02, 33.46it/s][A
 88%|████████▊ | 478/543 [00:12<00:01, 36.40it/s][A
 89%|████████▉ | 483/543 [00:13<00:01, 38.70it/s][A
 90%|████████▉ | 488/543 [00:13<00:01, 40.24it/s][A
 91%|█████████ | 493/543 [00:13<00:01, 41.32it/s][A
 92%|█████████▏| 498/543 [00:13<00:01, 42.31it/s][A
 93%|█████████▎| 503/543 [00:13<00:00, 43.19it/s][A
 94%|█████████▎| 508/543 [00:13<00:00, 43.95it/s][A
 94%|█████████▍| 513/543 [00:13<00:00, 44.35it/s][A
 95%|█████████▌| 518/543 [00:13<00:00, 44.81it/s][A
 96%|█████████▋| 523/543 [00:13<00:00, 45.19it/s][A
 97%|█████████▋| 528/543 [00:14<00:00, 45.28it/s][A
 98%|█████████▊| 533/543 [00:14<00:00, 45.15it/s][A
 99%|█████████▉| 538/543 [00:14<00:00, 44.91it/s][A
100%|██████████| 543/543 [00:14<00:00, 44.86it/s][A                                                
                                                 [A 80%|████████  | 96/120 [03:55<00:07,  3.03it/s]
100%|██████████| 543/543 [00:14<00:00, 44.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:39:30,394 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-28 19:39:32,379 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:39:53,936 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:39:54,676 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:39:55,519 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96/special_tokens_map.json
 81%|████████  | 97/120 [05:00<09:17, 24.25s/it] 82%|████████▏ | 98/120 [05:00<06:17, 17.15s/it] 82%|████████▎ | 99/120 [05:01<04:13, 12.09s/it] 83%|████████▎ | 100/120 [05:01<02:51,  8.55s/it] 84%|████████▍ | 101/120 [05:01<01:55,  6.07s/it] 85%|████████▌ | 102/120 [05:01<01:18,  4.34s/it] 86%|████████▌ | 103/120 [05:02<00:53,  3.13s/it] 87%|████████▋ | 104/120 [05:02<00:36,  2.27s/it] 88%|████████▊ | 105/120 [05:02<00:25,  1.68s/it] 88%|████████▊ | 106/120 [05:03<00:17,  1.26s/it] 89%|████████▉ | 107/120 [05:03<00:12,  1.03it/s] 90%|█████████ | 108/120 [05:03<00:09,  1.26it/s] 91%|█████████ | 109/120 [05:04<00:07,  1.56it/s] 92%|█████████▏| 110/120 [05:04<00:05,  1.86it/s] 92%|█████████▎| 111/120 [05:04<00:04,  2.15it/s] 93%|█████████▎| 112/120 [05:04<00:03,  2.42it/s] 94%|█████████▍| 113/120 [05:05<00:02,  2.66it/s] 95%|█████████▌| 114/120 [05:05<00:02,  2.85it/s] 96%|█████████▌| 115/120 [05:05<00:01,  3.00it/s] 97%|█████████▋| 116/120 [05:06<00:01,  2.51it/s] 98%|█████████▊| 117/120 [05:06<00:01,  2.73it/s] 98%|█████████▊| 118/120 [05:07<00:00,  2.13it/s] 99%|█████████▉| 119/120 [05:07<00:00,  2.40it/s]100%|██████████| 120/120 [05:07<00:00,  2.78it/s][INFO|trainer.py:2140] 2023-08-28 19:40:42,354 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:40:42,354 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 19:40:42,355 >>   Batch size = 8
{'eval_loss': 1.036316156387329, 'eval_runtime': 14.9442, 'eval_samples_per_second': 290.548, 'eval_steps_per_second': 36.335, 'epoch': 4.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.80it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.58it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.45it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.51it/s][A
  5%|▌         | 28/543 [00:00<00:11, 45.86it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.60it/s][A
  7%|▋         | 38/543 [00:00<00:11, 45.50it/s][A
  8%|▊         | 43/543 [00:00<00:11, 45.36it/s][A
  9%|▉         | 48/543 [00:01<00:10, 45.45it/s][A
 10%|▉         | 53/543 [00:01<00:10, 45.63it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.62it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.52it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.28it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 45.20it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 45.20it/s][A
 15%|█▌        | 83/543 [00:02<00:10, 45.13it/s][A
 16%|█▌        | 88/543 [00:02<00:26, 17.16it/s][A
 17%|█▋        | 93/543 [00:02<00:21, 21.13it/s][A
 18%|█▊        | 98/543 [00:02<00:17, 25.22it/s][A
 19%|█▉        | 103/543 [00:02<00:15, 29.15it/s][A
 20%|█▉        | 108/543 [00:02<00:13, 32.72it/s][A
 21%|██        | 113/543 [00:03<00:11, 35.85it/s][A
 22%|██▏       | 118/543 [00:03<00:11, 38.31it/s][A
 23%|██▎       | 123/543 [00:03<00:10, 40.25it/s][A
 24%|██▎       | 128/543 [00:03<00:10, 41.30it/s][A
 24%|██▍       | 133/543 [00:03<00:09, 42.10it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 42.83it/s][A
 26%|██▋       | 143/543 [00:03<00:09, 43.51it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 44.20it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 44.59it/s][A
 29%|██▉       | 158/543 [00:04<00:08, 44.90it/s][A
 30%|███       | 163/543 [00:04<00:08, 45.06it/s][A
 31%|███       | 168/543 [00:04<00:08, 45.12it/s][A
 32%|███▏      | 173/543 [00:04<00:08, 44.97it/s][A
 33%|███▎      | 178/543 [00:04<00:08, 44.67it/s][A
 34%|███▎      | 183/543 [00:04<00:08, 44.68it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 44.69it/s][A
 36%|███▌      | 193/543 [00:05<00:07, 44.92it/s][A
 36%|███▋      | 198/543 [00:05<00:14, 23.53it/s][A
 37%|███▋      | 203/543 [00:05<00:12, 27.54it/s][A
 38%|███▊      | 208/543 [00:05<00:10, 31.30it/s][A
 39%|███▉      | 213/543 [00:05<00:09, 34.60it/s][A
 40%|████      | 218/543 [00:05<00:08, 37.19it/s][A
 41%|████      | 223/543 [00:05<00:08, 39.47it/s][A
 42%|████▏     | 228/543 [00:05<00:07, 41.18it/s][A
 43%|████▎     | 233/543 [00:06<00:07, 42.44it/s][A
 44%|████▍     | 238/543 [00:06<00:07, 42.90it/s][A
 45%|████▍     | 243/543 [00:06<00:06, 43.23it/s][A
 46%|████▌     | 248/543 [00:06<00:06, 43.62it/s][A
 47%|████▋     | 253/543 [00:06<00:06, 44.03it/s][A
 48%|████▊     | 258/543 [00:06<00:06, 44.50it/s][A
 48%|████▊     | 263/543 [00:06<00:06, 44.88it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 45.05it/s][A
 50%|█████     | 273/543 [00:06<00:05, 45.23it/s][A
 51%|█████     | 278/543 [00:07<00:05, 45.39it/s][A
 52%|█████▏    | 283/543 [00:07<00:05, 45.11it/s][A
 53%|█████▎    | 288/543 [00:07<00:05, 44.94it/s][A
 54%|█████▍    | 293/543 [00:07<00:05, 44.79it/s][A
 55%|█████▍    | 298/543 [00:07<00:05, 44.95it/s][A
 56%|█████▌    | 303/543 [00:07<00:05, 45.08it/s][A
 57%|█████▋    | 308/543 [00:07<00:05, 45.22it/s][A
 58%|█████▊    | 313/543 [00:07<00:05, 45.38it/s][A
 59%|█████▊    | 318/543 [00:08<00:04, 45.39it/s][A
 59%|█████▉    | 323/543 [00:08<00:14, 15.62it/s][A
 60%|██████    | 328/543 [00:08<00:11, 19.46it/s][A
 61%|██████▏   | 333/543 [00:08<00:08, 23.51it/s][A
 62%|██████▏   | 338/543 [00:09<00:07, 27.50it/s][A
 63%|██████▎   | 343/543 [00:09<00:06, 31.24it/s][A
 64%|██████▍   | 348/543 [00:09<00:05, 34.48it/s][A
 65%|██████▌   | 353/543 [00:09<00:05, 37.19it/s][A
 66%|██████▌   | 358/543 [00:09<00:04, 39.33it/s][A
 67%|██████▋   | 363/543 [00:10<00:04, 40.63it/s][A
 68%|██████▊   | 368/543 [00:10<00:12, 13.49it/s][A
 69%|██████▊   | 373/543 [00:10<00:09, 17.11it/s][A
 70%|██████▉   | 378/543 [00:10<00:07, 21.05it/s][A
 71%|███████   | 383/543 [00:10<00:06, 25.08it/s][A
 71%|███████▏  | 388/543 [00:11<00:05, 29.03it/s][A
 72%|███████▏  | 393/543 [00:11<00:04, 32.58it/s][A
 73%|███████▎  | 398/543 [00:11<00:04, 35.63it/s][A
 74%|███████▍  | 403/543 [00:11<00:03, 38.05it/s][A
 75%|███████▌  | 408/543 [00:14<00:28,  4.77it/s][A
 76%|███████▌  | 412/543 [00:14<00:21,  6.03it/s][A
 77%|███████▋  | 417/543 [00:14<00:15,  8.28it/s][A
 78%|███████▊  | 422/543 [00:14<00:10, 11.08it/s][A
 79%|███████▊  | 427/543 [00:15<00:08, 14.42it/s][A
 80%|███████▉  | 432/543 [00:15<00:06, 18.22it/s][A
 80%|████████  | 437/543 [00:15<00:04, 22.28it/s][A
 81%|████████▏ | 442/543 [00:15<00:03, 26.33it/s][A
 82%|████████▏ | 447/543 [00:15<00:03, 30.20it/s][A
 83%|████████▎ | 452/543 [00:15<00:02, 33.39it/s][A
 84%|████████▍ | 457/543 [00:15<00:02, 36.16it/s][A
 85%|████████▌ | 462/543 [00:15<00:02, 38.49it/s][A
 86%|████████▌ | 467/543 [00:15<00:01, 40.29it/s][A
 87%|████████▋ | 472/543 [00:16<00:01, 41.67it/s][A
 88%|████████▊ | 477/543 [00:16<00:01, 42.73it/s][A
 89%|████████▉ | 482/543 [00:16<00:01, 43.54it/s][A
 90%|████████▉ | 487/543 [00:16<00:01, 44.07it/s][A
 91%|█████████ | 492/543 [00:16<00:01, 44.44it/s][A
 92%|█████████▏| 497/543 [00:16<00:01, 44.54it/s][A
 92%|█████████▏| 502/543 [00:16<00:00, 44.56it/s][A
 93%|█████████▎| 507/543 [00:16<00:00, 44.54it/s][A
 94%|█████████▍| 512/543 [00:16<00:00, 44.78it/s][A
 95%|█████████▌| 517/543 [00:17<00:00, 45.03it/s][A
 96%|█████████▌| 522/543 [00:17<00:00, 45.23it/s][A
 97%|█████████▋| 527/543 [00:17<00:00, 45.28it/s][A
 98%|█████████▊| 532/543 [00:17<00:00, 45.30it/s][A
 99%|█████████▉| 537/543 [00:17<00:00, 45.28it/s][A
100%|█████████▉| 542/543 [00:17<00:00, 45.11it/s][A                                                 
                                                 [A100%|██████████| 120/120 [05:25<00:00,  2.78it/s]
100%|██████████| 543/543 [00:17<00:00, 45.11it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:41:00,260 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 19:41:00,761 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:41:23,123 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:41:23,320 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:41:23,529 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:41:58,830 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:41:58,863 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48 (score: 1.0222392082214355).
                                                 100%|██████████| 120/120 [07:03<00:00,  2.78it/s]100%|██████████| 120/120 [07:03<00:00,  3.53s/it]
[INFO|trainer.py:1894] 2023-08-28 19:42:37,467 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 19:42:37,712 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:42:56,068 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:42:56,881 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:42:57,105 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:42:59,385 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,421 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,421 >>   train_loss               =     0.5763
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,422 >>   train_runtime            = 0:07:02.96
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,422 >>   train_samples            =       1520
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,422 >>   train_samples_per_second =     17.968
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:42:59,422 >>   train_steps_per_second   =      0.284
{'eval_loss': 1.0379360914230347, 'eval_runtime': 17.7366, 'eval_samples_per_second': 244.804, 'eval_steps_per_second': 30.615, 'epoch': 5.0}
{'train_runtime': 422.9665, 'train_samples_per_second': 17.968, 'train_steps_per_second': 0.284, 'train_loss': 0.5762881596883138, 'epoch': 5.0}
08/28/2023 19:43:00 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:43:00,625 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:43:00,625 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 19:43:00,625 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 56.57it/s]  2%|▏         | 12/543 [00:00<00:10, 49.73it/s]  3%|▎         | 18/543 [00:00<00:10, 47.89it/s]  4%|▍         | 23/543 [00:00<00:11, 47.25it/s]  5%|▌         | 28/543 [00:00<00:11, 46.81it/s]  6%|▌         | 33/543 [00:00<00:10, 46.53it/s]  7%|▋         | 38/543 [00:00<00:10, 46.23it/s]  8%|▊         | 43/543 [00:00<00:10, 45.95it/s]  9%|▉         | 48/543 [00:01<00:10, 45.49it/s] 10%|▉         | 53/543 [00:01<00:10, 45.30it/s] 11%|█         | 58/543 [00:01<00:10, 45.17it/s] 12%|█▏        | 63/543 [00:01<00:10, 45.43it/s] 13%|█▎        | 68/543 [00:01<00:10, 45.64it/s] 13%|█▎        | 73/543 [00:01<00:10, 45.71it/s] 14%|█▍        | 78/543 [00:01<00:10, 45.78it/s] 15%|█▌        | 83/543 [00:01<00:10, 45.78it/s] 16%|█▌        | 88/543 [00:02<00:12, 35.23it/s] 17%|█▋        | 93/543 [00:02<00:11, 37.82it/s] 18%|█▊        | 98/543 [00:02<00:11, 39.93it/s] 19%|█▉        | 103/543 [00:02<00:10, 41.48it/s] 20%|█▉        | 108/543 [00:02<00:10, 42.63it/s] 21%|██        | 113/543 [00:02<00:09, 43.47it/s] 22%|██▏       | 118/543 [00:02<00:09, 44.05it/s] 23%|██▎       | 123/543 [00:02<00:09, 44.60it/s] 24%|██▎       | 128/543 [00:02<00:09, 44.56it/s] 24%|██▍       | 133/543 [00:03<00:09, 44.76it/s] 25%|██▌       | 138/543 [00:03<00:09, 44.87it/s] 26%|██▋       | 143/543 [00:03<00:08, 45.13it/s] 27%|██▋       | 148/543 [00:03<00:08, 45.31it/s] 28%|██▊       | 153/543 [00:03<00:08, 45.52it/s] 29%|██▉       | 158/543 [00:03<00:08, 45.69it/s] 30%|███       | 163/543 [00:03<00:08, 45.71it/s] 31%|███       | 168/543 [00:03<00:08, 45.62it/s] 32%|███▏      | 173/543 [00:03<00:08, 45.42it/s] 33%|███▎      | 178/543 [00:03<00:08, 45.27it/s] 34%|███▎      | 183/543 [00:04<00:07, 45.24it/s] 35%|███▍      | 188/543 [00:04<00:07, 45.04it/s] 36%|███▌      | 193/543 [00:04<00:07, 45.31it/s] 36%|███▋      | 198/543 [00:04<00:07, 45.48it/s] 37%|███▋      | 203/543 [00:04<00:07, 45.57it/s] 38%|███▊      | 208/543 [00:04<00:07, 45.65it/s] 39%|███▉      | 213/543 [00:04<00:07, 45.64it/s] 40%|████      | 218/543 [00:04<00:07, 45.51it/s] 41%|████      | 223/543 [00:05<00:17, 17.95it/s] 42%|████▏     | 228/543 [00:05<00:14, 21.98it/s] 43%|████▎     | 233/543 [00:05<00:11, 26.03it/s] 44%|████▍     | 238/543 [00:05<00:10, 29.87it/s] 45%|████▍     | 243/543 [00:05<00:09, 33.33it/s] 46%|████▌     | 248/543 [00:06<00:08, 36.31it/s] 47%|████▋     | 253/543 [00:06<00:07, 38.72it/s] 48%|████▊     | 258/543 [00:06<00:07, 40.51it/s] 48%|████▊     | 263/543 [00:06<00:06, 41.67it/s] 49%|████▉     | 268/543 [00:06<00:06, 42.70it/s] 50%|█████     | 273/543 [00:06<00:06, 43.51it/s] 51%|█████     | 278/543 [00:06<00:06, 44.17it/s] 52%|█████▏    | 283/543 [00:06<00:05, 44.59it/s] 53%|█████▎    | 288/543 [00:06<00:05, 44.90it/s] 54%|█████▍    | 293/543 [00:07<00:05, 45.16it/s] 55%|█████▍    | 298/543 [00:07<00:05, 45.23it/s] 56%|█████▌    | 303/543 [00:07<00:05, 45.20it/s] 57%|█████▋    | 308/543 [00:07<00:05, 45.08it/s] 58%|█████▊    | 313/543 [00:07<00:05, 45.04it/s] 59%|█████▊    | 318/543 [00:07<00:04, 45.11it/s] 59%|█████▉    | 323/543 [00:07<00:04, 45.16it/s] 60%|██████    | 328/543 [00:07<00:04, 45.34it/s] 61%|██████▏   | 333/543 [00:07<00:04, 45.40it/s] 62%|██████▏   | 338/543 [00:08<00:06, 32.20it/s] 63%|██████▎   | 343/543 [00:08<00:05, 35.38it/s] 64%|██████▍   | 348/543 [00:08<00:05, 37.98it/s] 65%|██████▌   | 353/543 [00:08<00:04, 39.87it/s] 66%|██████▌   | 358/543 [00:08<00:04, 41.68it/s] 67%|██████▋   | 363/543 [00:08<00:04, 42.80it/s] 68%|██████▊   | 368/543 [00:08<00:04, 43.68it/s] 69%|██████▊   | 373/543 [00:08<00:03, 44.16it/s] 70%|██████▉   | 378/543 [00:09<00:03, 44.19it/s] 71%|███████   | 383/543 [00:09<00:03, 44.17it/s] 71%|███████▏  | 388/543 [00:09<00:03, 44.38it/s] 72%|███████▏  | 393/543 [00:09<00:03, 44.65it/s] 73%|███████▎  | 398/543 [00:09<00:03, 44.93it/s] 74%|███████▍  | 403/543 [00:09<00:03, 45.17it/s] 75%|███████▌  | 408/543 [00:09<00:02, 45.30it/s] 76%|███████▌  | 413/543 [00:09<00:02, 45.32it/s] 77%|███████▋  | 418/543 [00:09<00:02, 45.47it/s] 78%|███████▊  | 423/543 [00:10<00:02, 45.38it/s] 79%|███████▉  | 428/543 [00:10<00:02, 45.27it/s] 80%|███████▉  | 433/543 [00:10<00:02, 45.10it/s] 81%|████████  | 438/543 [00:10<00:02, 45.15it/s] 82%|████████▏ | 443/543 [00:10<00:02, 45.17it/s] 83%|████████▎ | 448/543 [00:10<00:02, 45.24it/s] 83%|████████▎ | 453/543 [00:10<00:01, 45.34it/s] 84%|████████▍ | 458/543 [00:10<00:01, 45.44it/s] 85%|████████▌ | 463/543 [00:10<00:01, 45.44it/s] 86%|████████▌ | 468/543 [00:11<00:03, 23.40it/s] 87%|████████▋ | 473/543 [00:11<00:02, 27.43it/s] 88%|████████▊ | 478/543 [00:11<00:02, 31.23it/s] 89%|████████▉ | 483/543 [00:11<00:01, 34.48it/s] 90%|████████▉ | 488/543 [00:11<00:01, 37.27it/s] 91%|█████████ | 493/543 [00:11<00:01, 39.49it/s] 92%|█████████▏| 498/543 [00:12<00:01, 41.25it/s] 93%|█████████▎| 503/543 [00:12<00:00, 42.48it/s] 94%|█████████▎| 508/543 [00:12<00:00, 43.00it/s] 94%|█████████▍| 513/543 [00:12<00:00, 43.25it/s] 95%|█████████▌| 518/543 [00:12<00:00, 43.60it/s] 96%|█████████▋| 523/543 [00:12<00:00, 44.07it/s] 97%|█████████▋| 528/543 [00:12<00:00, 44.48it/s] 98%|█████████▊| 533/543 [00:12<00:00, 44.85it/s] 99%|█████████▉| 538/543 [00:12<00:00, 45.08it/s]100%|██████████| 543/543 [00:13<00:00, 45.24it/s]100%|██████████| 543/543 [00:13<00:00, 41.48it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:43:13,752 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:13,752 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:13,752 >>   eval_loss               =     1.0222
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:13,752 >>   eval_runtime            = 0:00:13.12
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:13,752 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:13,752 >>   eval_samples_per_second =     330.78
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:13,752 >>   eval_steps_per_second   =     41.367
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:43:13,752 >>   perplexity              =     2.7794
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,801 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,853 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,853 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,853 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:45,853 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:43:46,760 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:43:46,761 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:43:47,671 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:43:48,987 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:43:49,127 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,567 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,567 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,567 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:51,567 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:43:52,793 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:43:53,007 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:43:53,653 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:43:53,886 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:43:53,887 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-120
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-48
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-24
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-96
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-72
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.81it/s]Extractor Predicting: 2it [00:01,  1.79it/s]Extractor Predicting: 3it [00:01,  1.75it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.62it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.75it/s]Extractor Predicting: 11it [00:06,  1.77it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:07,  1.55it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.43it/s]Extractor Predicting: 16it [00:09,  1.48it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:11,  1.49it/s]Extractor Predicting: 20it [00:12,  1.46it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:17,  1.53it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.29it/s]Extractor Predicting: 32it [00:20,  1.37it/s]Extractor Predicting: 33it [00:21,  1.42it/s]Extractor Predicting: 34it [00:22,  1.45it/s]Extractor Predicting: 35it [00:22,  1.43it/s]Extractor Predicting: 36it [00:23,  1.44it/s]Extractor Predicting: 37it [00:24,  1.46it/s]Extractor Predicting: 38it [00:24,  1.47it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.55it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:30,  1.56it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.56it/s]Extractor Predicting: 51it [00:33,  1.57it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.54it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:36,  1.55it/s]Extractor Predicting: 57it [00:37,  1.53it/s]Extractor Predicting: 58it [00:37,  1.57it/s]Extractor Predicting: 59it [00:38,  1.40it/s]Extractor Predicting: 60it [00:39,  1.44it/s]Extractor Predicting: 61it [00:39,  1.49it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:41,  1.53it/s]Extractor Predicting: 64it [00:41,  1.54it/s]Extractor Predicting: 65it [00:42,  1.59it/s]Extractor Predicting: 66it [00:43,  1.60it/s]Extractor Predicting: 67it [00:43,  1.60it/s]Extractor Predicting: 68it [00:44,  1.37it/s]Extractor Predicting: 69it [00:45,  1.43it/s]Extractor Predicting: 70it [00:46,  1.06it/s]Extractor Predicting: 71it [00:47,  1.18it/s]Extractor Predicting: 72it [00:48,  1.28it/s]Extractor Predicting: 73it [00:48,  1.34it/s]Extractor Predicting: 74it [00:49,  1.32it/s]Extractor Predicting: 75it [00:50,  1.35it/s]Extractor Predicting: 76it [00:50,  1.41it/s]Extractor Predicting: 77it [00:51,  1.47it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:52,  1.37it/s]Extractor Predicting: 80it [00:53,  1.44it/s]Extractor Predicting: 81it [00:54,  1.47it/s]Extractor Predicting: 82it [00:55,  1.23it/s]Extractor Predicting: 83it [00:55,  1.32it/s]Extractor Predicting: 84it [00:56,  1.23it/s]Extractor Predicting: 85it [00:57,  1.33it/s]Extractor Predicting: 86it [00:58,  1.38it/s]Extractor Predicting: 87it [00:58,  1.43it/s]Extractor Predicting: 88it [00:59,  1.46it/s]Extractor Predicting: 89it [01:00,  1.51it/s]Extractor Predicting: 90it [01:00,  1.52it/s]Extractor Predicting: 91it [01:01,  1.51it/s]Extractor Predicting: 92it [01:02,  1.50it/s]Extractor Predicting: 93it [01:02,  1.45it/s]Extractor Predicting: 94it [01:03,  1.50it/s]Extractor Predicting: 95it [01:04,  1.51it/s]Extractor Predicting: 96it [01:04,  1.55it/s]Extractor Predicting: 97it [01:05,  1.58it/s]Extractor Predicting: 98it [01:06,  1.47it/s]Extractor Predicting: 99it [01:06,  1.48it/s]Extractor Predicting: 100it [01:07,  1.52it/s]Extractor Predicting: 101it [01:07,  1.54it/s]Extractor Predicting: 102it [01:08,  1.55it/s]Extractor Predicting: 103it [01:09,  1.52it/s]Extractor Predicting: 104it [01:09,  1.55it/s]Extractor Predicting: 105it [01:10,  1.53it/s]Extractor Predicting: 106it [01:11,  1.56it/s]Extractor Predicting: 107it [01:11,  1.56it/s]Extractor Predicting: 108it [01:12,  1.47it/s]Extractor Predicting: 109it [01:13,  1.50it/s]Extractor Predicting: 110it [01:13,  1.51it/s]Extractor Predicting: 111it [01:14,  1.54it/s]Extractor Predicting: 112it [01:15,  1.58it/s]Extractor Predicting: 113it [01:15,  1.57it/s]Extractor Predicting: 114it [01:16,  1.57it/s]Extractor Predicting: 115it [01:17,  1.56it/s]Extractor Predicting: 116it [01:17,  1.57it/s]Extractor Predicting: 117it [01:18,  1.58it/s]Extractor Predicting: 118it [01:19,  1.53it/s]Extractor Predicting: 119it [01:19,  1.53it/s]Extractor Predicting: 120it [01:20,  1.53it/s]Extractor Predicting: 121it [01:20,  1.52it/s]Extractor Predicting: 122it [01:21,  1.51it/s]Extractor Predicting: 123it [01:22,  1.51it/s]Extractor Predicting: 124it [01:22,  1.53it/s]Extractor Predicting: 125it [01:23,  1.56it/s]Extractor Predicting: 126it [01:24,  1.53it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:25,  1.42it/s]Extractor Predicting: 129it [01:26,  1.47it/s]Extractor Predicting: 130it [01:26,  1.53it/s]Extractor Predicting: 131it [01:27,  1.45it/s]Extractor Predicting: 132it [01:28,  1.48it/s]Extractor Predicting: 133it [01:28,  1.51it/s]Extractor Predicting: 134it [01:29,  1.53it/s]Extractor Predicting: 135it [01:30,  1.56it/s]Extractor Predicting: 136it [01:31,  1.39it/s]Extractor Predicting: 137it [01:31,  1.45it/s]Extractor Predicting: 138it [01:32,  1.46it/s]Extractor Predicting: 139it [01:33,  1.48it/s]Extractor Predicting: 140it [01:33,  1.53it/s]Extractor Predicting: 141it [01:34,  1.46it/s]Extractor Predicting: 142it [01:35,  1.52it/s]Extractor Predicting: 143it [01:35,  1.55it/s]Extractor Predicting: 144it [01:36,  1.57it/s]Extractor Predicting: 145it [01:37,  1.43it/s]Extractor Predicting: 146it [01:37,  1.42it/s]Extractor Predicting: 147it [01:38,  1.47it/s]Extractor Predicting: 148it [01:39,  1.52it/s]Extractor Predicting: 149it [01:39,  1.55it/s]Extractor Predicting: 150it [01:40,  1.53it/s]Extractor Predicting: 151it [01:41,  1.24it/s]Extractor Predicting: 152it [01:42,  1.32it/s]Extractor Predicting: 153it [01:42,  1.37it/s]Extractor Predicting: 154it [01:43,  1.43it/s]Extractor Predicting: 155it [01:44,  1.23it/s]Extractor Predicting: 156it [01:45,  1.30it/s]Extractor Predicting: 157it [01:45,  1.37it/s]Extractor Predicting: 158it [01:46,  1.43it/s]Extractor Predicting: 159it [01:47,  1.35it/s]Extractor Predicting: 160it [01:47,  1.44it/s]Extractor Predicting: 161it [01:48,  1.50it/s]Extractor Predicting: 162it [01:49,  1.53it/s]Extractor Predicting: 163it [01:49,  1.48it/s]Extractor Predicting: 163it [01:49,  1.48it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:18,099 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:18,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:18,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:18,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:18,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:46:19,142 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:46:19,143 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:46:19,967 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:46:20,999 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:46:20,999 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,050 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:46:24,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:46:25,529 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:46:25,530 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:46:26,895 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:46:27,585 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:46:27,683 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.18072289156626506,
  "recall": 0.031091662828189773,
  "score": 0.0530556101395166,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.26it/s]Extractor Predicting: 11it [00:07,  1.37it/s]Extractor Predicting: 12it [00:07,  1.46it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:10,  1.35it/s]Extractor Predicting: 16it [00:10,  1.42it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:12,  1.51it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:13,  1.39it/s]Extractor Predicting: 21it [00:14,  1.46it/s]Extractor Predicting: 22it [00:14,  1.49it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:16,  1.54it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.57it/s]Extractor Predicting: 27it [00:17,  1.62it/s]Extractor Predicting: 28it [00:18,  1.62it/s]Extractor Predicting: 29it [00:19,  1.63it/s]Extractor Predicting: 30it [00:20,  1.25it/s]Extractor Predicting: 31it [00:20,  1.35it/s]Extractor Predicting: 32it [00:21,  1.42it/s]Extractor Predicting: 33it [00:22,  1.48it/s]Extractor Predicting: 34it [00:22,  1.41it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:24,  1.52it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.45it/s]Extractor Predicting: 39it [00:26,  1.45it/s]Extractor Predicting: 40it [00:26,  1.49it/s]Extractor Predicting: 41it [00:27,  1.47it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:29,  1.46it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:31,  1.31it/s]Extractor Predicting: 47it [00:31,  1.36it/s]Extractor Predicting: 48it [00:32,  1.42it/s]Extractor Predicting: 49it [00:33,  1.44it/s]Extractor Predicting: 50it [00:33,  1.45it/s]Extractor Predicting: 51it [00:34,  1.48it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:36,  1.46it/s]Extractor Predicting: 55it [00:37,  1.44it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.51it/s]Extractor Predicting: 59it [00:39,  1.49it/s]Extractor Predicting: 60it [00:40,  1.48it/s]Extractor Predicting: 61it [00:41,  1.52it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:43,  1.49it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.48it/s]Extractor Predicting: 67it [00:45,  1.49it/s]Extractor Predicting: 68it [00:45,  1.49it/s]Extractor Predicting: 69it [00:46,  1.49it/s]Extractor Predicting: 70it [00:47,  1.44it/s]Extractor Predicting: 71it [00:47,  1.47it/s]Extractor Predicting: 72it [00:48,  1.49it/s]Extractor Predicting: 73it [00:49,  1.49it/s]Extractor Predicting: 74it [00:49,  1.53it/s]Extractor Predicting: 75it [00:50,  1.45it/s]Extractor Predicting: 76it [00:51,  1.49it/s]Extractor Predicting: 77it [00:51,  1.53it/s]Extractor Predicting: 78it [00:52,  1.54it/s]Extractor Predicting: 79it [00:53,  1.51it/s]Extractor Predicting: 80it [00:53,  1.51it/s]Extractor Predicting: 81it [00:54,  1.43it/s]Extractor Predicting: 82it [00:55,  1.48it/s]Extractor Predicting: 83it [00:55,  1.52it/s]Extractor Predicting: 84it [00:56,  1.49it/s]Extractor Predicting: 85it [00:57,  1.41it/s]Extractor Predicting: 86it [00:58,  1.46it/s]Extractor Predicting: 87it [00:58,  1.50it/s]Extractor Predicting: 88it [00:59,  1.50it/s]Extractor Predicting: 89it [00:59,  1.55it/s]Extractor Predicting: 90it [01:00,  1.37it/s]Extractor Predicting: 91it [01:01,  1.45it/s]Extractor Predicting: 92it [01:02,  1.45it/s]Extractor Predicting: 93it [01:02,  1.49it/s]Extractor Predicting: 94it [01:03,  1.51it/s]Extractor Predicting: 95it [01:04,  1.54it/s]Extractor Predicting: 96it [01:04,  1.58it/s]Extractor Predicting: 97it [01:05,  1.43it/s]Extractor Predicting: 98it [01:06,  1.43it/s]Extractor Predicting: 99it [01:06,  1.47it/s]Extractor Predicting: 100it [01:07,  1.49it/s]Extractor Predicting: 101it [01:08,  1.50it/s]Extractor Predicting: 102it [01:08,  1.50it/s]Extractor Predicting: 103it [01:09,  1.50it/s]Extractor Predicting: 104it [01:10,  1.53it/s]Extractor Predicting: 105it [01:10,  1.53it/s]Extractor Predicting: 106it [01:11,  1.50it/s]Extractor Predicting: 107it [01:12,  1.36it/s]Extractor Predicting: 108it [01:12,  1.42it/s]Extractor Predicting: 109it [01:13,  1.41it/s]Extractor Predicting: 110it [01:14,  1.45it/s]Extractor Predicting: 111it [01:15,  1.46it/s]Extractor Predicting: 112it [01:15,  1.44it/s]Extractor Predicting: 113it [01:16,  1.46it/s]Extractor Predicting: 114it [01:17,  1.48it/s]Extractor Predicting: 115it [01:17,  1.49it/s]Extractor Predicting: 116it [01:18,  1.48it/s]Extractor Predicting: 117it [01:19,  1.48it/s]Extractor Predicting: 118it [01:19,  1.53it/s]Extractor Predicting: 119it [01:20,  1.53it/s]Extractor Predicting: 120it [01:20,  1.58it/s]Extractor Predicting: 121it [01:21,  1.57it/s]Extractor Predicting: 122it [01:22,  1.50it/s]Extractor Predicting: 123it [01:22,  1.54it/s]Extractor Predicting: 124it [01:23,  1.59it/s]Extractor Predicting: 125it [01:24,  1.59it/s]Extractor Predicting: 126it [01:25,  1.41it/s]Extractor Predicting: 127it [01:25,  1.38it/s]Extractor Predicting: 128it [01:26,  1.42it/s]Extractor Predicting: 129it [01:27,  1.46it/s]Extractor Predicting: 130it [01:27,  1.49it/s]Extractor Predicting: 131it [01:28,  1.57it/s]Extractor Predicting: 132it [01:28,  1.52it/s]Extractor Predicting: 133it [01:29,  1.54it/s]Extractor Predicting: 134it [01:30,  1.58it/s]Extractor Predicting: 135it [01:30,  1.59it/s]Extractor Predicting: 136it [01:31,  1.60it/s]Extractor Predicting: 137it [01:32,  1.57it/s]Extractor Predicting: 138it [01:33,  1.39it/s]Extractor Predicting: 139it [01:33,  1.46it/s]Extractor Predicting: 140it [01:34,  1.49it/s]Extractor Predicting: 141it [01:34,  1.54it/s]Extractor Predicting: 142it [01:35,  1.54it/s]Extractor Predicting: 143it [01:36,  1.48it/s]Extractor Predicting: 144it [01:36,  1.51it/s]Extractor Predicting: 145it [01:37,  1.49it/s]Extractor Predicting: 146it [01:38,  1.47it/s]Extractor Predicting: 147it [01:38,  1.50it/s]Extractor Predicting: 148it [01:39,  1.31it/s]Extractor Predicting: 149it [01:40,  1.39it/s]Extractor Predicting: 150it [01:41,  1.44it/s]Extractor Predicting: 151it [01:41,  1.48it/s]Extractor Predicting: 152it [01:42,  1.50it/s]Extractor Predicting: 153it [01:43,  1.48it/s]Extractor Predicting: 154it [01:43,  1.52it/s]Extractor Predicting: 155it [01:44,  1.57it/s]Extractor Predicting: 156it [01:44,  1.54it/s]Extractor Predicting: 157it [01:45,  1.57it/s]Extractor Predicting: 158it [01:46,  1.50it/s]Extractor Predicting: 159it [01:46,  1.52it/s]Extractor Predicting: 160it [01:47,  1.57it/s]Extractor Predicting: 161it [01:48,  1.51it/s]Extractor Predicting: 162it [01:48,  1.57it/s]Extractor Predicting: 163it [01:49,  1.36it/s]Extractor Predicting: 164it [01:50,  1.41it/s]Extractor Predicting: 165it [01:51,  1.51it/s]Extractor Predicting: 166it [01:51,  1.55it/s]Extractor Predicting: 167it [01:52,  1.57it/s]Extractor Predicting: 168it [01:52,  1.62it/s]Extractor Predicting: 169it [01:53,  1.64it/s]Extractor Predicting: 170it [01:54,  1.62it/s]Extractor Predicting: 171it [01:54,  1.62it/s]Extractor Predicting: 172it [01:55,  1.63it/s]Extractor Predicting: 173it [01:56,  1.43it/s]Extractor Predicting: 174it [01:56,  1.51it/s]Extractor Predicting: 175it [01:57,  1.54it/s]Extractor Predicting: 176it [01:58,  1.55it/s]Extractor Predicting: 177it [01:58,  1.55it/s]Extractor Predicting: 178it [01:59,  1.44it/s]Extractor Predicting: 179it [01:59,  1.55it/s]Extractor Predicting: 180it [02:00,  1.56it/s]Extractor Predicting: 181it [02:01,  1.58it/s]Extractor Predicting: 182it [02:01,  1.58it/s]Extractor Predicting: 183it [02:02,  1.52it/s]Extractor Predicting: 184it [02:03,  1.54it/s]Extractor Predicting: 185it [02:03,  1.58it/s]Extractor Predicting: 186it [02:04,  1.57it/s]Extractor Predicting: 187it [02:05,  1.56it/s]Extractor Predicting: 188it [02:05,  1.55it/s]Extractor Predicting: 189it [02:06,  1.55it/s]Extractor Predicting: 190it [02:07,  1.50it/s]Extractor Predicting: 191it [02:07,  1.51it/s]Extractor Predicting: 192it [02:08,  1.54it/s]Extractor Predicting: 193it [02:08,  1.58it/s]Extractor Predicting: 194it [02:09,  1.60it/s]Extractor Predicting: 195it [02:10,  1.48it/s]Extractor Predicting: 196it [02:11,  1.52it/s]Extractor Predicting: 197it [02:11,  1.54it/s]Extractor Predicting: 198it [02:12,  1.55it/s]Extractor Predicting: 199it [02:12,  1.56it/s]Extractor Predicting: 200it [02:13,  1.48it/s]Extractor Predicting: 201it [02:14,  1.51it/s]Extractor Predicting: 202it [02:14,  1.53it/s]Extractor Predicting: 203it [02:15,  1.55it/s]Extractor Predicting: 204it [02:16,  1.57it/s]Extractor Predicting: 205it [02:16,  1.47it/s]Extractor Predicting: 206it [02:17,  1.49it/s]Extractor Predicting: 207it [02:18,  1.54it/s]Extractor Predicting: 208it [02:18,  1.55it/s]Extractor Predicting: 209it [02:19,  1.56it/s]Extractor Predicting: 210it [02:20,  1.43it/s]Extractor Predicting: 211it [02:20,  1.47it/s]Extractor Predicting: 212it [02:21,  1.52it/s]Extractor Predicting: 213it [02:22,  1.53it/s]Extractor Predicting: 214it [02:22,  1.55it/s]Extractor Predicting: 215it [02:23,  1.49it/s]Extractor Predicting: 216it [02:24,  1.53it/s]Extractor Predicting: 217it [02:24,  1.57it/s]Extractor Predicting: 218it [02:25,  1.58it/s]Extractor Predicting: 219it [02:26,  1.56it/s]Extractor Predicting: 220it [02:26,  1.48it/s]Extractor Predicting: 221it [02:27,  1.49it/s]Extractor Predicting: 222it [02:28,  1.37it/s]Extractor Predicting: 223it [02:29,  1.37it/s]Extractor Predicting: 224it [02:29,  1.39it/s]Extractor Predicting: 225it [02:30,  1.41it/s]Extractor Predicting: 226it [02:31,  1.45it/s]Extractor Predicting: 227it [02:31,  1.44it/s]Extractor Predicting: 228it [02:32,  1.41it/s]Extractor Predicting: 229it [02:33,  1.36it/s]Extractor Predicting: 230it [02:33,  1.41it/s]Extractor Predicting: 231it [02:34,  1.44it/s]Extractor Predicting: 232it [02:35,  1.46it/s]Extractor Predicting: 233it [02:35,  1.48it/s]Extractor Predicting: 234it [02:36,  1.50it/s]Extractor Predicting: 235it [02:37,  1.44it/s]Extractor Predicting: 236it [02:37,  1.48it/s]Extractor Predicting: 237it [02:38,  1.49it/s]Extractor Predicting: 238it [02:39,  1.56it/s]Extractor Predicting: 239it [02:39,  1.59it/s]Extractor Predicting: 240it [02:40,  1.61it/s]Extractor Predicting: 241it [02:41,  1.62it/s]Extractor Predicting: 242it [02:41,  1.62it/s]Extractor Predicting: 243it [02:42,  1.58it/s]Extractor Predicting: 244it [02:42,  1.60it/s]Extractor Predicting: 245it [02:43,  1.53it/s]Extractor Predicting: 246it [02:44,  1.55it/s]Extractor Predicting: 247it [02:44,  1.58it/s]Extractor Predicting: 248it [02:45,  1.60it/s]Extractor Predicting: 249it [02:46,  1.66it/s]Extractor Predicting: 250it [02:46,  1.42it/s]Extractor Predicting: 251it [02:47,  1.53it/s]Extractor Predicting: 252it [02:48,  1.60it/s]Extractor Predicting: 253it [02:48,  1.62it/s]Extractor Predicting: 254it [02:49,  1.66it/s]Extractor Predicting: 255it [02:50,  1.49it/s]Extractor Predicting: 256it [02:50,  1.58it/s]Extractor Predicting: 257it [02:51,  1.58it/s]Extractor Predicting: 258it [02:51,  1.57it/s]Extractor Predicting: 259it [02:52,  1.55it/s]Extractor Predicting: 260it [02:53,  1.52it/s]Extractor Predicting: 261it [02:53,  1.53it/s]Extractor Predicting: 262it [02:54,  1.56it/s]Extractor Predicting: 263it [02:55,  1.62it/s]Extractor Predicting: 264it [02:55,  1.65it/s]Extractor Predicting: 265it [02:56,  1.50it/s]Extractor Predicting: 266it [02:56,  1.57it/s]Extractor Predicting: 267it [02:57,  1.63it/s]Extractor Predicting: 268it [02:58,  1.65it/s]Extractor Predicting: 269it [02:58,  1.63it/s]Extractor Predicting: 270it [02:59,  1.60it/s]Extractor Predicting: 271it [03:00,  1.61it/s]Extractor Predicting: 272it [03:00,  1.60it/s]Extractor Predicting: 273it [03:01,  1.67it/s]Extractor Predicting: 274it [03:01,  1.69it/s]Extractor Predicting: 275it [03:02,  1.44it/s]Extractor Predicting: 276it [03:03,  1.49it/s]Extractor Predicting: 277it [03:03,  1.54it/s]Extractor Predicting: 278it [03:04,  1.60it/s]Extractor Predicting: 279it [03:05,  1.35it/s]Extractor Predicting: 280it [03:06,  1.47it/s]Extractor Predicting: 281it [03:06,  1.54it/s]Extractor Predicting: 282it [03:07,  1.60it/s]Extractor Predicting: 283it [03:07,  1.65it/s]Extractor Predicting: 284it [03:08,  1.41it/s]Extractor Predicting: 285it [03:09,  1.49it/s]Extractor Predicting: 286it [03:09,  1.55it/s]Extractor Predicting: 287it [03:10,  1.60it/s]Extractor Predicting: 288it [03:11,  1.60it/s]Extractor Predicting: 289it [03:11,  1.58it/s]Extractor Predicting: 290it [03:12,  1.54it/s]Extractor Predicting: 291it [03:13,  1.58it/s]Extractor Predicting: 292it [03:13,  1.64it/s]Extractor Predicting: 293it [03:14,  1.66it/s]Extractor Predicting: 294it [03:15,  1.39it/s]Extractor Predicting: 295it [03:15,  1.47it/s]Extractor Predicting: 296it [03:16,  1.46it/s]Extractor Predicting: 297it [03:17,  1.50it/s]Extractor Predicting: 298it [03:17,  1.48it/s]Extractor Predicting: 299it [03:18,  1.50it/s]Extractor Predicting: 300it [03:19,  1.30it/s]Extractor Predicting: 301it [03:20,  1.38it/s]Extractor Predicting: 302it [03:20,  1.44it/s]Extractor Predicting: 303it [03:21,  1.45it/s]Extractor Predicting: 304it [03:21,  1.48it/s]Extractor Predicting: 305it [03:22,  1.53it/s]Extractor Predicting: 306it [03:23,  1.51it/s]Extractor Predicting: 307it [03:24,  1.41it/s]Extractor Predicting: 308it [03:24,  1.44it/s]Extractor Predicting: 309it [03:25,  1.49it/s]Extractor Predicting: 310it [03:26,  1.28it/s]Extractor Predicting: 311it [03:27,  1.36it/s]Extractor Predicting: 312it [03:27,  1.41it/s]Extractor Predicting: 313it [03:28,  1.43it/s]Extractor Predicting: 314it [03:29,  1.35it/s]Extractor Predicting: 315it [03:29,  1.40it/s]Extractor Predicting: 316it [03:30,  1.43it/s]Extractor Predicting: 317it [03:31,  1.46it/s]Extractor Predicting: 318it [03:31,  1.49it/s]Extractor Predicting: 319it [03:32,  1.51it/s]Extractor Predicting: 320it [03:33,  1.53it/s]Extractor Predicting: 321it [03:33,  1.59it/s]Extractor Predicting: 322it [03:34,  1.59it/s]Extractor Predicting: 323it [03:35,  1.50it/s]Extractor Predicting: 324it [03:35,  1.50it/s]Extractor Predicting: 325it [03:36,  1.35it/s]Extractor Predicting: 326it [03:37,  1.40it/s]Extractor Predicting: 327it [03:37,  1.42it/s]Extractor Predicting: 328it [03:38,  1.45it/s]Extractor Predicting: 329it [03:39,  1.51it/s]Extractor Predicting: 330it [03:39,  1.49it/s]Extractor Predicting: 331it [03:40,  1.69it/s]Extractor Predicting: 331it [03:40,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:42,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:42,891 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:42,891 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:42,891 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:42,891 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:50:43,671 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:50:43,672 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:50:44,348 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:50:45,419 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:50:45,419 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:49,054 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:49,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:49,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:49,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:50:49,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:50:50,406 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:50:50,408 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:50:50,959 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:50:51,132 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:50:51,132 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.30410790925812387,
  "recall": 0.12507880469045518,
  "score": 0.17725364066827481,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.25it/s]Extractor Predicting: 7it [00:05,  1.33it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.43it/s]Extractor Predicting: 10it [00:07,  1.45it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.44it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:11,  1.49it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.52it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:13,  1.43it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.44it/s]Extractor Predicting: 23it [00:15,  1.43it/s]Extractor Predicting: 24it [00:16,  1.39it/s]Extractor Predicting: 25it [00:17,  1.42it/s]Extractor Predicting: 26it [00:18,  1.47it/s]Extractor Predicting: 27it [00:18,  1.51it/s]Extractor Predicting: 28it [00:19,  1.49it/s]Extractor Predicting: 29it [00:20,  1.46it/s]Extractor Predicting: 30it [00:20,  1.49it/s]Extractor Predicting: 31it [00:21,  1.49it/s]Extractor Predicting: 32it [00:22,  1.52it/s]Extractor Predicting: 33it [00:22,  1.54it/s]Extractor Predicting: 34it [00:23,  1.53it/s]Extractor Predicting: 35it [00:24,  1.44it/s]Extractor Predicting: 36it [00:24,  1.47it/s]Extractor Predicting: 37it [00:25,  1.49it/s]Extractor Predicting: 38it [00:26,  1.36it/s]Extractor Predicting: 39it [00:27,  1.37it/s]Extractor Predicting: 40it [00:27,  1.42it/s]Extractor Predicting: 41it [00:28,  1.45it/s]Extractor Predicting: 42it [00:28,  1.46it/s]Extractor Predicting: 43it [00:29,  1.48it/s]Extractor Predicting: 44it [00:30,  1.50it/s]Extractor Predicting: 45it [00:30,  1.51it/s]Extractor Predicting: 46it [00:31,  1.53it/s]Extractor Predicting: 47it [00:32,  1.52it/s]Extractor Predicting: 48it [00:32,  1.55it/s]Extractor Predicting: 49it [00:33,  1.57it/s]Extractor Predicting: 50it [00:34,  1.59it/s]Extractor Predicting: 51it [00:34,  1.51it/s]Extractor Predicting: 52it [00:35,  1.53it/s]Extractor Predicting: 53it [00:36,  1.47it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:37,  1.54it/s]Extractor Predicting: 56it [00:37,  1.61it/s]Extractor Predicting: 57it [00:38,  1.66it/s]Extractor Predicting: 58it [00:39,  1.71it/s]Extractor Predicting: 59it [00:39,  1.69it/s]Extractor Predicting: 60it [00:40,  1.78it/s]Extractor Predicting: 61it [00:40,  1.85it/s]Extractor Predicting: 62it [00:41,  1.86it/s]Extractor Predicting: 63it [00:41,  1.89it/s]Extractor Predicting: 64it [00:42,  1.88it/s]Extractor Predicting: 65it [00:42,  1.87it/s]Extractor Predicting: 66it [00:43,  1.87it/s]Extractor Predicting: 67it [00:43,  1.86it/s]Extractor Predicting: 68it [00:44,  1.88it/s]Extractor Predicting: 69it [00:45,  1.78it/s]Extractor Predicting: 70it [00:45,  1.78it/s]Extractor Predicting: 71it [00:46,  1.80it/s]Extractor Predicting: 72it [00:46,  1.84it/s]Extractor Predicting: 73it [00:47,  1.66it/s]Extractor Predicting: 74it [00:47,  1.75it/s]Extractor Predicting: 75it [00:48,  1.79it/s]Extractor Predicting: 76it [00:48,  1.81it/s]Extractor Predicting: 77it [00:49,  1.89it/s]Extractor Predicting: 78it [00:50,  1.85it/s]Extractor Predicting: 79it [00:50,  1.75it/s]Extractor Predicting: 80it [00:51,  1.78it/s]Extractor Predicting: 81it [00:51,  1.80it/s]Extractor Predicting: 82it [00:52,  1.84it/s]Extractor Predicting: 83it [00:52,  1.80it/s]Extractor Predicting: 84it [00:53,  1.82it/s]Extractor Predicting: 85it [00:54,  1.66it/s]Extractor Predicting: 86it [00:54,  1.62it/s]Extractor Predicting: 87it [00:55,  1.62it/s]Extractor Predicting: 88it [00:56,  1.56it/s]Extractor Predicting: 89it [00:56,  1.58it/s]Extractor Predicting: 90it [00:57,  1.62it/s]Extractor Predicting: 91it [00:57,  1.61it/s]Extractor Predicting: 92it [00:58,  1.62it/s]Extractor Predicting: 93it [00:59,  1.63it/s]Extractor Predicting: 94it [00:59,  1.62it/s]Extractor Predicting: 95it [01:00,  1.66it/s]Extractor Predicting: 96it [01:00,  1.67it/s]Extractor Predicting: 97it [01:01,  1.63it/s]Extractor Predicting: 98it [01:02,  1.65it/s]Extractor Predicting: 99it [01:02,  1.63it/s]Extractor Predicting: 100it [01:03,  1.37it/s]Extractor Predicting: 101it [01:04,  1.45it/s]Extractor Predicting: 102it [01:04,  1.50it/s]Extractor Predicting: 103it [01:05,  1.49it/s]Extractor Predicting: 104it [01:06,  1.50it/s]Extractor Predicting: 105it [01:07,  1.44it/s]Extractor Predicting: 106it [01:07,  1.46it/s]Extractor Predicting: 107it [01:08,  1.46it/s]Extractor Predicting: 108it [01:09,  1.45it/s]Extractor Predicting: 108it [01:09,  1.56it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:52:14,001 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:52:14,380 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:52:14,657 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:52:14,759 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:52:14,838 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:53:09,907 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:53:10,461 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:53:11,501 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:53:11,502 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:53:11,801 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:11,993 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:11,993 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:11,993 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:11,993 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:11,993 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:53:11,994 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5566849100860047,
  "recall": 0.11404773346147686,
  "score": 0.18931135336346716,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:53:13,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:14,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:14,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:15,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:15,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:16,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:17,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:17,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:18,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:19,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:19,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:20,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:20,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:21,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:22,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:22,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:23,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:24,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:24,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:25,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:25,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<03:00, 12.86s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:26,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:26,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:27,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:28,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:28,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:29,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:29,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:30,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:31,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:32,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:32,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:33,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:34,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:34,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:35,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:35,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:36,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:37,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:37,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:38,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:39,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:40,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:57, 13.63s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:40,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:41,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:41,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:42,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:42,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:43,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:44,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:44,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:45,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:46,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:46,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:47,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:48,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:48,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:50,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:50,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:51,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:52,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:53,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:53,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:54,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:55,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:52, 14.38s/it][WARNING|generation_utils.py:914] 2023-08-28 19:53:55,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:56,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:56,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:58,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:58,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:59,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:53:59,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:00,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:01,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:01,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:02,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:02,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:03,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:04,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:04,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:05,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:06,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:06,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:07,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:08,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:08,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:56<02:35, 14.14s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:09,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:10,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:10,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:11,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:12,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:12,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:13,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:14,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:14,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:15,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:16,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:16,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:17,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:17,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:18,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:19,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:19,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:20,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:20,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:21,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:22,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:09<02:18, 13.83s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:22,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:23,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:23,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:24,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:25,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:25,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:26,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:26,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:27,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:27,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:28,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:29,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:29,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:30,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:30,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:31,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:32,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:32,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:33,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:33,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:34,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:35,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:22<02:01, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:35,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:36,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:36,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:37,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:38,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:38,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:39,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:40,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:40,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:41,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:41,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:42,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:43,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:43,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:44,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:45,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:45,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:46,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:46,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:47,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:48,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:48,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:35<01:48, 13.52s/it][WARNING|generation_utils.py:914] 2023-08-28 19:54:49,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:49,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:50,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:51,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:52,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:52,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:53,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:53,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:54,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:55,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:55,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:56,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:56,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:57,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:57,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:58,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:59,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:59,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:00,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:01,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:01,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:02,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:02,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:03,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:04,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:04,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:51<01:40, 14.36s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:05,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:05,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:06,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:07,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:08,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:08,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:09,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:09,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:10,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:10,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:11,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:12,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:12,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:13,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:13,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:14,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:14,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:15,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:15,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:16,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:17,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:17,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:04<01:23, 13.91s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:18,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:18,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:19,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:20,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:20,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:21,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:21,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:22,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:22,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:23,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:23,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:24,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:25,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:25,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:26,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:26,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:27,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:27,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:28,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:29,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:29,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:30,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:17<01:06, 13.38s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:30,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:31,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:31,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:32,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:33,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:33,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:34,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:34,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:35,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:36,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:36,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:37,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:38,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:38,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:39,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:39,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:40,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:41,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:42,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:42,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:43,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:44,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:31<00:54, 13.64s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:44,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:45,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:46,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:46,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:47,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:48,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:48,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:49,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:50,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:51,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:51,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:52,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:53,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:54,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:55,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:55,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:56,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:57,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:58,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:58,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:59,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:00,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:01,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:48<00:44, 14.75s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:02,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:02,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:03,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:03,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:04,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:05,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:05,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:06,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:06,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:08,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:08,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:09,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:10,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:10,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:11,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:11,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:12,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:13,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:13,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:14,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:14,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:16,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:16,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:17,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:04<00:30, 15.07s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:18,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:18,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:19,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:19,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:20,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:21,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:21,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:22,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:22,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:23,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:24,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:24,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:25,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:26,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:26,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:27,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:27,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:28,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:29,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:29,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:30,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:30,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:18<00:14, 14.66s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:31,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:32,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:32,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:33,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:34,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:34,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:35,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:36,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:36,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:37,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:38,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:38,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:39,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:39,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:40,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:41,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:41,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:42,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:42,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:43,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:44,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:44,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:31<00:00, 14.42s/it]Generating: 100%|██████████| 15/15 [03:31<00:00, 14.13s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:02,941 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:02,943 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:02,943 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:02,943 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:02,943 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:57:03,266 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:57:03,267 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:57:04,230 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:57:05,316 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:57:05,498 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:09,129 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:09,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:09,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:09,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:57:09,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:57:10,506 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:57:10,508 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:57:11,180 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:57:11,355 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:57:11,355 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : characters . Context : Later in Life , he played the title character , a young son of a Scottish novelist . Head Entity : son of a Scottish novelist , Tail Entity : Harry Potter .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : made from material .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9092261904761905, 'errors': {'not enough values to unpack (expected 2, got 1)', ''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : follows .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 298, 'raw': 416}
{'target': 600, 'success': 320, 'raw': 448}
{'target': 600, 'success': 347, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 402, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 468, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 613, 'raw': 832}
{'prompt': 'Relation : league .', 'success_rate': 0.7367788461538461, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8536931818181818, 'errors': {''}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : Liberal Party .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8505434782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8650568181818182, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8551136363636364, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 11205
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11305, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.57it/s]Extractor Estimating: 2it [00:01,  1.59it/s]Extractor Estimating: 3it [00:01,  1.60it/s]Extractor Estimating: 4it [00:02,  1.59it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.56it/s]Extractor Estimating: 8it [00:05,  1.47it/s]Extractor Estimating: 9it [00:05,  1.54it/s]Extractor Estimating: 10it [00:06,  1.55it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:08,  1.26it/s]Extractor Estimating: 13it [00:08,  1.39it/s]Extractor Estimating: 14it [00:09,  1.43it/s]Extractor Estimating: 15it [00:10,  1.41it/s]Extractor Estimating: 16it [00:10,  1.40it/s]Extractor Estimating: 17it [00:11,  1.44it/s]Extractor Estimating: 18it [00:12,  1.51it/s]Extractor Estimating: 19it [00:12,  1.46it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:13,  1.57it/s]Extractor Estimating: 22it [00:14,  1.61it/s]Extractor Estimating: 23it [00:15,  1.65it/s]Extractor Estimating: 24it [00:15,  1.55it/s]Extractor Estimating: 25it [00:16,  1.58it/s]Extractor Estimating: 26it [00:17,  1.60it/s]Extractor Estimating: 27it [00:17,  1.63it/s]Extractor Estimating: 28it [00:18,  1.63it/s]Extractor Estimating: 29it [00:18,  1.64it/s]Extractor Estimating: 30it [00:19,  1.62it/s]Extractor Estimating: 31it [00:20,  1.60it/s]Extractor Estimating: 32it [00:20,  1.63it/s]Extractor Estimating: 33it [00:21,  1.49it/s]Extractor Estimating: 34it [00:22,  1.57it/s]Extractor Estimating: 35it [00:22,  1.47it/s]Extractor Estimating: 36it [00:23,  1.53it/s]Extractor Estimating: 37it [00:24,  1.53it/s]Extractor Estimating: 38it [00:24,  1.51it/s]Extractor Estimating: 39it [00:25,  1.58it/s]Extractor Estimating: 40it [00:25,  1.59it/s]Extractor Estimating: 41it [00:26,  1.41it/s]Extractor Estimating: 42it [00:27,  1.48it/s]Extractor Estimating: 43it [00:28,  1.51it/s]Extractor Estimating: 44it [00:28,  1.58it/s]Extractor Estimating: 45it [00:29,  1.62it/s]Extractor Estimating: 46it [00:30,  1.49it/s]Extractor Estimating: 47it [00:30,  1.54it/s]Extractor Estimating: 48it [00:31,  1.47it/s]Extractor Estimating: 49it [00:32,  1.53it/s]Extractor Estimating: 50it [00:32,  1.57it/s]Extractor Estimating: 51it [00:33,  1.57it/s]Extractor Estimating: 52it [00:34,  1.04it/s]Extractor Estimating: 53it [00:35,  1.15it/s]Extractor Estimating: 54it [00:36,  1.25it/s]Extractor Estimating: 55it [00:36,  1.30it/s]Extractor Estimating: 56it [00:37,  1.41it/s]Extractor Estimating: 57it [00:38,  1.48it/s]Extractor Estimating: 58it [00:38,  1.52it/s]Extractor Estimating: 59it [00:39,  1.59it/s]Extractor Estimating: 60it [00:40,  1.27it/s]Extractor Estimating: 61it [00:41,  1.37it/s]Extractor Estimating: 62it [00:41,  1.45it/s]Extractor Estimating: 63it [00:42,  1.53it/s]Extractor Estimating: 64it [00:42,  1.57it/s]Extractor Estimating: 65it [00:43,  1.26it/s]Extractor Estimating: 66it [00:44,  1.36it/s]Extractor Estimating: 67it [00:45,  1.48it/s]Extractor Estimating: 68it [00:45,  1.55it/s]Extractor Estimating: 69it [00:46,  1.58it/s]Extractor Estimating: 70it [00:46,  1.59it/s]Extractor Estimating: 71it [00:47,  1.63it/s]Extractor Estimating: 72it [00:48,  1.66it/s]Extractor Estimating: 73it [00:49,  1.37it/s]Extractor Estimating: 74it [00:49,  1.51it/s]Extractor Estimating: 75it [00:50,  1.55it/s]Extractor Estimating: 76it [00:51,  1.34it/s]Extractor Estimating: 77it [00:51,  1.40it/s]Extractor Estimating: 78it [00:52,  1.47it/s]Extractor Estimating: 79it [00:52,  1.56it/s]Extractor Estimating: 80it [00:53,  1.36it/s]Extractor Estimating: 81it [00:54,  1.41it/s]Extractor Estimating: 82it [00:55,  1.47it/s]Extractor Estimating: 83it [00:56,  1.14it/s]Extractor Estimating: 84it [00:57,  1.28it/s]Extractor Estimating: 85it [00:57,  1.42it/s]Extractor Estimating: 86it [00:58,  1.41it/s]Extractor Estimating: 87it [00:59,  1.40it/s]Extractor Estimating: 88it [00:59,  1.50it/s]Extractor Estimating: 89it [01:00,  1.56it/s]Extractor Estimating: 90it [01:00,  1.57it/s]Extractor Estimating: 91it [01:01,  1.49it/s]Extractor Estimating: 92it [01:02,  1.55it/s]Extractor Estimating: 93it [01:02,  1.51it/s]Extractor Estimating: 94it [01:03,  1.46it/s]Extractor Estimating: 95it [01:04,  1.52it/s]Extractor Estimating: 96it [01:04,  1.47it/s]Extractor Estimating: 97it [01:05,  1.52it/s]Extractor Estimating: 98it [01:06,  1.59it/s]Extractor Estimating: 99it [01:06,  1.62it/s]Extractor Estimating: 100it [01:07,  1.63it/s]Extractor Estimating: 101it [01:07,  1.61it/s]Extractor Estimating: 102it [01:08,  1.60it/s]Extractor Estimating: 103it [01:09,  1.60it/s]Extractor Estimating: 104it [01:09,  1.56it/s]Extractor Estimating: 105it [01:10,  1.61it/s]Extractor Estimating: 106it [01:11,  1.62it/s]Extractor Estimating: 107it [01:11,  1.62it/s]Extractor Estimating: 108it [01:12,  1.66it/s]Extractor Estimating: 109it [01:12,  1.70it/s]Extractor Estimating: 110it [01:13,  1.72it/s]Extractor Estimating: 111it [01:13,  1.70it/s]Extractor Estimating: 112it [01:14,  1.65it/s]Extractor Estimating: 113it [01:15,  1.40it/s]Extractor Estimating: 114it [01:16,  1.47it/s]Extractor Estimating: 115it [01:16,  1.53it/s]Extractor Estimating: 116it [01:17,  1.52it/s]Extractor Estimating: 117it [01:17,  1.59it/s]Extractor Estimating: 118it [01:18,  1.41it/s]Extractor Estimating: 119it [01:19,  1.46it/s]Extractor Estimating: 120it [01:20,  1.38it/s]Extractor Estimating: 121it [01:20,  1.45it/s]Extractor Estimating: 122it [01:21,  1.51it/s]Extractor Estimating: 123it [01:22,  1.61it/s]Extractor Estimating: 124it [01:22,  1.63it/s]Extractor Estimating: 125it [01:23,  1.61it/s]Extractor Estimating: 126it [01:23,  1.64it/s]Extractor Estimating: 127it [01:24,  1.67it/s]Extractor Estimating: 128it [01:25,  1.68it/s]Extractor Estimating: 129it [01:25,  1.65it/s]Extractor Estimating: 130it [01:26,  1.62it/s]Extractor Estimating: 131it [01:26,  1.66it/s]Extractor Estimating: 132it [01:27,  1.61it/s]Extractor Estimating: 133it [01:28,  1.62it/s]Extractor Estimating: 134it [01:28,  1.64it/s]Extractor Estimating: 135it [01:29,  1.61it/s]Extractor Estimating: 136it [01:29,  1.65it/s]Extractor Estimating: 137it [01:30,  1.53it/s]Extractor Estimating: 138it [01:31,  1.58it/s]Extractor Estimating: 139it [01:31,  1.59it/s]Extractor Estimating: 140it [01:32,  1.61it/s]Extractor Estimating: 141it [01:33,  1.61it/s]Extractor Estimating: 142it [01:33,  1.58it/s]Extractor Estimating: 143it [01:34,  1.67it/s]Extractor Estimating: 144it [01:34,  1.65it/s]Extractor Estimating: 145it [01:36,  1.28it/s]Extractor Estimating: 146it [01:36,  1.35it/s]Extractor Estimating: 147it [01:37,  1.44it/s]Extractor Estimating: 148it [01:38,  1.22it/s]Extractor Estimating: 149it [01:39,  1.35it/s]Extractor Estimating: 150it [01:39,  1.46it/s]Extractor Estimating: 151it [01:40,  1.53it/s]Extractor Estimating: 152it [01:41,  1.40it/s]Extractor Estimating: 153it [01:41,  1.47it/s]Extractor Estimating: 154it [01:42,  1.49it/s]Extractor Estimating: 155it [01:42,  1.56it/s]Extractor Estimating: 156it [01:43,  1.56it/s]Extractor Estimating: 157it [01:44,  1.60it/s]Extractor Estimating: 158it [01:44,  1.68it/s]Extractor Estimating: 159it [01:45,  1.67it/s]Extractor Estimating: 160it [01:45,  1.67it/s]Extractor Estimating: 161it [01:46,  1.61it/s]Extractor Estimating: 162it [01:47,  1.64it/s]Extractor Estimating: 163it [01:47,  1.59it/s]Extractor Estimating: 164it [01:48,  1.57it/s]Extractor Estimating: 165it [01:49,  1.58it/s]Extractor Estimating: 166it [01:49,  1.60it/s]Extractor Estimating: 167it [01:50,  1.69it/s]Extractor Estimating: 168it [01:50,  1.69it/s]Extractor Estimating: 169it [01:51,  1.67it/s]Extractor Estimating: 170it [01:52,  1.41it/s]Extractor Estimating: 171it [01:52,  1.47it/s]Extractor Estimating: 172it [01:53,  1.55it/s]Extractor Estimating: 173it [01:54,  1.36it/s]Extractor Estimating: 174it [01:55,  1.45it/s]Extractor Estimating: 175it [01:55,  1.51it/s]Extractor Estimating: 176it [01:56,  1.57it/s]Extractor Estimating: 177it [01:57,  1.22it/s]Extractor Estimating: 178it [01:58,  1.33it/s]Extractor Estimating: 179it [01:58,  1.38it/s]Extractor Estimating: 180it [02:00,  1.05it/s]Extractor Estimating: 181it [02:00,  1.18it/s]Extractor Estimating: 182it [02:01,  1.29it/s]Extractor Estimating: 183it [02:02,  1.36it/s]Extractor Estimating: 184it [02:03,  1.22it/s]Extractor Estimating: 185it [02:03,  1.30it/s]Extractor Estimating: 186it [02:04,  1.37it/s]Extractor Estimating: 187it [02:04,  1.42it/s]Extractor Estimating: 188it [02:05,  1.50it/s]Extractor Estimating: 189it [02:06,  1.53it/s]Extractor Estimating: 190it [02:06,  1.52it/s]Extractor Estimating: 191it [02:07,  1.58it/s]Extractor Estimating: 192it [02:08,  1.60it/s]Extractor Estimating: 193it [02:08,  1.44it/s]Extractor Estimating: 194it [02:09,  1.50it/s]Extractor Estimating: 195it [02:10,  1.54it/s]Extractor Estimating: 196it [02:11,  1.33it/s]Extractor Estimating: 197it [02:11,  1.43it/s]Extractor Estimating: 198it [02:12,  1.52it/s]Extractor Estimating: 199it [02:12,  1.54it/s]Extractor Estimating: 200it [02:13,  1.51it/s]Extractor Estimating: 201it [02:14,  1.61it/s]Extractor Estimating: 202it [02:14,  1.67it/s]Extractor Estimating: 203it [02:15,  1.55it/s]Extractor Estimating: 204it [02:15,  1.64it/s]Extractor Estimating: 205it [02:16,  1.58it/s]Extractor Estimating: 206it [02:17,  1.65it/s]Extractor Estimating: 207it [02:17,  1.71it/s]Extractor Estimating: 208it [02:18,  1.74it/s]Extractor Estimating: 209it [02:18,  1.77it/s]Extractor Estimating: 210it [02:19,  1.77it/s]Extractor Estimating: 211it [02:20,  1.57it/s]Extractor Estimating: 212it [02:20,  1.60it/s]Extractor Estimating: 213it [02:21,  1.49it/s]Extractor Estimating: 214it [02:22,  1.61it/s]Extractor Estimating: 215it [02:22,  1.70it/s]Extractor Estimating: 216it [02:23,  1.65it/s]Extractor Estimating: 217it [02:23,  1.66it/s]Extractor Estimating: 218it [02:24,  1.64it/s]Extractor Estimating: 219it [02:24,  1.70it/s]Extractor Estimating: 220it [02:25,  1.76it/s]Extractor Estimating: 221it [02:25,  1.79it/s]Extractor Estimating: 222it [02:26,  1.79it/s]Extractor Estimating: 223it [02:27,  1.88it/s]Extractor Estimating: 224it [02:27,  1.78it/s]Extractor Estimating: 225it [02:28,  1.81it/s]Extractor Estimating: 226it [02:28,  1.83it/s]Extractor Estimating: 227it [02:29,  1.76it/s]Extractor Estimating: 228it [02:29,  1.77it/s]Extractor Estimating: 229it [02:30,  1.79it/s]Extractor Estimating: 230it [02:31,  1.51it/s]Extractor Estimating: 231it [02:31,  1.65it/s]Extractor Estimating: 232it [02:32,  1.69it/s]Extractor Estimating: 233it [02:32,  1.74it/s]Extractor Estimating: 234it [02:33,  1.78it/s]Extractor Estimating: 235it [02:34,  1.56it/s]Extractor Estimating: 236it [02:34,  1.61it/s]Extractor Estimating: 237it [02:35,  1.67it/s]Extractor Estimating: 238it [02:35,  1.75it/s]Extractor Estimating: 239it [02:36,  1.84it/s]Extractor Estimating: 240it [02:36,  1.78it/s]Extractor Estimating: 241it [02:37,  1.78it/s]Extractor Estimating: 242it [02:38,  1.78it/s]Extractor Estimating: 243it [02:38,  1.80it/s]Extractor Estimating: 244it [02:39,  1.88it/s]Extractor Estimating: 245it [02:39,  1.92it/s]Extractor Estimating: 246it [02:40,  1.90it/s]Extractor Estimating: 247it [02:40,  1.62it/s]Extractor Estimating: 248it [02:41,  1.72it/s]Extractor Estimating: 249it [02:42,  1.73it/s]Extractor Estimating: 250it [02:42,  1.73it/s]Extractor Estimating: 251it [02:43,  1.73it/s]Extractor Estimating: 252it [02:43,  1.72it/s]Extractor Estimating: 253it [02:44,  1.72it/s]Extractor Estimating: 254it [02:45,  1.50it/s]Extractor Estimating: 255it [02:45,  1.54it/s]Extractor Estimating: 256it [02:46,  1.60it/s]Extractor Estimating: 257it [02:47,  1.59it/s]Extractor Estimating: 258it [02:47,  1.58it/s]Extractor Estimating: 259it [02:48,  1.56it/s]Extractor Estimating: 260it [02:48,  1.58it/s]Extractor Estimating: 261it [02:49,  1.61it/s]Extractor Estimating: 262it [02:50,  1.42it/s]Extractor Estimating: 263it [02:51,  1.46it/s]Extractor Estimating: 264it [02:51,  1.52it/s]Extractor Estimating: 265it [02:52,  1.54it/s]Extractor Estimating: 266it [02:53,  1.49it/s]Extractor Estimating: 267it [02:53,  1.53it/s]Extractor Estimating: 268it [02:54,  1.60it/s]Extractor Estimating: 269it [02:54,  1.58it/s]Extractor Estimating: 270it [02:55,  1.42it/s]Extractor Estimating: 271it [02:56,  1.48it/s]Extractor Estimating: 272it [02:56,  1.52it/s]Extractor Estimating: 273it [02:57,  1.57it/s]Extractor Estimating: 274it [02:58,  1.27it/s]Extractor Estimating: 275it [02:59,  1.40it/s]Extractor Estimating: 276it [02:59,  1.50it/s]Extractor Estimating: 277it [03:00,  1.53it/s]Extractor Estimating: 278it [03:01,  1.32it/s]Extractor Estimating: 279it [03:02,  1.34it/s]Extractor Estimating: 280it [03:02,  1.37it/s]Extractor Estimating: 281it [03:03,  1.45it/s]Extractor Estimating: 282it [03:04,  1.24it/s]Extractor Estimating: 283it [03:05,  1.31it/s]Extractor Estimating: 284it [03:05,  1.42it/s]Extractor Estimating: 285it [03:06,  1.47it/s]Extractor Estimating: 286it [03:06,  1.50it/s]Extractor Estimating: 287it [03:08,  1.27it/s]Extractor Estimating: 288it [03:08,  1.36it/s]Extractor Estimating: 289it [03:09,  1.41it/s]Extractor Estimating: 290it [03:09,  1.47it/s]Extractor Estimating: 291it [03:10,  1.52it/s]Extractor Estimating: 292it [03:11,  1.60it/s]Extractor Estimating: 293it [03:11,  1.60it/s]Extractor Estimating: 294it [03:12,  1.48it/s]Extractor Estimating: 295it [03:13,  1.52it/s]Extractor Estimating: 296it [03:13,  1.54it/s]Extractor Estimating: 297it [03:14,  1.56it/s]Extractor Estimating: 298it [03:15,  1.53it/s]Extractor Estimating: 299it [03:15,  1.50it/s]Extractor Estimating: 300it [03:16,  1.56it/s]Extractor Estimating: 301it [03:16,  1.64it/s]Extractor Estimating: 302it [03:17,  1.57it/s]Extractor Estimating: 303it [03:18,  1.56it/s]Extractor Estimating: 304it [03:18,  1.59it/s]Extractor Estimating: 305it [03:19,  1.63it/s]Extractor Estimating: 306it [03:20,  1.48it/s]Extractor Estimating: 307it [03:20,  1.52it/s]Extractor Estimating: 308it [03:21,  1.55it/s]Extractor Estimating: 309it [03:22,  1.53it/s]Extractor Estimating: 310it [03:22,  1.59it/s]Extractor Estimating: 311it [03:23,  1.56it/s]Extractor Estimating: 312it [03:23,  1.58it/s]Extractor Estimating: 313it [03:24,  1.62it/s]Extractor Estimating: 314it [03:25,  1.63it/s]Extractor Estimating: 315it [03:25,  1.60it/s]Extractor Estimating: 316it [03:26,  1.67it/s]Extractor Estimating: 317it [03:26,  1.70it/s]Extractor Estimating: 318it [03:27,  1.66it/s]Extractor Estimating: 319it [03:28,  1.67it/s]Extractor Estimating: 320it [03:28,  1.57it/s]Extractor Estimating: 321it [03:29,  1.56it/s]Extractor Estimating: 322it [03:30,  1.55it/s]Extractor Estimating: 323it [03:30,  1.57it/s]Extractor Estimating: 324it [03:31,  1.60it/s]Extractor Estimating: 325it [03:31,  1.71it/s]Extractor Estimating: 326it [03:32,  1.75it/s]Extractor Estimating: 327it [03:32,  1.75it/s]Extractor Estimating: 328it [03:33,  1.55it/s]Extractor Estimating: 329it [03:34,  1.61it/s]Extractor Estimating: 330it [03:34,  1.63it/s]Extractor Estimating: 331it [03:35,  1.68it/s]Extractor Estimating: 332it [03:36,  1.62it/s]Extractor Estimating: 333it [03:36,  1.70it/s]Extractor Estimating: 334it [03:37,  1.75it/s]Extractor Estimating: 335it [03:37,  1.66it/s]Extractor Estimating: 336it [03:38,  1.70it/s]Extractor Estimating: 337it [03:39,  1.32it/s]Extractor Estimating: 338it [03:40,  1.44it/s]Extractor Estimating: 339it [03:40,  1.54it/s]Extractor Estimating: 340it [03:42,  1.04it/s]Extractor Estimating: 341it [03:42,  1.18it/s]Extractor Estimating: 342it [03:43,  1.31it/s]Extractor Estimating: 343it [03:44,  1.43it/s]Extractor Estimating: 344it [03:45,  1.18it/s]Extractor Estimating: 345it [03:45,  1.32it/s]Extractor Estimating: 346it [03:46,  1.42it/s]Extractor Estimating: 347it [03:46,  1.52it/s]Extractor Estimating: 348it [03:47,  1.56it/s]Extractor Estimating: 349it [03:48,  1.65it/s]Extractor Estimating: 350it [03:48,  1.65it/s]Extractor Estimating: 351it [03:49,  1.68it/s]Extractor Estimating: 352it [03:49,  1.63it/s]Extractor Estimating: 353it [03:50,  1.36it/s]Extractor Estimating: 354it [03:51,  1.41it/s]Extractor Estimating: 355it [03:52,  1.47it/s]Extractor Estimating: 356it [03:52,  1.55it/s]Extractor Estimating: 357it [03:53,  1.60it/s]Extractor Estimating: 358it [03:54,  1.55it/s]Extractor Estimating: 359it [03:54,  1.57it/s]Extractor Estimating: 360it [03:55,  1.52it/s]Extractor Estimating: 361it [03:56,  1.51it/s]Extractor Estimating: 362it [03:56,  1.54it/s]Extractor Estimating: 363it [03:57,  1.38it/s]Extractor Estimating: 364it [03:58,  1.44it/s]Extractor Estimating: 365it [03:58,  1.54it/s]Extractor Estimating: 366it [03:59,  1.59it/s]Extractor Estimating: 367it [03:59,  1.61it/s]Extractor Estimating: 368it [04:00,  1.38it/s]Extractor Estimating: 369it [04:01,  1.42it/s]Extractor Estimating: 370it [04:02,  1.35it/s]Extractor Estimating: 371it [04:02,  1.42it/s]Extractor Estimating: 372it [04:03,  1.42it/s]Extractor Estimating: 373it [04:04,  1.51it/s]Extractor Estimating: 374it [04:04,  1.54it/s]Extractor Estimating: 375it [04:05,  1.75it/s]Extractor Estimating: 375it [04:05,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:54,484 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:54,768 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:54,768 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:54,768 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:54,768 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:01:55,558 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:01:55,559 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:01:56,238 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:01:57,315 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:01:57,810 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:02,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:02,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:02,409 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:02,409 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:02,409 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:02:03,227 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:02:03,228 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:02:03,943 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:02:04,115 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:02:04,115 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 20:55:40,376 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 20:55:40,409 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 3012 mean pseudo reward: 0.9538580618023518
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 15727
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15827, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15827, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.988, loss:358.6144
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 74, avg_time 0.995, loss:324.7161
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 48, avg_time 0.978, loss:323.7044
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 22, avg_time 0.992, loss:311.6034
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 122, avg_time 0.989, loss:313.5391
>> valid entity prec:0.4582, rec:0.3690, f1:0.4088
>> valid relation prec:0.0296, rec:0.0076, f1:0.0121
>> valid relation with NER prec:0.0296, rec:0.0076, f1:0.0121
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 96, avg_time 2.413, loss:283.3468
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 70, avg_time 0.992, loss:292.3118
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 44, avg_time 0.972, loss:268.9521
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 18, avg_time 0.995, loss:275.6662
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 118, avg_time 0.982, loss:282.1187
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4510, rec:0.5730, f1:0.5047
>> valid relation prec:0.0676, rec:0.0327, f1:0.0441
>> valid relation with NER prec:0.0676, rec:0.0327, f1:0.0441
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 92, avg_time 2.450, loss:260.1814
g_step 1200, step 66, avg_time 1.010, loss:250.8051
g_step 1300, step 40, avg_time 0.973, loss:243.8077
g_step 1400, step 14, avg_time 0.990, loss:238.1103
g_step 1500, step 114, avg_time 0.983, loss:232.8335
>> valid entity prec:0.4658, rec:0.4531, f1:0.4594
>> valid relation prec:0.0272, rec:0.0104, f1:0.0150
>> valid relation with NER prec:0.0272, rec:0.0104, f1:0.0150
g_step 1600, step 88, avg_time 2.418, loss:220.4178
g_step 1700, step 62, avg_time 0.983, loss:207.9942
g_step 1800, step 36, avg_time 0.983, loss:199.0837
g_step 1900, step 10, avg_time 0.995, loss:207.6267
g_step 2000, step 110, avg_time 0.983, loss:199.2596
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4657, rec:0.4213, f1:0.4424
>> valid relation prec:0.0517, rec:0.0210, f1:0.0299
>> valid relation with NER prec:0.0517, rec:0.0210, f1:0.0299
g_step 2100, step 84, avg_time 2.407, loss:183.7715
g_step 2200, step 58, avg_time 0.983, loss:171.9926
g_step 2300, step 32, avg_time 0.987, loss:163.4544
g_step 2400, step 6, avg_time 0.996, loss:177.0183
g_step 2500, step 106, avg_time 0.983, loss:161.6912
>> valid entity prec:0.4643, rec:0.4286, f1:0.4457
>> valid relation prec:0.0371, rec:0.0182, f1:0.0244
>> valid relation with NER prec:0.0371, rec:0.0182, f1:0.0244
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:55:40 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:55:40 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-55-40_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:55:41 - WARNING - datasets.builder -   Using custom data configuration default-bfb541749c0671e7
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-bfb541749c0671e7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:55:46,763 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:55:46,791 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:55:46,792 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:55:46,793 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:55:46,941 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:46,996 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:46,996 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:46,996 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:46,996 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:46,996 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:55:46,996 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:55:48,215 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:55:51,419 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:55:51,419 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-bfb541749c0671e7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.43ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.46ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.32ba/s]100%|██████████| 4/4 [00:00<00:00,  4.27ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.83ba/s] 40%|████      | 2/5 [00:00<00:00,  3.29ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.79ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.08ba/s]100%|██████████| 5/5 [00:01<00:00,  4.41ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.43ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.49ba/s]100%|██████████| 4/4 [00:00<00:00, 10.61ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:03,  1.22ba/s] 60%|██████    | 3/5 [00:01<00:00,  3.55ba/s]100%|██████████| 5/5 [00:01<00:00,  5.89ba/s]100%|██████████| 5/5 [00:01<00:00,  4.41ba/s]
[INFO|trainer.py:414] 2023-08-28 20:55:58,312 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:55:58,324 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:55:58,325 >>   Num examples = 3013
[INFO|trainer.py:1149] 2023-08-28 20:55:58,325 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:55:58,325 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:55:58,325 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:55:58,325 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:55:58,325 >>   Total optimization steps = 235
  0%|          | 0/235 [00:00<?, ?it/s]  0%|          | 1/235 [00:00<01:08,  3.41it/s]  1%|          | 2/235 [00:00<01:07,  3.45it/s]  1%|▏         | 3/235 [00:00<01:13,  3.17it/s]  2%|▏         | 4/235 [00:01<01:10,  3.26it/s]  2%|▏         | 5/235 [00:01<01:09,  3.32it/s]  3%|▎         | 6/235 [00:01<01:08,  3.35it/s]  3%|▎         | 7/235 [00:02<01:07,  3.38it/s]  3%|▎         | 8/235 [00:02<01:06,  3.39it/s]  4%|▍         | 9/235 [00:02<01:06,  3.40it/s]  4%|▍         | 10/235 [00:02<01:06,  3.41it/s]  5%|▍         | 11/235 [00:03<01:18,  2.87it/s]  5%|▌         | 12/235 [00:03<01:13,  3.01it/s]  6%|▌         | 13/235 [00:04<01:10,  3.13it/s]  6%|▌         | 14/235 [00:04<01:08,  3.21it/s]  6%|▋         | 15/235 [00:04<01:07,  3.27it/s]  7%|▋         | 16/235 [00:04<01:05,  3.32it/s]  7%|▋         | 17/235 [00:05<01:05,  3.35it/s]  8%|▊         | 18/235 [00:05<01:04,  3.37it/s]  8%|▊         | 19/235 [00:05<01:03,  3.39it/s]  9%|▊         | 20/235 [00:06<01:03,  3.40it/s]  9%|▉         | 21/235 [00:06<01:31,  2.33it/s]  9%|▉         | 22/235 [00:07<01:22,  2.57it/s] 10%|▉         | 23/235 [00:07<01:16,  2.78it/s] 10%|█         | 24/235 [00:07<01:11,  2.95it/s] 11%|█         | 25/235 [00:07<01:08,  3.07it/s] 11%|█         | 26/235 [00:08<01:05,  3.17it/s] 11%|█▏        | 27/235 [00:09<01:33,  2.23it/s] 12%|█▏        | 28/235 [00:09<01:23,  2.49it/s] 12%|█▏        | 29/235 [00:09<01:15,  2.71it/s] 13%|█▎        | 30/235 [00:09<01:10,  2.89it/s] 13%|█▎        | 31/235 [00:10<01:07,  3.03it/s] 14%|█▎        | 32/235 [00:10<01:04,  3.14it/s] 14%|█▍        | 33/235 [00:10<01:02,  3.22it/s] 14%|█▍        | 34/235 [00:11<01:01,  3.28it/s] 15%|█▍        | 35/235 [00:11<01:00,  3.32it/s] 15%|█▌        | 36/235 [00:11<01:11,  2.80it/s] 16%|█▌        | 37/235 [00:12<01:06,  2.96it/s] 16%|█▌        | 38/235 [00:12<01:03,  3.09it/s] 17%|█▋        | 39/235 [00:12<01:01,  3.18it/s] 17%|█▋        | 40/235 [00:13<01:00,  3.25it/s] 17%|█▋        | 41/235 [00:13<00:58,  3.30it/s] 18%|█▊        | 42/235 [00:13<00:57,  3.33it/s] 18%|█▊        | 43/235 [00:13<00:57,  3.36it/s] 19%|█▊        | 44/235 [00:14<01:25,  2.24it/s] 19%|█▉        | 45/235 [00:14<01:16,  2.50it/s] 20%|█▉        | 46/235 [00:15<01:09,  2.72it/s] 20%|██        | 47/235 [00:15<01:04,  2.90it/s][INFO|trainer.py:2140] 2023-08-28 20:56:13,951 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:56:13,951 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 20:56:13,951 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.78it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.63it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.58it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.86it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.10it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.75it/s][A
  7%|▋         | 38/543 [00:00<00:11, 45.55it/s][A
  8%|▊         | 43/543 [00:00<00:11, 45.21it/s][A
  9%|▉         | 48/543 [00:01<00:10, 45.18it/s][A
 10%|▉         | 53/543 [00:01<00:15, 30.82it/s][A
 10%|█         | 57/543 [00:01<00:17, 27.31it/s][A
 11%|█▏        | 62/543 [00:01<00:15, 31.37it/s][A
 12%|█▏        | 67/543 [00:01<00:13, 34.68it/s][A
 13%|█▎        | 72/543 [00:01<00:12, 37.54it/s][A
 14%|█▍        | 77/543 [00:01<00:11, 39.60it/s][A
 15%|█▌        | 82/543 [00:02<00:11, 41.25it/s][A
 16%|█▌        | 87/543 [00:02<00:10, 42.48it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.42it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.61it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 43.92it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.35it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.76it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 45.05it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 45.11it/s][A
 23%|██▎       | 127/543 [00:03<00:09, 45.19it/s][A
 24%|██▍       | 132/543 [00:03<00:09, 45.26it/s][A
 25%|██▌       | 137/543 [00:03<00:08, 45.24it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 45.06it/s][A
 27%|██▋       | 147/543 [00:03<00:19, 19.81it/s][A
 28%|██▊       | 152/543 [00:04<00:16, 23.88it/s][A
 29%|██▉       | 157/543 [00:04<00:13, 27.91it/s][A
 30%|██▉       | 162/543 [00:04<00:12, 31.58it/s][A
 31%|███       | 167/543 [00:04<00:10, 34.78it/s][A
 32%|███▏      | 172/543 [00:04<00:09, 37.51it/s][A
 33%|███▎      | 177/543 [00:05<00:20, 17.52it/s][A
 34%|███▎      | 182/543 [00:05<00:18, 19.67it/s][A
 34%|███▍      | 187/543 [00:05<00:14, 23.94it/s][A
 35%|███▌      | 192/543 [00:05<00:12, 27.96it/s][A
 36%|███▋      | 197/543 [00:05<00:10, 31.64it/s][A
 37%|███▋      | 202/543 [00:05<00:09, 34.85it/s][A
 38%|███▊      | 207/543 [00:05<00:08, 37.54it/s][A
 39%|███▉      | 212/543 [00:05<00:08, 39.56it/s][A
 40%|███▉      | 217/543 [00:06<00:07, 41.21it/s][A
 41%|████      | 222/543 [00:06<00:07, 42.29it/s][A
 42%|████▏     | 227/543 [00:06<00:07, 42.83it/s][A
 43%|████▎     | 232/543 [00:06<00:10, 28.88it/s][A
 44%|████▎     | 237/543 [00:06<00:09, 32.49it/s][A
 45%|████▍     | 242/543 [00:06<00:08, 35.58it/s][A
 45%|████▌     | 247/543 [00:06<00:07, 38.09it/s][A
 46%|████▋     | 252/543 [00:07<00:07, 40.09it/s][A
 47%|████▋     | 257/543 [00:07<00:06, 41.59it/s][A
 48%|████▊     | 262/543 [00:07<00:06, 42.80it/s][A
 49%|████▉     | 267/543 [00:07<00:06, 43.51it/s][A
 50%|█████     | 272/543 [00:07<00:06, 43.68it/s][A
 51%|█████     | 277/543 [00:07<00:06, 43.88it/s][A
 52%|█████▏    | 282/543 [00:07<00:05, 44.21it/s][A
 53%|█████▎    | 287/543 [00:07<00:05, 44.64it/s][A
 54%|█████▍    | 292/543 [00:07<00:05, 44.89it/s][A
 55%|█████▍    | 297/543 [00:08<00:05, 45.06it/s][A
 56%|█████▌    | 302/543 [00:08<00:08, 26.99it/s][A
 56%|█████▋    | 306/543 [00:09<00:13, 17.15it/s][A
 57%|█████▋    | 309/543 [00:09<00:17, 13.13it/s][A
 58%|█████▊    | 314/543 [00:09<00:13, 17.32it/s][A
 59%|█████▊    | 319/543 [00:09<00:10, 21.71it/s][A
 60%|█████▉    | 324/543 [00:09<00:08, 26.06it/s][A
 61%|██████    | 329/543 [00:09<00:07, 30.09it/s][A
 62%|██████▏   | 334/543 [00:09<00:06, 33.60it/s][A
 62%|██████▏   | 339/543 [00:09<00:05, 36.53it/s][A
 63%|██████▎   | 344/543 [00:10<00:05, 38.92it/s][A
 64%|██████▍   | 349/543 [00:10<00:04, 40.40it/s][A
 65%|██████▌   | 354/543 [00:10<00:06, 31.09it/s][A
 66%|██████▌   | 359/543 [00:10<00:05, 34.92it/s][A
 67%|██████▋   | 364/543 [00:10<00:04, 37.50it/s][A
 68%|██████▊   | 369/543 [00:10<00:04, 39.65it/s][A
 69%|██████▉   | 374/543 [00:10<00:04, 41.32it/s][A
 70%|██████▉   | 379/543 [00:10<00:03, 42.52it/s][A
 71%|███████   | 384/543 [00:11<00:03, 43.49it/s][A
 72%|███████▏  | 389/543 [00:11<00:03, 44.05it/s][A
 73%|███████▎  | 394/543 [00:11<00:03, 44.32it/s][A
 73%|███████▎  | 399/543 [00:11<00:03, 44.20it/s][A
 74%|███████▍  | 404/543 [00:11<00:03, 44.26it/s][A
 75%|███████▌  | 409/543 [00:11<00:03, 44.51it/s][A
 76%|███████▌  | 414/543 [00:12<00:02, 44.75it/s][A
 77%|███████▋  | 419/543 [00:12<00:05, 22.65it/s][A
 78%|███████▊  | 424/543 [00:12<00:06, 18.74it/s][A
 79%|███████▉  | 429/543 [00:12<00:04, 22.94it/s][A
 80%|███████▉  | 434/543 [00:12<00:04, 26.95it/s][A
 81%|████████  | 439/543 [00:12<00:03, 30.74it/s][A
 82%|████████▏ | 444/543 [00:13<00:02, 34.06it/s][A
 83%|████████▎ | 449/543 [00:13<00:02, 36.90it/s][A
 84%|████████▎ | 454/543 [00:13<00:02, 39.11it/s][A
 85%|████████▍ | 459/543 [00:13<00:02, 40.84it/s][A
 85%|████████▌ | 464/543 [00:13<00:01, 41.99it/s][A
 86%|████████▋ | 469/543 [00:13<00:01, 42.64it/s][A
 87%|████████▋ | 474/543 [00:13<00:01, 43.21it/s][A
 88%|████████▊ | 479/543 [00:13<00:01, 43.83it/s][A
 89%|████████▉ | 484/543 [00:13<00:01, 44.23it/s][A
 90%|█████████ | 489/543 [00:14<00:01, 44.63it/s][A
 91%|█████████ | 494/543 [00:14<00:01, 44.91it/s][A
 92%|█████████▏| 499/543 [00:14<00:01, 22.98it/s][A
 93%|█████████▎| 504/543 [00:14<00:01, 27.02it/s][A
 94%|█████████▎| 509/543 [00:14<00:01, 30.80it/s][A
 95%|█████████▍| 514/543 [00:14<00:00, 34.11it/s][A
 96%|█████████▌| 519/543 [00:15<00:00, 36.96it/s][A
 97%|█████████▋| 524/543 [00:15<00:00, 39.12it/s][A
 97%|█████████▋| 529/543 [00:15<00:00, 40.87it/s][A
 98%|█████████▊| 534/543 [00:15<00:00, 42.02it/s][A
 99%|█████████▉| 539/543 [00:15<00:00, 42.56it/s][A
                                                 [A                                                
100%|██████████| 543/543 [00:15<00:00, 42.56it/s][A 20%|██        | 47/235 [00:31<01:04,  2.90it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:56:30,307 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47
[INFO|configuration_utils.py:351] 2023-08-28 20:56:30,664 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:56:44,175 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:56:45,194 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:56:45,306 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47/special_tokens_map.json
 20%|██        | 48/235 [01:20<1:01:08, 19.62s/it] 21%|██        | 49/235 [01:20<42:55, 13.84s/it]   21%|██▏       | 50/235 [01:20<30:09,  9.78s/it] 22%|██▏       | 51/235 [01:21<21:15,  6.93s/it] 22%|██▏       | 52/235 [01:21<15:04,  4.94s/it] 23%|██▎       | 53/235 [01:21<10:45,  3.55s/it] 23%|██▎       | 54/235 [01:22<07:45,  2.57s/it] 23%|██▎       | 55/235 [01:22<05:39,  1.89s/it] 24%|██▍       | 56/235 [01:22<04:11,  1.41s/it] 24%|██▍       | 57/235 [01:22<03:11,  1.07s/it] 25%|██▍       | 58/235 [01:23<02:28,  1.19it/s] 25%|██▌       | 59/235 [01:23<02:06,  1.39it/s] 26%|██▌       | 60/235 [01:23<01:43,  1.70it/s] 26%|██▌       | 61/235 [01:24<01:26,  2.00it/s] 26%|██▋       | 62/235 [01:24<01:15,  2.28it/s] 27%|██▋       | 63/235 [01:24<01:07,  2.54it/s] 27%|██▋       | 64/235 [01:25<01:02,  2.75it/s] 28%|██▊       | 65/235 [01:25<00:58,  2.92it/s] 28%|██▊       | 66/235 [01:25<00:55,  3.06it/s] 29%|██▊       | 67/235 [01:25<00:53,  3.16it/s] 29%|██▉       | 68/235 [01:26<00:51,  3.24it/s] 29%|██▉       | 69/235 [01:26<00:58,  2.85it/s] 30%|██▉       | 70/235 [01:26<00:54,  3.00it/s] 30%|███       | 71/235 [01:27<00:52,  3.12it/s] 31%|███       | 72/235 [01:27<00:50,  3.20it/s] 31%|███       | 73/235 [01:27<00:49,  3.27it/s] 31%|███▏      | 74/235 [01:28<00:48,  3.31it/s] 32%|███▏      | 75/235 [01:28<00:47,  3.34it/s] 32%|███▏      | 76/235 [01:28<00:47,  3.37it/s] 33%|███▎      | 77/235 [01:29<00:46,  3.38it/s] 33%|███▎      | 78/235 [01:29<00:46,  3.40it/s] 34%|███▎      | 79/235 [01:29<00:52,  2.95it/s] 34%|███▍      | 80/235 [01:30<00:50,  3.08it/s] 34%|███▍      | 81/235 [01:30<00:48,  3.17it/s] 35%|███▍      | 82/235 [01:30<00:47,  3.25it/s] 35%|███▌      | 83/235 [01:30<00:46,  3.30it/s] 36%|███▌      | 84/235 [01:31<00:45,  3.33it/s] 36%|███▌      | 85/235 [01:31<00:44,  3.36it/s] 37%|███▋      | 86/235 [01:31<00:44,  3.38it/s] 37%|███▋      | 87/235 [01:32<00:43,  3.39it/s] 37%|███▋      | 88/235 [01:32<00:43,  3.40it/s] 38%|███▊      | 89/235 [01:32<00:54,  2.66it/s] 38%|███▊      | 90/235 [01:33<00:50,  2.85it/s] 39%|███▊      | 91/235 [01:33<00:48,  3.00it/s] 39%|███▉      | 92/235 [01:33<00:45,  3.12it/s] 40%|███▉      | 93/235 [01:34<00:44,  3.20it/s] 40%|████      | 94/235 [01:34<00:43,  3.26it/s][INFO|trainer.py:2140] 2023-08-28 20:57:32,774 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:57:32,774 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 20:57:32,774 >>   Batch size = 8
{'eval_loss': 1.0710638761520386, 'eval_runtime': 15.6132, 'eval_samples_per_second': 278.097, 'eval_steps_per_second': 34.778, 'epoch': 0.99}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.34it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.48it/s][A
  3%|▎         | 18/543 [00:00<00:10, 47.79it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.72it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.09it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.53it/s][A
  7%|▋         | 38/543 [00:00<00:11, 45.22it/s][A
  8%|▊         | 43/543 [00:01<00:11, 45.06it/s][A
  9%|▉         | 48/543 [00:01<00:15, 32.31it/s][A
 10%|▉         | 53/543 [00:01<00:13, 35.48it/s][A
 11%|█         | 58/543 [00:01<00:12, 38.09it/s][A
 12%|█▏        | 63/543 [00:01<00:11, 40.08it/s][A
 13%|█▎        | 68/543 [00:01<00:11, 41.58it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 42.73it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 43.66it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 44.25it/s][A
 16%|█▌        | 88/543 [00:02<00:10, 44.27it/s][A
 17%|█▋        | 93/543 [00:02<00:22, 20.22it/s][A
 18%|█▊        | 98/543 [00:02<00:18, 24.28it/s][A
 19%|█▉        | 103/543 [00:02<00:15, 28.31it/s][A
 20%|█▉        | 108/543 [00:02<00:13, 32.00it/s][A
 21%|██        | 113/543 [00:03<00:12, 35.18it/s][A
 22%|██▏       | 118/543 [00:03<00:11, 37.80it/s][A
 23%|██▎       | 123/543 [00:03<00:10, 39.96it/s][A
 24%|██▎       | 128/543 [00:03<00:09, 41.52it/s][A
 24%|██▍       | 133/543 [00:03<00:09, 42.35it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 42.86it/s][A
 26%|██▋       | 143/543 [00:03<00:09, 43.41it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 43.97it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 44.48it/s][A
 29%|██▉       | 158/543 [00:04<00:08, 44.90it/s][A
 30%|███       | 163/543 [00:04<00:08, 45.08it/s][A
 31%|███       | 168/543 [00:04<00:08, 45.29it/s][A
 32%|███▏      | 173/543 [00:04<00:08, 45.32it/s][A
 33%|███▎      | 178/543 [00:04<00:08, 45.31it/s][A
 34%|███▎      | 183/543 [00:04<00:07, 45.25it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 45.08it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 45.15it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 45.30it/s][A
 37%|███▋      | 203/543 [00:05<00:07, 45.28it/s][A
 38%|███▊      | 208/543 [00:05<00:07, 45.40it/s][A
 39%|███▉      | 213/543 [00:05<00:13, 23.78it/s][A
 40%|████      | 218/543 [00:05<00:11, 27.79it/s][A
 41%|████      | 223/543 [00:05<00:10, 31.54it/s][A
 42%|████▏     | 228/543 [00:05<00:09, 34.78it/s][A
 43%|████▎     | 233/543 [00:06<00:08, 37.52it/s][A
 44%|████▍     | 238/543 [00:06<00:07, 39.64it/s][A
 45%|████▍     | 243/543 [00:06<00:07, 41.37it/s][A
 46%|████▌     | 248/543 [00:06<00:06, 42.47it/s][A
 47%|████▋     | 253/543 [00:06<00:06, 43.00it/s][A
 48%|████▊     | 258/543 [00:06<00:06, 43.27it/s][A
 48%|████▊     | 263/543 [00:06<00:06, 43.72it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 44.26it/s][A
 50%|█████     | 273/543 [00:06<00:06, 44.67it/s][A
 51%|█████     | 278/543 [00:07<00:09, 27.21it/s][A
 52%|█████▏    | 284/543 [00:07<00:08, 32.22it/s][A
 53%|█████▎    | 289/543 [00:07<00:07, 35.24it/s][A
 54%|█████▍    | 294/543 [00:07<00:06, 37.76it/s][A
 55%|█████▌    | 299/543 [00:07<00:06, 39.87it/s][A
 56%|█████▌    | 304/543 [00:07<00:05, 41.46it/s][A
 57%|█████▋    | 309/543 [00:07<00:05, 42.61it/s][A
 58%|█████▊    | 314/543 [00:08<00:05, 43.46it/s][A
 59%|█████▊    | 319/543 [00:08<00:05, 43.96it/s][A
 60%|█████▉    | 324/543 [00:08<00:05, 38.78it/s][A
 61%|██████    | 329/543 [00:08<00:05, 40.62it/s][A
 62%|██████▏   | 334/543 [00:08<00:05, 41.75it/s][A
 62%|██████▏   | 339/543 [00:08<00:04, 42.90it/s][A
 63%|██████▎   | 344/543 [00:08<00:04, 43.71it/s][A
 64%|██████▍   | 349/543 [00:08<00:04, 44.24it/s][A
 65%|██████▌   | 354/543 [00:08<00:04, 44.64it/s][A
 66%|██████▌   | 359/543 [00:09<00:04, 44.88it/s][A
 67%|██████▋   | 364/543 [00:09<00:04, 44.72it/s][A
 68%|██████▊   | 369/543 [00:09<00:03, 44.72it/s][A
 69%|██████▉   | 374/543 [00:09<00:03, 44.83it/s][A
 70%|██████▉   | 379/543 [00:09<00:03, 45.05it/s][A
 71%|███████   | 384/543 [00:09<00:03, 45.24it/s][A
 72%|███████▏  | 389/543 [00:09<00:03, 45.37it/s][A
 73%|███████▎  | 394/543 [00:09<00:03, 45.39it/s][A
 73%|███████▎  | 399/543 [00:09<00:03, 45.45it/s][A
 74%|███████▍  | 404/543 [00:10<00:03, 45.35it/s][A
 75%|███████▌  | 409/543 [00:10<00:02, 45.18it/s][A
 76%|███████▌  | 414/543 [00:10<00:02, 45.06it/s][A
 77%|███████▋  | 419/543 [00:10<00:02, 44.97it/s][A
 78%|███████▊  | 424/543 [00:10<00:02, 45.02it/s][A
 79%|███████▉  | 429/543 [00:10<00:02, 45.24it/s][A
 80%|███████▉  | 434/543 [00:10<00:02, 45.38it/s][A
 81%|████████  | 439/543 [00:10<00:02, 45.45it/s][A
 82%|████████▏ | 444/543 [00:10<00:02, 45.44it/s][A
 83%|████████▎ | 449/543 [00:11<00:02, 45.43it/s][A
 84%|████████▎ | 454/543 [00:11<00:01, 45.37it/s][A
 85%|████████▍ | 459/543 [00:11<00:02, 36.60it/s][A
 85%|████████▌ | 464/543 [00:11<00:02, 38.94it/s][A
 86%|████████▋ | 469/543 [00:11<00:01, 40.83it/s][A
 87%|████████▋ | 474/543 [00:11<00:01, 42.20it/s][A
 88%|████████▊ | 479/543 [00:11<00:01, 43.17it/s][A
 89%|████████▉ | 484/543 [00:11<00:01, 43.91it/s][A
 90%|█████████ | 489/543 [00:12<00:01, 44.44it/s][A
 91%|█████████ | 494/543 [00:12<00:01, 44.78it/s][A
 92%|█████████▏| 499/543 [00:13<00:03, 14.01it/s][A
 93%|█████████▎| 504/543 [00:13<00:02, 17.78it/s][A
 94%|█████████▎| 509/543 [00:13<00:01, 21.79it/s][A
 95%|█████████▍| 514/543 [00:13<00:01, 25.84it/s][A
 96%|█████████▌| 519/543 [00:13<00:00, 29.75it/s][A
 97%|█████████▋| 524/543 [00:13<00:00, 33.24it/s][A
 97%|█████████▋| 529/543 [00:13<00:00, 36.17it/s][A
 98%|█████████▊| 534/543 [00:13<00:00, 38.58it/s][A
 99%|█████████▉| 539/543 [00:13<00:00, 40.34it/s][A
                                                 [A                                                
100%|██████████| 543/543 [00:14<00:00, 40.34it/s][A 40%|████      | 94/235 [01:48<00:43,  3.26it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:57:47,306 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-28 20:57:48,193 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:58:00,938 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:58:01,084 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:58:01,167 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94/special_tokens_map.json
 40%|████      | 95/235 [02:39<46:22, 19.88s/it] 41%|████      | 96/235 [02:40<32:32, 14.04s/it] 41%|████▏     | 97/235 [02:40<22:48,  9.92s/it] 42%|████▏     | 98/235 [02:40<16:03,  7.03s/it] 42%|████▏     | 99/235 [02:41<11:21,  5.01s/it] 43%|████▎     | 100/235 [02:41<08:12,  3.65s/it] 43%|████▎     | 101/235 [02:42<05:54,  2.64s/it] 43%|████▎     | 102/235 [02:42<04:17,  1.94s/it] 44%|████▍     | 103/235 [02:42<03:10,  1.44s/it] 44%|████▍     | 104/235 [02:42<02:23,  1.10s/it] 45%|████▍     | 105/235 [02:43<01:51,  1.17it/s] 45%|████▌     | 106/235 [02:43<01:35,  1.35it/s] 46%|████▌     | 107/235 [02:43<01:17,  1.65it/s] 46%|████▌     | 108/235 [02:44<01:05,  1.95it/s] 46%|████▋     | 109/235 [02:44<00:56,  2.24it/s] 47%|████▋     | 110/235 [02:44<00:49,  2.50it/s] 47%|████▋     | 111/235 [02:45<00:45,  2.72it/s] 48%|████▊     | 112/235 [02:45<00:42,  2.90it/s] 48%|████▊     | 113/235 [02:45<00:40,  3.04it/s] 49%|████▊     | 114/235 [02:46<00:38,  3.14it/s] 49%|████▉     | 115/235 [02:46<00:37,  3.22it/s] 49%|████▉     | 116/235 [02:46<00:44,  2.70it/s] 50%|████▉     | 117/235 [02:48<01:28,  1.33it/s] 50%|█████     | 118/235 [02:48<01:11,  1.63it/s] 51%|█████     | 119/235 [02:49<00:59,  1.94it/s] 51%|█████     | 120/235 [02:49<00:51,  2.23it/s] 51%|█████▏    | 121/235 [02:49<00:51,  2.22it/s] 52%|█████▏    | 122/235 [02:50<00:45,  2.48it/s] 52%|█████▏    | 123/235 [02:50<00:41,  2.70it/s] 53%|█████▎    | 124/235 [02:50<00:38,  2.89it/s] 53%|█████▎    | 125/235 [02:50<00:36,  3.03it/s] 54%|█████▎    | 126/235 [02:51<00:34,  3.14it/s] 54%|█████▍    | 127/235 [02:51<00:33,  3.22it/s] 54%|█████▍    | 128/235 [02:51<00:32,  3.28it/s] 55%|█████▍    | 129/235 [02:52<00:31,  3.32it/s] 55%|█████▌    | 130/235 [02:52<00:31,  3.35it/s] 56%|█████▌    | 131/235 [02:52<00:34,  3.01it/s] 56%|█████▌    | 132/235 [02:53<00:33,  3.12it/s] 57%|█████▋    | 133/235 [02:53<00:31,  3.21it/s] 57%|█████▋    | 134/235 [02:53<00:30,  3.27it/s] 57%|█████▋    | 135/235 [02:53<00:30,  3.31it/s] 58%|█████▊    | 136/235 [02:54<00:29,  3.34it/s] 58%|█████▊    | 137/235 [02:54<00:29,  3.37it/s] 59%|█████▊    | 138/235 [02:54<00:28,  3.38it/s] 59%|█████▉    | 139/235 [02:55<00:28,  3.40it/s] 60%|█████▉    | 140/235 [02:55<00:27,  3.40it/s] 60%|██████    | 141/235 [02:56<00:35,  2.68it/s][INFO|trainer.py:2140] 2023-08-28 20:58:54,377 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:58:54,377 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 20:58:54,377 >>   Batch size = 8
{'eval_loss': 1.065231442451477, 'eval_runtime': 14.064, 'eval_samples_per_second': 308.732, 'eval_steps_per_second': 38.609, 'epoch': 1.99}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.70it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.62it/s][A
  3%|▎         | 18/543 [00:00<00:10, 47.78it/s][A
  4%|▍         | 23/543 [00:00<00:11, 47.09it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.57it/s][A
  6%|▌         | 33/543 [00:00<00:11, 46.15it/s][A
  7%|▋         | 38/543 [00:00<00:10, 45.92it/s][A
  8%|▊         | 43/543 [00:00<00:11, 45.26it/s][A
  9%|▉         | 48/543 [00:01<00:11, 44.93it/s][A
 10%|▉         | 53/543 [00:01<00:10, 44.90it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.01it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.24it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.30it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 45.41it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 45.42it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 45.32it/s][A
 16%|█▌        | 88/543 [00:01<00:10, 45.02it/s][A
 17%|█▋        | 93/543 [00:02<00:10, 44.85it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 44.81it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 44.77it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 45.13it/s][A
 21%|██        | 113/543 [00:02<00:09, 45.29it/s][A
 22%|██▏       | 118/543 [00:02<00:11, 36.40it/s][A
 23%|██▎       | 123/543 [00:02<00:10, 38.74it/s][A
 24%|██▎       | 128/543 [00:02<00:10, 40.53it/s][A
 24%|██▍       | 133/543 [00:03<00:09, 41.92it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 42.96it/s][A
 26%|██▋       | 143/543 [00:03<00:09, 43.66it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 44.22it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 44.73it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 44.53it/s][A
 30%|███       | 163/543 [00:03<00:08, 44.75it/s][A
 31%|███       | 168/543 [00:03<00:08, 44.89it/s][A
 32%|███▏      | 173/543 [00:03<00:08, 45.07it/s][A
 33%|███▎      | 178/543 [00:03<00:08, 45.12it/s][A
 34%|███▎      | 183/543 [00:04<00:07, 45.12it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 45.24it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 45.28it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 45.39it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 45.18it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 45.18it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 45.21it/s][A
 40%|████      | 218/543 [00:04<00:07, 45.27it/s][A
 41%|████      | 223/543 [00:04<00:07, 45.18it/s][A
 42%|████▏     | 228/543 [00:05<00:06, 45.26it/s][A
 43%|████▎     | 233/543 [00:05<00:06, 45.18it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 45.30it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 45.18it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 45.23it/s][A
 47%|████▋     | 253/543 [00:05<00:08, 35.63it/s][A
 48%|████▊     | 258/543 [00:05<00:07, 38.17it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 40.05it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 41.59it/s][A
 50%|█████     | 273/543 [00:06<00:06, 42.66it/s][A
 51%|█████     | 278/543 [00:06<00:06, 43.47it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 43.97it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 44.37it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 44.45it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 44.79it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 45.03it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 45.14it/s][A
 58%|█████▊    | 313/543 [00:07<00:05, 45.18it/s][A
 59%|█████▊    | 318/543 [00:07<00:04, 45.23it/s][A
 59%|█████▉    | 323/543 [00:07<00:04, 45.20it/s][A
 60%|██████    | 328/543 [00:07<00:04, 45.07it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 45.05it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 45.03it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 45.09it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 45.20it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 45.28it/s][A
 66%|██████▌   | 358/543 [00:08<00:04, 45.31it/s][A
 67%|██████▋   | 363/543 [00:08<00:03, 45.29it/s][A
 68%|██████▊   | 368/543 [00:08<00:03, 45.25it/s][A
 69%|██████▊   | 373/543 [00:08<00:03, 45.07it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 45.07it/s][A
 71%|███████   | 383/543 [00:08<00:03, 45.10it/s][A
 71%|███████▏  | 388/543 [00:08<00:04, 35.71it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 38.16it/s][A
 73%|███████▎  | 398/543 [00:09<00:03, 40.05it/s][A
 74%|███████▍  | 403/543 [00:09<00:03, 41.54it/s][A
 75%|███████▌  | 408/543 [00:09<00:03, 42.51it/s][A
 76%|███████▌  | 413/543 [00:09<00:02, 43.40it/s][A
 77%|███████▋  | 418/543 [00:09<00:02, 43.90it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 44.43it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 44.44it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 44.75it/s][A
 81%|████████  | 438/543 [00:09<00:02, 44.95it/s][A
 82%|████████▏ | 443/543 [00:10<00:02, 45.13it/s][A
 83%|████████▎ | 448/543 [00:10<00:02, 45.21it/s][A
 83%|████████▎ | 453/543 [00:10<00:01, 45.12it/s][A
 84%|████████▍ | 458/543 [00:10<00:01, 45.17it/s][A
 85%|████████▌ | 463/543 [00:10<00:01, 45.06it/s][A
 86%|████████▌ | 468/543 [00:10<00:01, 45.06it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 44.95it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 45.03it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 45.13it/s][A
 90%|████████▉ | 488/543 [00:11<00:01, 45.27it/s][A
 91%|█████████ | 493/543 [00:11<00:01, 45.27it/s][A
 92%|█████████▏| 498/543 [00:11<00:00, 45.21it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 45.24it/s][A
 94%|█████████▎| 508/543 [00:11<00:00, 45.16it/s][A
 94%|█████████▍| 513/543 [00:11<00:00, 45.12it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 45.05it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 32.51it/s][A
 97%|█████████▋| 528/543 [00:12<00:00, 35.54it/s][A
 98%|█████████▊| 533/543 [00:12<00:00, 38.05it/s][A
 99%|█████████▉| 538/543 [00:12<00:00, 39.96it/s][A
100%|██████████| 543/543 [00:12<00:00, 41.28it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 41.28it/s][A 60%|██████    | 141/235 [03:08<00:35,  2.68it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:59:07,758 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141
[INFO|configuration_utils.py:351] 2023-08-28 20:59:08,003 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:59:26,102 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:59:27,350 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:59:27,605 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141/special_tokens_map.json
 60%|██████    | 142/235 [04:14<36:54, 23.81s/it] 61%|██████    | 143/235 [04:15<25:54, 16.90s/it] 61%|██████▏   | 144/235 [04:15<18:11, 12.00s/it] 62%|██████▏   | 145/235 [04:16<12:43,  8.48s/it] 62%|██████▏   | 146/235 [04:16<08:56,  6.03s/it] 63%|██████▎   | 147/235 [04:16<06:18,  4.31s/it] 63%|██████▎   | 148/235 [04:17<04:29,  3.10s/it] 63%|██████▎   | 149/235 [04:17<03:14,  2.26s/it] 64%|██████▍   | 150/235 [04:17<02:21,  1.67s/it] 64%|██████▍   | 151/235 [04:17<01:45,  1.26s/it] 65%|██████▍   | 152/235 [04:18<01:20,  1.03it/s] 65%|██████▌   | 153/235 [04:18<01:02,  1.31it/s] 66%|██████▌   | 154/235 [04:19<01:06,  1.21it/s] 66%|██████▌   | 155/235 [04:19<00:53,  1.50it/s] 66%|██████▋   | 156/235 [04:20<00:43,  1.81it/s] 67%|██████▋   | 157/235 [04:20<00:37,  2.11it/s] 67%|██████▋   | 158/235 [04:20<00:32,  2.38it/s] 68%|██████▊   | 159/235 [04:20<00:29,  2.62it/s] 68%|██████▊   | 160/235 [04:21<00:26,  2.82it/s] 69%|██████▊   | 161/235 [04:21<00:24,  2.97it/s] 69%|██████▉   | 162/235 [04:22<00:37,  1.96it/s] 69%|██████▉   | 163/235 [04:22<00:31,  2.25it/s] 70%|██████▉   | 164/235 [04:22<00:28,  2.51it/s] 70%|███████   | 165/235 [04:23<00:25,  2.73it/s] 71%|███████   | 166/235 [04:23<00:23,  2.90it/s] 71%|███████   | 167/235 [04:23<00:22,  3.04it/s] 71%|███████▏  | 168/235 [04:24<00:21,  3.15it/s] 72%|███████▏  | 169/235 [04:24<00:20,  3.23it/s] 72%|███████▏  | 170/235 [04:24<00:19,  3.28it/s] 73%|███████▎  | 171/235 [04:25<00:33,  1.89it/s] 73%|███████▎  | 172/235 [04:26<00:28,  2.18it/s] 74%|███████▎  | 173/235 [04:26<00:25,  2.45it/s] 74%|███████▍  | 174/235 [04:26<00:22,  2.68it/s] 74%|███████▍  | 175/235 [04:26<00:20,  2.87it/s] 75%|███████▍  | 176/235 [04:27<00:19,  3.01it/s] 75%|███████▌  | 177/235 [04:27<00:18,  3.12it/s] 76%|███████▌  | 178/235 [04:27<00:17,  3.21it/s] 76%|███████▌  | 179/235 [04:28<00:20,  2.76it/s] 77%|███████▋  | 180/235 [04:28<00:18,  2.93it/s] 77%|███████▋  | 181/235 [04:28<00:17,  3.06it/s] 77%|███████▋  | 182/235 [04:29<00:16,  3.16it/s] 78%|███████▊  | 183/235 [04:29<00:16,  3.23it/s] 78%|███████▊  | 184/235 [04:29<00:15,  3.29it/s] 79%|███████▊  | 185/235 [04:30<00:15,  3.33it/s] 79%|███████▉  | 186/235 [04:30<00:14,  3.36it/s] 80%|███████▉  | 187/235 [04:30<00:14,  3.38it/s] 80%|████████  | 188/235 [04:30<00:13,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 21:00:29,364 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:00:29,364 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 21:00:29,364 >>   Batch size = 8
{'eval_loss': 1.081190586090088, 'eval_runtime': 12.4277, 'eval_samples_per_second': 349.382, 'eval_steps_per_second': 43.693, 'epoch': 2.99}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.00it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.77it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.60it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.93it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.52it/s][A
  6%|▌         | 33/543 [00:00<00:11, 46.27it/s][A
  7%|▋         | 38/543 [00:00<00:10, 46.05it/s][A
  8%|▊         | 43/543 [00:00<00:11, 45.29it/s][A
  9%|▉         | 48/543 [00:01<00:10, 45.06it/s][A
 10%|▉         | 53/543 [00:01<00:10, 45.06it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.22it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.17it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.37it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 45.46it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 45.55it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 45.47it/s][A
 16%|█▌        | 88/543 [00:01<00:10, 45.09it/s][A
 17%|█▋        | 93/543 [00:02<00:10, 44.95it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 44.92it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 44.93it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 45.03it/s][A
 21%|██        | 113/543 [00:02<00:09, 45.22it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 45.22it/s][A
 23%|██▎       | 123/543 [00:02<00:09, 45.39it/s][A
 24%|██▎       | 128/543 [00:02<00:09, 45.28it/s][A
 24%|██▍       | 133/543 [00:03<00:09, 45.10it/s][A
 25%|██▌       | 138/543 [00:03<00:11, 34.92it/s][A
 26%|██▋       | 143/543 [00:03<00:10, 37.57it/s][A
 27%|██▋       | 148/543 [00:03<00:09, 39.62it/s][A
 28%|██▊       | 153/543 [00:03<00:09, 41.23it/s][A
 29%|██▉       | 158/543 [00:03<00:09, 42.50it/s][A
 30%|███       | 163/543 [00:03<00:08, 43.30it/s][A
 31%|███       | 168/543 [00:03<00:08, 43.95it/s][A
 32%|███▏      | 173/543 [00:03<00:08, 44.39it/s][A
 33%|███▎      | 178/543 [00:04<00:08, 44.28it/s][A
 34%|███▎      | 183/543 [00:04<00:08, 44.48it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 44.62it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 44.88it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 45.09it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 45.31it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 45.41it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 45.42it/s][A
 40%|████      | 218/543 [00:04<00:07, 45.30it/s][A
 41%|████      | 223/543 [00:05<00:07, 45.20it/s][A
 42%|████▏     | 228/543 [00:05<00:06, 45.03it/s][A
 43%|████▎     | 233/543 [00:05<00:06, 45.04it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 45.15it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 45.42it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 45.53it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 45.63it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 45.47it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 45.41it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 45.21it/s][A
 50%|█████     | 273/543 [00:06<00:08, 30.16it/s][A
 51%|█████     | 278/543 [00:06<00:07, 33.54it/s][A
 52%|█████▏    | 283/543 [00:06<00:07, 36.44it/s][A
 53%|█████▎    | 288/543 [00:06<00:06, 38.77it/s][A
 54%|█████▍    | 293/543 [00:06<00:06, 40.54it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 42.01it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 43.11it/s][A
 57%|█████▋    | 308/543 [00:07<00:05, 43.92it/s][A
 58%|█████▊    | 313/543 [00:07<00:05, 43.98it/s][A
 59%|█████▊    | 318/543 [00:07<00:05, 44.28it/s][A
 59%|█████▉    | 323/543 [00:07<00:04, 44.54it/s][A
 60%|██████    | 328/543 [00:07<00:04, 44.81it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 44.90it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 45.06it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 45.12it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 45.35it/s][A
 65%|██████▌   | 353/543 [00:08<00:04, 45.36it/s][A
 66%|██████▌   | 358/543 [00:08<00:04, 45.31it/s][A
 67%|██████▋   | 363/543 [00:08<00:03, 45.18it/s][A
 68%|██████▊   | 368/543 [00:08<00:03, 45.23it/s][A
 69%|██████▊   | 373/543 [00:08<00:03, 45.21it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 45.18it/s][A
 71%|███████   | 383/543 [00:08<00:03, 45.17it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 45.34it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 45.44it/s][A
 73%|███████▎  | 398/543 [00:09<00:03, 45.32it/s][A
 74%|███████▍  | 403/543 [00:09<00:03, 43.63it/s][A
 75%|███████▌  | 408/543 [00:09<00:03, 44.24it/s][A
 76%|███████▌  | 413/543 [00:09<00:02, 44.54it/s][A
 77%|███████▋  | 418/543 [00:09<00:02, 44.77it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 44.77it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 44.97it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 44.86it/s][A
 81%|████████  | 438/543 [00:09<00:02, 45.16it/s][A
 82%|████████▏ | 443/543 [00:10<00:02, 45.08it/s][A
 83%|████████▎ | 448/543 [00:10<00:02, 45.16it/s][A
 83%|████████▎ | 453/543 [00:10<00:01, 45.18it/s][A
 84%|████████▍ | 458/543 [00:10<00:01, 45.25it/s][A
 85%|████████▌ | 463/543 [00:10<00:01, 45.29it/s][A
 86%|████████▌ | 468/543 [00:10<00:01, 45.23it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 45.29it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 45.33it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 45.30it/s][A
 90%|████████▉ | 488/543 [00:11<00:01, 45.20it/s][A
 91%|█████████ | 493/543 [00:11<00:01, 45.18it/s][A
 92%|█████████▏| 498/543 [00:11<00:00, 45.13it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 45.24it/s][A
 94%|█████████▎| 508/543 [00:11<00:00, 45.25it/s][A
 94%|█████████▍| 513/543 [00:11<00:00, 45.33it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 45.24it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 45.34it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 45.30it/s][A
 98%|█████████▊| 533/543 [00:12<00:00, 45.27it/s][A
 99%|█████████▉| 538/543 [00:12<00:00, 45.19it/s][A
100%|██████████| 543/543 [00:12<00:00, 36.41it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 36.41it/s][A 80%|████████  | 188/235 [04:43<00:13,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:00:42,065 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-28 21:00:42,572 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:00:57,943 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:00:59,067 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:00:59,696 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188/special_tokens_map.json
 80%|████████  | 189/235 [05:34<14:44, 19.22s/it] 81%|████████  | 190/235 [05:34<10:12, 13.60s/it] 81%|████████▏ | 191/235 [05:35<07:02,  9.61s/it] 82%|████████▏ | 192/235 [05:35<04:52,  6.81s/it] 82%|████████▏ | 193/235 [05:35<03:23,  4.85s/it] 83%|████████▎ | 194/235 [05:35<02:22,  3.48s/it] 83%|████████▎ | 195/235 [05:36<01:41,  2.53s/it] 83%|████████▎ | 196/235 [05:36<01:12,  1.85s/it] 84%|████████▍ | 197/235 [05:36<00:52,  1.38s/it] 84%|████████▍ | 198/235 [05:37<00:39,  1.05s/it] 85%|████████▍ | 199/235 [05:37<00:29,  1.21it/s] 85%|████████▌ | 200/235 [05:37<00:24,  1.41it/s] 86%|████████▌ | 201/235 [05:38<00:19,  1.72it/s] 86%|████████▌ | 202/235 [05:38<00:16,  2.03it/s] 86%|████████▋ | 203/235 [05:38<00:13,  2.32it/s] 87%|████████▋ | 204/235 [05:38<00:12,  2.57it/s] 87%|████████▋ | 205/235 [05:39<00:10,  2.79it/s] 88%|████████▊ | 206/235 [05:39<00:09,  2.97it/s] 88%|████████▊ | 207/235 [05:39<00:09,  3.10it/s] 89%|████████▊ | 208/235 [05:40<00:08,  3.21it/s] 89%|████████▉ | 209/235 [05:40<00:07,  3.28it/s] 89%|████████▉ | 210/235 [05:40<00:08,  2.79it/s] 90%|████████▉ | 211/235 [05:41<00:08,  2.96it/s] 90%|█████████ | 212/235 [05:41<00:07,  3.10it/s] 91%|█████████ | 213/235 [05:41<00:06,  3.20it/s] 91%|█████████ | 214/235 [05:42<00:06,  3.28it/s] 91%|█████████▏| 215/235 [05:42<00:05,  3.33it/s] 92%|█████████▏| 216/235 [05:42<00:05,  3.38it/s] 92%|█████████▏| 217/235 [05:42<00:05,  3.40it/s] 93%|█████████▎| 218/235 [05:43<00:04,  3.42it/s] 93%|█████████▎| 219/235 [05:43<00:04,  3.43it/s] 94%|█████████▎| 220/235 [05:43<00:04,  3.15it/s] 94%|█████████▍| 221/235 [05:44<00:04,  3.24it/s] 94%|█████████▍| 222/235 [05:44<00:03,  3.31it/s] 95%|█████████▍| 223/235 [05:44<00:03,  3.35it/s] 95%|█████████▌| 224/235 [05:45<00:03,  3.39it/s] 96%|█████████▌| 225/235 [05:45<00:02,  3.41it/s] 96%|█████████▌| 226/235 [05:45<00:02,  3.43it/s] 97%|█████████▋| 227/235 [05:45<00:02,  3.44it/s] 97%|█████████▋| 228/235 [05:46<00:02,  3.45it/s] 97%|█████████▋| 229/235 [05:46<00:01,  3.46it/s] 98%|█████████▊| 230/235 [05:46<00:01,  3.46it/s] 98%|█████████▊| 231/235 [05:47<00:01,  3.20it/s] 99%|█████████▊| 232/235 [05:47<00:00,  3.28it/s] 99%|█████████▉| 233/235 [05:48<00:00,  2.19it/s]100%|█████████▉| 234/235 [05:48<00:00,  2.46it/s]100%|██████████| 235/235 [05:48<00:00,  2.69it/s][INFO|trainer.py:2140] 2023-08-28 21:01:47,128 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:01:47,129 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 21:01:47,129 >>   Batch size = 8
{'eval_loss': 1.0876038074493408, 'eval_runtime': 12.3805, 'eval_samples_per_second': 350.714, 'eval_steps_per_second': 43.859, 'epoch': 3.99}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.93it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.46it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.58it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.87it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.34it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.97it/s][A
  7%|▋         | 38/543 [00:00<00:11, 45.71it/s][A
  8%|▊         | 43/543 [00:00<00:10, 45.48it/s][A
  9%|▉         | 48/543 [00:01<00:10, 45.49it/s][A
 10%|▉         | 53/543 [00:01<00:10, 45.57it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.46it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.47it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.38it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 45.28it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 45.27it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 42.88it/s][A
 16%|█▌        | 88/543 [00:01<00:10, 43.72it/s][A
 17%|█▋        | 93/543 [00:02<00:10, 44.32it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 44.60it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 44.91it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 45.06it/s][A
 21%|██        | 113/543 [00:02<00:09, 45.03it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 45.12it/s][A
 23%|██▎       | 123/543 [00:02<00:09, 44.82it/s][A
 24%|██▎       | 128/543 [00:02<00:09, 44.95it/s][A
 24%|██▍       | 133/543 [00:02<00:09, 45.09it/s][A
 25%|██▌       | 138/543 [00:03<00:08, 45.14it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 45.30it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 45.44it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 45.47it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 45.42it/s][A
 30%|███       | 163/543 [00:03<00:08, 45.29it/s][A
 31%|███       | 168/543 [00:03<00:08, 45.22it/s][A
 32%|███▏      | 173/543 [00:03<00:08, 45.21it/s][A
 33%|███▎      | 178/543 [00:03<00:08, 45.23it/s][A
 34%|███▎      | 183/543 [00:04<00:07, 45.15it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 45.37it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 45.37it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 45.43it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 45.32it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 45.24it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 45.20it/s][A
 40%|████      | 218/543 [00:04<00:09, 34.38it/s][A
 41%|████      | 223/543 [00:05<00:08, 37.10it/s][A
 42%|████▏     | 228/543 [00:05<00:08, 39.26it/s][A
 43%|████▎     | 233/543 [00:05<00:07, 40.94it/s][A
 44%|████▍     | 238/543 [00:05<00:07, 42.17it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 43.08it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 43.79it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 44.20it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 44.41it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 44.64it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 44.96it/s][A
 50%|█████     | 273/543 [00:06<00:05, 45.13it/s][A
 51%|█████     | 278/543 [00:06<00:05, 45.15it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 45.26it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 45.17it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 45.15it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 45.10it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 45.03it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 45.06it/s][A
 58%|█████▊    | 313/543 [00:07<00:05, 45.23it/s][A
 59%|█████▊    | 318/543 [00:07<00:04, 45.16it/s][A
 59%|█████▉    | 323/543 [00:07<00:04, 45.34it/s][A
 60%|██████    | 328/543 [00:07<00:04, 45.29it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 45.01it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 45.06it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 45.10it/s][A
 64%|██████▍   | 348/543 [00:08<00:04, 45.05it/s][A
 65%|██████▌   | 353/543 [00:08<00:11, 16.24it/s][A
 66%|██████▌   | 358/543 [00:08<00:09, 20.13it/s][A
 67%|██████▋   | 363/543 [00:08<00:07, 24.17it/s][A
 68%|██████▊   | 368/543 [00:08<00:06, 28.09it/s][A
 69%|██████▊   | 373/543 [00:09<00:05, 31.78it/s][A
 70%|██████▉   | 378/543 [00:09<00:04, 34.91it/s][A
 71%|███████   | 383/543 [00:09<00:04, 37.51it/s][A
 71%|███████▏  | 388/543 [00:09<00:03, 39.56it/s][A
 72%|███████▏  | 393/543 [00:09<00:03, 40.94it/s][A
 73%|███████▎  | 398/543 [00:09<00:03, 42.09it/s][A
 74%|███████▍  | 403/543 [00:09<00:03, 43.03it/s][A
 75%|███████▌  | 408/543 [00:09<00:03, 43.76it/s][A
 76%|███████▌  | 413/543 [00:09<00:02, 44.28it/s][A
 77%|███████▋  | 418/543 [00:09<00:02, 44.66it/s][A
 78%|███████▊  | 423/543 [00:10<00:02, 44.76it/s][A
 79%|███████▉  | 428/543 [00:10<00:02, 44.86it/s][A
 80%|███████▉  | 433/543 [00:10<00:02, 44.95it/s][A
 81%|████████  | 438/543 [00:10<00:02, 44.87it/s][A
 82%|████████▏ | 443/543 [00:10<00:02, 44.85it/s][A
 83%|████████▎ | 448/543 [00:10<00:02, 44.96it/s][A
 83%|████████▎ | 453/543 [00:10<00:01, 45.15it/s][A
 84%|████████▍ | 458/543 [00:10<00:01, 45.30it/s][A
 85%|████████▌ | 463/543 [00:11<00:02, 37.78it/s][A
 86%|████████▌ | 468/543 [00:11<00:01, 39.81it/s][A
 87%|████████▋ | 473/543 [00:11<00:01, 41.38it/s][A
 88%|████████▊ | 478/543 [00:11<00:01, 42.62it/s][A
 89%|████████▉ | 483/543 [00:11<00:01, 43.47it/s][A
 90%|████████▉ | 488/543 [00:11<00:01, 44.10it/s][A
 91%|█████████ | 493/543 [00:11<00:01, 44.61it/s][A
 92%|█████████▏| 498/543 [00:11<00:01, 44.84it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 44.65it/s][A
 94%|█████████▎| 508/543 [00:12<00:00, 44.42it/s][A
 94%|█████████▍| 513/543 [00:12<00:00, 44.50it/s][A
 95%|█████████▌| 518/543 [00:12<00:00, 44.84it/s][A
 96%|█████████▋| 523/543 [00:12<00:00, 44.99it/s][A
 97%|█████████▋| 528/543 [00:12<00:00, 45.18it/s][A
 98%|█████████▊| 533/543 [00:12<00:00, 45.24it/s][A
 99%|█████████▉| 538/543 [00:12<00:00, 45.45it/s][A
100%|██████████| 543/543 [00:12<00:00, 45.39it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 45.39it/s][A100%|██████████| 235/235 [06:01<00:00,  2.69it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:02:00,106 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235
[INFO|configuration_utils.py:351] 2023-08-28 21:02:00,547 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:02:21,101 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:02:22,241 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:02:22,517 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:03:00,958 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:03:01,017 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94 (score: 1.065231442451477).
                                                 100%|██████████| 235/235 [07:36<00:00,  2.69it/s]100%|██████████| 235/235 [07:36<00:00,  1.94s/it]
[INFO|trainer.py:1894] 2023-08-28 21:03:35,331 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 21:03:35,695 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:03:48,606 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:03:49,573 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:03:50,055 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:03:51,041 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:51,042 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:51,042 >>   train_loss               =      0.451
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:51,042 >>   train_runtime            = 0:07:36.57
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:51,042 >>   train_samples            =       3013
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:51,042 >>   train_samples_per_second =     32.996
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:03:51,042 >>   train_steps_per_second   =      0.515
{'eval_loss': 1.0946050882339478, 'eval_runtime': 12.8492, 'eval_samples_per_second': 337.919, 'eval_steps_per_second': 42.259, 'epoch': 4.99}
{'train_runtime': 456.5738, 'train_samples_per_second': 32.996, 'train_steps_per_second': 0.515, 'train_loss': 0.45102795540018287, 'epoch': 4.99}
08/28/2023 21:03:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:03:52,584 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:03:52,584 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 21:03:52,584 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 55.62it/s]  2%|▏         | 12/543 [00:00<00:10, 49.77it/s]  3%|▎         | 18/543 [00:00<00:10, 47.97it/s]  4%|▍         | 23/543 [00:00<00:11, 47.23it/s]  5%|▌         | 28/543 [00:00<00:11, 46.81it/s]  6%|▌         | 33/543 [00:00<00:10, 46.57it/s]  7%|▋         | 38/543 [00:00<00:10, 46.41it/s]  8%|▊         | 43/543 [00:00<00:10, 45.99it/s]  9%|▉         | 48/543 [00:01<00:10, 45.46it/s] 10%|▉         | 53/543 [00:01<00:10, 45.45it/s] 11%|█         | 58/543 [00:01<00:10, 45.53it/s] 12%|█▏        | 63/543 [00:01<00:10, 45.67it/s] 13%|█▎        | 68/543 [00:01<00:10, 45.75it/s] 13%|█▎        | 73/543 [00:01<00:10, 45.71it/s] 14%|█▍        | 78/543 [00:01<00:10, 45.86it/s] 15%|█▌        | 83/543 [00:01<00:10, 45.81it/s] 16%|█▌        | 88/543 [00:01<00:09, 45.57it/s] 17%|█▋        | 93/543 [00:02<00:09, 45.44it/s] 18%|█▊        | 98/543 [00:02<00:09, 45.32it/s] 19%|█▉        | 103/543 [00:02<00:09, 45.39it/s] 20%|█▉        | 108/543 [00:02<00:09, 45.53it/s] 21%|██        | 113/543 [00:02<00:09, 45.64it/s] 22%|██▏       | 118/543 [00:02<00:09, 45.74it/s] 23%|██▎       | 123/543 [00:02<00:09, 45.77it/s] 24%|██▎       | 128/543 [00:02<00:09, 45.78it/s] 24%|██▍       | 133/543 [00:02<00:08, 45.76it/s] 25%|██▌       | 138/543 [00:03<00:16, 23.97it/s] 26%|██▋       | 143/543 [00:03<00:14, 28.05it/s] 27%|██▋       | 148/543 [00:03<00:12, 31.76it/s] 28%|██▊       | 153/543 [00:03<00:11, 35.07it/s] 29%|██▉       | 158/543 [00:03<00:10, 37.72it/s] 30%|███       | 163/543 [00:03<00:09, 39.86it/s] 31%|███       | 168/543 [00:03<00:09, 41.43it/s] 32%|███▏      | 173/543 [00:04<00:08, 42.71it/s] 33%|███▎      | 178/543 [00:04<00:08, 43.26it/s] 34%|███▎      | 183/543 [00:04<00:08, 43.58it/s] 35%|███▍      | 188/543 [00:04<00:08, 43.98it/s] 36%|███▌      | 193/543 [00:04<00:07, 44.51it/s] 36%|███▋      | 198/543 [00:04<00:07, 44.86it/s] 37%|███▋      | 203/543 [00:04<00:07, 45.20it/s] 38%|███▊      | 208/543 [00:04<00:07, 45.31it/s] 39%|███▉      | 213/543 [00:04<00:07, 45.39it/s] 40%|████      | 218/543 [00:05<00:08, 39.68it/s] 41%|████▏     | 224/543 [00:05<00:07, 42.79it/s] 42%|████▏     | 229/543 [00:05<00:07, 43.49it/s] 43%|████▎     | 234/543 [00:05<00:06, 44.16it/s] 44%|████▍     | 239/543 [00:05<00:06, 44.52it/s] 45%|████▍     | 244/543 [00:05<00:06, 44.93it/s] 46%|████▌     | 249/543 [00:05<00:06, 45.15it/s] 47%|████▋     | 254/543 [00:05<00:06, 45.24it/s] 48%|████▊     | 259/543 [00:06<00:06, 42.53it/s] 49%|████▊     | 264/543 [00:06<00:06, 43.38it/s] 50%|████▉     | 269/543 [00:06<00:06, 44.02it/s] 50%|█████     | 274/543 [00:06<00:06, 44.47it/s] 51%|█████▏    | 279/543 [00:06<00:05, 44.84it/s] 52%|█████▏    | 284/543 [00:06<00:05, 45.06it/s] 53%|█████▎    | 289/543 [00:07<00:23, 10.87it/s] 54%|█████▍    | 294/543 [00:07<00:17, 14.11it/s] 55%|█████▌    | 299/543 [00:08<00:13, 17.82it/s] 56%|█████▌    | 304/543 [00:08<00:10, 21.82it/s] 57%|█████▋    | 309/543 [00:08<00:09, 25.91it/s] 58%|█████▊    | 314/543 [00:08<00:07, 29.79it/s] 59%|█████▊    | 319/543 [00:08<00:06, 33.27it/s] 60%|█████▉    | 324/543 [00:08<00:06, 36.29it/s] 61%|██████    | 329/543 [00:08<00:05, 38.39it/s] 62%|██████▏   | 334/543 [00:08<00:05, 39.92it/s] 62%|██████▏   | 339/543 [00:08<00:04, 41.27it/s] 63%|██████▎   | 344/543 [00:09<00:10, 19.23it/s] 64%|██████▍   | 349/543 [00:09<00:08, 23.28it/s] 65%|██████▌   | 354/543 [00:09<00:06, 27.32it/s] 66%|██████▌   | 359/543 [00:09<00:05, 31.10it/s] 67%|██████▋   | 364/543 [00:09<00:05, 34.42it/s] 68%|██████▊   | 369/543 [00:10<00:04, 37.26it/s] 69%|██████▉   | 374/543 [00:10<00:04, 39.44it/s] 70%|██████▉   | 379/543 [00:10<00:03, 41.06it/s] 71%|███████   | 384/543 [00:10<00:03, 41.90it/s] 72%|███████▏  | 389/543 [00:10<00:03, 42.66it/s] 73%|███████▎  | 394/543 [00:10<00:03, 43.27it/s] 73%|███████▎  | 399/543 [00:10<00:03, 44.00it/s] 74%|███████▍  | 404/543 [00:10<00:03, 44.52it/s] 75%|███████▌  | 409/543 [00:10<00:02, 44.94it/s] 76%|███████▌  | 414/543 [00:11<00:02, 45.18it/s] 77%|███████▋  | 419/543 [00:11<00:02, 45.26it/s] 78%|███████▊  | 424/543 [00:11<00:02, 45.20it/s] 79%|███████▉  | 429/543 [00:11<00:02, 45.00it/s] 80%|███████▉  | 434/543 [00:11<00:02, 44.87it/s] 81%|████████  | 439/543 [00:11<00:02, 44.83it/s] 82%|████████▏ | 444/543 [00:11<00:02, 45.00it/s] 83%|████████▎ | 449/543 [00:11<00:02, 45.15it/s] 84%|████████▎ | 454/543 [00:11<00:01, 45.34it/s] 85%|████████▍ | 459/543 [00:12<00:03, 25.00it/s] 85%|████████▌ | 464/543 [00:12<00:02, 28.99it/s] 86%|████████▋ | 469/543 [00:12<00:02, 32.55it/s] 87%|████████▋ | 474/543 [00:12<00:01, 35.68it/s] 88%|████████▊ | 479/543 [00:12<00:01, 38.22it/s] 89%|████████▉ | 484/543 [00:12<00:01, 40.21it/s] 90%|█████████ | 489/543 [00:13<00:01, 41.62it/s] 91%|█████████ | 494/543 [00:13<00:01, 42.76it/s] 92%|█████████▏| 499/543 [00:13<00:01, 43.16it/s] 93%|█████████▎| 504/543 [00:13<00:00, 43.44it/s] 94%|█████████▎| 509/543 [00:13<00:00, 43.88it/s] 95%|█████████▍| 514/543 [00:13<00:00, 44.32it/s] 96%|█████████▌| 519/543 [00:13<00:00, 44.59it/s] 97%|█████████▋| 524/543 [00:13<00:00, 44.93it/s] 97%|█████████▋| 529/543 [00:13<00:00, 45.14it/s] 98%|█████████▊| 534/543 [00:14<00:00, 45.24it/s] 99%|█████████▉| 539/543 [00:14<00:00, 45.34it/s]100%|██████████| 543/543 [00:14<00:00, 38.12it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:04:06,847 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:06,847 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:06,847 >>   eval_loss               =     1.0652
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:06,847 >>   eval_runtime            = 0:00:14.26
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:06,847 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:06,847 >>   eval_samples_per_second =    304.427
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:06,847 >>   eval_steps_per_second   =     38.071
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:04:06,848 >>   perplexity              =     2.9015
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:32,778 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:32,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:32,919 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:32,919 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:32,919 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:04:34,033 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:04:34,034 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:04:34,671 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:04:35,783 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:04:35,864 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,245 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,271 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,272 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,272 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:04:39,272 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:04:40,453 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:04:40,454 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:04:41,322 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:04:41,578 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:04:41,578 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-94
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-235
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-141
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-188
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/checkpoint-47
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.71it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.75it/s]Extractor Predicting: 10it [00:05,  1.76it/s]Extractor Predicting: 11it [00:06,  1.77it/s]Extractor Predicting: 12it [00:06,  1.72it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:08,  1.59it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:10,  1.52it/s]Extractor Predicting: 19it [00:11,  1.49it/s]Extractor Predicting: 20it [00:12,  1.48it/s]Extractor Predicting: 21it [00:12,  1.51it/s]Extractor Predicting: 22it [00:13,  1.52it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:14,  1.54it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:16,  1.54it/s]Extractor Predicting: 28it [00:17,  1.50it/s]Extractor Predicting: 29it [00:18,  1.50it/s]Extractor Predicting: 30it [00:18,  1.52it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:20,  1.47it/s]Extractor Predicting: 34it [00:21,  1.49it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:22,  1.48it/s]Extractor Predicting: 37it [00:23,  1.49it/s]Extractor Predicting: 38it [00:24,  1.40it/s]Extractor Predicting: 39it [00:24,  1.45it/s]Extractor Predicting: 40it [00:25,  1.49it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:26,  1.48it/s]Extractor Predicting: 43it [00:27,  1.30it/s]Extractor Predicting: 44it [00:28,  1.38it/s]Extractor Predicting: 45it [00:29,  1.42it/s]Extractor Predicting: 46it [00:29,  1.47it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.27it/s]Extractor Predicting: 49it [00:32,  1.33it/s]Extractor Predicting: 50it [00:32,  1.40it/s]Extractor Predicting: 51it [00:33,  1.44it/s]Extractor Predicting: 52it [00:34,  1.36it/s]Extractor Predicting: 53it [00:35,  1.39it/s]Extractor Predicting: 54it [00:35,  1.45it/s]Extractor Predicting: 55it [00:36,  1.50it/s]Extractor Predicting: 56it [00:36,  1.49it/s]Extractor Predicting: 57it [00:37,  1.40it/s]Extractor Predicting: 58it [00:38,  1.47it/s]Extractor Predicting: 59it [00:39,  1.46it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:41,  1.46it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:42,  1.50it/s]Extractor Predicting: 65it [00:42,  1.56it/s]Extractor Predicting: 66it [00:43,  1.57it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:45,  1.38it/s]Extractor Predicting: 69it [00:45,  1.43it/s]Extractor Predicting: 70it [00:46,  1.46it/s]Extractor Predicting: 71it [00:47,  1.49it/s]Extractor Predicting: 72it [00:47,  1.39it/s]Extractor Predicting: 73it [00:48,  1.35it/s]Extractor Predicting: 74it [00:49,  1.38it/s]Extractor Predicting: 75it [00:50,  1.39it/s]Extractor Predicting: 76it [00:50,  1.43it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:52,  1.43it/s]Extractor Predicting: 79it [00:52,  1.45it/s]Extractor Predicting: 80it [00:53,  1.49it/s]Extractor Predicting: 81it [00:54,  1.51it/s]Extractor Predicting: 82it [00:54,  1.53it/s]Extractor Predicting: 83it [00:55,  1.48it/s]Extractor Predicting: 84it [00:56,  1.52it/s]Extractor Predicting: 85it [00:56,  1.54it/s]Extractor Predicting: 86it [00:57,  1.53it/s]Extractor Predicting: 87it [00:57,  1.54it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.53it/s]Extractor Predicting: 90it [00:59,  1.52it/s]Extractor Predicting: 91it [01:00,  1.51it/s]Extractor Predicting: 92it [01:01,  1.50it/s]Extractor Predicting: 93it [01:02,  1.48it/s]Extractor Predicting: 94it [01:02,  1.52it/s]Extractor Predicting: 95it [01:03,  1.53it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:04,  1.58it/s]Extractor Predicting: 98it [01:05,  1.44it/s]Extractor Predicting: 99it [01:06,  1.47it/s]Extractor Predicting: 100it [01:06,  1.50it/s]Extractor Predicting: 101it [01:07,  1.53it/s]Extractor Predicting: 102it [01:07,  1.53it/s]Extractor Predicting: 103it [01:09,  1.27it/s]Extractor Predicting: 104it [01:09,  1.36it/s]Extractor Predicting: 105it [01:10,  1.40it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:12,  1.49it/s]Extractor Predicting: 109it [01:12,  1.52it/s]Extractor Predicting: 110it [01:13,  1.52it/s]Extractor Predicting: 111it [01:14,  1.54it/s]Extractor Predicting: 112it [01:14,  1.58it/s]Extractor Predicting: 113it [01:15,  1.52it/s]Extractor Predicting: 114it [01:16,  1.53it/s]Extractor Predicting: 115it [01:16,  1.53it/s]Extractor Predicting: 116it [01:17,  1.54it/s]Extractor Predicting: 117it [01:18,  1.56it/s]Extractor Predicting: 118it [01:18,  1.53it/s]Extractor Predicting: 119it [01:19,  1.51it/s]Extractor Predicting: 120it [01:20,  1.51it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.50it/s]Extractor Predicting: 123it [01:21,  1.54it/s]Extractor Predicting: 124it [01:22,  1.52it/s]Extractor Predicting: 125it [01:23,  1.55it/s]Extractor Predicting: 126it [01:23,  1.51it/s]Extractor Predicting: 127it [01:24,  1.51it/s]Extractor Predicting: 128it [01:25,  1.53it/s]Extractor Predicting: 129it [01:26,  1.44it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:27,  1.38it/s]Extractor Predicting: 132it [01:28,  1.42it/s]Extractor Predicting: 133it [01:28,  1.46it/s]Extractor Predicting: 134it [01:29,  1.35it/s]Extractor Predicting: 135it [01:30,  1.42it/s]Extractor Predicting: 136it [01:30,  1.46it/s]Extractor Predicting: 137it [01:31,  1.50it/s]Extractor Predicting: 138it [01:32,  1.50it/s]Extractor Predicting: 139it [01:32,  1.45it/s]Extractor Predicting: 140it [01:33,  1.50it/s]Extractor Predicting: 141it [01:34,  1.53it/s]Extractor Predicting: 142it [01:34,  1.56it/s]Extractor Predicting: 143it [01:35,  1.58it/s]Extractor Predicting: 144it [01:36,  1.47it/s]Extractor Predicting: 145it [01:36,  1.48it/s]Extractor Predicting: 146it [01:37,  1.52it/s]Extractor Predicting: 147it [01:38,  1.54it/s]Extractor Predicting: 148it [01:38,  1.56it/s]Extractor Predicting: 149it [01:39,  1.47it/s]Extractor Predicting: 150it [01:40,  1.46it/s]Extractor Predicting: 151it [01:40,  1.49it/s]Extractor Predicting: 152it [01:41,  1.51it/s]Extractor Predicting: 153it [01:42,  1.50it/s]Extractor Predicting: 154it [01:43,  1.28it/s]Extractor Predicting: 155it [01:43,  1.33it/s]Extractor Predicting: 156it [01:44,  1.38it/s]Extractor Predicting: 157it [01:45,  1.42it/s]Extractor Predicting: 158it [01:45,  1.45it/s]Extractor Predicting: 159it [01:46,  1.50it/s]Extractor Predicting: 160it [01:47,  1.54it/s]Extractor Predicting: 161it [01:47,  1.56it/s]Extractor Predicting: 162it [01:48,  1.57it/s]Extractor Predicting: 163it [01:49,  1.50it/s]Extractor Predicting: 163it [01:49,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:03,528 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:03,792 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:03,792 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:03,793 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:03,793 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:07:05,542 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:07:05,543 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:07:06,484 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:07:07,876 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:07:08,015 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:10,669 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:10,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:10,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:10,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:07:10,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:07:12,265 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:07:12,420 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:07:13,109 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:07:14,034 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:07:14,035 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.12019826517967781,
  "recall": 0.04467987102717642,
  "score": 0.06514439220953659,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:10,  1.50it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:13,  1.52it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.60it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:22,  1.63it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:24,  1.58it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:25,  1.53it/s]Extractor Predicting: 42it [00:26,  1.54it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:27,  1.51it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:31,  1.45it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.50it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.29it/s]Extractor Predicting: 55it [00:35,  1.37it/s]Extractor Predicting: 56it [00:36,  1.45it/s]Extractor Predicting: 57it [00:36,  1.46it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:38,  1.38it/s]Extractor Predicting: 60it [00:39,  1.41it/s]Extractor Predicting: 61it [00:39,  1.47it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.45it/s]Extractor Predicting: 64it [00:41,  1.45it/s]Extractor Predicting: 65it [00:42,  1.47it/s]Extractor Predicting: 66it [00:43,  1.46it/s]Extractor Predicting: 67it [00:43,  1.48it/s]Extractor Predicting: 68it [00:44,  1.47it/s]Extractor Predicting: 69it [00:45,  1.46it/s]Extractor Predicting: 70it [00:45,  1.48it/s]Extractor Predicting: 71it [00:46,  1.50it/s]Extractor Predicting: 72it [00:47,  1.50it/s]Extractor Predicting: 73it [00:47,  1.49it/s]Extractor Predicting: 74it [00:48,  1.47it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:49,  1.51it/s]Extractor Predicting: 77it [00:50,  1.54it/s]Extractor Predicting: 78it [00:51,  1.54it/s]Extractor Predicting: 79it [00:51,  1.43it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:53,  1.41it/s]Extractor Predicting: 82it [00:53,  1.46it/s]Extractor Predicting: 83it [00:54,  1.50it/s]Extractor Predicting: 84it [00:55,  1.39it/s]Extractor Predicting: 85it [00:56,  1.43it/s]Extractor Predicting: 86it [00:56,  1.47it/s]Extractor Predicting: 87it [00:57,  1.51it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:59,  1.53it/s]Extractor Predicting: 91it [00:59,  1.56it/s]Extractor Predicting: 92it [01:00,  1.57it/s]Extractor Predicting: 93it [01:01,  1.57it/s]Extractor Predicting: 94it [01:01,  1.54it/s]Extractor Predicting: 95it [01:02,  1.56it/s]Extractor Predicting: 96it [01:03,  1.60it/s]Extractor Predicting: 97it [01:03,  1.60it/s]Extractor Predicting: 98it [01:04,  1.54it/s]Extractor Predicting: 99it [01:05,  1.39it/s]Extractor Predicting: 100it [01:05,  1.43it/s]Extractor Predicting: 101it [01:06,  1.46it/s]Extractor Predicting: 102it [01:07,  1.49it/s]Extractor Predicting: 103it [01:07,  1.49it/s]Extractor Predicting: 104it [01:08,  1.42it/s]Extractor Predicting: 105it [01:09,  1.45it/s]Extractor Predicting: 106it [01:10,  1.44it/s]Extractor Predicting: 107it [01:10,  1.44it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:12,  1.39it/s]Extractor Predicting: 110it [01:12,  1.43it/s]Extractor Predicting: 111it [01:13,  1.30it/s]Extractor Predicting: 112it [01:14,  1.37it/s]Extractor Predicting: 113it [01:15,  1.20it/s]Extractor Predicting: 114it [01:16,  1.27it/s]Extractor Predicting: 115it [01:16,  1.33it/s]Extractor Predicting: 116it [01:17,  1.36it/s]Extractor Predicting: 117it [01:18,  1.34it/s]Extractor Predicting: 118it [01:18,  1.41it/s]Extractor Predicting: 119it [01:19,  1.44it/s]Extractor Predicting: 120it [01:20,  1.50it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.52it/s]Extractor Predicting: 123it [01:22,  1.55it/s]Extractor Predicting: 124it [01:22,  1.49it/s]Extractor Predicting: 125it [01:23,  1.51it/s]Extractor Predicting: 126it [01:24,  1.51it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:25,  1.51it/s]Extractor Predicting: 129it [01:26,  1.45it/s]Extractor Predicting: 130it [01:26,  1.47it/s]Extractor Predicting: 131it [01:27,  1.55it/s]Extractor Predicting: 132it [01:28,  1.54it/s]Extractor Predicting: 133it [01:28,  1.55it/s]Extractor Predicting: 134it [01:29,  1.48it/s]Extractor Predicting: 135it [01:30,  1.51it/s]Extractor Predicting: 136it [01:30,  1.53it/s]Extractor Predicting: 137it [01:31,  1.58it/s]Extractor Predicting: 138it [01:31,  1.58it/s]Extractor Predicting: 139it [01:32,  1.53it/s]Extractor Predicting: 140it [01:33,  1.54it/s]Extractor Predicting: 141it [01:33,  1.56it/s]Extractor Predicting: 142it [01:34,  1.55it/s]Extractor Predicting: 143it [01:35,  1.54it/s]Extractor Predicting: 144it [01:36,  1.26it/s]Extractor Predicting: 145it [01:37,  1.31it/s]Extractor Predicting: 146it [01:37,  1.33it/s]Extractor Predicting: 147it [01:38,  1.40it/s]Extractor Predicting: 148it [01:39,  1.35it/s]Extractor Predicting: 149it [01:39,  1.41it/s]Extractor Predicting: 150it [01:40,  1.44it/s]Extractor Predicting: 151it [01:41,  1.47it/s]Extractor Predicting: 152it [01:41,  1.49it/s]Extractor Predicting: 153it [01:42,  1.47it/s]Extractor Predicting: 154it [01:43,  1.51it/s]Extractor Predicting: 155it [01:43,  1.55it/s]Extractor Predicting: 156it [01:44,  1.52it/s]Extractor Predicting: 157it [01:45,  1.53it/s]Extractor Predicting: 158it [01:45,  1.49it/s]Extractor Predicting: 159it [01:46,  1.51it/s]Extractor Predicting: 160it [01:47,  1.54it/s]Extractor Predicting: 161it [01:47,  1.48it/s]Extractor Predicting: 162it [01:48,  1.53it/s]Extractor Predicting: 163it [01:49,  1.54it/s]Extractor Predicting: 164it [01:49,  1.53it/s]Extractor Predicting: 165it [01:50,  1.59it/s]Extractor Predicting: 166it [01:50,  1.61it/s]Extractor Predicting: 167it [01:51,  1.60it/s]Extractor Predicting: 168it [01:52,  1.61it/s]Extractor Predicting: 169it [01:52,  1.63it/s]Extractor Predicting: 170it [01:53,  1.60it/s]Extractor Predicting: 171it [01:53,  1.59it/s]Extractor Predicting: 172it [01:54,  1.60it/s]Extractor Predicting: 173it [01:55,  1.57it/s]Extractor Predicting: 174it [01:55,  1.61it/s]Extractor Predicting: 175it [01:56,  1.53it/s]Extractor Predicting: 176it [01:57,  1.53it/s]Extractor Predicting: 177it [01:57,  1.53it/s]Extractor Predicting: 178it [01:58,  1.54it/s]Extractor Predicting: 179it [01:59,  1.61it/s]Extractor Predicting: 180it [01:59,  1.44it/s]Extractor Predicting: 181it [02:00,  1.48it/s]Extractor Predicting: 182it [02:01,  1.50it/s]Extractor Predicting: 183it [02:01,  1.51it/s]Extractor Predicting: 184it [02:02,  1.52it/s]Extractor Predicting: 185it [02:03,  1.54it/s]Extractor Predicting: 186it [02:03,  1.53it/s]Extractor Predicting: 187it [02:04,  1.54it/s]Extractor Predicting: 188it [02:05,  1.53it/s]Extractor Predicting: 189it [02:05,  1.52it/s]Extractor Predicting: 190it [02:06,  1.31it/s]Extractor Predicting: 191it [02:07,  1.36it/s]Extractor Predicting: 192it [02:08,  1.43it/s]Extractor Predicting: 193it [02:08,  1.48it/s]Extractor Predicting: 194it [02:09,  1.52it/s]Extractor Predicting: 195it [02:10,  1.31it/s]Extractor Predicting: 196it [02:10,  1.39it/s]Extractor Predicting: 197it [02:11,  1.43it/s]Extractor Predicting: 198it [02:12,  1.48it/s]Extractor Predicting: 199it [02:12,  1.50it/s]Extractor Predicting: 200it [02:13,  1.42it/s]Extractor Predicting: 201it [02:14,  1.46it/s]Extractor Predicting: 202it [02:14,  1.49it/s]Extractor Predicting: 203it [02:15,  1.51it/s]Extractor Predicting: 204it [02:16,  1.37it/s]Extractor Predicting: 205it [02:17,  1.18it/s]Extractor Predicting: 206it [02:18,  1.26it/s]Extractor Predicting: 207it [02:18,  1.35it/s]Extractor Predicting: 208it [02:19,  1.40it/s]Extractor Predicting: 209it [02:20,  1.36it/s]Extractor Predicting: 210it [02:20,  1.41it/s]Extractor Predicting: 211it [02:21,  1.44it/s]Extractor Predicting: 212it [02:22,  1.48it/s]Extractor Predicting: 213it [02:22,  1.50it/s]Extractor Predicting: 214it [02:23,  1.39it/s]Extractor Predicting: 215it [02:24,  1.40it/s]Extractor Predicting: 216it [02:25,  1.45it/s]Extractor Predicting: 217it [02:25,  1.50it/s]Extractor Predicting: 218it [02:26,  1.52it/s]Extractor Predicting: 219it [02:26,  1.51it/s]Extractor Predicting: 220it [02:27,  1.49it/s]Extractor Predicting: 221it [02:28,  1.49it/s]Extractor Predicting: 222it [02:28,  1.52it/s]Extractor Predicting: 223it [02:29,  1.47it/s]Extractor Predicting: 224it [02:30,  1.50it/s]Extractor Predicting: 225it [02:31,  1.43it/s]Extractor Predicting: 226it [02:31,  1.46it/s]Extractor Predicting: 227it [02:32,  1.44it/s]Extractor Predicting: 228it [02:33,  1.41it/s]Extractor Predicting: 229it [02:33,  1.43it/s]Extractor Predicting: 230it [02:34,  1.44it/s]Extractor Predicting: 231it [02:35,  1.45it/s]Extractor Predicting: 232it [02:35,  1.47it/s]Extractor Predicting: 233it [02:36,  1.48it/s]Extractor Predicting: 234it [02:37,  1.49it/s]Extractor Predicting: 235it [02:37,  1.45it/s]Extractor Predicting: 236it [02:38,  1.48it/s]Extractor Predicting: 237it [02:39,  1.49it/s]Extractor Predicting: 238it [02:39,  1.55it/s]Extractor Predicting: 239it [02:40,  1.58it/s]Extractor Predicting: 240it [02:41,  1.41it/s]Extractor Predicting: 241it [02:41,  1.46it/s]Extractor Predicting: 242it [02:42,  1.50it/s]Extractor Predicting: 243it [02:43,  1.50it/s]Extractor Predicting: 244it [02:43,  1.52it/s]Extractor Predicting: 245it [02:44,  1.45it/s]Extractor Predicting: 246it [02:45,  1.47it/s]Extractor Predicting: 247it [02:45,  1.51it/s]Extractor Predicting: 248it [02:46,  1.55it/s]Extractor Predicting: 249it [02:47,  1.62it/s]Extractor Predicting: 250it [02:47,  1.55it/s]Extractor Predicting: 251it [02:48,  1.64it/s]Extractor Predicting: 252it [02:48,  1.68it/s]Extractor Predicting: 253it [02:49,  1.66it/s]Extractor Predicting: 254it [02:50,  1.69it/s]Extractor Predicting: 255it [02:50,  1.64it/s]Extractor Predicting: 256it [02:51,  1.66it/s]Extractor Predicting: 257it [02:51,  1.63it/s]Extractor Predicting: 258it [02:52,  1.60it/s]Extractor Predicting: 259it [02:53,  1.56it/s]Extractor Predicting: 260it [02:53,  1.63it/s]Extractor Predicting: 261it [02:54,  1.57it/s]Extractor Predicting: 262it [02:55,  1.59it/s]Extractor Predicting: 263it [02:55,  1.63it/s]Extractor Predicting: 264it [02:56,  1.65it/s]Extractor Predicting: 265it [02:56,  1.64it/s]Extractor Predicting: 266it [02:57,  1.67it/s]Extractor Predicting: 267it [02:58,  1.68it/s]Extractor Predicting: 268it [02:58,  1.67it/s]Extractor Predicting: 269it [02:59,  1.64it/s]Extractor Predicting: 270it [02:59,  1.66it/s]Extractor Predicting: 271it [03:00,  1.65it/s]Extractor Predicting: 272it [03:01,  1.54it/s]Extractor Predicting: 273it [03:01,  1.61it/s]Extractor Predicting: 274it [03:02,  1.63it/s]Extractor Predicting: 275it [03:03,  1.63it/s]Extractor Predicting: 276it [03:03,  1.63it/s]Extractor Predicting: 277it [03:04,  1.60it/s]Extractor Predicting: 278it [03:04,  1.64it/s]Extractor Predicting: 279it [03:05,  1.59it/s]Extractor Predicting: 280it [03:06,  1.66it/s]Extractor Predicting: 281it [03:06,  1.67it/s]Extractor Predicting: 282it [03:07,  1.63it/s]Extractor Predicting: 283it [03:07,  1.66it/s]Extractor Predicting: 284it [03:08,  1.67it/s]Extractor Predicting: 285it [03:09,  1.68it/s]Extractor Predicting: 286it [03:09,  1.68it/s]Extractor Predicting: 287it [03:10,  1.55it/s]Extractor Predicting: 288it [03:11,  1.55it/s]Extractor Predicting: 289it [03:11,  1.58it/s]Extractor Predicting: 290it [03:12,  1.54it/s]Extractor Predicting: 291it [03:13,  1.57it/s]Extractor Predicting: 292it [03:13,  1.53it/s]Extractor Predicting: 293it [03:14,  1.57it/s]Extractor Predicting: 294it [03:14,  1.60it/s]Extractor Predicting: 295it [03:15,  1.62it/s]Extractor Predicting: 296it [03:16,  1.56it/s]Extractor Predicting: 297it [03:16,  1.52it/s]Extractor Predicting: 298it [03:17,  1.49it/s]Extractor Predicting: 299it [03:18,  1.50it/s]Extractor Predicting: 300it [03:18,  1.53it/s]Extractor Predicting: 301it [03:19,  1.54it/s]Extractor Predicting: 302it [03:20,  1.54it/s]Extractor Predicting: 303it [03:20,  1.52it/s]Extractor Predicting: 304it [03:21,  1.53it/s]Extractor Predicting: 305it [03:22,  1.56it/s]Extractor Predicting: 306it [03:22,  1.53it/s]Extractor Predicting: 307it [03:23,  1.48it/s]Extractor Predicting: 308it [03:24,  1.49it/s]Extractor Predicting: 309it [03:24,  1.53it/s]Extractor Predicting: 310it [03:25,  1.55it/s]Extractor Predicting: 311it [03:26,  1.54it/s]Extractor Predicting: 312it [03:26,  1.53it/s]Extractor Predicting: 313it [03:27,  1.51it/s]Extractor Predicting: 314it [03:28,  1.49it/s]Extractor Predicting: 315it [03:29,  1.27it/s]Extractor Predicting: 316it [03:29,  1.32it/s]Extractor Predicting: 317it [03:30,  1.37it/s]Extractor Predicting: 318it [03:31,  1.42it/s]Extractor Predicting: 319it [03:31,  1.36it/s]Extractor Predicting: 320it [03:32,  1.41it/s]Extractor Predicting: 321it [03:33,  1.49it/s]Extractor Predicting: 322it [03:33,  1.51it/s]Extractor Predicting: 323it [03:34,  1.51it/s]Extractor Predicting: 324it [03:35,  1.46it/s]Extractor Predicting: 325it [03:35,  1.48it/s]Extractor Predicting: 326it [03:36,  1.48it/s]Extractor Predicting: 327it [03:37,  1.47it/s]Extractor Predicting: 328it [03:37,  1.48it/s]Extractor Predicting: 329it [03:38,  1.37it/s]Extractor Predicting: 330it [03:39,  1.44it/s]Extractor Predicting: 331it [03:39,  1.64it/s]Extractor Predicting: 331it [03:39,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:20,997 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:21,161 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:21,162 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:21,162 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:21,162 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:11:22,834 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:11:22,835 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:11:23,492 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:11:24,802 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:11:24,933 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:26,896 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:26,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:26,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:26,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:26,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:11:28,041 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:11:28,099 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:11:28,445 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:11:28,785 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:11:28,785 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2643245503973233,
  "recall": 0.15937460597654773,
  "score": 0.1988515692598128,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.30it/s]Extractor Predicting: 7it [00:04,  1.36it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.43it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.32it/s]Extractor Predicting: 12it [00:08,  1.37it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.44it/s]Extractor Predicting: 16it [00:11,  1.47it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:13,  1.48it/s]Extractor Predicting: 20it [00:13,  1.44it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.43it/s]Extractor Predicting: 23it [00:16,  1.42it/s]Extractor Predicting: 24it [00:16,  1.44it/s]Extractor Predicting: 25it [00:17,  1.35it/s]Extractor Predicting: 26it [00:18,  1.41it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:19,  1.44it/s]Extractor Predicting: 29it [00:20,  1.34it/s]Extractor Predicting: 30it [00:21,  1.32it/s]Extractor Predicting: 31it [00:21,  1.35it/s]Extractor Predicting: 32it [00:22,  1.40it/s]Extractor Predicting: 33it [00:23,  1.43it/s]Extractor Predicting: 34it [00:23,  1.44it/s]Extractor Predicting: 35it [00:24,  1.45it/s]Extractor Predicting: 36it [00:25,  1.46it/s]Extractor Predicting: 37it [00:25,  1.47it/s]Extractor Predicting: 38it [00:26,  1.49it/s]Extractor Predicting: 39it [00:27,  1.44it/s]Extractor Predicting: 40it [00:28,  1.39it/s]Extractor Predicting: 41it [00:28,  1.42it/s]Extractor Predicting: 42it [00:29,  1.45it/s]Extractor Predicting: 43it [00:30,  1.46it/s]Extractor Predicting: 44it [00:30,  1.47it/s]Extractor Predicting: 45it [00:31,  1.42it/s]Extractor Predicting: 46it [00:32,  1.45it/s]Extractor Predicting: 47it [00:32,  1.45it/s]Extractor Predicting: 48it [00:33,  1.48it/s]Extractor Predicting: 49it [00:34,  1.50it/s]Extractor Predicting: 50it [00:34,  1.50it/s]Extractor Predicting: 51it [00:35,  1.52it/s]Extractor Predicting: 52it [00:36,  1.52it/s]Extractor Predicting: 53it [00:36,  1.45it/s]Extractor Predicting: 54it [00:37,  1.49it/s]Extractor Predicting: 55it [00:38,  1.44it/s]Extractor Predicting: 56it [00:38,  1.53it/s]Extractor Predicting: 57it [00:39,  1.60it/s]Extractor Predicting: 58it [00:39,  1.66it/s]Extractor Predicting: 59it [00:40,  1.75it/s]Extractor Predicting: 60it [00:40,  1.83it/s]Extractor Predicting: 61it [00:41,  1.86it/s]Extractor Predicting: 62it [00:41,  1.86it/s]Extractor Predicting: 63it [00:42,  1.89it/s]Extractor Predicting: 64it [00:43,  1.87it/s]Extractor Predicting: 65it [00:43,  1.87it/s]Extractor Predicting: 66it [00:44,  1.87it/s]Extractor Predicting: 67it [00:44,  1.81it/s]Extractor Predicting: 68it [00:45,  1.84it/s]Extractor Predicting: 69it [00:45,  1.89it/s]Extractor Predicting: 70it [00:46,  1.86it/s]Extractor Predicting: 71it [00:46,  1.86it/s]Extractor Predicting: 72it [00:47,  1.89it/s]Extractor Predicting: 73it [00:47,  1.86it/s]Extractor Predicting: 74it [00:48,  1.90it/s]Extractor Predicting: 75it [00:48,  1.90it/s]Extractor Predicting: 76it [00:49,  1.88it/s]Extractor Predicting: 77it [00:49,  1.95it/s]Extractor Predicting: 78it [00:50,  1.89it/s]Extractor Predicting: 79it [00:51,  1.81it/s]Extractor Predicting: 80it [00:51,  1.82it/s]Extractor Predicting: 81it [00:52,  1.83it/s]Extractor Predicting: 82it [00:52,  1.87it/s]Extractor Predicting: 83it [00:53,  1.88it/s]Extractor Predicting: 84it [00:53,  1.89it/s]Extractor Predicting: 85it [00:54,  1.89it/s]Extractor Predicting: 86it [00:54,  1.76it/s]Extractor Predicting: 87it [00:55,  1.71it/s]Extractor Predicting: 88it [00:56,  1.67it/s]Extractor Predicting: 89it [00:56,  1.65it/s]Extractor Predicting: 90it [00:57,  1.63it/s]Extractor Predicting: 91it [00:58,  1.61it/s]Extractor Predicting: 92it [00:58,  1.60it/s]Extractor Predicting: 93it [00:59,  1.61it/s]Extractor Predicting: 94it [00:59,  1.59it/s]Extractor Predicting: 95it [01:00,  1.55it/s]Extractor Predicting: 96it [01:01,  1.57it/s]Extractor Predicting: 97it [01:01,  1.60it/s]Extractor Predicting: 98it [01:02,  1.61it/s]Extractor Predicting: 99it [01:03,  1.59it/s]Extractor Predicting: 100it [01:04,  1.41it/s]Extractor Predicting: 101it [01:04,  1.47it/s]Extractor Predicting: 102it [01:05,  1.49it/s]Extractor Predicting: 103it [01:05,  1.48it/s]Extractor Predicting: 104it [01:06,  1.48it/s]Extractor Predicting: 105it [01:07,  1.38it/s]Extractor Predicting: 106it [01:08,  1.41it/s]Extractor Predicting: 107it [01:08,  1.41it/s]Extractor Predicting: 108it [01:09,  1.40it/s]Extractor Predicting: 108it [01:09,  1.55it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:12:49,365 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:12:49,366 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:12:49,534 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:12:49,536 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:12:49,635 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:13:18,806 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:13:18,806 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:13:19,865 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:13:19,865 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:13:20,337 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:20,489 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:20,489 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:20,489 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:20,489 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:20,489 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:13:20,489 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5177865612648221,
  "recall": 0.14688451065193017,
  "score": 0.22884951335163464,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:13:21,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:21,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:22,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:23,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:23,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:24,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:25,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:26,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:26,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:27,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:28,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:28,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:29,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:30,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:30,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:31,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:32,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:32,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:33,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:33,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:02, 13.05s/it][WARNING|generation_utils.py:914] 2023-08-28 21:13:34,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:35,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:35,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:36,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:36,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:37,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:38,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:38,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:39,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:40,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:40,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:41,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:41,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:43,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:43,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:44,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:44,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:45,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:46,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:46,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:47,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:47,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:56, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-28 21:13:48,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:49,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:49,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:50,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:51,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:51,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:52,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:53,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:53,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:54,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:54,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:55,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:56,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:56,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:57,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:57,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:58,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:58,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:59,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:00,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:00,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:38, 13.19s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:01,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:01,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:02,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:03,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:03,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:04,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:04,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:05,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:06,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:07,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:07,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:08,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:08,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:09,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:10,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:10,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:11,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:11,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:12,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:12,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:52<02:21, 12.90s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:13,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:14,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:14,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:15,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:16,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:16,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:17,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:17,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:18,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:18,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:19,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:20,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:20,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:21,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:21,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:22,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:22,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:23,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:24,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:24,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:25,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:04<02:07, 12.72s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:26,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:26,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:27,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:27,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:28,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:28,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:29,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:30,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:30,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:31,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:31,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:32,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:33,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:33,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:34,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:35,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:35,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:36,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:36,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:37,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:37,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:38,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:17<01:56, 12.91s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:39,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:39,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:40,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:41,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:41,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:42,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:42,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:43,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:44,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:44,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:45,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:45,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:46,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:47,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:47,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:48,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:48,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:49,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:50,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:50,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:30<01:41, 12.68s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:51,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:51,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:52,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:53,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:53,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:54,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:54,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:55,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:55,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:56,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:56,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:57,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:58,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:58,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:59,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:59,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:00,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:01,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:01,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:02,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:02,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:41<01:26, 12.38s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:03,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:03,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:04,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:04,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:05,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:06,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:06,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:07,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:07,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:08,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:08,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:09,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:10,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:10,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:11,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:12,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:12,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:13,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:13,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:14,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:14,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:54<01:14, 12.34s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:15,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:15,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:16,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:17,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:17,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:18,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:18,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:19,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:19,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:20,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:21,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:21,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:22,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:22,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:23,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:23,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:24,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:24,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:25,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:26,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:26,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:05<01:00, 12.16s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:27,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:27,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:28,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:29,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:29,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:30,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:31,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:31,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:32,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:32,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:33,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:33,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:34,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:35,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:35,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:36,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:37,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:37,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:38,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:38,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:39,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:39,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:19<00:49, 12.48s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:40,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:40,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:41,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:42,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:43,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:44,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:44,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:45,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:46,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:47,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:47,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:48,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:49,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:49,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:50,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:50,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:51,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:52,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:53,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:53,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:54,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:54,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:33<00:39, 13.24s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:55,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:56,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:56,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:57,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:58,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:58,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:59,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:00,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:00,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:01,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:01,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:02,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:02,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:03,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:04,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:04,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:05,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:05,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:06,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:07,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:07,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:47<00:26, 13.19s/it][WARNING|generation_utils.py:914] 2023-08-28 21:16:08,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:09,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:09,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:10,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:11,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:11,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:12,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:12,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:13,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:13,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:14,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:14,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:15,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:15,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:16,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:17,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:17,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:18,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:18,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:19,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:20,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:59<00:12, 12.89s/it][WARNING|generation_utils.py:914] 2023-08-28 21:16:20,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:21,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:21,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:22,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:23,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:24,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:24,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:25,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:26,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:26,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:27,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:27,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:28,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:29,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:29,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:30,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:31,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:31,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:32,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:32,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:33,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:12<00:00, 12.93s/it]Generating: 100%|██████████| 15/15 [03:12<00:00, 12.83s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:49,370 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:49,434 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:49,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:49,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:49,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:16:51,245 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:16:51,246 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:16:52,076 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:16:53,341 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:16:54,184 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:58,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:59,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:59,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:59,194 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:59,194 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:17:00,631 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:17:00,632 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:17:01,469 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:17:02,019 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:17:02,102 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : made from material .', 'success_rate': 0.94375, 'errors': {'not enough values to unpack (expected 2, got 1)', ''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8678977272727273, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9136904761904762, 'errors': {''}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : Liberal Party .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 9143
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9243, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.62it/s]Extractor Estimating: 2it [00:01,  1.58it/s]Extractor Estimating: 3it [00:01,  1.61it/s]Extractor Estimating: 4it [00:02,  1.63it/s]Extractor Estimating: 5it [00:03,  1.63it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.62it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:07,  1.22it/s]Extractor Estimating: 12it [00:08,  1.32it/s]Extractor Estimating: 13it [00:08,  1.36it/s]Extractor Estimating: 14it [00:09,  1.42it/s]Extractor Estimating: 15it [00:10,  1.21it/s]Extractor Estimating: 16it [00:11,  1.32it/s]Extractor Estimating: 17it [00:11,  1.39it/s]Extractor Estimating: 18it [00:12,  1.44it/s]Extractor Estimating: 19it [00:13,  1.45it/s]Extractor Estimating: 20it [00:14,  1.11it/s]Extractor Estimating: 21it [00:15,  1.25it/s]Extractor Estimating: 22it [00:15,  1.37it/s]Extractor Estimating: 23it [00:16,  1.43it/s]Extractor Estimating: 24it [00:16,  1.51it/s]Extractor Estimating: 25it [00:17,  1.42it/s]Extractor Estimating: 26it [00:18,  1.49it/s]Extractor Estimating: 27it [00:18,  1.53it/s]Extractor Estimating: 28it [00:19,  1.60it/s]Extractor Estimating: 29it [00:20,  1.58it/s]Extractor Estimating: 30it [00:20,  1.56it/s]Extractor Estimating: 31it [00:21,  1.64it/s]Extractor Estimating: 32it [00:21,  1.69it/s]Extractor Estimating: 33it [00:22,  1.69it/s]Extractor Estimating: 34it [00:23,  1.64it/s]Extractor Estimating: 35it [00:23,  1.56it/s]Extractor Estimating: 36it [00:24,  1.53it/s]Extractor Estimating: 37it [00:25,  1.54it/s]Extractor Estimating: 38it [00:25,  1.56it/s]Extractor Estimating: 39it [00:26,  1.54it/s]Extractor Estimating: 40it [00:27,  1.49it/s]Extractor Estimating: 41it [00:27,  1.51it/s]Extractor Estimating: 42it [00:28,  1.51it/s]Extractor Estimating: 43it [00:28,  1.56it/s]Extractor Estimating: 44it [00:29,  1.53it/s]Extractor Estimating: 45it [00:30,  1.50it/s]Extractor Estimating: 46it [00:30,  1.54it/s]Extractor Estimating: 47it [00:31,  1.55it/s]Extractor Estimating: 48it [00:32,  1.59it/s]Extractor Estimating: 49it [00:32,  1.65it/s]Extractor Estimating: 50it [00:33,  1.48it/s]Extractor Estimating: 51it [00:34,  1.57it/s]Extractor Estimating: 52it [00:34,  1.65it/s]Extractor Estimating: 53it [00:35,  1.61it/s]Extractor Estimating: 54it [00:35,  1.68it/s]Extractor Estimating: 55it [00:36,  1.68it/s]Extractor Estimating: 56it [00:37,  1.69it/s]Extractor Estimating: 57it [00:37,  1.74it/s]Extractor Estimating: 58it [00:38,  1.73it/s]Extractor Estimating: 59it [00:38,  1.71it/s]Extractor Estimating: 60it [00:39,  1.78it/s]Extractor Estimating: 61it [00:40,  1.34it/s]Extractor Estimating: 62it [00:40,  1.45it/s]Extractor Estimating: 63it [00:41,  1.52it/s]Extractor Estimating: 64it [00:42,  1.60it/s]Extractor Estimating: 65it [00:42,  1.65it/s]Extractor Estimating: 66it [00:43,  1.60it/s]Extractor Estimating: 67it [00:43,  1.67it/s]Extractor Estimating: 68it [00:44,  1.72it/s]Extractor Estimating: 69it [00:45,  1.58it/s]Extractor Estimating: 70it [00:45,  1.63it/s]Extractor Estimating: 71it [00:46,  1.64it/s]Extractor Estimating: 72it [00:46,  1.63it/s]Extractor Estimating: 73it [00:47,  1.68it/s]Extractor Estimating: 74it [00:48,  1.57it/s]Extractor Estimating: 75it [00:48,  1.58it/s]Extractor Estimating: 76it [00:49,  1.45it/s]Extractor Estimating: 77it [00:50,  1.44it/s]Extractor Estimating: 78it [00:51,  1.49it/s]Extractor Estimating: 79it [00:51,  1.38it/s]Extractor Estimating: 80it [00:52,  1.46it/s]Extractor Estimating: 81it [00:53,  1.49it/s]Extractor Estimating: 82it [00:53,  1.50it/s]Extractor Estimating: 83it [00:54,  1.56it/s]Extractor Estimating: 84it [00:55,  1.26it/s]Extractor Estimating: 85it [00:55,  1.42it/s]Extractor Estimating: 86it [00:56,  1.48it/s]Extractor Estimating: 87it [00:57,  1.58it/s]Extractor Estimating: 88it [00:57,  1.56it/s]Extractor Estimating: 89it [00:58,  1.43it/s]Extractor Estimating: 90it [00:59,  1.49it/s]Extractor Estimating: 91it [00:59,  1.55it/s]Extractor Estimating: 92it [01:00,  1.56it/s]Extractor Estimating: 93it [01:00,  1.64it/s]Extractor Estimating: 94it [01:01,  1.57it/s]Extractor Estimating: 95it [01:02,  1.61it/s]Extractor Estimating: 96it [01:02,  1.59it/s]Extractor Estimating: 97it [01:03,  1.64it/s]Extractor Estimating: 98it [01:04,  1.68it/s]Extractor Estimating: 99it [01:05,  1.21it/s]Extractor Estimating: 100it [01:06,  1.30it/s]Extractor Estimating: 101it [01:06,  1.40it/s]Extractor Estimating: 102it [01:07,  1.44it/s]Extractor Estimating: 103it [01:08,  1.30it/s]Extractor Estimating: 104it [01:08,  1.39it/s]Extractor Estimating: 105it [01:09,  1.53it/s]Extractor Estimating: 106it [01:09,  1.59it/s]Extractor Estimating: 107it [01:10,  1.64it/s]Extractor Estimating: 108it [01:11,  1.65it/s]Extractor Estimating: 109it [01:11,  1.71it/s]Extractor Estimating: 110it [01:12,  1.73it/s]Extractor Estimating: 111it [01:12,  1.70it/s]Extractor Estimating: 112it [01:13,  1.70it/s]Extractor Estimating: 113it [01:13,  1.70it/s]Extractor Estimating: 114it [01:14,  1.50it/s]Extractor Estimating: 115it [01:15,  1.35it/s]Extractor Estimating: 116it [01:16,  1.45it/s]Extractor Estimating: 117it [01:16,  1.47it/s]Extractor Estimating: 118it [01:17,  1.55it/s]Extractor Estimating: 119it [01:18,  1.62it/s]Extractor Estimating: 120it [01:18,  1.60it/s]Extractor Estimating: 121it [01:19,  1.62it/s]Extractor Estimating: 122it [01:19,  1.69it/s]Extractor Estimating: 123it [01:20,  1.66it/s]Extractor Estimating: 124it [01:21,  1.63it/s]Extractor Estimating: 125it [01:21,  1.61it/s]Extractor Estimating: 126it [01:22,  1.60it/s]Extractor Estimating: 127it [01:22,  1.62it/s]Extractor Estimating: 128it [01:23,  1.63it/s]Extractor Estimating: 129it [01:24,  1.62it/s]Extractor Estimating: 130it [01:25,  1.42it/s]Extractor Estimating: 131it [01:25,  1.46it/s]Extractor Estimating: 132it [01:26,  1.53it/s]Extractor Estimating: 133it [01:26,  1.58it/s]Extractor Estimating: 134it [01:27,  1.64it/s]Extractor Estimating: 135it [01:28,  1.33it/s]Extractor Estimating: 136it [01:29,  1.40it/s]Extractor Estimating: 137it [01:29,  1.46it/s]Extractor Estimating: 138it [01:30,  1.40it/s]Extractor Estimating: 139it [01:31,  1.25it/s]Extractor Estimating: 140it [01:32,  1.35it/s]Extractor Estimating: 141it [01:32,  1.41it/s]Extractor Estimating: 142it [01:33,  1.49it/s]Extractor Estimating: 143it [01:34,  1.51it/s]Extractor Estimating: 144it [01:34,  1.50it/s]Extractor Estimating: 145it [01:35,  1.55it/s]Extractor Estimating: 146it [01:35,  1.55it/s]Extractor Estimating: 147it [01:36,  1.61it/s]Extractor Estimating: 148it [01:37,  1.61it/s]Extractor Estimating: 149it [01:38,  1.42it/s]Extractor Estimating: 150it [01:38,  1.50it/s]Extractor Estimating: 151it [01:39,  1.51it/s]Extractor Estimating: 152it [01:39,  1.55it/s]Extractor Estimating: 153it [01:40,  1.58it/s]Extractor Estimating: 154it [01:41,  1.49it/s]Extractor Estimating: 155it [01:41,  1.55it/s]Extractor Estimating: 156it [01:42,  1.61it/s]Extractor Estimating: 157it [01:43,  1.61it/s]Extractor Estimating: 158it [01:43,  1.57it/s]Extractor Estimating: 159it [01:44,  1.39it/s]Extractor Estimating: 160it [01:45,  1.47it/s]Extractor Estimating: 161it [01:45,  1.58it/s]Extractor Estimating: 162it [01:46,  1.59it/s]Extractor Estimating: 163it [01:46,  1.60it/s]Extractor Estimating: 164it [01:47,  1.61it/s]Extractor Estimating: 165it [01:48,  1.62it/s]Extractor Estimating: 166it [01:48,  1.61it/s]Extractor Estimating: 167it [01:49,  1.58it/s]Extractor Estimating: 168it [01:50,  1.62it/s]Extractor Estimating: 169it [01:50,  1.64it/s]Extractor Estimating: 170it [01:51,  1.69it/s]Extractor Estimating: 171it [01:51,  1.67it/s]Extractor Estimating: 172it [01:52,  1.42it/s]Extractor Estimating: 173it [01:53,  1.45it/s]Extractor Estimating: 174it [01:53,  1.55it/s]Extractor Estimating: 175it [01:54,  1.63it/s]Extractor Estimating: 176it [01:55,  1.65it/s]Extractor Estimating: 177it [01:55,  1.67it/s]Extractor Estimating: 178it [01:56,  1.64it/s]Extractor Estimating: 179it [01:56,  1.66it/s]Extractor Estimating: 180it [01:57,  1.68it/s]Extractor Estimating: 181it [01:58,  1.71it/s]Extractor Estimating: 182it [01:58,  1.69it/s]Extractor Estimating: 183it [01:59,  1.49it/s]Extractor Estimating: 184it [02:00,  1.53it/s]Extractor Estimating: 185it [02:00,  1.53it/s]Extractor Estimating: 186it [02:01,  1.57it/s]Extractor Estimating: 187it [02:01,  1.62it/s]Extractor Estimating: 188it [02:02,  1.45it/s]Extractor Estimating: 189it [02:03,  1.50it/s]Extractor Estimating: 190it [02:03,  1.55it/s]Extractor Estimating: 191it [02:04,  1.60it/s]Extractor Estimating: 192it [02:05,  1.62it/s]Extractor Estimating: 193it [02:06,  1.37it/s]Extractor Estimating: 194it [02:06,  1.45it/s]Extractor Estimating: 195it [02:07,  1.51it/s]Extractor Estimating: 196it [02:07,  1.55it/s]Extractor Estimating: 197it [02:08,  1.61it/s]Extractor Estimating: 198it [02:09,  1.60it/s]Extractor Estimating: 199it [02:09,  1.67it/s]Extractor Estimating: 200it [02:10,  1.65it/s]Extractor Estimating: 201it [02:10,  1.73it/s]Extractor Estimating: 202it [02:11,  1.79it/s]Extractor Estimating: 203it [02:11,  1.85it/s]Extractor Estimating: 204it [02:13,  1.36it/s]Extractor Estimating: 205it [02:13,  1.48it/s]Extractor Estimating: 206it [02:14,  1.59it/s]Extractor Estimating: 207it [02:14,  1.69it/s]Extractor Estimating: 208it [02:15,  1.77it/s]Extractor Estimating: 209it [02:15,  1.70it/s]Extractor Estimating: 210it [02:16,  1.76it/s]Extractor Estimating: 211it [02:17,  1.52it/s]Extractor Estimating: 212it [02:17,  1.70it/s]Extractor Estimating: 213it [02:18,  1.71it/s]Extractor Estimating: 214it [02:18,  1.78it/s]Extractor Estimating: 215it [02:19,  1.86it/s]Extractor Estimating: 216it [02:19,  1.86it/s]Extractor Estimating: 217it [02:20,  1.82it/s]Extractor Estimating: 218it [02:20,  1.84it/s]Extractor Estimating: 219it [02:21,  1.86it/s]Extractor Estimating: 220it [02:21,  1.90it/s]Extractor Estimating: 221it [02:22,  1.95it/s]Extractor Estimating: 222it [02:22,  1.90it/s]Extractor Estimating: 223it [02:23,  1.70it/s]Extractor Estimating: 224it [02:24,  1.78it/s]Extractor Estimating: 225it [02:24,  1.85it/s]Extractor Estimating: 226it [02:25,  1.71it/s]Extractor Estimating: 227it [02:25,  1.79it/s]Extractor Estimating: 228it [02:26,  1.66it/s]Extractor Estimating: 229it [02:27,  1.57it/s]Extractor Estimating: 230it [02:27,  1.62it/s]Extractor Estimating: 231it [02:28,  1.69it/s]Extractor Estimating: 232it [02:28,  1.78it/s]Extractor Estimating: 233it [02:29,  1.84it/s]Extractor Estimating: 234it [02:29,  1.87it/s]Extractor Estimating: 235it [02:30,  1.78it/s]Extractor Estimating: 236it [02:31,  1.70it/s]Extractor Estimating: 237it [02:31,  1.72it/s]Extractor Estimating: 238it [02:32,  1.75it/s]Extractor Estimating: 239it [02:32,  1.81it/s]Extractor Estimating: 240it [02:33,  1.82it/s]Extractor Estimating: 241it [02:34,  1.57it/s]Extractor Estimating: 242it [02:34,  1.62it/s]Extractor Estimating: 243it [02:35,  1.72it/s]Extractor Estimating: 244it [02:35,  1.76it/s]Extractor Estimating: 245it [02:36,  1.82it/s]Extractor Estimating: 246it [02:36,  1.87it/s]Extractor Estimating: 247it [02:37,  1.66it/s]Extractor Estimating: 248it [02:37,  1.72it/s]Extractor Estimating: 249it [02:38,  1.71it/s]Extractor Estimating: 250it [02:39,  1.77it/s]Extractor Estimating: 251it [02:39,  1.76it/s]Extractor Estimating: 252it [02:40,  1.73it/s]Extractor Estimating: 253it [02:41,  1.28it/s]Extractor Estimating: 254it [02:42,  1.37it/s]Extractor Estimating: 255it [02:42,  1.47it/s]Extractor Estimating: 256it [02:43,  1.57it/s]Extractor Estimating: 257it [02:43,  1.62it/s]Extractor Estimating: 258it [02:44,  1.59it/s]Extractor Estimating: 259it [02:45,  1.63it/s]Extractor Estimating: 260it [02:45,  1.66it/s]Extractor Estimating: 261it [02:46,  1.68it/s]Extractor Estimating: 262it [02:46,  1.67it/s]Extractor Estimating: 263it [02:47,  1.63it/s]Extractor Estimating: 264it [02:48,  1.66it/s]Extractor Estimating: 265it [02:48,  1.68it/s]Extractor Estimating: 266it [02:49,  1.63it/s]Extractor Estimating: 267it [02:49,  1.64it/s]Extractor Estimating: 268it [02:50,  1.66it/s]Extractor Estimating: 269it [02:51,  1.60it/s]Extractor Estimating: 270it [02:51,  1.65it/s]Extractor Estimating: 271it [02:52,  1.67it/s]Extractor Estimating: 272it [02:52,  1.66it/s]Extractor Estimating: 273it [02:53,  1.72it/s]Extractor Estimating: 274it [02:53,  1.76it/s]Extractor Estimating: 275it [02:54,  1.41it/s]Extractor Estimating: 276it [02:55,  1.47it/s]Extractor Estimating: 277it [02:56,  1.52it/s]Extractor Estimating: 278it [02:56,  1.58it/s]Extractor Estimating: 279it [02:57,  1.65it/s]Extractor Estimating: 280it [02:58,  1.17it/s]Extractor Estimating: 281it [02:59,  1.29it/s]Extractor Estimating: 282it [02:59,  1.40it/s]Extractor Estimating: 283it [03:00,  1.51it/s]Extractor Estimating: 284it [03:01,  1.27it/s]Extractor Estimating: 285it [03:02,  1.38it/s]Extractor Estimating: 286it [03:02,  1.45it/s]Extractor Estimating: 287it [03:03,  1.56it/s]Extractor Estimating: 288it [03:03,  1.58it/s]Extractor Estimating: 289it [03:04,  1.56it/s]Extractor Estimating: 290it [03:05,  1.55it/s]Extractor Estimating: 291it [03:05,  1.59it/s]Extractor Estimating: 292it [03:06,  1.61it/s]Extractor Estimating: 293it [03:06,  1.62it/s]Extractor Estimating: 294it [03:07,  1.37it/s]Extractor Estimating: 295it [03:08,  1.47it/s]Extractor Estimating: 296it [03:09,  1.56it/s]Extractor Estimating: 297it [03:09,  1.63it/s]Extractor Estimating: 298it [03:10,  1.67it/s]Extractor Estimating: 299it [03:10,  1.63it/s]Extractor Estimating: 300it [03:11,  1.63it/s]Extractor Estimating: 301it [03:12,  1.64it/s]Extractor Estimating: 302it [03:12,  1.65it/s]Extractor Estimating: 303it [03:13,  1.68it/s]Extractor Estimating: 304it [03:14,  1.52it/s]Extractor Estimating: 305it [03:14,  1.58it/s]Extractor Estimating: 306it [03:15,  1.58it/s]Extractor Estimating: 307it [03:15,  1.61it/s]Extractor Estimating: 308it [03:16,  1.62it/s]Extractor Estimating: 309it [03:17,  1.63it/s]Extractor Estimating: 310it [03:17,  1.65it/s]Extractor Estimating: 311it [03:19,  1.18it/s]Extractor Estimating: 312it [03:19,  1.31it/s]Extractor Estimating: 313it [03:20,  1.41it/s]Extractor Estimating: 314it [03:20,  1.49it/s]Extractor Estimating: 315it [03:21,  1.46it/s]Extractor Estimating: 316it [03:22,  1.54it/s]Extractor Estimating: 317it [03:22,  1.60it/s]Extractor Estimating: 318it [03:23,  1.65it/s]Extractor Estimating: 319it [03:23,  1.67it/s]Extractor Estimating: 320it [03:24,  1.63it/s]Extractor Estimating: 321it [03:24,  1.66it/s]Extractor Estimating: 322it [03:25,  1.49it/s]Extractor Estimating: 323it [03:26,  1.55it/s]Extractor Estimating: 324it [03:27,  1.53it/s]Extractor Estimating: 325it [03:27,  1.57it/s]Extractor Estimating: 326it [03:28,  1.65it/s]Extractor Estimating: 327it [03:28,  1.71it/s]Extractor Estimating: 328it [03:29,  1.80it/s]Extractor Estimating: 329it [03:29,  1.85it/s]Extractor Estimating: 330it [03:30,  1.89it/s]Extractor Estimating: 331it [03:30,  1.90it/s]Extractor Estimating: 332it [03:31,  1.88it/s]Extractor Estimating: 333it [03:31,  1.91it/s]Extractor Estimating: 334it [03:32,  1.94it/s]Extractor Estimating: 335it [03:32,  1.95it/s]Extractor Estimating: 336it [03:33,  1.99it/s]Extractor Estimating: 337it [03:33,  2.04it/s]Extractor Estimating: 338it [03:34,  2.04it/s]Extractor Estimating: 339it [03:34,  2.08it/s]Extractor Estimating: 340it [03:35,  2.02it/s]Extractor Estimating: 341it [03:35,  2.01it/s]Extractor Estimating: 342it [03:36,  2.01it/s]Extractor Estimating: 343it [03:36,  1.97it/s]Extractor Estimating: 344it [03:37,  1.80it/s]Extractor Estimating: 345it [03:37,  1.84it/s]Extractor Estimating: 346it [03:38,  1.93it/s]Extractor Estimating: 347it [03:38,  1.93it/s]Extractor Estimating: 348it [03:39,  1.93it/s]Extractor Estimating: 349it [03:39,  1.94it/s]Extractor Estimating: 350it [03:40,  1.74it/s]Extractor Estimating: 351it [03:41,  1.73it/s]Extractor Estimating: 352it [03:41,  1.73it/s]Extractor Estimating: 353it [03:42,  1.76it/s]Extractor Estimating: 354it [03:42,  1.73it/s]Extractor Estimating: 355it [03:43,  1.71it/s]Extractor Estimating: 356it [03:44,  1.73it/s]Extractor Estimating: 357it [03:44,  1.73it/s]Extractor Estimating: 358it [03:45,  1.73it/s]Extractor Estimating: 359it [03:45,  1.73it/s]Extractor Estimating: 360it [03:46,  1.64it/s]Extractor Estimating: 361it [03:47,  1.62it/s]Extractor Estimating: 362it [03:47,  1.63it/s]Extractor Estimating: 363it [03:48,  1.63it/s]Extractor Estimating: 364it [03:48,  1.66it/s]Extractor Estimating: 365it [03:49,  1.69it/s]Extractor Estimating: 366it [03:50,  1.64it/s]Extractor Estimating: 367it [03:50,  1.64it/s]Extractor Estimating: 368it [03:51,  1.68it/s]Extractor Estimating: 369it [03:51,  1.70it/s]Extractor Estimating: 370it [03:52,  1.77it/s]Extractor Estimating: 371it [03:53,  1.78it/s]Extractor Estimating: 372it [03:53,  1.67it/s]Extractor Estimating: 373it [03:54,  1.73it/s]Extractor Estimating: 374it [03:54,  1.75it/s]Extractor Estimating: 375it [03:55,  1.97it/s]Extractor Estimating: 375it [03:55,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:33,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:34,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:34,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:34,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:34,029 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:21:34,774 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:21:34,775 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:21:35,668 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:21:36,730 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:21:36,730 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:39,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:40,070 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:40,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:40,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:21:40,071 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:21:40,460 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:21:40,461 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:21:40,927 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:21:41,098 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:21:41,098 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 22:39:16,792 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 22:39:17,496 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 4514 mean pseudo reward: 0.9742815619831338
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 15903
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16003, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16003, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.968, loss:327.0473
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 11, avg_time 0.956, loss:291.3918
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 111, avg_time 0.955, loss:280.4271
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 22, avg_time 0.944, loss:277.2662
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 122, avg_time 0.961, loss:278.3256
>> valid entity prec:0.4707, rec:0.4791, f1:0.4748
>> valid relation prec:0.0547, rec:0.0191, f1:0.0284
>> valid relation with NER prec:0.0547, rec:0.0191, f1:0.0284
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 33, avg_time 2.422, loss:263.5877
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 133, avg_time 0.959, loss:270.1356
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 44, avg_time 0.963, loss:254.2759
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 144, avg_time 0.967, loss:263.1821
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 55, avg_time 0.957, loss:267.7448
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4364, rec:0.4228, f1:0.4295
>> valid relation prec:0.0604, rec:0.0141, f1:0.0228
>> valid relation with NER prec:0.0604, rec:0.0141, f1:0.0228
g_step 1100, step 155, avg_time 2.384, loss:258.4333
g_step 1200, step 66, avg_time 0.948, loss:246.4079
g_step 1300, step 166, avg_time 0.965, loss:243.6859
g_step 1400, step 77, avg_time 0.962, loss:231.3421
g_step 1500, step 177, avg_time 0.976, loss:250.6529
>> valid entity prec:0.4512, rec:0.4058, f1:0.4273
>> valid relation prec:0.0520, rec:0.0173, f1:0.0260
>> valid relation with NER prec:0.0520, rec:0.0173, f1:0.0260
g_step 1600, step 88, avg_time 2.390, loss:220.0948
g_step 1700, step 188, avg_time 0.971, loss:226.7139
g_step 1800, step 99, avg_time 0.958, loss:205.0518
g_step 1900, step 10, avg_time 0.974, loss:215.9591
g_step 2000, step 110, avg_time 0.961, loss:204.9490
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4598, rec:0.3699, f1:0.4100
>> valid relation prec:0.0466, rec:0.0182, f1:0.0262
>> valid relation with NER prec:0.0466, rec:0.0182, f1:0.0262
g_step 2100, step 21, avg_time 2.392, loss:211.6182
g_step 2200, step 121, avg_time 0.976, loss:188.8533
g_step 2300, step 32, avg_time 0.958, loss:177.8413
g_step 2400, step 132, avg_time 0.969, loss:182.4656
g_step 2500, step 43, avg_time 0.974, loss:194.6659
>> valid entity prec:0.4520, rec:0.3846, f1:0.4156
>> valid relation prec:0.0420, rec:0.0136, f1:0.0206
>> valid relation with NER prec:0.0420, rec:0.0136, f1:0.0206
g_step 2600, step 143, avg_time 2.388, loss:180.3100
g_step 2700, step 54, avg_time 0.968, loss:174.9624
g_step 2800, step 154, avg_time 0.974, loss:177.7018
g_step 2900, step 65, avg_time 0.952, loss:176.7937
g_step 3000, step 165, avg_time 0.980, loss:173.7593
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4400, rec:0.3611, f1:0.3967
>> valid relation prec:0.0455, rec:0.0168, f1:0.0246
>> valid relation with NER prec:0.0455, rec:0.0168, f1:0.0246
g_step 3100, step 76, avg_time 2.373, loss:161.2947
g_step 3200, step 176, avg_time 0.973, loss:167.8319
g_step 3300, step 87, avg_time 0.967, loss:144.2144
g_step 3400, step 187, avg_time 0.980, loss:169.7292
g_step 3500, step 98, avg_time 0.966, loss:144.5077
>> valid entity prec:0.4650, rec:0.3887, f1:0.4235
>> valid relation prec:0.0727, rec:0.0297, f1:0.0422
>> valid relation with NER prec:0.0727, rec:0.0297, f1:0.0422
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 9, avg_time 2.389, loss:150.8339
g_step 3700, step 109, avg_time 0.977, loss:134.9716
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 22:39:17 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 22:39:17 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_22-39-16_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 22:39:19 - WARNING - datasets.builder -   Using custom data configuration default-0bf7f683a5abd761
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-0bf7f683a5abd761/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 22:39:26,961 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:39:26,962 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:39:26,962 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:39:26,963 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:39:27,083 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:39:27,140 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:39:27,140 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:39:27,140 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:39:27,140 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:39:27,140 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:39:27,140 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 22:39:28,189 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:39:31,475 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 22:39:31,475 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-0bf7f683a5abd761/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.03ba/s] 40%|████      | 2/5 [00:00<00:00,  3.12ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.76ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.17ba/s]100%|██████████| 5/5 [00:01<00:00,  5.23ba/s]100%|██████████| 5/5 [00:01<00:00,  4.17ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.45ba/s] 40%|████      | 2/5 [00:00<00:00,  3.40ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.87ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.16ba/s]100%|██████████| 5/5 [00:01<00:00,  4.42ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.25ba/s] 40%|████      | 2/5 [00:00<00:00,  4.72ba/s] 80%|████████  | 4/5 [00:00<00:00,  7.27ba/s]100%|██████████| 5/5 [00:00<00:00,  7.23ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.86ba/s] 40%|████      | 2/5 [00:00<00:00,  3.48ba/s] 80%|████████  | 4/5 [00:00<00:00,  6.00ba/s]100%|██████████| 5/5 [00:00<00:00,  5.72ba/s]
[INFO|trainer.py:414] 2023-08-28 22:39:41,488 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 22:39:41,739 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 22:39:41,993 >>   Num examples = 4516
[INFO|trainer.py:1149] 2023-08-28 22:39:41,993 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 22:39:41,993 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 22:39:41,993 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 22:39:41,993 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 22:39:41,993 >>   Total optimization steps = 355
  0%|          | 0/355 [00:00<?, ?it/s]  0%|          | 1/355 [00:00<01:46,  3.33it/s]  1%|          | 2/355 [00:00<01:43,  3.41it/s]  1%|          | 3/355 [00:00<01:42,  3.44it/s]  1%|          | 4/355 [00:01<01:41,  3.45it/s]  1%|▏         | 5/355 [00:01<01:41,  3.46it/s]  2%|▏         | 6/355 [00:01<01:40,  3.46it/s]  2%|▏         | 7/355 [00:02<01:40,  3.46it/s]  2%|▏         | 8/355 [00:02<01:40,  3.46it/s]  3%|▎         | 9/355 [00:02<01:39,  3.46it/s]  3%|▎         | 10/355 [00:02<01:39,  3.47it/s]  3%|▎         | 11/355 [00:03<01:39,  3.46it/s]  3%|▎         | 12/355 [00:03<01:38,  3.47it/s]  4%|▎         | 13/355 [00:03<01:38,  3.47it/s]  4%|▍         | 14/355 [00:04<01:38,  3.47it/s]  4%|▍         | 15/355 [00:04<01:38,  3.47it/s]  5%|▍         | 16/355 [00:04<01:37,  3.46it/s]  5%|▍         | 17/355 [00:04<01:37,  3.47it/s]  5%|▌         | 18/355 [00:05<01:37,  3.46it/s]  5%|▌         | 19/355 [00:05<01:36,  3.46it/s]  6%|▌         | 20/355 [00:05<01:36,  3.46it/s]  6%|▌         | 21/355 [00:06<01:36,  3.46it/s]  6%|▌         | 22/355 [00:06<01:36,  3.46it/s]  6%|▋         | 23/355 [00:06<01:35,  3.46it/s]  7%|▋         | 24/355 [00:06<01:35,  3.47it/s]  7%|▋         | 25/355 [00:07<01:35,  3.46it/s]  7%|▋         | 26/355 [00:07<01:34,  3.46it/s]  8%|▊         | 27/355 [00:07<01:34,  3.46it/s]  8%|▊         | 28/355 [00:08<01:34,  3.46it/s]  8%|▊         | 29/355 [00:08<01:34,  3.46it/s]  8%|▊         | 30/355 [00:08<01:33,  3.46it/s]  9%|▊         | 31/355 [00:08<01:33,  3.46it/s]  9%|▉         | 32/355 [00:09<01:33,  3.46it/s]  9%|▉         | 33/355 [00:09<01:33,  3.46it/s] 10%|▉         | 34/355 [00:09<01:32,  3.46it/s] 10%|▉         | 35/355 [00:10<01:32,  3.46it/s] 10%|█         | 36/355 [00:10<01:32,  3.46it/s] 10%|█         | 37/355 [00:10<01:31,  3.46it/s] 11%|█         | 38/355 [00:10<01:31,  3.46it/s] 11%|█         | 39/355 [00:11<01:31,  3.46it/s] 11%|█▏        | 40/355 [00:11<01:31,  3.46it/s] 12%|█▏        | 41/355 [00:11<01:30,  3.46it/s] 12%|█▏        | 42/355 [00:12<01:30,  3.46it/s] 12%|█▏        | 43/355 [00:12<01:30,  3.46it/s] 12%|█▏        | 44/355 [00:12<01:29,  3.46it/s] 13%|█▎        | 45/355 [00:13<01:29,  3.46it/s] 13%|█▎        | 46/355 [00:13<01:29,  3.46it/s] 13%|█▎        | 47/355 [00:13<01:29,  3.46it/s] 14%|█▎        | 48/355 [00:13<01:28,  3.46it/s] 14%|█▍        | 49/355 [00:14<01:28,  3.45it/s] 14%|█▍        | 50/355 [00:14<01:28,  3.46it/s] 14%|█▍        | 51/355 [00:14<01:27,  3.46it/s] 15%|█▍        | 52/355 [00:15<01:27,  3.46it/s] 15%|█▍        | 53/355 [00:15<01:27,  3.46it/s] 15%|█▌        | 54/355 [00:15<01:41,  2.98it/s] 15%|█▌        | 55/355 [00:16<01:36,  3.11it/s] 16%|█▌        | 56/355 [00:16<01:33,  3.20it/s] 16%|█▌        | 57/355 [00:16<01:31,  3.27it/s] 16%|█▋        | 58/355 [00:16<01:29,  3.32it/s] 17%|█▋        | 59/355 [00:17<01:28,  3.36it/s] 17%|█▋        | 60/355 [00:17<01:27,  3.39it/s] 17%|█▋        | 61/355 [00:17<01:26,  3.40it/s] 17%|█▋        | 62/355 [00:18<01:25,  3.42it/s] 18%|█▊        | 63/355 [00:18<01:25,  3.43it/s] 18%|█▊        | 64/355 [00:18<01:24,  3.43it/s] 18%|█▊        | 65/355 [00:18<01:24,  3.44it/s] 19%|█▊        | 66/355 [00:19<01:23,  3.44it/s] 19%|█▉        | 67/355 [00:19<01:23,  3.44it/s] 19%|█▉        | 68/355 [00:19<01:23,  3.44it/s] 19%|█▉        | 69/355 [00:20<01:23,  3.45it/s] 20%|█▉        | 70/355 [00:20<01:22,  3.45it/s] 20%|██        | 71/355 [00:20<01:13,  3.85it/s][INFO|trainer.py:2140] 2023-08-28 22:40:02,734 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:40:02,734 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:40:02,734 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.51it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.77it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.68it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.71it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.40it/s][A
  6%|▌         | 33/543 [00:00<00:11, 46.16it/s][A
  7%|▋         | 38/543 [00:00<00:10, 46.01it/s][A
  8%|▊         | 43/543 [00:00<00:11, 45.40it/s][A
  9%|▉         | 48/543 [00:01<00:10, 45.19it/s][A
 10%|▉         | 53/543 [00:01<00:15, 31.44it/s][A
 10%|█         | 57/543 [00:01<00:16, 29.48it/s][A
 11%|█▏        | 62/543 [00:01<00:14, 33.60it/s][A
 12%|█▏        | 67/543 [00:01<00:13, 36.52it/s][A
 13%|█▎        | 72/543 [00:01<00:12, 38.84it/s][A
 14%|█▍        | 77/543 [00:01<00:11, 40.48it/s][A
 15%|█▌        | 82/543 [00:02<00:10, 41.96it/s][A
 16%|█▌        | 87/543 [00:02<00:10, 43.06it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.79it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.23it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.22it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.37it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.61it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.78it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.80it/s][A
 23%|██▎       | 127/543 [00:03<00:09, 44.98it/s][A
 24%|██▍       | 132/543 [00:03<00:09, 45.22it/s][A
 25%|██▌       | 137/543 [00:03<00:08, 45.29it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 45.27it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 45.05it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.89it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 45.00it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.99it/s][A
 31%|███       | 167/543 [00:03<00:08, 45.04it/s][A
 32%|███▏      | 172/543 [00:04<00:08, 45.17it/s][A
 33%|███▎      | 177/543 [00:04<00:09, 39.06it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 40.87it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 42.19it/s][A
 35%|███▌      | 192/543 [00:04<00:08, 43.11it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 43.66it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.25it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.68it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 45.01it/s][A
 40%|███▉      | 217/543 [00:05<00:07, 44.86it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.70it/s][A
 42%|████▏     | 227/543 [00:05<00:08, 39.41it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 41.11it/s][A
 44%|████▎     | 237/543 [00:05<00:07, 42.43it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.30it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.90it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.40it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.66it/s][A
 48%|████▊     | 262/543 [00:06<00:06, 44.93it/s][A
 49%|████▉     | 267/543 [00:06<00:09, 28.05it/s][A
 50%|█████     | 272/543 [00:06<00:08, 32.09it/s][A
 51%|█████     | 277/543 [00:06<00:07, 35.20it/s][A
 52%|█████▏    | 282/543 [00:06<00:06, 37.72it/s][A
 53%|█████▎    | 287/543 [00:06<00:06, 39.56it/s][A
 54%|█████▍    | 292/543 [00:06<00:06, 41.16it/s][A
 55%|█████▍    | 297/543 [00:07<00:05, 42.44it/s][A
 56%|█████▌    | 302/543 [00:07<00:06, 40.08it/s][A
 57%|█████▋    | 307/543 [00:07<00:05, 41.46it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 42.59it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 43.30it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.91it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.31it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.60it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.66it/s][A
 63%|██████▎   | 342/543 [00:08<00:04, 44.55it/s][A
 64%|██████▍   | 347/543 [00:08<00:04, 44.69it/s][A
 65%|██████▍   | 352/543 [00:08<00:04, 44.85it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.92it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 45.00it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 45.12it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 45.24it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 45.23it/s][A
 70%|███████   | 382/543 [00:08<00:03, 45.13it/s][A
 71%|███████▏  | 387/543 [00:09<00:03, 45.04it/s][A
 72%|███████▏  | 392/543 [00:09<00:03, 45.03it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 45.05it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.94it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 41.09it/s][A
 76%|███████▌  | 412/543 [00:09<00:03, 42.21it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.29it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.84it/s][A
 79%|███████▊  | 427/543 [00:10<00:02, 38.95it/s][A
 80%|███████▉  | 432/543 [00:10<00:02, 40.71it/s][A
 80%|████████  | 437/543 [00:10<00:02, 36.56it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 38.81it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 40.62it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 42.02it/s][A
 84%|████████▍ | 457/543 [00:10<00:02, 42.89it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.75it/s][A
 86%|████████▌ | 467/543 [00:11<00:01, 44.28it/s][A
 87%|████████▋ | 472/543 [00:11<00:01, 44.58it/s][A
 88%|████████▊ | 477/543 [00:11<00:01, 44.37it/s][A
 89%|████████▉ | 482/543 [00:11<00:01, 44.42it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.69it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.74it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.94it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 45.00it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 45.17it/s][A
 94%|█████████▍| 512/543 [00:12<00:00, 45.33it/s][A
 95%|█████████▌| 517/543 [00:12<00:00, 45.16it/s][A
 96%|█████████▌| 522/543 [00:12<00:00, 44.92it/s][A
 97%|█████████▋| 527/543 [00:12<00:00, 44.84it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.87it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.96it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.97it/s][A                                                
                                                 [A 20%|██        | 71/355 [00:33<01:13,  3.85it/s]
100%|██████████| 543/543 [00:12<00:00, 44.97it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:40:16,190 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-71
[INFO|configuration_utils.py:351] 2023-08-28 22:40:16,551 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-71/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:40:22,456 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-71/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:40:22,869 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-71/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:40:23,029 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-71/special_tokens_map.json
 20%|██        | 72/355 [01:00<57:57, 12.29s/it] 21%|██        | 73/355 [01:01<41:22,  8.80s/it] 21%|██        | 74/355 [01:01<29:16,  6.25s/it] 21%|██        | 75/355 [01:02<20:53,  4.48s/it] 21%|██▏       | 76/355 [01:02<14:59,  3.22s/it] 22%|██▏       | 77/355 [01:02<10:51,  2.34s/it] 22%|██▏       | 78/355 [01:03<07:58,  1.73s/it] 22%|██▏       | 79/355 [01:03<05:58,  1.30s/it] 23%|██▎       | 80/355 [01:03<04:33,  1.00it/s] 23%|██▎       | 81/355 [01:04<03:34,  1.28it/s] 23%|██▎       | 82/355 [01:04<02:53,  1.57it/s] 23%|██▎       | 83/355 [01:04<02:24,  1.88it/s] 24%|██▎       | 84/355 [01:04<02:04,  2.18it/s] 24%|██▍       | 85/355 [01:05<02:01,  2.22it/s] 24%|██▍       | 86/355 [01:05<01:48,  2.49it/s] 25%|██▍       | 87/355 [01:05<01:38,  2.71it/s] 25%|██▍       | 88/355 [01:06<01:32,  2.90it/s] 25%|██▌       | 89/355 [01:06<01:27,  3.05it/s] 25%|██▌       | 90/355 [01:06<01:23,  3.16it/s] 26%|██▌       | 91/355 [01:07<01:21,  3.24it/s] 26%|██▌       | 92/355 [01:07<01:19,  3.30it/s] 26%|██▌       | 93/355 [01:07<01:18,  3.34it/s] 26%|██▋       | 94/355 [01:07<01:17,  3.37it/s] 27%|██▋       | 95/355 [01:08<01:26,  3.01it/s] 27%|██▋       | 96/355 [01:08<01:22,  3.13it/s] 27%|██▋       | 97/355 [01:08<01:20,  3.22it/s] 28%|██▊       | 98/355 [01:09<01:18,  3.29it/s] 28%|██▊       | 99/355 [01:09<01:16,  3.33it/s] 28%|██▊       | 100/355 [01:09<01:15,  3.37it/s] 28%|██▊       | 101/355 [01:10<01:14,  3.39it/s] 29%|██▊       | 102/355 [01:10<01:14,  3.41it/s] 29%|██▉       | 103/355 [01:10<01:13,  3.42it/s] 29%|██▉       | 104/355 [01:10<01:13,  3.43it/s] 30%|██▉       | 105/355 [01:11<01:27,  2.86it/s] 30%|██▉       | 106/355 [01:11<01:22,  3.01it/s] 30%|███       | 107/355 [01:12<01:19,  3.13it/s] 30%|███       | 108/355 [01:12<01:16,  3.22it/s] 31%|███       | 109/355 [01:12<01:14,  3.29it/s] 31%|███       | 110/355 [01:12<01:13,  3.33it/s] 31%|███▏      | 111/355 [01:13<01:12,  3.37it/s] 32%|███▏      | 112/355 [01:13<01:11,  3.39it/s] 32%|███▏      | 113/355 [01:13<01:10,  3.41it/s] 32%|███▏      | 114/355 [01:14<01:10,  3.42it/s] 32%|███▏      | 115/355 [01:14<01:12,  3.29it/s] 33%|███▎      | 116/355 [01:14<01:11,  3.34it/s] 33%|███▎      | 117/355 [01:14<01:10,  3.37it/s] 33%|███▎      | 118/355 [01:15<01:09,  3.39it/s] 34%|███▎      | 119/355 [01:15<01:09,  3.41it/s] 34%|███▍      | 120/355 [01:15<01:08,  3.42it/s] 34%|███▍      | 121/355 [01:16<01:08,  3.43it/s] 34%|███▍      | 122/355 [01:16<01:07,  3.43it/s] 35%|███▍      | 123/355 [01:16<01:07,  3.44it/s] 35%|███▍      | 124/355 [01:16<01:07,  3.44it/s] 35%|███▌      | 125/355 [01:17<01:06,  3.44it/s] 35%|███▌      | 126/355 [01:17<01:08,  3.33it/s] 36%|███▌      | 127/355 [01:17<01:07,  3.37it/s] 36%|███▌      | 128/355 [01:18<01:06,  3.39it/s] 36%|███▋      | 129/355 [01:18<01:06,  3.41it/s] 37%|███▋      | 130/355 [01:18<01:05,  3.42it/s] 37%|███▋      | 131/355 [01:19<01:05,  3.42it/s] 37%|███▋      | 132/355 [01:19<01:04,  3.43it/s] 37%|███▋      | 133/355 [01:19<01:04,  3.44it/s] 38%|███▊      | 134/355 [01:19<01:04,  3.44it/s] 38%|███▊      | 135/355 [01:20<01:03,  3.44it/s] 38%|███▊      | 136/355 [01:20<01:03,  3.44it/s] 39%|███▊      | 137/355 [01:20<01:07,  3.23it/s] 39%|███▉      | 138/355 [01:21<01:05,  3.29it/s] 39%|███▉      | 139/355 [01:21<01:04,  3.34it/s] 39%|███▉      | 140/355 [01:21<01:03,  3.37it/s] 40%|███▉      | 141/355 [01:22<01:03,  3.39it/s] 40%|████      | 142/355 [01:22<00:56,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 22:41:04,191 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:41:04,191 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:41:04,191 >>   Batch size = 8
{'eval_loss': 1.1039217710494995, 'eval_runtime': 12.7307, 'eval_samples_per_second': 341.066, 'eval_steps_per_second': 42.653, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.43it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.20it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.54it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.82it/s][A
  5%|▍         | 27/543 [00:00<00:11, 46.38it/s][A
  6%|▌         | 32/543 [00:00<00:11, 46.00it/s][A
  7%|▋         | 37/543 [00:00<00:11, 45.45it/s][A
  8%|▊         | 42/543 [00:00<00:11, 45.08it/s][A
  9%|▊         | 47/543 [00:01<00:11, 45.04it/s][A
 10%|▉         | 52/543 [00:01<00:10, 45.04it/s][A
 10%|█         | 57/543 [00:01<00:10, 45.10it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 45.25it/s][A
 12%|█▏        | 67/543 [00:01<00:14, 33.26it/s][A
 13%|█▎        | 72/543 [00:01<00:13, 36.18it/s][A
 14%|█▍        | 77/543 [00:01<00:12, 38.40it/s][A
 15%|█▌        | 82/543 [00:01<00:11, 40.26it/s][A
 16%|█▌        | 87/543 [00:02<00:10, 41.70it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 42.72it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.41it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.07it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.22it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.45it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.72it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.81it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.85it/s][A
 24%|██▍       | 132/543 [00:03<00:09, 44.96it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 45.00it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 45.00it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 45.07it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.94it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 45.06it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 45.11it/s][A
 31%|███       | 167/543 [00:03<00:08, 45.08it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 45.01it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 45.08it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 45.09it/s][A
 34%|███▍      | 187/543 [00:04<00:07, 45.00it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 45.06it/s][A
 36%|███▋      | 197/543 [00:04<00:12, 27.82it/s][A
 37%|███▋      | 202/543 [00:04<00:10, 31.41it/s][A
 38%|███▊      | 207/543 [00:04<00:09, 34.57it/s][A
 39%|███▉      | 212/543 [00:05<00:10, 31.93it/s][A
 40%|███▉      | 217/543 [00:05<00:09, 35.17it/s][A
 41%|████      | 222/543 [00:05<00:08, 37.75it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 39.81it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 41.46it/s][A
 44%|████▎     | 237/543 [00:05<00:07, 42.56it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.38it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.06it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.07it/s][A
 47%|████▋     | 257/543 [00:06<00:06, 44.14it/s][A
 48%|████▊     | 262/543 [00:06<00:06, 44.31it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.68it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.89it/s][A
 51%|█████     | 277/543 [00:06<00:05, 45.11it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 45.13it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 45.28it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 45.20it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.96it/s][A
 56%|█████▌    | 302/543 [00:07<00:05, 44.77it/s][A
 57%|█████▋    | 307/543 [00:07<00:05, 44.77it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.86it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.94it/s][A
 59%|█████▉    | 322/543 [00:07<00:07, 30.63it/s][A
 60%|██████    | 327/543 [00:07<00:06, 33.73it/s][A
 61%|██████    | 332/543 [00:07<00:05, 36.70it/s][A
 62%|██████▏   | 337/543 [00:08<00:07, 27.00it/s][A
 63%|██████▎   | 342/543 [00:08<00:06, 31.08it/s][A
 64%|██████▍   | 347/543 [00:08<00:05, 34.26it/s][A
 65%|██████▍   | 352/543 [00:08<00:05, 37.00it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 39.21it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 40.91it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 42.24it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.26it/s][A
 69%|██████▉   | 377/543 [00:09<00:03, 43.68it/s][A
 70%|███████   | 382/543 [00:09<00:03, 43.76it/s][A
 71%|███████▏  | 387/543 [00:09<00:03, 43.94it/s][A
 72%|███████▏  | 392/543 [00:09<00:03, 44.15it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 44.50it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.74it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.99it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 45.17it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 45.34it/s][A
 78%|███████▊  | 422/543 [00:10<00:02, 45.16it/s][A
 79%|███████▊  | 427/543 [00:10<00:02, 44.86it/s][A
 80%|███████▉  | 432/543 [00:10<00:03, 35.45it/s][A
 80%|████████  | 437/543 [00:10<00:02, 38.02it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 40.04it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 41.52it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 42.43it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.47it/s][A
 85%|████████▌ | 462/543 [00:11<00:01, 44.11it/s][A
 86%|████████▌ | 467/543 [00:11<00:01, 44.47it/s][A
 87%|████████▋ | 472/543 [00:11<00:01, 44.23it/s][A
 88%|████████▊ | 477/543 [00:11<00:01, 44.21it/s][A
 89%|████████▉ | 482/543 [00:11<00:01, 44.49it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.77it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 45.05it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 45.10it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 45.21it/s][A
 93%|█████████▎| 507/543 [00:12<00:00, 45.35it/s][A
 94%|█████████▍| 512/543 [00:12<00:00, 45.13it/s][A
 95%|█████████▌| 517/543 [00:12<00:00, 44.94it/s][A
 96%|█████████▌| 522/543 [00:12<00:00, 44.71it/s][A
 97%|█████████▋| 527/543 [00:12<00:00, 44.74it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.91it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 45.00it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 45.14it/s][A                                                 
                                                 [A 40%|████      | 142/355 [01:35<00:56,  3.80it/s]
100%|██████████| 543/543 [00:12<00:00, 45.14it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:41:18,230 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-142
[INFO|configuration_utils.py:351] 2023-08-28 22:41:19,143 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-142/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:41:26,438 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-142/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:41:26,618 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-142/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:41:26,710 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-142/special_tokens_map.json
 40%|████      | 143/355 [02:00<41:33, 11.76s/it] 41%|████      | 144/355 [02:01<29:32,  8.40s/it] 41%|████      | 145/355 [02:01<20:53,  5.97s/it] 41%|████      | 146/355 [02:01<14:51,  4.26s/it] 41%|████▏     | 147/355 [02:02<10:39,  3.07s/it] 42%|████▏     | 148/355 [02:02<07:43,  2.24s/it] 42%|████▏     | 149/355 [02:02<05:40,  1.66s/it] 42%|████▏     | 150/355 [02:03<04:15,  1.25s/it] 43%|████▎     | 151/355 [02:03<03:15,  1.04it/s] 43%|████▎     | 152/355 [02:03<02:34,  1.32it/s] 43%|████▎     | 153/355 [02:03<02:05,  1.61it/s] 43%|████▎     | 154/355 [02:04<02:02,  1.64it/s] 44%|████▎     | 155/355 [02:04<01:42,  1.95it/s] 44%|████▍     | 156/355 [02:05<01:29,  2.23it/s] 44%|████▍     | 157/355 [02:05<01:19,  2.49it/s] 45%|████▍     | 158/355 [02:05<01:12,  2.71it/s] 45%|████▍     | 159/355 [02:06<01:07,  2.89it/s] 45%|████▌     | 160/355 [02:06<01:04,  3.03it/s] 45%|████▌     | 161/355 [02:06<01:01,  3.13it/s] 46%|████▌     | 162/355 [02:06<01:00,  3.21it/s] 46%|████▌     | 163/355 [02:07<00:58,  3.27it/s] 46%|████▌     | 164/355 [02:07<01:03,  3.01it/s] 46%|████▋     | 165/355 [02:07<01:00,  3.12it/s] 47%|████▋     | 166/355 [02:08<00:59,  3.20it/s] 47%|████▋     | 167/355 [02:08<00:57,  3.26it/s] 47%|████▋     | 168/355 [02:08<00:56,  3.30it/s] 48%|████▊     | 169/355 [02:09<00:55,  3.33it/s] 48%|████▊     | 170/355 [02:09<00:55,  3.35it/s] 48%|████▊     | 171/355 [02:09<00:54,  3.37it/s] 48%|████▊     | 172/355 [02:09<00:54,  3.37it/s] 49%|████▊     | 173/355 [02:10<00:53,  3.38it/s] 49%|████▉     | 174/355 [02:10<00:53,  3.38it/s] 49%|████▉     | 175/355 [02:10<00:53,  3.39it/s] 50%|████▉     | 176/355 [02:11<00:52,  3.39it/s] 50%|████▉     | 177/355 [02:11<00:52,  3.39it/s] 50%|█████     | 178/355 [02:11<00:53,  3.28it/s] 50%|█████     | 179/355 [02:12<00:53,  3.31it/s] 51%|█████     | 180/355 [02:12<00:52,  3.34it/s] 51%|█████     | 181/355 [02:12<00:51,  3.36it/s] 51%|█████▏    | 182/355 [02:12<00:51,  3.37it/s] 52%|█████▏    | 183/355 [02:13<00:50,  3.38it/s] 52%|█████▏    | 184/355 [02:13<00:50,  3.39it/s] 52%|█████▏    | 185/355 [02:13<00:50,  3.39it/s] 52%|█████▏    | 186/355 [02:14<00:49,  3.39it/s] 53%|█████▎    | 187/355 [02:14<00:49,  3.40it/s] 53%|█████▎    | 188/355 [02:14<00:49,  3.40it/s] 53%|█████▎    | 189/355 [02:15<00:51,  3.20it/s] 54%|█████▎    | 190/355 [02:15<00:50,  3.25it/s] 54%|█████▍    | 191/355 [02:15<00:49,  3.30it/s] 54%|█████▍    | 192/355 [02:15<00:48,  3.33it/s] 54%|█████▍    | 193/355 [02:16<00:48,  3.35it/s] 55%|█████▍    | 194/355 [02:16<00:47,  3.36it/s] 55%|█████▍    | 195/355 [02:16<00:47,  3.37it/s] 55%|█████▌    | 196/355 [02:17<00:47,  3.38it/s] 55%|█████▌    | 197/355 [02:17<00:46,  3.38it/s] 56%|█████▌    | 198/355 [02:17<00:46,  3.39it/s] 56%|█████▌    | 199/355 [02:18<00:49,  3.15it/s] 56%|█████▋    | 200/355 [02:18<00:48,  3.22it/s] 57%|█████▋    | 201/355 [02:18<00:47,  3.27it/s] 57%|█████▋    | 202/355 [02:18<00:46,  3.31it/s] 57%|█████▋    | 203/355 [02:19<00:45,  3.33it/s] 57%|█████▋    | 204/355 [02:19<00:45,  3.35it/s] 58%|█████▊    | 205/355 [02:19<00:44,  3.37it/s] 58%|█████▊    | 206/355 [02:20<00:44,  3.38it/s] 58%|█████▊    | 207/355 [02:20<00:43,  3.38it/s] 59%|█████▊    | 208/355 [02:20<00:43,  3.39it/s] 59%|█████▉    | 209/355 [02:21<00:54,  2.66it/s] 59%|█████▉    | 210/355 [02:21<00:50,  2.84it/s] 59%|█████▉    | 211/355 [02:21<00:48,  2.99it/s] 60%|█████▉    | 212/355 [02:22<00:46,  3.10it/s] 60%|██████    | 213/355 [02:22<00:40,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 22:42:04,344 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:42:04,344 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:42:04,344 >>   Batch size = 8
{'eval_loss': 1.1258872747421265, 'eval_runtime': 12.9132, 'eval_samples_per_second': 336.246, 'eval_steps_per_second': 42.05, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.42it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.18it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.64it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.81it/s][A
  5%|▍         | 27/543 [00:00<00:11, 46.23it/s][A
  6%|▌         | 32/543 [00:00<00:11, 45.84it/s][A
  7%|▋         | 37/543 [00:00<00:11, 45.67it/s][A
  8%|▊         | 42/543 [00:00<00:11, 45.33it/s][A
  9%|▊         | 47/543 [00:01<00:10, 45.22it/s][A
 10%|▉         | 52/543 [00:01<00:10, 45.21it/s][A
 10%|█         | 57/543 [00:01<00:10, 45.28it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 45.41it/s][A
 12%|█▏        | 67/543 [00:01<00:11, 41.78it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 42.83it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.58it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.11it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.50it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.72it/s][A
 18%|█▊        | 97/543 [00:02<00:09, 44.97it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 45.11it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.71it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.74it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.85it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 45.06it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 45.23it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 45.28it/s][A
 25%|██▌       | 137/543 [00:03<00:08, 45.29it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 45.25it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 45.12it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.93it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.81it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.88it/s][A
 31%|███       | 167/543 [00:03<00:08, 45.06it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 45.26it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 45.34it/s][A
 34%|███▎      | 182/543 [00:04<00:07, 45.41it/s][A
 34%|███▍      | 187/543 [00:04<00:07, 45.35it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 45.18it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.95it/s][A
 37%|███▋      | 202/543 [00:04<00:08, 42.56it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.40it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.06it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.50it/s][A
 41%|████      | 222/543 [00:04<00:08, 38.73it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 40.66it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 42.07it/s][A
 44%|████▎     | 237/543 [00:05<00:07, 43.05it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.80it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.16it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.65it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.99it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.76it/s][A
 49%|████▉     | 267/543 [00:05<00:06, 44.51it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.54it/s][A
 51%|█████     | 277/543 [00:06<00:05, 44.79it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.94it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 45.15it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 45.25it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 45.43it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 45.33it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 45.09it/s][A
 57%|█████▋    | 312/543 [00:06<00:05, 44.84it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.78it/s][A
 59%|█████▉    | 322/543 [00:07<00:04, 44.82it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.96it/s][A
 61%|██████    | 332/543 [00:07<00:04, 45.16it/s][A
 62%|██████▏   | 337/543 [00:07<00:05, 35.75it/s][A
 63%|██████▎   | 342/543 [00:07<00:05, 38.21it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 40.06it/s][A
 65%|██████▍   | 352/543 [00:08<00:06, 28.89it/s][A
 66%|██████▌   | 358/543 [00:08<00:05, 33.72it/s][A
 67%|██████▋   | 363/543 [00:08<00:04, 36.35it/s][A
 68%|██████▊   | 368/543 [00:08<00:04, 38.57it/s][A
 69%|██████▊   | 373/543 [00:08<00:04, 40.30it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 41.75it/s][A
 71%|███████   | 383/543 [00:08<00:03, 42.86it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 43.62it/s][A
 72%|███████▏  | 393/543 [00:09<00:03, 44.07it/s][A
 73%|███████▎  | 398/543 [00:09<00:03, 44.06it/s][A
 74%|███████▍  | 403/543 [00:09<00:03, 44.18it/s][A
 75%|███████▌  | 408/543 [00:09<00:03, 44.47it/s][A
 76%|███████▌  | 413/543 [00:09<00:02, 44.56it/s][A
 77%|███████▋  | 418/543 [00:09<00:02, 44.68it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 45.00it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 45.13it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 45.28it/s][A
 81%|████████  | 438/543 [00:10<00:02, 45.26it/s][A
 82%|████████▏ | 443/543 [00:10<00:02, 45.11it/s][A
 83%|████████▎ | 448/543 [00:10<00:02, 44.98it/s][A
 83%|████████▎ | 453/543 [00:10<00:02, 44.98it/s][A
 84%|████████▍ | 458/543 [00:10<00:01, 44.99it/s][A
 85%|████████▌ | 463/543 [00:10<00:02, 30.49it/s][A
 86%|████████▌ | 468/543 [00:10<00:02, 33.83it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 36.60it/s][A
 88%|████████▊ | 478/543 [00:11<00:01, 38.83it/s][A
 89%|████████▉ | 483/543 [00:11<00:01, 40.63it/s][A
 90%|████████▉ | 488/543 [00:11<00:01, 42.00it/s][A
 91%|█████████ | 493/543 [00:11<00:01, 43.06it/s][A
 92%|█████████▏| 498/543 [00:11<00:01, 43.71it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 43.73it/s][A
 94%|█████████▎| 508/543 [00:11<00:00, 43.97it/s][A
 94%|█████████▍| 513/543 [00:11<00:00, 44.20it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 44.52it/s][A
 96%|█████████▋| 523/543 [00:12<00:00, 44.72it/s][A
 97%|█████████▋| 528/543 [00:12<00:00, 45.03it/s][A
 98%|█████████▊| 533/543 [00:12<00:00, 45.14it/s][A
 99%|█████████▉| 538/543 [00:12<00:00, 45.28it/s][A
100%|██████████| 543/543 [00:12<00:00, 45.23it/s][A                                                 
                                                 [A 60%|██████    | 213/355 [02:34<00:40,  3.52it/s]
100%|██████████| 543/543 [00:12<00:00, 45.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:42:16,977 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-213
[INFO|configuration_utils.py:351] 2023-08-28 22:42:17,243 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-213/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:42:22,967 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-213/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:42:23,396 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-213/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:42:23,489 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-213/special_tokens_map.json
 60%|██████    | 214/355 [02:55<23:29, 10.00s/it] 61%|██████    | 215/355 [02:55<16:33,  7.10s/it] 61%|██████    | 216/355 [02:55<11:42,  5.05s/it] 61%|██████    | 217/355 [02:55<08:20,  3.63s/it] 61%|██████▏   | 218/355 [02:56<05:59,  2.63s/it] 62%|██████▏   | 219/355 [02:56<04:21,  1.93s/it] 62%|██████▏   | 220/355 [02:56<03:13,  1.44s/it] 62%|██████▏   | 221/355 [02:57<02:26,  1.09s/it] 63%|██████▎   | 222/355 [02:57<01:53,  1.17it/s] 63%|██████▎   | 223/355 [02:57<01:30,  1.46it/s] 63%|██████▎   | 224/355 [02:57<01:14,  1.76it/s] 63%|██████▎   | 225/355 [02:58<01:03,  2.06it/s] 64%|██████▎   | 226/355 [02:58<01:01,  2.08it/s] 64%|██████▍   | 227/355 [02:59<00:54,  2.37it/s] 64%|██████▍   | 228/355 [02:59<00:48,  2.61it/s] 65%|██████▍   | 229/355 [02:59<00:44,  2.82it/s] 65%|██████▍   | 230/355 [02:59<00:41,  2.98it/s] 65%|██████▌   | 231/355 [03:00<00:39,  3.11it/s] 65%|██████▌   | 232/355 [03:00<00:38,  3.20it/s] 66%|██████▌   | 233/355 [03:00<00:37,  3.27it/s] 66%|██████▌   | 234/355 [03:01<00:36,  3.32it/s] 66%|██████▌   | 235/355 [03:01<00:35,  3.36it/s] 66%|██████▋   | 236/355 [03:01<00:37,  3.19it/s] 67%|██████▋   | 237/355 [03:01<00:36,  3.27it/s] 67%|██████▋   | 238/355 [03:02<00:35,  3.32it/s] 67%|██████▋   | 239/355 [03:02<00:34,  3.36it/s] 68%|██████▊   | 240/355 [03:02<00:33,  3.39it/s] 68%|██████▊   | 241/355 [03:03<00:33,  3.40it/s] 68%|██████▊   | 242/355 [03:03<00:33,  3.42it/s] 68%|██████▊   | 243/355 [03:03<00:32,  3.43it/s] 69%|██████▊   | 244/355 [03:04<00:32,  3.43it/s] 69%|██████▉   | 245/355 [03:04<00:31,  3.44it/s] 69%|██████▉   | 246/355 [03:04<00:31,  3.44it/s] 70%|██████▉   | 247/355 [03:04<00:33,  3.21it/s] 70%|██████▉   | 248/355 [03:05<00:32,  3.28it/s] 70%|███████   | 249/355 [03:05<00:31,  3.33it/s] 70%|███████   | 250/355 [03:05<00:31,  3.36it/s] 71%|███████   | 251/355 [03:06<00:30,  3.39it/s] 71%|███████   | 252/355 [03:06<00:30,  3.41it/s] 71%|███████▏  | 253/355 [03:06<00:29,  3.42it/s] 72%|███████▏  | 254/355 [03:06<00:29,  3.43it/s] 72%|███████▏  | 255/355 [03:07<00:29,  3.43it/s] 72%|███████▏  | 256/355 [03:07<00:28,  3.44it/s] 72%|███████▏  | 257/355 [03:07<00:28,  3.44it/s] 73%|███████▎  | 258/355 [03:08<00:38,  2.55it/s] 73%|███████▎  | 259/355 [03:08<00:34,  2.77it/s] 73%|███████▎  | 260/355 [03:09<00:32,  2.94it/s] 74%|███████▎  | 261/355 [03:09<00:30,  3.07it/s] 74%|███████▍  | 262/355 [03:09<00:29,  3.18it/s] 74%|███████▍  | 263/355 [03:09<00:28,  3.25it/s] 74%|███████▍  | 264/355 [03:10<00:27,  3.31it/s] 75%|███████▍  | 265/355 [03:10<00:26,  3.35it/s] 75%|███████▍  | 266/355 [03:10<00:26,  3.38it/s] 75%|███████▌  | 267/355 [03:11<00:25,  3.40it/s] 75%|███████▌  | 268/355 [03:11<00:25,  3.41it/s] 76%|███████▌  | 269/355 [03:11<00:25,  3.42it/s] 76%|███████▌  | 270/355 [03:11<00:24,  3.43it/s] 76%|███████▋  | 271/355 [03:12<00:24,  3.44it/s] 77%|███████▋  | 272/355 [03:12<00:24,  3.44it/s] 77%|███████▋  | 273/355 [03:12<00:23,  3.44it/s] 77%|███████▋  | 274/355 [03:13<00:27,  2.99it/s] 77%|███████▋  | 275/355 [03:13<00:25,  3.12it/s] 78%|███████▊  | 276/355 [03:13<00:24,  3.21it/s] 78%|███████▊  | 277/355 [03:14<00:23,  3.28it/s] 78%|███████▊  | 278/355 [03:14<00:23,  3.33it/s] 79%|███████▊  | 279/355 [03:14<00:22,  3.36it/s] 79%|███████▉  | 280/355 [03:15<00:22,  3.39it/s] 79%|███████▉  | 281/355 [03:15<00:21,  3.40it/s] 79%|███████▉  | 282/355 [03:15<00:21,  3.42it/s] 80%|███████▉  | 283/355 [03:15<00:21,  3.43it/s] 80%|████████  | 284/355 [03:16<00:21,  3.28it/s][INFO|trainer.py:2140] 2023-08-28 22:42:58,204 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:42:58,204 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:42:58,204 >>   Batch size = 8
{'eval_loss': 1.1392894983291626, 'eval_runtime': 12.5569, 'eval_samples_per_second': 345.785, 'eval_steps_per_second': 43.243, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.30it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.41it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.71it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.71it/s][A
  5%|▍         | 27/543 [00:00<00:11, 46.39it/s][A
  6%|▌         | 32/543 [00:00<00:11, 46.05it/s][A
  7%|▋         | 37/543 [00:00<00:11, 45.65it/s][A
  8%|▊         | 42/543 [00:00<00:11, 45.24it/s][A
  9%|▊         | 47/543 [00:01<00:10, 45.15it/s][A
 10%|▉         | 52/543 [00:01<00:10, 45.27it/s][A
 10%|█         | 57/543 [00:01<00:10, 45.34it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 45.25it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 45.28it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 45.35it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 45.31it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 45.12it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.93it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.97it/s][A
 18%|█▊        | 97/543 [00:02<00:09, 45.03it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 45.06it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 45.17it/s][A
 21%|██        | 112/543 [00:02<00:09, 45.28it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 45.38it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 45.30it/s][A
 23%|██▎       | 127/543 [00:02<00:13, 30.54it/s][A
 24%|██▍       | 132/543 [00:03<00:12, 33.84it/s][A
 25%|██▌       | 137/543 [00:03<00:11, 36.60it/s][A
 26%|██▌       | 142/543 [00:03<00:10, 38.96it/s][A
 27%|██▋       | 147/543 [00:03<00:09, 40.68it/s][A
 28%|██▊       | 152/543 [00:03<00:09, 42.12it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.06it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.80it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.84it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.98it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 44.18it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.45it/s][A
 34%|███▍      | 187/543 [00:04<00:07, 44.79it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.95it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 45.15it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 45.34it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 45.24it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 45.10it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.80it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.71it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.83it/s][A
 43%|████▎     | 232/543 [00:05<00:06, 44.98it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 45.14it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 45.19it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 45.33it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 45.42it/s][A
 47%|████▋     | 257/543 [00:05<00:07, 40.33it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 41.77it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 42.85it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.61it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.14it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.46it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.84it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 45.01it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.70it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.53it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.71it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.84it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 45.10it/s][A
 59%|█████▉    | 322/543 [00:07<00:04, 45.16it/s][A
 60%|██████    | 327/543 [00:07<00:04, 45.26it/s][A
 61%|██████    | 332/543 [00:07<00:04, 45.29it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 45.16it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.99it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.74it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.73it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.91it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 45.03it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 45.21it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 45.33it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 45.35it/s][A
 70%|███████   | 382/543 [00:08<00:03, 45.25it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 45.10it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 38.50it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 40.43it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 41.79it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 42.88it/s][A
 76%|███████▌  | 412/543 [00:09<00:03, 43.63it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.17it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.57it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.82it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.59it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.54it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.67it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.91it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 45.07it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 45.17it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 45.31it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 45.39it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 45.31it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 45.07it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.91it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.83it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 41.17it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 42.55it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.36it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.02it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.43it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.86it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.99it/s][A
 97%|█████████▋| 527/543 [00:12<00:00, 36.27it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 38.66it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 40.51it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 41.86it/s][A                                                 
                                                 [A 80%|████████  | 284/355 [03:28<00:21,  3.28it/s]
100%|██████████| 543/543 [00:12<00:00, 41.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:43:10,944 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-284
[INFO|configuration_utils.py:351] 2023-08-28 22:43:11,546 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-284/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:43:19,295 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-284/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:43:20,515 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-284/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:43:20,925 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-284/special_tokens_map.json
 80%|████████  | 285/355 [03:51<12:46, 10.94s/it] 81%|████████  | 286/355 [03:52<08:57,  7.79s/it] 81%|████████  | 287/355 [03:52<06:16,  5.54s/it] 81%|████████  | 288/355 [03:52<04:25,  3.97s/it] 81%|████████▏ | 289/355 [03:53<03:08,  2.86s/it] 82%|████████▏ | 290/355 [03:53<02:15,  2.09s/it] 82%|████████▏ | 291/355 [03:53<01:39,  1.55s/it] 82%|████████▏ | 292/355 [03:54<01:13,  1.17s/it] 83%|████████▎ | 293/355 [03:54<00:56,  1.10it/s] 83%|████████▎ | 294/355 [03:54<00:44,  1.38it/s] 83%|████████▎ | 295/355 [03:55<00:35,  1.68it/s] 83%|████████▎ | 296/355 [03:55<00:30,  1.93it/s] 84%|████████▎ | 297/355 [03:55<00:26,  2.22it/s] 84%|████████▍ | 298/355 [03:55<00:22,  2.48it/s] 84%|████████▍ | 299/355 [03:56<00:20,  2.70it/s] 85%|████████▍ | 300/355 [03:56<00:19,  2.88it/s] 85%|████████▍ | 301/355 [03:56<00:17,  3.02it/s] 85%|████████▌ | 302/355 [03:57<00:16,  3.12it/s] 85%|████████▌ | 303/355 [03:57<00:16,  3.20it/s] 86%|████████▌ | 304/355 [03:57<00:15,  3.26it/s] 86%|████████▌ | 305/355 [03:58<00:15,  3.30it/s] 86%|████████▌ | 306/355 [03:58<00:14,  3.33it/s] 86%|████████▋ | 307/355 [03:58<00:17,  2.77it/s] 87%|████████▋ | 308/355 [03:59<00:16,  2.93it/s] 87%|████████▋ | 309/355 [03:59<00:15,  3.06it/s] 87%|████████▋ | 310/355 [03:59<00:14,  3.16it/s] 88%|████████▊ | 311/355 [03:59<00:13,  3.23it/s] 88%|████████▊ | 312/355 [04:00<00:13,  3.28it/s] 88%|████████▊ | 313/355 [04:00<00:12,  3.31it/s] 88%|████████▊ | 314/355 [04:00<00:12,  3.34it/s] 89%|████████▊ | 315/355 [04:01<00:11,  3.36it/s] 89%|████████▉ | 316/355 [04:01<00:11,  3.37it/s] 89%|████████▉ | 317/355 [04:01<00:12,  3.02it/s] 90%|████████▉ | 318/355 [04:02<00:11,  3.12it/s] 90%|████████▉ | 319/355 [04:02<00:11,  3.20it/s] 90%|█████████ | 320/355 [04:02<00:10,  3.26it/s] 90%|█████████ | 321/355 [04:03<00:10,  3.30it/s] 91%|█████████ | 322/355 [04:03<00:09,  3.33it/s] 91%|█████████ | 323/355 [04:03<00:09,  3.35it/s] 91%|█████████▏| 324/355 [04:03<00:09,  3.37it/s] 92%|█████████▏| 325/355 [04:04<00:08,  3.38it/s] 92%|█████████▏| 326/355 [04:04<00:08,  3.38it/s] 92%|█████████▏| 327/355 [04:05<00:12,  2.20it/s] 92%|█████████▏| 328/355 [04:05<00:10,  2.47it/s] 93%|█████████▎| 329/355 [04:05<00:09,  2.69it/s] 93%|█████████▎| 330/355 [04:06<00:08,  2.86it/s] 93%|█████████▎| 331/355 [04:06<00:07,  3.01it/s] 94%|█████████▎| 332/355 [04:06<00:07,  3.12it/s] 94%|█████████▍| 333/355 [04:07<00:06,  3.20it/s] 94%|█████████▍| 334/355 [04:07<00:06,  3.26it/s] 94%|█████████▍| 335/355 [04:07<00:06,  3.30it/s] 95%|█████████▍| 336/355 [04:08<00:06,  3.11it/s] 95%|█████████▍| 337/355 [04:08<00:05,  3.19it/s] 95%|█████████▌| 338/355 [04:08<00:05,  3.25it/s] 95%|█████████▌| 339/355 [04:08<00:04,  3.31it/s] 96%|█████████▌| 340/355 [04:09<00:04,  3.35it/s] 96%|█████████▌| 341/355 [04:09<00:04,  3.37it/s] 96%|█████████▋| 342/355 [04:09<00:03,  3.40it/s] 97%|█████████▋| 343/355 [04:10<00:03,  3.41it/s] 97%|█████████▋| 344/355 [04:10<00:03,  3.42it/s] 97%|█████████▋| 345/355 [04:10<00:02,  3.43it/s] 97%|█████████▋| 346/355 [04:10<00:02,  3.44it/s] 98%|█████████▊| 347/355 [04:11<00:02,  3.27it/s] 98%|█████████▊| 348/355 [04:11<00:02,  3.32it/s] 98%|█████████▊| 349/355 [04:11<00:01,  3.36it/s] 99%|█████████▊| 350/355 [04:12<00:01,  3.38it/s] 99%|█████████▉| 351/355 [04:12<00:01,  3.40it/s] 99%|█████████▉| 352/355 [04:12<00:00,  3.42it/s] 99%|█████████▉| 353/355 [04:13<00:00,  3.43it/s]100%|█████████▉| 354/355 [04:13<00:00,  3.43it/s]100%|██████████| 355/355 [04:13<00:00,  3.83it/s][INFO|trainer.py:2140] 2023-08-28 22:43:55,521 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:43:55,521 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:43:55,521 >>   Batch size = 8
{'eval_loss': 1.1495848894119263, 'eval_runtime': 12.4126, 'eval_samples_per_second': 349.807, 'eval_steps_per_second': 43.746, 'epoch': 4.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.81it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.97it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.46it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.58it/s][A
  5%|▍         | 27/543 [00:00<00:11, 46.16it/s][A
  6%|▌         | 32/543 [00:00<00:13, 38.72it/s][A
  7%|▋         | 37/543 [00:00<00:12, 40.71it/s][A
  8%|▊         | 42/543 [00:00<00:11, 42.19it/s][A
  9%|▊         | 47/543 [00:01<00:11, 43.17it/s][A
 10%|▉         | 52/543 [00:01<00:11, 43.91it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.40it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.76it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.95it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.63it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.50it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.58it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.85it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.96it/s][A
 18%|█▊        | 97/543 [00:02<00:09, 45.26it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 45.30it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 45.45it/s][A
 21%|██        | 112/543 [00:02<00:09, 45.34it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 45.25it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 45.01it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.90it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 45.02it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 45.10it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 45.27it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 45.42it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 45.40it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 45.39it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 45.20it/s][A
 31%|███       | 167/543 [00:03<00:09, 39.92it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 41.42it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 42.62it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 43.48it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.11it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.46it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.88it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 45.00it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.65it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.60it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.77it/s][A
 41%|████      | 222/543 [00:04<00:07, 44.99it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 45.14it/s][A
 43%|████▎     | 232/543 [00:05<00:06, 45.19it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 45.42it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 45.42it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 45.24it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.92it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.74it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.91it/s][A
 49%|████▉     | 267/543 [00:05<00:06, 45.05it/s][A
 50%|█████     | 272/543 [00:06<00:06, 45.13it/s][A
 51%|█████     | 277/543 [00:06<00:05, 45.15it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 45.39it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 45.44it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 45.45it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 45.12it/s][A
 56%|█████▌    | 302/543 [00:06<00:06, 38.24it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 40.21it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 41.65it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 42.83it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.63it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.29it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.56it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.77it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.56it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.35it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.54it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.73it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.99it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 45.21it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 45.42it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 45.38it/s][A
 70%|███████   | 382/543 [00:08<00:03, 45.36it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 45.05it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.78it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.77it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.78it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 45.07it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 45.32it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 45.40it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 45.40it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 45.34it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 45.05it/s][A
 80%|████████  | 437/543 [00:09<00:02, 38.13it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 40.06it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 41.61it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 42.68it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.63it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.09it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.54it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.79it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.50it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.51it/s][A
 90%|████████▉ | 487/543 [00:10<00:01, 44.68it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.87it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 45.06it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 45.29it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 45.36it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 45.36it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 45.24it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 45.08it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.99it/s][A
 98%|█████████▊| 532/543 [00:11<00:00, 44.98it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 45.00it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 45.07it/s][A                                                 
                                                 [A100%|██████████| 355/355 [04:25<00:00,  3.83it/s]
100%|██████████| 543/543 [00:12<00:00, 45.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:44:07,952 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-355
[INFO|configuration_utils.py:351] 2023-08-28 22:44:08,295 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-355/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:44:16,334 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-355/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:44:16,658 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-355/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:44:17,266 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-355/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 22:44:36,670 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 22:44:36,692 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-71 (score: 1.1039217710494995).
                                                 100%|██████████| 355/355 [05:40<00:00,  3.83it/s]100%|██████████| 355/355 [05:40<00:00,  1.04it/s]
[INFO|trainer.py:1894] 2023-08-28 22:45:22,282 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 22:45:22,776 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:45:30,248 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:45:30,699 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:45:31,253 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:45:32,927 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:32,927 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:32,927 >>   train_loss               =     0.3947
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:32,927 >>   train_runtime            = 0:05:39.98
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:32,928 >>   train_samples            =       4516
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:32,928 >>   train_samples_per_second =     66.415
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:32,928 >>   train_steps_per_second   =      1.044
{'eval_loss': 1.1567074060440063, 'eval_runtime': 12.2462, 'eval_samples_per_second': 354.559, 'eval_steps_per_second': 44.34, 'epoch': 5.0}
{'train_runtime': 339.9829, 'train_samples_per_second': 66.415, 'train_steps_per_second': 1.044, 'train_loss': 0.3946902422837808, 'epoch': 5.0}
08/28/2023 22:45:33 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 22:45:33,565 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:45:33,566 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:45:33,566 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 55.84it/s]  2%|▏         | 12/543 [00:00<00:10, 49.48it/s]  3%|▎         | 17/543 [00:00<00:11, 47.79it/s]  4%|▍         | 22/543 [00:00<00:11, 47.00it/s]  5%|▍         | 27/543 [00:00<00:11, 46.45it/s]  6%|▌         | 32/543 [00:00<00:11, 46.21it/s]  7%|▋         | 37/543 [00:00<00:11, 45.98it/s]  8%|▊         | 42/543 [00:00<00:10, 45.75it/s]  9%|▊         | 47/543 [00:01<00:10, 45.21it/s] 10%|▉         | 52/543 [00:01<00:10, 44.99it/s] 10%|█         | 57/543 [00:01<00:10, 45.15it/s] 11%|█▏        | 62/543 [00:01<00:10, 45.35it/s] 12%|█▏        | 67/543 [00:01<00:10, 45.34it/s] 13%|█▎        | 72/543 [00:01<00:10, 45.47it/s] 14%|█▍        | 77/543 [00:01<00:10, 45.45it/s] 15%|█▌        | 82/543 [00:01<00:10, 45.51it/s] 16%|█▌        | 87/543 [00:01<00:10, 45.37it/s] 17%|█▋        | 92/543 [00:02<00:10, 45.07it/s] 18%|█▊        | 97/543 [00:02<00:09, 44.91it/s] 19%|█▉        | 102/543 [00:02<00:09, 44.99it/s] 20%|█▉        | 107/543 [00:02<00:09, 45.20it/s] 21%|██        | 112/543 [00:02<00:09, 45.22it/s] 22%|██▏       | 117/543 [00:02<00:09, 45.33it/s] 22%|██▏       | 122/543 [00:02<00:09, 45.45it/s] 23%|██▎       | 127/543 [00:02<00:09, 45.53it/s] 24%|██▍       | 132/543 [00:02<00:09, 45.41it/s] 25%|██▌       | 137/543 [00:02<00:08, 45.23it/s] 26%|██▌       | 142/543 [00:03<00:09, 44.16it/s] 27%|██▋       | 147/543 [00:03<00:08, 44.54it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.84it/s] 29%|██▉       | 157/543 [00:03<00:08, 45.16it/s] 30%|██▉       | 162/543 [00:03<00:08, 45.27it/s] 31%|███       | 167/543 [00:03<00:08, 45.43it/s] 32%|███▏      | 172/543 [00:03<00:08, 45.54it/s] 33%|███▎      | 177/543 [00:03<00:08, 45.56it/s] 34%|███▎      | 182/543 [00:03<00:07, 45.36it/s] 34%|███▍      | 187/543 [00:04<00:07, 45.29it/s] 35%|███▌      | 192/543 [00:04<00:07, 45.40it/s] 36%|███▋      | 197/543 [00:04<00:07, 45.43it/s] 37%|███▋      | 202/543 [00:04<00:07, 45.37it/s] 38%|███▊      | 207/543 [00:04<00:07, 45.49it/s] 39%|███▉      | 212/543 [00:04<00:07, 45.55it/s] 40%|███▉      | 217/543 [00:04<00:07, 45.60it/s] 41%|████      | 222/543 [00:04<00:07, 45.51it/s] 42%|████▏     | 227/543 [00:04<00:06, 45.25it/s] 43%|████▎     | 232/543 [00:05<00:06, 45.23it/s] 44%|████▎     | 237/543 [00:05<00:06, 45.27it/s] 45%|████▍     | 242/543 [00:05<00:06, 45.17it/s] 45%|████▌     | 247/543 [00:05<00:06, 45.38it/s] 46%|████▋     | 252/543 [00:05<00:06, 45.41it/s] 47%|████▋     | 257/543 [00:05<00:06, 45.36it/s] 48%|████▊     | 262/543 [00:05<00:06, 45.40it/s] 49%|████▉     | 267/543 [00:05<00:06, 45.38it/s] 50%|█████     | 272/543 [00:05<00:05, 45.34it/s] 51%|█████     | 277/543 [00:06<00:05, 45.29it/s] 52%|█████▏    | 282/543 [00:06<00:06, 42.21it/s] 53%|█████▎    | 287/543 [00:06<00:05, 43.30it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.03it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.62it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.99it/s] 57%|█████▋    | 307/543 [00:06<00:05, 45.18it/s] 57%|█████▋    | 312/543 [00:06<00:05, 45.22it/s] 58%|█████▊    | 317/543 [00:06<00:05, 45.19it/s] 59%|█████▉    | 322/543 [00:07<00:04, 44.82it/s] 60%|██████    | 327/543 [00:07<00:04, 44.77it/s] 61%|██████    | 332/543 [00:07<00:04, 44.92it/s] 62%|██████▏   | 337/543 [00:07<00:04, 45.17it/s] 63%|██████▎   | 342/543 [00:07<00:04, 45.27it/s] 64%|██████▍   | 347/543 [00:07<00:04, 45.40it/s] 65%|██████▍   | 352/543 [00:07<00:04, 45.44it/s] 66%|██████▌   | 357/543 [00:07<00:04, 45.57it/s] 67%|██████▋   | 362/543 [00:07<00:03, 45.54it/s] 68%|██████▊   | 367/543 [00:08<00:03, 45.28it/s] 69%|██████▊   | 372/543 [00:08<00:03, 45.17it/s] 69%|██████▉   | 377/543 [00:08<00:03, 45.13it/s] 70%|███████   | 382/543 [00:08<00:03, 45.20it/s] 71%|███████▏  | 387/543 [00:08<00:03, 45.25it/s] 72%|███████▏  | 392/543 [00:08<00:03, 45.38it/s] 73%|███████▎  | 397/543 [00:08<00:03, 45.47it/s] 74%|███████▍  | 402/543 [00:08<00:03, 45.52it/s] 75%|███████▍  | 407/543 [00:08<00:02, 45.41it/s] 76%|███████▌  | 412/543 [00:09<00:02, 45.27it/s] 77%|███████▋  | 417/543 [00:09<00:03, 38.44it/s] 78%|███████▊  | 422/543 [00:09<00:02, 40.42it/s] 79%|███████▊  | 427/543 [00:09<00:02, 41.87it/s] 80%|███████▉  | 432/543 [00:09<00:02, 43.03it/s] 80%|████████  | 437/543 [00:09<00:02, 43.82it/s] 81%|████████▏ | 442/543 [00:09<00:02, 44.41it/s] 82%|████████▏ | 447/543 [00:09<00:02, 44.66it/s] 83%|████████▎ | 452/543 [00:10<00:02, 44.99it/s] 84%|████████▍ | 457/543 [00:10<00:01, 44.71it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.59it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.68it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.82it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.99it/s] 89%|████████▉ | 482/543 [00:10<00:01, 45.15it/s] 90%|████████▉ | 487/543 [00:10<00:01, 45.36it/s] 91%|█████████ | 492/543 [00:10<00:01, 45.49it/s] 92%|█████████▏| 497/543 [00:11<00:01, 45.40it/s] 92%|█████████▏| 502/543 [00:11<00:00, 45.41it/s] 93%|█████████▎| 507/543 [00:11<00:00, 45.30it/s] 94%|█████████▍| 512/543 [00:11<00:00, 45.13it/s] 95%|█████████▌| 517/543 [00:11<00:00, 45.22it/s] 96%|█████████▌| 522/543 [00:11<00:00, 45.23it/s] 97%|█████████▋| 527/543 [00:11<00:00, 45.31it/s] 98%|█████████▊| 532/543 [00:11<00:00, 45.36it/s] 99%|█████████▉| 537/543 [00:11<00:00, 45.33it/s]100%|█████████▉| 542/543 [00:12<00:00, 45.33it/s]100%|██████████| 543/543 [00:12<00:00, 45.05it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:45:45,638 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:45,638 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:45,638 >>   eval_loss               =     1.1039
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:45,638 >>   eval_runtime            = 0:00:12.07
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:45,638 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:45,638 >>   eval_samples_per_second =    359.675
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:45,638 >>   eval_steps_per_second   =      44.98
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:45:45,638 >>   perplexity              =      3.016
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:11,683 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:11,884 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:11,885 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:11,885 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:11,885 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:46:13,653 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:46:13,654 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:46:15,077 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:46:16,541 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:46:16,541 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:23,472 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:23,517 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:23,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:23,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:46:23,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:46:24,700 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:46:24,701 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:46:26,029 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:46:26,367 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:46:26,368 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-355
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-284
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-142
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-213
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/checkpoint-71
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.75it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.74it/s]Extractor Predicting: 12it [00:07,  1.76it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:09,  1.58it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.46it/s]Extractor Predicting: 19it [00:11,  1.45it/s]Extractor Predicting: 20it [00:12,  1.45it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:17,  1.47it/s]Extractor Predicting: 29it [00:18,  1.48it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:21,  1.49it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.49it/s]Extractor Predicting: 37it [00:23,  1.42it/s]Extractor Predicting: 38it [00:24,  1.44it/s]Extractor Predicting: 39it [00:25,  1.49it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:26,  1.50it/s]Extractor Predicting: 42it [00:27,  1.38it/s]Extractor Predicting: 43it [00:27,  1.44it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:30,  1.47it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:31,  1.50it/s]Extractor Predicting: 50it [00:32,  1.52it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:34,  1.41it/s]Extractor Predicting: 53it [00:34,  1.42it/s]Extractor Predicting: 54it [00:35,  1.48it/s]Extractor Predicting: 55it [00:35,  1.53it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.46it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:38,  1.50it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:39,  1.54it/s]Extractor Predicting: 62it [00:40,  1.53it/s]Extractor Predicting: 63it [00:41,  1.54it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:42,  1.60it/s]Extractor Predicting: 66it [00:43,  1.60it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.52it/s]Extractor Predicting: 69it [00:45,  1.54it/s]Extractor Predicting: 70it [00:45,  1.55it/s]Extractor Predicting: 71it [00:46,  1.55it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:47,  1.53it/s]Extractor Predicting: 74it [00:48,  1.52it/s]Extractor Predicting: 75it [00:49,  1.49it/s]Extractor Predicting: 76it [00:49,  1.52it/s]Extractor Predicting: 77it [00:50,  1.55it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:51,  1.54it/s]Extractor Predicting: 80it [00:52,  1.56it/s]Extractor Predicting: 81it [00:52,  1.53it/s]Extractor Predicting: 82it [00:53,  1.55it/s]Extractor Predicting: 83it [00:54,  1.56it/s]Extractor Predicting: 84it [00:54,  1.57it/s]Extractor Predicting: 85it [00:55,  1.59it/s]Extractor Predicting: 86it [00:56,  1.54it/s]Extractor Predicting: 87it [00:56,  1.55it/s]Extractor Predicting: 88it [00:57,  1.58it/s]Extractor Predicting: 89it [00:57,  1.60it/s]Extractor Predicting: 90it [00:58,  1.57it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [00:59,  1.52it/s]Extractor Predicting: 93it [01:00,  1.57it/s]Extractor Predicting: 94it [01:01,  1.59it/s]Extractor Predicting: 95it [01:01,  1.58it/s]Extractor Predicting: 96it [01:02,  1.58it/s]Extractor Predicting: 97it [01:03,  1.60it/s]Extractor Predicting: 98it [01:03,  1.58it/s]Extractor Predicting: 99it [01:04,  1.56it/s]Extractor Predicting: 100it [01:04,  1.57it/s]Extractor Predicting: 101it [01:05,  1.55it/s]Extractor Predicting: 102it [01:06,  1.54it/s]Extractor Predicting: 103it [01:06,  1.55it/s]Extractor Predicting: 104it [01:07,  1.57it/s]Extractor Predicting: 105it [01:08,  1.55it/s]Extractor Predicting: 106it [01:08,  1.54it/s]Extractor Predicting: 107it [01:09,  1.41it/s]Extractor Predicting: 108it [01:10,  1.45it/s]Extractor Predicting: 109it [01:11,  1.48it/s]Extractor Predicting: 110it [01:11,  1.49it/s]Extractor Predicting: 111it [01:12,  1.43it/s]Extractor Predicting: 112it [01:13,  1.49it/s]Extractor Predicting: 113it [01:13,  1.52it/s]Extractor Predicting: 114it [01:14,  1.53it/s]Extractor Predicting: 115it [01:14,  1.53it/s]Extractor Predicting: 116it [01:15,  1.44it/s]Extractor Predicting: 117it [01:16,  1.49it/s]Extractor Predicting: 118it [01:17,  1.48it/s]Extractor Predicting: 119it [01:17,  1.50it/s]Extractor Predicting: 120it [01:18,  1.50it/s]Extractor Predicting: 121it [01:19,  1.46it/s]Extractor Predicting: 122it [01:19,  1.47it/s]Extractor Predicting: 123it [01:20,  1.52it/s]Extractor Predicting: 124it [01:20,  1.54it/s]Extractor Predicting: 125it [01:21,  1.57it/s]Extractor Predicting: 126it [01:22,  1.53it/s]Extractor Predicting: 127it [01:22,  1.53it/s]Extractor Predicting: 128it [01:23,  1.50it/s]Extractor Predicting: 129it [01:24,  1.53it/s]Extractor Predicting: 130it [01:24,  1.58it/s]Extractor Predicting: 131it [01:25,  1.55it/s]Extractor Predicting: 132it [01:26,  1.55it/s]Extractor Predicting: 133it [01:26,  1.53it/s]Extractor Predicting: 134it [01:27,  1.54it/s]Extractor Predicting: 135it [01:28,  1.57it/s]Extractor Predicting: 136it [01:28,  1.57it/s]Extractor Predicting: 137it [01:29,  1.59it/s]Extractor Predicting: 138it [01:30,  1.54it/s]Extractor Predicting: 139it [01:30,  1.53it/s]Extractor Predicting: 140it [01:31,  1.57it/s]Extractor Predicting: 141it [01:31,  1.58it/s]Extractor Predicting: 142it [01:32,  1.61it/s]Extractor Predicting: 143it [01:33,  1.55it/s]Extractor Predicting: 144it [01:33,  1.56it/s]Extractor Predicting: 145it [01:34,  1.55it/s]Extractor Predicting: 146it [01:35,  1.57it/s]Extractor Predicting: 147it [01:35,  1.58it/s]Extractor Predicting: 148it [01:36,  1.51it/s]Extractor Predicting: 149it [01:37,  1.55it/s]Extractor Predicting: 150it [01:37,  1.52it/s]Extractor Predicting: 151it [01:38,  1.54it/s]Extractor Predicting: 152it [01:39,  1.55it/s]Extractor Predicting: 153it [01:39,  1.47it/s]Extractor Predicting: 154it [01:40,  1.50it/s]Extractor Predicting: 155it [01:41,  1.50it/s]Extractor Predicting: 156it [01:41,  1.50it/s]Extractor Predicting: 157it [01:42,  1.51it/s]Extractor Predicting: 158it [01:43,  1.45it/s]Extractor Predicting: 159it [01:43,  1.50it/s]Extractor Predicting: 160it [01:44,  1.55it/s]Extractor Predicting: 161it [01:44,  1.57it/s]Extractor Predicting: 162it [01:45,  1.59it/s]Extractor Predicting: 163it [01:46,  1.47it/s]Extractor Predicting: 163it [01:46,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:37,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:37,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:37,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:37,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:37,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:48:38,004 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:48:38,005 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:48:39,138 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:48:40,163 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:48:40,231 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:45,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:45,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:45,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:45,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:48:45,251 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:48:46,402 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:48:46,403 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:48:47,007 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:48:47,436 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:48:47,436 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.1093044263775971,
  "recall": 0.027867342238599723,
  "score": 0.044411818682327034,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.55it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.61it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:19,  1.60it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:24,  1.59it/s]Extractor Predicting: 41it [00:25,  1.53it/s]Extractor Predicting: 42it [00:26,  1.55it/s]Extractor Predicting: 43it [00:26,  1.54it/s]Extractor Predicting: 44it [00:27,  1.55it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:28,  1.49it/s]Extractor Predicting: 47it [00:29,  1.49it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:30,  1.51it/s]Extractor Predicting: 50it [00:31,  1.50it/s]Extractor Predicting: 51it [00:32,  1.52it/s]Extractor Predicting: 52it [00:32,  1.51it/s]Extractor Predicting: 53it [00:33,  1.50it/s]Extractor Predicting: 54it [00:34,  1.47it/s]Extractor Predicting: 55it [00:35,  1.38it/s]Extractor Predicting: 56it [00:35,  1.46it/s]Extractor Predicting: 57it [00:36,  1.47it/s]Extractor Predicting: 58it [00:37,  1.49it/s]Extractor Predicting: 59it [00:37,  1.47it/s]Extractor Predicting: 60it [00:38,  1.46it/s]Extractor Predicting: 61it [00:39,  1.50it/s]Extractor Predicting: 62it [00:39,  1.51it/s]Extractor Predicting: 63it [00:40,  1.47it/s]Extractor Predicting: 64it [00:41,  1.49it/s]Extractor Predicting: 65it [00:41,  1.41it/s]Extractor Predicting: 66it [00:42,  1.42it/s]Extractor Predicting: 67it [00:43,  1.46it/s]Extractor Predicting: 68it [00:43,  1.46it/s]Extractor Predicting: 69it [00:44,  1.48it/s]Extractor Predicting: 70it [00:45,  1.49it/s]Extractor Predicting: 71it [00:46,  1.41it/s]Extractor Predicting: 72it [00:46,  1.31it/s]Extractor Predicting: 73it [00:47,  1.36it/s]Extractor Predicting: 74it [00:48,  1.43it/s]Extractor Predicting: 75it [00:48,  1.45it/s]Extractor Predicting: 76it [00:49,  1.47it/s]Extractor Predicting: 77it [00:50,  1.52it/s]Extractor Predicting: 78it [00:50,  1.52it/s]Extractor Predicting: 79it [00:51,  1.50it/s]Extractor Predicting: 80it [00:52,  1.54it/s]Extractor Predicting: 81it [00:53,  1.35it/s]Extractor Predicting: 82it [00:53,  1.42it/s]Extractor Predicting: 83it [00:54,  1.47it/s]Extractor Predicting: 84it [00:54,  1.45it/s]Extractor Predicting: 85it [00:55,  1.48it/s]Extractor Predicting: 86it [00:56,  1.46it/s]Extractor Predicting: 87it [00:56,  1.50it/s]Extractor Predicting: 88it [00:57,  1.50it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:58,  1.56it/s]Extractor Predicting: 91it [00:59,  1.49it/s]Extractor Predicting: 92it [01:00,  1.52it/s]Extractor Predicting: 93it [01:00,  1.54it/s]Extractor Predicting: 94it [01:01,  1.54it/s]Extractor Predicting: 95it [01:02,  1.57it/s]Extractor Predicting: 96it [01:02,  1.51it/s]Extractor Predicting: 97it [01:03,  1.54it/s]Extractor Predicting: 98it [01:04,  1.50it/s]Extractor Predicting: 99it [01:04,  1.51it/s]Extractor Predicting: 100it [01:05,  1.52it/s]Extractor Predicting: 101it [01:06,  1.46it/s]Extractor Predicting: 102it [01:06,  1.50it/s]Extractor Predicting: 103it [01:07,  1.49it/s]Extractor Predicting: 104it [01:08,  1.52it/s]Extractor Predicting: 105it [01:08,  1.52it/s]Extractor Predicting: 106it [01:09,  1.42it/s]Extractor Predicting: 107it [01:10,  1.43it/s]Extractor Predicting: 108it [01:10,  1.48it/s]Extractor Predicting: 109it [01:11,  1.44it/s]Extractor Predicting: 110it [01:12,  1.48it/s]Extractor Predicting: 111it [01:13,  1.42it/s]Extractor Predicting: 112it [01:13,  1.46it/s]Extractor Predicting: 113it [01:14,  1.47it/s]Extractor Predicting: 114it [01:15,  1.49it/s]Extractor Predicting: 115it [01:15,  1.50it/s]Extractor Predicting: 116it [01:16,  1.47it/s]Extractor Predicting: 117it [01:17,  1.48it/s]Extractor Predicting: 118it [01:17,  1.53it/s]Extractor Predicting: 119it [01:18,  1.53it/s]Extractor Predicting: 120it [01:18,  1.57it/s]Extractor Predicting: 121it [01:19,  1.56it/s]Extractor Predicting: 122it [01:20,  1.49it/s]Extractor Predicting: 123it [01:20,  1.54it/s]Extractor Predicting: 124it [01:21,  1.58it/s]Extractor Predicting: 125it [01:22,  1.58it/s]Extractor Predicting: 126it [01:22,  1.56it/s]Extractor Predicting: 127it [01:23,  1.50it/s]Extractor Predicting: 128it [01:24,  1.51it/s]Extractor Predicting: 129it [01:24,  1.53it/s]Extractor Predicting: 130it [01:25,  1.53it/s]Extractor Predicting: 131it [01:26,  1.60it/s]Extractor Predicting: 132it [01:26,  1.52it/s]Extractor Predicting: 133it [01:27,  1.54it/s]Extractor Predicting: 134it [01:27,  1.58it/s]Extractor Predicting: 135it [01:28,  1.59it/s]Extractor Predicting: 136it [01:29,  1.60it/s]Extractor Predicting: 137it [01:29,  1.55it/s]Extractor Predicting: 138it [01:30,  1.56it/s]Extractor Predicting: 139it [01:31,  1.58it/s]Extractor Predicting: 140it [01:31,  1.58it/s]Extractor Predicting: 141it [01:32,  1.60it/s]Extractor Predicting: 142it [01:33,  1.52it/s]Extractor Predicting: 143it [01:33,  1.52it/s]Extractor Predicting: 144it [01:34,  1.55it/s]Extractor Predicting: 145it [01:35,  1.52it/s]Extractor Predicting: 146it [01:35,  1.48it/s]Extractor Predicting: 147it [01:36,  1.43it/s]Extractor Predicting: 148it [01:37,  1.45it/s]Extractor Predicting: 149it [01:37,  1.49it/s]Extractor Predicting: 150it [01:38,  1.51it/s]Extractor Predicting: 151it [01:39,  1.53it/s]Extractor Predicting: 152it [01:40,  1.33it/s]Extractor Predicting: 153it [01:40,  1.41it/s]Extractor Predicting: 154it [01:41,  1.46it/s]Extractor Predicting: 155it [01:41,  1.52it/s]Extractor Predicting: 156it [01:42,  1.50it/s]Extractor Predicting: 157it [01:43,  1.51it/s]Extractor Predicting: 158it [01:43,  1.53it/s]Extractor Predicting: 159it [01:44,  1.53it/s]Extractor Predicting: 160it [01:45,  1.57it/s]Extractor Predicting: 161it [01:45,  1.50it/s]Extractor Predicting: 162it [01:46,  1.56it/s]Extractor Predicting: 163it [01:47,  1.58it/s]Extractor Predicting: 164it [01:47,  1.51it/s]Extractor Predicting: 165it [01:48,  1.58it/s]Extractor Predicting: 166it [01:49,  1.60it/s]Extractor Predicting: 167it [01:49,  1.59it/s]Extractor Predicting: 168it [01:50,  1.65it/s]Extractor Predicting: 169it [01:51,  1.48it/s]Extractor Predicting: 170it [01:51,  1.50it/s]Extractor Predicting: 171it [01:52,  1.52it/s]Extractor Predicting: 172it [01:52,  1.55it/s]Extractor Predicting: 173it [01:53,  1.53it/s]Extractor Predicting: 174it [01:54,  1.48it/s]Extractor Predicting: 175it [01:54,  1.52it/s]Extractor Predicting: 176it [01:55,  1.53it/s]Extractor Predicting: 177it [01:56,  1.52it/s]Extractor Predicting: 178it [01:56,  1.54it/s]Extractor Predicting: 179it [01:57,  1.50it/s]Extractor Predicting: 180it [01:58,  1.52it/s]Extractor Predicting: 181it [01:58,  1.55it/s]Extractor Predicting: 182it [01:59,  1.55it/s]Extractor Predicting: 183it [02:00,  1.56it/s]Extractor Predicting: 184it [02:00,  1.41it/s]Extractor Predicting: 185it [02:01,  1.49it/s]Extractor Predicting: 186it [02:02,  1.49it/s]Extractor Predicting: 187it [02:02,  1.52it/s]Extractor Predicting: 188it [02:03,  1.52it/s]Extractor Predicting: 189it [02:04,  1.41it/s]Extractor Predicting: 190it [02:05,  1.42it/s]Extractor Predicting: 191it [02:05,  1.45it/s]Extractor Predicting: 192it [02:06,  1.50it/s]Extractor Predicting: 193it [02:06,  1.55it/s]Extractor Predicting: 194it [02:07,  1.49it/s]Extractor Predicting: 195it [02:08,  1.49it/s]Extractor Predicting: 196it [02:08,  1.53it/s]Extractor Predicting: 197it [02:09,  1.54it/s]Extractor Predicting: 198it [02:10,  1.55it/s]Extractor Predicting: 199it [02:10,  1.50it/s]Extractor Predicting: 200it [02:11,  1.52it/s]Extractor Predicting: 201it [02:12,  1.54it/s]Extractor Predicting: 202it [02:12,  1.55it/s]Extractor Predicting: 203it [02:13,  1.56it/s]Extractor Predicting: 204it [02:14,  1.55it/s]Extractor Predicting: 205it [02:14,  1.55it/s]Extractor Predicting: 206it [02:15,  1.55it/s]Extractor Predicting: 207it [02:16,  1.57it/s]Extractor Predicting: 208it [02:16,  1.58it/s]Extractor Predicting: 209it [02:17,  1.58it/s]Extractor Predicting: 210it [02:17,  1.57it/s]Extractor Predicting: 211it [02:18,  1.47it/s]Extractor Predicting: 212it [02:19,  1.51it/s]Extractor Predicting: 213it [02:19,  1.53it/s]Extractor Predicting: 214it [02:20,  1.54it/s]Extractor Predicting: 215it [02:21,  1.51it/s]Extractor Predicting: 216it [02:22,  1.47it/s]Extractor Predicting: 217it [02:22,  1.52it/s]Extractor Predicting: 218it [02:23,  1.54it/s]Extractor Predicting: 219it [02:23,  1.53it/s]Extractor Predicting: 220it [02:24,  1.55it/s]Extractor Predicting: 221it [02:25,  1.48it/s]Extractor Predicting: 222it [02:25,  1.52it/s]Extractor Predicting: 223it [02:26,  1.47it/s]Extractor Predicting: 224it [02:27,  1.50it/s]Extractor Predicting: 225it [02:27,  1.49it/s]Extractor Predicting: 226it [02:28,  1.47it/s]Extractor Predicting: 227it [02:29,  1.45it/s]Extractor Predicting: 228it [02:30,  1.41it/s]Extractor Predicting: 229it [02:30,  1.44it/s]Extractor Predicting: 230it [02:31,  1.47it/s]Extractor Predicting: 231it [02:32,  1.42it/s]Extractor Predicting: 232it [02:32,  1.45it/s]Extractor Predicting: 233it [02:33,  1.47it/s]Extractor Predicting: 234it [02:34,  1.49it/s]Extractor Predicting: 235it [02:34,  1.50it/s]Extractor Predicting: 236it [02:35,  1.50it/s]Extractor Predicting: 237it [02:36,  1.50it/s]Extractor Predicting: 238it [02:36,  1.57it/s]Extractor Predicting: 239it [02:37,  1.60it/s]Extractor Predicting: 240it [02:37,  1.66it/s]Extractor Predicting: 241it [02:38,  1.62it/s]Extractor Predicting: 242it [02:39,  1.62it/s]Extractor Predicting: 243it [02:39,  1.59it/s]Extractor Predicting: 244it [02:40,  1.60it/s]Extractor Predicting: 245it [02:40,  1.63it/s]Extractor Predicting: 246it [02:41,  1.57it/s]Extractor Predicting: 247it [02:42,  1.59it/s]Extractor Predicting: 248it [02:42,  1.61it/s]Extractor Predicting: 249it [02:43,  1.67it/s]Extractor Predicting: 250it [02:44,  1.66it/s]Extractor Predicting: 251it [02:44,  1.66it/s]Extractor Predicting: 252it [02:45,  1.69it/s]Extractor Predicting: 253it [02:45,  1.68it/s]Extractor Predicting: 254it [02:46,  1.71it/s]Extractor Predicting: 255it [02:47,  1.66it/s]Extractor Predicting: 256it [02:47,  1.71it/s]Extractor Predicting: 257it [02:48,  1.67it/s]Extractor Predicting: 258it [02:48,  1.62it/s]Extractor Predicting: 259it [02:49,  1.58it/s]Extractor Predicting: 260it [02:50,  1.64it/s]Extractor Predicting: 261it [02:50,  1.61it/s]Extractor Predicting: 262it [02:51,  1.62it/s]Extractor Predicting: 263it [02:51,  1.64it/s]Extractor Predicting: 264it [02:52,  1.65it/s]Extractor Predicting: 265it [02:53,  1.46it/s]Extractor Predicting: 266it [02:53,  1.53it/s]Extractor Predicting: 267it [02:54,  1.59it/s]Extractor Predicting: 268it [02:55,  1.57it/s]Extractor Predicting: 269it [02:55,  1.57it/s]Extractor Predicting: 270it [02:56,  1.62it/s]Extractor Predicting: 271it [02:57,  1.62it/s]Extractor Predicting: 272it [02:57,  1.61it/s]Extractor Predicting: 273it [02:58,  1.59it/s]Extractor Predicting: 274it [02:58,  1.62it/s]Extractor Predicting: 275it [02:59,  1.63it/s]Extractor Predicting: 276it [03:00,  1.63it/s]Extractor Predicting: 277it [03:00,  1.63it/s]Extractor Predicting: 278it [03:01,  1.57it/s]Extractor Predicting: 279it [03:02,  1.55it/s]Extractor Predicting: 280it [03:02,  1.63it/s]Extractor Predicting: 281it [03:03,  1.66it/s]Extractor Predicting: 282it [03:03,  1.69it/s]Extractor Predicting: 283it [03:04,  1.67it/s]Extractor Predicting: 284it [03:04,  1.68it/s]Extractor Predicting: 285it [03:05,  1.69it/s]Extractor Predicting: 286it [03:06,  1.70it/s]Extractor Predicting: 287it [03:06,  1.69it/s]Extractor Predicting: 288it [03:07,  1.66it/s]Extractor Predicting: 289it [03:08,  1.61it/s]Extractor Predicting: 290it [03:08,  1.56it/s]Extractor Predicting: 291it [03:09,  1.60it/s]Extractor Predicting: 292it [03:09,  1.64it/s]Extractor Predicting: 293it [03:10,  1.66it/s]Extractor Predicting: 294it [03:11,  1.48it/s]Extractor Predicting: 295it [03:11,  1.54it/s]Extractor Predicting: 296it [03:12,  1.51it/s]Extractor Predicting: 297it [03:13,  1.53it/s]Extractor Predicting: 298it [03:13,  1.49it/s]Extractor Predicting: 299it [03:14,  1.45it/s]Extractor Predicting: 300it [03:15,  1.49it/s]Extractor Predicting: 301it [03:15,  1.52it/s]Extractor Predicting: 302it [03:16,  1.54it/s]Extractor Predicting: 303it [03:17,  1.52it/s]Extractor Predicting: 304it [03:17,  1.52it/s]Extractor Predicting: 305it [03:18,  1.56it/s]Extractor Predicting: 306it [03:19,  1.53it/s]Extractor Predicting: 307it [03:19,  1.44it/s]Extractor Predicting: 308it [03:20,  1.46it/s]Extractor Predicting: 309it [03:21,  1.50it/s]Extractor Predicting: 310it [03:21,  1.54it/s]Extractor Predicting: 311it [03:22,  1.56it/s]Extractor Predicting: 312it [03:23,  1.50it/s]Extractor Predicting: 313it [03:23,  1.49it/s]Extractor Predicting: 314it [03:24,  1.48it/s]Extractor Predicting: 315it [03:25,  1.49it/s]Extractor Predicting: 316it [03:25,  1.49it/s]Extractor Predicting: 317it [03:26,  1.46it/s]Extractor Predicting: 318it [03:27,  1.49it/s]Extractor Predicting: 319it [03:27,  1.51it/s]Extractor Predicting: 320it [03:28,  1.52it/s]Extractor Predicting: 321it [03:29,  1.58it/s]Extractor Predicting: 322it [03:29,  1.55it/s]Extractor Predicting: 323it [03:30,  1.54it/s]Extractor Predicting: 324it [03:31,  1.54it/s]Extractor Predicting: 325it [03:31,  1.53it/s]Extractor Predicting: 326it [03:32,  1.52it/s]Extractor Predicting: 327it [03:33,  1.46it/s]Extractor Predicting: 328it [03:33,  1.48it/s]Extractor Predicting: 329it [03:34,  1.53it/s]Extractor Predicting: 330it [03:35,  1.57it/s]Extractor Predicting: 331it [03:35,  1.74it/s]Extractor Predicting: 331it [03:35,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:47,766 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:47,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:47,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:47,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:47,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:52:48,788 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:52:48,812 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:52:49,794 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:52:51,096 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:52:51,096 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:55,644 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:55,746 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:55,746 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:55,746 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:52:55,746 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:52:56,951 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:52:56,953 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:52:57,672 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:52:58,270 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:52:58,270 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.31369524984577424,
  "recall": 0.12823099230866222,
  "score": 0.18204600375906207,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.38it/s]Extractor Predicting: 17it [00:11,  1.40it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:13,  1.41it/s]Extractor Predicting: 20it [00:13,  1.41it/s]Extractor Predicting: 21it [00:14,  1.42it/s]Extractor Predicting: 22it [00:15,  1.42it/s]Extractor Predicting: 23it [00:15,  1.38it/s]Extractor Predicting: 24it [00:16,  1.40it/s]Extractor Predicting: 25it [00:17,  1.41it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.49it/s]Extractor Predicting: 28it [00:19,  1.47it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:22,  1.49it/s]Extractor Predicting: 34it [00:23,  1.49it/s]Extractor Predicting: 35it [00:23,  1.51it/s]Extractor Predicting: 36it [00:24,  1.52it/s]Extractor Predicting: 37it [00:25,  1.53it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.44it/s]Extractor Predicting: 40it [00:27,  1.47it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:28,  1.52it/s]Extractor Predicting: 43it [00:29,  1.51it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:30,  1.52it/s]Extractor Predicting: 46it [00:31,  1.54it/s]Extractor Predicting: 47it [00:31,  1.45it/s]Extractor Predicting: 48it [00:32,  1.50it/s]Extractor Predicting: 49it [00:33,  1.53it/s]Extractor Predicting: 50it [00:33,  1.56it/s]Extractor Predicting: 51it [00:34,  1.57it/s]Extractor Predicting: 52it [00:35,  1.56it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:37,  1.52it/s]Extractor Predicting: 56it [00:37,  1.60it/s]Extractor Predicting: 57it [00:38,  1.64it/s]Extractor Predicting: 58it [00:38,  1.70it/s]Extractor Predicting: 59it [00:39,  1.78it/s]Extractor Predicting: 60it [00:39,  1.85it/s]Extractor Predicting: 61it [00:40,  1.89it/s]Extractor Predicting: 62it [00:40,  1.88it/s]Extractor Predicting: 63it [00:41,  1.90it/s]Extractor Predicting: 64it [00:41,  1.86it/s]Extractor Predicting: 65it [00:42,  1.86it/s]Extractor Predicting: 66it [00:42,  1.86it/s]Extractor Predicting: 67it [00:43,  1.85it/s]Extractor Predicting: 68it [00:44,  1.88it/s]Extractor Predicting: 69it [00:44,  1.91it/s]Extractor Predicting: 70it [00:45,  1.87it/s]Extractor Predicting: 71it [00:45,  1.87it/s]Extractor Predicting: 72it [00:46,  1.89it/s]Extractor Predicting: 73it [00:46,  1.92it/s]Extractor Predicting: 74it [00:47,  1.94it/s]Extractor Predicting: 75it [00:47,  1.92it/s]Extractor Predicting: 76it [00:48,  1.90it/s]Extractor Predicting: 77it [00:48,  1.85it/s]Extractor Predicting: 78it [00:49,  1.82it/s]Extractor Predicting: 79it [00:49,  1.84it/s]Extractor Predicting: 80it [00:50,  1.84it/s]Extractor Predicting: 81it [00:51,  1.84it/s]Extractor Predicting: 82it [00:51,  1.88it/s]Extractor Predicting: 83it [00:52,  1.77it/s]Extractor Predicting: 84it [00:52,  1.81it/s]Extractor Predicting: 85it [00:53,  1.85it/s]Extractor Predicting: 86it [00:53,  1.74it/s]Extractor Predicting: 87it [00:54,  1.70it/s]Extractor Predicting: 88it [00:55,  1.67it/s]Extractor Predicting: 89it [00:55,  1.66it/s]Extractor Predicting: 90it [00:56,  1.67it/s]Extractor Predicting: 91it [00:56,  1.65it/s]Extractor Predicting: 92it [00:57,  1.61it/s]Extractor Predicting: 93it [00:58,  1.62it/s]Extractor Predicting: 94it [00:58,  1.61it/s]Extractor Predicting: 95it [00:59,  1.65it/s]Extractor Predicting: 96it [00:59,  1.66it/s]Extractor Predicting: 97it [01:00,  1.66it/s]Extractor Predicting: 98it [01:01,  1.67it/s]Extractor Predicting: 99it [01:01,  1.64it/s]Extractor Predicting: 100it [01:02,  1.53it/s]Extractor Predicting: 101it [01:03,  1.57it/s]Extractor Predicting: 102it [01:03,  1.58it/s]Extractor Predicting: 103it [01:04,  1.55it/s]Extractor Predicting: 104it [01:05,  1.53it/s]Extractor Predicting: 105it [01:05,  1.53it/s]Extractor Predicting: 106it [01:06,  1.52it/s]Extractor Predicting: 107it [01:07,  1.36it/s]Extractor Predicting: 108it [01:08,  1.31it/s]Extractor Predicting: 108it [01:08,  1.58it/s]
[INFO|configuration_utils.py:515] 2023-08-28 22:54:15,020 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:54:15,022 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:54:15,087 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:54:15,088 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 22:54:15,173 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:54:44,854 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 22:54:44,884 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 22:54:45,090 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:54:45,091 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:54:45,161 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:54:45,320 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:54:45,320 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:54:45,320 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:54:45,320 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:54:45,320 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:54:45,320 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.560530679933665,
  "recall": 0.10828127502803139,
  "score": 0.18150087260034903,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 22:54:46,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:46,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:47,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:47,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:48,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:49,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:50,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:50,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:51,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:51,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:52,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:52,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:53,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:54,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:54,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:55,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:56,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:56,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:57,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:57,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:58,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:04, 13.19s/it][WARNING|generation_utils.py:914] 2023-08-28 22:54:59,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:54:59,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:00,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:01,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:01,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:02,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:03,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:04,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:04,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:05,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:05,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:06,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:06,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:07,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:08,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:09,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:09,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:10,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:10,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:11,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:49, 13.02s/it][WARNING|generation_utils.py:914] 2023-08-28 22:55:12,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:12,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:13,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:13,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:14,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:15,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:15,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:16,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:16,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:17,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:17,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:18,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:19,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:19,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:20,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:21,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:21,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:22,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:22,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:23,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:24,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:36, 13.01s/it][WARNING|generation_utils.py:914] 2023-08-28 22:55:25,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:25,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:26,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:26,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:27,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:28,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:28,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:29,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:29,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:30,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:30,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:31,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:32,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:32,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:33,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:33,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:34,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:35,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:35,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:36,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:50<02:17, 12.50s/it][WARNING|generation_utils.py:914] 2023-08-28 22:55:36,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:37,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:38,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:38,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:39,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:39,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:40,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:41,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:41,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:42,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:42,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:43,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:44,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:44,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:45,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:45,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:46,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:46,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:47,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:48,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:48,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:03<02:03, 12.39s/it][WARNING|generation_utils.py:914] 2023-08-28 22:55:49,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:49,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:50,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:50,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:51,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:51,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:52,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:53,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:53,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:54,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:55,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:55,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:56,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:56,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:57,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:58,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:58,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:59,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:55:59,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:00,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:01,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:15<01:52, 12.50s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:01,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:02,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:03,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:03,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:04,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:04,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:05,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:06,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:06,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:07,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:07,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:08,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:08,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:09,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:10,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:10,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:11,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:11,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:12,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:13,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:27<01:38, 12.31s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:13,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:14,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:14,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:15,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:15,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:16,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:16,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:17,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:17,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:18,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:19,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:19,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:20,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:20,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:21,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:22,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:22,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:23,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:23,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:24,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:24,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:39<01:24, 12.10s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:25,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:25,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:26,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:27,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:27,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:28,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:28,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:29,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:29,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:30,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:30,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:31,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:31,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:32,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:32,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:33,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:33,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:34,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:34,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:35,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:35,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:50<01:10, 11.81s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:36,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:37,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:37,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:38,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:38,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:39,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:39,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:40,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:41,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:41,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:42,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:42,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:43,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:44,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:44,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:45,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:45,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:46,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:47,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:47,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:02<00:58, 11.77s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:48,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:48,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:49,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:49,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:50,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:50,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:51,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:52,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:52,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:53,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:53,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:54,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:54,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:55,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:55,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:56,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:56,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:57,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:58,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:58,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:56:59,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:13<00:46, 11.72s/it][WARNING|generation_utils.py:914] 2023-08-28 22:56:59,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:00,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:01,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:01,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:02,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:02,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:03,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:04,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:04,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:05,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:05,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:06,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:07,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:07,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:08,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:09,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:09,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:10,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:10,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:11,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:11,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:26<00:36, 12.01s/it][WARNING|generation_utils.py:914] 2023-08-28 22:57:12,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:13,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:13,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:14,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:14,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:15,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:15,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:16,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:16,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:17,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:18,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:18,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:19,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:19,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:20,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:20,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:21,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:21,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:22,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:23,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:23,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:38<00:23, 11.95s/it][WARNING|generation_utils.py:914] 2023-08-28 22:57:24,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:24,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:25,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:25,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:26,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:26,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:27,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:28,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:28,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:29,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:29,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:30,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:30,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:31,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:31,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:32,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:32,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:33,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:33,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:34,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:34,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:49<00:11, 11.70s/it][WARNING|generation_utils.py:914] 2023-08-28 22:57:35,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:35,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:36,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:37,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:37,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:38,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:38,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:39,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:40,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:40,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:41,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:41,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:42,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:42,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:43,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:43,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:44,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:45,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:45,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:57:46,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:00<00:00, 11.66s/it]Generating: 100%|██████████| 15/15 [03:00<00:00, 12.06s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:57:58,474 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:57:58,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:57:58,523 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:57:58,523 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:57:58,523 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:57:59,463 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:57:59,464 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:58:00,832 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:58:02,111 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:58:02,186 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:08,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:08,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:08,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:08,725 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:58:08,725 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:58:11,142 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:58:11,143 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:58:12,518 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:58:12,953 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:58:12,953 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : made from material .', 'success_rate': 0.971875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9330357142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9640625, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.90625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9484375, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 8069
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8169, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.50it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:02,  1.42it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:04,  1.36it/s]Extractor Estimating: 7it [00:04,  1.45it/s]Extractor Estimating: 8it [00:05,  1.55it/s]Extractor Estimating: 9it [00:05,  1.56it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.54it/s]Extractor Estimating: 12it [00:07,  1.55it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:09,  1.61it/s]Extractor Estimating: 15it [00:09,  1.60it/s]Extractor Estimating: 16it [00:10,  1.61it/s]Extractor Estimating: 17it [00:10,  1.62it/s]Extractor Estimating: 18it [00:11,  1.64it/s]Extractor Estimating: 19it [00:12,  1.67it/s]Extractor Estimating: 20it [00:12,  1.67it/s]Extractor Estimating: 21it [00:13,  1.63it/s]Extractor Estimating: 22it [00:14,  1.59it/s]Extractor Estimating: 23it [00:14,  1.58it/s]Extractor Estimating: 24it [00:15,  1.57it/s]Extractor Estimating: 25it [00:15,  1.54it/s]Extractor Estimating: 26it [00:16,  1.54it/s]Extractor Estimating: 27it [00:17,  1.53it/s]Extractor Estimating: 28it [00:17,  1.52it/s]Extractor Estimating: 29it [00:18,  1.57it/s]Extractor Estimating: 30it [00:19,  1.50it/s]Extractor Estimating: 31it [00:19,  1.54it/s]Extractor Estimating: 32it [00:20,  1.53it/s]Extractor Estimating: 33it [00:21,  1.63it/s]Extractor Estimating: 34it [00:21,  1.66it/s]Extractor Estimating: 35it [00:22,  1.65it/s]Extractor Estimating: 36it [00:23,  1.50it/s]Extractor Estimating: 37it [00:23,  1.52it/s]Extractor Estimating: 38it [00:24,  1.52it/s]Extractor Estimating: 39it [00:24,  1.59it/s]Extractor Estimating: 40it [00:25,  1.59it/s]Extractor Estimating: 41it [00:26,  1.57it/s]Extractor Estimating: 42it [00:26,  1.55it/s]Extractor Estimating: 43it [00:27,  1.54it/s]Extractor Estimating: 44it [00:28,  1.46it/s]Extractor Estimating: 45it [00:29,  1.46it/s]Extractor Estimating: 46it [00:29,  1.52it/s]Extractor Estimating: 47it [00:30,  1.52it/s]Extractor Estimating: 48it [00:30,  1.57it/s]Extractor Estimating: 49it [00:31,  1.58it/s]Extractor Estimating: 50it [00:32,  1.59it/s]Extractor Estimating: 51it [00:32,  1.63it/s]Extractor Estimating: 52it [00:33,  1.67it/s]Extractor Estimating: 53it [00:33,  1.67it/s]Extractor Estimating: 54it [00:34,  1.71it/s]Extractor Estimating: 55it [00:34,  1.75it/s]Extractor Estimating: 56it [00:35,  1.78it/s]Extractor Estimating: 57it [00:36,  1.80it/s]Extractor Estimating: 58it [00:36,  1.77it/s]Extractor Estimating: 59it [00:37,  1.78it/s]Extractor Estimating: 60it [00:37,  1.79it/s]Extractor Estimating: 61it [00:38,  1.60it/s]Extractor Estimating: 62it [00:39,  1.63it/s]Extractor Estimating: 63it [00:39,  1.67it/s]Extractor Estimating: 64it [00:40,  1.65it/s]Extractor Estimating: 65it [00:40,  1.68it/s]Extractor Estimating: 66it [00:41,  1.68it/s]Extractor Estimating: 67it [00:42,  1.64it/s]Extractor Estimating: 68it [00:42,  1.72it/s]Extractor Estimating: 69it [00:43,  1.74it/s]Extractor Estimating: 70it [00:43,  1.72it/s]Extractor Estimating: 71it [00:44,  1.71it/s]Extractor Estimating: 72it [00:44,  1.73it/s]Extractor Estimating: 73it [00:45,  1.76it/s]Extractor Estimating: 74it [00:45,  1.80it/s]Extractor Estimating: 75it [00:46,  1.69it/s]Extractor Estimating: 76it [00:47,  1.71it/s]Extractor Estimating: 77it [00:47,  1.70it/s]Extractor Estimating: 78it [00:48,  1.71it/s]Extractor Estimating: 79it [00:48,  1.70it/s]Extractor Estimating: 80it [00:49,  1.49it/s]Extractor Estimating: 81it [00:50,  1.53it/s]Extractor Estimating: 82it [00:51,  1.57it/s]Extractor Estimating: 83it [00:51,  1.62it/s]Extractor Estimating: 84it [00:52,  1.61it/s]Extractor Estimating: 85it [00:53,  1.50it/s]Extractor Estimating: 86it [00:53,  1.53it/s]Extractor Estimating: 87it [00:54,  1.60it/s]Extractor Estimating: 88it [00:54,  1.64it/s]Extractor Estimating: 89it [00:55,  1.62it/s]Extractor Estimating: 90it [00:56,  1.55it/s]Extractor Estimating: 91it [00:56,  1.59it/s]Extractor Estimating: 92it [00:57,  1.68it/s]Extractor Estimating: 93it [00:57,  1.71it/s]Extractor Estimating: 94it [00:58,  1.69it/s]Extractor Estimating: 95it [00:59,  1.69it/s]Extractor Estimating: 96it [00:59,  1.61it/s]Extractor Estimating: 97it [01:00,  1.68it/s]Extractor Estimating: 98it [01:00,  1.64it/s]Extractor Estimating: 99it [01:01,  1.68it/s]Extractor Estimating: 100it [01:02,  1.71it/s]Extractor Estimating: 101it [01:02,  1.76it/s]Extractor Estimating: 102it [01:03,  1.77it/s]Extractor Estimating: 103it [01:03,  1.76it/s]Extractor Estimating: 104it [01:04,  1.76it/s]Extractor Estimating: 105it [01:04,  1.74it/s]Extractor Estimating: 106it [01:05,  1.69it/s]Extractor Estimating: 107it [01:06,  1.72it/s]Extractor Estimating: 108it [01:06,  1.68it/s]Extractor Estimating: 109it [01:07,  1.68it/s]Extractor Estimating: 110it [01:07,  1.78it/s]Extractor Estimating: 111it [01:08,  1.81it/s]Extractor Estimating: 112it [01:08,  1.70it/s]Extractor Estimating: 113it [01:09,  1.74it/s]Extractor Estimating: 114it [01:09,  1.78it/s]Extractor Estimating: 115it [01:10,  1.78it/s]Extractor Estimating: 116it [01:11,  1.70it/s]Extractor Estimating: 117it [01:11,  1.75it/s]Extractor Estimating: 118it [01:12,  1.71it/s]Extractor Estimating: 119it [01:12,  1.74it/s]Extractor Estimating: 120it [01:13,  1.77it/s]Extractor Estimating: 121it [01:14,  1.72it/s]Extractor Estimating: 122it [01:14,  1.74it/s]Extractor Estimating: 123it [01:15,  1.75it/s]Extractor Estimating: 124it [01:15,  1.69it/s]Extractor Estimating: 125it [01:16,  1.75it/s]Extractor Estimating: 126it [01:17,  1.68it/s]Extractor Estimating: 127it [01:17,  1.68it/s]Extractor Estimating: 128it [01:18,  1.68it/s]Extractor Estimating: 129it [01:18,  1.64it/s]Extractor Estimating: 130it [01:19,  1.67it/s]Extractor Estimating: 131it [01:20,  1.64it/s]Extractor Estimating: 132it [01:20,  1.49it/s]Extractor Estimating: 133it [01:21,  1.55it/s]Extractor Estimating: 134it [01:22,  1.58it/s]Extractor Estimating: 135it [01:22,  1.55it/s]Extractor Estimating: 136it [01:23,  1.61it/s]Extractor Estimating: 137it [01:23,  1.56it/s]Extractor Estimating: 138it [01:24,  1.61it/s]Extractor Estimating: 139it [01:25,  1.63it/s]Extractor Estimating: 140it [01:25,  1.64it/s]Extractor Estimating: 141it [01:26,  1.66it/s]Extractor Estimating: 142it [01:26,  1.67it/s]Extractor Estimating: 143it [01:27,  1.61it/s]Extractor Estimating: 144it [01:28,  1.57it/s]Extractor Estimating: 145it [01:28,  1.61it/s]Extractor Estimating: 146it [01:29,  1.57it/s]Extractor Estimating: 147it [01:30,  1.59it/s]Extractor Estimating: 148it [01:30,  1.59it/s]Extractor Estimating: 149it [01:31,  1.65it/s]Extractor Estimating: 150it [01:31,  1.62it/s]Extractor Estimating: 151it [01:32,  1.59it/s]Extractor Estimating: 152it [01:33,  1.55it/s]Extractor Estimating: 153it [01:33,  1.57it/s]Extractor Estimating: 154it [01:34,  1.58it/s]Extractor Estimating: 155it [01:35,  1.63it/s]Extractor Estimating: 156it [01:35,  1.63it/s]Extractor Estimating: 157it [01:36,  1.57it/s]Extractor Estimating: 158it [01:37,  1.59it/s]Extractor Estimating: 159it [01:37,  1.62it/s]Extractor Estimating: 160it [01:38,  1.68it/s]Extractor Estimating: 161it [01:38,  1.71it/s]Extractor Estimating: 162it [01:39,  1.64it/s]Extractor Estimating: 163it [01:39,  1.65it/s]Extractor Estimating: 164it [01:40,  1.61it/s]Extractor Estimating: 165it [01:41,  1.64it/s]Extractor Estimating: 166it [01:41,  1.64it/s]Extractor Estimating: 167it [01:42,  1.65it/s]Extractor Estimating: 168it [01:43,  1.67it/s]Extractor Estimating: 169it [01:43,  1.69it/s]Extractor Estimating: 170it [01:44,  1.71it/s]Extractor Estimating: 171it [01:44,  1.63it/s]Extractor Estimating: 172it [01:45,  1.67it/s]Extractor Estimating: 173it [01:46,  1.64it/s]Extractor Estimating: 174it [01:46,  1.60it/s]Extractor Estimating: 175it [01:47,  1.62it/s]Extractor Estimating: 176it [01:47,  1.66it/s]Extractor Estimating: 177it [01:48,  1.69it/s]Extractor Estimating: 178it [01:48,  1.73it/s]Extractor Estimating: 179it [01:49,  1.55it/s]Extractor Estimating: 180it [01:50,  1.62it/s]Extractor Estimating: 181it [01:50,  1.67it/s]Extractor Estimating: 182it [01:51,  1.69it/s]Extractor Estimating: 183it [01:52,  1.69it/s]Extractor Estimating: 184it [01:52,  1.70it/s]Extractor Estimating: 185it [01:53,  1.67it/s]Extractor Estimating: 186it [01:53,  1.71it/s]Extractor Estimating: 187it [01:54,  1.63it/s]Extractor Estimating: 188it [01:55,  1.53it/s]Extractor Estimating: 189it [01:55,  1.59it/s]Extractor Estimating: 190it [01:56,  1.66it/s]Extractor Estimating: 191it [01:57,  1.52it/s]Extractor Estimating: 192it [01:57,  1.50it/s]Extractor Estimating: 193it [01:58,  1.58it/s]Extractor Estimating: 194it [01:58,  1.60it/s]Extractor Estimating: 195it [01:59,  1.63it/s]Extractor Estimating: 196it [02:00,  1.56it/s]Extractor Estimating: 197it [02:00,  1.63it/s]Extractor Estimating: 198it [02:01,  1.65it/s]Extractor Estimating: 199it [02:02,  1.65it/s]Extractor Estimating: 200it [02:02,  1.70it/s]Extractor Estimating: 201it [02:03,  1.79it/s]Extractor Estimating: 202it [02:03,  1.85it/s]Extractor Estimating: 203it [02:04,  1.90it/s]Extractor Estimating: 204it [02:04,  1.96it/s]Extractor Estimating: 205it [02:05,  1.89it/s]Extractor Estimating: 206it [02:05,  1.96it/s]Extractor Estimating: 207it [02:06,  2.03it/s]Extractor Estimating: 208it [02:06,  2.02it/s]Extractor Estimating: 209it [02:07,  1.95it/s]Extractor Estimating: 210it [02:07,  1.98it/s]Extractor Estimating: 211it [02:08,  1.99it/s]Extractor Estimating: 212it [02:08,  2.00it/s]Extractor Estimating: 213it [02:09,  1.96it/s]Extractor Estimating: 214it [02:09,  1.93it/s]Extractor Estimating: 215it [02:10,  1.78it/s]Extractor Estimating: 216it [02:10,  1.86it/s]Extractor Estimating: 217it [02:11,  1.92it/s]Extractor Estimating: 218it [02:11,  1.92it/s]Extractor Estimating: 219it [02:12,  1.95it/s]Extractor Estimating: 220it [02:12,  1.73it/s]Extractor Estimating: 221it [02:13,  1.76it/s]Extractor Estimating: 222it [02:14,  1.83it/s]Extractor Estimating: 223it [02:14,  1.86it/s]Extractor Estimating: 224it [02:15,  1.85it/s]Extractor Estimating: 225it [02:15,  1.84it/s]Extractor Estimating: 226it [02:16,  1.82it/s]Extractor Estimating: 227it [02:16,  1.77it/s]Extractor Estimating: 228it [02:17,  1.86it/s]Extractor Estimating: 229it [02:17,  1.89it/s]Extractor Estimating: 230it [02:18,  1.86it/s]Extractor Estimating: 231it [02:18,  1.79it/s]Extractor Estimating: 232it [02:19,  1.82it/s]Extractor Estimating: 233it [02:20,  1.80it/s]Extractor Estimating: 234it [02:20,  1.51it/s]Extractor Estimating: 235it [02:21,  1.62it/s]Extractor Estimating: 236it [02:21,  1.70it/s]Extractor Estimating: 237it [02:22,  1.82it/s]Extractor Estimating: 238it [02:22,  1.83it/s]Extractor Estimating: 239it [02:23,  1.82it/s]Extractor Estimating: 240it [02:24,  1.84it/s]Extractor Estimating: 241it [02:24,  1.86it/s]Extractor Estimating: 242it [02:25,  1.90it/s]Extractor Estimating: 243it [02:25,  1.74it/s]Extractor Estimating: 244it [02:26,  1.79it/s]Extractor Estimating: 245it [02:26,  1.78it/s]Extractor Estimating: 246it [02:27,  1.74it/s]Extractor Estimating: 247it [02:28,  1.74it/s]Extractor Estimating: 248it [02:28,  1.79it/s]Extractor Estimating: 249it [02:29,  1.83it/s]Extractor Estimating: 250it [02:29,  1.82it/s]Extractor Estimating: 251it [02:30,  1.81it/s]Extractor Estimating: 252it [02:30,  1.75it/s]Extractor Estimating: 253it [02:31,  1.74it/s]Extractor Estimating: 254it [02:32,  1.70it/s]Extractor Estimating: 255it [02:32,  1.67it/s]Extractor Estimating: 256it [02:33,  1.68it/s]Extractor Estimating: 257it [02:33,  1.67it/s]Extractor Estimating: 258it [02:34,  1.66it/s]Extractor Estimating: 259it [02:35,  1.67it/s]Extractor Estimating: 260it [02:35,  1.69it/s]Extractor Estimating: 261it [02:36,  1.66it/s]Extractor Estimating: 262it [02:36,  1.65it/s]Extractor Estimating: 263it [02:37,  1.70it/s]Extractor Estimating: 264it [02:38,  1.64it/s]Extractor Estimating: 265it [02:38,  1.68it/s]Extractor Estimating: 266it [02:39,  1.70it/s]Extractor Estimating: 267it [02:39,  1.72it/s]Extractor Estimating: 268it [02:40,  1.73it/s]Extractor Estimating: 269it [02:41,  1.64it/s]Extractor Estimating: 270it [02:41,  1.67it/s]Extractor Estimating: 271it [02:42,  1.70it/s]Extractor Estimating: 272it [02:42,  1.69it/s]Extractor Estimating: 273it [02:43,  1.55it/s]Extractor Estimating: 274it [02:44,  1.55it/s]Extractor Estimating: 275it [02:44,  1.59it/s]Extractor Estimating: 276it [02:45,  1.63it/s]Extractor Estimating: 277it [02:45,  1.65it/s]Extractor Estimating: 278it [02:46,  1.58it/s]Extractor Estimating: 279it [02:47,  1.63it/s]Extractor Estimating: 280it [02:47,  1.69it/s]Extractor Estimating: 281it [02:48,  1.72it/s]Extractor Estimating: 282it [02:48,  1.74it/s]Extractor Estimating: 283it [02:49,  1.68it/s]Extractor Estimating: 284it [02:50,  1.72it/s]Extractor Estimating: 285it [02:50,  1.76it/s]Extractor Estimating: 286it [02:51,  1.76it/s]Extractor Estimating: 287it [02:51,  1.73it/s]Extractor Estimating: 288it [02:52,  1.69it/s]Extractor Estimating: 289it [02:52,  1.72it/s]Extractor Estimating: 290it [02:53,  1.73it/s]Extractor Estimating: 291it [02:54,  1.65it/s]Extractor Estimating: 292it [02:54,  1.65it/s]Extractor Estimating: 293it [02:55,  1.66it/s]Extractor Estimating: 294it [02:55,  1.68it/s]Extractor Estimating: 295it [02:56,  1.69it/s]Extractor Estimating: 296it [02:57,  1.48it/s]Extractor Estimating: 297it [02:57,  1.54it/s]Extractor Estimating: 298it [02:58,  1.59it/s]Extractor Estimating: 299it [02:59,  1.63it/s]Extractor Estimating: 300it [02:59,  1.68it/s]Extractor Estimating: 301it [03:00,  1.74it/s]Extractor Estimating: 302it [03:00,  1.70it/s]Extractor Estimating: 303it [03:01,  1.76it/s]Extractor Estimating: 304it [03:01,  1.80it/s]Extractor Estimating: 305it [03:02,  1.68it/s]Extractor Estimating: 306it [03:03,  1.75it/s]Extractor Estimating: 307it [03:03,  1.75it/s]Extractor Estimating: 308it [03:04,  1.65it/s]Extractor Estimating: 309it [03:04,  1.63it/s]Extractor Estimating: 310it [03:05,  1.69it/s]Extractor Estimating: 311it [03:06,  1.73it/s]Extractor Estimating: 312it [03:06,  1.75it/s]Extractor Estimating: 313it [03:07,  1.72it/s]Extractor Estimating: 314it [03:07,  1.67it/s]Extractor Estimating: 315it [03:08,  1.73it/s]Extractor Estimating: 316it [03:08,  1.75it/s]Extractor Estimating: 317it [03:09,  1.78it/s]Extractor Estimating: 318it [03:10,  1.77it/s]Extractor Estimating: 319it [03:10,  1.84it/s]Extractor Estimating: 320it [03:11,  1.81it/s]Extractor Estimating: 321it [03:11,  1.87it/s]Extractor Estimating: 322it [03:12,  1.86it/s]Extractor Estimating: 323it [03:12,  1.76it/s]Extractor Estimating: 324it [03:13,  1.73it/s]Extractor Estimating: 325it [03:13,  1.81it/s]Extractor Estimating: 326it [03:14,  1.77it/s]Extractor Estimating: 327it [03:14,  1.84it/s]Extractor Estimating: 328it [03:15,  1.93it/s]Extractor Estimating: 329it [03:16,  1.77it/s]Extractor Estimating: 330it [03:16,  1.91it/s]Extractor Estimating: 331it [03:17,  1.98it/s]Extractor Estimating: 332it [03:17,  1.91it/s]Extractor Estimating: 333it [03:18,  1.93it/s]Extractor Estimating: 334it [03:18,  1.94it/s]Extractor Estimating: 335it [03:19,  1.95it/s]Extractor Estimating: 336it [03:19,  1.84it/s]Extractor Estimating: 337it [03:20,  1.90it/s]Extractor Estimating: 338it [03:20,  1.92it/s]Extractor Estimating: 339it [03:21,  2.00it/s]Extractor Estimating: 340it [03:21,  1.95it/s]Extractor Estimating: 341it [03:22,  1.98it/s]Extractor Estimating: 342it [03:22,  1.96it/s]Extractor Estimating: 343it [03:23,  2.01it/s]Extractor Estimating: 344it [03:23,  2.03it/s]Extractor Estimating: 345it [03:24,  2.03it/s]Extractor Estimating: 346it [03:24,  1.93it/s]Extractor Estimating: 347it [03:25,  2.06it/s]Extractor Estimating: 348it [03:25,  2.09it/s]Extractor Estimating: 349it [03:26,  2.05it/s]Extractor Estimating: 350it [03:26,  1.93it/s]Extractor Estimating: 351it [03:27,  1.84it/s]Extractor Estimating: 352it [03:27,  1.75it/s]Extractor Estimating: 353it [03:28,  1.76it/s]Extractor Estimating: 354it [03:29,  1.70it/s]Extractor Estimating: 355it [03:29,  1.74it/s]Extractor Estimating: 356it [03:30,  1.75it/s]Extractor Estimating: 357it [03:30,  1.76it/s]Extractor Estimating: 358it [03:31,  1.79it/s]Extractor Estimating: 359it [03:31,  1.82it/s]Extractor Estimating: 360it [03:32,  1.84it/s]Extractor Estimating: 361it [03:33,  1.74it/s]Extractor Estimating: 362it [03:33,  1.75it/s]Extractor Estimating: 363it [03:34,  1.75it/s]Extractor Estimating: 364it [03:34,  1.76it/s]Extractor Estimating: 365it [03:35,  1.81it/s]Extractor Estimating: 366it [03:35,  1.81it/s]Extractor Estimating: 367it [03:36,  1.83it/s]Extractor Estimating: 368it [03:36,  1.80it/s]Extractor Estimating: 369it [03:37,  1.78it/s]Extractor Estimating: 370it [03:38,  1.72it/s]Extractor Estimating: 371it [03:38,  1.73it/s]Extractor Estimating: 372it [03:39,  1.71it/s]Extractor Estimating: 373it [03:39,  1.70it/s]Extractor Estimating: 374it [03:40,  1.72it/s]Extractor Estimating: 375it [03:40,  2.07it/s]Extractor Estimating: 375it [03:40,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:23,946 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:24,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:24,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:24,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:24,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:02:25,723 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:02:25,724 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:02:26,949 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:02:28,114 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:02:28,115 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:32,682 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:32,731 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:32,732 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:32,732 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:02:32,732 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:02:34,303 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:02:34,304 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:02:35,024 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:02:35,271 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:02:35,271 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:47:38,729 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:47:38,768 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 5991 mean pseudo reward: 0.9587008383297119
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 16038
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16138, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16138, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.990, loss:300.5186
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.974, loss:312.7741
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 50, avg_time 0.966, loss:289.9244
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 150, avg_time 0.970, loss:284.4421
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 250, avg_time 0.967, loss:289.7641
>> valid entity prec:0.4580, rec:0.3620, f1:0.4044
>> valid relation prec:0.0507, rec:0.0154, f1:0.0237
>> valid relation with NER prec:0.0507, rec:0.0154, f1:0.0237
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 0.973, loss:275.3191
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 0.968, loss:283.9880
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 0.973, loss:280.1298
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 0.972, loss:286.1082
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 0.968, loss:278.2146
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4435, rec:0.3961, f1:0.4185
>> valid relation prec:0.0755, rec:0.0217, f1:0.0337
>> valid relation with NER prec:0.0755, rec:0.0217, f1:0.0337
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 100, avg_time 0.974, loss:277.8879
g_step 1200, step 200, avg_time 0.972, loss:272.8290
g_step 1300, step 50, avg_time 0.965, loss:271.9249
g_step 1400, step 150, avg_time 0.979, loss:266.2889
g_step 1500, step 250, avg_time 0.972, loss:258.7135
>> valid entity prec:0.4301, rec:0.3192, f1:0.3665
>> valid relation prec:0.0541, rec:0.0150, f1:0.0235
>> valid relation with NER prec:0.0541, rec:0.0150, f1:0.0235
g_step 1600, step 100, avg_time 0.970, loss:238.7592
g_step 1700, step 200, avg_time 0.968, loss:248.3262
g_step 1800, step 50, avg_time 0.971, loss:247.0991
g_step 1900, step 150, avg_time 0.969, loss:256.2815
g_step 2000, step 250, avg_time 0.981, loss:237.2316
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4491, rec:0.4234, f1:0.4358
>> valid relation prec:0.0578, rec:0.0205, f1:0.0303
>> valid relation with NER prec:0.0578, rec:0.0205, f1:0.0303
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 100, avg_time 0.980, loss:226.7536
g_step 2200, step 200, avg_time 0.977, loss:221.9597
g_step 2300, step 50, avg_time 0.979, loss:222.3364
g_step 2400, step 150, avg_time 0.964, loss:206.4555
g_step 2500, step 250, avg_time 0.959, loss:223.6491
>> valid entity prec:0.4370, rec:0.3791, f1:0.4060
>> valid relation prec:0.0511, rec:0.0148, f1:0.0229
>> valid relation with NER prec:0.0511, rec:0.0148, f1:0.0229
g_step 2600, step 100, avg_time 0.964, loss:205.9825
g_step 2700, step 200, avg_time 0.992, loss:219.8334
g_step 2800, step 50, avg_time 0.962, loss:198.0234
g_step 2900, step 150, avg_time 0.962, loss:201.5651
g_step 3000, step 250, avg_time 0.985, loss:208.8912
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4434, rec:0.3925, f1:0.4164
>> valid relation prec:0.0577, rec:0.0210, f1:0.0308
>> valid relation with NER prec:0.0577, rec:0.0210, f1:0.0308
g_step 3100, step 100, avg_time 0.966, loss:191.4539
g_step 3200, step 200, avg_time 0.970, loss:195.0596
g_step 3300, step 50, avg_time 0.977, loss:193.8246
g_step 3400, step 150, avg_time 0.968, loss:187.6947
g_step 3500, step 250, avg_time 0.973, loss:195.7220
>> valid entity prec:0.4346, rec:0.3663, f1:0.3976
>> valid relation prec:0.0510, rec:0.0182, f1:0.0268
>> valid relation with NER prec:0.0510, rec:0.0182, f1:0.0268
g_step 3600, step 100, avg_time 0.973, loss:177.5258
g_step 3700, step 200, avg_time 0.976, loss:182.5185
g_step 3800, step 50, avg_time 0.972, loss:177.8088
g_step 3900, step 150, avg_time 0.977, loss:168.4487
g_step 4000, step 250, avg_time 0.962, loss:177.1643
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4559, rec:0.4138, f1:0.4338
>> valid relation prec:0.0641, rec:0.0212, f1:0.0319
>> valid relation with NER prec:0.0641, rec:0.0212, f1:0.0319
g_step 4100, step 100, avg_time 0.975, loss:161.4028
g_step 4200, step 200, avg_time 0.979, loss:175.0885
g_step 4300, step 50, avg_time 0.958, loss:155.8300
g_step 4400, step 150, avg_time 0.975, loss:154.5464
g_step 4500, step 250, avg_time 0.981, loss:167.5599
>> valid entity prec:0.3454, rec:0.4327, f1:0.3842
>> valid relation prec:0.0544, rec:0.0198, f1:0.0291
>> valid relation with NER prec:0.0544, rec:0.0198, f1:0.0291
g_step 4600, step 100, avg_time 0.969, loss:153.1790
g_step 4700, step 200, avg_time 0.972, loss:167.4098
g_step 4800, step 50, avg_time 0.968, loss:158.6149
g_step 4900, step 150, avg_time 0.981, loss:145.6609
g_step 5000, step 250, avg_time 0.960, loss:154.5864
learning rate was adjusted to 0.0008
>> valid entity prec:0.4346, rec:0.3498, f1:0.3876
>> valid relation prec:0.0582, rec:0.0191, f1:0.0288
>> valid relation with NER prec:0.0582, rec:0.0191, f1:0.0288
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:47:38 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:47:38 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-47-38_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:47:40 - WARNING - datasets.builder -   Using custom data configuration default-c3f91247db460437
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c3f91247db460437/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:47:43,504 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:47:43,506 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:47:43,506 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:47:43,507 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:47:43,713 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:47:43,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:47:43,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:47:43,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:47:43,803 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:47:43,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:47:43,803 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:47:44,454 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:47:47,789 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:47:47,809 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c3f91247db460437/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.71ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.73ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.24ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  4.51ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.69ba/s]100%|██████████| 6/6 [00:01<00:00,  4.82ba/s]100%|██████████| 6/6 [00:01<00:00,  4.42ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.75ba/s] 40%|████      | 2/5 [00:00<00:00,  3.63ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.05ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.25ba/s]100%|██████████| 5/5 [00:01<00:00,  4.61ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  4.59ba/s] 50%|█████     | 3/6 [00:00<00:00,  7.87ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  9.10ba/s]100%|██████████| 6/6 [00:00<00:00,  8.72ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  4.00ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.42ba/s]100%|██████████| 5/5 [00:00<00:00, 10.27ba/s]100%|██████████| 5/5 [00:00<00:00,  8.88ba/s]
[INFO|trainer.py:414] 2023-08-29 00:47:53,054 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:47:53,176 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:47:53,177 >>   Num examples = 6000
[INFO|trainer.py:1149] 2023-08-29 00:47:53,177 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:47:53,177 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:47:53,177 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:47:53,177 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:47:53,177 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:00<02:20,  3.33it/s]  0%|          | 2/470 [00:00<02:16,  3.42it/s]  1%|          | 3/470 [00:00<02:15,  3.44it/s]  1%|          | 4/470 [00:01<02:18,  3.37it/s]  1%|          | 5/470 [00:01<02:17,  3.38it/s]  1%|▏         | 6/470 [00:01<02:16,  3.39it/s]  1%|▏         | 7/470 [00:02<02:16,  3.40it/s]  2%|▏         | 8/470 [00:02<02:15,  3.41it/s]  2%|▏         | 9/470 [00:02<02:15,  3.41it/s]  2%|▏         | 10/470 [00:02<02:14,  3.41it/s]  2%|▏         | 11/470 [00:03<02:14,  3.41it/s]  3%|▎         | 12/470 [00:03<02:14,  3.42it/s]  3%|▎         | 13/470 [00:03<02:13,  3.41it/s]  3%|▎         | 14/470 [00:04<02:13,  3.41it/s]  3%|▎         | 15/470 [00:04<02:17,  3.31it/s]  3%|▎         | 16/470 [00:04<02:15,  3.34it/s]  4%|▎         | 17/470 [00:05<02:14,  3.36it/s]  4%|▍         | 18/470 [00:05<02:14,  3.37it/s]  4%|▍         | 19/470 [00:05<02:13,  3.38it/s]  4%|▍         | 20/470 [00:05<02:12,  3.39it/s]  4%|▍         | 21/470 [00:06<02:12,  3.40it/s]  5%|▍         | 22/470 [00:06<02:11,  3.41it/s]  5%|▍         | 23/470 [00:06<02:11,  3.41it/s]  5%|▌         | 24/470 [00:07<02:10,  3.41it/s]  5%|▌         | 25/470 [00:07<02:10,  3.41it/s]  6%|▌         | 26/470 [00:07<02:16,  3.25it/s]  6%|▌         | 27/470 [00:07<02:14,  3.30it/s]  6%|▌         | 28/470 [00:08<02:12,  3.33it/s]  6%|▌         | 29/470 [00:08<02:32,  2.89it/s]  6%|▋         | 30/470 [00:09<02:25,  3.03it/s]  7%|▋         | 31/470 [00:09<02:19,  3.14it/s]  7%|▋         | 32/470 [00:09<02:16,  3.22it/s]  7%|▋         | 33/470 [00:09<02:13,  3.27it/s]  7%|▋         | 34/470 [00:10<02:11,  3.31it/s]  7%|▋         | 35/470 [00:10<02:17,  3.17it/s]  8%|▊         | 36/470 [00:11<03:26,  2.10it/s]  8%|▊         | 37/470 [00:11<03:02,  2.37it/s]  8%|▊         | 38/470 [00:11<02:45,  2.61it/s]  8%|▊         | 39/470 [00:12<02:33,  2.81it/s]  9%|▊         | 40/470 [00:12<02:24,  2.97it/s]  9%|▊         | 41/470 [00:12<02:18,  3.09it/s]  9%|▉         | 42/470 [00:13<02:14,  3.18it/s]  9%|▉         | 43/470 [00:13<02:11,  3.24it/s]  9%|▉         | 44/470 [00:13<02:09,  3.29it/s] 10%|▉         | 45/470 [00:14<02:14,  3.16it/s] 10%|▉         | 46/470 [00:14<02:11,  3.23it/s] 10%|█         | 47/470 [00:14<02:08,  3.28it/s] 10%|█         | 48/470 [00:14<02:07,  3.32it/s] 10%|█         | 49/470 [00:15<02:05,  3.35it/s] 11%|█         | 50/470 [00:15<02:04,  3.37it/s] 11%|█         | 51/470 [00:15<02:03,  3.38it/s] 11%|█         | 52/470 [00:16<02:03,  3.39it/s] 11%|█▏        | 53/470 [00:16<02:02,  3.40it/s] 11%|█▏        | 54/470 [00:16<02:02,  3.40it/s] 12%|█▏        | 55/470 [00:17<02:01,  3.40it/s] 12%|█▏        | 56/470 [00:17<02:07,  3.26it/s] 12%|█▏        | 57/470 [00:17<02:05,  3.30it/s] 12%|█▏        | 58/470 [00:17<02:03,  3.33it/s] 13%|█▎        | 59/470 [00:18<02:02,  3.36it/s] 13%|█▎        | 60/470 [00:18<02:01,  3.37it/s] 13%|█▎        | 61/470 [00:18<02:02,  3.34it/s] 13%|█▎        | 62/470 [00:19<02:01,  3.36it/s] 13%|█▎        | 63/470 [00:19<02:00,  3.37it/s] 14%|█▎        | 64/470 [00:19<01:59,  3.38it/s] 14%|█▍        | 65/470 [00:20<01:59,  3.39it/s] 14%|█▍        | 66/470 [00:20<01:58,  3.40it/s] 14%|█▍        | 67/470 [00:20<01:58,  3.40it/s] 14%|█▍        | 68/470 [00:20<01:58,  3.41it/s] 15%|█▍        | 69/470 [00:21<01:57,  3.41it/s] 15%|█▍        | 70/470 [00:21<01:57,  3.41it/s] 15%|█▌        | 71/470 [00:21<02:03,  3.24it/s] 15%|█▌        | 72/470 [00:22<02:00,  3.29it/s] 16%|█▌        | 73/470 [00:22<01:59,  3.32it/s] 16%|█▌        | 74/470 [00:22<01:58,  3.35it/s] 16%|█▌        | 75/470 [00:23<01:57,  3.37it/s] 16%|█▌        | 76/470 [00:23<01:56,  3.38it/s] 16%|█▋        | 77/470 [00:23<01:55,  3.39it/s] 17%|█▋        | 78/470 [00:23<01:55,  3.40it/s] 17%|█▋        | 79/470 [00:24<01:54,  3.40it/s] 17%|█▋        | 80/470 [00:24<01:54,  3.40it/s] 17%|█▋        | 81/470 [00:24<01:54,  3.40it/s] 17%|█▋        | 82/470 [00:25<01:58,  3.29it/s] 18%|█▊        | 83/470 [00:25<01:56,  3.32it/s] 18%|█▊        | 84/470 [00:25<01:55,  3.35it/s] 18%|█▊        | 85/470 [00:25<01:54,  3.36it/s] 18%|█▊        | 86/470 [00:26<01:53,  3.38it/s] 19%|█▊        | 87/470 [00:26<01:53,  3.39it/s] 19%|█▊        | 88/470 [00:26<01:52,  3.39it/s] 19%|█▉        | 89/470 [00:27<01:52,  3.39it/s] 19%|█▉        | 90/470 [00:27<01:51,  3.40it/s] 19%|█▉        | 91/470 [00:27<01:51,  3.40it/s] 20%|█▉        | 92/470 [00:28<01:51,  3.40it/s] 20%|█▉        | 93/470 [00:28<01:54,  3.28it/s] 20%|██        | 94/470 [00:28<01:46,  3.54it/s][INFO|trainer.py:2140] 2023-08-29 00:48:21,768 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:48:21,768 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:48:21,768 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.43it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.46it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.62it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.63it/s][A
  5%|▌         | 28/543 [00:00<00:11, 45.95it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.64it/s][A
  7%|▋         | 38/543 [00:00<00:11, 45.27it/s][A
  8%|▊         | 43/543 [00:00<00:11, 45.27it/s][A
  9%|▉         | 48/543 [00:01<00:10, 45.30it/s][A
 10%|▉         | 53/543 [00:01<00:10, 45.41it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.43it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.51it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.45it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 45.38it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 45.14it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 45.06it/s][A
 16%|█▌        | 88/543 [00:01<00:10, 45.06it/s][A
 17%|█▋        | 93/543 [00:02<00:09, 45.01it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 45.23it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 45.29it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 45.39it/s][A
 21%|██        | 113/543 [00:02<00:09, 45.33it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 44.71it/s][A
 23%|██▎       | 123/543 [00:02<00:09, 44.84it/s][A
 24%|██▎       | 128/543 [00:02<00:09, 44.82it/s][A
 24%|██▍       | 133/543 [00:02<00:09, 44.79it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 44.88it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 45.04it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 45.26it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 45.32it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 45.31it/s][A
 30%|███       | 163/543 [00:03<00:08, 45.20it/s][A
 31%|███       | 168/543 [00:03<00:08, 45.12it/s][A
 32%|███▏      | 173/543 [00:03<00:08, 45.01it/s][A
 33%|███▎      | 178/543 [00:03<00:08, 44.95it/s][A
 34%|███▎      | 183/543 [00:04<00:07, 45.03it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 45.11it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 45.17it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 45.27it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 45.31it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 45.21it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 45.15it/s][A
 40%|████      | 218/543 [00:04<00:07, 45.03it/s][A
 41%|████      | 223/543 [00:04<00:07, 44.99it/s][A
 42%|████▏     | 228/543 [00:05<00:06, 45.01it/s][A
 43%|████▎     | 233/543 [00:05<00:06, 44.89it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 44.98it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 45.05it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 45.07it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 45.22it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 41.56it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 42.65it/s][A
 49%|████▉     | 268/543 [00:05<00:06, 43.41it/s][A
 50%|█████     | 273/543 [00:06<00:06, 43.99it/s][A
 51%|█████     | 278/543 [00:06<00:05, 44.44it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 44.66it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 44.90it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 45.00it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 44.75it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 44.74it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 44.77it/s][A
 58%|█████▊    | 313/543 [00:06<00:05, 44.98it/s][A
 59%|█████▊    | 318/543 [00:07<00:04, 45.15it/s][A
 59%|█████▉    | 323/543 [00:07<00:04, 45.26it/s][A
 60%|██████    | 328/543 [00:07<00:04, 45.27it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 45.36it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 45.11it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 44.86it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 44.85it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 44.96it/s][A
 66%|██████▌   | 358/543 [00:07<00:04, 45.06it/s][A
 67%|██████▋   | 363/543 [00:08<00:03, 45.16it/s][A
 68%|██████▊   | 368/543 [00:08<00:03, 45.20it/s][A
 69%|██████▊   | 373/543 [00:08<00:03, 45.34it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 45.31it/s][A
 71%|███████   | 383/543 [00:08<00:03, 45.22it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 45.01it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 44.18it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 44.47it/s][A
 74%|███████▍  | 403/543 [00:08<00:03, 44.72it/s][A
 75%|███████▌  | 408/543 [00:09<00:02, 45.01it/s][A
 76%|███████▌  | 413/543 [00:09<00:02, 45.22it/s][A
 77%|███████▋  | 418/543 [00:09<00:02, 45.31it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 45.37it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 45.20it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 45.16it/s][A
 81%|████████  | 438/543 [00:09<00:02, 45.03it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 45.04it/s][A
 83%|████████▎ | 448/543 [00:09<00:02, 45.09it/s][A
 83%|████████▎ | 453/543 [00:10<00:01, 45.15it/s][A
 84%|████████▍ | 458/543 [00:10<00:01, 45.30it/s][A
 85%|████████▌ | 463/543 [00:10<00:01, 45.28it/s][A
 86%|████████▌ | 468/543 [00:10<00:01, 45.32it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 45.15it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 44.67it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 44.87it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 45.00it/s][A
 91%|█████████ | 493/543 [00:10<00:01, 44.95it/s][A
 92%|█████████▏| 498/543 [00:11<00:00, 45.16it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 45.22it/s][A
 94%|█████████▎| 508/543 [00:11<00:00, 45.31it/s][A
 94%|█████████▍| 513/543 [00:11<00:00, 45.32it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 45.17it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 45.04it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 44.97it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 44.17it/s][A
 99%|█████████▉| 538/543 [00:11<00:00, 44.57it/s][A
100%|██████████| 543/543 [00:12<00:00, 44.83it/s][A
                                                 [A                                                
100%|██████████| 543/543 [00:12<00:00, 44.83it/s][A 20%|██        | 94/470 [00:40<01:46,  3.54it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:48:33,908 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-29 00:48:34,028 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:48:37,441 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:48:37,818 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:48:37,938 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [00:55<51:36,  8.26s/it] 20%|██        | 96/470 [00:55<36:40,  5.88s/it] 21%|██        | 97/470 [00:56<26:09,  4.21s/it] 21%|██        | 98/470 [00:56<18:48,  3.03s/it] 21%|██        | 99/470 [00:56<13:40,  2.21s/it] 21%|██▏       | 100/470 [00:56<10:05,  1.64s/it] 21%|██▏       | 101/470 [00:57<07:34,  1.23s/it] 22%|██▏       | 102/470 [00:57<05:49,  1.05it/s] 22%|██▏       | 103/470 [00:57<04:36,  1.33it/s] 22%|██▏       | 104/470 [00:58<03:45,  1.63it/s] 22%|██▏       | 105/470 [00:58<03:09,  1.93it/s] 23%|██▎       | 106/470 [00:58<02:44,  2.22it/s] 23%|██▎       | 107/470 [00:59<02:32,  2.38it/s] 23%|██▎       | 108/470 [00:59<02:18,  2.62it/s] 23%|██▎       | 109/470 [00:59<02:08,  2.82it/s] 23%|██▎       | 110/470 [00:59<02:01,  2.97it/s] 24%|██▎       | 111/470 [01:00<01:56,  3.09it/s] 24%|██▍       | 112/470 [01:00<01:52,  3.18it/s] 24%|██▍       | 113/470 [01:00<01:49,  3.25it/s] 24%|██▍       | 114/470 [01:01<01:48,  3.29it/s] 24%|██▍       | 115/470 [01:01<01:46,  3.33it/s] 25%|██▍       | 116/470 [01:01<01:45,  3.35it/s] 25%|██▍       | 117/470 [01:02<01:44,  3.37it/s] 25%|██▌       | 118/470 [01:02<01:47,  3.28it/s] 25%|██▌       | 119/470 [01:02<01:45,  3.32it/s] 26%|██▌       | 120/470 [01:02<01:44,  3.35it/s] 26%|██▌       | 121/470 [01:03<01:43,  3.36it/s] 26%|██▌       | 122/470 [01:03<01:43,  3.38it/s] 26%|██▌       | 123/470 [01:03<01:42,  3.38it/s] 26%|██▋       | 124/470 [01:04<01:42,  3.39it/s] 27%|██▋       | 125/470 [01:04<01:41,  3.39it/s] 27%|██▋       | 126/470 [01:04<01:41,  3.40it/s] 27%|██▋       | 127/470 [01:04<01:40,  3.40it/s] 27%|██▋       | 128/470 [01:05<01:40,  3.40it/s] 27%|██▋       | 129/470 [01:05<01:42,  3.33it/s] 28%|██▊       | 130/470 [01:05<01:41,  3.36it/s] 28%|██▊       | 131/470 [01:06<01:40,  3.37it/s] 28%|██▊       | 132/470 [01:06<01:39,  3.38it/s] 28%|██▊       | 133/470 [01:06<01:39,  3.39it/s] 29%|██▊       | 134/470 [01:07<01:38,  3.40it/s] 29%|██▊       | 135/470 [01:07<01:38,  3.40it/s] 29%|██▉       | 136/470 [01:07<01:38,  3.40it/s] 29%|██▉       | 137/470 [01:07<01:37,  3.41it/s] 29%|██▉       | 138/470 [01:08<01:37,  3.41it/s] 30%|██▉       | 139/470 [01:08<01:37,  3.40it/s] 30%|██▉       | 140/470 [01:08<01:44,  3.17it/s] 30%|███       | 141/470 [01:09<01:41,  3.24it/s] 30%|███       | 142/470 [01:09<01:39,  3.28it/s] 30%|███       | 143/470 [01:09<01:38,  3.32it/s] 31%|███       | 144/470 [01:10<01:37,  3.34it/s] 31%|███       | 145/470 [01:10<01:36,  3.36it/s] 31%|███       | 146/470 [01:10<01:35,  3.38it/s] 31%|███▏      | 147/470 [01:11<02:28,  2.17it/s] 31%|███▏      | 148/470 [01:11<02:12,  2.44it/s] 32%|███▏      | 149/470 [01:12<02:04,  2.57it/s] 32%|███▏      | 150/470 [01:12<01:55,  2.77it/s] 32%|███▏      | 151/470 [01:12<01:48,  2.94it/s] 32%|███▏      | 152/470 [01:13<01:43,  3.07it/s] 33%|███▎      | 153/470 [01:13<01:40,  3.16it/s] 33%|███▎      | 154/470 [01:13<01:37,  3.23it/s] 33%|███▎      | 155/470 [01:13<01:36,  3.28it/s] 33%|███▎      | 156/470 [01:14<01:34,  3.32it/s] 33%|███▎      | 157/470 [01:14<01:33,  3.34it/s] 34%|███▎      | 158/470 [01:14<01:32,  3.36it/s] 34%|███▍      | 159/470 [01:15<01:32,  3.38it/s] 34%|███▍      | 160/470 [01:15<01:34,  3.28it/s] 34%|███▍      | 161/470 [01:15<01:33,  3.32it/s] 34%|███▍      | 162/470 [01:15<01:32,  3.35it/s] 35%|███▍      | 163/470 [01:16<01:31,  3.37it/s] 35%|███▍      | 164/470 [01:16<01:30,  3.38it/s] 35%|███▌      | 165/470 [01:16<01:30,  3.39it/s] 35%|███▌      | 166/470 [01:17<01:29,  3.39it/s] 36%|███▌      | 167/470 [01:17<01:29,  3.40it/s] 36%|███▌      | 168/470 [01:17<01:28,  3.40it/s] 36%|███▌      | 169/470 [01:18<01:28,  3.40it/s] 36%|███▌      | 170/470 [01:18<01:28,  3.40it/s] 36%|███▋      | 171/470 [01:18<01:31,  3.27it/s] 37%|███▋      | 172/470 [01:18<01:30,  3.31it/s] 37%|███▋      | 173/470 [01:19<01:28,  3.34it/s] 37%|███▋      | 174/470 [01:19<01:28,  3.36it/s] 37%|███▋      | 175/470 [01:19<01:27,  3.37it/s] 37%|███▋      | 176/470 [01:20<01:26,  3.38it/s] 38%|███▊      | 177/470 [01:20<01:26,  3.39it/s] 38%|███▊      | 178/470 [01:20<01:26,  3.39it/s] 38%|███▊      | 179/470 [01:20<01:25,  3.40it/s] 38%|███▊      | 180/470 [01:21<01:25,  3.40it/s] 39%|███▊      | 181/470 [01:21<01:24,  3.40it/s] 39%|███▊      | 182/470 [01:21<01:24,  3.40it/s] 39%|███▉      | 183/470 [01:22<01:24,  3.40it/s] 39%|███▉      | 184/470 [01:22<01:24,  3.40it/s] 39%|███▉      | 185/470 [01:22<01:23,  3.40it/s] 40%|███▉      | 186/470 [01:23<01:23,  3.40it/s] 40%|███▉      | 187/470 [01:23<01:26,  3.28it/s] 40%|████      | 188/470 [01:23<01:19,  3.54it/s][INFO|trainer.py:2140] 2023-08-29 00:49:16,797 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:49:16,797 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:49:16,797 >>   Batch size = 8
{'eval_loss': 1.1325722932815552, 'eval_runtime': 12.069, 'eval_samples_per_second': 359.765, 'eval_steps_per_second': 44.991, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.35it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.42it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.73it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.25it/s][A
  5%|▍         | 27/543 [00:00<00:11, 45.60it/s][A
  6%|▌         | 32/543 [00:00<00:11, 45.22it/s][A
  7%|▋         | 37/543 [00:00<00:11, 45.14it/s][A
  8%|▊         | 42/543 [00:00<00:11, 45.03it/s][A
  9%|▊         | 47/543 [00:01<00:11, 45.08it/s][A
 10%|▉         | 52/543 [00:01<00:10, 45.18it/s][A
 10%|█         | 57/543 [00:01<00:10, 45.38it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 45.44it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 45.36it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.98it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.80it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.89it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.91it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 45.03it/s][A
 18%|█▊        | 97/543 [00:02<00:09, 45.20it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 45.16it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 45.29it/s][A
 21%|██        | 112/543 [00:02<00:09, 45.15it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.78it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.98it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.14it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.26it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.33it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 44.66it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 45.05it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 45.14it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 45.03it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.88it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.80it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.84it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.89it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.90it/s][A
 34%|███▍      | 187/543 [00:04<00:07, 45.01it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 45.09it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 45.25it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 45.13it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 45.12it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.99it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.82it/s][A
 41%|████      | 222/543 [00:04<00:07, 44.88it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.93it/s][A
 43%|████▎     | 232/543 [00:05<00:06, 45.04it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 45.06it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 45.24it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 45.21it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 41.83it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 42.94it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.55it/s][A
 49%|████▉     | 267/543 [00:05<00:06, 44.03it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.35it/s][A
 51%|█████     | 277/543 [00:06<00:05, 44.48it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.73it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.96it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.59it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.68it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.86it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.97it/s][A
 57%|█████▋    | 312/543 [00:06<00:05, 45.11it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 45.10it/s][A
 59%|█████▉    | 322/543 [00:07<00:04, 45.10it/s][A
 60%|██████    | 327/543 [00:07<00:04, 45.18it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.86it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.76it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.87it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.98it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 45.17it/s][A
 66%|██████▌   | 357/543 [00:07<00:04, 45.24it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 45.14it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 45.26it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 45.21it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 45.17it/s][A
 70%|███████   | 382/543 [00:08<00:03, 45.06it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.71it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.26it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.65it/s][A
 74%|███████▍  | 402/543 [00:08<00:03, 44.84it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 45.02it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 45.06it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.96it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 45.03it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.91it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.88it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.95it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 44.99it/s][A
 82%|████████▏ | 447/543 [00:09<00:02, 45.08it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 45.11it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 45.03it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 45.13it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.97it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.87it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.96it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 45.03it/s][A
 90%|████████▉ | 487/543 [00:10<00:01, 45.09it/s][A
 91%|█████████ | 492/543 [00:10<00:01, 45.10it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 45.08it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 45.07it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 45.04it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.94it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.98it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.63it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.03it/s][A
 98%|█████████▊| 532/543 [00:11<00:00, 44.49it/s][A
 99%|█████████▉| 537/543 [00:11<00:00, 44.71it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.89it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.89it/s][A 40%|████      | 188/470 [01:35<01:19,  3.54it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:49:28,988 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-29 00:49:29,145 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:49:31,722 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:49:31,867 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:49:31,952 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:46<32:23,  6.92s/it] 40%|████      | 190/470 [01:46<23:03,  4.94s/it] 41%|████      | 191/470 [01:46<16:29,  3.55s/it] 41%|████      | 192/470 [01:46<11:54,  2.57s/it] 41%|████      | 193/470 [01:47<08:43,  1.89s/it] 41%|████▏     | 194/470 [01:47<06:29,  1.41s/it] 41%|████▏     | 195/470 [01:47<04:55,  1.08s/it] 42%|████▏     | 196/470 [01:48<03:50,  1.19it/s] 42%|████▏     | 197/470 [01:48<03:04,  1.48it/s] 42%|████▏     | 198/470 [01:48<02:32,  1.78it/s] 42%|████▏     | 199/470 [01:48<02:10,  2.08it/s] 43%|████▎     | 200/470 [01:49<01:54,  2.35it/s] 43%|████▎     | 201/470 [01:49<01:48,  2.49it/s] 43%|████▎     | 202/470 [01:49<01:39,  2.71it/s] 43%|████▎     | 203/470 [01:50<01:32,  2.88it/s] 43%|████▎     | 204/470 [01:50<01:27,  3.02it/s] 44%|████▎     | 205/470 [01:50<01:24,  3.13it/s] 44%|████▍     | 206/470 [01:51<01:22,  3.20it/s] 44%|████▍     | 207/470 [01:51<01:20,  3.26it/s] 44%|████▍     | 208/470 [01:51<01:19,  3.31it/s] 44%|████▍     | 209/470 [01:51<01:18,  3.33it/s] 45%|████▍     | 210/470 [01:52<01:17,  3.35it/s] 45%|████▍     | 211/470 [01:52<01:16,  3.37it/s] 45%|████▌     | 212/470 [01:52<01:16,  3.37it/s] 45%|████▌     | 213/470 [01:53<01:16,  3.38it/s] 46%|████▌     | 214/470 [01:53<01:15,  3.39it/s] 46%|████▌     | 215/470 [01:53<01:15,  3.39it/s] 46%|████▌     | 216/470 [01:54<01:18,  3.24it/s] 46%|████▌     | 217/470 [01:54<01:16,  3.30it/s] 46%|████▋     | 218/470 [01:54<01:15,  3.34it/s] 47%|████▋     | 219/470 [01:54<01:14,  3.37it/s] 47%|████▋     | 220/470 [01:55<01:13,  3.40it/s] 47%|████▋     | 221/470 [01:55<01:13,  3.41it/s] 47%|████▋     | 222/470 [01:55<01:12,  3.42it/s] 47%|████▋     | 223/470 [01:56<01:12,  3.43it/s] 48%|████▊     | 224/470 [01:56<01:11,  3.43it/s] 48%|████▊     | 225/470 [01:56<01:11,  3.44it/s] 48%|████▊     | 226/470 [01:56<01:10,  3.44it/s] 48%|████▊     | 227/470 [01:57<01:13,  3.29it/s] 49%|████▊     | 228/470 [01:57<01:12,  3.34it/s] 49%|████▊     | 229/470 [01:57<01:11,  3.37it/s] 49%|████▉     | 230/470 [01:58<01:10,  3.39it/s] 49%|████▉     | 231/470 [01:58<01:10,  3.41it/s] 49%|████▉     | 232/470 [01:58<01:09,  3.43it/s] 50%|████▉     | 233/470 [01:59<01:09,  3.43it/s] 50%|████▉     | 234/470 [01:59<01:08,  3.44it/s] 50%|█████     | 235/470 [01:59<01:08,  3.44it/s] 50%|█████     | 236/470 [01:59<01:07,  3.45it/s] 50%|█████     | 237/470 [02:00<01:07,  3.45it/s] 51%|█████     | 238/470 [02:00<01:10,  3.31it/s] 51%|█████     | 239/470 [02:00<01:08,  3.35it/s] 51%|█████     | 240/470 [02:01<01:07,  3.38it/s] 51%|█████▏    | 241/470 [02:01<01:07,  3.40it/s] 51%|█████▏    | 242/470 [02:01<01:06,  3.42it/s] 52%|█████▏    | 243/470 [02:02<01:06,  3.43it/s] 52%|█████▏    | 244/470 [02:02<01:05,  3.44it/s] 52%|█████▏    | 245/470 [02:02<01:05,  3.44it/s] 52%|█████▏    | 246/470 [02:02<01:05,  3.44it/s] 53%|█████▎    | 247/470 [02:03<01:04,  3.45it/s] 53%|█████▎    | 248/470 [02:03<01:04,  3.45it/s] 53%|█████▎    | 249/470 [02:03<01:06,  3.31it/s] 53%|█████▎    | 250/470 [02:04<01:05,  3.36it/s] 53%|█████▎    | 251/470 [02:04<01:04,  3.38it/s] 54%|█████▎    | 252/470 [02:04<01:04,  3.40it/s] 54%|█████▍    | 253/470 [02:04<01:03,  3.42it/s] 54%|█████▍    | 254/470 [02:05<01:02,  3.43it/s] 54%|█████▍    | 255/470 [02:05<01:02,  3.44it/s] 54%|█████▍    | 256/470 [02:05<01:02,  3.44it/s] 55%|█████▍    | 257/470 [02:06<01:01,  3.44it/s] 55%|█████▍    | 258/470 [02:06<01:01,  3.45it/s] 55%|█████▌    | 259/470 [02:06<01:01,  3.45it/s] 55%|█████▌    | 260/470 [02:06<01:02,  3.38it/s] 56%|█████▌    | 261/470 [02:07<01:01,  3.40it/s] 56%|█████▌    | 262/470 [02:07<01:00,  3.42it/s] 56%|█████▌    | 263/470 [02:07<01:00,  3.43it/s] 56%|█████▌    | 264/470 [02:08<00:59,  3.43it/s] 56%|█████▋    | 265/470 [02:08<00:59,  3.44it/s] 57%|█████▋    | 266/470 [02:08<00:59,  3.44it/s] 57%|█████▋    | 267/470 [02:09<00:58,  3.44it/s] 57%|█████▋    | 268/470 [02:09<00:58,  3.45it/s] 57%|█████▋    | 269/470 [02:09<00:58,  3.45it/s] 57%|█████▋    | 270/470 [02:09<00:58,  3.45it/s] 58%|█████▊    | 271/470 [02:10<00:59,  3.35it/s] 58%|█████▊    | 272/470 [02:10<00:58,  3.38it/s] 58%|█████▊    | 273/470 [02:10<00:57,  3.40it/s] 58%|█████▊    | 274/470 [02:11<01:04,  3.06it/s] 59%|█████▊    | 275/470 [02:11<01:03,  3.08it/s] 59%|█████▊    | 276/470 [02:11<01:00,  3.18it/s] 59%|█████▉    | 277/470 [02:12<00:59,  3.26it/s] 59%|█████▉    | 278/470 [02:12<00:57,  3.31it/s] 59%|█████▉    | 279/470 [02:12<00:56,  3.36it/s] 60%|█████▉    | 280/470 [02:12<00:56,  3.38it/s] 60%|█████▉    | 281/470 [02:13<00:56,  3.32it/s] 60%|██████    | 282/470 [02:13<00:52,  3.58it/s][INFO|trainer.py:2140] 2023-08-29 00:50:06,676 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:50:06,677 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:50:06,677 >>   Batch size = 8
{'eval_loss': 1.1506288051605225, 'eval_runtime': 12.1093, 'eval_samples_per_second': 358.569, 'eval_steps_per_second': 44.842, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.08it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.58it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.47it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.10it/s][A
  5%|▌         | 28/543 [00:00<00:11, 45.64it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.30it/s][A
  7%|▋         | 38/543 [00:00<00:11, 44.96it/s][A
  8%|▊         | 43/543 [00:00<00:11, 44.81it/s][A
  9%|▉         | 48/543 [00:01<00:11, 44.86it/s][A
 10%|▉         | 53/543 [00:01<00:10, 45.00it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.27it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.24it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.17it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 45.01it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 44.82it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 44.77it/s][A
 16%|█▌        | 88/543 [00:01<00:10, 44.87it/s][A
 17%|█▋        | 93/543 [00:02<00:10, 44.88it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 45.11it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 45.28it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 45.15it/s][A
 21%|██        | 113/543 [00:02<00:09, 45.08it/s][A
 22%|██▏       | 118/543 [00:02<00:10, 41.09it/s][A
 23%|██▎       | 123/543 [00:02<00:09, 42.21it/s][A
 24%|██▎       | 128/543 [00:02<00:09, 43.17it/s][A
 24%|██▍       | 133/543 [00:02<00:09, 43.79it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 44.25it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 44.64it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 44.78it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 44.93it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 44.64it/s][A
 30%|███       | 163/543 [00:03<00:08, 44.53it/s][A
 31%|███       | 168/543 [00:03<00:08, 44.65it/s][A
 32%|███▏      | 173/543 [00:03<00:08, 44.91it/s][A
 33%|███▎      | 178/543 [00:03<00:08, 44.93it/s][A
 34%|███▎      | 183/543 [00:04<00:08, 44.96it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 45.10it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 45.10it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 45.22it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 44.93it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 44.91it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 44.83it/s][A
 40%|████      | 218/543 [00:04<00:07, 44.85it/s][A
 41%|████      | 223/543 [00:04<00:07, 44.89it/s][A
 42%|████▏     | 228/543 [00:05<00:07, 44.92it/s][A
 43%|████▎     | 233/543 [00:05<00:06, 44.98it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 44.96it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 44.91it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 44.76it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 41.82it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 42.94it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 43.62it/s][A
 49%|████▉     | 268/543 [00:05<00:06, 44.18it/s][A
 50%|█████     | 273/543 [00:06<00:06, 44.48it/s][A
 51%|█████     | 278/543 [00:06<00:05, 44.76it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 44.81it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 44.73it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 44.58it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 44.51it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 44.79it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 44.84it/s][A
 58%|█████▊    | 313/543 [00:06<00:05, 45.06it/s][A
 59%|█████▊    | 318/543 [00:07<00:04, 45.13it/s][A
 59%|█████▉    | 323/543 [00:07<00:04, 45.23it/s][A
 60%|██████    | 328/543 [00:07<00:04, 45.23it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 45.02it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 44.76it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 44.71it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 44.81it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 44.99it/s][A
 66%|██████▌   | 358/543 [00:08<00:04, 41.34it/s][A
 67%|██████▋   | 363/543 [00:08<00:04, 42.58it/s][A
 68%|██████▊   | 368/543 [00:08<00:04, 43.42it/s][A
 69%|██████▊   | 373/543 [00:08<00:03, 43.97it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 44.46it/s][A
 71%|███████   | 383/543 [00:08<00:03, 44.47it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 44.60it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 44.71it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 44.40it/s][A
 74%|███████▍  | 403/543 [00:09<00:03, 44.57it/s][A
 75%|███████▌  | 408/543 [00:09<00:03, 44.65it/s][A
 76%|███████▌  | 413/543 [00:09<00:02, 45.04it/s][A
 77%|███████▋  | 418/543 [00:09<00:02, 45.23it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 45.30it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 45.09it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 43.76it/s][A
 81%|████████  | 438/543 [00:09<00:02, 44.21it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 44.31it/s][A
 83%|████████▎ | 448/543 [00:10<00:02, 44.48it/s][A
 83%|████████▎ | 453/543 [00:10<00:02, 44.55it/s][A
 84%|████████▍ | 458/543 [00:10<00:01, 44.72it/s][A
 85%|████████▌ | 463/543 [00:10<00:01, 44.98it/s][A
 86%|████████▌ | 468/543 [00:10<00:01, 45.15it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 45.22it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 44.90it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 44.90it/s][A
 90%|████████▉ | 488/543 [00:10<00:01, 44.78it/s][A
 91%|█████████ | 493/543 [00:11<00:01, 44.10it/s][A
 92%|█████████▏| 498/543 [00:11<00:01, 44.32it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 44.51it/s][A
 94%|█████████▎| 508/543 [00:11<00:00, 44.80it/s][A
 94%|█████████▍| 513/543 [00:11<00:00, 44.91it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 45.07it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 45.15it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 44.95it/s][A
 98%|█████████▊| 533/543 [00:11<00:00, 44.93it/s][A
 99%|█████████▉| 538/543 [00:12<00:00, 44.85it/s][A
100%|██████████| 543/543 [00:12<00:00, 44.90it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.90it/s][A 60%|██████    | 282/470 [02:25<00:52,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:50:18,942 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-29 00:50:19,140 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:50:22,599 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:50:22,800 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:50:22,903 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [02:36<22:26,  7.20s/it] 60%|██████    | 284/470 [02:37<15:55,  5.14s/it] 61%|██████    | 285/470 [02:37<11:21,  3.68s/it] 61%|██████    | 286/470 [02:37<08:10,  2.67s/it] 61%|██████    | 287/470 [02:38<05:57,  1.95s/it] 61%|██████▏   | 288/470 [02:38<04:24,  1.46s/it] 61%|██████▏   | 289/470 [02:38<03:20,  1.11s/it] 62%|██████▏   | 290/470 [02:38<02:35,  1.16it/s] 62%|██████▏   | 291/470 [02:39<02:03,  1.44it/s] 62%|██████▏   | 292/470 [02:39<01:41,  1.75it/s] 62%|██████▏   | 293/470 [02:39<01:26,  2.04it/s] 63%|██████▎   | 294/470 [02:40<01:15,  2.32it/s] 63%|██████▎   | 295/470 [02:40<01:10,  2.50it/s] 63%|██████▎   | 296/470 [02:40<01:04,  2.71it/s] 63%|██████▎   | 297/470 [02:41<00:59,  2.89it/s] 63%|██████▎   | 298/470 [02:41<00:56,  3.03it/s] 64%|██████▎   | 299/470 [02:41<00:54,  3.13it/s] 64%|██████▍   | 300/470 [02:41<00:52,  3.21it/s] 64%|██████▍   | 301/470 [02:42<00:51,  3.26it/s] 64%|██████▍   | 302/470 [02:42<00:50,  3.30it/s] 64%|██████▍   | 303/470 [02:42<00:50,  3.34it/s] 65%|██████▍   | 304/470 [02:43<00:49,  3.35it/s] 65%|██████▍   | 305/470 [02:43<00:48,  3.37it/s] 65%|██████▌   | 306/470 [02:43<00:50,  3.27it/s] 65%|██████▌   | 307/470 [02:43<00:49,  3.31it/s] 66%|██████▌   | 308/470 [02:44<00:48,  3.33it/s] 66%|██████▌   | 309/470 [02:44<00:48,  3.35it/s] 66%|██████▌   | 310/470 [02:44<00:47,  3.36it/s] 66%|██████▌   | 311/470 [02:45<00:47,  3.37it/s] 66%|██████▋   | 312/470 [02:45<00:46,  3.39it/s] 67%|██████▋   | 313/470 [02:45<00:46,  3.39it/s] 67%|██████▋   | 314/470 [02:46<00:45,  3.40it/s] 67%|██████▋   | 315/470 [02:46<00:45,  3.40it/s] 67%|██████▋   | 316/470 [02:46<00:45,  3.40it/s] 67%|██████▋   | 317/470 [02:46<00:47,  3.23it/s] 68%|██████▊   | 318/470 [02:47<00:46,  3.28it/s] 68%|██████▊   | 319/470 [02:47<00:45,  3.31it/s] 68%|██████▊   | 320/470 [02:47<00:44,  3.34it/s] 68%|██████▊   | 321/470 [02:48<00:44,  3.36it/s] 69%|██████▊   | 322/470 [02:48<00:43,  3.37it/s] 69%|██████▊   | 323/470 [02:48<00:43,  3.38it/s] 69%|██████▉   | 324/470 [02:49<00:43,  3.39it/s] 69%|██████▉   | 325/470 [02:49<00:42,  3.39it/s] 69%|██████▉   | 326/470 [02:49<00:42,  3.40it/s] 70%|██████▉   | 327/470 [02:49<00:42,  3.40it/s] 70%|██████▉   | 328/470 [02:50<00:44,  3.18it/s] 70%|███████   | 329/470 [02:50<00:43,  3.24it/s] 70%|███████   | 330/470 [02:50<00:42,  3.29it/s] 70%|███████   | 331/470 [02:51<00:41,  3.32it/s] 71%|███████   | 332/470 [02:51<00:41,  3.35it/s] 71%|███████   | 333/470 [02:51<00:40,  3.36it/s] 71%|███████   | 334/470 [02:52<00:40,  3.37it/s] 71%|███████▏  | 335/470 [02:52<00:39,  3.38it/s] 71%|███████▏  | 336/470 [02:52<00:39,  3.38it/s] 72%|███████▏  | 337/470 [02:52<00:39,  3.39it/s] 72%|███████▏  | 338/470 [02:53<00:38,  3.39it/s] 72%|███████▏  | 339/470 [02:53<00:38,  3.40it/s] 72%|███████▏  | 340/470 [02:53<00:38,  3.40it/s] 73%|███████▎  | 341/470 [02:54<00:37,  3.40it/s] 73%|███████▎  | 342/470 [02:54<00:37,  3.40it/s] 73%|███████▎  | 343/470 [02:54<00:37,  3.40it/s] 73%|███████▎  | 344/470 [02:54<00:37,  3.40it/s] 73%|███████▎  | 345/470 [02:55<00:36,  3.40it/s] 74%|███████▎  | 346/470 [02:55<00:37,  3.28it/s] 74%|███████▍  | 347/470 [02:55<00:37,  3.32it/s] 74%|███████▍  | 348/470 [02:56<00:36,  3.34it/s] 74%|███████▍  | 349/470 [02:56<00:35,  3.36it/s] 74%|███████▍  | 350/470 [02:56<00:35,  3.37it/s] 75%|███████▍  | 351/470 [02:57<00:35,  3.38it/s] 75%|███████▍  | 352/470 [02:57<00:34,  3.39it/s] 75%|███████▌  | 353/470 [02:57<00:34,  3.39it/s] 75%|███████▌  | 354/470 [02:57<00:34,  3.39it/s] 76%|███████▌  | 355/470 [02:58<00:33,  3.40it/s] 76%|███████▌  | 356/470 [02:58<00:33,  3.40it/s] 76%|███████▌  | 357/470 [02:58<00:34,  3.24it/s] 76%|███████▌  | 358/470 [02:59<00:34,  3.28it/s] 76%|███████▋  | 359/470 [02:59<00:33,  3.32it/s] 77%|███████▋  | 360/470 [02:59<00:32,  3.35it/s] 77%|███████▋  | 361/470 [03:00<00:32,  3.36it/s] 77%|███████▋  | 362/470 [03:00<00:32,  3.37it/s] 77%|███████▋  | 363/470 [03:00<00:31,  3.38it/s] 77%|███████▋  | 364/470 [03:00<00:31,  3.39it/s] 78%|███████▊  | 365/470 [03:01<00:30,  3.39it/s] 78%|███████▊  | 366/470 [03:01<00:30,  3.40it/s] 78%|███████▊  | 367/470 [03:01<00:30,  3.41it/s] 78%|███████▊  | 368/470 [03:02<00:30,  3.29it/s] 79%|███████▊  | 369/470 [03:02<00:30,  3.34it/s] 79%|███████▊  | 370/470 [03:02<00:29,  3.37it/s] 79%|███████▉  | 371/470 [03:03<00:29,  3.39it/s] 79%|███████▉  | 372/470 [03:03<00:28,  3.41it/s] 79%|███████▉  | 373/470 [03:03<00:28,  3.42it/s] 80%|███████▉  | 374/470 [03:03<00:27,  3.43it/s] 80%|███████▉  | 375/470 [03:04<00:27,  3.44it/s] 80%|████████  | 376/470 [03:04<00:25,  3.68it/s][INFO|trainer.py:2140] 2023-08-29 00:50:57,590 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:50:57,590 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:50:57,590 >>   Batch size = 8
{'eval_loss': 1.161561369895935, 'eval_runtime': 12.1694, 'eval_samples_per_second': 356.797, 'eval_steps_per_second': 44.62, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.22it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.06it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.34it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.27it/s][A
  5%|▍         | 27/543 [00:00<00:12, 40.99it/s][A
  6%|▌         | 32/543 [00:00<00:12, 42.43it/s][A
  7%|▋         | 37/543 [00:00<00:11, 43.38it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.95it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.41it/s][A
 10%|▉         | 52/543 [00:01<00:10, 44.85it/s][A
 10%|█         | 57/543 [00:01<00:10, 45.06it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 45.04it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.70it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.62it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.71it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.77it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.98it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 45.08it/s][A
 18%|█▊        | 97/543 [00:02<00:09, 45.22it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 45.39it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 45.26it/s][A
 21%|██        | 112/543 [00:02<00:09, 45.03it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.74it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.77it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.88it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 45.03it/s][A
 25%|██▌       | 137/543 [00:03<00:08, 45.23it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 45.34it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 45.40it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 45.35it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 45.09it/s][A
 30%|██▉       | 162/543 [00:03<00:09, 41.46it/s][A
 31%|███       | 167/543 [00:03<00:08, 42.66it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.42it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.05it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.51it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 40.24it/s][A
 36%|███▌      | 193/543 [00:04<00:08, 43.11it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 43.61it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 43.78it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 44.25it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 44.09it/s][A
 40%|████      | 218/543 [00:04<00:07, 44.97it/s][A
 41%|████      | 223/543 [00:05<00:07, 45.09it/s][A
 42%|████▏     | 228/543 [00:05<00:06, 45.24it/s][A
 43%|████▎     | 233/543 [00:05<00:06, 44.94it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 45.04it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 44.99it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 44.75it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 44.93it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 45.07it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 45.24it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 45.29it/s][A
 50%|█████     | 273/543 [00:06<00:06, 40.30it/s][A
 51%|█████     | 278/543 [00:06<00:06, 42.27it/s][A
 52%|█████▏    | 283/543 [00:06<00:06, 38.11it/s][A
 53%|█████▎    | 288/543 [00:06<00:06, 37.68it/s][A
 54%|█████▍    | 292/543 [00:06<00:08, 29.33it/s][A
 55%|█████▍    | 296/543 [00:07<00:11, 21.45it/s][A
 56%|█████▌    | 302/543 [00:07<00:08, 27.09it/s][A
 57%|█████▋    | 307/543 [00:07<00:07, 30.92it/s][A
 57%|█████▋    | 312/543 [00:07<00:06, 34.27it/s][A
 58%|█████▊    | 317/543 [00:07<00:06, 36.98it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 39.12it/s][A
 60%|██████    | 327/543 [00:07<00:05, 40.88it/s][A
 61%|██████    | 332/543 [00:07<00:05, 42.15it/s][A
 62%|██████▏   | 337/543 [00:08<00:04, 42.69it/s][A
 63%|██████▎   | 342/543 [00:08<00:04, 42.93it/s][A
 64%|██████▍   | 347/543 [00:08<00:04, 43.31it/s][A
 65%|██████▍   | 352/543 [00:08<00:04, 43.72it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.18it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.53it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.90it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 45.11it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 45.23it/s][A
 70%|███████   | 382/543 [00:09<00:03, 44.96it/s][A
 71%|███████▏  | 387/543 [00:09<00:03, 44.82it/s][A
 72%|███████▏  | 392/543 [00:09<00:03, 44.70it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 44.69it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.85it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.89it/s][A
 76%|███████▌  | 412/543 [00:09<00:03, 38.30it/s][A
 77%|███████▋  | 417/543 [00:09<00:03, 40.20it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 41.63it/s][A
 79%|███████▊  | 427/543 [00:10<00:02, 42.61it/s][A
 80%|███████▉  | 432/543 [00:10<00:02, 43.45it/s][A
 80%|████████  | 437/543 [00:10<00:02, 43.99it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.38it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.71it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.48it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.49it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.74it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.90it/s][A
 87%|████████▋ | 472/543 [00:11<00:01, 44.99it/s][A
 88%|████████▊ | 477/543 [00:11<00:01, 45.17it/s][A
 89%|████████▉ | 482/543 [00:11<00:01, 45.09it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 45.22it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 45.21it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.88it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.94it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 45.01it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 45.00it/s][A
 95%|█████████▌| 517/543 [00:12<00:00, 45.10it/s][A
 96%|█████████▌| 522/543 [00:12<00:00, 45.22it/s][A
 97%|█████████▋| 527/543 [00:12<00:00, 45.17it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 45.10it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 45.05it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.86it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.86it/s][A 80%|████████  | 376/470 [03:17<00:25,  3.68it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:51:10,465 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-29 00:51:10,796 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:51:15,266 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:51:15,400 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:51:15,494 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [03:29<11:50,  7.64s/it] 80%|████████  | 378/470 [03:29<08:20,  5.44s/it] 81%|████████  | 379/470 [03:29<05:54,  3.90s/it] 81%|████████  | 380/470 [03:30<04:13,  2.82s/it] 81%|████████  | 381/470 [03:30<03:03,  2.06s/it] 81%|████████▏ | 382/470 [03:30<02:14,  1.53s/it] 81%|████████▏ | 383/470 [03:31<01:40,  1.16s/it] 82%|████████▏ | 384/470 [03:31<01:17,  1.12it/s] 82%|████████▏ | 385/470 [03:31<01:00,  1.40it/s] 82%|████████▏ | 386/470 [03:31<00:49,  1.70it/s] 82%|████████▏ | 387/470 [03:32<00:41,  2.01it/s] 83%|████████▎ | 388/470 [03:32<00:35,  2.30it/s] 83%|████████▎ | 389/470 [03:32<00:32,  2.50it/s] 83%|████████▎ | 390/470 [03:33<00:29,  2.72it/s] 83%|████████▎ | 391/470 [03:33<00:27,  2.91it/s] 83%|████████▎ | 392/470 [03:33<00:25,  3.05it/s] 84%|████████▎ | 393/470 [03:33<00:24,  3.16it/s] 84%|████████▍ | 394/470 [03:34<00:23,  3.24it/s] 84%|████████▍ | 395/470 [03:34<00:22,  3.31it/s] 84%|████████▍ | 396/470 [03:34<00:22,  3.35it/s] 84%|████████▍ | 397/470 [03:35<00:21,  3.38it/s] 85%|████████▍ | 398/470 [03:35<00:21,  3.40it/s] 85%|████████▍ | 399/470 [03:35<00:20,  3.42it/s] 85%|████████▌ | 400/470 [03:35<00:20,  3.37it/s] 85%|████████▌ | 401/470 [03:36<00:20,  3.39it/s] 86%|████████▌ | 402/470 [03:36<00:19,  3.41it/s] 86%|████████▌ | 403/470 [03:36<00:19,  3.43it/s] 86%|████████▌ | 404/470 [03:37<00:19,  3.44it/s] 86%|████████▌ | 405/470 [03:37<00:18,  3.44it/s] 86%|████████▋ | 406/470 [03:37<00:18,  3.45it/s] 87%|████████▋ | 407/470 [03:38<00:18,  3.45it/s] 87%|████████▋ | 408/470 [03:38<00:17,  3.45it/s] 87%|████████▋ | 409/470 [03:38<00:17,  3.45it/s] 87%|████████▋ | 410/470 [03:38<00:17,  3.45it/s] 87%|████████▋ | 411/470 [03:39<00:17,  3.34it/s] 88%|████████▊ | 412/470 [03:39<00:17,  3.38it/s] 88%|████████▊ | 413/470 [03:39<00:16,  3.40it/s] 88%|████████▊ | 414/470 [03:40<00:16,  3.42it/s] 88%|████████▊ | 415/470 [03:40<00:16,  3.43it/s] 89%|████████▊ | 416/470 [03:40<00:15,  3.43it/s] 89%|████████▊ | 417/470 [03:40<00:15,  3.44it/s] 89%|████████▉ | 418/470 [03:41<00:15,  3.45it/s] 89%|████████▉ | 419/470 [03:41<00:14,  3.45it/s] 89%|████████▉ | 420/470 [03:41<00:14,  3.45it/s] 90%|████████▉ | 421/470 [03:42<00:14,  3.45it/s] 90%|████████▉ | 422/470 [03:42<00:14,  3.35it/s] 90%|█████████ | 423/470 [03:42<00:13,  3.39it/s] 90%|█████████ | 424/470 [03:42<00:13,  3.40it/s] 90%|█████████ | 425/470 [03:43<00:13,  3.42it/s] 91%|█████████ | 426/470 [03:43<00:12,  3.43it/s] 91%|█████████ | 427/470 [03:43<00:12,  3.44it/s] 91%|█████████ | 428/470 [03:44<00:12,  3.44it/s] 91%|█████████▏| 429/470 [03:44<00:11,  3.45it/s] 91%|█████████▏| 430/470 [03:44<00:11,  3.45it/s] 92%|█████████▏| 431/470 [03:45<00:11,  3.45it/s] 92%|█████████▏| 432/470 [03:45<00:11,  3.45it/s] 92%|█████████▏| 433/470 [03:45<00:11,  3.36it/s] 92%|█████████▏| 434/470 [03:45<00:10,  3.39it/s] 93%|█████████▎| 435/470 [03:46<00:10,  3.41it/s] 93%|█████████▎| 436/470 [03:46<00:09,  3.42it/s] 93%|█████████▎| 437/470 [03:46<00:09,  3.43it/s] 93%|█████████▎| 438/470 [03:47<00:09,  3.43it/s] 93%|█████████▎| 439/470 [03:47<00:09,  3.44it/s] 94%|█████████▎| 440/470 [03:47<00:08,  3.44it/s] 94%|█████████▍| 441/470 [03:47<00:08,  3.45it/s] 94%|█████████▍| 442/470 [03:48<00:08,  3.45it/s] 94%|█████████▍| 443/470 [03:48<00:07,  3.45it/s] 94%|█████████▍| 444/470 [03:48<00:07,  3.29it/s] 95%|█████████▍| 445/470 [03:49<00:07,  3.34it/s] 95%|█████████▍| 446/470 [03:49<00:07,  3.37it/s] 95%|█████████▌| 447/470 [03:49<00:06,  3.39it/s] 95%|█████████▌| 448/470 [03:50<00:06,  3.41it/s] 96%|█████████▌| 449/470 [03:50<00:06,  3.43it/s] 96%|█████████▌| 450/470 [03:50<00:05,  3.44it/s] 96%|█████████▌| 451/470 [03:50<00:05,  3.44it/s] 96%|█████████▌| 452/470 [03:51<00:05,  3.44it/s] 96%|█████████▋| 453/470 [03:51<00:04,  3.45it/s] 97%|█████████▋| 454/470 [03:51<00:04,  3.45it/s] 97%|█████████▋| 455/470 [03:52<00:04,  3.19it/s] 97%|█████████▋| 456/470 [03:52<00:04,  3.26it/s] 97%|█████████▋| 457/470 [03:52<00:03,  3.32it/s] 97%|█████████▋| 458/470 [03:52<00:03,  3.36it/s] 98%|█████████▊| 459/470 [03:53<00:03,  3.39it/s] 98%|█████████▊| 460/470 [03:53<00:02,  3.41it/s] 98%|█████████▊| 461/470 [03:53<00:02,  3.25it/s] 98%|█████████▊| 462/470 [03:54<00:02,  3.31it/s] 99%|█████████▊| 463/470 [03:54<00:02,  3.35it/s] 99%|█████████▊| 464/470 [03:54<00:01,  3.38it/s] 99%|█████████▉| 465/470 [03:55<00:01,  3.40it/s] 99%|█████████▉| 466/470 [03:55<00:01,  3.42it/s] 99%|█████████▉| 467/470 [03:55<00:00,  3.43it/s]100%|█████████▉| 468/470 [03:55<00:00,  3.43it/s]100%|█████████▉| 469/470 [03:56<00:00,  3.44it/s]100%|██████████| 470/470 [03:56<00:00,  3.68it/s][INFO|trainer.py:2140] 2023-08-29 00:51:49,631 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:51:49,631 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:51:49,631 >>   Batch size = 8
{'eval_loss': 1.1779732704162598, 'eval_runtime': 12.7071, 'eval_samples_per_second': 341.7, 'eval_steps_per_second': 42.732, 'epoch': 4.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.58it/s][A
  2%|▏         | 12/543 [00:00<00:12, 42.80it/s][A
  3%|▎         | 17/543 [00:00<00:11, 43.94it/s][A
  4%|▍         | 22/543 [00:00<00:11, 44.41it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.65it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.63it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.77it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.82it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.92it/s][A
 10%|▉         | 52/543 [00:01<00:10, 44.72it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.84it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 45.12it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 45.21it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 45.24it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 45.10it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 45.00it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.98it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.97it/s][A
 18%|█▊        | 97/543 [00:02<00:09, 44.96it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.84it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 45.04it/s][A
 21%|██        | 112/543 [00:02<00:09, 45.11it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 45.25it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 45.23it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 45.11it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 45.02it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 45.02it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 44.96it/s][A
 27%|██▋       | 147/543 [00:03<00:09, 43.90it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.25it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.66it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.89it/s][A
 31%|███       | 167/543 [00:03<00:08, 45.01it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.87it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.95it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.91it/s][A
 34%|███▍      | 187/543 [00:04<00:07, 44.83it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.80it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.91it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 45.06it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 45.14it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 45.21it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 45.17it/s][A
 41%|████      | 222/543 [00:04<00:07, 45.07it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.94it/s][A
 43%|████▎     | 232/543 [00:05<00:06, 44.88it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.87it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.99it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 45.11it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 45.16it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 45.21it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 45.18it/s][A
 49%|████▉     | 267/543 [00:05<00:06, 45.16it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.90it/s][A
 51%|█████     | 277/543 [00:06<00:05, 44.85it/s][A
 52%|█████▏    | 282/543 [00:06<00:06, 43.06it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.83it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.36it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.70it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.81it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.96it/s][A
 57%|█████▋    | 312/543 [00:06<00:05, 44.84it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.88it/s][A
 59%|█████▉    | 322/543 [00:07<00:04, 44.57it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.48it/s][A
 61%|██████    | 332/543 [00:07<00:05, 39.14it/s][A
 62%|██████▏   | 337/543 [00:07<00:05, 41.00it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 42.30it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.28it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.90it/s][A
 66%|██████▌   | 357/543 [00:07<00:04, 44.45it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.72it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.90it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.55it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.34it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.46it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.68it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.86it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 45.06it/s][A
 74%|███████▍  | 402/543 [00:08<00:03, 45.25it/s][A
 75%|███████▍  | 407/543 [00:09<00:02, 45.36it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 45.28it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.98it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.72it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.72it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.84it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.91it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 45.08it/s][A
 82%|████████▏ | 447/543 [00:09<00:02, 45.21it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 45.32it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 45.26it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.97it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.70it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.85it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.26it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.47it/s][A
 90%|████████▉ | 487/543 [00:10<00:01, 44.77it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.91it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.98it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.80it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.58it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.54it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.55it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.79it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.81it/s][A
 98%|█████████▊| 532/543 [00:11<00:00, 44.99it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 45.19it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 45.28it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 45.28it/s][A100%|██████████| 470/470 [04:08<00:00,  3.68it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:52:02,155 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-29 00:52:02,527 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:52:09,766 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:52:10,019 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:52:10,144 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:52:19,188 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:52:19,216 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94 (score: 1.1325722932815552).
                                                 100%|██████████| 470/470 [04:34<00:00,  3.68it/s]100%|██████████| 470/470 [04:34<00:00,  1.71it/s]
[INFO|trainer.py:1894] 2023-08-29 00:52:27,503 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 00:52:27,574 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:52:31,069 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:52:31,229 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:52:31,315 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:52:31,724 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:31,725 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:31,725 >>   train_loss               =      0.374
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:31,725 >>   train_runtime            = 0:04:34.27
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:31,725 >>   train_samples            =       6000
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:31,725 >>   train_samples_per_second =     109.38
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:31,725 >>   train_steps_per_second   =      1.714
{'eval_loss': 1.1857049465179443, 'eval_runtime': 12.214, 'eval_samples_per_second': 355.494, 'eval_steps_per_second': 44.457, 'epoch': 5.0}
{'train_runtime': 274.2728, 'train_samples_per_second': 109.38, 'train_steps_per_second': 1.714, 'train_loss': 0.373983082872756, 'epoch': 5.0}
08/29/2023 00:52:31 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:52:31,895 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:52:31,895 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 00:52:31,895 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 55.38it/s]  2%|▏         | 12/543 [00:00<00:10, 49.32it/s]  3%|▎         | 17/543 [00:00<00:10, 47.97it/s]  4%|▍         | 22/543 [00:00<00:11, 47.09it/s]  5%|▍         | 27/543 [00:00<00:11, 46.74it/s]  6%|▌         | 32/543 [00:00<00:10, 46.47it/s]  7%|▋         | 37/543 [00:00<00:10, 46.20it/s]  8%|▊         | 42/543 [00:00<00:10, 45.87it/s]  9%|▊         | 47/543 [00:01<00:10, 45.33it/s] 10%|▉         | 52/543 [00:01<00:10, 44.99it/s] 10%|█         | 57/543 [00:01<00:10, 44.92it/s] 11%|█▏        | 62/543 [00:01<00:10, 45.14it/s] 12%|█▏        | 67/543 [00:01<00:10, 45.16it/s] 13%|█▎        | 72/543 [00:01<00:10, 45.37it/s] 14%|█▍        | 77/543 [00:01<00:10, 45.42it/s] 15%|█▌        | 82/543 [00:01<00:10, 45.59it/s] 16%|█▌        | 87/543 [00:01<00:10, 45.52it/s] 17%|█▋        | 92/543 [00:02<00:09, 45.34it/s] 18%|█▊        | 97/543 [00:02<00:09, 45.01it/s] 19%|█▉        | 102/543 [00:02<00:09, 44.94it/s] 20%|█▉        | 107/543 [00:02<00:09, 43.75it/s] 21%|██        | 112/543 [00:02<00:09, 44.46it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.90it/s] 22%|██▏       | 122/543 [00:02<00:09, 45.17it/s] 23%|██▎       | 127/543 [00:02<00:09, 45.24it/s] 24%|██▍       | 132/543 [00:02<00:09, 45.29it/s] 25%|██▌       | 137/543 [00:03<00:08, 45.14it/s] 26%|██▌       | 142/543 [00:03<00:08, 45.06it/s] 27%|██▋       | 147/543 [00:03<00:08, 44.77it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.83it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.90it/s] 30%|██▉       | 162/543 [00:03<00:08, 45.14it/s] 31%|███       | 167/543 [00:03<00:08, 45.21it/s] 32%|███▏      | 172/543 [00:03<00:08, 45.41it/s] 33%|███▎      | 177/543 [00:03<00:08, 45.42it/s] 34%|███▎      | 182/543 [00:04<00:07, 45.40it/s] 34%|███▍      | 187/543 [00:04<00:07, 45.18it/s] 35%|███▌      | 192/543 [00:04<00:07, 44.98it/s] 36%|███▋      | 197/543 [00:04<00:07, 44.91it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.98it/s] 38%|███▊      | 207/543 [00:04<00:07, 45.18it/s] 39%|███▉      | 212/543 [00:04<00:07, 45.34it/s] 40%|███▉      | 217/543 [00:04<00:07, 45.41it/s] 41%|████      | 222/543 [00:04<00:07, 45.43it/s] 42%|████▏     | 227/543 [00:04<00:06, 45.39it/s] 43%|████▎     | 232/543 [00:05<00:06, 45.19it/s] 44%|████▎     | 237/543 [00:05<00:06, 45.07it/s] 45%|████▍     | 242/543 [00:05<00:06, 45.06it/s] 45%|████▌     | 247/543 [00:05<00:06, 43.97it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.50it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.70it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.93it/s] 49%|████▉     | 267/543 [00:05<00:06, 45.06it/s] 50%|█████     | 272/543 [00:06<00:06, 45.13it/s] 51%|█████     | 277/543 [00:06<00:05, 45.08it/s] 52%|█████▏    | 282/543 [00:06<00:05, 45.02it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.81it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.82it/s] 55%|█████▍    | 297/543 [00:06<00:05, 45.10it/s] 56%|█████▌    | 302/543 [00:06<00:05, 45.27it/s] 57%|█████▋    | 307/543 [00:06<00:05, 45.46it/s] 57%|█████▋    | 312/543 [00:06<00:05, 45.30it/s] 58%|█████▊    | 317/543 [00:06<00:04, 45.34it/s] 59%|█████▉    | 322/543 [00:07<00:04, 45.37it/s] 60%|██████    | 327/543 [00:07<00:04, 45.28it/s] 61%|██████    | 332/543 [00:07<00:04, 45.08it/s] 62%|██████▏   | 337/543 [00:07<00:04, 45.03it/s] 63%|██████▎   | 342/543 [00:07<00:04, 45.20it/s] 64%|██████▍   | 347/543 [00:07<00:04, 45.33it/s] 65%|██████▍   | 352/543 [00:07<00:04, 45.45it/s] 66%|██████▌   | 357/543 [00:07<00:04, 45.61it/s] 67%|██████▋   | 362/543 [00:07<00:03, 45.51it/s] 68%|██████▊   | 367/543 [00:08<00:03, 45.41it/s] 69%|██████▊   | 372/543 [00:08<00:03, 45.28it/s] 69%|██████▉   | 377/543 [00:08<00:03, 45.22it/s] 70%|███████   | 382/543 [00:08<00:03, 45.13it/s] 71%|███████▏  | 387/543 [00:08<00:03, 43.11it/s] 72%|███████▏  | 392/543 [00:08<00:03, 43.90it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.38it/s] 74%|███████▍  | 402/543 [00:08<00:03, 44.71it/s] 75%|███████▍  | 407/543 [00:09<00:03, 44.98it/s] 76%|███████▌  | 412/543 [00:09<00:02, 45.15it/s] 77%|███████▋  | 417/543 [00:09<00:02, 45.30it/s] 78%|███████▊  | 422/543 [00:09<00:02, 45.26it/s] 79%|███████▊  | 427/543 [00:09<00:02, 45.07it/s] 80%|███████▉  | 432/543 [00:09<00:02, 45.06it/s] 80%|████████  | 437/543 [00:09<00:02, 45.13it/s] 81%|████████▏ | 442/543 [00:09<00:02, 45.32it/s] 82%|████████▏ | 447/543 [00:09<00:02, 45.50it/s] 83%|████████▎ | 452/543 [00:09<00:01, 45.53it/s] 84%|████████▍ | 457/543 [00:10<00:01, 45.43it/s] 85%|████████▌ | 462/543 [00:10<00:01, 45.42it/s] 86%|████████▌ | 467/543 [00:10<00:01, 45.25it/s] 87%|████████▋ | 472/543 [00:10<00:01, 45.09it/s] 88%|████████▊ | 477/543 [00:10<00:01, 45.07it/s] 89%|████████▉ | 482/543 [00:10<00:01, 45.11it/s] 90%|████████▉ | 487/543 [00:10<00:01, 45.19it/s] 91%|█████████ | 492/543 [00:10<00:01, 45.31it/s] 92%|█████████▏| 497/543 [00:10<00:01, 45.34it/s] 92%|█████████▏| 502/543 [00:11<00:00, 45.50it/s] 93%|█████████▎| 507/543 [00:11<00:00, 45.44it/s] 94%|█████████▍| 512/543 [00:11<00:00, 45.43it/s] 95%|█████████▌| 517/543 [00:11<00:00, 45.27it/s] 96%|█████████▌| 522/543 [00:11<00:00, 45.14it/s] 97%|█████████▋| 527/543 [00:11<00:00, 41.60it/s] 98%|█████████▊| 532/543 [00:11<00:00, 42.83it/s] 99%|█████████▉| 537/543 [00:11<00:00, 43.78it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.40it/s]100%|██████████| 543/543 [00:12<00:00, 45.11it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:52:43,949 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:43,950 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:43,950 >>   eval_loss               =     1.1326
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:43,950 >>   eval_runtime            = 0:00:12.05
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:43,950 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:43,950 >>   eval_samples_per_second =    360.196
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:43,950 >>   eval_steps_per_second   =     45.045
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:52:43,950 >>   perplexity              =     3.1036
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:52:57,046 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:52:57,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:52:57,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:52:57,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:52:57,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:52:57,546 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:52:57,547 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:52:57,846 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:52:59,018 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:52:59,018 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:01,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:01,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:01,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:01,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:53:01,620 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:53:02,215 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:53:02,217 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:53:02,738 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:53:02,972 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:53:02,972 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-376
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-470
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-282
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-188
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.75it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.75it/s]Extractor Predicting: 11it [00:06,  1.77it/s]Extractor Predicting: 12it [00:06,  1.78it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:08,  1.58it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:10,  1.54it/s]Extractor Predicting: 19it [00:11,  1.51it/s]Extractor Predicting: 20it [00:12,  1.49it/s]Extractor Predicting: 21it [00:12,  1.52it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:14,  1.54it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:16,  1.53it/s]Extractor Predicting: 28it [00:17,  1.53it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:18,  1.53it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:20,  1.54it/s]Extractor Predicting: 34it [00:21,  1.53it/s]Extractor Predicting: 35it [00:22,  1.44it/s]Extractor Predicting: 36it [00:22,  1.44it/s]Extractor Predicting: 37it [00:23,  1.46it/s]Extractor Predicting: 38it [00:24,  1.47it/s]Extractor Predicting: 39it [00:24,  1.50it/s]Extractor Predicting: 40it [00:25,  1.52it/s]Extractor Predicting: 41it [00:26,  1.50it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.52it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.55it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:32,  1.54it/s]Extractor Predicting: 52it [00:33,  1.53it/s]Extractor Predicting: 53it [00:33,  1.51it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:35,  1.52it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.57it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.54it/s]Extractor Predicting: 61it [00:39,  1.54it/s]Extractor Predicting: 62it [00:39,  1.55it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:41,  1.57it/s]Extractor Predicting: 65it [00:41,  1.61it/s]Extractor Predicting: 66it [00:42,  1.60it/s]Extractor Predicting: 67it [00:42,  1.60it/s]Extractor Predicting: 68it [00:43,  1.57it/s]Extractor Predicting: 69it [00:44,  1.58it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:45,  1.53it/s]Extractor Predicting: 72it [00:46,  1.55it/s]Extractor Predicting: 73it [00:46,  1.54it/s]Extractor Predicting: 74it [00:47,  1.52it/s]Extractor Predicting: 75it [00:48,  1.50it/s]Extractor Predicting: 76it [00:48,  1.50it/s]Extractor Predicting: 77it [00:49,  1.53it/s]Extractor Predicting: 78it [00:50,  1.53it/s]Extractor Predicting: 79it [00:50,  1.52it/s]Extractor Predicting: 80it [00:51,  1.55it/s]Extractor Predicting: 81it [00:52,  1.54it/s]Extractor Predicting: 82it [00:52,  1.55it/s]Extractor Predicting: 83it [00:53,  1.57it/s]Extractor Predicting: 84it [00:53,  1.58it/s]Extractor Predicting: 85it [00:54,  1.59it/s]Extractor Predicting: 86it [00:55,  1.57it/s]Extractor Predicting: 87it [00:55,  1.57it/s]Extractor Predicting: 88it [00:56,  1.60it/s]Extractor Predicting: 89it [00:57,  1.57it/s]Extractor Predicting: 90it [00:57,  1.55it/s]Extractor Predicting: 91it [00:58,  1.53it/s]Extractor Predicting: 92it [00:59,  1.52it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [01:00,  1.57it/s]Extractor Predicting: 95it [01:01,  1.43it/s]Extractor Predicting: 96it [01:01,  1.48it/s]Extractor Predicting: 97it [01:02,  1.52it/s]Extractor Predicting: 98it [01:03,  1.53it/s]Extractor Predicting: 99it [01:03,  1.49it/s]Extractor Predicting: 100it [01:04,  1.51it/s]Extractor Predicting: 101it [01:04,  1.54it/s]Extractor Predicting: 102it [01:05,  1.53it/s]Extractor Predicting: 103it [01:06,  1.55it/s]Extractor Predicting: 104it [01:06,  1.54it/s]Extractor Predicting: 105it [01:07,  1.52it/s]Extractor Predicting: 106it [01:08,  1.55it/s]Extractor Predicting: 107it [01:08,  1.55it/s]Extractor Predicting: 108it [01:09,  1.55it/s]Extractor Predicting: 109it [01:10,  1.54it/s]Extractor Predicting: 110it [01:10,  1.53it/s]Extractor Predicting: 111it [01:11,  1.54it/s]Extractor Predicting: 112it [01:12,  1.58it/s]Extractor Predicting: 113it [01:12,  1.59it/s]Extractor Predicting: 114it [01:13,  1.55it/s]Extractor Predicting: 115it [01:14,  1.55it/s]Extractor Predicting: 116it [01:14,  1.55it/s]Extractor Predicting: 117it [01:15,  1.56it/s]Extractor Predicting: 118it [01:15,  1.53it/s]Extractor Predicting: 119it [01:16,  1.52it/s]Extractor Predicting: 120it [01:17,  1.52it/s]Extractor Predicting: 121it [01:17,  1.51it/s]Extractor Predicting: 122it [01:18,  1.51it/s]Extractor Predicting: 123it [01:19,  1.55it/s]Extractor Predicting: 124it [01:19,  1.56it/s]Extractor Predicting: 125it [01:20,  1.57it/s]Extractor Predicting: 126it [01:21,  1.53it/s]Extractor Predicting: 127it [01:21,  1.53it/s]Extractor Predicting: 128it [01:22,  1.56it/s]Extractor Predicting: 129it [01:23,  1.54it/s]Extractor Predicting: 130it [01:23,  1.59it/s]Extractor Predicting: 131it [01:24,  1.56it/s]Extractor Predicting: 132it [01:25,  1.56it/s]Extractor Predicting: 133it [01:25,  1.57it/s]Extractor Predicting: 134it [01:26,  1.58it/s]Extractor Predicting: 135it [01:26,  1.60it/s]Extractor Predicting: 136it [01:27,  1.58it/s]Extractor Predicting: 137it [01:28,  1.59it/s]Extractor Predicting: 138it [01:28,  1.57it/s]Extractor Predicting: 139it [01:29,  1.56it/s]Extractor Predicting: 140it [01:30,  1.59it/s]Extractor Predicting: 141it [01:30,  1.59it/s]Extractor Predicting: 142it [01:31,  1.62it/s]Extractor Predicting: 143it [01:31,  1.62it/s]Extractor Predicting: 144it [01:32,  1.62it/s]Extractor Predicting: 145it [01:33,  1.59it/s]Extractor Predicting: 146it [01:33,  1.58it/s]Extractor Predicting: 147it [01:34,  1.59it/s]Extractor Predicting: 148it [01:35,  1.61it/s]Extractor Predicting: 149it [01:35,  1.62it/s]Extractor Predicting: 150it [01:36,  1.57it/s]Extractor Predicting: 151it [01:36,  1.57it/s]Extractor Predicting: 152it [01:37,  1.58it/s]Extractor Predicting: 153it [01:38,  1.55it/s]Extractor Predicting: 154it [01:38,  1.57it/s]Extractor Predicting: 155it [01:39,  1.54it/s]Extractor Predicting: 156it [01:40,  1.50it/s]Extractor Predicting: 157it [01:40,  1.52it/s]Extractor Predicting: 158it [01:41,  1.54it/s]Extractor Predicting: 159it [01:42,  1.57it/s]Extractor Predicting: 160it [01:42,  1.60it/s]Extractor Predicting: 161it [01:43,  1.59it/s]Extractor Predicting: 162it [01:43,  1.60it/s]Extractor Predicting: 163it [01:44,  1.50it/s]Extractor Predicting: 163it [01:44,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:01,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:01,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:01,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:01,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:01,236 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:55:01,550 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:55:01,552 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:01,828 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:02,887 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:02,887 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:05,631 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:05,666 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:05,666 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:05,666 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:05,666 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:55:06,011 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:55:06,012 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:06,701 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:06,860 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:06,860 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.09595959595959595,
  "recall": 0.0262551819438047,
  "score": 0.04122965641952984,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.61it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.60it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.64it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:19,  1.63it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:20,  1.61it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.60it/s]Extractor Predicting: 38it [00:23,  1.57it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:25,  1.55it/s]Extractor Predicting: 42it [00:26,  1.56it/s]Extractor Predicting: 43it [00:26,  1.54it/s]Extractor Predicting: 44it [00:27,  1.54it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:28,  1.51it/s]Extractor Predicting: 47it [00:29,  1.50it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:30,  1.51it/s]Extractor Predicting: 50it [00:31,  1.53it/s]Extractor Predicting: 51it [00:31,  1.53it/s]Extractor Predicting: 52it [00:32,  1.53it/s]Extractor Predicting: 53it [00:33,  1.51it/s]Extractor Predicting: 54it [00:34,  1.47it/s]Extractor Predicting: 55it [00:34,  1.51it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:35,  1.54it/s]Extractor Predicting: 58it [00:36,  1.54it/s]Extractor Predicting: 59it [00:37,  1.37it/s]Extractor Predicting: 60it [00:38,  1.41it/s]Extractor Predicting: 61it [00:38,  1.43it/s]Extractor Predicting: 62it [00:39,  1.45it/s]Extractor Predicting: 63it [00:40,  1.43it/s]Extractor Predicting: 64it [00:40,  1.45it/s]Extractor Predicting: 65it [00:41,  1.47it/s]Extractor Predicting: 66it [00:42,  1.46it/s]Extractor Predicting: 67it [00:42,  1.48it/s]Extractor Predicting: 68it [00:43,  1.44it/s]Extractor Predicting: 69it [00:44,  1.46it/s]Extractor Predicting: 70it [00:44,  1.48it/s]Extractor Predicting: 71it [00:45,  1.50it/s]Extractor Predicting: 72it [00:46,  1.50it/s]Extractor Predicting: 73it [00:46,  1.48it/s]Extractor Predicting: 74it [00:47,  1.51it/s]Extractor Predicting: 75it [00:48,  1.52it/s]Extractor Predicting: 76it [00:48,  1.54it/s]Extractor Predicting: 77it [00:49,  1.56it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:50,  1.52it/s]Extractor Predicting: 80it [00:51,  1.55it/s]Extractor Predicting: 81it [00:52,  1.43it/s]Extractor Predicting: 82it [00:52,  1.48it/s]Extractor Predicting: 83it [00:53,  1.50it/s]Extractor Predicting: 84it [00:54,  1.47it/s]Extractor Predicting: 85it [00:54,  1.49it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:56,  1.55it/s]Extractor Predicting: 88it [00:56,  1.50it/s]Extractor Predicting: 89it [00:57,  1.55it/s]Extractor Predicting: 90it [00:58,  1.56it/s]Extractor Predicting: 91it [00:58,  1.59it/s]Extractor Predicting: 92it [00:59,  1.59it/s]Extractor Predicting: 93it [00:59,  1.57it/s]Extractor Predicting: 94it [01:00,  1.56it/s]Extractor Predicting: 95it [01:01,  1.58it/s]Extractor Predicting: 96it [01:01,  1.61it/s]Extractor Predicting: 97it [01:02,  1.60it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:03,  1.54it/s]Extractor Predicting: 100it [01:04,  1.50it/s]Extractor Predicting: 101it [01:05,  1.51it/s]Extractor Predicting: 102it [01:05,  1.53it/s]Extractor Predicting: 103it [01:06,  1.52it/s]Extractor Predicting: 104it [01:07,  1.55it/s]Extractor Predicting: 105it [01:07,  1.54it/s]Extractor Predicting: 106it [01:08,  1.51it/s]Extractor Predicting: 107it [01:09,  1.48it/s]Extractor Predicting: 108it [01:09,  1.52it/s]Extractor Predicting: 109it [01:10,  1.46it/s]Extractor Predicting: 110it [01:11,  1.44it/s]Extractor Predicting: 111it [01:11,  1.44it/s]Extractor Predicting: 112it [01:12,  1.48it/s]Extractor Predicting: 113it [01:13,  1.49it/s]Extractor Predicting: 114it [01:13,  1.50it/s]Extractor Predicting: 115it [01:14,  1.49it/s]Extractor Predicting: 116it [01:15,  1.48it/s]Extractor Predicting: 117it [01:15,  1.50it/s]Extractor Predicting: 118it [01:16,  1.54it/s]Extractor Predicting: 119it [01:17,  1.54it/s]Extractor Predicting: 120it [01:17,  1.58it/s]Extractor Predicting: 121it [01:18,  1.58it/s]Extractor Predicting: 122it [01:18,  1.60it/s]Extractor Predicting: 123it [01:19,  1.60it/s]Extractor Predicting: 124it [01:20,  1.63it/s]Extractor Predicting: 125it [01:20,  1.62it/s]Extractor Predicting: 126it [01:21,  1.58it/s]Extractor Predicting: 127it [01:22,  1.58it/s]Extractor Predicting: 128it [01:22,  1.56it/s]Extractor Predicting: 129it [01:23,  1.56it/s]Extractor Predicting: 130it [01:24,  1.56it/s]Extractor Predicting: 131it [01:24,  1.61it/s]Extractor Predicting: 132it [01:25,  1.59it/s]Extractor Predicting: 133it [01:25,  1.59it/s]Extractor Predicting: 134it [01:26,  1.60it/s]Extractor Predicting: 135it [01:27,  1.60it/s]Extractor Predicting: 136it [01:27,  1.61it/s]Extractor Predicting: 137it [01:28,  1.65it/s]Extractor Predicting: 138it [01:28,  1.63it/s]Extractor Predicting: 139it [01:29,  1.63it/s]Extractor Predicting: 140it [01:30,  1.61it/s]Extractor Predicting: 141it [01:30,  1.62it/s]Extractor Predicting: 142it [01:31,  1.60it/s]Extractor Predicting: 143it [01:32,  1.58it/s]Extractor Predicting: 144it [01:32,  1.59it/s]Extractor Predicting: 145it [01:33,  1.54it/s]Extractor Predicting: 146it [01:34,  1.49it/s]Extractor Predicting: 147it [01:34,  1.51it/s]Extractor Predicting: 148it [01:35,  1.51it/s]Extractor Predicting: 149it [01:36,  1.53it/s]Extractor Predicting: 150it [01:36,  1.54it/s]Extractor Predicting: 151it [01:37,  1.55it/s]Extractor Predicting: 152it [01:38,  1.38it/s]Extractor Predicting: 153it [01:38,  1.45it/s]Extractor Predicting: 154it [01:39,  1.50it/s]Extractor Predicting: 155it [01:40,  1.51it/s]Extractor Predicting: 156it [01:40,  1.50it/s]Extractor Predicting: 157it [01:41,  1.52it/s]Extractor Predicting: 158it [01:42,  1.54it/s]Extractor Predicting: 159it [01:42,  1.54it/s]Extractor Predicting: 160it [01:43,  1.56it/s]Extractor Predicting: 161it [01:44,  1.50it/s]Extractor Predicting: 162it [01:44,  1.55it/s]Extractor Predicting: 163it [01:45,  1.58it/s]Extractor Predicting: 164it [01:45,  1.57it/s]Extractor Predicting: 165it [01:46,  1.60it/s]Extractor Predicting: 166it [01:47,  1.61it/s]Extractor Predicting: 167it [01:47,  1.60it/s]Extractor Predicting: 168it [01:48,  1.66it/s]Extractor Predicting: 169it [01:48,  1.67it/s]Extractor Predicting: 170it [01:49,  1.63it/s]Extractor Predicting: 171it [01:50,  1.61it/s]Extractor Predicting: 172it [01:50,  1.61it/s]Extractor Predicting: 173it [01:51,  1.58it/s]Extractor Predicting: 174it [01:52,  1.62it/s]Extractor Predicting: 175it [01:52,  1.61it/s]Extractor Predicting: 176it [01:53,  1.58it/s]Extractor Predicting: 177it [01:54,  1.57it/s]Extractor Predicting: 178it [01:54,  1.57it/s]Extractor Predicting: 179it [01:55,  1.65it/s]Extractor Predicting: 180it [01:55,  1.62it/s]Extractor Predicting: 181it [01:56,  1.61it/s]Extractor Predicting: 182it [01:57,  1.59it/s]Extractor Predicting: 183it [01:57,  1.59it/s]Extractor Predicting: 184it [01:58,  1.58it/s]Extractor Predicting: 185it [01:58,  1.61it/s]Extractor Predicting: 186it [01:59,  1.58it/s]Extractor Predicting: 187it [02:00,  1.55it/s]Extractor Predicting: 188it [02:00,  1.53it/s]Extractor Predicting: 189it [02:01,  1.53it/s]Extractor Predicting: 190it [02:02,  1.50it/s]Extractor Predicting: 191it [02:02,  1.51it/s]Extractor Predicting: 192it [02:03,  1.53it/s]Extractor Predicting: 193it [02:04,  1.57it/s]Extractor Predicting: 194it [02:04,  1.59it/s]Extractor Predicting: 195it [02:05,  1.54it/s]Extractor Predicting: 196it [02:06,  1.56it/s]Extractor Predicting: 197it [02:06,  1.56it/s]Extractor Predicting: 198it [02:07,  1.58it/s]Extractor Predicting: 199it [02:08,  1.57it/s]Extractor Predicting: 200it [02:08,  1.56it/s]Extractor Predicting: 201it [02:09,  1.57it/s]Extractor Predicting: 202it [02:09,  1.58it/s]Extractor Predicting: 203it [02:10,  1.56it/s]Extractor Predicting: 204it [02:11,  1.58it/s]Extractor Predicting: 205it [02:11,  1.57it/s]Extractor Predicting: 206it [02:12,  1.56it/s]Extractor Predicting: 207it [02:13,  1.58it/s]Extractor Predicting: 208it [02:13,  1.58it/s]Extractor Predicting: 209it [02:14,  1.58it/s]Extractor Predicting: 210it [02:15,  1.58it/s]Extractor Predicting: 211it [02:15,  1.56it/s]Extractor Predicting: 212it [02:16,  1.58it/s]Extractor Predicting: 213it [02:16,  1.58it/s]Extractor Predicting: 214it [02:17,  1.58it/s]Extractor Predicting: 215it [02:18,  1.53it/s]Extractor Predicting: 216it [02:18,  1.56it/s]Extractor Predicting: 217it [02:19,  1.59it/s]Extractor Predicting: 218it [02:20,  1.59it/s]Extractor Predicting: 219it [02:20,  1.56it/s]Extractor Predicting: 220it [02:21,  1.57it/s]Extractor Predicting: 221it [02:22,  1.56it/s]Extractor Predicting: 222it [02:22,  1.58it/s]Extractor Predicting: 223it [02:23,  1.52it/s]Extractor Predicting: 224it [02:24,  1.53it/s]Extractor Predicting: 225it [02:24,  1.51it/s]Extractor Predicting: 226it [02:25,  1.53it/s]Extractor Predicting: 227it [02:26,  1.47it/s]Extractor Predicting: 228it [02:26,  1.43it/s]Extractor Predicting: 229it [02:27,  1.46it/s]Extractor Predicting: 230it [02:28,  1.48it/s]Extractor Predicting: 231it [02:28,  1.49it/s]Extractor Predicting: 232it [02:29,  1.50it/s]Extractor Predicting: 233it [02:30,  1.51it/s]Extractor Predicting: 234it [02:30,  1.52it/s]Extractor Predicting: 235it [02:31,  1.52it/s]Extractor Predicting: 236it [02:32,  1.54it/s]Extractor Predicting: 237it [02:32,  1.54it/s]Extractor Predicting: 238it [02:33,  1.60it/s]Extractor Predicting: 239it [02:33,  1.62it/s]Extractor Predicting: 240it [02:34,  1.67it/s]Extractor Predicting: 241it [02:35,  1.66it/s]Extractor Predicting: 242it [02:35,  1.65it/s]Extractor Predicting: 243it [02:36,  1.58it/s]Extractor Predicting: 244it [02:36,  1.59it/s]Extractor Predicting: 245it [02:37,  1.63it/s]Extractor Predicting: 246it [02:38,  1.62it/s]Extractor Predicting: 247it [02:38,  1.63it/s]Extractor Predicting: 248it [02:39,  1.60it/s]Extractor Predicting: 249it [02:39,  1.66it/s]Extractor Predicting: 250it [02:40,  1.67it/s]Extractor Predicting: 251it [02:41,  1.73it/s]Extractor Predicting: 252it [02:41,  1.75it/s]Extractor Predicting: 253it [02:42,  1.72it/s]Extractor Predicting: 254it [02:42,  1.71it/s]Extractor Predicting: 255it [02:43,  1.67it/s]Extractor Predicting: 256it [02:44,  1.72it/s]Extractor Predicting: 257it [02:44,  1.47it/s]Extractor Predicting: 258it [02:45,  1.49it/s]Extractor Predicting: 259it [02:46,  1.46it/s]Extractor Predicting: 260it [02:46,  1.55it/s]Extractor Predicting: 261it [02:47,  1.54it/s]Extractor Predicting: 262it [02:48,  1.57it/s]Extractor Predicting: 263it [02:48,  1.62it/s]Extractor Predicting: 264it [02:49,  1.63it/s]Extractor Predicting: 265it [02:49,  1.64it/s]Extractor Predicting: 266it [02:50,  1.66it/s]Extractor Predicting: 267it [02:51,  1.69it/s]Extractor Predicting: 268it [02:51,  1.68it/s]Extractor Predicting: 269it [02:52,  1.65it/s]Extractor Predicting: 270it [02:52,  1.65it/s]Extractor Predicting: 271it [02:53,  1.65it/s]Extractor Predicting: 272it [02:54,  1.63it/s]Extractor Predicting: 273it [02:54,  1.68it/s]Extractor Predicting: 274it [02:55,  1.69it/s]Extractor Predicting: 275it [02:55,  1.68it/s]Extractor Predicting: 276it [02:56,  1.64it/s]Extractor Predicting: 277it [02:57,  1.63it/s]Extractor Predicting: 278it [02:57,  1.67it/s]Extractor Predicting: 279it [02:58,  1.62it/s]Extractor Predicting: 280it [02:58,  1.68it/s]Extractor Predicting: 281it [02:59,  1.68it/s]Extractor Predicting: 282it [03:00,  1.70it/s]Extractor Predicting: 283it [03:00,  1.71it/s]Extractor Predicting: 284it [03:01,  1.70it/s]Extractor Predicting: 285it [03:01,  1.71it/s]Extractor Predicting: 286it [03:02,  1.70it/s]Extractor Predicting: 287it [03:03,  1.68it/s]Extractor Predicting: 288it [03:03,  1.65it/s]Extractor Predicting: 289it [03:04,  1.65it/s]Extractor Predicting: 290it [03:04,  1.58it/s]Extractor Predicting: 291it [03:05,  1.61it/s]Extractor Predicting: 292it [03:06,  1.66it/s]Extractor Predicting: 293it [03:06,  1.67it/s]Extractor Predicting: 294it [03:07,  1.67it/s]Extractor Predicting: 295it [03:07,  1.68it/s]Extractor Predicting: 296it [03:08,  1.60it/s]Extractor Predicting: 297it [03:09,  1.59it/s]Extractor Predicting: 298it [03:09,  1.54it/s]Extractor Predicting: 299it [03:10,  1.53it/s]Extractor Predicting: 300it [03:11,  1.56it/s]Extractor Predicting: 301it [03:11,  1.56it/s]Extractor Predicting: 302it [03:12,  1.57it/s]Extractor Predicting: 303it [03:13,  1.54it/s]Extractor Predicting: 304it [03:13,  1.54it/s]Extractor Predicting: 305it [03:14,  1.57it/s]Extractor Predicting: 306it [03:15,  1.54it/s]Extractor Predicting: 307it [03:15,  1.51it/s]Extractor Predicting: 308it [03:16,  1.51it/s]Extractor Predicting: 309it [03:17,  1.54it/s]Extractor Predicting: 310it [03:17,  1.58it/s]Extractor Predicting: 311it [03:18,  1.58it/s]Extractor Predicting: 312it [03:18,  1.56it/s]Extractor Predicting: 313it [03:19,  1.53it/s]Extractor Predicting: 314it [03:20,  1.51it/s]Extractor Predicting: 315it [03:20,  1.51it/s]Extractor Predicting: 316it [03:21,  1.51it/s]Extractor Predicting: 317it [03:22,  1.52it/s]Extractor Predicting: 318it [03:22,  1.53it/s]Extractor Predicting: 319it [03:23,  1.54it/s]Extractor Predicting: 320it [03:24,  1.54it/s]Extractor Predicting: 321it [03:24,  1.60it/s]Extractor Predicting: 322it [03:25,  1.60it/s]Extractor Predicting: 323it [03:26,  1.58it/s]Extractor Predicting: 324it [03:26,  1.55it/s]Extractor Predicting: 325it [03:27,  1.55it/s]Extractor Predicting: 326it [03:28,  1.54it/s]Extractor Predicting: 327it [03:28,  1.51it/s]Extractor Predicting: 328it [03:29,  1.52it/s]Extractor Predicting: 329it [03:29,  1.56it/s]Extractor Predicting: 330it [03:30,  1.59it/s]Extractor Predicting: 331it [03:31,  1.76it/s]Extractor Predicting: 331it [03:31,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:51,817 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:51,857 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:51,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:51,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:51,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:58:52,830 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:58:52,831 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:58:53,497 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:58:54,702 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:58:54,702 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:57,799 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:57,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:57,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:57,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:57,804 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:58:58,465 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:58:58,466 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:58:59,042 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:58:59,218 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:58:59,218 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.22836199605300253,
  "recall": 0.10213087882990796,
  "score": 0.1411395713538944,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.50it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.48it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.41it/s]Extractor Predicting: 19it [00:12,  1.43it/s]Extractor Predicting: 20it [00:13,  1.42it/s]Extractor Predicting: 21it [00:14,  1.43it/s]Extractor Predicting: 22it [00:14,  1.43it/s]Extractor Predicting: 23it [00:15,  1.42it/s]Extractor Predicting: 24it [00:16,  1.41it/s]Extractor Predicting: 25it [00:17,  1.43it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.50it/s]Extractor Predicting: 28it [00:18,  1.48it/s]Extractor Predicting: 29it [00:19,  1.48it/s]Extractor Predicting: 30it [00:20,  1.49it/s]Extractor Predicting: 31it [00:20,  1.48it/s]Extractor Predicting: 32it [00:21,  1.50it/s]Extractor Predicting: 33it [00:22,  1.52it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:24,  1.51it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:26,  1.48it/s]Extractor Predicting: 40it [00:26,  1.49it/s]Extractor Predicting: 41it [00:27,  1.49it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:30,  1.51it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.53it/s]Extractor Predicting: 49it [00:32,  1.55it/s]Extractor Predicting: 50it [00:33,  1.57it/s]Extractor Predicting: 51it [00:34,  1.56it/s]Extractor Predicting: 52it [00:34,  1.55it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:36,  1.52it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:37,  1.58it/s]Extractor Predicting: 57it [00:37,  1.63it/s]Extractor Predicting: 58it [00:38,  1.69it/s]Extractor Predicting: 59it [00:38,  1.77it/s]Extractor Predicting: 60it [00:39,  1.84it/s]Extractor Predicting: 61it [00:39,  1.88it/s]Extractor Predicting: 62it [00:40,  1.86it/s]Extractor Predicting: 63it [00:41,  1.88it/s]Extractor Predicting: 64it [00:41,  1.87it/s]Extractor Predicting: 65it [00:42,  1.86it/s]Extractor Predicting: 66it [00:42,  1.86it/s]Extractor Predicting: 67it [00:43,  1.86it/s]Extractor Predicting: 68it [00:43,  1.84it/s]Extractor Predicting: 69it [00:44,  1.89it/s]Extractor Predicting: 70it [00:44,  1.85it/s]Extractor Predicting: 71it [00:45,  1.86it/s]Extractor Predicting: 72it [00:45,  1.89it/s]Extractor Predicting: 73it [00:46,  1.92it/s]Extractor Predicting: 74it [00:46,  1.92it/s]Extractor Predicting: 75it [00:47,  1.91it/s]Extractor Predicting: 76it [00:47,  1.90it/s]Extractor Predicting: 77it [00:48,  1.96it/s]Extractor Predicting: 78it [00:48,  1.89it/s]Extractor Predicting: 79it [00:49,  1.89it/s]Extractor Predicting: 80it [00:50,  1.88it/s]Extractor Predicting: 81it [00:50,  1.87it/s]Extractor Predicting: 82it [00:51,  1.90it/s]Extractor Predicting: 83it [00:51,  1.88it/s]Extractor Predicting: 84it [00:52,  1.88it/s]Extractor Predicting: 85it [00:52,  1.90it/s]Extractor Predicting: 86it [00:53,  1.77it/s]Extractor Predicting: 87it [00:53,  1.72it/s]Extractor Predicting: 88it [00:54,  1.69it/s]Extractor Predicting: 89it [00:55,  1.67it/s]Extractor Predicting: 90it [00:55,  1.68it/s]Extractor Predicting: 91it [00:56,  1.66it/s]Extractor Predicting: 92it [00:57,  1.61it/s]Extractor Predicting: 93it [00:57,  1.63it/s]Extractor Predicting: 94it [00:58,  1.62it/s]Extractor Predicting: 95it [00:58,  1.66it/s]Extractor Predicting: 96it [00:59,  1.66it/s]Extractor Predicting: 97it [01:00,  1.67it/s]Extractor Predicting: 98it [01:00,  1.67it/s]Extractor Predicting: 99it [01:01,  1.65it/s]Extractor Predicting: 100it [01:01,  1.58it/s]Extractor Predicting: 101it [01:02,  1.61it/s]Extractor Predicting: 102it [01:03,  1.61it/s]Extractor Predicting: 103it [01:03,  1.57it/s]Extractor Predicting: 104it [01:04,  1.55it/s]Extractor Predicting: 105it [01:05,  1.54it/s]Extractor Predicting: 106it [01:05,  1.53it/s]Extractor Predicting: 107it [01:06,  1.51it/s]Extractor Predicting: 108it [01:07,  1.45it/s]Extractor Predicting: 108it [01:07,  1.61it/s]
[INFO|configuration_utils.py:515] 2023-08-29 01:00:08,679 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:00:08,680 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:00:08,747 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:00:08,748 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 01:00:08,769 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:00:20,258 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 01:00:20,286 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 01:00:20,466 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:00:20,466 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:00:20,548 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:00:20,606 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:00:20,606 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:00:20,606 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:00:20,606 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:00:20,606 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:00:20,606 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5224719101123596,
  "recall": 0.10427679000480539,
  "score": 0.17385498731472826,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 01:00:21,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:21,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:22,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:23,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:23,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:24,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:25,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:25,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:26,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:27,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:28,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:28,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:29,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:29,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:30,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:30,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:31,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:32,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:32,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:33,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:02, 13.04s/it][WARNING|generation_utils.py:914] 2023-08-29 01:00:34,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:34,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:35,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:35,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:36,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:37,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:37,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:38,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:38,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:39,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:39,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:40,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:41,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:41,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:42,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:43,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:43,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:44,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:45,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:45,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:25<02:43, 12.59s/it][WARNING|generation_utils.py:914] 2023-08-29 01:00:46,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:46,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:47,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:48,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:49,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:49,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:50,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:50,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:51,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:51,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:52,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:53,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:53,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:54,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:55,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:55,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:56,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:56,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:57,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:58,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:58,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:38<02:33, 12.76s/it][WARNING|generation_utils.py:914] 2023-08-29 01:00:59,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:00:59,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:00,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:01,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:01,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:02,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:02,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:03,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:03,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:04,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:05,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:05,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:06,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:07,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:07,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:08,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:08,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:09,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:09,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:10,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:50<02:15, 12.36s/it][WARNING|generation_utils.py:914] 2023-08-29 01:01:11,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:11,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:12,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:12,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:13,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:13,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:14,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:15,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:15,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:16,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:16,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:17,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:17,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:18,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:19,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:19,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:20,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:20,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:21,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:22,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:01<02:00, 12.09s/it][WARNING|generation_utils.py:914] 2023-08-29 01:01:22,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:23,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:23,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:24,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:24,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:25,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:26,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:26,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:27,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:27,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:28,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:28,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:29,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:30,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:30,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:31,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:31,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:32,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:32,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:33,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:34,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:13<01:48, 12.07s/it][WARNING|generation_utils.py:914] 2023-08-29 01:01:34,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:35,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:35,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:36,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:36,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:37,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:38,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:38,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:39,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:39,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:40,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:40,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:41,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:42,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:42,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:43,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:43,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:44,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:44,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:45,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:25<01:34, 11.84s/it][WARNING|generation_utils.py:914] 2023-08-29 01:01:46,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:46,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:47,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:47,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:48,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:48,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:49,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:50,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:50,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:51,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:51,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:52,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:52,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:53,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:53,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:54,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:55,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:55,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:56,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:56,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:36<01:21, 11.58s/it][WARNING|generation_utils.py:914] 2023-08-29 01:01:57,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:57,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:58,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:58,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:59,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:01:59,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:00,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:00,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:01,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:01,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:02,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:02,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:03,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:03,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:04,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:04,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:05,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:05,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:06,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:06,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:46<01:06, 11.14s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:07,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:07,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:08,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:09,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:09,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:10,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:10,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:11,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:11,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:12,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:13,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:13,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:14,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:14,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:15,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:15,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:16,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:16,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:17,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:17,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [01:57<00:56, 11.21s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:18,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:19,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:19,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:20,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:20,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:21,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:22,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:22,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:23,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:23,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:24,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:24,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:25,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:25,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:26,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:26,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:27,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:28,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:28,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:29,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:29,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:09<00:45, 11.29s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:30,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:30,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:31,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:31,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:32,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:33,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:33,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:34,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:34,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:35,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:35,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:36,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:37,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:37,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:38,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:38,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:39,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:40,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:40,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:41,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:20<00:34, 11.43s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:41,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:42,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:43,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:43,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:44,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:44,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:45,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:45,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:46,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:47,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:47,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:48,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:48,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:49,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:49,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:50,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:50,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:51,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:52,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:52,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:32<00:22, 11.43s/it][WARNING|generation_utils.py:914] 2023-08-29 01:02:53,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:53,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:54,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:54,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:55,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:55,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:56,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:56,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:57,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:58,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:58,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:58,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:59,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:02:59,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:00,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:00,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:01,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:01,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:02,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:02,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:42<00:11, 11.02s/it][WARNING|generation_utils.py:914] 2023-08-29 01:03:03,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:03,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:04,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:05,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:05,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:06,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:07,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:07,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:08,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:08,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:09,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:09,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:10,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:11,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:11,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:12,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:12,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:13,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:14,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:03:14,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [02:54<00:00, 11.31s/it]Generating: 100%|██████████| 15/15 [02:54<00:00, 11.62s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:22,678 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:22,700 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:22,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:22,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:22,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:03:23,574 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:03:23,576 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:03:24,188 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:03:25,251 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:03:25,251 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:28,256 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:28,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:28,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:28,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:28,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:03:28,925 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:03:28,926 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:03:29,508 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:03:29,678 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:03:29,678 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.9390625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : made from material .', 'success_rate': 0.9671875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 472, 'raw': 480}
{'target': 600, 'success': 504, 'raw': 512}
{'target': 600, 'success': 536, 'raw': 544}
{'target': 600, 'success': 568, 'raw': 576}
{'target': 600, 'success': 598, 'raw': 608}
{'target': 600, 'success': 628, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.98125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : league .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9375, 'errors': {'', "('freshwater', 'located in or next to body of water', '', 'It is a popular drinking water source for several species of fish , including fish that feed on freshwater as it feeds on other species .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : mother .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : residence .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9421875, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 7178
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7278, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.56it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:01,  1.63it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.64it/s]Extractor Estimating: 8it [00:05,  1.56it/s]Extractor Estimating: 9it [00:05,  1.54it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:06,  1.60it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.60it/s]Extractor Estimating: 15it [00:09,  1.49it/s]Extractor Estimating: 16it [00:10,  1.51it/s]Extractor Estimating: 17it [00:10,  1.52it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.62it/s]Extractor Estimating: 20it [00:12,  1.66it/s]Extractor Estimating: 21it [00:13,  1.64it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:15,  1.67it/s]Extractor Estimating: 25it [00:15,  1.62it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:16,  1.63it/s]Extractor Estimating: 28it [00:17,  1.58it/s]Extractor Estimating: 29it [00:18,  1.59it/s]Extractor Estimating: 30it [00:18,  1.61it/s]Extractor Estimating: 31it [00:19,  1.57it/s]Extractor Estimating: 32it [00:20,  1.49it/s]Extractor Estimating: 33it [00:20,  1.49it/s]Extractor Estimating: 34it [00:21,  1.55it/s]Extractor Estimating: 35it [00:22,  1.59it/s]Extractor Estimating: 36it [00:22,  1.57it/s]Extractor Estimating: 37it [00:23,  1.60it/s]Extractor Estimating: 38it [00:23,  1.59it/s]Extractor Estimating: 39it [00:24,  1.57it/s]Extractor Estimating: 40it [00:25,  1.60it/s]Extractor Estimating: 41it [00:25,  1.57it/s]Extractor Estimating: 42it [00:26,  1.44it/s]Extractor Estimating: 43it [00:27,  1.45it/s]Extractor Estimating: 44it [00:28,  1.48it/s]Extractor Estimating: 45it [00:28,  1.51it/s]Extractor Estimating: 46it [00:29,  1.52it/s]Extractor Estimating: 47it [00:29,  1.54it/s]Extractor Estimating: 48it [00:30,  1.52it/s]Extractor Estimating: 49it [00:31,  1.51it/s]Extractor Estimating: 50it [00:32,  1.48it/s]Extractor Estimating: 51it [00:32,  1.54it/s]Extractor Estimating: 52it [00:33,  1.57it/s]Extractor Estimating: 53it [00:33,  1.51it/s]Extractor Estimating: 54it [00:34,  1.60it/s]Extractor Estimating: 55it [00:35,  1.58it/s]Extractor Estimating: 56it [00:35,  1.63it/s]Extractor Estimating: 57it [00:36,  1.65it/s]Extractor Estimating: 58it [00:36,  1.69it/s]Extractor Estimating: 59it [00:37,  1.73it/s]Extractor Estimating: 60it [00:38,  1.69it/s]Extractor Estimating: 61it [00:38,  1.74it/s]Extractor Estimating: 62it [00:39,  1.73it/s]Extractor Estimating: 63it [00:39,  1.65it/s]Extractor Estimating: 64it [00:40,  1.69it/s]Extractor Estimating: 65it [00:40,  1.71it/s]Extractor Estimating: 66it [00:41,  1.69it/s]Extractor Estimating: 67it [00:42,  1.72it/s]Extractor Estimating: 68it [00:42,  1.73it/s]Extractor Estimating: 69it [00:43,  1.76it/s]Extractor Estimating: 70it [00:43,  1.77it/s]Extractor Estimating: 71it [00:44,  1.75it/s]Extractor Estimating: 72it [00:44,  1.74it/s]Extractor Estimating: 73it [00:45,  1.75it/s]Extractor Estimating: 74it [00:46,  1.71it/s]Extractor Estimating: 75it [00:46,  1.68it/s]Extractor Estimating: 76it [00:47,  1.62it/s]Extractor Estimating: 77it [00:48,  1.61it/s]Extractor Estimating: 78it [00:48,  1.60it/s]Extractor Estimating: 79it [00:49,  1.56it/s]Extractor Estimating: 80it [00:50,  1.55it/s]Extractor Estimating: 81it [00:50,  1.60it/s]Extractor Estimating: 82it [00:51,  1.64it/s]Extractor Estimating: 83it [00:51,  1.65it/s]Extractor Estimating: 84it [00:52,  1.66it/s]Extractor Estimating: 85it [00:52,  1.67it/s]Extractor Estimating: 86it [00:53,  1.64it/s]Extractor Estimating: 87it [00:54,  1.61it/s]Extractor Estimating: 88it [00:55,  1.46it/s]Extractor Estimating: 89it [00:55,  1.44it/s]Extractor Estimating: 90it [00:56,  1.48it/s]Extractor Estimating: 91it [00:57,  1.45it/s]Extractor Estimating: 92it [00:57,  1.52it/s]Extractor Estimating: 93it [00:58,  1.55it/s]Extractor Estimating: 94it [00:58,  1.59it/s]Extractor Estimating: 95it [00:59,  1.59it/s]Extractor Estimating: 96it [01:00,  1.59it/s]Extractor Estimating: 97it [01:00,  1.60it/s]Extractor Estimating: 98it [01:01,  1.67it/s]Extractor Estimating: 99it [01:01,  1.64it/s]Extractor Estimating: 100it [01:02,  1.63it/s]Extractor Estimating: 101it [01:03,  1.67it/s]Extractor Estimating: 102it [01:03,  1.65it/s]Extractor Estimating: 103it [01:04,  1.65it/s]Extractor Estimating: 104it [01:04,  1.68it/s]Extractor Estimating: 105it [01:05,  1.69it/s]Extractor Estimating: 106it [01:06,  1.69it/s]Extractor Estimating: 107it [01:06,  1.66it/s]Extractor Estimating: 108it [01:07,  1.71it/s]Extractor Estimating: 109it [01:07,  1.73it/s]Extractor Estimating: 110it [01:08,  1.72it/s]Extractor Estimating: 111it [01:09,  1.74it/s]Extractor Estimating: 112it [01:09,  1.76it/s]Extractor Estimating: 113it [01:10,  1.76it/s]Extractor Estimating: 114it [01:10,  1.80it/s]Extractor Estimating: 115it [01:11,  1.74it/s]Extractor Estimating: 116it [01:11,  1.73it/s]Extractor Estimating: 117it [01:12,  1.73it/s]Extractor Estimating: 118it [01:13,  1.73it/s]Extractor Estimating: 119it [01:13,  1.73it/s]Extractor Estimating: 120it [01:14,  1.74it/s]Extractor Estimating: 121it [01:14,  1.73it/s]Extractor Estimating: 122it [01:15,  1.74it/s]Extractor Estimating: 123it [01:15,  1.70it/s]Extractor Estimating: 124it [01:16,  1.73it/s]Extractor Estimating: 125it [01:17,  1.69it/s]Extractor Estimating: 126it [01:17,  1.66it/s]Extractor Estimating: 127it [01:18,  1.61it/s]Extractor Estimating: 128it [01:19,  1.62it/s]Extractor Estimating: 129it [01:19,  1.66it/s]Extractor Estimating: 130it [01:20,  1.65it/s]Extractor Estimating: 131it [01:20,  1.66it/s]Extractor Estimating: 132it [01:21,  1.64it/s]Extractor Estimating: 133it [01:22,  1.58it/s]Extractor Estimating: 134it [01:22,  1.61it/s]Extractor Estimating: 135it [01:23,  1.64it/s]Extractor Estimating: 136it [01:23,  1.68it/s]Extractor Estimating: 137it [01:24,  1.65it/s]Extractor Estimating: 138it [01:25,  1.67it/s]Extractor Estimating: 139it [01:25,  1.65it/s]Extractor Estimating: 140it [01:26,  1.67it/s]Extractor Estimating: 141it [01:26,  1.69it/s]Extractor Estimating: 142it [01:27,  1.65it/s]Extractor Estimating: 143it [01:28,  1.65it/s]Extractor Estimating: 144it [01:28,  1.70it/s]Extractor Estimating: 145it [01:29,  1.62it/s]Extractor Estimating: 146it [01:29,  1.64it/s]Extractor Estimating: 147it [01:30,  1.63it/s]Extractor Estimating: 148it [01:31,  1.63it/s]Extractor Estimating: 149it [01:31,  1.64it/s]Extractor Estimating: 150it [01:32,  1.59it/s]Extractor Estimating: 151it [01:33,  1.61it/s]Extractor Estimating: 152it [01:33,  1.62it/s]Extractor Estimating: 153it [01:34,  1.62it/s]Extractor Estimating: 154it [01:34,  1.65it/s]Extractor Estimating: 155it [01:35,  1.66it/s]Extractor Estimating: 156it [01:36,  1.61it/s]Extractor Estimating: 157it [01:36,  1.62it/s]Extractor Estimating: 158it [01:37,  1.66it/s]Extractor Estimating: 159it [01:37,  1.64it/s]Extractor Estimating: 160it [01:38,  1.59it/s]Extractor Estimating: 161it [01:39,  1.55it/s]Extractor Estimating: 162it [01:40,  1.46it/s]Extractor Estimating: 163it [01:40,  1.52it/s]Extractor Estimating: 164it [01:41,  1.56it/s]Extractor Estimating: 165it [01:41,  1.61it/s]Extractor Estimating: 166it [01:42,  1.65it/s]Extractor Estimating: 167it [01:42,  1.67it/s]Extractor Estimating: 168it [01:43,  1.65it/s]Extractor Estimating: 169it [01:44,  1.65it/s]Extractor Estimating: 170it [01:44,  1.63it/s]Extractor Estimating: 171it [01:45,  1.65it/s]Extractor Estimating: 172it [01:46,  1.62it/s]Extractor Estimating: 173it [01:46,  1.65it/s]Extractor Estimating: 174it [01:47,  1.65it/s]Extractor Estimating: 175it [01:47,  1.60it/s]Extractor Estimating: 176it [01:48,  1.62it/s]Extractor Estimating: 177it [01:49,  1.62it/s]Extractor Estimating: 178it [01:49,  1.60it/s]Extractor Estimating: 179it [01:50,  1.64it/s]Extractor Estimating: 180it [01:50,  1.64it/s]Extractor Estimating: 181it [01:51,  1.65it/s]Extractor Estimating: 182it [01:52,  1.57it/s]Extractor Estimating: 183it [01:52,  1.61it/s]Extractor Estimating: 184it [01:53,  1.64it/s]Extractor Estimating: 185it [01:53,  1.66it/s]Extractor Estimating: 186it [01:54,  1.63it/s]Extractor Estimating: 187it [01:55,  1.66it/s]Extractor Estimating: 188it [01:55,  1.67it/s]Extractor Estimating: 189it [01:56,  1.69it/s]Extractor Estimating: 190it [01:57,  1.65it/s]Extractor Estimating: 191it [01:57,  1.67it/s]Extractor Estimating: 192it [01:58,  1.65it/s]Extractor Estimating: 193it [01:58,  1.66it/s]Extractor Estimating: 194it [01:59,  1.64it/s]Extractor Estimating: 195it [02:00,  1.64it/s]Extractor Estimating: 196it [02:00,  1.63it/s]Extractor Estimating: 197it [02:01,  1.63it/s]Extractor Estimating: 198it [02:01,  1.67it/s]Extractor Estimating: 199it [02:02,  1.64it/s]Extractor Estimating: 200it [02:03,  1.68it/s]Extractor Estimating: 201it [02:03,  1.77it/s]Extractor Estimating: 202it [02:04,  1.83it/s]Extractor Estimating: 203it [02:04,  1.96it/s]Extractor Estimating: 204it [02:04,  1.95it/s]Extractor Estimating: 205it [02:05,  1.95it/s]Extractor Estimating: 206it [02:06,  1.97it/s]Extractor Estimating: 207it [02:06,  1.96it/s]Extractor Estimating: 208it [02:07,  1.98it/s]Extractor Estimating: 209it [02:07,  2.01it/s]Extractor Estimating: 210it [02:08,  1.97it/s]Extractor Estimating: 211it [02:08,  2.01it/s]Extractor Estimating: 212it [02:09,  2.00it/s]Extractor Estimating: 213it [02:09,  2.00it/s]Extractor Estimating: 214it [02:09,  2.04it/s]Extractor Estimating: 215it [02:10,  1.97it/s]Extractor Estimating: 216it [02:11,  1.98it/s]Extractor Estimating: 217it [02:11,  2.07it/s]Extractor Estimating: 218it [02:11,  2.09it/s]Extractor Estimating: 219it [02:12,  2.08it/s]Extractor Estimating: 220it [02:12,  2.02it/s]Extractor Estimating: 221it [02:13,  2.01it/s]Extractor Estimating: 222it [02:13,  1.99it/s]Extractor Estimating: 223it [02:14,  2.02it/s]Extractor Estimating: 224it [02:14,  2.02it/s]Extractor Estimating: 225it [02:15,  2.04it/s]Extractor Estimating: 226it [02:15,  2.04it/s]Extractor Estimating: 227it [02:16,  2.01it/s]Extractor Estimating: 228it [02:16,  1.95it/s]Extractor Estimating: 229it [02:17,  1.87it/s]Extractor Estimating: 230it [02:18,  1.92it/s]Extractor Estimating: 231it [02:18,  1.83it/s]Extractor Estimating: 232it [02:19,  1.87it/s]Extractor Estimating: 233it [02:19,  1.78it/s]Extractor Estimating: 234it [02:20,  1.83it/s]Extractor Estimating: 235it [02:20,  1.79it/s]Extractor Estimating: 236it [02:21,  1.78it/s]Extractor Estimating: 237it [02:21,  1.81it/s]Extractor Estimating: 238it [02:22,  1.84it/s]Extractor Estimating: 239it [02:23,  1.84it/s]Extractor Estimating: 240it [02:23,  1.81it/s]Extractor Estimating: 241it [02:24,  1.90it/s]Extractor Estimating: 242it [02:24,  1.89it/s]Extractor Estimating: 243it [02:25,  1.82it/s]Extractor Estimating: 244it [02:25,  1.85it/s]Extractor Estimating: 245it [02:26,  1.82it/s]Extractor Estimating: 246it [02:26,  1.76it/s]Extractor Estimating: 247it [02:27,  1.74it/s]Extractor Estimating: 248it [02:28,  1.79it/s]Extractor Estimating: 249it [02:28,  1.76it/s]Extractor Estimating: 250it [02:29,  1.65it/s]Extractor Estimating: 251it [02:29,  1.66it/s]Extractor Estimating: 252it [02:30,  1.65it/s]Extractor Estimating: 253it [02:31,  1.68it/s]Extractor Estimating: 254it [02:31,  1.68it/s]Extractor Estimating: 255it [02:32,  1.54it/s]Extractor Estimating: 256it [02:33,  1.59it/s]Extractor Estimating: 257it [02:33,  1.57it/s]Extractor Estimating: 258it [02:34,  1.65it/s]Extractor Estimating: 259it [02:34,  1.67it/s]Extractor Estimating: 260it [02:35,  1.63it/s]Extractor Estimating: 261it [02:36,  1.68it/s]Extractor Estimating: 262it [02:36,  1.71it/s]Extractor Estimating: 263it [02:37,  1.66it/s]Extractor Estimating: 264it [02:37,  1.71it/s]Extractor Estimating: 265it [02:38,  1.65it/s]Extractor Estimating: 266it [02:39,  1.62it/s]Extractor Estimating: 267it [02:39,  1.60it/s]Extractor Estimating: 268it [02:40,  1.61it/s]Extractor Estimating: 269it [02:40,  1.62it/s]Extractor Estimating: 270it [02:41,  1.62it/s]Extractor Estimating: 271it [02:42,  1.59it/s]Extractor Estimating: 272it [02:42,  1.66it/s]Extractor Estimating: 273it [02:43,  1.65it/s]Extractor Estimating: 274it [02:43,  1.67it/s]Extractor Estimating: 275it [02:44,  1.65it/s]Extractor Estimating: 276it [02:45,  1.62it/s]Extractor Estimating: 277it [02:45,  1.63it/s]Extractor Estimating: 278it [02:46,  1.69it/s]Extractor Estimating: 279it [02:46,  1.69it/s]Extractor Estimating: 280it [02:47,  1.56it/s]Extractor Estimating: 281it [02:48,  1.58it/s]Extractor Estimating: 282it [02:48,  1.62it/s]Extractor Estimating: 283it [02:49,  1.69it/s]Extractor Estimating: 284it [02:49,  1.74it/s]Extractor Estimating: 285it [02:50,  1.77it/s]Extractor Estimating: 286it [02:51,  1.75it/s]Extractor Estimating: 287it [02:51,  1.72it/s]Extractor Estimating: 288it [02:52,  1.77it/s]Extractor Estimating: 289it [02:52,  1.75it/s]Extractor Estimating: 290it [02:53,  1.72it/s]Extractor Estimating: 291it [02:53,  1.72it/s]Extractor Estimating: 292it [02:54,  1.65it/s]Extractor Estimating: 293it [02:55,  1.65it/s]Extractor Estimating: 294it [02:55,  1.69it/s]Extractor Estimating: 295it [02:56,  1.74it/s]Extractor Estimating: 296it [02:56,  1.72it/s]Extractor Estimating: 297it [02:57,  1.72it/s]Extractor Estimating: 298it [02:58,  1.74it/s]Extractor Estimating: 299it [02:58,  1.70it/s]Extractor Estimating: 300it [02:59,  1.72it/s]Extractor Estimating: 301it [02:59,  1.73it/s]Extractor Estimating: 302it [03:00,  1.71it/s]Extractor Estimating: 303it [03:01,  1.71it/s]Extractor Estimating: 304it [03:01,  1.75it/s]Extractor Estimating: 305it [03:02,  1.67it/s]Extractor Estimating: 306it [03:02,  1.74it/s]Extractor Estimating: 307it [03:03,  1.75it/s]Extractor Estimating: 308it [03:03,  1.77it/s]Extractor Estimating: 309it [03:04,  1.78it/s]Extractor Estimating: 310it [03:04,  1.80it/s]Extractor Estimating: 311it [03:05,  1.77it/s]Extractor Estimating: 312it [03:06,  1.80it/s]Extractor Estimating: 313it [03:06,  1.85it/s]Extractor Estimating: 314it [03:07,  1.83it/s]Extractor Estimating: 315it [03:07,  1.79it/s]Extractor Estimating: 316it [03:08,  1.81it/s]Extractor Estimating: 317it [03:08,  1.86it/s]Extractor Estimating: 318it [03:09,  1.84it/s]Extractor Estimating: 319it [03:09,  1.81it/s]Extractor Estimating: 320it [03:10,  1.84it/s]Extractor Estimating: 321it [03:10,  1.84it/s]Extractor Estimating: 322it [03:11,  1.82it/s]Extractor Estimating: 323it [03:12,  1.79it/s]Extractor Estimating: 324it [03:12,  1.81it/s]Extractor Estimating: 325it [03:13,  1.81it/s]Extractor Estimating: 326it [03:13,  1.92it/s]Extractor Estimating: 327it [03:14,  2.00it/s]Extractor Estimating: 328it [03:14,  2.03it/s]Extractor Estimating: 329it [03:15,  2.11it/s]Extractor Estimating: 330it [03:15,  2.11it/s]Extractor Estimating: 331it [03:15,  2.09it/s]Extractor Estimating: 332it [03:16,  1.97it/s]Extractor Estimating: 333it [03:17,  2.00it/s]Extractor Estimating: 334it [03:17,  2.02it/s]Extractor Estimating: 335it [03:17,  2.08it/s]Extractor Estimating: 336it [03:18,  2.12it/s]Extractor Estimating: 337it [03:18,  2.21it/s]Extractor Estimating: 338it [03:19,  2.23it/s]Extractor Estimating: 339it [03:19,  2.21it/s]Extractor Estimating: 340it [03:20,  2.21it/s]Extractor Estimating: 341it [03:20,  2.24it/s]Extractor Estimating: 342it [03:21,  2.18it/s]Extractor Estimating: 343it [03:21,  2.14it/s]Extractor Estimating: 344it [03:22,  2.14it/s]Extractor Estimating: 345it [03:22,  2.13it/s]Extractor Estimating: 346it [03:22,  2.14it/s]Extractor Estimating: 347it [03:23,  2.22it/s]Extractor Estimating: 348it [03:23,  2.22it/s]Extractor Estimating: 349it [03:24,  2.25it/s]Extractor Estimating: 350it [03:24,  2.06it/s]Extractor Estimating: 351it [03:25,  1.86it/s]Extractor Estimating: 352it [03:26,  1.81it/s]Extractor Estimating: 353it [03:26,  1.77it/s]Extractor Estimating: 354it [03:27,  1.78it/s]Extractor Estimating: 355it [03:27,  1.76it/s]Extractor Estimating: 356it [03:28,  1.66it/s]Extractor Estimating: 357it [03:29,  1.72it/s]Extractor Estimating: 358it [03:29,  1.78it/s]Extractor Estimating: 359it [03:30,  1.74it/s]Extractor Estimating: 360it [03:30,  1.77it/s]Extractor Estimating: 361it [03:31,  1.74it/s]Extractor Estimating: 362it [03:31,  1.76it/s]Extractor Estimating: 363it [03:32,  1.59it/s]Extractor Estimating: 364it [03:33,  1.53it/s]Extractor Estimating: 365it [03:33,  1.61it/s]Extractor Estimating: 366it [03:34,  1.66it/s]Extractor Estimating: 367it [03:35,  1.62it/s]Extractor Estimating: 368it [03:35,  1.69it/s]Extractor Estimating: 369it [03:36,  1.62it/s]Extractor Estimating: 370it [03:36,  1.62it/s]Extractor Estimating: 371it [03:37,  1.65it/s]Extractor Estimating: 372it [03:38,  1.65it/s]Extractor Estimating: 373it [03:38,  1.69it/s]Extractor Estimating: 374it [03:39,  1.68it/s]Extractor Estimating: 375it [03:39,  1.74it/s]Extractor Estimating: 375it [03:39,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:29,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:29,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:29,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:29,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:29,726 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:07:30,094 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:07:30,096 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:07:30,786 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:07:31,847 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:07:31,847 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:34,554 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:34,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:34,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:34,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:07:34,576 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:07:34,954 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:07:34,955 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:07:35,251 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:07:35,418 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:07:35,418 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 03:20:07,387 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 03:20:07,592 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7493 mean pseudo reward: 0.9597558163273434
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 16111
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16211, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16211, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.002, loss:399.2397
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.992, loss:362.4785
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.006, loss:352.6218
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.006, loss:344.3002
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.006, loss:346.6832
>> valid entity prec:0.4457, rec:0.3942, f1:0.4183
>> valid relation prec:0.0602, rec:0.0175, f1:0.0271
>> valid relation with NER prec:0.0602, rec:0.0175, f1:0.0271
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.432, loss:351.0844
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.009, loss:321.5636
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.016, loss:341.0289
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.997, loss:357.9532
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.992, loss:331.8510
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4416, rec:0.3924, f1:0.4155
>> valid relation prec:0.0522, rec:0.0143, f1:0.0224
>> valid relation with NER prec:0.0522, rec:0.0143, f1:0.0224
g_step 1100, step 161, avg_time 2.413, loss:338.6472
g_step 1200, step 261, avg_time 1.013, loss:350.9916
g_step 1300, step 48, avg_time 0.988, loss:319.7632
g_step 1400, step 148, avg_time 1.003, loss:315.7213
g_step 1500, step 248, avg_time 0.994, loss:334.7112
>> valid entity prec:0.4364, rec:0.3685, f1:0.3996
>> valid relation prec:0.0490, rec:0.0148, f1:0.0227
>> valid relation with NER prec:0.0490, rec:0.0148, f1:0.0227
g_step 1600, step 35, avg_time 2.419, loss:321.1555
g_step 1700, step 135, avg_time 1.007, loss:322.2796
g_step 1800, step 235, avg_time 1.006, loss:313.7412
g_step 1900, step 22, avg_time 0.986, loss:305.9864
g_step 2000, step 122, avg_time 0.996, loss:310.1201
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4154, rec:0.3581, f1:0.3847
>> valid relation prec:0.0425, rec:0.0085, f1:0.0142
>> valid relation with NER prec:0.0425, rec:0.0085, f1:0.0142
g_step 2100, step 222, avg_time 2.416, loss:301.1210
g_step 2200, step 9, avg_time 1.013, loss:303.4065
g_step 2300, step 109, avg_time 1.008, loss:287.4252
g_step 2400, step 209, avg_time 0.998, loss:294.5686
g_step 2500, step 309, avg_time 0.999, loss:301.6454
>> valid entity prec:0.4278, rec:0.2936, f1:0.3482
>> valid relation prec:0.0718, rec:0.0138, f1:0.0232
>> valid relation with NER prec:0.0718, rec:0.0138, f1:0.0232
g_step 2600, step 96, avg_time 2.404, loss:277.4011
g_step 2700, step 196, avg_time 0.992, loss:280.1531
g_step 2800, step 296, avg_time 0.993, loss:274.9533
g_step 2900, step 83, avg_time 0.995, loss:261.9204
g_step 3000, step 183, avg_time 0.986, loss:277.3432
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4337, rec:0.3482, f1:0.3862
>> valid relation prec:0.0555, rec:0.0171, f1:0.0261
>> valid relation with NER prec:0.0555, rec:0.0171, f1:0.0261
g_step 3100, step 283, avg_time 2.422, loss:271.7354
g_step 3200, step 70, avg_time 0.997, loss:263.8918
g_step 3300, step 170, avg_time 0.999, loss:259.0875
g_step 3400, step 270, avg_time 1.002, loss:273.1786
g_step 3500, step 57, avg_time 0.983, loss:262.6512
>> valid entity prec:0.4647, rec:0.3839, f1:0.4204
>> valid relation prec:0.0663, rec:0.0205, f1:0.0313
>> valid relation with NER prec:0.0663, rec:0.0205, f1:0.0313
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 157, avg_time 2.419, loss:248.7188
g_step 3700, step 257, avg_time 1.000, loss:252.9758
g_step 3800, step 44, avg_time 1.003, loss:256.6901
g_step 3900, step 144, avg_time 0.990, loss:244.4599
g_step 4000, step 244, avg_time 1.003, loss:245.6962
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4534, rec:0.2840, f1:0.3493
>> valid relation prec:0.0667, rec:0.0154, f1:0.0251
>> valid relation with NER prec:0.0667, rec:0.0154, f1:0.0251
g_step 4100, step 31, avg_time 2.398, loss:239.0904
g_step 4200, step 131, avg_time 1.003, loss:232.4624
g_step 4300, step 231, avg_time 0.980, loss:239.5667
g_step 4400, step 18, avg_time 0.998, loss:239.1188
g_step 4500, step 118, avg_time 0.996, loss:221.5534
>> valid entity prec:0.4336, rec:0.3889, f1:0.4100
>> valid relation prec:0.0718, rec:0.0240, f1:0.0359
>> valid relation with NER prec:0.0718, rec:0.0240, f1:0.0359
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 218, avg_time 2.415, loss:230.8755
g_step 4700, step 5, avg_time 1.005, loss:241.9340
g_step 4800, step 105, avg_time 1.002, loss:220.2056
g_step 4900, step 205, avg_time 0.992, loss:214.1203
g_step 5000, step 305, avg_time 0.991, loss:229.6434
learning rate was adjusted to 0.0008
>> valid entity prec:0.4399, rec:0.3988, f1:0.4183
>> valid relation prec:0.0664, rec:0.0233, f1:0.0345
>> valid relation with NER prec:0.0664, rec:0.0233, f1:0.0345
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5100, step 92, avg_time 2.405, loss:211.2747
g_step 5200, step 192, avg_time 1.008, loss:219.2432
g_step 5300, step 292, avg_time 0.995, loss:220.9701
g_step 5400, step 79, avg_time 0.974, loss:207.9160
g_step 5500, step 179, avg_time 0.999, loss:214.1816
>> valid entity prec:0.4300, rec:0.4333, f1:0.4316
>> valid relation prec:0.0581, rec:0.0208, f1:0.0306
>> valid relation with NER prec:0.0581, rec:0.0208, f1:0.0306
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5600, step 279, avg_time 2.436, loss:209.9854
g_step 5700, step 66, avg_time 0.975, loss:203.5703
g_step 5800, step 166, avg_time 0.992, loss:206.8957
g_step 5900, step 266, avg_time 1.011, loss:208.3413
g_step 6000, step 53, avg_time 1.016, loss:203.2917
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4243, rec:0.3396, f1:0.3772
>> valid relation prec:0.0642, rec:0.0233, f1:0.0342
>> valid relation with NER prec:0.0642, rec:0.0233, f1:0.0342
g_step 6100, step 153, avg_time 2.406, loss:188.9512
g_step 6200, step 253, avg_time 0.992, loss:195.9476
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 03:20:07 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 03:20:07 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_03-20-07_ctolab07.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 03:20:08 - WARNING - datasets.builder -   Using custom data configuration default-5961267f838c032e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5961267f838c032e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  1.15 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 03:20:12,750 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:20:12,751 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:20:12,752 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:20:12,753 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:20:12,891 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:12,957 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:12,957 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:12,957 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:12,957 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:12,957 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:20:12,957 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 03:20:13,523 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:20:16,709 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 03:20:16,761 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5961267f838c032e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:04,  1.45ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.49ba/s] 38%|███▊      | 3/8 [00:01<00:01,  3.25ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.77ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.14ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.43ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.63ba/s]100%|██████████| 8/8 [00:01<00:00,  4.05ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.96ba/s] 40%|████      | 2/5 [00:00<00:00,  3.43ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.90ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.16ba/s]100%|██████████| 5/5 [00:01<00:00,  4.52ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  6.28ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.96ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.79ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.22ba/s]100%|██████████| 8/8 [00:00<00:00, 10.41ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.57ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.27ba/s]100%|██████████| 5/5 [00:00<00:00, 11.63ba/s]100%|██████████| 5/5 [00:00<00:00, 10.69ba/s]
[INFO|trainer.py:414] 2023-08-29 03:20:22,558 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 03:20:22,613 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 03:20:22,614 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 03:20:22,614 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 03:20:22,614 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 03:20:22,614 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 03:20:22,614 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 03:20:22,614 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<05:03,  1.93it/s]  0%|          | 2/585 [00:00<03:49,  2.54it/s]  1%|          | 3/585 [00:01<03:21,  2.89it/s]  1%|          | 4/585 [00:01<03:08,  3.08it/s]  1%|          | 5/585 [00:01<03:02,  3.19it/s]  1%|          | 6/585 [00:01<02:57,  3.26it/s]  1%|          | 7/585 [00:02<02:54,  3.31it/s]  1%|▏         | 8/585 [00:02<02:52,  3.34it/s]  2%|▏         | 9/585 [00:02<02:51,  3.36it/s]  2%|▏         | 10/585 [00:03<02:50,  3.38it/s]  2%|▏         | 11/585 [00:03<02:49,  3.39it/s]  2%|▏         | 12/585 [00:03<02:48,  3.39it/s]  2%|▏         | 13/585 [00:04<02:53,  3.30it/s]  2%|▏         | 14/585 [00:04<02:51,  3.33it/s]  3%|▎         | 15/585 [00:04<02:49,  3.36it/s]  3%|▎         | 16/585 [00:04<02:48,  3.37it/s]  3%|▎         | 17/585 [00:05<02:47,  3.38it/s]  3%|▎         | 18/585 [00:05<02:47,  3.39it/s]  3%|▎         | 19/585 [00:05<02:46,  3.40it/s]  3%|▎         | 20/585 [00:06<02:46,  3.40it/s]  4%|▎         | 21/585 [00:06<02:45,  3.40it/s]  4%|▍         | 22/585 [00:06<02:45,  3.41it/s]  4%|▍         | 23/585 [00:07<02:44,  3.41it/s]  4%|▍         | 24/585 [00:07<02:49,  3.31it/s]  4%|▍         | 25/585 [00:07<02:47,  3.34it/s]  4%|▍         | 26/585 [00:07<02:46,  3.36it/s]  5%|▍         | 27/585 [00:08<02:45,  3.37it/s]  5%|▍         | 28/585 [00:08<02:44,  3.38it/s]  5%|▍         | 29/585 [00:08<02:44,  3.39it/s]  5%|▌         | 30/585 [00:09<02:43,  3.40it/s]  5%|▌         | 31/585 [00:09<02:43,  3.40it/s]  5%|▌         | 32/585 [00:09<02:42,  3.40it/s]  6%|▌         | 33/585 [00:09<02:41,  3.41it/s]  6%|▌         | 34/585 [00:10<02:41,  3.41it/s]  6%|▌         | 35/585 [00:10<02:46,  3.31it/s]  6%|▌         | 36/585 [00:10<02:44,  3.34it/s]  6%|▋         | 37/585 [00:11<02:42,  3.36it/s]  6%|▋         | 38/585 [00:11<02:42,  3.38it/s]  7%|▋         | 39/585 [00:11<02:41,  3.38it/s]  7%|▋         | 40/585 [00:12<02:40,  3.40it/s]  7%|▋         | 41/585 [00:12<02:40,  3.40it/s]  7%|▋         | 42/585 [00:12<02:39,  3.40it/s]  7%|▋         | 43/585 [00:12<02:39,  3.41it/s]  8%|▊         | 44/585 [00:13<02:38,  3.40it/s]  8%|▊         | 45/585 [00:13<02:38,  3.41it/s]  8%|▊         | 46/585 [00:13<02:46,  3.24it/s]  8%|▊         | 47/585 [00:14<02:43,  3.29it/s]  8%|▊         | 48/585 [00:14<02:41,  3.32it/s]  8%|▊         | 49/585 [00:14<02:40,  3.35it/s]  9%|▊         | 50/585 [00:15<02:38,  3.37it/s]  9%|▊         | 51/585 [00:15<02:37,  3.38it/s]  9%|▉         | 52/585 [00:15<02:37,  3.39it/s]  9%|▉         | 53/585 [00:15<02:36,  3.39it/s]  9%|▉         | 54/585 [00:16<02:36,  3.40it/s]  9%|▉         | 55/585 [00:16<02:35,  3.40it/s] 10%|▉         | 56/585 [00:16<02:35,  3.40it/s] 10%|▉         | 57/585 [00:17<02:35,  3.41it/s] 10%|▉         | 58/585 [00:17<02:34,  3.40it/s] 10%|█         | 59/585 [00:17<02:34,  3.41it/s] 10%|█         | 60/585 [00:17<02:34,  3.41it/s] 10%|█         | 61/585 [00:18<02:33,  3.41it/s] 11%|█         | 62/585 [00:18<02:37,  3.31it/s] 11%|█         | 63/585 [00:18<02:36,  3.34it/s] 11%|█         | 64/585 [00:19<02:35,  3.36it/s] 11%|█         | 65/585 [00:19<02:34,  3.37it/s] 11%|█▏        | 66/585 [00:19<02:33,  3.38it/s] 11%|█▏        | 67/585 [00:20<02:32,  3.39it/s] 12%|█▏        | 68/585 [00:20<02:32,  3.40it/s] 12%|█▏        | 69/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 70/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 71/585 [00:21<02:31,  3.40it/s] 12%|█▏        | 72/585 [00:21<02:30,  3.40it/s] 12%|█▏        | 73/585 [00:21<02:34,  3.30it/s] 13%|█▎        | 74/585 [00:22<02:33,  3.33it/s] 13%|█▎        | 75/585 [00:22<02:31,  3.36it/s] 13%|█▎        | 76/585 [00:22<02:31,  3.37it/s] 13%|█▎        | 77/585 [00:23<02:30,  3.37it/s] 13%|█▎        | 78/585 [00:23<02:29,  3.39it/s] 14%|█▎        | 79/585 [00:23<02:29,  3.39it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.39it/s] 14%|█▍        | 81/585 [00:24<02:28,  3.40it/s] 14%|█▍        | 82/585 [00:24<02:27,  3.40it/s] 14%|█▍        | 83/585 [00:24<02:27,  3.40it/s] 14%|█▍        | 84/585 [00:25<02:31,  3.31it/s] 15%|█▍        | 85/585 [00:25<02:29,  3.34it/s] 15%|█▍        | 86/585 [00:25<02:28,  3.36it/s] 15%|█▍        | 87/585 [00:25<02:27,  3.37it/s] 15%|█▌        | 88/585 [00:26<02:26,  3.38it/s] 15%|█▌        | 89/585 [00:26<02:26,  3.39it/s] 15%|█▌        | 90/585 [00:26<02:25,  3.40it/s] 16%|█▌        | 91/585 [00:27<02:25,  3.40it/s] 16%|█▌        | 92/585 [00:27<02:25,  3.40it/s] 16%|█▌        | 93/585 [00:27<02:24,  3.40it/s] 16%|█▌        | 94/585 [00:28<02:24,  3.40it/s] 16%|█▌        | 95/585 [00:28<02:32,  3.22it/s] 16%|█▋        | 96/585 [00:28<02:28,  3.29it/s] 17%|█▋        | 97/585 [00:28<02:26,  3.34it/s] 17%|█▋        | 98/585 [00:29<02:24,  3.37it/s] 17%|█▋        | 99/585 [00:29<02:23,  3.40it/s] 17%|█▋        | 100/585 [00:29<02:22,  3.41it/s] 17%|█▋        | 101/585 [00:30<02:21,  3.43it/s] 17%|█▋        | 102/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.44it/s] 18%|█▊        | 104/585 [00:30<02:19,  3.45it/s] 18%|█▊        | 105/585 [00:31<02:19,  3.45it/s] 18%|█▊        | 106/585 [00:31<02:24,  3.32it/s] 18%|█▊        | 107/585 [00:31<02:22,  3.36it/s] 18%|█▊        | 108/585 [00:32<02:20,  3.39it/s] 19%|█▊        | 109/585 [00:32<02:19,  3.41it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.42it/s] 19%|█▉        | 111/585 [00:33<02:18,  3.43it/s] 19%|█▉        | 112/585 [00:33<02:17,  3.44it/s] 19%|█▉        | 113/585 [00:33<02:17,  3.44it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.45it/s] 20%|█▉        | 115/585 [00:34<02:16,  3.45it/s] 20%|█▉        | 116/585 [00:34<02:15,  3.45it/s] 20%|██        | 117/585 [00:34<02:18,  3.38it/s][INFO|trainer.py:2140] 2023-08-29 03:20:57,498 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:20:57,498 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:20:57,498 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.34it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.57it/s][A
  3%|▎         | 18/543 [00:00<00:10, 47.76it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.94it/s][A
  5%|▌         | 28/543 [00:00<00:11, 46.04it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.70it/s][A
  7%|▋         | 38/543 [00:00<00:11, 45.41it/s][A
  8%|▊         | 43/543 [00:00<00:11, 45.03it/s][A
  9%|▉         | 48/543 [00:01<00:11, 44.97it/s][A
 10%|▉         | 53/543 [00:01<00:10, 45.06it/s][A
 11%|█         | 58/543 [00:01<00:10, 45.23it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.39it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.43it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 45.29it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 45.18it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 45.03it/s][A
 16%|█▌        | 88/543 [00:01<00:10, 44.85it/s][A
 17%|█▋        | 93/543 [00:02<00:10, 44.86it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 44.71it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 45.03it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 45.18it/s][A
 21%|██        | 113/543 [00:02<00:09, 45.34it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 45.35it/s][A
 23%|██▎       | 123/543 [00:02<00:09, 45.27it/s][A
 24%|██▎       | 128/543 [00:02<00:09, 43.89it/s][A
 24%|██▍       | 133/543 [00:02<00:09, 44.17it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 44.31it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 44.49it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 44.77it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 44.94it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 45.14it/s][A
 30%|███       | 163/543 [00:03<00:08, 45.17it/s][A
 31%|███       | 168/543 [00:03<00:08, 44.97it/s][A
 32%|███▏      | 173/543 [00:03<00:08, 44.93it/s][A
 33%|███▎      | 178/543 [00:03<00:08, 44.98it/s][A
 34%|███▎      | 183/543 [00:04<00:08, 44.80it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 44.87it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 45.02it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 45.15it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 45.25it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 45.25it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 45.21it/s][A
 40%|████      | 218/543 [00:04<00:07, 45.18it/s][A
 41%|████      | 223/543 [00:04<00:07, 45.10it/s][A
 42%|████▏     | 228/543 [00:05<00:06, 45.06it/s][A
 43%|████▎     | 233/543 [00:05<00:06, 45.00it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 45.01it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 45.06it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 45.17it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 45.15it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 45.09it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 43.72it/s][A
 49%|████▉     | 268/543 [00:05<00:06, 44.17it/s][A
 50%|█████     | 273/543 [00:06<00:06, 44.41it/s][A
 51%|█████     | 278/543 [00:06<00:05, 44.62it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 44.69it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 44.85it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 44.97it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 45.05it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 44.80it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 44.91it/s][A
 58%|█████▊    | 313/543 [00:06<00:05, 45.04it/s][A
 59%|█████▊    | 318/543 [00:07<00:04, 45.11it/s][A
 59%|█████▉    | 323/543 [00:07<00:04, 45.03it/s][A
 60%|██████    | 328/543 [00:07<00:04, 45.00it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 45.10it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 45.14it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 45.07it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 44.77it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 44.93it/s][A
 66%|██████▌   | 358/543 [00:07<00:04, 45.02it/s][A
 67%|██████▋   | 363/543 [00:08<00:03, 45.13it/s][A
 68%|██████▊   | 368/543 [00:08<00:03, 45.08it/s][A
 69%|██████▊   | 373/543 [00:08<00:03, 44.85it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 45.05it/s][A
 71%|███████   | 383/543 [00:08<00:03, 45.03it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 45.02it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 44.82it/s][A
 73%|███████▎  | 398/543 [00:08<00:03, 43.42it/s][A
 74%|███████▍  | 403/543 [00:08<00:03, 44.08it/s][A
 75%|███████▌  | 408/543 [00:09<00:03, 44.52it/s][A
 76%|███████▌  | 413/543 [00:09<00:02, 44.72it/s][A
 77%|███████▋  | 418/543 [00:09<00:02, 44.88it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 45.02it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 45.05it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 45.00it/s][A
 81%|████████  | 438/543 [00:09<00:02, 44.80it/s][A
 82%|████████▏ | 443/543 [00:09<00:02, 44.91it/s][A
 83%|████████▎ | 448/543 [00:10<00:03, 28.89it/s][A
 83%|████████▎ | 453/543 [00:10<00:02, 32.64it/s][A
 84%|████████▍ | 458/543 [00:10<00:02, 35.72it/s][A
 85%|████████▌ | 463/543 [00:10<00:02, 38.12it/s][A
 86%|████████▌ | 468/543 [00:10<00:01, 40.08it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 41.62it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 42.78it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 43.61it/s][A
 90%|████████▉ | 488/543 [00:11<00:01, 43.85it/s][A
 91%|█████████ | 493/543 [00:11<00:01, 43.84it/s][A
 92%|█████████▏| 498/543 [00:11<00:01, 44.00it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 44.28it/s][A
 94%|█████████▎| 508/543 [00:11<00:00, 44.54it/s][A
 94%|█████████▍| 513/543 [00:11<00:00, 44.80it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 44.97it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 45.16it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 45.28it/s][A
 98%|█████████▊| 533/543 [00:12<00:00, 45.20it/s][A
 99%|█████████▉| 538/543 [00:12<00:00, 44.84it/s][A
100%|██████████| 543/543 [00:12<00:00, 44.76it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.76it/s][A 20%|██        | 117/585 [00:47<02:18,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:21:10,190 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 03:21:10,631 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:21:14,118 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:21:14,384 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:21:14,495 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:02<1:06:08,  8.50s/it] 20%|██        | 119/585 [01:02<47:00,  6.05s/it]   21%|██        | 120/585 [01:03<33:30,  4.32s/it] 21%|██        | 121/585 [01:03<24:05,  3.11s/it] 21%|██        | 122/585 [01:03<17:30,  2.27s/it] 21%|██        | 123/585 [01:03<12:54,  1.68s/it] 21%|██        | 124/585 [01:04<09:41,  1.26s/it] 21%|██▏       | 125/585 [01:04<07:26,  1.03it/s] 22%|██▏       | 126/585 [01:04<05:52,  1.30it/s] 22%|██▏       | 127/585 [01:05<04:46,  1.60it/s] 22%|██▏       | 128/585 [01:05<04:00,  1.90it/s] 22%|██▏       | 129/585 [01:05<03:28,  2.19it/s] 22%|██▏       | 130/585 [01:06<03:13,  2.35it/s] 22%|██▏       | 131/585 [01:06<02:55,  2.59it/s] 23%|██▎       | 132/585 [01:06<02:42,  2.79it/s] 23%|██▎       | 133/585 [01:06<02:33,  2.95it/s] 23%|██▎       | 134/585 [01:07<02:26,  3.08it/s] 23%|██▎       | 135/585 [01:07<02:21,  3.17it/s] 23%|██▎       | 136/585 [01:07<02:18,  3.24it/s] 23%|██▎       | 137/585 [01:08<02:16,  3.29it/s] 24%|██▎       | 138/585 [01:08<02:14,  3.32it/s] 24%|██▍       | 139/585 [01:08<02:13,  3.35it/s] 24%|██▍       | 140/585 [01:09<02:12,  3.37it/s] 24%|██▍       | 141/585 [01:09<02:17,  3.23it/s] 24%|██▍       | 142/585 [01:09<02:14,  3.28it/s] 24%|██▍       | 143/585 [01:09<02:13,  3.32it/s] 25%|██▍       | 144/585 [01:10<02:11,  3.35it/s] 25%|██▍       | 145/585 [01:10<02:10,  3.36it/s] 25%|██▍       | 146/585 [01:10<02:09,  3.38it/s] 25%|██▌       | 147/585 [01:11<02:09,  3.39it/s] 25%|██▌       | 148/585 [01:11<02:08,  3.39it/s] 25%|██▌       | 149/585 [01:11<02:08,  3.39it/s] 26%|██▌       | 150/585 [01:11<02:07,  3.40it/s] 26%|██▌       | 151/585 [01:12<02:07,  3.40it/s] 26%|██▌       | 152/585 [01:12<02:11,  3.30it/s] 26%|██▌       | 153/585 [01:12<02:09,  3.33it/s] 26%|██▋       | 154/585 [01:13<02:08,  3.35it/s] 26%|██▋       | 155/585 [01:13<02:07,  3.37it/s] 27%|██▋       | 156/585 [01:13<02:06,  3.38it/s] 27%|██▋       | 157/585 [01:14<02:06,  3.39it/s] 27%|██▋       | 158/585 [01:14<02:05,  3.40it/s] 27%|██▋       | 159/585 [01:14<02:05,  3.40it/s] 27%|██▋       | 160/585 [01:14<02:05,  3.40it/s] 28%|██▊       | 161/585 [01:15<02:04,  3.40it/s] 28%|██▊       | 162/585 [01:15<02:04,  3.40it/s] 28%|██▊       | 163/585 [01:15<02:08,  3.28it/s] 28%|██▊       | 164/585 [01:16<02:06,  3.32it/s] 28%|██▊       | 165/585 [01:16<02:05,  3.34it/s] 28%|██▊       | 166/585 [01:16<02:04,  3.36it/s] 29%|██▊       | 167/585 [01:17<02:03,  3.37it/s] 29%|██▊       | 168/585 [01:17<02:03,  3.38it/s] 29%|██▉       | 169/585 [01:17<02:02,  3.39it/s] 29%|██▉       | 170/585 [01:17<02:02,  3.39it/s] 29%|██▉       | 171/585 [01:18<02:01,  3.40it/s] 29%|██▉       | 172/585 [01:18<02:01,  3.40it/s] 30%|██▉       | 173/585 [01:18<02:01,  3.40it/s] 30%|██▉       | 174/585 [01:19<02:00,  3.40it/s] 30%|██▉       | 175/585 [01:19<02:00,  3.40it/s] 30%|███       | 176/585 [01:19<02:00,  3.40it/s] 30%|███       | 177/585 [01:20<02:03,  3.29it/s] 30%|███       | 178/585 [01:20<02:02,  3.33it/s] 31%|███       | 179/585 [01:20<02:01,  3.35it/s] 31%|███       | 180/585 [01:20<02:00,  3.37it/s] 31%|███       | 181/585 [01:21<01:59,  3.38it/s] 31%|███       | 182/585 [01:21<01:59,  3.38it/s] 31%|███▏      | 183/585 [01:21<01:58,  3.39it/s] 31%|███▏      | 184/585 [01:22<01:58,  3.39it/s] 32%|███▏      | 185/585 [01:22<01:57,  3.40it/s] 32%|███▏      | 186/585 [01:22<01:57,  3.40it/s] 32%|███▏      | 187/585 [01:22<01:57,  3.40it/s] 32%|███▏      | 188/585 [01:23<01:59,  3.31it/s] 32%|███▏      | 189/585 [01:23<01:58,  3.34it/s] 32%|███▏      | 190/585 [01:23<01:57,  3.37it/s] 33%|███▎      | 191/585 [01:24<01:56,  3.39it/s] 33%|███▎      | 192/585 [01:24<01:55,  3.41it/s] 33%|███▎      | 193/585 [01:24<01:54,  3.42it/s] 33%|███▎      | 194/585 [01:25<01:53,  3.43it/s] 33%|███▎      | 195/585 [01:25<01:53,  3.44it/s] 34%|███▎      | 196/585 [01:25<01:53,  3.44it/s] 34%|███▎      | 197/585 [01:25<01:52,  3.44it/s] 34%|███▍      | 198/585 [01:26<01:52,  3.45it/s] 34%|███▍      | 199/585 [01:26<01:54,  3.36it/s] 34%|███▍      | 200/585 [01:26<01:53,  3.39it/s] 34%|███▍      | 201/585 [01:27<01:52,  3.41it/s] 35%|███▍      | 202/585 [01:27<01:51,  3.42it/s] 35%|███▍      | 203/585 [01:27<01:51,  3.43it/s] 35%|███▍      | 204/585 [01:27<01:50,  3.44it/s] 35%|███▌      | 205/585 [01:28<01:50,  3.44it/s] 35%|███▌      | 206/585 [01:28<01:50,  3.44it/s] 35%|███▌      | 207/585 [01:28<01:49,  3.44it/s] 36%|███▌      | 208/585 [01:29<01:49,  3.45it/s] 36%|███▌      | 209/585 [01:29<01:49,  3.45it/s] 36%|███▌      | 210/585 [01:29<01:52,  3.34it/s] 36%|███▌      | 211/585 [01:30<01:50,  3.38it/s] 36%|███▌      | 212/585 [01:30<01:49,  3.40it/s] 36%|███▋      | 213/585 [01:30<01:49,  3.41it/s] 37%|███▋      | 214/585 [01:30<01:48,  3.42it/s] 37%|███▋      | 215/585 [01:31<01:47,  3.43it/s] 37%|███▋      | 216/585 [01:31<01:47,  3.44it/s] 37%|███▋      | 217/585 [01:31<01:46,  3.44it/s] 37%|███▋      | 218/585 [01:32<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:32<01:46,  3.45it/s] 38%|███▊      | 220/585 [01:32<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:32<01:47,  3.39it/s] 38%|███▊      | 222/585 [01:33<01:46,  3.41it/s] 38%|███▊      | 223/585 [01:33<01:45,  3.42it/s] 38%|███▊      | 224/585 [01:33<01:45,  3.43it/s] 38%|███▊      | 225/585 [01:34<01:44,  3.44it/s] 39%|███▊      | 226/585 [01:34<01:44,  3.44it/s] 39%|███▉      | 227/585 [01:34<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:34<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:35<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:35<01:42,  3.45it/s] 39%|███▉      | 231/585 [01:35<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:36<01:44,  3.39it/s] 40%|███▉      | 233/585 [01:36<01:43,  3.41it/s] 40%|████      | 234/585 [01:36<01:42,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 03:21:59,357 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:21:59,357 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:21:59,357 >>   Batch size = 8
{'eval_loss': 1.1606554985046387, 'eval_runtime': 12.2907, 'eval_samples_per_second': 353.275, 'eval_steps_per_second': 44.18, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.94it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.99it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.08it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.19it/s][A
  5%|▍         | 27/543 [00:00<00:11, 45.63it/s][A
  6%|▌         | 32/543 [00:00<00:11, 45.38it/s][A
  7%|▋         | 37/543 [00:00<00:11, 45.25it/s][A
  8%|▊         | 42/543 [00:00<00:11, 45.16it/s][A
  9%|▊         | 47/543 [00:01<00:10, 45.11it/s][A
 10%|▉         | 52/543 [00:01<00:10, 45.19it/s][A
 10%|█         | 57/543 [00:01<00:10, 45.30it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 45.29it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 45.15it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 45.04it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.96it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.97it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.98it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 45.05it/s][A
 18%|█▊        | 97/543 [00:02<00:09, 45.10it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 43.61it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.27it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.49it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.68it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.68it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.74it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.81it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.94it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 44.92it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.90it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 45.03it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 45.14it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 45.24it/s][A
 31%|███       | 167/543 [00:03<00:08, 45.10it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 45.05it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.91it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 45.04it/s][A
 34%|███▍      | 187/543 [00:04<00:07, 45.00it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 45.07it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 45.04it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 45.20it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 45.15it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 45.09it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 45.02it/s][A
 41%|████      | 222/543 [00:04<00:07, 44.98it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.92it/s][A
 43%|████▎     | 232/543 [00:05<00:06, 44.96it/s][A
 44%|████▎     | 237/543 [00:05<00:07, 43.69it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.26it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.59it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.77it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.87it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.84it/s][A
 49%|████▉     | 267/543 [00:05<00:06, 44.76it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.84it/s][A
 51%|█████     | 277/543 [00:06<00:05, 44.75it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.85it/s][A
 53%|█████▎    | 287/543 [00:06<00:06, 41.95it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.61it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.25it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.63it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.71it/s][A
 57%|█████▋    | 312/543 [00:06<00:05, 44.82it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.77it/s][A
 59%|█████▉    | 322/543 [00:07<00:04, 44.76it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.83it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.64it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.95it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 45.11it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 45.27it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 45.19it/s][A
 66%|██████▌   | 357/543 [00:07<00:04, 45.14it/s][A
 67%|██████▋   | 362/543 [00:08<00:06, 27.31it/s][A
 67%|██████▋   | 366/543 [00:08<00:06, 29.07it/s][A
 68%|██████▊   | 371/543 [00:08<00:05, 32.78it/s][A
 69%|██████▉   | 376/543 [00:08<00:04, 35.91it/s][A
 70%|███████   | 381/543 [00:08<00:04, 38.33it/s][A
 71%|███████   | 386/543 [00:08<00:03, 40.15it/s][A
 72%|███████▏  | 391/543 [00:08<00:03, 41.60it/s][A
 73%|███████▎  | 396/543 [00:09<00:03, 42.70it/s][A
 74%|███████▍  | 401/543 [00:09<00:03, 43.40it/s][A
 75%|███████▍  | 406/543 [00:09<00:03, 43.65it/s][A
 76%|███████▌  | 411/543 [00:09<00:03, 44.00it/s][A
 77%|███████▋  | 416/543 [00:09<00:02, 44.38it/s][A
 78%|███████▊  | 421/543 [00:09<00:02, 44.69it/s][A
 78%|███████▊  | 426/543 [00:09<00:02, 44.77it/s][A
 79%|███████▉  | 431/543 [00:09<00:02, 44.92it/s][A
 80%|████████  | 436/543 [00:09<00:02, 44.98it/s][A
 81%|████████  | 441/543 [00:10<00:02, 45.13it/s][A
 82%|████████▏ | 446/543 [00:10<00:02, 44.97it/s][A
 83%|████████▎ | 451/543 [00:10<00:02, 44.76it/s][A
 84%|████████▍ | 456/543 [00:10<00:01, 44.78it/s][A
 85%|████████▍ | 461/543 [00:10<00:01, 44.84it/s][A
 86%|████████▌ | 466/543 [00:10<00:01, 44.98it/s][A
 87%|████████▋ | 471/543 [00:10<00:01, 44.99it/s][A
 88%|████████▊ | 476/543 [00:10<00:01, 45.13it/s][A
 89%|████████▊ | 481/543 [00:10<00:01, 45.16it/s][A
 90%|████████▉ | 486/543 [00:11<00:01, 45.19it/s][A
 90%|█████████ | 491/543 [00:11<00:01, 45.08it/s][A
 91%|█████████▏| 496/543 [00:11<00:01, 44.87it/s][A
 92%|█████████▏| 501/543 [00:11<00:00, 44.84it/s][A
 93%|█████████▎| 506/543 [00:11<00:00, 44.87it/s][A
 94%|█████████▍| 511/543 [00:11<00:00, 45.01it/s][A
 95%|█████████▌| 516/543 [00:11<00:00, 45.10it/s][A
 96%|█████████▌| 521/543 [00:11<00:00, 45.14it/s][A
 97%|█████████▋| 526/543 [00:11<00:00, 45.27it/s][A
 98%|█████████▊| 531/543 [00:12<00:00, 45.22it/s][A
 99%|█████████▊| 536/543 [00:12<00:00, 45.10it/s][A
100%|█████████▉| 541/543 [00:12<00:00, 44.87it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.87it/s][A 40%|████      | 234/585 [01:49<01:42,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:22:12,053 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 03:22:12,309 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:22:15,559 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:22:15,796 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:22:15,898 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:02<45:41,  7.83s/it] 40%|████      | 236/585 [02:02<32:29,  5.59s/it] 41%|████      | 237/585 [02:02<23:11,  4.00s/it] 41%|████      | 238/585 [02:03<16:41,  2.89s/it] 41%|████      | 239/585 [02:03<12:09,  2.11s/it] 41%|████      | 240/585 [02:03<08:59,  1.56s/it] 41%|████      | 241/585 [02:03<06:46,  1.18s/it] 41%|████▏     | 242/585 [02:04<05:14,  1.09it/s] 42%|████▏     | 243/585 [02:04<04:09,  1.37it/s] 42%|████▏     | 244/585 [02:04<03:24,  1.67it/s] 42%|████▏     | 245/585 [02:05<02:52,  1.97it/s] 42%|████▏     | 246/585 [02:05<02:30,  2.26it/s] 42%|████▏     | 247/585 [02:05<02:18,  2.44it/s] 42%|████▏     | 248/585 [02:06<02:06,  2.66it/s] 43%|████▎     | 249/585 [02:06<01:57,  2.85it/s] 43%|████▎     | 250/585 [02:06<01:51,  3.00it/s] 43%|████▎     | 251/585 [02:06<01:47,  3.11it/s] 43%|████▎     | 252/585 [02:07<01:44,  3.19it/s] 43%|████▎     | 253/585 [02:07<01:41,  3.26it/s] 43%|████▎     | 254/585 [02:07<01:40,  3.30it/s] 44%|████▎     | 255/585 [02:08<01:39,  3.33it/s] 44%|████▍     | 256/585 [02:08<01:38,  3.36it/s] 44%|████▍     | 257/585 [02:08<01:37,  3.37it/s] 44%|████▍     | 258/585 [02:09<01:40,  3.27it/s] 44%|████▍     | 259/585 [02:09<01:38,  3.31it/s] 44%|████▍     | 260/585 [02:09<01:37,  3.34it/s] 45%|████▍     | 261/585 [02:09<01:36,  3.36it/s] 45%|████▍     | 262/585 [02:10<01:35,  3.38it/s] 45%|████▍     | 263/585 [02:10<01:35,  3.39it/s] 45%|████▌     | 264/585 [02:10<01:34,  3.39it/s] 45%|████▌     | 265/585 [02:11<01:34,  3.40it/s] 45%|████▌     | 266/585 [02:11<01:33,  3.42it/s] 46%|████▌     | 267/585 [02:11<01:32,  3.43it/s] 46%|████▌     | 268/585 [02:11<01:32,  3.44it/s] 46%|████▌     | 269/585 [02:12<01:34,  3.34it/s] 46%|████▌     | 270/585 [02:12<01:33,  3.37it/s] 46%|████▋     | 271/585 [02:12<01:32,  3.40it/s] 46%|████▋     | 272/585 [02:13<01:31,  3.41it/s] 47%|████▋     | 273/585 [02:13<01:31,  3.43it/s] 47%|████▋     | 274/585 [02:13<01:30,  3.44it/s] 47%|████▋     | 275/585 [02:13<01:30,  3.44it/s] 47%|████▋     | 276/585 [02:14<01:29,  3.45it/s] 47%|████▋     | 277/585 [02:14<01:29,  3.45it/s] 48%|████▊     | 278/585 [02:14<01:28,  3.45it/s] 48%|████▊     | 279/585 [02:15<01:28,  3.46it/s] 48%|████▊     | 280/585 [02:15<01:31,  3.34it/s] 48%|████▊     | 281/585 [02:15<01:30,  3.37it/s] 48%|████▊     | 282/585 [02:16<01:29,  3.40it/s] 48%|████▊     | 283/585 [02:16<01:28,  3.41it/s] 49%|████▊     | 284/585 [02:16<01:27,  3.43it/s] 49%|████▊     | 285/585 [02:16<01:27,  3.43it/s] 49%|████▉     | 286/585 [02:17<01:26,  3.44it/s] 49%|████▉     | 287/585 [02:17<01:26,  3.44it/s] 49%|████▉     | 288/585 [02:17<01:26,  3.45it/s] 49%|████▉     | 289/585 [02:18<01:25,  3.45it/s] 50%|████▉     | 290/585 [02:18<01:25,  3.45it/s] 50%|████▉     | 291/585 [02:18<01:25,  3.45it/s] 50%|████▉     | 292/585 [02:18<01:24,  3.46it/s] 50%|█████     | 293/585 [02:19<01:24,  3.46it/s] 50%|█████     | 294/585 [02:19<01:24,  3.46it/s] 50%|█████     | 295/585 [02:19<01:23,  3.45it/s] 51%|█████     | 296/585 [02:20<01:23,  3.45it/s] 51%|█████     | 297/585 [02:20<01:23,  3.45it/s] 51%|█████     | 298/585 [02:20<01:23,  3.45it/s] 51%|█████     | 299/585 [02:20<01:22,  3.45it/s] 51%|█████▏    | 300/585 [02:21<01:22,  3.46it/s] 51%|█████▏    | 301/585 [02:21<01:23,  3.39it/s] 52%|█████▏    | 302/585 [02:21<01:23,  3.41it/s] 52%|█████▏    | 303/585 [02:22<01:22,  3.42it/s] 52%|█████▏    | 304/585 [02:22<01:21,  3.43it/s] 52%|█████▏    | 305/585 [02:22<01:21,  3.44it/s] 52%|█████▏    | 306/585 [02:22<01:21,  3.44it/s] 52%|█████▏    | 307/585 [02:23<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:23<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:23<01:19,  3.45it/s] 53%|█████▎    | 310/585 [02:24<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:24<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:24<01:21,  3.37it/s] 54%|█████▎    | 313/585 [02:25<01:20,  3.40it/s] 54%|█████▎    | 314/585 [02:25<01:19,  3.41it/s] 54%|█████▍    | 315/585 [02:25<01:18,  3.43it/s] 54%|█████▍    | 316/585 [02:25<01:18,  3.43it/s] 54%|█████▍    | 317/585 [02:26<01:17,  3.44it/s] 54%|█████▍    | 318/585 [02:26<01:17,  3.44it/s] 55%|█████▍    | 319/585 [02:26<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:27<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:27<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:27<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:27<01:17,  3.37it/s] 55%|█████▌    | 324/585 [02:28<01:16,  3.40it/s] 56%|█████▌    | 325/585 [02:28<01:16,  3.42it/s] 56%|█████▌    | 326/585 [02:28<01:15,  3.43it/s] 56%|█████▌    | 327/585 [02:29<01:15,  3.44it/s] 56%|█████▌    | 328/585 [02:29<01:14,  3.44it/s] 56%|█████▌    | 329/585 [02:29<01:14,  3.45it/s] 56%|█████▋    | 330/585 [02:29<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:30<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:30<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:30<01:12,  3.45it/s] 57%|█████▋    | 334/585 [02:31<01:14,  3.36it/s] 57%|█████▋    | 335/585 [02:31<01:13,  3.39it/s] 57%|█████▋    | 336/585 [02:31<01:13,  3.41it/s] 58%|█████▊    | 337/585 [02:32<01:12,  3.42it/s] 58%|█████▊    | 338/585 [02:32<01:11,  3.43it/s] 58%|█████▊    | 339/585 [02:32<01:11,  3.44it/s] 58%|█████▊    | 340/585 [02:32<01:11,  3.44it/s] 58%|█████▊    | 341/585 [02:33<01:10,  3.45it/s] 58%|█████▊    | 342/585 [02:33<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:33<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:34<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:34<01:11,  3.38it/s] 59%|█████▉    | 346/585 [02:34<01:10,  3.40it/s] 59%|█████▉    | 347/585 [02:34<01:09,  3.42it/s] 59%|█████▉    | 348/585 [02:35<01:09,  3.43it/s] 60%|█████▉    | 349/585 [02:35<01:08,  3.44it/s] 60%|█████▉    | 350/585 [02:35<01:08,  3.44it/s] 60%|██████    | 351/585 [02:36<01:07,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 03:22:58,772 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:22:58,772 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:22:58,772 >>   Batch size = 8
{'eval_loss': 1.1745545864105225, 'eval_runtime': 12.3707, 'eval_samples_per_second': 350.99, 'eval_steps_per_second': 43.894, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.11it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.03it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.14it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.25it/s][A
  5%|▍         | 27/543 [00:00<00:11, 45.75it/s][A
  6%|▌         | 32/543 [00:00<00:11, 45.47it/s][A
  7%|▋         | 37/543 [00:00<00:11, 45.38it/s][A
  8%|▊         | 42/543 [00:00<00:11, 45.19it/s][A
  9%|▊         | 47/543 [00:01<00:11, 43.83it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.37it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.69it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.90it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.95it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.86it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.86it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.85it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.77it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.88it/s][A
 18%|█▊        | 97/543 [00:02<00:09, 44.96it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 45.17it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 45.24it/s][A
 21%|██        | 112/543 [00:02<00:09, 45.26it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 45.06it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 45.06it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.90it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.84it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.83it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 44.96it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 45.12it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 45.22it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 45.23it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 45.25it/s][A
 31%|███       | 167/543 [00:03<00:08, 45.23it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 45.06it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.92it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 42.77it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 43.55it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.19it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.59it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.81it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.96it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.88it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.83it/s][A
 41%|████      | 222/543 [00:04<00:07, 44.51it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.63it/s][A
 43%|████▎     | 232/543 [00:05<00:06, 44.72it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.94it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 45.02it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 45.20it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 45.34it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 45.33it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 45.04it/s][A
 49%|████▉     | 267/543 [00:05<00:06, 44.84it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.72it/s][A
 51%|█████     | 277/543 [00:06<00:05, 44.76it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.93it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 45.16it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 45.21it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 45.33it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 45.23it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 45.09it/s][A
 57%|█████▋    | 312/543 [00:06<00:05, 44.87it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 42.68it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.52it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.05it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.49it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.82it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.96it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.93it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.87it/s][A
 66%|██████▌   | 357/543 [00:07<00:04, 44.65it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.58it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.82it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 45.01it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 45.12it/s][A
 70%|███████   | 382/543 [00:08<00:03, 45.25it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 45.30it/s][A
 72%|███████▏  | 392/543 [00:08<00:05, 29.08it/s][A
 73%|███████▎  | 397/543 [00:09<00:04, 32.77it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 35.78it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 38.25it/s][A
 76%|███████▌  | 412/543 [00:09<00:03, 40.22it/s][A
 77%|███████▋  | 417/543 [00:09<00:03, 41.71it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 42.83it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.39it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.56it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.65it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.98it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 42.02it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.07it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.73it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.33it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.66it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.85it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.88it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.69it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.52it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.48it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.79it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.90it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 45.01it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 45.12it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 45.20it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 45.26it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 45.17it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 45.03it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.88it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.99it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.99it/s][A 60%|██████    | 351/585 [02:48<01:07,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:23:11,192 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 03:23:11,416 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:23:14,846 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:23:15,323 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:23:15,436 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:03<32:08,  8.28s/it] 60%|██████    | 353/585 [03:03<22:46,  5.89s/it] 61%|██████    | 354/585 [03:03<16:13,  4.21s/it] 61%|██████    | 355/585 [03:03<11:38,  3.04s/it] 61%|██████    | 356/585 [03:04<08:26,  2.21s/it] 61%|██████    | 357/585 [03:04<06:13,  1.64s/it] 61%|██████    | 358/585 [03:04<04:40,  1.23s/it] 61%|██████▏   | 359/585 [03:05<03:35,  1.05it/s] 62%|██████▏   | 360/585 [03:05<02:49,  1.33it/s] 62%|██████▏   | 361/585 [03:05<02:18,  1.62it/s] 62%|██████▏   | 362/585 [03:05<01:55,  1.93it/s] 62%|██████▏   | 363/585 [03:06<01:40,  2.21it/s] 62%|██████▏   | 364/585 [03:06<01:32,  2.39it/s] 62%|██████▏   | 365/585 [03:06<01:23,  2.63it/s] 63%|██████▎   | 366/585 [03:07<01:17,  2.82it/s] 63%|██████▎   | 367/585 [03:07<01:13,  2.97it/s] 63%|██████▎   | 368/585 [03:07<01:10,  3.09it/s] 63%|██████▎   | 369/585 [03:08<01:07,  3.18it/s] 63%|██████▎   | 370/585 [03:08<01:06,  3.25it/s] 63%|██████▎   | 371/585 [03:08<01:05,  3.29it/s] 64%|██████▎   | 372/585 [03:08<01:04,  3.33it/s] 64%|██████▍   | 373/585 [03:09<01:03,  3.35it/s] 64%|██████▍   | 374/585 [03:09<01:02,  3.37it/s] 64%|██████▍   | 375/585 [03:09<01:05,  3.23it/s] 64%|██████▍   | 376/585 [03:10<01:03,  3.28it/s] 64%|██████▍   | 377/585 [03:10<01:02,  3.32it/s] 65%|██████▍   | 378/585 [03:10<01:01,  3.34it/s] 65%|██████▍   | 379/585 [03:11<01:01,  3.37it/s] 65%|██████▍   | 380/585 [03:11<01:00,  3.38it/s] 65%|██████▌   | 381/585 [03:11<01:00,  3.38it/s] 65%|██████▌   | 382/585 [03:11<00:59,  3.39it/s] 65%|██████▌   | 383/585 [03:12<00:59,  3.40it/s] 66%|██████▌   | 384/585 [03:12<00:59,  3.39it/s] 66%|██████▌   | 385/585 [03:12<00:58,  3.40it/s] 66%|██████▌   | 386/585 [03:13<01:00,  3.30it/s] 66%|██████▌   | 387/585 [03:13<00:59,  3.33it/s] 66%|██████▋   | 388/585 [03:13<00:58,  3.35it/s] 66%|██████▋   | 389/585 [03:14<00:58,  3.37it/s] 67%|██████▋   | 390/585 [03:14<00:57,  3.38it/s] 67%|██████▋   | 391/585 [03:14<00:57,  3.39it/s] 67%|██████▋   | 392/585 [03:14<00:56,  3.39it/s] 67%|██████▋   | 393/585 [03:15<00:56,  3.40it/s] 67%|██████▋   | 394/585 [03:15<00:56,  3.40it/s] 68%|██████▊   | 395/585 [03:15<00:55,  3.40it/s] 68%|██████▊   | 396/585 [03:16<00:55,  3.40it/s] 68%|██████▊   | 397/585 [03:16<00:56,  3.30it/s] 68%|██████▊   | 398/585 [03:16<00:56,  3.33it/s] 68%|██████▊   | 399/585 [03:17<00:55,  3.35it/s] 68%|██████▊   | 400/585 [03:17<00:54,  3.37it/s] 69%|██████▊   | 401/585 [03:17<00:54,  3.38it/s] 69%|██████▊   | 402/585 [03:17<00:53,  3.39it/s] 69%|██████▉   | 403/585 [03:18<00:53,  3.39it/s] 69%|██████▉   | 404/585 [03:18<00:53,  3.40it/s] 69%|██████▉   | 405/585 [03:18<00:52,  3.40it/s] 69%|██████▉   | 406/585 [03:19<00:52,  3.40it/s] 70%|██████▉   | 407/585 [03:19<00:52,  3.41it/s] 70%|██████▉   | 408/585 [03:19<00:54,  3.27it/s] 70%|██████▉   | 409/585 [03:19<00:53,  3.31it/s] 70%|███████   | 410/585 [03:20<00:52,  3.34it/s] 70%|███████   | 411/585 [03:20<00:51,  3.36it/s] 70%|███████   | 412/585 [03:20<00:51,  3.37it/s] 71%|███████   | 413/585 [03:21<00:50,  3.38it/s] 71%|███████   | 414/585 [03:21<00:50,  3.39it/s] 71%|███████   | 415/585 [03:21<00:50,  3.39it/s] 71%|███████   | 416/585 [03:22<00:49,  3.40it/s] 71%|███████▏  | 417/585 [03:22<00:49,  3.40it/s] 71%|███████▏  | 418/585 [03:22<00:49,  3.40it/s] 72%|███████▏  | 419/585 [03:22<00:50,  3.28it/s] 72%|███████▏  | 420/585 [03:23<00:49,  3.33it/s] 72%|███████▏  | 421/585 [03:23<00:48,  3.37it/s] 72%|███████▏  | 422/585 [03:23<00:48,  3.39it/s] 72%|███████▏  | 423/585 [03:24<00:47,  3.41it/s] 72%|███████▏  | 424/585 [03:24<00:46,  3.43it/s] 73%|███████▎  | 425/585 [03:24<00:46,  3.44it/s] 73%|███████▎  | 426/585 [03:24<00:46,  3.44it/s] 73%|███████▎  | 427/585 [03:25<00:45,  3.45it/s] 73%|███████▎  | 428/585 [03:25<00:45,  3.45it/s] 73%|███████▎  | 429/585 [03:25<00:45,  3.45it/s] 74%|███████▎  | 430/585 [03:26<01:05,  2.36it/s] 74%|███████▎  | 431/585 [03:26<00:59,  2.61it/s] 74%|███████▍  | 432/585 [03:27<00:54,  2.81it/s] 74%|███████▍  | 433/585 [03:27<00:51,  2.98it/s] 74%|███████▍  | 434/585 [03:27<00:48,  3.11it/s] 74%|███████▍  | 435/585 [03:28<00:46,  3.20it/s] 75%|███████▍  | 436/585 [03:28<00:45,  3.27it/s] 75%|███████▍  | 437/585 [03:28<00:44,  3.33it/s] 75%|███████▍  | 438/585 [03:28<00:43,  3.36it/s] 75%|███████▌  | 439/585 [03:29<00:44,  3.28it/s] 75%|███████▌  | 440/585 [03:29<00:43,  3.33it/s] 75%|███████▌  | 441/585 [03:29<00:42,  3.37it/s] 76%|███████▌  | 442/585 [03:30<00:42,  3.39it/s] 76%|███████▌  | 443/585 [03:30<00:41,  3.41it/s] 76%|███████▌  | 444/585 [03:30<00:41,  3.42it/s] 76%|███████▌  | 445/585 [03:30<00:40,  3.43it/s] 76%|███████▌  | 446/585 [03:31<00:40,  3.44it/s] 76%|███████▋  | 447/585 [03:31<00:40,  3.44it/s] 77%|███████▋  | 448/585 [03:31<00:39,  3.44it/s] 77%|███████▋  | 449/585 [03:32<00:39,  3.45it/s] 77%|███████▋  | 450/585 [03:32<00:40,  3.37it/s] 77%|███████▋  | 451/585 [03:32<00:39,  3.40it/s] 77%|███████▋  | 452/585 [03:33<00:38,  3.42it/s] 77%|███████▋  | 453/585 [03:33<00:38,  3.42it/s] 78%|███████▊  | 454/585 [03:33<00:38,  3.43it/s] 78%|███████▊  | 455/585 [03:33<00:37,  3.44it/s] 78%|███████▊  | 456/585 [03:34<00:37,  3.44it/s] 78%|███████▊  | 457/585 [03:34<00:37,  3.45it/s] 78%|███████▊  | 458/585 [03:34<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:35<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:35<00:36,  3.45it/s] 79%|███████▉  | 461/585 [03:35<00:37,  3.32it/s] 79%|███████▉  | 462/585 [03:35<00:36,  3.36it/s] 79%|███████▉  | 463/585 [03:36<00:36,  3.39it/s] 79%|███████▉  | 464/585 [03:36<00:35,  3.41it/s] 79%|███████▉  | 465/585 [03:36<00:35,  3.42it/s] 80%|███████▉  | 466/585 [03:37<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:37<00:34,  3.44it/s] 80%|████████  | 468/585 [03:37<00:33,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 03:24:00,336 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:24:00,336 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:24:00,336 >>   Batch size = 8
{'eval_loss': 1.1858922243118286, 'eval_runtime': 12.3289, 'eval_samples_per_second': 352.181, 'eval_steps_per_second': 44.043, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.41it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.24it/s][A
  3%|▎         | 17/543 [00:00<00:11, 47.30it/s][A
  4%|▍         | 22/543 [00:00<00:11, 46.30it/s][A
  5%|▍         | 27/543 [00:00<00:11, 45.82it/s][A
  6%|▌         | 32/543 [00:00<00:11, 45.48it/s][A
  7%|▋         | 37/543 [00:00<00:13, 37.60it/s][A
  8%|▊         | 42/543 [00:00<00:12, 39.77it/s][A
  9%|▊         | 47/543 [00:01<00:11, 41.45it/s][A
 10%|▉         | 52/543 [00:01<00:11, 42.63it/s][A
 10%|█         | 57/543 [00:01<00:11, 43.51it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.18it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.58it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.77it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.50it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.30it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.45it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.76it/s][A
 18%|█▊        | 97/543 [00:02<00:09, 44.96it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 45.19it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 45.29it/s][A
 21%|██        | 112/543 [00:02<00:09, 45.44it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 45.31it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.99it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.72it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.71it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.82it/s][A
 26%|██▌       | 142/543 [00:03<00:08, 44.98it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 45.10it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 45.17it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 45.21it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 45.22it/s][A
 31%|███       | 167/543 [00:03<00:08, 45.27it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.91it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.22it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.46it/s][A
 34%|███▍      | 187/543 [00:04<00:07, 44.65it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.91it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.97it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 45.15it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 45.05it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.92it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.88it/s][A
 41%|████      | 222/543 [00:04<00:07, 44.96it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.92it/s][A
 43%|████▎     | 232/543 [00:05<00:06, 45.02it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 45.12it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.16it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 44.98it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 45.09it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 45.01it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 44.95it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 44.99it/s][A
 50%|█████     | 273/543 [00:06<00:05, 45.04it/s][A
 51%|█████     | 278/543 [00:06<00:05, 45.13it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 45.18it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 45.09it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 45.16it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 45.12it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 44.96it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 43.60it/s][A
 58%|█████▊    | 313/543 [00:07<00:05, 44.13it/s][A
 59%|█████▊    | 318/543 [00:07<00:08, 27.47it/s][A
 59%|█████▉    | 323/543 [00:07<00:07, 31.33it/s][A
 60%|██████    | 328/543 [00:07<00:06, 34.57it/s][A
 61%|██████▏   | 333/543 [00:07<00:05, 37.30it/s][A
 62%|██████▏   | 338/543 [00:07<00:05, 39.45it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 41.12it/s][A
 64%|██████▍   | 348/543 [00:08<00:04, 42.42it/s][A
 65%|██████▌   | 353/543 [00:08<00:04, 43.35it/s][A
 66%|██████▌   | 358/543 [00:08<00:04, 43.62it/s][A
 67%|██████▋   | 363/543 [00:08<00:04, 43.75it/s][A
 68%|██████▊   | 368/543 [00:08<00:03, 43.86it/s][A
 69%|██████▊   | 373/543 [00:08<00:03, 44.30it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 44.61it/s][A
 71%|███████   | 383/543 [00:08<00:03, 44.86it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 45.04it/s][A
 72%|███████▏  | 393/543 [00:09<00:03, 45.15it/s][A
 73%|███████▎  | 398/543 [00:09<00:03, 45.22it/s][A
 74%|███████▍  | 403/543 [00:09<00:03, 45.15it/s][A
 75%|███████▌  | 408/543 [00:09<00:03, 44.90it/s][A
 76%|███████▌  | 413/543 [00:09<00:02, 44.83it/s][A
 77%|███████▋  | 418/543 [00:09<00:02, 44.86it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 44.96it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 45.12it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 43.91it/s][A
 81%|████████  | 438/543 [00:10<00:02, 44.39it/s][A
 82%|████████▏ | 443/543 [00:10<00:02, 44.74it/s][A
 83%|████████▎ | 448/543 [00:10<00:02, 44.67it/s][A
 83%|████████▎ | 453/543 [00:10<00:02, 44.73it/s][A
 84%|████████▍ | 458/543 [00:10<00:01, 44.72it/s][A
 85%|████████▌ | 463/543 [00:10<00:01, 44.72it/s][A
 86%|████████▌ | 468/543 [00:10<00:01, 44.85it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 44.80it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 45.00it/s][A
 89%|████████▉ | 483/543 [00:11<00:01, 45.15it/s][A
 90%|████████▉ | 488/543 [00:11<00:01, 45.24it/s][A
 91%|█████████ | 493/543 [00:11<00:01, 45.29it/s][A
 92%|█████████▏| 498/543 [00:11<00:00, 45.25it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 45.08it/s][A
 94%|█████████▎| 508/543 [00:11<00:00, 45.01it/s][A
 94%|█████████▍| 513/543 [00:11<00:00, 45.01it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 45.00it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 45.10it/s][A
 97%|█████████▋| 528/543 [00:12<00:00, 45.17it/s][A
 98%|█████████▊| 533/543 [00:12<00:00, 45.23it/s][A
 99%|█████████▉| 538/543 [00:12<00:00, 45.28it/s][A
100%|██████████| 543/543 [00:12<00:00, 45.21it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 45.21it/s][A 80%|████████  | 468/585 [03:50<00:33,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:24:12,795 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 03:24:12,911 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:24:15,573 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:24:16,119 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:24:16,373 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:02<14:36,  7.56s/it] 80%|████████  | 470/585 [04:02<10:20,  5.40s/it] 81%|████████  | 471/585 [04:02<07:20,  3.87s/it] 81%|████████  | 472/585 [04:03<05:15,  2.79s/it] 81%|████████  | 473/585 [04:03<03:48,  2.04s/it] 81%|████████  | 474/585 [04:03<02:48,  1.52s/it] 81%|████████  | 475/585 [04:04<02:06,  1.15s/it] 81%|████████▏ | 476/585 [04:04<01:37,  1.12it/s] 82%|████████▏ | 477/585 [04:04<01:17,  1.40it/s] 82%|████████▏ | 478/585 [04:04<01:02,  1.70it/s] 82%|████████▏ | 479/585 [04:05<00:52,  2.00it/s] 82%|████████▏ | 480/585 [04:05<00:45,  2.29it/s] 82%|████████▏ | 481/585 [04:05<00:41,  2.48it/s] 82%|████████▏ | 482/585 [04:06<00:38,  2.71it/s] 83%|████████▎ | 483/585 [04:06<00:35,  2.89it/s] 83%|████████▎ | 484/585 [04:06<00:33,  3.04it/s] 83%|████████▎ | 485/585 [04:06<00:31,  3.15it/s] 83%|████████▎ | 486/585 [04:07<00:30,  3.24it/s] 83%|████████▎ | 487/585 [04:07<00:29,  3.30it/s] 83%|████████▎ | 488/585 [04:07<00:28,  3.34it/s] 84%|████████▎ | 489/585 [04:08<00:28,  3.38it/s] 84%|████████▍ | 490/585 [04:08<00:27,  3.40it/s] 84%|████████▍ | 491/585 [04:08<00:27,  3.42it/s] 84%|████████▍ | 492/585 [04:09<00:28,  3.24it/s] 84%|████████▍ | 493/585 [04:09<00:27,  3.30it/s] 84%|████████▍ | 494/585 [04:09<00:27,  3.35it/s] 85%|████████▍ | 495/585 [04:09<00:26,  3.38it/s] 85%|████████▍ | 496/585 [04:10<00:26,  3.40it/s] 85%|████████▍ | 497/585 [04:10<00:25,  3.42it/s] 85%|████████▌ | 498/585 [04:10<00:25,  3.43it/s] 85%|████████▌ | 499/585 [04:11<00:25,  3.44it/s] 85%|████████▌ | 500/585 [04:11<00:24,  3.44it/s]                                                  85%|████████▌ | 500/585 [04:11<00:24,  3.44it/s] 86%|████████▌ | 501/585 [04:11<00:24,  3.44it/s] 86%|████████▌ | 502/585 [04:11<00:24,  3.45it/s] 86%|████████▌ | 503/585 [04:12<00:24,  3.38it/s] 86%|████████▌ | 504/585 [04:12<00:23,  3.40it/s] 86%|████████▋ | 505/585 [04:12<00:23,  3.42it/s] 86%|████████▋ | 506/585 [04:13<00:23,  3.43it/s] 87%|████████▋ | 507/585 [04:13<00:22,  3.44it/s] 87%|████████▋ | 508/585 [04:13<00:22,  3.44it/s] 87%|████████▋ | 509/585 [04:13<00:22,  3.44it/s] 87%|████████▋ | 510/585 [04:14<00:21,  3.45it/s] 87%|████████▋ | 511/585 [04:14<00:21,  3.45it/s] 88%|████████▊ | 512/585 [04:14<00:21,  3.45it/s] 88%|████████▊ | 513/585 [04:15<00:20,  3.45it/s] 88%|████████▊ | 514/585 [04:15<00:21,  3.35it/s] 88%|████████▊ | 515/585 [04:15<00:20,  3.38it/s] 88%|████████▊ | 516/585 [04:16<00:20,  3.40it/s] 88%|████████▊ | 517/585 [04:16<00:19,  3.42it/s] 89%|████████▊ | 518/585 [04:16<00:19,  3.43it/s] 89%|████████▊ | 519/585 [04:16<00:19,  3.44it/s] 89%|████████▉ | 520/585 [04:17<00:18,  3.44it/s] 89%|████████▉ | 521/585 [04:17<00:18,  3.44it/s] 89%|████████▉ | 522/585 [04:17<00:18,  3.45it/s] 89%|████████▉ | 523/585 [04:18<00:17,  3.45it/s] 90%|████████▉ | 524/585 [04:18<00:17,  3.45it/s] 90%|████████▉ | 525/585 [04:18<00:17,  3.37it/s] 90%|████████▉ | 526/585 [04:18<00:17,  3.39it/s] 90%|█████████ | 527/585 [04:19<00:16,  3.41it/s] 90%|█████████ | 528/585 [04:19<00:16,  3.43it/s] 90%|█████████ | 529/585 [04:19<00:16,  3.44it/s] 91%|█████████ | 530/585 [04:20<00:15,  3.44it/s] 91%|█████████ | 531/585 [04:20<00:15,  3.45it/s] 91%|█████████ | 532/585 [04:20<00:15,  3.45it/s] 91%|█████████ | 533/585 [04:20<00:15,  3.45it/s] 91%|█████████▏| 534/585 [04:21<00:14,  3.45it/s] 91%|█████████▏| 535/585 [04:21<00:14,  3.45it/s] 92%|█████████▏| 536/585 [04:21<00:14,  3.45it/s] 92%|█████████▏| 537/585 [04:22<00:13,  3.45it/s] 92%|█████████▏| 538/585 [04:22<00:13,  3.45it/s] 92%|█████████▏| 539/585 [04:22<00:13,  3.45it/s] 92%|█████████▏| 540/585 [04:23<00:13,  3.45it/s] 92%|█████████▏| 541/585 [04:23<00:12,  3.45it/s] 93%|█████████▎| 542/585 [04:23<00:12,  3.46it/s] 93%|█████████▎| 543/585 [04:23<00:12,  3.45it/s] 93%|█████████▎| 544/585 [04:24<00:11,  3.46it/s] 93%|█████████▎| 545/585 [04:24<00:12,  3.29it/s] 93%|█████████▎| 546/585 [04:24<00:11,  3.34it/s] 94%|█████████▎| 547/585 [04:25<00:11,  3.37it/s] 94%|█████████▎| 548/585 [04:25<00:10,  3.40it/s] 94%|█████████▍| 549/585 [04:25<00:10,  3.41it/s] 94%|█████████▍| 550/585 [04:25<00:10,  3.43it/s] 94%|█████████▍| 551/585 [04:26<00:09,  3.43it/s] 94%|█████████▍| 552/585 [04:26<00:09,  3.44it/s] 95%|█████████▍| 553/585 [04:26<00:09,  3.44it/s] 95%|█████████▍| 554/585 [04:27<00:08,  3.45it/s] 95%|█████████▍| 555/585 [04:27<00:08,  3.45it/s] 95%|█████████▌| 556/585 [04:27<00:08,  3.36it/s] 95%|█████████▌| 557/585 [04:28<00:08,  3.39it/s] 95%|█████████▌| 558/585 [04:28<00:07,  3.41it/s] 96%|█████████▌| 559/585 [04:28<00:07,  3.42it/s] 96%|█████████▌| 560/585 [04:28<00:07,  3.43it/s] 96%|█████████▌| 561/585 [04:29<00:06,  3.44it/s] 96%|█████████▌| 562/585 [04:29<00:06,  3.44it/s] 96%|█████████▌| 563/585 [04:29<00:06,  3.45it/s] 96%|█████████▋| 564/585 [04:30<00:06,  3.45it/s] 97%|█████████▋| 565/585 [04:30<00:05,  3.45it/s] 97%|█████████▋| 566/585 [04:30<00:05,  3.45it/s] 97%|█████████▋| 567/585 [04:30<00:05,  3.35it/s] 97%|█████████▋| 568/585 [04:31<00:05,  3.38it/s] 97%|█████████▋| 569/585 [04:31<00:04,  3.40it/s] 97%|█████████▋| 570/585 [04:31<00:04,  3.42it/s] 98%|█████████▊| 571/585 [04:32<00:04,  3.43it/s] 98%|█████████▊| 572/585 [04:32<00:03,  3.43it/s] 98%|█████████▊| 573/585 [04:32<00:03,  3.44it/s] 98%|█████████▊| 574/585 [04:32<00:03,  3.44it/s] 98%|█████████▊| 575/585 [04:33<00:02,  3.45it/s] 98%|█████████▊| 576/585 [04:33<00:02,  3.45it/s] 99%|█████████▊| 577/585 [04:33<00:02,  3.45it/s] 99%|█████████▉| 578/585 [04:34<00:02,  3.38it/s] 99%|█████████▉| 579/585 [04:34<00:01,  3.40it/s] 99%|█████████▉| 580/585 [04:34<00:01,  3.42it/s] 99%|█████████▉| 581/585 [04:35<00:01,  3.43it/s] 99%|█████████▉| 582/585 [04:35<00:00,  3.44it/s]100%|█████████▉| 583/585 [04:35<00:00,  3.44it/s]100%|█████████▉| 584/585 [04:35<00:00,  3.44it/s]100%|██████████| 585/585 [04:36<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 03:24:58,787 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:24:58,787 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:24:58,787 >>   Batch size = 8
{'eval_loss': 1.1986002922058105, 'eval_runtime': 12.3827, 'eval_samples_per_second': 350.651, 'eval_steps_per_second': 43.852, 'epoch': 4.0}
{'loss': 0.3735, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.44it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.49it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.40it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.49it/s][A
  5%|▌         | 28/543 [00:00<00:11, 45.84it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.44it/s][A
  7%|▋         | 38/543 [00:00<00:11, 42.95it/s][A
  8%|▊         | 43/543 [00:00<00:11, 43.62it/s][A
  9%|▉         | 48/543 [00:01<00:11, 44.19it/s][A
 10%|▉         | 53/543 [00:01<00:11, 44.49it/s][A
 11%|█         | 58/543 [00:01<00:10, 44.86it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 45.09it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 45.06it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 44.93it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 44.63it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 44.70it/s][A
 16%|█▌        | 88/543 [00:01<00:10, 44.78it/s][A
 17%|█▋        | 93/543 [00:02<00:10, 44.89it/s][A
 18%|█▊        | 98/543 [00:02<00:09, 45.05it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 45.24it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 45.35it/s][A
 21%|██        | 113/543 [00:02<00:09, 45.28it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 45.07it/s][A
 23%|██▎       | 123/543 [00:02<00:09, 44.97it/s][A
 24%|██▎       | 128/543 [00:02<00:09, 44.90it/s][A
 24%|██▍       | 133/543 [00:02<00:09, 44.85it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 44.99it/s][A
 26%|██▋       | 143/543 [00:03<00:08, 45.09it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 45.29it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 45.40it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 45.27it/s][A
 30%|███       | 163/543 [00:03<00:08, 45.07it/s][A
 31%|███       | 168/543 [00:03<00:08, 44.96it/s][A
 32%|███▏      | 173/543 [00:03<00:08, 44.58it/s][A
 33%|███▎      | 178/543 [00:03<00:08, 44.63it/s][A
 34%|███▎      | 183/543 [00:04<00:08, 44.74it/s][A
 35%|███▍      | 188/543 [00:04<00:07, 44.89it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 45.05it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 45.13it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 45.32it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 45.17it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 45.08it/s][A
 40%|████      | 218/543 [00:04<00:07, 44.99it/s][A
 41%|████      | 223/543 [00:04<00:07, 44.89it/s][A
 42%|████▏     | 228/543 [00:05<00:07, 44.85it/s][A
 43%|████▎     | 233/543 [00:05<00:06, 44.97it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 45.14it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 45.27it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 45.38it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 45.23it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 45.08it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 44.98it/s][A
 49%|████▉     | 268/543 [00:05<00:06, 44.87it/s][A
 50%|█████     | 273/543 [00:06<00:06, 44.80it/s][A
 51%|█████     | 278/543 [00:06<00:05, 44.85it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 45.03it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 45.22it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 45.31it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 45.28it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 45.21it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 45.04it/s][A
 58%|█████▊    | 313/543 [00:06<00:05, 42.46it/s][A
 59%|█████▊    | 318/543 [00:07<00:05, 42.23it/s][A
 59%|█████▉    | 323/543 [00:07<00:05, 43.15it/s][A
 60%|██████    | 328/543 [00:07<00:04, 43.89it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 44.40it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 44.70it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 44.87it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 44.85it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 44.58it/s][A
 66%|██████▌   | 358/543 [00:07<00:04, 44.51it/s][A
 67%|██████▋   | 363/543 [00:08<00:04, 44.59it/s][A
 68%|██████▊   | 368/543 [00:08<00:03, 44.88it/s][A
 69%|██████▊   | 373/543 [00:08<00:03, 45.15it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 45.14it/s][A
 71%|███████   | 383/543 [00:08<00:03, 45.31it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 45.29it/s][A
 72%|███████▏  | 393/543 [00:08<00:04, 30.65it/s][A
 73%|███████▎  | 399/543 [00:09<00:04, 35.29it/s][A
 74%|███████▍  | 404/543 [00:09<00:03, 37.80it/s][A
 75%|███████▌  | 409/543 [00:09<00:03, 39.79it/s][A
 76%|███████▌  | 414/543 [00:09<00:03, 41.32it/s][A
 77%|███████▋  | 419/543 [00:09<00:02, 42.47it/s][A
 78%|███████▊  | 424/543 [00:09<00:02, 43.36it/s][A
 79%|███████▉  | 429/543 [00:09<00:02, 43.95it/s][A
 80%|███████▉  | 434/543 [00:09<00:02, 43.92it/s][A
 81%|████████  | 439/543 [00:09<00:02, 43.85it/s][A
 82%|████████▏ | 444/543 [00:10<00:02, 43.98it/s][A
 83%|████████▎ | 449/543 [00:10<00:02, 44.32it/s][A
 84%|████████▎ | 454/543 [00:10<00:01, 44.68it/s][A
 85%|████████▍ | 459/543 [00:10<00:01, 44.75it/s][A
 85%|████████▌ | 464/543 [00:10<00:01, 45.21it/s][A
 86%|████████▋ | 469/543 [00:10<00:01, 45.34it/s][A
 87%|████████▋ | 474/543 [00:10<00:01, 45.26it/s][A
 88%|████████▊ | 479/543 [00:10<00:01, 44.76it/s][A
 89%|████████▉ | 484/543 [00:10<00:01, 44.66it/s][A
 90%|█████████ | 489/543 [00:11<00:01, 44.66it/s][A
 91%|█████████ | 494/543 [00:11<00:01, 44.78it/s][A
 92%|█████████▏| 499/543 [00:11<00:00, 44.99it/s][A
 93%|█████████▎| 504/543 [00:11<00:00, 45.21it/s][A
 94%|█████████▎| 509/543 [00:11<00:00, 45.33it/s][A
 95%|█████████▍| 514/543 [00:11<00:00, 45.38it/s][A
 96%|█████████▌| 519/543 [00:11<00:00, 45.34it/s][A
 97%|█████████▋| 524/543 [00:11<00:00, 45.05it/s][A
 97%|█████████▋| 529/543 [00:11<00:00, 44.68it/s][A
 98%|█████████▊| 534/543 [00:12<00:00, 44.76it/s][A
 99%|█████████▉| 539/543 [00:12<00:00, 44.75it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.75it/s][A100%|██████████| 585/585 [04:48<00:00,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:25:11,113 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 03:25:11,241 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:25:13,946 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:25:14,039 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:25:14,088 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 03:25:22,691 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 03:25:22,730 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117 (score: 1.1606554985046387).
                                                 100%|██████████| 585/585 [05:13<00:00,  3.44it/s]100%|██████████| 585/585 [05:13<00:00,  1.86it/s]
[INFO|trainer.py:1894] 2023-08-29 03:25:36,355 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 03:25:36,509 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:25:39,527 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:25:39,652 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:25:39,714 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:25:40,282 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:40,283 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:40,283 >>   train_loss               =      0.371
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:40,283 >>   train_runtime            = 0:05:13.68
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:40,283 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:40,283 >>   train_samples_per_second =    119.546
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:40,283 >>   train_steps_per_second   =      1.865
{'eval_loss': 1.2050474882125854, 'eval_runtime': 12.253, 'eval_samples_per_second': 354.361, 'eval_steps_per_second': 44.316, 'epoch': 5.0}
{'train_runtime': 313.6875, 'train_samples_per_second': 119.546, 'train_steps_per_second': 1.865, 'train_loss': 0.3710194807786208, 'epoch': 5.0}
08/29/2023 03:25:40 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 03:25:40,605 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:25:40,605 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 03:25:40,605 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 56.25it/s]  2%|▏         | 12/543 [00:00<00:10, 49.48it/s]  3%|▎         | 18/543 [00:00<00:10, 47.78it/s]  4%|▍         | 23/543 [00:00<00:11, 46.98it/s]  5%|▌         | 28/543 [00:00<00:11, 46.46it/s]  6%|▌         | 33/543 [00:00<00:11, 46.20it/s]  7%|▋         | 38/543 [00:00<00:10, 46.06it/s]  8%|▊         | 43/543 [00:00<00:10, 45.69it/s]  9%|▉         | 48/543 [00:01<00:10, 45.36it/s] 10%|▉         | 53/543 [00:01<00:10, 45.24it/s] 11%|█         | 58/543 [00:01<00:10, 45.38it/s] 12%|█▏        | 63/543 [00:01<00:10, 45.52it/s] 13%|█▎        | 68/543 [00:01<00:10, 45.46it/s] 13%|█▎        | 73/543 [00:01<00:10, 45.49it/s] 14%|█▍        | 78/543 [00:01<00:10, 45.49it/s] 15%|█▌        | 83/543 [00:01<00:10, 45.45it/s] 16%|█▌        | 88/543 [00:01<00:10, 45.28it/s] 17%|█▋        | 93/543 [00:02<00:10, 42.32it/s] 18%|█▊        | 98/543 [00:02<00:10, 43.29it/s] 19%|█▉        | 103/543 [00:02<00:10, 43.98it/s] 20%|█▉        | 108/543 [00:02<00:09, 44.43it/s] 21%|██        | 113/543 [00:02<00:09, 44.73it/s] 22%|██▏       | 118/543 [00:02<00:09, 45.00it/s] 23%|██▎       | 123/543 [00:02<00:09, 45.19it/s] 24%|██▎       | 128/543 [00:02<00:09, 45.29it/s] 24%|██▍       | 133/543 [00:02<00:09, 44.93it/s] 25%|██▌       | 138/543 [00:03<00:09, 44.95it/s] 26%|██▋       | 143/543 [00:03<00:08, 45.04it/s] 27%|██▋       | 148/543 [00:03<00:08, 45.23it/s] 28%|██▊       | 153/543 [00:03<00:08, 45.28it/s] 29%|██▉       | 158/543 [00:03<00:08, 45.34it/s] 30%|███       | 163/543 [00:03<00:08, 45.37it/s] 31%|███       | 168/543 [00:03<00:08, 45.35it/s] 32%|███▏      | 173/543 [00:03<00:08, 45.31it/s] 33%|███▎      | 178/543 [00:03<00:08, 45.05it/s] 34%|███▎      | 183/543 [00:04<00:08, 44.96it/s] 35%|███▍      | 188/543 [00:04<00:07, 45.06it/s] 36%|███▌      | 193/543 [00:04<00:07, 45.14it/s] 36%|███▋      | 198/543 [00:04<00:07, 45.31it/s] 37%|███▋      | 203/543 [00:04<00:07, 45.32it/s] 38%|███▊      | 208/543 [00:04<00:07, 45.47it/s] 39%|███▉      | 213/543 [00:04<00:07, 45.48it/s] 40%|████      | 218/543 [00:04<00:07, 45.35it/s] 41%|████      | 223/543 [00:04<00:07, 45.13it/s] 42%|████▏     | 228/543 [00:05<00:06, 45.00it/s] 43%|████▎     | 233/543 [00:05<00:06, 45.00it/s] 44%|████▍     | 238/543 [00:05<00:06, 45.09it/s] 45%|████▍     | 243/543 [00:05<00:06, 45.21it/s] 46%|████▌     | 248/543 [00:05<00:06, 45.21it/s] 47%|████▋     | 253/543 [00:05<00:06, 45.41it/s] 48%|████▊     | 258/543 [00:05<00:06, 45.37it/s] 48%|████▊     | 263/543 [00:05<00:06, 45.25it/s] 49%|████▉     | 268/543 [00:05<00:06, 45.06it/s] 50%|█████     | 273/543 [00:06<00:06, 44.89it/s] 51%|█████     | 278/543 [00:06<00:05, 44.87it/s] 52%|█████▏    | 283/543 [00:06<00:05, 44.84it/s] 53%|█████▎    | 288/543 [00:06<00:05, 45.08it/s] 54%|█████▍    | 293/543 [00:06<00:05, 45.20it/s] 55%|█████▍    | 298/543 [00:06<00:05, 45.35it/s] 56%|█████▌    | 303/543 [00:06<00:05, 45.36it/s] 57%|█████▋    | 308/543 [00:06<00:05, 45.28it/s] 58%|█████▊    | 313/543 [00:06<00:05, 45.18it/s] 59%|█████▊    | 318/543 [00:07<00:04, 45.05it/s] 59%|█████▉    | 323/543 [00:07<00:04, 44.88it/s] 60%|██████    | 328/543 [00:07<00:04, 44.97it/s] 61%|██████▏   | 333/543 [00:07<00:04, 45.03it/s] 62%|██████▏   | 338/543 [00:07<00:04, 41.84it/s] 63%|██████▎   | 343/543 [00:07<00:04, 42.88it/s] 64%|██████▍   | 348/543 [00:07<00:04, 43.73it/s] 65%|██████▌   | 353/543 [00:07<00:04, 44.31it/s] 66%|██████▌   | 358/543 [00:07<00:04, 44.68it/s] 67%|██████▋   | 363/543 [00:08<00:04, 44.75it/s] 68%|██████▊   | 368/543 [00:08<00:03, 44.81it/s] 69%|██████▊   | 373/543 [00:08<00:03, 44.90it/s] 70%|██████▉   | 378/543 [00:08<00:03, 44.74it/s] 71%|███████   | 383/543 [00:08<00:03, 44.77it/s] 71%|███████▏  | 388/543 [00:08<00:03, 45.00it/s] 72%|███████▏  | 393/543 [00:08<00:03, 45.16it/s] 73%|███████▎  | 398/543 [00:08<00:03, 45.28it/s] 74%|███████▍  | 403/543 [00:08<00:03, 45.37it/s] 75%|███████▌  | 408/543 [00:09<00:02, 45.25it/s] 76%|███████▌  | 413/543 [00:09<00:02, 45.18it/s] 77%|███████▋  | 418/543 [00:09<00:02, 45.08it/s] 78%|███████▊  | 423/543 [00:09<00:02, 44.97it/s] 79%|███████▉  | 428/543 [00:09<00:02, 45.00it/s] 80%|███████▉  | 433/543 [00:09<00:02, 45.06it/s] 81%|████████  | 438/543 [00:09<00:02, 45.09it/s] 82%|████████▏ | 443/543 [00:09<00:02, 45.27it/s] 83%|████████▎ | 448/543 [00:09<00:02, 45.37it/s] 83%|████████▎ | 453/543 [00:10<00:01, 45.37it/s] 84%|████████▍ | 458/543 [00:10<00:01, 45.16it/s] 85%|████████▌ | 463/543 [00:10<00:01, 45.05it/s] 86%|████████▌ | 468/543 [00:10<00:01, 44.97it/s] 87%|████████▋ | 473/543 [00:10<00:01, 44.76it/s] 88%|████████▊ | 478/543 [00:10<00:01, 44.90it/s] 89%|████████▉ | 483/543 [00:10<00:01, 45.01it/s] 90%|████████▉ | 488/543 [00:10<00:01, 45.21it/s] 91%|█████████ | 493/543 [00:10<00:01, 45.29it/s] 92%|█████████▏| 498/543 [00:11<00:00, 45.31it/s] 93%|█████████▎| 503/543 [00:11<00:00, 45.09it/s] 94%|█████████▎| 508/543 [00:11<00:00, 45.02it/s] 94%|█████████▍| 513/543 [00:11<00:00, 44.94it/s] 95%|█████████▌| 518/543 [00:11<00:00, 44.87it/s] 96%|█████████▋| 523/543 [00:11<00:00, 45.02it/s] 97%|█████████▋| 528/543 [00:11<00:00, 45.10it/s] 98%|█████████▊| 533/543 [00:11<00:00, 45.28it/s] 99%|█████████▉| 538/543 [00:11<00:00, 45.31it/s]100%|██████████| 543/543 [00:12<00:00, 45.33it/s]100%|██████████| 543/543 [00:12<00:00, 45.09it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:25:52,668 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:52,668 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:52,668 >>   eval_loss               =     1.1607
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:52,668 >>   eval_runtime            = 0:00:12.06
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:52,668 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:52,668 >>   eval_samples_per_second =    359.966
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:52,668 >>   eval_steps_per_second   =     45.016
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:25:52,668 >>   perplexity              =      3.192
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:02,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:02,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:02,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:02,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:02,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:26:03,268 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:26:03,269 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:26:03,877 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:26:04,961 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:26:04,962 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:08,203 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:08,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:08,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:08,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:26:08,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:26:09,000 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:26:09,002 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:26:09,621 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:26:09,822 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:26:09,822 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.71it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.70it/s]Extractor Predicting: 9it [00:05,  1.75it/s]Extractor Predicting: 10it [00:05,  1.76it/s]Extractor Predicting: 11it [00:06,  1.78it/s]Extractor Predicting: 12it [00:06,  1.75it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:09,  1.54it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:10,  1.54it/s]Extractor Predicting: 19it [00:11,  1.50it/s]Extractor Predicting: 20it [00:12,  1.47it/s]Extractor Predicting: 21it [00:12,  1.50it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:14,  1.55it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:16,  1.54it/s]Extractor Predicting: 28it [00:17,  1.54it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:21,  1.56it/s]Extractor Predicting: 35it [00:21,  1.54it/s]Extractor Predicting: 36it [00:22,  1.40it/s]Extractor Predicting: 37it [00:23,  1.43it/s]Extractor Predicting: 38it [00:24,  1.45it/s]Extractor Predicting: 39it [00:24,  1.50it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.52it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.57it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.55it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:32,  1.52it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:33,  1.50it/s]Extractor Predicting: 54it [00:34,  1.54it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:35,  1.53it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.56it/s]Extractor Predicting: 59it [00:37,  1.52it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:39,  1.56it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:41,  1.60it/s]Extractor Predicting: 66it [00:42,  1.60it/s]Extractor Predicting: 67it [00:42,  1.59it/s]Extractor Predicting: 68it [00:43,  1.56it/s]Extractor Predicting: 69it [00:44,  1.58it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:45,  1.56it/s]Extractor Predicting: 72it [00:46,  1.57it/s]Extractor Predicting: 73it [00:46,  1.55it/s]Extractor Predicting: 74it [00:47,  1.53it/s]Extractor Predicting: 75it [00:48,  1.50it/s]Extractor Predicting: 76it [00:48,  1.51it/s]Extractor Predicting: 77it [00:49,  1.55it/s]Extractor Predicting: 78it [00:50,  1.54it/s]Extractor Predicting: 79it [00:50,  1.53it/s]Extractor Predicting: 80it [00:51,  1.56it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:52,  1.56it/s]Extractor Predicting: 83it [00:53,  1.57it/s]Extractor Predicting: 84it [00:53,  1.58it/s]Extractor Predicting: 85it [00:54,  1.60it/s]Extractor Predicting: 86it [00:55,  1.57it/s]Extractor Predicting: 87it [00:55,  1.57it/s]Extractor Predicting: 88it [00:56,  1.59it/s]Extractor Predicting: 89it [00:56,  1.60it/s]Extractor Predicting: 90it [00:57,  1.58it/s]Extractor Predicting: 91it [00:58,  1.54it/s]Extractor Predicting: 92it [00:58,  1.52it/s]Extractor Predicting: 93it [00:59,  1.58it/s]Extractor Predicting: 94it [01:00,  1.59it/s]Extractor Predicting: 95it [01:00,  1.58it/s]Extractor Predicting: 96it [01:01,  1.60it/s]Extractor Predicting: 97it [01:02,  1.62it/s]Extractor Predicting: 98it [01:02,  1.60it/s]Extractor Predicting: 99it [01:03,  1.57it/s]Extractor Predicting: 100it [01:04,  1.44it/s]Extractor Predicting: 101it [01:04,  1.49it/s]Extractor Predicting: 102it [01:05,  1.50it/s]Extractor Predicting: 103it [01:06,  1.52it/s]Extractor Predicting: 104it [01:06,  1.55it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:07,  1.56it/s]Extractor Predicting: 107it [01:08,  1.54it/s]Extractor Predicting: 108it [01:09,  1.54it/s]Extractor Predicting: 109it [01:09,  1.55it/s]Extractor Predicting: 110it [01:10,  1.54it/s]Extractor Predicting: 111it [01:11,  1.55it/s]Extractor Predicting: 112it [01:11,  1.58it/s]Extractor Predicting: 113it [01:12,  1.59it/s]Extractor Predicting: 114it [01:13,  1.58it/s]Extractor Predicting: 115it [01:13,  1.53it/s]Extractor Predicting: 116it [01:14,  1.53it/s]Extractor Predicting: 117it [01:15,  1.55it/s]Extractor Predicting: 118it [01:15,  1.52it/s]Extractor Predicting: 119it [01:16,  1.53it/s]Extractor Predicting: 120it [01:17,  1.52it/s]Extractor Predicting: 121it [01:17,  1.51it/s]Extractor Predicting: 122it [01:18,  1.50it/s]Extractor Predicting: 123it [01:19,  1.53it/s]Extractor Predicting: 124it [01:19,  1.55it/s]Extractor Predicting: 125it [01:20,  1.57it/s]Extractor Predicting: 126it [01:20,  1.52it/s]Extractor Predicting: 127it [01:21,  1.52it/s]Extractor Predicting: 128it [01:22,  1.54it/s]Extractor Predicting: 129it [01:22,  1.56it/s]Extractor Predicting: 130it [01:23,  1.61it/s]Extractor Predicting: 131it [01:24,  1.55it/s]Extractor Predicting: 132it [01:24,  1.56it/s]Extractor Predicting: 133it [01:25,  1.56it/s]Extractor Predicting: 134it [01:26,  1.57it/s]Extractor Predicting: 135it [01:26,  1.58it/s]Extractor Predicting: 136it [01:27,  1.58it/s]Extractor Predicting: 137it [01:27,  1.59it/s]Extractor Predicting: 138it [01:28,  1.57it/s]Extractor Predicting: 139it [01:29,  1.54it/s]Extractor Predicting: 140it [01:29,  1.57it/s]Extractor Predicting: 141it [01:30,  1.58it/s]Extractor Predicting: 142it [01:31,  1.62it/s]Extractor Predicting: 143it [01:31,  1.62it/s]Extractor Predicting: 144it [01:32,  1.62it/s]Extractor Predicting: 145it [01:32,  1.58it/s]Extractor Predicting: 146it [01:33,  1.60it/s]Extractor Predicting: 147it [01:34,  1.59it/s]Extractor Predicting: 148it [01:34,  1.60it/s]Extractor Predicting: 149it [01:35,  1.62it/s]Extractor Predicting: 150it [01:36,  1.57it/s]Extractor Predicting: 151it [01:36,  1.58it/s]Extractor Predicting: 152it [01:37,  1.58it/s]Extractor Predicting: 153it [01:38,  1.56it/s]Extractor Predicting: 154it [01:38,  1.57it/s]Extractor Predicting: 155it [01:39,  1.53it/s]Extractor Predicting: 156it [01:40,  1.52it/s]Extractor Predicting: 157it [01:40,  1.54it/s]Extractor Predicting: 158it [01:41,  1.54it/s]Extractor Predicting: 159it [01:41,  1.57it/s]Extractor Predicting: 160it [01:42,  1.60it/s]Extractor Predicting: 161it [01:43,  1.61it/s]Extractor Predicting: 162it [01:43,  1.61it/s]Extractor Predicting: 163it [01:44,  1.49it/s]Extractor Predicting: 163it [01:44,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:06,548 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:06,562 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:06,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:06,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:06,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:28:07,413 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:28:07,414 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:28:08,069 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:28:09,130 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:28:09,130 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:12,327 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:12,350 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:12,350 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:12,350 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:28:12,350 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:28:13,125 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:28:13,126 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:28:13,717 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:28:13,872 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:28:13,872 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.08708487084870849,
  "recall": 0.027176416397973285,
  "score": 0.04142531156749166,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.61it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.62it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:14,  1.57it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.60it/s]Extractor Predicting: 38it [00:23,  1.58it/s]Extractor Predicting: 39it [00:24,  1.64it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:25,  1.55it/s]Extractor Predicting: 42it [00:26,  1.54it/s]Extractor Predicting: 43it [00:26,  1.53it/s]Extractor Predicting: 44it [00:27,  1.54it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:28,  1.50it/s]Extractor Predicting: 47it [00:29,  1.48it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:30,  1.49it/s]Extractor Predicting: 50it [00:31,  1.51it/s]Extractor Predicting: 51it [00:32,  1.52it/s]Extractor Predicting: 52it [00:32,  1.51it/s]Extractor Predicting: 53it [00:33,  1.50it/s]Extractor Predicting: 54it [00:34,  1.46it/s]Extractor Predicting: 55it [00:34,  1.50it/s]Extractor Predicting: 56it [00:35,  1.55it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:36,  1.52it/s]Extractor Predicting: 59it [00:37,  1.49it/s]Extractor Predicting: 60it [00:38,  1.50it/s]Extractor Predicting: 61it [00:38,  1.53it/s]Extractor Predicting: 62it [00:39,  1.53it/s]Extractor Predicting: 63it [00:40,  1.48it/s]Extractor Predicting: 64it [00:40,  1.50it/s]Extractor Predicting: 65it [00:41,  1.50it/s]Extractor Predicting: 66it [00:42,  1.48it/s]Extractor Predicting: 67it [00:42,  1.49it/s]Extractor Predicting: 68it [00:43,  1.49it/s]Extractor Predicting: 69it [00:44,  1.49it/s]Extractor Predicting: 70it [00:44,  1.51it/s]Extractor Predicting: 71it [00:45,  1.53it/s]Extractor Predicting: 72it [00:46,  1.52it/s]Extractor Predicting: 73it [00:46,  1.51it/s]Extractor Predicting: 74it [00:47,  1.54it/s]Extractor Predicting: 75it [00:47,  1.54it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:49,  1.58it/s]Extractor Predicting: 78it [00:49,  1.57it/s]Extractor Predicting: 79it [00:50,  1.53it/s]Extractor Predicting: 80it [00:51,  1.55it/s]Extractor Predicting: 81it [00:52,  1.43it/s]Extractor Predicting: 82it [00:52,  1.48it/s]Extractor Predicting: 83it [00:53,  1.52it/s]Extractor Predicting: 84it [00:53,  1.49it/s]Extractor Predicting: 85it [00:54,  1.51it/s]Extractor Predicting: 86it [00:55,  1.53it/s]Extractor Predicting: 87it [00:55,  1.56it/s]Extractor Predicting: 88it [00:56,  1.52it/s]Extractor Predicting: 89it [00:57,  1.57it/s]Extractor Predicting: 90it [00:57,  1.58it/s]Extractor Predicting: 91it [00:58,  1.61it/s]Extractor Predicting: 92it [00:58,  1.60it/s]Extractor Predicting: 93it [00:59,  1.60it/s]Extractor Predicting: 94it [01:00,  1.59it/s]Extractor Predicting: 95it [01:00,  1.60it/s]Extractor Predicting: 96it [01:01,  1.59it/s]Extractor Predicting: 97it [01:02,  1.60it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:03,  1.55it/s]Extractor Predicting: 100it [01:04,  1.55it/s]Extractor Predicting: 101it [01:04,  1.55it/s]Extractor Predicting: 102it [01:05,  1.57it/s]Extractor Predicting: 103it [01:06,  1.54it/s]Extractor Predicting: 104it [01:06,  1.54it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:08,  1.35it/s]Extractor Predicting: 107it [01:08,  1.38it/s]Extractor Predicting: 108it [01:09,  1.44it/s]Extractor Predicting: 109it [01:10,  1.41it/s]Extractor Predicting: 110it [01:10,  1.45it/s]Extractor Predicting: 111it [01:11,  1.45it/s]Extractor Predicting: 112it [01:12,  1.45it/s]Extractor Predicting: 113it [01:13,  1.46it/s]Extractor Predicting: 114it [01:13,  1.48it/s]Extractor Predicting: 115it [01:14,  1.48it/s]Extractor Predicting: 116it [01:15,  1.46it/s]Extractor Predicting: 117it [01:15,  1.48it/s]Extractor Predicting: 118it [01:16,  1.53it/s]Extractor Predicting: 119it [01:17,  1.50it/s]Extractor Predicting: 120it [01:17,  1.55it/s]Extractor Predicting: 121it [01:18,  1.54it/s]Extractor Predicting: 122it [01:18,  1.57it/s]Extractor Predicting: 123it [01:19,  1.59it/s]Extractor Predicting: 124it [01:20,  1.61it/s]Extractor Predicting: 125it [01:20,  1.59it/s]Extractor Predicting: 126it [01:21,  1.56it/s]Extractor Predicting: 127it [01:22,  1.55it/s]Extractor Predicting: 128it [01:22,  1.54it/s]Extractor Predicting: 129it [01:23,  1.55it/s]Extractor Predicting: 130it [01:24,  1.54it/s]Extractor Predicting: 131it [01:24,  1.60it/s]Extractor Predicting: 132it [01:25,  1.57it/s]Extractor Predicting: 133it [01:25,  1.57it/s]Extractor Predicting: 134it [01:26,  1.60it/s]Extractor Predicting: 135it [01:27,  1.58it/s]Extractor Predicting: 136it [01:27,  1.58it/s]Extractor Predicting: 137it [01:28,  1.62it/s]Extractor Predicting: 138it [01:28,  1.61it/s]Extractor Predicting: 139it [01:29,  1.61it/s]Extractor Predicting: 140it [01:30,  1.59it/s]Extractor Predicting: 141it [01:30,  1.60it/s]Extractor Predicting: 142it [01:31,  1.58it/s]Extractor Predicting: 143it [01:32,  1.54it/s]Extractor Predicting: 144it [01:32,  1.56it/s]Extractor Predicting: 145it [01:33,  1.51it/s]Extractor Predicting: 146it [01:34,  1.47it/s]Extractor Predicting: 147it [01:34,  1.50it/s]Extractor Predicting: 148it [01:35,  1.50it/s]Extractor Predicting: 149it [01:36,  1.52it/s]Extractor Predicting: 150it [01:36,  1.52it/s]Extractor Predicting: 151it [01:37,  1.52it/s]Extractor Predicting: 152it [01:38,  1.52it/s]Extractor Predicting: 153it [01:38,  1.56it/s]Extractor Predicting: 154it [01:39,  1.57it/s]Extractor Predicting: 155it [01:40,  1.60it/s]Extractor Predicting: 156it [01:40,  1.56it/s]Extractor Predicting: 157it [01:41,  1.57it/s]Extractor Predicting: 158it [01:41,  1.57it/s]Extractor Predicting: 159it [01:42,  1.56it/s]Extractor Predicting: 160it [01:43,  1.59it/s]Extractor Predicting: 161it [01:43,  1.52it/s]Extractor Predicting: 162it [01:44,  1.57it/s]Extractor Predicting: 163it [01:45,  1.59it/s]Extractor Predicting: 164it [01:45,  1.57it/s]Extractor Predicting: 165it [01:46,  1.63it/s]Extractor Predicting: 166it [01:46,  1.63it/s]Extractor Predicting: 167it [01:47,  1.61it/s]Extractor Predicting: 168it [01:48,  1.67it/s]Extractor Predicting: 169it [01:48,  1.68it/s]Extractor Predicting: 170it [01:49,  1.64it/s]Extractor Predicting: 171it [01:50,  1.62it/s]Extractor Predicting: 172it [01:50,  1.62it/s]Extractor Predicting: 173it [01:51,  1.59it/s]Extractor Predicting: 174it [01:51,  1.63it/s]Extractor Predicting: 175it [01:52,  1.62it/s]Extractor Predicting: 176it [01:53,  1.59it/s]Extractor Predicting: 177it [01:53,  1.57it/s]Extractor Predicting: 178it [01:54,  1.57it/s]Extractor Predicting: 179it [01:54,  1.65it/s]Extractor Predicting: 180it [01:55,  1.62it/s]Extractor Predicting: 181it [01:56,  1.62it/s]Extractor Predicting: 182it [01:56,  1.61it/s]Extractor Predicting: 183it [01:57,  1.60it/s]Extractor Predicting: 184it [01:58,  1.58it/s]Extractor Predicting: 185it [01:58,  1.62it/s]Extractor Predicting: 186it [01:59,  1.59it/s]Extractor Predicting: 187it [02:00,  1.58it/s]Extractor Predicting: 188it [02:00,  1.57it/s]Extractor Predicting: 189it [02:01,  1.56it/s]Extractor Predicting: 190it [02:02,  1.53it/s]Extractor Predicting: 191it [02:02,  1.53it/s]Extractor Predicting: 192it [02:03,  1.53it/s]Extractor Predicting: 193it [02:03,  1.57it/s]Extractor Predicting: 194it [02:04,  1.59it/s]Extractor Predicting: 195it [02:05,  1.56it/s]Extractor Predicting: 196it [02:05,  1.58it/s]Extractor Predicting: 197it [02:06,  1.58it/s]Extractor Predicting: 198it [02:07,  1.59it/s]Extractor Predicting: 199it [02:07,  1.58it/s]Extractor Predicting: 200it [02:08,  1.54it/s]Extractor Predicting: 201it [02:09,  1.56it/s]Extractor Predicting: 202it [02:09,  1.56it/s]Extractor Predicting: 203it [02:10,  1.57it/s]Extractor Predicting: 204it [02:10,  1.58it/s]Extractor Predicting: 205it [02:11,  1.57it/s]Extractor Predicting: 206it [02:12,  1.56it/s]Extractor Predicting: 207it [02:12,  1.58it/s]Extractor Predicting: 208it [02:13,  1.56it/s]Extractor Predicting: 209it [02:14,  1.57it/s]Extractor Predicting: 210it [02:14,  1.56it/s]Extractor Predicting: 211it [02:15,  1.56it/s]Extractor Predicting: 212it [02:15,  1.58it/s]Extractor Predicting: 213it [02:16,  1.58it/s]Extractor Predicting: 214it [02:17,  1.40it/s]Extractor Predicting: 215it [02:18,  1.40it/s]Extractor Predicting: 216it [02:18,  1.45it/s]Extractor Predicting: 217it [02:19,  1.50it/s]Extractor Predicting: 218it [02:20,  1.52it/s]Extractor Predicting: 219it [02:20,  1.52it/s]Extractor Predicting: 220it [02:21,  1.54it/s]Extractor Predicting: 221it [02:22,  1.53it/s]Extractor Predicting: 222it [02:22,  1.55it/s]Extractor Predicting: 223it [02:23,  1.49it/s]Extractor Predicting: 224it [02:24,  1.49it/s]Extractor Predicting: 225it [02:24,  1.48it/s]Extractor Predicting: 226it [02:25,  1.49it/s]Extractor Predicting: 227it [02:26,  1.46it/s]Extractor Predicting: 228it [02:26,  1.42it/s]Extractor Predicting: 229it [02:27,  1.44it/s]Extractor Predicting: 230it [02:28,  1.46it/s]Extractor Predicting: 231it [02:28,  1.45it/s]Extractor Predicting: 232it [02:29,  1.46it/s]Extractor Predicting: 233it [02:30,  1.47it/s]Extractor Predicting: 234it [02:30,  1.49it/s]Extractor Predicting: 235it [02:31,  1.50it/s]Extractor Predicting: 236it [02:32,  1.51it/s]Extractor Predicting: 237it [02:32,  1.51it/s]Extractor Predicting: 238it [02:33,  1.57it/s]Extractor Predicting: 239it [02:34,  1.57it/s]Extractor Predicting: 240it [02:34,  1.63it/s]Extractor Predicting: 241it [02:35,  1.63it/s]Extractor Predicting: 242it [02:35,  1.62it/s]Extractor Predicting: 243it [02:36,  1.58it/s]Extractor Predicting: 244it [02:37,  1.59it/s]Extractor Predicting: 245it [02:37,  1.62it/s]Extractor Predicting: 246it [02:38,  1.60it/s]Extractor Predicting: 247it [02:39,  1.60it/s]Extractor Predicting: 248it [02:39,  1.61it/s]Extractor Predicting: 249it [02:40,  1.67it/s]Extractor Predicting: 250it [02:40,  1.68it/s]Extractor Predicting: 251it [02:41,  1.73it/s]Extractor Predicting: 252it [02:41,  1.75it/s]Extractor Predicting: 253it [02:42,  1.71it/s]Extractor Predicting: 254it [02:43,  1.73it/s]Extractor Predicting: 255it [02:43,  1.68it/s]Extractor Predicting: 256it [02:44,  1.70it/s]Extractor Predicting: 257it [02:44,  1.66it/s]Extractor Predicting: 258it [02:45,  1.63it/s]Extractor Predicting: 259it [02:46,  1.58it/s]Extractor Predicting: 260it [02:46,  1.65it/s]Extractor Predicting: 261it [02:47,  1.61it/s]Extractor Predicting: 262it [02:48,  1.63it/s]Extractor Predicting: 263it [02:48,  1.67it/s]Extractor Predicting: 264it [02:49,  1.67it/s]Extractor Predicting: 265it [02:49,  1.66it/s]Extractor Predicting: 266it [02:50,  1.69it/s]Extractor Predicting: 267it [02:50,  1.71it/s]Extractor Predicting: 268it [02:51,  1.70it/s]Extractor Predicting: 269it [02:52,  1.67it/s]Extractor Predicting: 270it [02:52,  1.68it/s]Extractor Predicting: 271it [02:53,  1.67it/s]Extractor Predicting: 272it [02:53,  1.64it/s]Extractor Predicting: 273it [02:54,  1.68it/s]Extractor Predicting: 274it [02:55,  1.70it/s]Extractor Predicting: 275it [02:55,  1.68it/s]Extractor Predicting: 276it [02:56,  1.66it/s]Extractor Predicting: 277it [02:56,  1.66it/s]Extractor Predicting: 278it [02:57,  1.69it/s]Extractor Predicting: 279it [02:58,  1.64it/s]Extractor Predicting: 280it [02:58,  1.70it/s]Extractor Predicting: 281it [02:59,  1.70it/s]Extractor Predicting: 282it [02:59,  1.72it/s]Extractor Predicting: 283it [03:00,  1.73it/s]Extractor Predicting: 284it [03:01,  1.72it/s]Extractor Predicting: 285it [03:01,  1.73it/s]Extractor Predicting: 286it [03:02,  1.72it/s]Extractor Predicting: 287it [03:02,  1.72it/s]Extractor Predicting: 288it [03:03,  1.67it/s]Extractor Predicting: 289it [03:04,  1.67it/s]Extractor Predicting: 290it [03:04,  1.59it/s]Extractor Predicting: 291it [03:05,  1.61it/s]Extractor Predicting: 292it [03:05,  1.65it/s]Extractor Predicting: 293it [03:06,  1.66it/s]Extractor Predicting: 294it [03:07,  1.67it/s]Extractor Predicting: 295it [03:07,  1.68it/s]Extractor Predicting: 296it [03:08,  1.60it/s]Extractor Predicting: 297it [03:08,  1.59it/s]Extractor Predicting: 298it [03:09,  1.52it/s]Extractor Predicting: 299it [03:10,  1.53it/s]Extractor Predicting: 300it [03:10,  1.55it/s]Extractor Predicting: 301it [03:11,  1.57it/s]Extractor Predicting: 302it [03:12,  1.57it/s]Extractor Predicting: 303it [03:12,  1.54it/s]Extractor Predicting: 304it [03:13,  1.54it/s]Extractor Predicting: 305it [03:14,  1.58it/s]Extractor Predicting: 306it [03:14,  1.52it/s]Extractor Predicting: 307it [03:15,  1.49it/s]Extractor Predicting: 308it [03:16,  1.50it/s]Extractor Predicting: 309it [03:16,  1.54it/s]Extractor Predicting: 310it [03:17,  1.57it/s]Extractor Predicting: 311it [03:18,  1.59it/s]Extractor Predicting: 312it [03:18,  1.57it/s]Extractor Predicting: 313it [03:19,  1.54it/s]Extractor Predicting: 314it [03:20,  1.51it/s]Extractor Predicting: 315it [03:20,  1.51it/s]Extractor Predicting: 316it [03:21,  1.51it/s]Extractor Predicting: 317it [03:22,  1.51it/s]Extractor Predicting: 318it [03:22,  1.53it/s]Extractor Predicting: 319it [03:23,  1.54it/s]Extractor Predicting: 320it [03:24,  1.54it/s]Extractor Predicting: 321it [03:24,  1.60it/s]Extractor Predicting: 322it [03:25,  1.59it/s]Extractor Predicting: 323it [03:25,  1.57it/s]Extractor Predicting: 324it [03:26,  1.55it/s]Extractor Predicting: 325it [03:27,  1.55it/s]Extractor Predicting: 326it [03:27,  1.54it/s]Extractor Predicting: 327it [03:28,  1.35it/s]Extractor Predicting: 328it [03:29,  1.40it/s]Extractor Predicting: 329it [03:30,  1.46it/s]Extractor Predicting: 330it [03:30,  1.51it/s]Extractor Predicting: 331it [03:31,  1.67it/s]Extractor Predicting: 331it [03:31,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:56,304 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:56,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:56,322 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:56,322 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:31:56,322 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:31:57,181 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:31:57,182 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:31:57,938 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:31:59,031 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:31:59,032 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:01,748 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:01,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:01,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:01,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:32:01,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:32:02,266 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:32:02,268 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:32:02,606 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:32:02,805 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:32:02,806 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.23255813953488372,
  "recall": 0.10339175387719077,
  "score": 0.14314392947542987,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.47it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.44it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:14,  1.43it/s]Extractor Predicting: 23it [00:15,  1.42it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:16,  1.43it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:21,  1.47it/s]Extractor Predicting: 32it [00:21,  1.50it/s]Extractor Predicting: 33it [00:22,  1.51it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.51it/s]Extractor Predicting: 36it [00:24,  1.52it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.49it/s]Extractor Predicting: 40it [00:26,  1.50it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:28,  1.50it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:30,  1.51it/s]Extractor Predicting: 46it [00:31,  1.40it/s]Extractor Predicting: 47it [00:31,  1.42it/s]Extractor Predicting: 48it [00:32,  1.47it/s]Extractor Predicting: 49it [00:33,  1.51it/s]Extractor Predicting: 50it [00:33,  1.53it/s]Extractor Predicting: 51it [00:34,  1.54it/s]Extractor Predicting: 52it [00:34,  1.55it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:36,  1.53it/s]Extractor Predicting: 56it [00:37,  1.59it/s]Extractor Predicting: 57it [00:38,  1.64it/s]Extractor Predicting: 58it [00:38,  1.68it/s]Extractor Predicting: 59it [00:39,  1.75it/s]Extractor Predicting: 60it [00:39,  1.83it/s]Extractor Predicting: 61it [00:40,  1.87it/s]Extractor Predicting: 62it [00:40,  1.87it/s]Extractor Predicting: 63it [00:41,  1.89it/s]Extractor Predicting: 64it [00:41,  1.87it/s]Extractor Predicting: 65it [00:42,  1.86it/s]Extractor Predicting: 66it [00:42,  1.86it/s]Extractor Predicting: 67it [00:43,  1.85it/s]Extractor Predicting: 68it [00:43,  1.85it/s]Extractor Predicting: 69it [00:44,  1.88it/s]Extractor Predicting: 70it [00:45,  1.85it/s]Extractor Predicting: 71it [00:45,  1.85it/s]Extractor Predicting: 72it [00:46,  1.88it/s]Extractor Predicting: 73it [00:46,  1.91it/s]Extractor Predicting: 74it [00:47,  1.90it/s]Extractor Predicting: 75it [00:47,  1.90it/s]Extractor Predicting: 76it [00:48,  1.88it/s]Extractor Predicting: 77it [00:48,  1.94it/s]Extractor Predicting: 78it [00:49,  1.88it/s]Extractor Predicting: 79it [00:49,  1.88it/s]Extractor Predicting: 80it [00:50,  1.84it/s]Extractor Predicting: 81it [00:50,  1.84it/s]Extractor Predicting: 82it [00:51,  1.88it/s]Extractor Predicting: 83it [00:51,  1.88it/s]Extractor Predicting: 84it [00:52,  1.89it/s]Extractor Predicting: 85it [00:52,  1.90it/s]Extractor Predicting: 86it [00:53,  1.77it/s]Extractor Predicting: 87it [00:54,  1.70it/s]Extractor Predicting: 88it [00:54,  1.67it/s]Extractor Predicting: 89it [00:55,  1.65it/s]Extractor Predicting: 90it [00:56,  1.66it/s]Extractor Predicting: 91it [00:56,  1.64it/s]Extractor Predicting: 92it [00:57,  1.62it/s]Extractor Predicting: 93it [00:57,  1.63it/s]Extractor Predicting: 94it [00:58,  1.61it/s]Extractor Predicting: 95it [00:59,  1.64it/s]Extractor Predicting: 96it [00:59,  1.65it/s]Extractor Predicting: 97it [01:00,  1.65it/s]Extractor Predicting: 98it [01:00,  1.66it/s]Extractor Predicting: 99it [01:01,  1.63it/s]Extractor Predicting: 100it [01:02,  1.58it/s]Extractor Predicting: 101it [01:02,  1.61it/s]Extractor Predicting: 102it [01:03,  1.60it/s]Extractor Predicting: 103it [01:04,  1.54it/s]Extractor Predicting: 104it [01:04,  1.53it/s]Extractor Predicting: 105it [01:05,  1.52it/s]Extractor Predicting: 106it [01:06,  1.49it/s]Extractor Predicting: 107it [01:06,  1.49it/s]Extractor Predicting: 108it [01:07,  1.42it/s]Extractor Predicting: 108it [01:07,  1.60it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5139344262295082,
  "recall": 0.10043248438250842,
  "score": 0.16802894278440306,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
