Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:17<04:05, 17.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:32<03:27, 15.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:43<02:45, 13.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:58<02:38, 14.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:14<02:28, 14.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:28<02:12, 14.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:42<01:54, 14.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:55<01:37, 13.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:11<01:26, 14.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:25<01:12, 14.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:38<00:55, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:56<00:45, 15.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:11<00:30, 15.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:24<00:14, 14.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:39<00:00, 14.72s/it]Generating: 100%|██████████| 15/15 [03:39<00:00, 14.64s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 469, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : developer .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 626, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.7825, 'errors': {'', "('London', 'work location', '', 'The New York Times ( July 18 , 1923 October 6 , 1977 ) reported that the first and only English book on the topic appeared in London during the reigns of William Shakespeare and William Shakespeare .')"}}
['Relation : composer . Context : Later in the year ( 1143 ) , he composed The Seven Kingdoms , the first play in the epic of the Old Kingdom . Head Entity : The Seven Kingdoms , Tail Entity : Robert I .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best Rap Album at the 2004 MTV Video Music Awards and had two acts on the chart . Head Entity : music , Tail Entity : Rap Album .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 590, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7319711538461539, 'errors': {'', "('James S. Cain', 'field of work', '', 'He is also known for his work with James S. Cain , and for his work on the painting of the Seven Wonders of Oz by William S. Burroughs .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 194, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 298, 'raw': 448}
{'target': 600, 'success': 322, 'raw': 480}
{'target': 600, 'success': 346, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 387, 'raw': 576}
{'target': 600, 'success': 408, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 518, 'raw': 768}
{'target': 600, 'success': 543, 'raw': 800}
{'target': 600, 'success': 563, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 606, 'raw': 896}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6763392857142857, 'errors': {'', "('John Lennon', 'occupation', '', 'The band released their debut album In Search of a Way ( 2000 ) , which featured a cover from John Lennon and Steve Dizzy .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 13757
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13857, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.42s/it]Extractor Estimating: 2it [00:17,  7.50s/it]Extractor Estimating: 3it [00:18,  4.35s/it]Extractor Estimating: 4it [00:18,  2.84s/it]Extractor Estimating: 5it [00:19,  2.02s/it]Extractor Estimating: 6it [00:19,  1.53s/it]Extractor Estimating: 7it [00:20,  1.22s/it]Extractor Estimating: 8it [00:21,  1.03s/it]Extractor Estimating: 9it [00:22,  1.07s/it]Extractor Estimating: 10it [00:22,  1.08it/s]Extractor Estimating: 11it [00:23,  1.19it/s]Extractor Estimating: 12it [00:24,  1.30it/s]Extractor Estimating: 13it [00:24,  1.34it/s]Extractor Estimating: 14it [00:25,  1.43it/s]Extractor Estimating: 15it [00:26,  1.51it/s]Extractor Estimating: 16it [00:26,  1.57it/s]Extractor Estimating: 17it [00:27,  1.57it/s]Extractor Estimating: 18it [00:27,  1.60it/s]Extractor Estimating: 19it [00:28,  1.60it/s]Extractor Estimating: 20it [00:29,  1.62it/s]Extractor Estimating: 21it [00:29,  1.68it/s]Extractor Estimating: 22it [00:30,  1.73it/s]Extractor Estimating: 23it [00:30,  1.72it/s]Extractor Estimating: 24it [00:31,  1.67it/s]Extractor Estimating: 25it [00:31,  1.70it/s]Extractor Estimating: 26it [00:32,  1.76it/s]Extractor Estimating: 27it [00:33,  1.72it/s]Extractor Estimating: 28it [00:33,  1.76it/s]Extractor Estimating: 29it [00:34,  1.75it/s]Extractor Estimating: 30it [00:35,  1.09it/s]Extractor Estimating: 31it [00:36,  1.20it/s]Extractor Estimating: 32it [00:37,  1.36it/s]Extractor Estimating: 33it [00:37,  1.44it/s]Extractor Estimating: 34it [00:38,  1.51it/s]Extractor Estimating: 35it [00:38,  1.53it/s]Extractor Estimating: 36it [00:39,  1.57it/s]Extractor Estimating: 37it [00:40,  1.61it/s]Extractor Estimating: 38it [00:40,  1.60it/s]Extractor Estimating: 39it [00:41,  1.64it/s]Extractor Estimating: 40it [00:41,  1.69it/s]Extractor Estimating: 41it [00:42,  1.66it/s]Extractor Estimating: 42it [00:42,  1.71it/s]Extractor Estimating: 43it [00:43,  1.74it/s]Extractor Estimating: 44it [00:44,  1.73it/s]Extractor Estimating: 45it [00:44,  1.76it/s]Extractor Estimating: 46it [00:45,  1.75it/s]Extractor Estimating: 47it [00:45,  1.75it/s]Extractor Estimating: 48it [00:46,  1.74it/s]Extractor Estimating: 49it [00:47,  1.70it/s]Extractor Estimating: 50it [00:47,  1.77it/s]Extractor Estimating: 51it [00:48,  1.80it/s]Extractor Estimating: 52it [00:48,  1.83it/s]Extractor Estimating: 53it [00:49,  1.86it/s]Extractor Estimating: 54it [00:49,  1.90it/s]Extractor Estimating: 55it [00:50,  1.91it/s]Extractor Estimating: 56it [00:50,  2.00it/s]Extractor Estimating: 57it [00:51,  1.97it/s]Extractor Estimating: 58it [00:51,  1.99it/s]Extractor Estimating: 59it [00:52,  2.05it/s]Extractor Estimating: 60it [00:52,  1.97it/s]Extractor Estimating: 61it [00:53,  1.94it/s]Extractor Estimating: 62it [00:53,  1.99it/s]Extractor Estimating: 63it [00:54,  2.04it/s]Extractor Estimating: 64it [00:54,  1.98it/s]Extractor Estimating: 65it [00:55,  1.92it/s]Extractor Estimating: 66it [00:55,  2.00it/s]Extractor Estimating: 67it [00:56,  2.00it/s]Extractor Estimating: 68it [00:56,  1.96it/s]Extractor Estimating: 69it [00:57,  2.00it/s]Extractor Estimating: 70it [00:57,  1.98it/s]Extractor Estimating: 71it [00:58,  1.85it/s]Extractor Estimating: 72it [00:58,  1.88it/s]Extractor Estimating: 73it [00:59,  1.94it/s]Extractor Estimating: 74it [00:59,  1.96it/s]Extractor Estimating: 75it [01:00,  1.95it/s]Extractor Estimating: 76it [01:00,  1.88it/s]Extractor Estimating: 77it [01:01,  1.74it/s]Extractor Estimating: 78it [01:02,  1.73it/s]Extractor Estimating: 79it [01:02,  1.70it/s]Extractor Estimating: 80it [01:03,  1.61it/s]Extractor Estimating: 81it [01:04,  1.61it/s]Extractor Estimating: 82it [01:04,  1.56it/s]Extractor Estimating: 83it [01:05,  1.58it/s]Extractor Estimating: 84it [01:06,  1.55it/s]Extractor Estimating: 85it [01:06,  1.43it/s]Extractor Estimating: 86it [01:07,  1.48it/s]Extractor Estimating: 87it [01:08,  1.49it/s]Extractor Estimating: 88it [01:08,  1.53it/s]Extractor Estimating: 89it [01:09,  1.60it/s]Extractor Estimating: 90it [01:09,  1.64it/s]Extractor Estimating: 91it [01:10,  1.64it/s]Extractor Estimating: 92it [01:11,  1.66it/s]Extractor Estimating: 93it [01:11,  1.68it/s]Extractor Estimating: 94it [01:12,  1.69it/s]Extractor Estimating: 95it [01:12,  1.64it/s]Extractor Estimating: 96it [01:13,  1.66it/s]Extractor Estimating: 97it [01:14,  1.64it/s]Extractor Estimating: 98it [01:14,  1.64it/s]Extractor Estimating: 99it [01:15,  1.64it/s]Extractor Estimating: 100it [01:15,  1.63it/s]Extractor Estimating: 101it [01:16,  1.62it/s]Extractor Estimating: 102it [01:17,  1.63it/s]Extractor Estimating: 103it [01:17,  1.65it/s]Extractor Estimating: 104it [01:18,  1.69it/s]Extractor Estimating: 105it [01:18,  1.68it/s]Extractor Estimating: 106it [01:19,  1.66it/s]Extractor Estimating: 107it [01:20,  1.64it/s]Extractor Estimating: 108it [01:20,  1.66it/s]Extractor Estimating: 109it [01:21,  1.61it/s]Extractor Estimating: 110it [01:22,  1.61it/s]Extractor Estimating: 111it [01:22,  1.57it/s]Extractor Estimating: 112it [01:23,  1.50it/s]Extractor Estimating: 113it [01:24,  1.53it/s]Extractor Estimating: 114it [01:24,  1.53it/s]Extractor Estimating: 115it [01:25,  1.51it/s]Extractor Estimating: 116it [01:26,  1.53it/s]Extractor Estimating: 117it [01:26,  1.53it/s]Extractor Estimating: 118it [01:27,  1.60it/s]Extractor Estimating: 119it [01:27,  1.60it/s]Extractor Estimating: 120it [01:28,  1.57it/s]Extractor Estimating: 121it [01:29,  1.59it/s]Extractor Estimating: 122it [01:29,  1.58it/s]Extractor Estimating: 123it [01:30,  1.58it/s]Extractor Estimating: 124it [01:31,  1.60it/s]Extractor Estimating: 125it [01:31,  1.63it/s]Extractor Estimating: 126it [01:32,  1.68it/s]Extractor Estimating: 127it [01:32,  1.62it/s]Extractor Estimating: 128it [01:33,  1.64it/s]Extractor Estimating: 129it [01:34,  1.63it/s]Extractor Estimating: 130it [01:34,  1.64it/s]Extractor Estimating: 131it [01:35,  1.61it/s]Extractor Estimating: 132it [01:35,  1.57it/s]Extractor Estimating: 133it [01:36,  1.55it/s]Extractor Estimating: 134it [01:37,  1.57it/s]Extractor Estimating: 135it [01:37,  1.60it/s]Extractor Estimating: 136it [01:38,  1.58it/s]Extractor Estimating: 137it [01:39,  1.21it/s]Extractor Estimating: 138it [01:40,  1.30it/s]Extractor Estimating: 139it [01:41,  1.36it/s]Extractor Estimating: 140it [01:41,  1.43it/s]Extractor Estimating: 141it [01:42,  1.50it/s]Extractor Estimating: 142it [01:42,  1.56it/s]Extractor Estimating: 143it [01:43,  1.62it/s]Extractor Estimating: 144it [01:44,  1.62it/s]Extractor Estimating: 145it [01:44,  1.66it/s]Extractor Estimating: 146it [01:45,  1.61it/s]Extractor Estimating: 147it [01:45,  1.62it/s]Extractor Estimating: 148it [01:46,  1.60it/s]Extractor Estimating: 149it [01:47,  1.66it/s]Extractor Estimating: 150it [01:47,  1.64it/s]Extractor Estimating: 151it [01:48,  1.65it/s]Extractor Estimating: 152it [01:48,  1.68it/s]Extractor Estimating: 153it [01:49,  1.70it/s]Extractor Estimating: 154it [01:49,  1.73it/s]Extractor Estimating: 155it [01:50,  1.72it/s]Extractor Estimating: 156it [01:51,  1.68it/s]Extractor Estimating: 157it [01:51,  1.67it/s]Extractor Estimating: 158it [01:52,  1.66it/s]Extractor Estimating: 159it [01:53,  1.67it/s]Extractor Estimating: 160it [01:53,  1.71it/s]Extractor Estimating: 161it [01:54,  1.61it/s]Extractor Estimating: 162it [01:54,  1.61it/s]Extractor Estimating: 163it [01:55,  1.67it/s]Extractor Estimating: 164it [01:56,  1.69it/s]Extractor Estimating: 165it [01:56,  1.73it/s]Extractor Estimating: 166it [01:57,  1.73it/s]Extractor Estimating: 167it [01:57,  1.59it/s]Extractor Estimating: 168it [01:58,  1.65it/s]Extractor Estimating: 169it [01:58,  1.71it/s]Extractor Estimating: 170it [01:59,  1.66it/s]Extractor Estimating: 171it [02:00,  1.67it/s]Extractor Estimating: 172it [02:00,  1.71it/s]Extractor Estimating: 173it [02:01,  1.73it/s]Extractor Estimating: 174it [02:01,  1.69it/s]Extractor Estimating: 175it [02:02,  1.63it/s]Extractor Estimating: 176it [02:03,  1.63it/s]Extractor Estimating: 177it [02:03,  1.65it/s]Extractor Estimating: 178it [02:04,  1.66it/s]Extractor Estimating: 179it [02:05,  1.65it/s]Extractor Estimating: 180it [02:05,  1.63it/s]Extractor Estimating: 181it [02:06,  1.64it/s]Extractor Estimating: 182it [02:06,  1.63it/s]Extractor Estimating: 183it [02:07,  1.63it/s]Extractor Estimating: 184it [02:08,  1.65it/s]Extractor Estimating: 185it [02:08,  1.63it/s]Extractor Estimating: 186it [02:09,  1.62it/s]Extractor Estimating: 187it [02:09,  1.67it/s]Extractor Estimating: 188it [02:10,  1.65it/s]Extractor Estimating: 189it [02:11,  1.64it/s]Extractor Estimating: 190it [02:11,  1.60it/s]Extractor Estimating: 191it [02:12,  1.58it/s]Extractor Estimating: 192it [02:13,  1.59it/s]Extractor Estimating: 193it [02:13,  1.56it/s]Extractor Estimating: 194it [02:14,  1.60it/s]Extractor Estimating: 195it [02:14,  1.62it/s]Extractor Estimating: 196it [02:15,  1.58it/s]Extractor Estimating: 197it [02:16,  1.59it/s]Extractor Estimating: 198it [02:16,  1.58it/s]Extractor Estimating: 199it [02:17,  1.62it/s]Extractor Estimating: 200it [02:18,  1.61it/s]Extractor Estimating: 201it [02:18,  1.60it/s]Extractor Estimating: 202it [02:19,  1.64it/s]Extractor Estimating: 203it [02:19,  1.60it/s]Extractor Estimating: 204it [02:20,  1.62it/s]Extractor Estimating: 205it [02:21,  1.56it/s]Extractor Estimating: 206it [02:21,  1.61it/s]Extractor Estimating: 207it [02:22,  1.64it/s]Extractor Estimating: 208it [02:22,  1.64it/s]Extractor Estimating: 209it [02:23,  1.65it/s]Extractor Estimating: 210it [02:24,  1.64it/s]Extractor Estimating: 211it [02:24,  1.66it/s]Extractor Estimating: 212it [02:25,  1.67it/s]Extractor Estimating: 213it [02:25,  1.68it/s]Extractor Estimating: 214it [02:26,  1.65it/s]Extractor Estimating: 215it [02:27,  1.63it/s]Extractor Estimating: 216it [02:27,  1.59it/s]Extractor Estimating: 217it [02:28,  1.61it/s]Extractor Estimating: 218it [02:29,  1.62it/s]Extractor Estimating: 219it [02:29,  1.68it/s]Extractor Estimating: 220it [02:30,  1.68it/s]Extractor Estimating: 221it [02:30,  1.67it/s]Extractor Estimating: 222it [02:31,  1.66it/s]Extractor Estimating: 223it [02:32,  1.68it/s]Extractor Estimating: 224it [02:32,  1.65it/s]Extractor Estimating: 225it [02:33,  1.64it/s]Extractor Estimating: 226it [02:33,  1.61it/s]Extractor Estimating: 227it [02:34,  1.56it/s]Extractor Estimating: 228it [02:35,  1.55it/s]Extractor Estimating: 229it [02:35,  1.57it/s]Extractor Estimating: 230it [02:36,  1.58it/s]Extractor Estimating: 231it [02:37,  1.60it/s]Extractor Estimating: 232it [02:37,  1.63it/s]Extractor Estimating: 233it [02:38,  1.66it/s]Extractor Estimating: 234it [02:38,  1.62it/s]Extractor Estimating: 235it [02:39,  1.57it/s]Extractor Estimating: 236it [02:40,  1.57it/s]Extractor Estimating: 237it [02:40,  1.57it/s]Extractor Estimating: 238it [02:41,  1.60it/s]Extractor Estimating: 239it [02:42,  1.63it/s]Extractor Estimating: 240it [02:42,  1.62it/s]Extractor Estimating: 241it [02:43,  1.60it/s]Extractor Estimating: 242it [02:44,  1.56it/s]Extractor Estimating: 243it [02:44,  1.56it/s]Extractor Estimating: 244it [02:45,  1.58it/s]Extractor Estimating: 245it [02:45,  1.57it/s]Extractor Estimating: 246it [02:46,  1.43it/s]Extractor Estimating: 247it [02:47,  1.46it/s]Extractor Estimating: 248it [02:48,  1.53it/s]Extractor Estimating: 249it [02:48,  1.43it/s]Extractor Estimating: 250it [02:49,  1.44it/s]Extractor Estimating: 251it [02:50,  1.49it/s]Extractor Estimating: 252it [02:50,  1.50it/s]Extractor Estimating: 253it [02:51,  1.53it/s]Extractor Estimating: 254it [02:51,  1.56it/s]Extractor Estimating: 255it [02:52,  1.59it/s]Extractor Estimating: 256it [02:53,  1.60it/s]Extractor Estimating: 257it [02:53,  1.64it/s]Extractor Estimating: 258it [02:54,  1.67it/s]Extractor Estimating: 259it [02:54,  1.65it/s]Extractor Estimating: 260it [02:55,  1.69it/s]Extractor Estimating: 261it [02:56,  1.64it/s]Extractor Estimating: 262it [02:56,  1.63it/s]Extractor Estimating: 263it [02:57,  1.65it/s]Extractor Estimating: 264it [02:57,  1.68it/s]Extractor Estimating: 265it [02:58,  1.67it/s]Extractor Estimating: 266it [02:59,  1.69it/s]Extractor Estimating: 267it [02:59,  1.67it/s]Extractor Estimating: 268it [03:00,  1.61it/s]Extractor Estimating: 269it [03:01,  1.64it/s]Extractor Estimating: 270it [03:01,  1.63it/s]Extractor Estimating: 271it [03:02,  1.60it/s]Extractor Estimating: 272it [03:02,  1.56it/s]Extractor Estimating: 273it [03:03,  1.61it/s]Extractor Estimating: 274it [03:04,  1.61it/s]Extractor Estimating: 275it [03:04,  1.62it/s]Extractor Estimating: 276it [03:05,  1.58it/s]Extractor Estimating: 277it [03:06,  1.61it/s]Extractor Estimating: 278it [03:06,  1.64it/s]Extractor Estimating: 279it [03:07,  1.62it/s]Extractor Estimating: 280it [03:07,  1.63it/s]Extractor Estimating: 281it [03:08,  1.63it/s]Extractor Estimating: 282it [03:09,  1.66it/s]Extractor Estimating: 283it [03:09,  1.60it/s]Extractor Estimating: 284it [03:10,  1.59it/s]Extractor Estimating: 285it [03:11,  1.59it/s]Extractor Estimating: 286it [03:11,  1.59it/s]Extractor Estimating: 287it [03:12,  1.56it/s]Extractor Estimating: 288it [03:12,  1.61it/s]Extractor Estimating: 289it [03:13,  1.58it/s]Extractor Estimating: 290it [03:14,  1.64it/s]Extractor Estimating: 291it [03:14,  1.64it/s]Extractor Estimating: 292it [03:15,  1.60it/s]Extractor Estimating: 293it [03:16,  1.59it/s]Extractor Estimating: 294it [03:16,  1.62it/s]Extractor Estimating: 295it [03:17,  1.66it/s]Extractor Estimating: 296it [03:17,  1.65it/s]Extractor Estimating: 297it [03:18,  1.68it/s]Extractor Estimating: 298it [03:18,  1.73it/s]Extractor Estimating: 299it [03:19,  1.66it/s]Extractor Estimating: 300it [03:20,  1.65it/s]Extractor Estimating: 301it [03:20,  1.63it/s]Extractor Estimating: 302it [03:21,  1.58it/s]Extractor Estimating: 303it [03:22,  1.57it/s]Extractor Estimating: 304it [03:22,  1.55it/s]Extractor Estimating: 305it [03:23,  1.55it/s]Extractor Estimating: 306it [03:24,  1.53it/s]Extractor Estimating: 307it [03:24,  1.59it/s]Extractor Estimating: 308it [03:25,  1.58it/s]Extractor Estimating: 309it [03:25,  1.57it/s]Extractor Estimating: 310it [03:26,  1.57it/s]Extractor Estimating: 311it [03:27,  1.56it/s]Extractor Estimating: 312it [03:27,  1.55it/s]Extractor Estimating: 313it [03:28,  1.57it/s]Extractor Estimating: 314it [03:29,  1.59it/s]Extractor Estimating: 315it [03:29,  1.58it/s]Extractor Estimating: 316it [03:30,  1.58it/s]Extractor Estimating: 317it [03:31,  1.60it/s]Extractor Estimating: 318it [03:31,  1.43it/s]Extractor Estimating: 319it [03:32,  1.47it/s]Extractor Estimating: 320it [03:33,  1.50it/s]Extractor Estimating: 321it [03:33,  1.49it/s]Extractor Estimating: 322it [03:34,  1.52it/s]Extractor Estimating: 323it [03:35,  1.54it/s]Extractor Estimating: 324it [03:35,  1.58it/s]Extractor Estimating: 325it [03:36,  1.59it/s]Extractor Estimating: 326it [03:36,  1.60it/s]Extractor Estimating: 327it [03:37,  1.63it/s]Extractor Estimating: 328it [03:38,  1.63it/s]Extractor Estimating: 329it [03:38,  1.66it/s]Extractor Estimating: 330it [03:39,  1.62it/s]Extractor Estimating: 331it [03:39,  1.62it/s]Extractor Estimating: 332it [03:40,  1.65it/s]Extractor Estimating: 333it [03:41,  1.65it/s]Extractor Estimating: 334it [03:41,  1.64it/s]Extractor Estimating: 335it [03:42,  1.62it/s]Extractor Estimating: 336it [03:42,  1.64it/s]Extractor Estimating: 337it [03:43,  1.55it/s]Extractor Estimating: 338it [03:44,  1.59it/s]Extractor Estimating: 339it [03:44,  1.56it/s]Extractor Estimating: 340it [03:45,  1.62it/s]Extractor Estimating: 341it [03:46,  1.61it/s]Extractor Estimating: 342it [03:46,  1.61it/s]Extractor Estimating: 343it [03:47,  1.63it/s]Extractor Estimating: 344it [03:48,  1.60it/s]Extractor Estimating: 345it [03:48,  1.57it/s]Extractor Estimating: 346it [03:49,  1.58it/s]Extractor Estimating: 347it [03:49,  1.61it/s]Extractor Estimating: 348it [03:50,  1.63it/s]Extractor Estimating: 349it [03:51,  1.66it/s]Extractor Estimating: 350it [03:51,  1.63it/s]Extractor Estimating: 351it [03:52,  1.65it/s]Extractor Estimating: 352it [03:52,  1.62it/s]Extractor Estimating: 353it [03:53,  1.65it/s]Extractor Estimating: 354it [03:54,  1.65it/s]Extractor Estimating: 355it [03:54,  1.58it/s]Extractor Estimating: 356it [03:55,  1.55it/s]Extractor Estimating: 357it [03:56,  1.56it/s]Extractor Estimating: 358it [03:56,  1.64it/s]Extractor Estimating: 359it [03:57,  1.64it/s]Extractor Estimating: 360it [03:57,  1.66it/s]Extractor Estimating: 361it [03:58,  1.68it/s]Extractor Estimating: 362it [03:59,  1.63it/s]Extractor Estimating: 363it [03:59,  1.61it/s]Extractor Estimating: 364it [04:00,  1.58it/s]Extractor Estimating: 365it [04:01,  1.60it/s]Extractor Estimating: 366it [04:01,  1.67it/s]Extractor Estimating: 367it [04:02,  1.64it/s]Extractor Estimating: 368it [04:02,  1.62it/s]Extractor Estimating: 369it [04:03,  1.63it/s]Extractor Estimating: 370it [04:04,  1.65it/s]Extractor Estimating: 371it [04:04,  1.61it/s]Extractor Estimating: 372it [04:05,  1.65it/s]Extractor Estimating: 373it [04:05,  1.65it/s]Extractor Estimating: 374it [04:06,  1.62it/s]Extractor Estimating: 375it [04:07,  1.62it/s]Extractor Estimating: 375it [04:07,  1.52it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7436 mean pseudo reward: 0.9490521987925571
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 27244
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27344, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27344, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.229, loss:2613.8354
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.959, loss:1942.4010
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.951, loss:1606.2166
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 90, avg_time 0.957, loss:1499.9484
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 190, avg_time 0.963, loss:1433.1187
>> valid entity prec:0.5674, rec:0.4748, f1:0.5170
>> valid relation prec:0.4587, rec:0.0683, f1:0.1188
>> valid relation with NER prec:0.4587, rec:0.0683, f1:0.1188
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 290, avg_time 2.551, loss:1478.1450
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 80, avg_time 0.953, loss:1314.6433
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 180, avg_time 0.946, loss:1273.0415
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 280, avg_time 0.961, loss:1251.3906
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 70, avg_time 0.957, loss:1192.8725
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5395, rec:0.6131, f1:0.5739
>> valid relation prec:0.3893, rec:0.0836, f1:0.1377
>> valid relation with NER prec:0.3893, rec:0.0836, f1:0.1377
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 170, avg_time 2.549, loss:1118.9708
g_step 1200, step 270, avg_time 0.959, loss:1119.8854
g_step 1300, step 60, avg_time 0.951, loss:1080.2648
g_step 1400, step 160, avg_time 0.962, loss:1066.1948
g_step 1500, step 260, avg_time 0.952, loss:1000.8958
>> valid entity prec:0.4497, rec:0.4776, f1:0.4632
>> valid relation prec:0.3696, rec:0.0529, f1:0.0925
>> valid relation with NER prec:0.3696, rec:0.0529, f1:0.0925
g_step 1600, step 50, avg_time 2.534, loss:1005.8593
g_step 1700, step 150, avg_time 0.960, loss:982.2573
g_step 1800, step 250, avg_time 0.954, loss:974.4230
g_step 1900, step 40, avg_time 0.964, loss:948.8022
g_step 2000, step 140, avg_time 0.952, loss:961.2244
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5541, rec:0.6291, f1:0.5892
>> valid relation prec:0.3661, rec:0.0451, f1:0.0803
>> valid relation with NER prec:0.3661, rec:0.0451, f1:0.0803
new max entity f1 on valid!
g_step 2100, step 240, avg_time 2.560, loss:946.1227
g_step 2200, step 30, avg_time 0.941, loss:884.5546
g_step 2300, step 130, avg_time 0.964, loss:885.6921
g_step 2400, step 230, avg_time 0.961, loss:912.3431
g_step 2500, step 20, avg_time 0.949, loss:879.8198
>> valid entity prec:0.6027, rec:0.5621, f1:0.5817
>> valid relation prec:0.2917, rec:0.0330, f1:0.0593
>> valid relation with NER prec:0.2917, rec:0.0330, f1:0.0593
g_step 2600, step 120, avg_time 2.540, loss:851.6525
g_step 2700, step 220, avg_time 0.952, loss:852.7324
g_step 2800, step 10, avg_time 0.955, loss:858.9208
g_step 2900, step 110, avg_time 0.951, loss:805.9678
g_step 3000, step 210, avg_time 0.964, loss:840.2677
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5615, rec:0.4646, f1:0.5085
>> valid relation prec:0.2670, rec:0.0410, f1:0.0711
>> valid relation with NER prec:0.2670, rec:0.0410, f1:0.0711
g_step 3100, step 310, avg_time 2.533, loss:833.3173
g_step 3200, step 100, avg_time 0.960, loss:771.9954
g_step 3300, step 200, avg_time 0.961, loss:796.1407
g_step 3400, step 300, avg_time 0.955, loss:808.2866
g_step 3500, step 90, avg_time 0.963, loss:732.0847
>> valid entity prec:0.6197, rec:0.4166, f1:0.4982
>> valid relation prec:0.2204, rec:0.0195, f1:0.0358
>> valid relation with NER prec:0.2204, rec:0.0195, f1:0.0358
g_step 3600, step 190, avg_time 2.531, loss:763.8724
g_step 3700, step 290, avg_time 0.946, loss:786.4545
g_step 3800, step 80, avg_time 0.955, loss:739.7891
g_step 3900, step 180, avg_time 0.966, loss:733.9053
g_step 4000, step 280, avg_time 0.956, loss:747.7253
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5569, rec:0.5371, f1:0.5468
>> valid relation prec:0.2065, rec:0.0353, f1:0.0602
>> valid relation with NER prec:0.2065, rec:0.0353, f1:0.0602
g_step 4100, step 70, avg_time 2.555, loss:725.7722
g_step 4200, step 170, avg_time 0.959, loss:725.4859
g_step 4300, step 270, avg_time 0.950, loss:717.7342
g_step 4400, step 60, avg_time 0.958, loss:673.9470
g_step 4500, step 160, avg_time 0.953, loss:697.7406
>> valid entity prec:0.5410, rec:0.4577, f1:0.4958
>> valid relation prec:0.2148, rec:0.0422, f1:0.0706
>> valid relation with NER prec:0.2148, rec:0.0422, f1:0.0706
g_step 4600, step 260, avg_time 2.550, loss:693.5912
g_step 4700, step 50, avg_time 0.965, loss:670.5252
g_step 4800, step 150, avg_time 0.954, loss:647.1710
g_step 4900, step 250, avg_time 0.965, loss:692.1467
g_step 5000, step 40, avg_time 0.951, loss:657.3338
learning rate was adjusted to 0.0008
>> valid entity prec:0.5521, rec:0.4433, f1:0.4918
>> valid relation prec:0.2181, rec:0.0406, f1:0.0684
>> valid relation with NER prec:0.2181, rec:0.0406, f1:0.0684
g_step 5100, step 140, avg_time 2.546, loss:637.6118
g_step 5200, step 240, avg_time 0.954, loss:644.7284
g_step 5300, step 30, avg_time 0.978, loss:638.3732
g_step 5400, step 130, avg_time 0.968, loss:622.9013
g_step 5500, step 230, avg_time 0.950, loss:621.5603
>> valid entity prec:0.5433, rec:0.4748, f1:0.5068
>> valid relation prec:0.1921, rec:0.0381, f1:0.0636
>> valid relation with NER prec:0.1921, rec:0.0381, f1:0.0636
g_step 5600, step 20, avg_time 2.547, loss:630.5319
g_step 5700, step 120, avg_time 0.957, loss:599.7877
g_step 5800, step 220, avg_time 0.965, loss:589.4164
g_step 5900, step 10, avg_time 0.962, loss:608.8606
g_step 6000, step 110, avg_time 0.956, loss:545.6011
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5436, rec:0.3840, f1:0.4501
>> valid relation prec:0.1424, rec:0.0277, f1:0.0463
>> valid relation with NER prec:0.1424, rec:0.0277, f1:0.0463
g_step 6100, step 210, avg_time 2.538, loss:602.9075
g_step 6200, step 310, avg_time 0.965, loss:599.1042
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 23:17:08 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 23:17:08 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_23-17-08_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 23:17:09 - WARNING - datasets.builder -   Using custom data configuration default-f4cb1dbc195081c0
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f4cb1dbc195081c0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 23:17:09,988 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:17:09,989 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 23:17:09,989 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:17:09,990 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 23:17:10,002 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:17:10,010 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:17:10,010 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:17:10,010 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:17:10,010 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:17:10,010 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:17:10,010 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 23:17:10,174 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 23:17:13,244 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 23:17:13,247 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f4cb1dbc195081c0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 23:17:13 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x148fafabe560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.04ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.87ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.17ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.33ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.41ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.48ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.51ba/s]100%|██████████| 8/8 [00:01<00:00,  5.20ba/s]100%|██████████| 8/8 [00:01<00:00,  4.55ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.14ba/s] 40%|████      | 2/5 [00:00<00:00,  4.35ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.44ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.46ba/s]100%|██████████| 5/5 [00:01<00:00,  4.67ba/s]100%|██████████| 5/5 [00:01<00:00,  4.54ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.79ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.69ba/s] 50%|█████     | 4/8 [00:00<00:00,  6.78ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  8.15ba/s]100%|██████████| 8/8 [00:00<00:00,  9.64ba/s]100%|██████████| 8/8 [00:00<00:00,  8.95ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.87ba/s] 40%|████      | 2/5 [00:00<00:00,  9.46ba/s] 80%|████████  | 4/5 [00:00<00:00,  9.97ba/s]100%|██████████| 5/5 [00:00<00:00, 10.11ba/s]
[INFO|trainer.py:414] 2023-08-27 23:17:17,857 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 23:17:17,871 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 23:17:17,871 >>   Num examples = 7574
[INFO|trainer.py:1149] 2023-08-27 23:17:17,871 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 23:17:17,871 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 23:17:17,871 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 23:17:17,871 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 23:17:17,871 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:58,  3.30it/s]  0%|          | 2/590 [00:00<02:53,  3.38it/s]  1%|          | 3/590 [00:00<02:52,  3.41it/s]  1%|          | 4/590 [00:01<02:51,  3.42it/s]  1%|          | 5/590 [00:01<02:50,  3.43it/s]  1%|          | 6/590 [00:01<02:50,  3.42it/s]  1%|          | 7/590 [00:02<02:50,  3.42it/s]  1%|▏         | 8/590 [00:02<02:50,  3.41it/s]  2%|▏         | 9/590 [00:02<02:50,  3.41it/s]  2%|▏         | 10/590 [00:02<02:50,  3.40it/s]  2%|▏         | 11/590 [00:03<02:50,  3.39it/s]  2%|▏         | 12/590 [00:03<02:50,  3.39it/s]  2%|▏         | 13/590 [00:03<02:50,  3.39it/s]  2%|▏         | 14/590 [00:04<02:49,  3.39it/s]  3%|▎         | 15/590 [00:04<02:49,  3.39it/s]  3%|▎         | 16/590 [00:04<02:49,  3.39it/s]  3%|▎         | 17/590 [00:05<02:49,  3.39it/s]  3%|▎         | 18/590 [00:05<02:48,  3.39it/s]  3%|▎         | 19/590 [00:05<02:48,  3.39it/s]  3%|▎         | 20/590 [00:05<02:48,  3.39it/s]  4%|▎         | 21/590 [00:06<02:47,  3.39it/s]  4%|▎         | 22/590 [00:06<02:47,  3.39it/s]  4%|▍         | 23/590 [00:06<02:47,  3.39it/s]  4%|▍         | 24/590 [00:07<02:46,  3.39it/s]  4%|▍         | 25/590 [00:07<02:46,  3.39it/s]  4%|▍         | 26/590 [00:07<02:46,  3.39it/s]  5%|▍         | 27/590 [00:07<02:46,  3.39it/s]  5%|▍         | 28/590 [00:08<02:45,  3.39it/s]  5%|▍         | 29/590 [00:08<02:45,  3.39it/s]  5%|▌         | 30/590 [00:08<02:45,  3.39it/s]  5%|▌         | 31/590 [00:09<02:44,  3.39it/s]  5%|▌         | 32/590 [00:09<02:44,  3.39it/s]  6%|▌         | 33/590 [00:09<02:44,  3.39it/s]  6%|▌         | 34/590 [00:10<02:44,  3.39it/s]  6%|▌         | 35/590 [00:10<02:43,  3.39it/s]  6%|▌         | 36/590 [00:10<02:43,  3.39it/s]  6%|▋         | 37/590 [00:10<02:43,  3.39it/s]  6%|▋         | 38/590 [00:11<02:42,  3.39it/s]  7%|▋         | 39/590 [00:11<02:42,  3.39it/s]  7%|▋         | 40/590 [00:11<02:42,  3.38it/s]  7%|▋         | 41/590 [00:12<02:42,  3.39it/s]  7%|▋         | 42/590 [00:12<02:41,  3.39it/s]  7%|▋         | 43/590 [00:12<02:41,  3.39it/s]  7%|▋         | 44/590 [00:12<02:41,  3.39it/s]  8%|▊         | 45/590 [00:13<02:40,  3.39it/s]  8%|▊         | 46/590 [00:13<02:41,  3.38it/s]  8%|▊         | 47/590 [00:13<02:41,  3.36it/s]  8%|▊         | 48/590 [00:14<02:44,  3.30it/s]  8%|▊         | 49/590 [00:14<02:42,  3.33it/s]  8%|▊         | 50/590 [00:14<02:41,  3.34it/s]  9%|▊         | 51/590 [00:15<02:40,  3.35it/s]  9%|▉         | 52/590 [00:15<02:39,  3.36it/s]  9%|▉         | 53/590 [00:15<02:39,  3.37it/s]  9%|▉         | 54/590 [00:15<02:38,  3.38it/s]  9%|▉         | 55/590 [00:16<02:38,  3.38it/s]  9%|▉         | 56/590 [00:16<02:37,  3.38it/s] 10%|▉         | 57/590 [00:16<02:37,  3.38it/s] 10%|▉         | 58/590 [00:17<02:37,  3.38it/s] 10%|█         | 59/590 [00:17<02:36,  3.38it/s] 10%|█         | 60/590 [00:17<02:36,  3.38it/s] 10%|█         | 61/590 [00:18<02:36,  3.39it/s] 11%|█         | 62/590 [00:18<02:35,  3.39it/s] 11%|█         | 63/590 [00:18<02:35,  3.39it/s] 11%|█         | 64/590 [00:18<02:35,  3.38it/s] 11%|█         | 65/590 [00:19<02:35,  3.38it/s] 11%|█         | 66/590 [00:19<02:34,  3.39it/s] 11%|█▏        | 67/590 [00:19<02:34,  3.39it/s] 12%|█▏        | 68/590 [00:20<02:33,  3.39it/s] 12%|█▏        | 69/590 [00:20<02:32,  3.41it/s] 12%|█▏        | 70/590 [00:20<02:32,  3.41it/s] 12%|█▏        | 71/590 [00:20<02:31,  3.42it/s] 12%|█▏        | 72/590 [00:21<02:31,  3.43it/s] 12%|█▏        | 73/590 [00:21<02:31,  3.41it/s] 13%|█▎        | 74/590 [00:21<02:30,  3.42it/s] 13%|█▎        | 75/590 [00:22<02:30,  3.43it/s] 13%|█▎        | 76/590 [00:22<02:29,  3.43it/s] 13%|█▎        | 77/590 [00:22<02:29,  3.43it/s] 13%|█▎        | 78/590 [00:23<02:29,  3.43it/s] 13%|█▎        | 79/590 [00:23<02:28,  3.43it/s] 14%|█▎        | 80/590 [00:23<02:28,  3.43it/s] 14%|█▎        | 81/590 [00:23<02:28,  3.43it/s] 14%|█▍        | 82/590 [00:24<02:27,  3.43it/s] 14%|█▍        | 83/590 [00:24<02:27,  3.43it/s] 14%|█▍        | 84/590 [00:24<02:27,  3.44it/s] 14%|█▍        | 85/590 [00:25<02:26,  3.44it/s] 15%|█▍        | 86/590 [00:25<02:26,  3.44it/s] 15%|█▍        | 87/590 [00:25<02:26,  3.44it/s] 15%|█▍        | 88/590 [00:25<02:26,  3.44it/s] 15%|█▌        | 89/590 [00:26<02:25,  3.43it/s] 15%|█▌        | 90/590 [00:26<02:25,  3.44it/s] 15%|█▌        | 91/590 [00:26<02:25,  3.43it/s] 16%|█▌        | 92/590 [00:27<02:25,  3.43it/s] 16%|█▌        | 93/590 [00:27<02:24,  3.43it/s] 16%|█▌        | 94/590 [00:27<02:24,  3.43it/s] 16%|█▌        | 95/590 [00:27<02:24,  3.43it/s] 16%|█▋        | 96/590 [00:28<02:23,  3.43it/s] 16%|█▋        | 97/590 [00:28<02:23,  3.44it/s] 17%|█▋        | 98/590 [00:28<02:23,  3.43it/s] 17%|█▋        | 99/590 [00:29<02:22,  3.43it/s] 17%|█▋        | 100/590 [00:29<02:22,  3.43it/s] 17%|█▋        | 101/590 [00:29<02:22,  3.43it/s] 17%|█▋        | 102/590 [00:29<02:22,  3.43it/s] 17%|█▋        | 103/590 [00:30<02:21,  3.43it/s] 18%|█▊        | 104/590 [00:30<02:21,  3.43it/s] 18%|█▊        | 105/590 [00:30<02:21,  3.43it/s] 18%|█▊        | 106/590 [00:31<02:20,  3.44it/s] 18%|█▊        | 107/590 [00:31<02:20,  3.43it/s] 18%|█▊        | 108/590 [00:31<02:20,  3.43it/s] 18%|█▊        | 109/590 [00:32<02:20,  3.43it/s] 19%|█▊        | 110/590 [00:32<02:19,  3.43it/s] 19%|█▉        | 111/590 [00:32<02:19,  3.43it/s] 19%|█▉        | 112/590 [00:32<02:19,  3.43it/s] 19%|█▉        | 113/590 [00:33<02:18,  3.43it/s] 19%|█▉        | 114/590 [00:33<02:18,  3.43it/s] 19%|█▉        | 115/590 [00:33<02:18,  3.43it/s] 20%|█▉        | 116/590 [00:34<02:18,  3.43it/s] 20%|█▉        | 117/590 [00:34<02:17,  3.43it/s] 20%|██        | 118/590 [00:34<02:17,  3.43it/s][INFO|trainer.py:2140] 2023-08-27 23:17:52,564 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:17:52,564 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-27 23:17:52,564 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.08it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.66it/s][A
  3%|▎         | 17/611 [00:00<00:12, 45.99it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.32it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.73it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.48it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.28it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.28it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.35it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.30it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.37it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.29it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.08it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.06it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.89it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.01it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.14it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.26it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.21it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.18it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.20it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 43.99it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.01it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.98it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.06it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.17it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.25it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.26it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.34it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.24it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.08it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.01it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.02it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.04it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.17it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.33it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.28it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.28it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.16it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.06it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.04it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.95it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.84it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.10it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.26it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.25it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.25it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.18it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.10it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.03it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.01it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.00it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.08it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.26it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.32it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.28it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.15it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.10it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.03it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.04it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.04it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.11it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.16it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.29it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.27it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.19it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.90it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.06it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.08it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.22it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.23it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.18it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.13it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.21it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.13it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.07it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.95it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.08it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.28it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.22it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.19it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.27it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.13it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.08it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.05it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.04it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.07it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.12it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.23it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 43.92it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.32it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.21it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.12it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.08it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.07it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.09it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.19it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.19it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.19it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.20it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.19it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.15it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.13it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.10it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.10it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.23it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.25it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.14it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.16it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.15it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.16it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.16it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.05it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.02it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.26it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.17it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.17it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.00it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.16it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.18it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:48<02:17,  3.43it/s]
100%|██████████| 611/611 [00:13<00:00, 44.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:18:06,491 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-27 23:18:06,516 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:18:08,263 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:18:08,278 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:18:08,288 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:54<47:13,  6.02s/it] 20%|██        | 120/590 [00:54<33:41,  4.30s/it] 21%|██        | 121/590 [00:54<24:13,  3.10s/it] 21%|██        | 122/590 [00:54<17:36,  2.26s/it] 21%|██        | 123/590 [00:55<12:59,  1.67s/it] 21%|██        | 124/590 [00:55<09:45,  1.26s/it] 21%|██        | 125/590 [00:55<07:29,  1.03it/s] 21%|██▏       | 126/590 [00:56<05:54,  1.31it/s] 22%|██▏       | 127/590 [00:56<04:48,  1.61it/s] 22%|██▏       | 128/590 [00:56<04:01,  1.91it/s] 22%|██▏       | 129/590 [00:56<03:29,  2.20it/s] 22%|██▏       | 130/590 [00:57<03:06,  2.47it/s] 22%|██▏       | 131/590 [00:57<02:50,  2.69it/s] 22%|██▏       | 132/590 [00:57<02:39,  2.88it/s] 23%|██▎       | 133/590 [00:58<02:31,  3.02it/s] 23%|██▎       | 134/590 [00:58<02:25,  3.14it/s] 23%|██▎       | 135/590 [00:58<02:21,  3.22it/s] 23%|██▎       | 136/590 [00:59<02:18,  3.28it/s] 23%|██▎       | 137/590 [00:59<02:16,  3.32it/s] 23%|██▎       | 138/590 [00:59<02:14,  3.36it/s] 24%|██▎       | 139/590 [00:59<02:14,  3.36it/s] 24%|██▎       | 140/590 [01:00<02:13,  3.38it/s] 24%|██▍       | 141/590 [01:00<02:12,  3.40it/s] 24%|██▍       | 142/590 [01:00<02:11,  3.41it/s] 24%|██▍       | 143/590 [01:01<02:10,  3.42it/s] 24%|██▍       | 144/590 [01:01<02:10,  3.42it/s] 25%|██▍       | 145/590 [01:01<02:10,  3.42it/s] 25%|██▍       | 146/590 [01:01<02:09,  3.43it/s] 25%|██▍       | 147/590 [01:02<02:09,  3.43it/s] 25%|██▌       | 148/590 [01:02<02:08,  3.43it/s] 25%|██▌       | 149/590 [01:02<02:08,  3.43it/s] 25%|██▌       | 150/590 [01:03<02:09,  3.41it/s] 26%|██▌       | 151/590 [01:03<02:08,  3.41it/s] 26%|██▌       | 152/590 [01:03<02:08,  3.42it/s] 26%|██▌       | 153/590 [01:03<02:07,  3.42it/s] 26%|██▌       | 154/590 [01:04<02:07,  3.43it/s] 26%|██▋       | 155/590 [01:04<02:07,  3.42it/s] 26%|██▋       | 156/590 [01:04<02:06,  3.43it/s] 27%|██▋       | 157/590 [01:05<02:06,  3.43it/s] 27%|██▋       | 158/590 [01:05<02:05,  3.43it/s] 27%|██▋       | 159/590 [01:05<02:05,  3.43it/s] 27%|██▋       | 160/590 [01:06<02:05,  3.43it/s] 27%|██▋       | 161/590 [01:06<02:05,  3.42it/s] 27%|██▋       | 162/590 [01:06<02:05,  3.42it/s] 28%|██▊       | 163/590 [01:06<02:04,  3.42it/s] 28%|██▊       | 164/590 [01:07<02:04,  3.43it/s] 28%|██▊       | 165/590 [01:07<02:03,  3.43it/s] 28%|██▊       | 166/590 [01:07<02:03,  3.43it/s] 28%|██▊       | 167/590 [01:08<02:03,  3.43it/s] 28%|██▊       | 168/590 [01:08<02:02,  3.43it/s] 29%|██▊       | 169/590 [01:08<02:02,  3.43it/s] 29%|██▉       | 170/590 [01:08<02:02,  3.43it/s] 29%|██▉       | 171/590 [01:09<02:02,  3.43it/s] 29%|██▉       | 172/590 [01:09<02:02,  3.42it/s] 29%|██▉       | 173/590 [01:09<02:01,  3.42it/s] 29%|██▉       | 174/590 [01:10<02:01,  3.42it/s] 30%|██▉       | 175/590 [01:10<02:01,  3.43it/s] 30%|██▉       | 176/590 [01:10<02:00,  3.43it/s] 30%|███       | 177/590 [01:10<02:00,  3.43it/s] 30%|███       | 178/590 [01:11<02:00,  3.43it/s] 30%|███       | 179/590 [01:11<01:59,  3.43it/s] 31%|███       | 180/590 [01:11<01:59,  3.43it/s] 31%|███       | 181/590 [01:12<01:59,  3.43it/s] 31%|███       | 182/590 [01:12<01:58,  3.43it/s] 31%|███       | 183/590 [01:12<01:58,  3.42it/s] 31%|███       | 184/590 [01:13<01:58,  3.42it/s] 31%|███▏      | 185/590 [01:13<01:58,  3.43it/s] 32%|███▏      | 186/590 [01:13<01:57,  3.43it/s] 32%|███▏      | 187/590 [01:13<01:57,  3.43it/s] 32%|███▏      | 188/590 [01:14<01:59,  3.36it/s] 32%|███▏      | 189/590 [01:14<01:58,  3.38it/s] 32%|███▏      | 190/590 [01:14<01:57,  3.40it/s] 32%|███▏      | 191/590 [01:15<01:57,  3.41it/s] 33%|███▎      | 192/590 [01:15<01:56,  3.41it/s] 33%|███▎      | 193/590 [01:15<01:56,  3.42it/s] 33%|███▎      | 194/590 [01:15<01:56,  3.41it/s] 33%|███▎      | 195/590 [01:16<01:55,  3.42it/s] 33%|███▎      | 196/590 [01:16<01:55,  3.42it/s] 33%|███▎      | 197/590 [01:16<01:54,  3.42it/s] 34%|███▎      | 198/590 [01:17<01:54,  3.43it/s] 34%|███▎      | 199/590 [01:17<01:54,  3.43it/s] 34%|███▍      | 200/590 [01:17<01:53,  3.43it/s] 34%|███▍      | 201/590 [01:17<01:53,  3.43it/s] 34%|███▍      | 202/590 [01:18<01:53,  3.43it/s] 34%|███▍      | 203/590 [01:18<01:52,  3.43it/s] 35%|███▍      | 204/590 [01:18<01:52,  3.43it/s] 35%|███▍      | 205/590 [01:19<01:52,  3.42it/s] 35%|███▍      | 206/590 [01:19<01:52,  3.42it/s] 35%|███▌      | 207/590 [01:19<01:51,  3.43it/s] 35%|███▌      | 208/590 [01:20<01:51,  3.43it/s] 35%|███▌      | 209/590 [01:20<01:51,  3.43it/s] 36%|███▌      | 210/590 [01:20<01:50,  3.43it/s] 36%|███▌      | 211/590 [01:20<01:50,  3.43it/s] 36%|███▌      | 212/590 [01:21<01:50,  3.43it/s] 36%|███▌      | 213/590 [01:21<01:49,  3.43it/s] 36%|███▋      | 214/590 [01:21<01:49,  3.43it/s] 36%|███▋      | 215/590 [01:22<01:49,  3.43it/s] 37%|███▋      | 216/590 [01:22<01:49,  3.42it/s] 37%|███▋      | 217/590 [01:22<01:49,  3.42it/s] 37%|███▋      | 218/590 [01:22<01:48,  3.42it/s] 37%|███▋      | 219/590 [01:23<01:48,  3.42it/s] 37%|███▋      | 220/590 [01:23<01:48,  3.42it/s] 37%|███▋      | 221/590 [01:23<01:47,  3.42it/s] 38%|███▊      | 222/590 [01:24<01:47,  3.42it/s] 38%|███▊      | 223/590 [01:24<01:47,  3.43it/s] 38%|███▊      | 224/590 [01:24<01:46,  3.43it/s] 38%|███▊      | 225/590 [01:24<01:46,  3.43it/s] 38%|███▊      | 226/590 [01:25<01:46,  3.43it/s] 38%|███▊      | 227/590 [01:25<01:46,  3.41it/s] 39%|███▊      | 228/590 [01:25<01:46,  3.41it/s] 39%|███▉      | 229/590 [01:26<01:45,  3.41it/s] 39%|███▉      | 230/590 [01:26<01:45,  3.42it/s] 39%|███▉      | 231/590 [01:26<01:44,  3.42it/s] 39%|███▉      | 232/590 [01:27<01:44,  3.42it/s] 39%|███▉      | 233/590 [01:27<01:44,  3.42it/s] 40%|███▉      | 234/590 [01:27<01:44,  3.42it/s] 40%|███▉      | 235/590 [01:27<01:43,  3.42it/s] 40%|████      | 236/590 [01:28<01:43,  3.42it/s][INFO|trainer.py:2140] 2023-08-27 23:18:46,128 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:18:46,128 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-27 23:18:46,128 >>   Batch size = 8
{'eval_loss': 0.899056613445282, 'eval_runtime': 13.8873, 'eval_samples_per_second': 351.545, 'eval_steps_per_second': 43.997, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:11, 54.92it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.58it/s][A
  3%|▎         | 17/611 [00:00<00:12, 45.85it/s][A
  4%|▎         | 22/611 [00:00<00:13, 44.86it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.36it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.39it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.20it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.07it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.16it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.30it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.15it/s][A
 10%|█         | 62/611 [00:01<00:12, 43.97it/s][A
 11%|█         | 67/611 [00:01<00:12, 43.77it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 43.78it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.75it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.80it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.90it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.12it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.22it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.03it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 43.97it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 43.78it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.81it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.82it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.90it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.93it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.05it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.02it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.03it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 43.97it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 43.95it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 43.95it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.80it/s][A
 28%|██▊       | 172/611 [00:03<00:10, 43.88it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 44.01it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.07it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.11it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.05it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 43.92it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.89it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.77it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.85it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.82it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.02it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.08it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.03it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 43.58it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 43.95it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.00it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.81it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.70it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.04it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.11it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.07it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 43.95it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.01it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 43.88it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.82it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.94it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.03it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.08it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.11it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.11it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.02it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 43.94it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.88it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.86it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.91it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.84it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.02it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.07it/s][A
 61%|██████    | 372/611 [00:08<00:05, 43.99it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 43.90it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 43.85it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.91it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.98it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 44.05it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.10it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.09it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.02it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 43.91it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 43.86it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.86it/s][A
 71%|███████   | 432/611 [00:09<00:04, 43.99it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.92it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.07it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.07it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.07it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.01it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 43.92it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.93it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.91it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.97it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.01it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 43.93it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.00it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.04it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 43.98it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 43.96it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.88it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.83it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.97it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.99it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.06it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.04it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.05it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 43.97it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 43.86it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.83it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.93it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.06it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 43.98it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.01it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.00it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.03it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 43.98it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 43.89it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.76it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 43.87it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:42<01:43,  3.42it/s]
100%|██████████| 611/611 [00:13<00:00, 43.87it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:19:00,090 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-27 23:19:00,117 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:19:04,340 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:19:04,356 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:19:04,368 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:50<40:32,  6.89s/it] 40%|████      | 238/590 [01:50<28:50,  4.92s/it] 41%|████      | 239/590 [01:51<20:39,  3.53s/it] 41%|████      | 240/590 [01:51<14:56,  2.56s/it] 41%|████      | 241/590 [01:51<10:56,  1.88s/it] 41%|████      | 242/590 [01:51<08:08,  1.40s/it] 41%|████      | 243/590 [01:52<06:11,  1.07s/it] 41%|████▏     | 244/590 [01:52<04:50,  1.19it/s] 42%|████▏     | 245/590 [01:52<03:53,  1.48it/s] 42%|████▏     | 246/590 [01:53<03:13,  1.78it/s] 42%|████▏     | 247/590 [01:53<02:45,  2.08it/s] 42%|████▏     | 248/590 [01:53<02:25,  2.35it/s] 42%|████▏     | 249/590 [01:54<02:12,  2.58it/s] 42%|████▏     | 250/590 [01:54<02:02,  2.78it/s] 43%|████▎     | 251/590 [01:54<01:55,  2.94it/s] 43%|████▎     | 252/590 [01:54<01:50,  3.06it/s] 43%|████▎     | 253/590 [01:55<01:46,  3.15it/s] 43%|████▎     | 254/590 [01:55<01:44,  3.22it/s] 43%|████▎     | 255/590 [01:55<01:42,  3.26it/s] 43%|████▎     | 256/590 [01:56<01:41,  3.30it/s] 44%|████▎     | 257/590 [01:56<01:40,  3.33it/s] 44%|████▎     | 258/590 [01:56<01:39,  3.34it/s] 44%|████▍     | 259/590 [01:57<01:38,  3.35it/s] 44%|████▍     | 260/590 [01:57<01:38,  3.35it/s] 44%|████▍     | 261/590 [01:57<01:37,  3.36it/s] 44%|████▍     | 262/590 [01:57<01:37,  3.37it/s] 45%|████▍     | 263/590 [01:58<01:36,  3.37it/s] 45%|████▍     | 264/590 [01:58<01:36,  3.38it/s] 45%|████▍     | 265/590 [01:58<01:36,  3.38it/s] 45%|████▌     | 266/590 [01:59<01:35,  3.38it/s] 45%|████▌     | 267/590 [01:59<01:35,  3.38it/s] 45%|████▌     | 268/590 [01:59<01:35,  3.38it/s] 46%|████▌     | 269/590 [01:59<01:34,  3.38it/s] 46%|████▌     | 270/590 [02:00<01:34,  3.38it/s] 46%|████▌     | 271/590 [02:00<01:34,  3.38it/s] 46%|████▌     | 272/590 [02:00<01:34,  3.38it/s] 46%|████▋     | 273/590 [02:01<01:33,  3.38it/s] 46%|████▋     | 274/590 [02:01<01:33,  3.38it/s] 47%|████▋     | 275/590 [02:01<01:33,  3.38it/s] 47%|████▋     | 276/590 [02:02<01:32,  3.38it/s] 47%|████▋     | 277/590 [02:02<01:32,  3.38it/s] 47%|████▋     | 278/590 [02:02<01:32,  3.38it/s] 47%|████▋     | 279/590 [02:02<01:31,  3.38it/s] 47%|████▋     | 280/590 [02:03<01:31,  3.38it/s] 48%|████▊     | 281/590 [02:03<01:31,  3.38it/s] 48%|████▊     | 282/590 [02:03<01:31,  3.38it/s] 48%|████▊     | 283/590 [02:04<01:30,  3.38it/s] 48%|████▊     | 284/590 [02:04<01:30,  3.38it/s] 48%|████▊     | 285/590 [02:04<01:30,  3.38it/s] 48%|████▊     | 286/590 [02:05<01:29,  3.38it/s] 49%|████▊     | 287/590 [02:05<01:29,  3.38it/s] 49%|████▉     | 288/590 [02:05<01:29,  3.38it/s] 49%|████▉     | 289/590 [02:05<01:29,  3.38it/s] 49%|████▉     | 290/590 [02:06<01:28,  3.38it/s] 49%|████▉     | 291/590 [02:06<01:28,  3.39it/s] 49%|████▉     | 292/590 [02:06<01:28,  3.38it/s] 50%|████▉     | 293/590 [02:07<01:27,  3.38it/s] 50%|████▉     | 294/590 [02:07<01:27,  3.38it/s] 50%|█████     | 295/590 [02:07<01:27,  3.38it/s] 50%|█████     | 296/590 [02:07<01:26,  3.39it/s] 50%|█████     | 297/590 [02:08<01:26,  3.38it/s] 51%|█████     | 298/590 [02:08<01:26,  3.38it/s] 51%|█████     | 299/590 [02:08<01:26,  3.38it/s] 51%|█████     | 300/590 [02:09<01:25,  3.38it/s] 51%|█████     | 301/590 [02:09<01:25,  3.38it/s] 51%|█████     | 302/590 [02:09<01:25,  3.38it/s] 51%|█████▏    | 303/590 [02:10<01:24,  3.38it/s] 52%|█████▏    | 304/590 [02:10<01:24,  3.37it/s] 52%|█████▏    | 305/590 [02:10<01:24,  3.38it/s] 52%|█████▏    | 306/590 [02:10<01:24,  3.38it/s] 52%|█████▏    | 307/590 [02:11<01:23,  3.38it/s] 52%|█████▏    | 308/590 [02:11<01:23,  3.38it/s] 52%|█████▏    | 309/590 [02:11<01:23,  3.38it/s] 53%|█████▎    | 310/590 [02:12<01:22,  3.38it/s] 53%|█████▎    | 311/590 [02:12<01:22,  3.38it/s] 53%|█████▎    | 312/590 [02:12<01:22,  3.38it/s] 53%|█████▎    | 313/590 [02:12<01:21,  3.38it/s] 53%|█████▎    | 314/590 [02:13<01:21,  3.38it/s] 53%|█████▎    | 315/590 [02:13<01:21,  3.39it/s] 54%|█████▎    | 316/590 [02:13<01:20,  3.38it/s] 54%|█████▎    | 317/590 [02:14<01:20,  3.38it/s] 54%|█████▍    | 318/590 [02:14<01:20,  3.38it/s] 54%|█████▍    | 319/590 [02:14<01:20,  3.39it/s] 54%|█████▍    | 320/590 [02:15<01:19,  3.39it/s] 54%|█████▍    | 321/590 [02:15<01:19,  3.38it/s] 55%|█████▍    | 322/590 [02:15<01:19,  3.38it/s] 55%|█████▍    | 323/590 [02:15<01:18,  3.38it/s] 55%|█████▍    | 324/590 [02:16<01:18,  3.39it/s] 55%|█████▌    | 325/590 [02:16<01:18,  3.38it/s] 55%|█████▌    | 326/590 [02:16<01:18,  3.38it/s] 55%|█████▌    | 327/590 [02:17<01:17,  3.38it/s] 56%|█████▌    | 328/590 [02:17<01:17,  3.38it/s] 56%|█████▌    | 329/590 [02:17<01:17,  3.38it/s] 56%|█████▌    | 330/590 [02:18<01:16,  3.38it/s] 56%|█████▌    | 331/590 [02:18<01:16,  3.38it/s] 56%|█████▋    | 332/590 [02:18<01:16,  3.38it/s] 56%|█████▋    | 333/590 [02:18<01:15,  3.38it/s] 57%|█████▋    | 334/590 [02:19<01:15,  3.38it/s] 57%|█████▋    | 335/590 [02:19<01:15,  3.38it/s] 57%|█████▋    | 336/590 [02:19<01:15,  3.37it/s] 57%|█████▋    | 337/590 [02:20<01:14,  3.37it/s] 57%|█████▋    | 338/590 [02:20<01:14,  3.38it/s] 57%|█████▋    | 339/590 [02:20<01:14,  3.38it/s] 58%|█████▊    | 340/590 [02:20<01:13,  3.38it/s] 58%|█████▊    | 341/590 [02:21<01:13,  3.38it/s] 58%|█████▊    | 342/590 [02:21<01:13,  3.38it/s] 58%|█████▊    | 343/590 [02:21<01:13,  3.38it/s] 58%|█████▊    | 344/590 [02:22<01:12,  3.38it/s] 58%|█████▊    | 345/590 [02:22<01:12,  3.38it/s] 59%|█████▊    | 346/590 [02:22<01:12,  3.38it/s] 59%|█████▉    | 347/590 [02:23<01:12,  3.35it/s] 59%|█████▉    | 348/590 [02:23<01:12,  3.36it/s] 59%|█████▉    | 349/590 [02:23<01:11,  3.36it/s] 59%|█████▉    | 350/590 [02:23<01:11,  3.37it/s] 59%|█████▉    | 351/590 [02:24<01:10,  3.37it/s] 60%|█████▉    | 352/590 [02:24<01:10,  3.37it/s] 60%|█████▉    | 353/590 [02:24<01:10,  3.38it/s] 60%|██████    | 354/590 [02:25<01:09,  3.38it/s][INFO|trainer.py:2140] 2023-08-27 23:19:43,040 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:19:43,040 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-27 23:19:43,040 >>   Batch size = 8
{'eval_loss': 0.8953657746315002, 'eval_runtime': 13.9436, 'eval_samples_per_second': 350.126, 'eval_steps_per_second': 43.82, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.05it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.57it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.06it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.24it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.55it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.28it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.31it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.23it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.34it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.51it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.33it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.37it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.15it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.04it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.10it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.90it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.08it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.25it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.29it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.14it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.10it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.11it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.99it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.98it/s][A
 21%|██        | 127/611 [00:02<00:11, 44.00it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.11it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.27it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.29it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.19it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.22it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.23it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.05it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.96it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.01it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.21it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.26it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.23it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.12it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.25it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.15it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.05it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.14it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.07it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.20it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.23it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.22it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.17it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.15it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.10it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.02it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.08it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.19it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.18it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.17it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.19it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.09it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.14it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.05it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.05it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.03it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.19it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.15it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.21it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.18it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.13it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.12it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.13it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.09it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.92it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.23it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.28it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.12it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.26it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.16it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 43.98it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.11it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.18it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.13it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.22it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.16it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.15it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.21it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.11it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.27it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.21it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.16it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.08it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.19it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.18it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.11it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.10it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.09it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.24it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.12it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.08it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.09it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.10it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.15it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.10it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.17it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.21it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.15it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.20it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.08it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.06it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.12it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.18it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.18it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.12it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.28it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.22it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.20it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.14it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.11it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.17it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.16it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.12it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.20it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.25it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.26it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.18it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:39<01:09,  3.38it/s]
100%|██████████| 611/611 [00:13<00:00, 44.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:19:56,936 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-27 23:19:56,951 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:19:59,729 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:19:59,747 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:19:59,759 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:47<27:09,  6.93s/it] 60%|██████    | 356/590 [02:47<19:16,  4.94s/it] 61%|██████    | 357/590 [02:48<13:46,  3.55s/it] 61%|██████    | 358/590 [02:48<09:56,  2.57s/it] 61%|██████    | 359/590 [02:48<07:16,  1.89s/it] 61%|██████    | 360/590 [02:49<05:24,  1.41s/it] 61%|██████    | 361/590 [02:49<04:06,  1.08s/it] 61%|██████▏   | 362/590 [02:49<03:12,  1.19it/s] 62%|██████▏   | 363/590 [02:49<02:34,  1.47it/s] 62%|██████▏   | 364/590 [02:50<02:07,  1.77it/s] 62%|██████▏   | 365/590 [02:50<01:48,  2.07it/s] 62%|██████▏   | 366/590 [02:50<01:35,  2.34it/s] 62%|██████▏   | 367/590 [02:51<01:26,  2.57it/s] 62%|██████▏   | 368/590 [02:51<01:20,  2.77it/s] 63%|██████▎   | 369/590 [02:51<01:15,  2.94it/s] 63%|██████▎   | 370/590 [02:51<01:11,  3.06it/s] 63%|██████▎   | 371/590 [02:52<01:09,  3.15it/s] 63%|██████▎   | 372/590 [02:52<01:07,  3.22it/s] 63%|██████▎   | 373/590 [02:52<01:06,  3.27it/s] 63%|██████▎   | 374/590 [02:53<01:05,  3.30it/s] 64%|██████▎   | 375/590 [02:53<01:04,  3.33it/s] 64%|██████▎   | 376/590 [02:53<01:04,  3.34it/s] 64%|██████▍   | 377/590 [02:54<01:03,  3.36it/s] 64%|██████▍   | 378/590 [02:54<01:03,  3.36it/s] 64%|██████▍   | 379/590 [02:54<01:02,  3.37it/s] 64%|██████▍   | 380/590 [02:54<01:02,  3.37it/s] 65%|██████▍   | 381/590 [02:55<01:01,  3.38it/s] 65%|██████▍   | 382/590 [02:55<01:01,  3.38it/s] 65%|██████▍   | 383/590 [02:55<01:01,  3.38it/s] 65%|██████▌   | 384/590 [02:56<01:00,  3.38it/s] 65%|██████▌   | 385/590 [02:56<01:00,  3.38it/s] 65%|██████▌   | 386/590 [02:56<01:00,  3.39it/s] 66%|██████▌   | 387/590 [02:57<00:59,  3.39it/s] 66%|██████▌   | 388/590 [02:57<00:59,  3.39it/s] 66%|██████▌   | 389/590 [02:57<00:59,  3.37it/s] 66%|██████▌   | 390/590 [02:57<00:59,  3.37it/s] 66%|██████▋   | 391/590 [02:58<00:58,  3.38it/s] 66%|██████▋   | 392/590 [02:58<00:58,  3.38it/s] 67%|██████▋   | 393/590 [02:58<00:58,  3.38it/s] 67%|██████▋   | 394/590 [02:59<00:57,  3.38it/s] 67%|██████▋   | 395/590 [02:59<00:57,  3.38it/s] 67%|██████▋   | 396/590 [02:59<00:57,  3.38it/s] 67%|██████▋   | 397/590 [02:59<00:57,  3.39it/s] 67%|██████▋   | 398/590 [03:00<00:56,  3.39it/s] 68%|██████▊   | 399/590 [03:00<00:56,  3.38it/s] 68%|██████▊   | 400/590 [03:00<00:56,  3.38it/s] 68%|██████▊   | 401/590 [03:01<00:55,  3.38it/s] 68%|██████▊   | 402/590 [03:01<00:55,  3.38it/s] 68%|██████▊   | 403/590 [03:01<00:55,  3.38it/s] 68%|██████▊   | 404/590 [03:02<00:54,  3.38it/s] 69%|██████▊   | 405/590 [03:02<00:54,  3.38it/s] 69%|██████▉   | 406/590 [03:02<00:54,  3.39it/s] 69%|██████▉   | 407/590 [03:02<00:54,  3.39it/s] 69%|██████▉   | 408/590 [03:03<00:53,  3.38it/s] 69%|██████▉   | 409/590 [03:03<00:53,  3.39it/s] 69%|██████▉   | 410/590 [03:03<00:53,  3.39it/s] 70%|██████▉   | 411/590 [03:04<00:53,  3.38it/s] 70%|██████▉   | 412/590 [03:04<00:52,  3.38it/s] 70%|███████   | 413/590 [03:04<00:52,  3.38it/s] 70%|███████   | 414/590 [03:04<00:52,  3.37it/s] 70%|███████   | 415/590 [03:05<00:51,  3.38it/s] 71%|███████   | 416/590 [03:05<00:51,  3.38it/s] 71%|███████   | 417/590 [03:05<00:51,  3.38it/s] 71%|███████   | 418/590 [03:06<00:50,  3.38it/s] 71%|███████   | 419/590 [03:06<00:50,  3.38it/s] 71%|███████   | 420/590 [03:06<00:50,  3.38it/s] 71%|███████▏  | 421/590 [03:07<00:49,  3.38it/s] 72%|███████▏  | 422/590 [03:07<00:49,  3.36it/s] 72%|███████▏  | 423/590 [03:07<00:49,  3.37it/s] 72%|███████▏  | 424/590 [03:07<00:49,  3.37it/s] 72%|███████▏  | 425/590 [03:08<00:48,  3.38it/s] 72%|███████▏  | 426/590 [03:08<00:48,  3.38it/s] 72%|███████▏  | 427/590 [03:08<00:48,  3.38it/s] 73%|███████▎  | 428/590 [03:09<00:47,  3.38it/s] 73%|███████▎  | 429/590 [03:09<00:47,  3.38it/s] 73%|███████▎  | 430/590 [03:09<00:47,  3.38it/s] 73%|███████▎  | 431/590 [03:10<00:46,  3.38it/s] 73%|███████▎  | 432/590 [03:10<00:46,  3.38it/s] 73%|███████▎  | 433/590 [03:10<00:46,  3.36it/s] 74%|███████▎  | 434/590 [03:10<00:46,  3.37it/s] 74%|███████▎  | 435/590 [03:11<00:45,  3.37it/s] 74%|███████▍  | 436/590 [03:11<00:45,  3.38it/s] 74%|███████▍  | 437/590 [03:11<00:45,  3.38it/s] 74%|███████▍  | 438/590 [03:12<00:44,  3.38it/s] 74%|███████▍  | 439/590 [03:12<00:44,  3.38it/s] 75%|███████▍  | 440/590 [03:12<00:44,  3.38it/s] 75%|███████▍  | 441/590 [03:12<00:44,  3.38it/s] 75%|███████▍  | 442/590 [03:13<00:43,  3.38it/s] 75%|███████▌  | 443/590 [03:13<00:43,  3.38it/s] 75%|███████▌  | 444/590 [03:13<00:43,  3.36it/s] 75%|███████▌  | 445/590 [03:14<00:43,  3.36it/s] 76%|███████▌  | 446/590 [03:14<00:42,  3.37it/s] 76%|███████▌  | 447/590 [03:14<00:42,  3.38it/s] 76%|███████▌  | 448/590 [03:15<00:42,  3.38it/s] 76%|███████▌  | 449/590 [03:15<00:41,  3.38it/s] 76%|███████▋  | 450/590 [03:15<00:41,  3.38it/s] 76%|███████▋  | 451/590 [03:15<00:41,  3.38it/s] 77%|███████▋  | 452/590 [03:16<00:40,  3.39it/s] 77%|███████▋  | 453/590 [03:16<00:40,  3.38it/s] 77%|███████▋  | 454/590 [03:16<00:40,  3.38it/s] 77%|███████▋  | 455/590 [03:17<00:39,  3.38it/s] 77%|███████▋  | 456/590 [03:17<00:39,  3.38it/s] 77%|███████▋  | 457/590 [03:17<00:39,  3.38it/s] 78%|███████▊  | 458/590 [03:18<00:39,  3.37it/s] 78%|███████▊  | 459/590 [03:18<00:38,  3.37it/s] 78%|███████▊  | 460/590 [03:18<00:38,  3.38it/s] 78%|███████▊  | 461/590 [03:18<00:38,  3.38it/s] 78%|███████▊  | 462/590 [03:19<00:37,  3.38it/s] 78%|███████▊  | 463/590 [03:19<00:37,  3.38it/s] 79%|███████▊  | 464/590 [03:19<00:37,  3.38it/s] 79%|███████▉  | 465/590 [03:20<00:36,  3.38it/s] 79%|███████▉  | 466/590 [03:20<00:36,  3.38it/s] 79%|███████▉  | 467/590 [03:20<00:36,  3.38it/s] 79%|███████▉  | 468/590 [03:20<00:36,  3.38it/s] 79%|███████▉  | 469/590 [03:21<00:35,  3.37it/s] 80%|███████▉  | 470/590 [03:21<00:35,  3.37it/s] 80%|███████▉  | 471/590 [03:21<00:35,  3.38it/s] 80%|████████  | 472/590 [03:22<00:34,  3.38it/s][INFO|trainer.py:2140] 2023-08-27 23:20:40,075 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:20:40,075 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-27 23:20:40,075 >>   Batch size = 8
{'eval_loss': 0.8997663259506226, 'eval_runtime': 13.8849, 'eval_samples_per_second': 351.604, 'eval_steps_per_second': 44.004, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.14it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.53it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.10it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.25it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.70it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.47it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.24it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.25it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.34it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.42it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.41it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.30it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.19it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.12it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.05it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.06it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.17it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.25it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.16it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.28it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.24it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.17it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.17it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.13it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.07it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.17it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.25it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.28it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.19it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.13it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.12it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.13it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.15it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.16it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.22it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.21it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.25it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.24it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.08it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.08it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.07it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.98it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.20it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.16it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.09it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.32it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.22it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.13it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.15it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.09it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.07it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.21it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.19it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.23it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.26it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.16it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.12it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.27it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.09it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.12it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.17it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.24it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.16it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.27it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.19it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.12it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.14it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.15it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.11it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.20it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.30it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.22it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.17it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.01it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.07it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.06it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.03it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.10it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.22it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.40it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.36it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.12it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.12it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.16it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.02it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.10it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.09it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.16it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.39it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.29it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.16it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.13it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.13it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.24it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.10it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.10it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.25it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.28it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.20it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.10it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.95it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.13it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.19it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.17it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.14it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.21it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.21it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.18it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.16it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.02it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.12it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.18it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.06it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.13it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.22it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.21it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.03it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.97it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.02it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:36<00:34,  3.38it/s]
100%|██████████| 611/611 [00:13<00:00, 44.02it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:20:53,967 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-27 23:20:53,990 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:20:55,700 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:20:55,713 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:20:55,720 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:42<12:25,  6.37s/it] 80%|████████  | 474/590 [03:43<08:47,  4.55s/it] 81%|████████  | 475/590 [03:43<06:16,  3.27s/it] 81%|████████  | 476/590 [03:43<04:31,  2.38s/it] 81%|████████  | 477/590 [03:43<03:18,  1.75s/it] 81%|████████  | 478/590 [03:44<02:27,  1.32s/it] 81%|████████  | 479/590 [03:44<01:52,  1.01s/it] 81%|████████▏ | 480/590 [03:44<01:27,  1.26it/s] 82%|████████▏ | 481/590 [03:45<01:10,  1.55it/s] 82%|████████▏ | 482/590 [03:45<00:58,  1.85it/s] 82%|████████▏ | 483/590 [03:45<00:49,  2.14it/s] 82%|████████▏ | 484/590 [03:45<00:44,  2.41it/s] 82%|████████▏ | 485/590 [03:46<00:39,  2.64it/s] 82%|████████▏ | 486/590 [03:46<00:36,  2.82it/s] 83%|████████▎ | 487/590 [03:46<00:34,  2.97it/s] 83%|████████▎ | 488/590 [03:47<00:33,  3.09it/s] 83%|████████▎ | 489/590 [03:47<00:31,  3.17it/s] 83%|████████▎ | 490/590 [03:47<00:30,  3.23it/s] 83%|████████▎ | 491/590 [03:48<00:30,  3.27it/s] 83%|████████▎ | 492/590 [03:48<00:29,  3.31it/s] 84%|████████▎ | 493/590 [03:48<00:29,  3.33it/s] 84%|████████▎ | 494/590 [03:48<00:28,  3.35it/s] 84%|████████▍ | 495/590 [03:49<00:28,  3.37it/s] 84%|████████▍ | 496/590 [03:49<00:27,  3.39it/s] 84%|████████▍ | 497/590 [03:49<00:27,  3.40it/s] 84%|████████▍ | 498/590 [03:50<00:26,  3.42it/s] 85%|████████▍ | 499/590 [03:50<00:26,  3.42it/s] 85%|████████▍ | 500/590 [03:50<00:26,  3.41it/s]                                                  85%|████████▍ | 500/590 [03:50<00:26,  3.41it/s] 85%|████████▍ | 501/590 [03:50<00:26,  3.40it/s] 85%|████████▌ | 502/590 [03:51<00:25,  3.40it/s] 85%|████████▌ | 503/590 [03:51<00:25,  3.39it/s] 85%|████████▌ | 504/590 [03:51<00:25,  3.38it/s] 86%|████████▌ | 505/590 [03:52<00:25,  3.38it/s] 86%|████████▌ | 506/590 [03:52<00:24,  3.38it/s] 86%|████████▌ | 507/590 [03:52<00:24,  3.39it/s] 86%|████████▌ | 508/590 [03:53<00:24,  3.39it/s] 86%|████████▋ | 509/590 [03:53<00:23,  3.39it/s] 86%|████████▋ | 510/590 [03:53<00:23,  3.39it/s] 87%|████████▋ | 511/590 [03:53<00:23,  3.39it/s] 87%|████████▋ | 512/590 [03:54<00:23,  3.39it/s] 87%|████████▋ | 513/590 [03:54<00:22,  3.38it/s] 87%|████████▋ | 514/590 [03:54<00:22,  3.39it/s] 87%|████████▋ | 515/590 [03:55<00:22,  3.38it/s] 87%|████████▋ | 516/590 [03:55<00:21,  3.38it/s] 88%|████████▊ | 517/590 [03:55<00:21,  3.38it/s] 88%|████████▊ | 518/590 [03:55<00:21,  3.38it/s] 88%|████████▊ | 519/590 [03:56<00:20,  3.38it/s] 88%|████████▊ | 520/590 [03:56<00:20,  3.40it/s] 88%|████████▊ | 521/590 [03:56<00:20,  3.41it/s] 88%|████████▊ | 522/590 [03:57<00:19,  3.42it/s] 89%|████████▊ | 523/590 [03:57<00:19,  3.42it/s] 89%|████████▉ | 524/590 [03:57<00:19,  3.43it/s] 89%|████████▉ | 525/590 [03:58<00:18,  3.43it/s] 89%|████████▉ | 526/590 [03:58<00:18,  3.42it/s] 89%|████████▉ | 527/590 [03:58<00:18,  3.42it/s] 89%|████████▉ | 528/590 [03:58<00:18,  3.43it/s] 90%|████████▉ | 529/590 [03:59<00:17,  3.43it/s] 90%|████████▉ | 530/590 [03:59<00:17,  3.43it/s] 90%|█████████ | 531/590 [03:59<00:17,  3.43it/s] 90%|█████████ | 532/590 [04:00<00:16,  3.43it/s] 90%|█████████ | 533/590 [04:00<00:16,  3.43it/s] 91%|█████████ | 534/590 [04:00<00:16,  3.44it/s] 91%|█████████ | 535/590 [04:00<00:16,  3.44it/s] 91%|█████████ | 536/590 [04:01<00:15,  3.44it/s] 91%|█████████ | 537/590 [04:01<00:15,  3.41it/s] 91%|█████████ | 538/590 [04:01<00:15,  3.42it/s] 91%|█████████▏| 539/590 [04:02<00:14,  3.42it/s] 92%|█████████▏| 540/590 [04:02<00:14,  3.43it/s] 92%|█████████▏| 541/590 [04:02<00:14,  3.43it/s] 92%|█████████▏| 542/590 [04:02<00:13,  3.43it/s] 92%|█████████▏| 543/590 [04:03<00:13,  3.43it/s] 92%|█████████▏| 544/590 [04:03<00:13,  3.43it/s] 92%|█████████▏| 545/590 [04:03<00:13,  3.43it/s] 93%|█████████▎| 546/590 [04:04<00:12,  3.43it/s] 93%|█████████▎| 547/590 [04:04<00:12,  3.43it/s] 93%|█████████▎| 548/590 [04:04<00:12,  3.42it/s] 93%|█████████▎| 549/590 [04:05<00:11,  3.42it/s] 93%|█████████▎| 550/590 [04:05<00:11,  3.43it/s] 93%|█████████▎| 551/590 [04:05<00:11,  3.43it/s] 94%|█████████▎| 552/590 [04:05<00:11,  3.43it/s] 94%|█████████▎| 553/590 [04:06<00:10,  3.43it/s] 94%|█████████▍| 554/590 [04:06<00:10,  3.43it/s] 94%|█████████▍| 555/590 [04:06<00:10,  3.44it/s] 94%|█████████▍| 556/590 [04:07<00:09,  3.43it/s] 94%|█████████▍| 557/590 [04:07<00:09,  3.44it/s] 95%|█████████▍| 558/590 [04:07<00:09,  3.44it/s] 95%|█████████▍| 559/590 [04:07<00:09,  3.43it/s] 95%|█████████▍| 560/590 [04:08<00:08,  3.43it/s] 95%|█████████▌| 561/590 [04:08<00:08,  3.43it/s] 95%|█████████▌| 562/590 [04:08<00:08,  3.44it/s] 95%|█████████▌| 563/590 [04:09<00:07,  3.43it/s] 96%|█████████▌| 564/590 [04:09<00:07,  3.44it/s] 96%|█████████▌| 565/590 [04:09<00:07,  3.44it/s] 96%|█████████▌| 566/590 [04:09<00:06,  3.44it/s] 96%|█████████▌| 567/590 [04:10<00:06,  3.44it/s] 96%|█████████▋| 568/590 [04:10<00:06,  3.44it/s] 96%|█████████▋| 569/590 [04:10<00:06,  3.44it/s] 97%|█████████▋| 570/590 [04:11<00:05,  3.43it/s] 97%|█████████▋| 571/590 [04:11<00:05,  3.43it/s] 97%|█████████▋| 572/590 [04:11<00:05,  3.43it/s] 97%|█████████▋| 573/590 [04:12<00:04,  3.43it/s] 97%|█████████▋| 574/590 [04:12<00:04,  3.43it/s] 97%|█████████▋| 575/590 [04:12<00:04,  3.43it/s] 98%|█████████▊| 576/590 [04:12<00:04,  3.44it/s] 98%|█████████▊| 577/590 [04:13<00:03,  3.44it/s] 98%|█████████▊| 578/590 [04:13<00:03,  3.44it/s] 98%|█████████▊| 579/590 [04:13<00:03,  3.43it/s] 98%|█████████▊| 580/590 [04:14<00:02,  3.43it/s] 98%|█████████▊| 581/590 [04:14<00:02,  3.42it/s] 99%|█████████▊| 582/590 [04:14<00:02,  3.35it/s] 99%|█████████▉| 583/590 [04:14<00:02,  3.38it/s] 99%|█████████▉| 584/590 [04:15<00:01,  3.40it/s] 99%|█████████▉| 585/590 [04:15<00:01,  3.41it/s] 99%|█████████▉| 586/590 [04:15<00:01,  3.42it/s] 99%|█████████▉| 587/590 [04:16<00:00,  3.42it/s]100%|█████████▉| 588/590 [04:16<00:00,  3.42it/s]100%|█████████▉| 589/590 [04:16<00:00,  3.43it/s]100%|██████████| 590/590 [04:16<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-27 23:21:34,855 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:21:34,855 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-27 23:21:34,855 >>   Batch size = 8
{'eval_loss': 0.9048450589179993, 'eval_runtime': 13.8809, 'eval_samples_per_second': 351.707, 'eval_steps_per_second': 44.017, 'epoch': 4.0}
{'loss': 0.7478, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.12it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.05it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.20it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.29it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.83it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.52it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.46it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.37it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.00it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.30it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.24it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.20it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.15it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.07it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.02it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.13it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.12it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.28it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.40it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.30it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.23it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.17it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.08it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.19it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.24it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.34it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.37it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.30it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.34it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.23it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.07it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.11it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.11it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.22it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.28it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.37it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.32it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.32it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.20it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.17it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.15it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.19it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.10it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.15it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.31it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.35it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.20it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.13it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.10it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.19it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.19it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.19it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.17it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.29it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.31it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.31it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.21it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.13it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.16it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.19it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.05it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.19it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.29it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.31it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.32it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.19it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.18it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.15it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.15it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.11it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.96it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.23it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.30it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.18it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.15it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.14it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.11it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.08it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.11it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.19it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.28it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.38it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.33it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.27it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.25it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.14it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.06it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.12it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.18it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.29it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.36it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.33it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.32it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.15it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.10it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.84it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 43.98it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.19it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.27it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.34it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.30it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.23it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.08it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.00it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.98it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.20it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.24it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.36it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.40it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.37it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.19it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.15it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.16it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.15it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.17it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.12it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.36it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.44it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.29it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.82it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.03it/s][A                                                 
                                                 [A100%|██████████| 590/590 [04:30<00:00,  3.43it/s]
100%|██████████| 611/611 [00:13<00:00, 44.03it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:21:48,695 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-27 23:21:48,725 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:21:50,816 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:21:50,837 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:21:50,853 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 23:21:55,287 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 23:21:55,323 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236 (score: 0.8953657746315002).
                                                 100%|██████████| 590/590 [04:40<00:00,  3.43it/s]100%|██████████| 590/590 [04:40<00:00,  2.10it/s]
[INFO|trainer.py:1894] 2023-08-27 23:21:58,762 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 23:21:58,779 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:22:00,453 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:22:00,464 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:22:00,480 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 23:22:00,674 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:00,674 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:00,674 >>   train_loss               =     0.7422
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:00,674 >>   train_runtime            = 0:04:40.88
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:00,674 >>   train_samples            =       7574
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:00,674 >>   train_samples_per_second =    134.825
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:00,674 >>   train_steps_per_second   =      2.101
{'eval_loss': 0.9070471525192261, 'eval_runtime': 13.8172, 'eval_samples_per_second': 353.328, 'eval_steps_per_second': 44.22, 'epoch': 5.0}
{'train_runtime': 280.8836, 'train_samples_per_second': 134.825, 'train_steps_per_second': 2.101, 'train_loss': 0.7422440480377714, 'epoch': 5.0}
08/27/2023 23:22:00 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 23:22:00,711 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:22:00,711 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-27 23:22:00,711 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.28it/s]  2%|▏         | 12/611 [00:00<00:12, 48.70it/s]  3%|▎         | 17/611 [00:00<00:12, 47.16it/s]  4%|▎         | 22/611 [00:00<00:12, 46.51it/s]  4%|▍         | 27/611 [00:00<00:12, 45.96it/s]  5%|▌         | 32/611 [00:00<00:12, 45.61it/s]  6%|▌         | 37/611 [00:00<00:12, 45.38it/s]  7%|▋         | 42/611 [00:00<00:12, 44.94it/s]  8%|▊         | 47/611 [00:01<00:12, 44.39it/s]  9%|▊         | 52/611 [00:01<00:12, 44.18it/s]  9%|▉         | 57/611 [00:01<00:12, 44.23it/s] 10%|█         | 62/611 [00:01<00:12, 44.55it/s] 11%|█         | 67/611 [00:01<00:12, 44.67it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.80it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.80it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.80it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.36it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.07it/s] 16%|█▌        | 97/611 [00:02<00:11, 44.03it/s] 17%|█▋        | 102/611 [00:02<00:11, 44.20it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.36it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.54it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.71it/s] 20%|█▉        | 122/611 [00:02<00:10, 44.74it/s] 21%|██        | 127/611 [00:02<00:10, 44.69it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.31it/s] 22%|██▏       | 137/611 [00:03<00:10, 44.06it/s] 23%|██▎       | 142/611 [00:03<00:10, 44.03it/s] 24%|██▍       | 147/611 [00:03<00:10, 44.23it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.35it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.56it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.64it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.78it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.65it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.46it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.16it/s] 31%|███       | 187/611 [00:04<00:09, 44.11it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.18it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.34it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.54it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.61it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.67it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.54it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.44it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.24it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.11it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.28it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.40it/s] 40%|████      | 247/611 [00:05<00:08, 44.58it/s] 41%|████      | 252/611 [00:05<00:08, 44.56it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.61it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.51it/s] 44%|████▎     | 267/611 [00:05<00:07, 44.27it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.00it/s] 45%|████▌     | 277/611 [00:06<00:07, 44.13it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.17it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.47it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.56it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.62it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.57it/s] 50%|█████     | 307/611 [00:06<00:06, 44.57it/s] 51%|█████     | 312/611 [00:06<00:06, 44.44it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.21it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.13it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.17it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.40it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.49it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.60it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.57it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.51it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.35it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.25it/s] 60%|██████    | 367/611 [00:08<00:05, 44.09it/s] 61%|██████    | 372/611 [00:08<00:05, 44.31it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.51it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.53it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.47it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.62it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.49it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.29it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.26it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.16it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.29it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.39it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.54it/s] 71%|███████   | 432/611 [00:09<00:04, 44.47it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.58it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.49it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.36it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.26it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.15it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.26it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.47it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.50it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.49it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.56it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.45it/s] 81%|████████  | 492/611 [00:11<00:02, 44.31it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.25it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.26it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.42it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.46it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.56it/s] 85%|████████▌ | 522/611 [00:11<00:01, 44.56it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.55it/s] 87%|████████▋ | 532/611 [00:11<00:01, 44.42it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.27it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.15it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.22it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.32it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.37it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.55it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.51it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.50it/s] 94%|█████████▍| 577/611 [00:12<00:00, 44.27it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.26it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.23it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.28it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.38it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.42it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.49it/s]100%|██████████| 611/611 [00:13<00:00, 44.51it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 23:22:14,459 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:14,459 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:14,459 >>   eval_loss               =     0.8954
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:14,459 >>   eval_runtime            = 0:00:13.74
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:14,459 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:14,459 >>   eval_samples_per_second =    355.111
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:14,459 >>   eval_steps_per_second   =     44.443
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:22:14,459 >>   perplexity              =     2.4482
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_1', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:17<04:08, 17.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:32<03:29, 16.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:44<02:47, 13.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:59<02:38, 14.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:15<02:30, 15.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:29<02:13, 14.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:43<01:55, 14.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:56<01:38, 14.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:12<01:27, 14.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:26<01:12, 14.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:39<00:55, 13.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:57<00:45, 15.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:12<00:30, 15.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:25<00:14, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:41<00:00, 14.87s/it]Generating: 100%|██████████| 15/15 [03:41<00:00, 14.74s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 469, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : developer .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 626, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.7825, 'errors': {'', "('London', 'work location', '', 'The New York Times ( July 18 , 1923 October 6 , 1977 ) reported that the first and only English book on the topic appeared in London during the reigns of William Shakespeare and William Shakespeare .')"}}
['Relation : composer . Context : Later in the year ( 1143 ) , he composed The Seven Kingdoms , the first play in the epic of the Old Kingdom . Head Entity : The Seven Kingdoms , Tail Entity : Robert I .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best Rap Album at the 2004 MTV Video Music Awards and had two acts on the chart . Head Entity : music , Tail Entity : Rap Album .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 590, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7319711538461539, 'errors': {'', "('James S. Cain', 'field of work', '', 'He is also known for his work with James S. Cain , and for his work on the painting of the Seven Wonders of Oz by William S. Burroughs .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 194, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 298, 'raw': 448}
{'target': 600, 'success': 322, 'raw': 480}
{'target': 600, 'success': 346, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 387, 'raw': 576}
{'target': 600, 'success': 408, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 518, 'raw': 768}
{'target': 600, 'success': 543, 'raw': 800}
{'target': 600, 'success': 563, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 606, 'raw': 896}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6763392857142857, 'errors': {'', "('John Lennon', 'occupation', '', 'The band released their debut album In Search of a Way ( 2000 ) , which featured a cover from John Lennon and Steve Dizzy .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 13757
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13857, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.49s/it]Extractor Estimating: 2it [00:17,  7.54s/it]Extractor Estimating: 3it [00:18,  4.37s/it]Extractor Estimating: 4it [00:18,  2.86s/it]Extractor Estimating: 5it [00:19,  2.04s/it]Extractor Estimating: 6it [00:20,  1.54s/it]Extractor Estimating: 7it [00:20,  1.23s/it]Extractor Estimating: 8it [00:21,  1.03s/it]Extractor Estimating: 9it [00:22,  1.07s/it]Extractor Estimating: 10it [00:22,  1.08it/s]Extractor Estimating: 11it [00:23,  1.19it/s]Extractor Estimating: 12it [00:24,  1.31it/s]Extractor Estimating: 13it [00:24,  1.35it/s]Extractor Estimating: 14it [00:25,  1.44it/s]Extractor Estimating: 15it [00:26,  1.52it/s]Extractor Estimating: 16it [00:26,  1.57it/s]Extractor Estimating: 17it [00:27,  1.58it/s]Extractor Estimating: 18it [00:27,  1.60it/s]Extractor Estimating: 19it [00:28,  1.61it/s]Extractor Estimating: 20it [00:29,  1.63it/s]Extractor Estimating: 21it [00:29,  1.68it/s]Extractor Estimating: 22it [00:30,  1.73it/s]Extractor Estimating: 23it [00:30,  1.72it/s]Extractor Estimating: 24it [00:31,  1.68it/s]Extractor Estimating: 25it [00:31,  1.71it/s]Extractor Estimating: 26it [00:32,  1.77it/s]Extractor Estimating: 27it [00:33,  1.73it/s]Extractor Estimating: 28it [00:33,  1.77it/s]Extractor Estimating: 29it [00:34,  1.75it/s]Extractor Estimating: 30it [00:35,  1.08it/s]Extractor Estimating: 31it [00:36,  1.20it/s]Extractor Estimating: 32it [00:37,  1.36it/s]Extractor Estimating: 33it [00:37,  1.44it/s]Extractor Estimating: 34it [00:38,  1.52it/s]Extractor Estimating: 35it [00:38,  1.54it/s]Extractor Estimating: 36it [00:39,  1.58it/s]Extractor Estimating: 37it [00:40,  1.62it/s]Extractor Estimating: 38it [00:40,  1.61it/s]Extractor Estimating: 39it [00:41,  1.66it/s]Extractor Estimating: 40it [00:41,  1.70it/s]Extractor Estimating: 41it [00:42,  1.67it/s]Extractor Estimating: 42it [00:42,  1.71it/s]Extractor Estimating: 43it [00:43,  1.74it/s]Extractor Estimating: 44it [00:44,  1.74it/s]Extractor Estimating: 45it [00:44,  1.76it/s]Extractor Estimating: 46it [00:45,  1.75it/s]Extractor Estimating: 47it [00:45,  1.76it/s]Extractor Estimating: 48it [00:46,  1.75it/s]Extractor Estimating: 49it [00:47,  1.71it/s]Extractor Estimating: 50it [00:47,  1.78it/s]Extractor Estimating: 51it [00:48,  1.80it/s]Extractor Estimating: 52it [00:48,  1.84it/s]Extractor Estimating: 53it [00:49,  1.87it/s]Extractor Estimating: 54it [00:49,  1.91it/s]Extractor Estimating: 55it [00:50,  1.92it/s]Extractor Estimating: 56it [00:50,  2.00it/s]Extractor Estimating: 57it [00:51,  1.98it/s]Extractor Estimating: 58it [00:51,  2.00it/s]Extractor Estimating: 59it [00:52,  2.06it/s]Extractor Estimating: 60it [00:52,  1.98it/s]Extractor Estimating: 61it [00:53,  1.95it/s]Extractor Estimating: 62it [00:53,  1.99it/s]Extractor Estimating: 63it [00:54,  2.04it/s]Extractor Estimating: 64it [00:54,  1.98it/s]Extractor Estimating: 65it [00:55,  1.93it/s]Extractor Estimating: 66it [00:55,  2.01it/s]Extractor Estimating: 67it [00:56,  2.01it/s]Extractor Estimating: 68it [00:56,  1.97it/s]Extractor Estimating: 69it [00:57,  2.01it/s]Extractor Estimating: 70it [00:57,  1.99it/s]Extractor Estimating: 71it [00:58,  2.03it/s]Extractor Estimating: 72it [00:58,  2.01it/s]Extractor Estimating: 73it [00:59,  2.03it/s]Extractor Estimating: 74it [00:59,  2.03it/s]Extractor Estimating: 75it [01:00,  2.00it/s]Extractor Estimating: 76it [01:00,  1.92it/s]Extractor Estimating: 77it [01:01,  1.78it/s]Extractor Estimating: 78it [01:01,  1.76it/s]Extractor Estimating: 79it [01:02,  1.72it/s]Extractor Estimating: 80it [01:03,  1.63it/s]Extractor Estimating: 81it [01:03,  1.62it/s]Extractor Estimating: 82it [01:04,  1.57it/s]Extractor Estimating: 83it [01:05,  1.58it/s]Extractor Estimating: 84it [01:05,  1.45it/s]Extractor Estimating: 85it [01:06,  1.47it/s]Extractor Estimating: 86it [01:07,  1.51it/s]Extractor Estimating: 87it [01:07,  1.51it/s]Extractor Estimating: 88it [01:08,  1.55it/s]Extractor Estimating: 89it [01:09,  1.61it/s]Extractor Estimating: 90it [01:09,  1.65it/s]Extractor Estimating: 91it [01:10,  1.65it/s]Extractor Estimating: 92it [01:10,  1.67it/s]Extractor Estimating: 93it [01:11,  1.69it/s]Extractor Estimating: 94it [01:11,  1.69it/s]Extractor Estimating: 95it [01:12,  1.65it/s]Extractor Estimating: 96it [01:13,  1.66it/s]Extractor Estimating: 97it [01:13,  1.64it/s]Extractor Estimating: 98it [01:14,  1.65it/s]Extractor Estimating: 99it [01:15,  1.65it/s]Extractor Estimating: 100it [01:15,  1.63it/s]Extractor Estimating: 101it [01:16,  1.63it/s]Extractor Estimating: 102it [01:16,  1.64it/s]Extractor Estimating: 103it [01:17,  1.66it/s]Extractor Estimating: 104it [01:18,  1.69it/s]Extractor Estimating: 105it [01:18,  1.69it/s]Extractor Estimating: 106it [01:19,  1.68it/s]Extractor Estimating: 107it [01:19,  1.65it/s]Extractor Estimating: 108it [01:20,  1.67it/s]Extractor Estimating: 109it [01:21,  1.61it/s]Extractor Estimating: 110it [01:21,  1.62it/s]Extractor Estimating: 111it [01:22,  1.58it/s]Extractor Estimating: 112it [01:23,  1.51it/s]Extractor Estimating: 113it [01:23,  1.54it/s]Extractor Estimating: 114it [01:24,  1.54it/s]Extractor Estimating: 115it [01:25,  1.52it/s]Extractor Estimating: 116it [01:25,  1.54it/s]Extractor Estimating: 117it [01:26,  1.55it/s]Extractor Estimating: 118it [01:26,  1.61it/s]Extractor Estimating: 119it [01:27,  1.62it/s]Extractor Estimating: 120it [01:28,  1.58it/s]Extractor Estimating: 121it [01:28,  1.60it/s]Extractor Estimating: 122it [01:29,  1.59it/s]Extractor Estimating: 123it [01:30,  1.58it/s]Extractor Estimating: 124it [01:30,  1.61it/s]Extractor Estimating: 125it [01:31,  1.65it/s]Extractor Estimating: 126it [01:31,  1.70it/s]Extractor Estimating: 127it [01:32,  1.64it/s]Extractor Estimating: 128it [01:33,  1.66it/s]Extractor Estimating: 129it [01:33,  1.66it/s]Extractor Estimating: 130it [01:34,  1.66it/s]Extractor Estimating: 131it [01:34,  1.63it/s]Extractor Estimating: 132it [01:35,  1.58it/s]Extractor Estimating: 133it [01:36,  1.57it/s]Extractor Estimating: 134it [01:36,  1.58it/s]Extractor Estimating: 135it [01:37,  1.61it/s]Extractor Estimating: 136it [01:38,  1.60it/s]Extractor Estimating: 137it [01:39,  1.22it/s]Extractor Estimating: 138it [01:39,  1.31it/s]Extractor Estimating: 139it [01:40,  1.37it/s]Extractor Estimating: 140it [01:41,  1.44it/s]Extractor Estimating: 141it [01:41,  1.51it/s]Extractor Estimating: 142it [01:42,  1.57it/s]Extractor Estimating: 143it [01:42,  1.62it/s]Extractor Estimating: 144it [01:43,  1.63it/s]Extractor Estimating: 145it [01:44,  1.67it/s]Extractor Estimating: 146it [01:44,  1.62it/s]Extractor Estimating: 147it [01:45,  1.62it/s]Extractor Estimating: 148it [01:46,  1.60it/s]Extractor Estimating: 149it [01:46,  1.66it/s]Extractor Estimating: 150it [01:47,  1.64it/s]Extractor Estimating: 151it [01:47,  1.65it/s]Extractor Estimating: 152it [01:48,  1.68it/s]Extractor Estimating: 153it [01:48,  1.71it/s]Extractor Estimating: 154it [01:49,  1.73it/s]Extractor Estimating: 155it [01:50,  1.73it/s]Extractor Estimating: 156it [01:50,  1.69it/s]Extractor Estimating: 157it [01:51,  1.68it/s]Extractor Estimating: 158it [01:51,  1.67it/s]Extractor Estimating: 159it [01:52,  1.67it/s]Extractor Estimating: 160it [01:53,  1.72it/s]Extractor Estimating: 161it [01:53,  1.48it/s]Extractor Estimating: 162it [01:54,  1.52it/s]Extractor Estimating: 163it [01:55,  1.60it/s]Extractor Estimating: 164it [01:55,  1.64it/s]Extractor Estimating: 165it [01:56,  1.70it/s]Extractor Estimating: 166it [01:56,  1.72it/s]Extractor Estimating: 167it [01:57,  1.76it/s]Extractor Estimating: 168it [01:57,  1.79it/s]Extractor Estimating: 169it [01:58,  1.80it/s]Extractor Estimating: 170it [01:59,  1.71it/s]Extractor Estimating: 171it [01:59,  1.71it/s]Extractor Estimating: 172it [02:00,  1.74it/s]Extractor Estimating: 173it [02:00,  1.76it/s]Extractor Estimating: 174it [02:01,  1.71it/s]Extractor Estimating: 175it [02:02,  1.64it/s]Extractor Estimating: 176it [02:02,  1.63it/s]Extractor Estimating: 177it [02:03,  1.66it/s]Extractor Estimating: 178it [02:03,  1.67it/s]Extractor Estimating: 179it [02:04,  1.66it/s]Extractor Estimating: 180it [02:05,  1.50it/s]Extractor Estimating: 181it [02:05,  1.55it/s]Extractor Estimating: 182it [02:06,  1.58it/s]Extractor Estimating: 183it [02:07,  1.59it/s]Extractor Estimating: 184it [02:07,  1.62it/s]Extractor Estimating: 185it [02:08,  1.61it/s]Extractor Estimating: 186it [02:08,  1.61it/s]Extractor Estimating: 187it [02:09,  1.66it/s]Extractor Estimating: 188it [02:10,  1.66it/s]Extractor Estimating: 189it [02:10,  1.65it/s]Extractor Estimating: 190it [02:11,  1.61it/s]Extractor Estimating: 191it [02:12,  1.58it/s]Extractor Estimating: 192it [02:12,  1.40it/s]Extractor Estimating: 193it [02:13,  1.43it/s]Extractor Estimating: 194it [02:14,  1.50it/s]Extractor Estimating: 195it [02:14,  1.55it/s]Extractor Estimating: 196it [02:15,  1.54it/s]Extractor Estimating: 197it [02:16,  1.56it/s]Extractor Estimating: 198it [02:16,  1.56it/s]Extractor Estimating: 199it [02:17,  1.61it/s]Extractor Estimating: 200it [02:17,  1.61it/s]Extractor Estimating: 201it [02:18,  1.60it/s]Extractor Estimating: 202it [02:19,  1.64it/s]Extractor Estimating: 203it [02:19,  1.60it/s]Extractor Estimating: 204it [02:20,  1.62it/s]Extractor Estimating: 205it [02:21,  1.56it/s]Extractor Estimating: 206it [02:21,  1.62it/s]Extractor Estimating: 207it [02:22,  1.64it/s]Extractor Estimating: 208it [02:22,  1.65it/s]Extractor Estimating: 209it [02:23,  1.66it/s]Extractor Estimating: 210it [02:24,  1.65it/s]Extractor Estimating: 211it [02:24,  1.67it/s]Extractor Estimating: 212it [02:25,  1.68it/s]Extractor Estimating: 213it [02:25,  1.68it/s]Extractor Estimating: 214it [02:26,  1.66it/s]Extractor Estimating: 215it [02:27,  1.64it/s]Extractor Estimating: 216it [02:27,  1.60it/s]Extractor Estimating: 217it [02:28,  1.62it/s]Extractor Estimating: 218it [02:28,  1.62it/s]Extractor Estimating: 219it [02:29,  1.69it/s]Extractor Estimating: 220it [02:30,  1.69it/s]Extractor Estimating: 221it [02:30,  1.68it/s]Extractor Estimating: 222it [02:31,  1.67it/s]Extractor Estimating: 223it [02:31,  1.69it/s]Extractor Estimating: 224it [02:32,  1.66it/s]Extractor Estimating: 225it [02:33,  1.65it/s]Extractor Estimating: 226it [02:33,  1.62it/s]Extractor Estimating: 227it [02:34,  1.56it/s]Extractor Estimating: 228it [02:35,  1.56it/s]Extractor Estimating: 229it [02:35,  1.57it/s]Extractor Estimating: 230it [02:36,  1.58it/s]Extractor Estimating: 231it [02:36,  1.60it/s]Extractor Estimating: 232it [02:37,  1.64it/s]Extractor Estimating: 233it [02:38,  1.66it/s]Extractor Estimating: 234it [02:38,  1.62it/s]Extractor Estimating: 235it [02:39,  1.57it/s]Extractor Estimating: 236it [02:40,  1.57it/s]Extractor Estimating: 237it [02:40,  1.57it/s]Extractor Estimating: 238it [02:41,  1.60it/s]Extractor Estimating: 239it [02:41,  1.63it/s]Extractor Estimating: 240it [02:42,  1.63it/s]Extractor Estimating: 241it [02:43,  1.60it/s]Extractor Estimating: 242it [02:43,  1.56it/s]Extractor Estimating: 243it [02:44,  1.56it/s]Extractor Estimating: 244it [02:45,  1.46it/s]Extractor Estimating: 245it [02:45,  1.48it/s]Extractor Estimating: 246it [02:46,  1.49it/s]Extractor Estimating: 247it [02:47,  1.50it/s]Extractor Estimating: 248it [02:47,  1.56it/s]Extractor Estimating: 249it [02:48,  1.59it/s]Extractor Estimating: 250it [02:49,  1.56it/s]Extractor Estimating: 251it [02:49,  1.57it/s]Extractor Estimating: 252it [02:50,  1.56it/s]Extractor Estimating: 253it [02:50,  1.58it/s]Extractor Estimating: 254it [02:51,  1.60it/s]Extractor Estimating: 255it [02:52,  1.62it/s]Extractor Estimating: 256it [02:52,  1.62it/s]Extractor Estimating: 257it [02:53,  1.66it/s]Extractor Estimating: 258it [02:53,  1.70it/s]Extractor Estimating: 259it [02:54,  1.66it/s]Extractor Estimating: 260it [02:55,  1.70it/s]Extractor Estimating: 261it [02:55,  1.66it/s]Extractor Estimating: 262it [02:56,  1.64it/s]Extractor Estimating: 263it [02:56,  1.66it/s]Extractor Estimating: 264it [02:57,  1.69it/s]Extractor Estimating: 265it [02:58,  1.68it/s]Extractor Estimating: 266it [02:58,  1.70it/s]Extractor Estimating: 267it [02:59,  1.69it/s]Extractor Estimating: 268it [02:59,  1.63it/s]Extractor Estimating: 269it [03:00,  1.66it/s]Extractor Estimating: 270it [03:01,  1.63it/s]Extractor Estimating: 271it [03:01,  1.60it/s]Extractor Estimating: 272it [03:02,  1.55it/s]Extractor Estimating: 273it [03:03,  1.60it/s]Extractor Estimating: 274it [03:03,  1.61it/s]Extractor Estimating: 275it [03:04,  1.63it/s]Extractor Estimating: 276it [03:04,  1.57it/s]Extractor Estimating: 277it [03:05,  1.60it/s]Extractor Estimating: 278it [03:06,  1.64it/s]Extractor Estimating: 279it [03:06,  1.62it/s]Extractor Estimating: 280it [03:07,  1.63it/s]Extractor Estimating: 281it [03:07,  1.63it/s]Extractor Estimating: 282it [03:08,  1.66it/s]Extractor Estimating: 283it [03:09,  1.60it/s]Extractor Estimating: 284it [03:09,  1.59it/s]Extractor Estimating: 285it [03:10,  1.59it/s]Extractor Estimating: 286it [03:11,  1.60it/s]Extractor Estimating: 287it [03:11,  1.57it/s]Extractor Estimating: 288it [03:12,  1.61it/s]Extractor Estimating: 289it [03:12,  1.59it/s]Extractor Estimating: 290it [03:13,  1.64it/s]Extractor Estimating: 291it [03:14,  1.65it/s]Extractor Estimating: 292it [03:14,  1.61it/s]Extractor Estimating: 293it [03:15,  1.60it/s]Extractor Estimating: 294it [03:16,  1.63it/s]Extractor Estimating: 295it [03:16,  1.67it/s]Extractor Estimating: 296it [03:17,  1.65it/s]Extractor Estimating: 297it [03:17,  1.69it/s]Extractor Estimating: 298it [03:18,  1.74it/s]Extractor Estimating: 299it [03:18,  1.67it/s]Extractor Estimating: 300it [03:19,  1.66it/s]Extractor Estimating: 301it [03:20,  1.64it/s]Extractor Estimating: 302it [03:20,  1.58it/s]Extractor Estimating: 303it [03:21,  1.58it/s]Extractor Estimating: 304it [03:22,  1.56it/s]Extractor Estimating: 305it [03:22,  1.56it/s]Extractor Estimating: 306it [03:23,  1.53it/s]Extractor Estimating: 307it [03:24,  1.57it/s]Extractor Estimating: 308it [03:24,  1.56it/s]Extractor Estimating: 309it [03:25,  1.55it/s]Extractor Estimating: 310it [03:26,  1.55it/s]Extractor Estimating: 311it [03:26,  1.55it/s]Extractor Estimating: 312it [03:27,  1.55it/s]Extractor Estimating: 313it [03:27,  1.57it/s]Extractor Estimating: 314it [03:28,  1.59it/s]Extractor Estimating: 315it [03:29,  1.58it/s]Extractor Estimating: 316it [03:29,  1.58it/s]Extractor Estimating: 317it [03:30,  1.60it/s]Extractor Estimating: 318it [03:31,  1.56it/s]Extractor Estimating: 319it [03:31,  1.57it/s]Extractor Estimating: 320it [03:32,  1.57it/s]Extractor Estimating: 321it [03:33,  1.54it/s]Extractor Estimating: 322it [03:33,  1.56it/s]Extractor Estimating: 323it [03:34,  1.57it/s]Extractor Estimating: 324it [03:34,  1.60it/s]Extractor Estimating: 325it [03:35,  1.61it/s]Extractor Estimating: 326it [03:36,  1.47it/s]Extractor Estimating: 327it [03:36,  1.53it/s]Extractor Estimating: 328it [03:37,  1.56it/s]Extractor Estimating: 329it [03:38,  1.61it/s]Extractor Estimating: 330it [03:38,  1.58it/s]Extractor Estimating: 331it [03:39,  1.60it/s]Extractor Estimating: 332it [03:39,  1.64it/s]Extractor Estimating: 333it [03:40,  1.65it/s]Extractor Estimating: 334it [03:41,  1.65it/s]Extractor Estimating: 335it [03:41,  1.62it/s]Extractor Estimating: 336it [03:42,  1.65it/s]Extractor Estimating: 337it [03:43,  1.56it/s]Extractor Estimating: 338it [03:43,  1.61it/s]Extractor Estimating: 339it [03:44,  1.57it/s]Extractor Estimating: 340it [03:44,  1.63it/s]Extractor Estimating: 341it [03:45,  1.62it/s]Extractor Estimating: 342it [03:46,  1.61it/s]Extractor Estimating: 343it [03:46,  1.65it/s]Extractor Estimating: 344it [03:47,  1.62it/s]Extractor Estimating: 345it [03:48,  1.58it/s]Extractor Estimating: 346it [03:48,  1.59it/s]Extractor Estimating: 347it [03:49,  1.62it/s]Extractor Estimating: 348it [03:49,  1.64it/s]Extractor Estimating: 349it [03:50,  1.66it/s]Extractor Estimating: 350it [03:51,  1.63it/s]Extractor Estimating: 351it [03:51,  1.66it/s]Extractor Estimating: 352it [03:52,  1.64it/s]Extractor Estimating: 353it [03:52,  1.67it/s]Extractor Estimating: 354it [03:53,  1.66it/s]Extractor Estimating: 355it [03:54,  1.59it/s]Extractor Estimating: 356it [03:54,  1.56it/s]Extractor Estimating: 357it [03:55,  1.58it/s]Extractor Estimating: 358it [03:55,  1.66it/s]Extractor Estimating: 359it [03:56,  1.65it/s]Extractor Estimating: 360it [03:57,  1.67it/s]Extractor Estimating: 361it [03:57,  1.69it/s]Extractor Estimating: 362it [03:58,  1.65it/s]Extractor Estimating: 363it [03:59,  1.62it/s]Extractor Estimating: 364it [03:59,  1.60it/s]Extractor Estimating: 365it [04:00,  1.61it/s]Extractor Estimating: 366it [04:00,  1.68it/s]Extractor Estimating: 367it [04:01,  1.65it/s]Extractor Estimating: 368it [04:02,  1.63it/s]Extractor Estimating: 369it [04:02,  1.65it/s]Extractor Estimating: 370it [04:03,  1.65it/s]Extractor Estimating: 371it [04:03,  1.62it/s]Extractor Estimating: 372it [04:04,  1.65it/s]Extractor Estimating: 373it [04:05,  1.66it/s]Extractor Estimating: 374it [04:05,  1.63it/s]Extractor Estimating: 375it [04:06,  1.63it/s]Extractor Estimating: 375it [04:06,  1.52it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7596 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 27419
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27519, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27519, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.230, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.987, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.957, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 83, avg_time 0.958, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 183, avg_time 0.953, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 283, avg_time 2.278, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 66, avg_time 0.959, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 166, avg_time 0.961, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 266, avg_time 0.963, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 49, avg_time 0.945, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 149, avg_time 2.271, loss:nan
g_step 1200, step 249, avg_time 0.952, loss:nan
g_step 1300, step 32, avg_time 0.958, loss:nan
g_step 1400, step 132, avg_time 0.963, loss:nan
g_step 1500, step 232, avg_time 0.962, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 15, avg_time 2.259, loss:nan
g_step 1700, step 115, avg_time 0.954, loss:nan
g_step 1800, step 215, avg_time 0.960, loss:nan
g_step 1900, step 315, avg_time 0.962, loss:nan
g_step 2000, step 98, avg_time 0.950, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 198, avg_time 2.267, loss:nan
g_step 2200, step 298, avg_time 0.963, loss:nan
g_step 2300, step 81, avg_time 0.963, loss:nan
g_step 2400, step 181, avg_time 0.949, loss:nan
g_step 2500, step 281, avg_time 0.958, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 64, avg_time 2.259, loss:nan
g_step 2700, step 164, avg_time 0.966, loss:nan
g_step 2800, step 264, avg_time 0.951, loss:nan
g_step 2900, step 47, avg_time 0.963, loss:nan
g_step 3000, step 147, avg_time 0.960, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 247, avg_time 2.258, loss:nan
g_step 3200, step 30, avg_time 0.952, loss:nan
g_step 3300, step 130, avg_time 0.957, loss:nan
g_step 3400, step 230, avg_time 0.954, loss:nan
g_step 3500, step 13, avg_time 0.956, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 113, avg_time 2.272, loss:nan
g_step 3700, step 213, avg_time 0.948, loss:nan
g_step 3800, step 313, avg_time 0.960, loss:nan
g_step 3900, step 96, avg_time 0.943, loss:nan
g_step 4000, step 196, avg_time 0.964, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 296, avg_time 2.261, loss:nan
g_step 4200, step 79, avg_time 0.956, loss:nan
g_step 4300, step 179, avg_time 0.954, loss:nan
g_step 4400, step 279, avg_time 0.957, loss:nan
g_step 4500, step 62, avg_time 0.963, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 162, avg_time 2.263, loss:nan
g_step 4700, step 262, avg_time 0.947, loss:nan
g_step 4800, step 45, avg_time 0.942, loss:nan
g_step 4900, step 145, avg_time 0.953, loss:nan
g_step 5000, step 245, avg_time 0.963, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 28, avg_time 2.268, loss:nan
g_step 5200, step 128, avg_time 0.951, loss:nan
g_step 5300, step 228, avg_time 0.961, loss:nan
g_step 5400, step 11, avg_time 0.943, loss:nan
g_step 5500, step 111, avg_time 0.952, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 211, avg_time 2.258, loss:nan
g_step 5700, step 311, avg_time 0.960, loss:nan
g_step 5800, step 94, avg_time 0.950, loss:nan
g_step 5900, step 194, avg_time 0.952, loss:nan
g_step 6000, step 294, avg_time 0.947, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 77, avg_time 2.283, loss:nan
g_step 6200, step 177, avg_time 0.954, loss:nan
g_step 6300, step 277, avg_time 0.946, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 01:38:58 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 01:38:58 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_01-38-57_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 01:38:59 - WARNING - datasets.builder -   Using custom data configuration default-33296e752a247ea5
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-33296e752a247ea5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 01:38:59,335 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:38:59,336 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 01:38:59,336 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:38:59,337 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 01:38:59,343 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:38:59,347 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:38:59,347 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:38:59,347 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:38:59,348 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:38:59,348 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:38:59,348 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 01:38:59,684 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 01:39:02,750 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 01:39:02,758 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-33296e752a247ea5/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 01:39:02 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14a8dc806290> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.11ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.93ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.20ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.37ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.47ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.51ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.56ba/s]100%|██████████| 8/8 [00:01<00:00,  4.98ba/s]100%|██████████| 8/8 [00:01<00:00,  4.52ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.15ba/s] 40%|████      | 2/5 [00:00<00:00,  4.38ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.52ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.87ba/s]100%|██████████| 5/5 [00:01<00:00,  4.25ba/s]100%|██████████| 5/5 [00:01<00:00,  4.07ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.44ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.99ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.34ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.44ba/s]100%|██████████| 8/8 [00:00<00:00, 10.62ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.94ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.25ba/s]100%|██████████| 5/5 [00:00<00:00, 10.79ba/s]100%|██████████| 5/5 [00:00<00:00, 10.56ba/s]
[INFO|trainer.py:414] 2023-08-28 01:39:07,357 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 01:39:07,373 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 01:39:07,373 >>   Num examples = 7740
[INFO|trainer.py:1149] 2023-08-28 01:39:07,373 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 01:39:07,373 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 01:39:07,373 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 01:39:07,373 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 01:39:07,373 >>   Total optimization steps = 605
  0%|          | 0/605 [00:00<?, ?it/s]  0%|          | 1/605 [00:00<02:50,  3.54it/s]  0%|          | 2/605 [00:00<02:48,  3.59it/s]  0%|          | 3/605 [00:00<02:47,  3.60it/s]  1%|          | 4/605 [00:01<02:46,  3.61it/s]  1%|          | 5/605 [00:01<02:45,  3.62it/s]  1%|          | 6/605 [00:01<02:45,  3.62it/s]  1%|          | 7/605 [00:01<02:45,  3.61it/s]  1%|▏         | 8/605 [00:02<02:45,  3.62it/s]  1%|▏         | 9/605 [00:02<02:44,  3.62it/s]  2%|▏         | 10/605 [00:02<02:44,  3.62it/s]  2%|▏         | 11/605 [00:03<02:44,  3.62it/s]  2%|▏         | 12/605 [00:03<02:44,  3.62it/s]  2%|▏         | 13/605 [00:03<02:43,  3.62it/s]  2%|▏         | 14/605 [00:03<02:43,  3.62it/s]  2%|▏         | 15/605 [00:04<02:42,  3.62it/s]  3%|▎         | 16/605 [00:04<02:42,  3.62it/s]  3%|▎         | 17/605 [00:04<02:42,  3.62it/s]  3%|▎         | 18/605 [00:04<02:42,  3.61it/s]  3%|▎         | 19/605 [00:05<02:42,  3.61it/s]  3%|▎         | 20/605 [00:05<02:41,  3.62it/s]  3%|▎         | 21/605 [00:05<02:41,  3.62it/s]  4%|▎         | 22/605 [00:06<02:41,  3.62it/s]  4%|▍         | 23/605 [00:06<02:40,  3.62it/s]  4%|▍         | 24/605 [00:06<02:40,  3.61it/s]  4%|▍         | 25/605 [00:06<02:40,  3.61it/s]  4%|▍         | 26/605 [00:07<02:39,  3.62it/s]  4%|▍         | 27/605 [00:07<02:39,  3.62it/s]  5%|▍         | 28/605 [00:07<02:39,  3.62it/s]  5%|▍         | 29/605 [00:08<02:39,  3.61it/s]  5%|▍         | 30/605 [00:08<02:39,  3.62it/s]  5%|▌         | 31/605 [00:08<02:38,  3.61it/s]  5%|▌         | 32/605 [00:08<02:38,  3.62it/s]  5%|▌         | 33/605 [00:09<02:38,  3.62it/s]  6%|▌         | 34/605 [00:09<02:37,  3.62it/s]  6%|▌         | 35/605 [00:09<02:37,  3.62it/s]  6%|▌         | 36/605 [00:09<02:37,  3.62it/s]  6%|▌         | 37/605 [00:10<02:36,  3.62it/s]  6%|▋         | 38/605 [00:10<02:36,  3.62it/s]  6%|▋         | 39/605 [00:10<02:36,  3.62it/s]  7%|▋         | 40/605 [00:11<02:36,  3.61it/s]  7%|▋         | 41/605 [00:11<02:36,  3.61it/s]  7%|▋         | 42/605 [00:11<02:35,  3.61it/s]  7%|▋         | 43/605 [00:11<02:35,  3.62it/s]  7%|▋         | 44/605 [00:12<02:34,  3.62it/s]  7%|▋         | 45/605 [00:12<02:34,  3.62it/s]  8%|▊         | 46/605 [00:12<02:34,  3.61it/s]  8%|▊         | 47/605 [00:12<02:34,  3.61it/s]  8%|▊         | 48/605 [00:13<02:34,  3.61it/s]  8%|▊         | 49/605 [00:13<02:33,  3.61it/s]  8%|▊         | 50/605 [00:13<02:33,  3.62it/s]  8%|▊         | 51/605 [00:14<02:33,  3.61it/s]  9%|▊         | 52/605 [00:14<02:33,  3.61it/s]  9%|▉         | 53/605 [00:14<02:32,  3.61it/s]  9%|▉         | 54/605 [00:14<02:32,  3.61it/s]  9%|▉         | 55/605 [00:15<02:32,  3.61it/s]  9%|▉         | 56/605 [00:15<02:31,  3.61it/s]  9%|▉         | 57/605 [00:15<02:32,  3.60it/s] 10%|▉         | 58/605 [00:16<02:32,  3.59it/s] 10%|▉         | 59/605 [00:16<02:32,  3.58it/s] 10%|▉         | 60/605 [00:16<02:32,  3.58it/s] 10%|█         | 61/605 [00:16<02:32,  3.58it/s] 10%|█         | 62/605 [00:17<02:32,  3.55it/s] 10%|█         | 63/605 [00:17<02:32,  3.55it/s] 11%|█         | 64/605 [00:17<02:32,  3.56it/s] 11%|█         | 65/605 [00:18<02:31,  3.56it/s] 11%|█         | 66/605 [00:18<02:31,  3.55it/s] 11%|█         | 67/605 [00:18<02:31,  3.56it/s] 11%|█         | 68/605 [00:18<02:30,  3.56it/s] 11%|█▏        | 69/605 [00:19<02:30,  3.56it/s] 12%|█▏        | 70/605 [00:19<02:30,  3.56it/s] 12%|█▏        | 71/605 [00:19<02:29,  3.56it/s] 12%|█▏        | 72/605 [00:19<02:29,  3.56it/s] 12%|█▏        | 73/605 [00:20<02:30,  3.54it/s] 12%|█▏        | 74/605 [00:20<02:29,  3.55it/s] 12%|█▏        | 75/605 [00:20<02:29,  3.55it/s] 13%|█▎        | 76/605 [00:21<02:28,  3.55it/s] 13%|█▎        | 77/605 [00:21<02:28,  3.55it/s] 13%|█▎        | 78/605 [00:21<02:28,  3.55it/s] 13%|█▎        | 79/605 [00:21<02:28,  3.55it/s] 13%|█▎        | 80/605 [00:22<02:27,  3.55it/s] 13%|█▎        | 81/605 [00:22<02:27,  3.55it/s] 14%|█▎        | 82/605 [00:22<02:27,  3.56it/s] 14%|█▎        | 83/605 [00:23<02:26,  3.55it/s] 14%|█▍        | 84/605 [00:23<02:26,  3.56it/s] 14%|█▍        | 85/605 [00:23<02:26,  3.56it/s] 14%|█▍        | 86/605 [00:23<02:25,  3.56it/s] 14%|█▍        | 87/605 [00:24<02:25,  3.56it/s] 15%|█▍        | 88/605 [00:24<02:25,  3.55it/s] 15%|█▍        | 89/605 [00:24<02:25,  3.55it/s] 15%|█▍        | 90/605 [00:25<02:25,  3.55it/s] 15%|█▌        | 91/605 [00:25<02:24,  3.55it/s] 15%|█▌        | 92/605 [00:25<02:24,  3.55it/s] 15%|█▌        | 93/605 [00:25<02:24,  3.55it/s] 16%|█▌        | 94/605 [00:26<02:23,  3.55it/s] 16%|█▌        | 95/605 [00:26<02:23,  3.56it/s] 16%|█▌        | 96/605 [00:26<02:23,  3.55it/s] 16%|█▌        | 97/605 [00:27<02:22,  3.55it/s] 16%|█▌        | 98/605 [00:27<02:22,  3.56it/s] 16%|█▋        | 99/605 [00:27<02:23,  3.53it/s] 17%|█▋        | 100/605 [00:27<02:22,  3.53it/s] 17%|█▋        | 101/605 [00:28<02:22,  3.54it/s] 17%|█▋        | 102/605 [00:28<02:21,  3.54it/s] 17%|█▋        | 103/605 [00:28<02:21,  3.55it/s] 17%|█▋        | 104/605 [00:29<02:21,  3.55it/s] 17%|█▋        | 105/605 [00:29<02:20,  3.56it/s] 18%|█▊        | 106/605 [00:29<02:20,  3.56it/s] 18%|█▊        | 107/605 [00:29<02:19,  3.56it/s] 18%|█▊        | 108/605 [00:30<02:19,  3.56it/s] 18%|█▊        | 109/605 [00:30<02:19,  3.56it/s] 18%|█▊        | 110/605 [00:30<02:20,  3.53it/s] 18%|█▊        | 111/605 [00:30<02:19,  3.53it/s] 19%|█▊        | 112/605 [00:31<02:19,  3.54it/s] 19%|█▊        | 113/605 [00:31<02:18,  3.54it/s] 19%|█▉        | 114/605 [00:31<02:18,  3.55it/s] 19%|█▉        | 115/605 [00:32<02:17,  3.55it/s] 19%|█▉        | 116/605 [00:32<02:17,  3.56it/s] 19%|█▉        | 117/605 [00:32<02:17,  3.56it/s] 20%|█▉        | 118/605 [00:32<02:16,  3.56it/s] 20%|█▉        | 119/605 [00:33<02:16,  3.56it/s] 20%|█▉        | 120/605 [00:33<02:16,  3.56it/s] 20%|██        | 121/605 [00:33<02:14,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 01:39:41,152 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:39:41,152 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 01:39:41,152 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.80it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.76it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.95it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.81it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.11it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.63it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.31it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.07it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.21it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.46it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.63it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.60it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.42it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.29it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.15it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.93it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.94it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.18it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.31it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.49it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.53it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.39it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.21it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.16it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.08it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.08it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.21it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.40it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.40it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.32it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.31it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.22it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.08it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.03it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.04it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.17it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.32it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.41it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.46it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.43it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.34it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.12it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.09it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.11it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.09it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.27it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.39it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.50it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.39it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.30it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.11it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.08it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.16it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.17it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.25it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.35it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.45it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.46it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.30it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.12it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.15it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.13it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.17it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.29it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.42it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.40it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.39it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.28it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.13it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.11it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.18it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.27it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.44it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.47it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.38it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.26it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.18it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.09it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.07it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.09it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.16it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.40it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.45it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.43it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.18it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.18it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.14it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.08it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.21it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.30it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.36it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.43it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.36it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.08it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.14it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.08it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.15it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.18it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.28it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.39it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.13it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.48it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.36it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.28it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.24it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.21it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.16it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.24it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.35it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.40it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.21it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.21it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.20it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.17it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.10it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.17it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.27it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.31it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.29it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.25it/s][A                                                 
                                                 [A 20%|██        | 121/605 [00:47<02:14,  3.60it/s]
100%|██████████| 611/611 [00:13<00:00, 44.25it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:39:54,973 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121
[INFO|configuration_utils.py:351] 2023-08-28 01:39:54,995 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:39:57,010 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:39:57,038 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:39:57,050 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121/special_tokens_map.json
 20%|██        | 122/605 [00:50<41:19,  5.13s/it] 20%|██        | 123/605 [00:50<29:32,  3.68s/it] 20%|██        | 124/605 [00:50<21:21,  2.66s/it] 21%|██        | 125/605 [00:51<15:36,  1.95s/it] 21%|██        | 126/605 [00:51<11:34,  1.45s/it] 21%|██        | 127/605 [00:51<08:45,  1.10s/it] 21%|██        | 128/605 [00:51<06:47,  1.17it/s] 21%|██▏       | 129/605 [00:52<05:24,  1.47it/s] 21%|██▏       | 130/605 [00:52<04:26,  1.78it/s] 22%|██▏       | 131/605 [00:52<03:46,  2.09it/s] 22%|██▏       | 132/605 [00:53<03:18,  2.39it/s] 22%|██▏       | 133/605 [00:53<02:58,  2.65it/s] 22%|██▏       | 134/605 [00:53<02:44,  2.87it/s] 22%|██▏       | 135/605 [00:53<02:34,  3.05it/s] 22%|██▏       | 136/605 [00:54<02:27,  3.19it/s] 23%|██▎       | 137/605 [00:54<02:22,  3.29it/s] 23%|██▎       | 138/605 [00:54<02:18,  3.37it/s] 23%|██▎       | 139/605 [00:55<02:16,  3.42it/s] 23%|██▎       | 140/605 [00:55<02:14,  3.45it/s] 23%|██▎       | 141/605 [00:55<02:13,  3.48it/s] 23%|██▎       | 142/605 [00:55<02:12,  3.50it/s] 24%|██▎       | 143/605 [00:56<02:11,  3.51it/s] 24%|██▍       | 144/605 [00:56<02:10,  3.53it/s] 24%|██▍       | 145/605 [00:56<02:10,  3.54it/s] 24%|██▍       | 146/605 [00:57<02:09,  3.55it/s] 24%|██▍       | 147/605 [00:57<02:09,  3.55it/s] 24%|██▍       | 148/605 [00:57<02:08,  3.55it/s] 25%|██▍       | 149/605 [00:57<02:08,  3.56it/s] 25%|██▍       | 150/605 [00:58<02:07,  3.56it/s] 25%|██▍       | 151/605 [00:58<02:08,  3.54it/s] 25%|██▌       | 152/605 [00:58<02:07,  3.55it/s] 25%|██▌       | 153/605 [00:58<02:07,  3.55it/s] 25%|██▌       | 154/605 [00:59<02:07,  3.54it/s] 26%|██▌       | 155/605 [00:59<02:06,  3.54it/s] 26%|██▌       | 156/605 [00:59<02:06,  3.55it/s] 26%|██▌       | 157/605 [01:00<02:06,  3.55it/s] 26%|██▌       | 158/605 [01:00<02:05,  3.55it/s] 26%|██▋       | 159/605 [01:00<02:05,  3.56it/s] 26%|██▋       | 160/605 [01:00<02:05,  3.56it/s] 27%|██▋       | 161/605 [01:01<02:04,  3.56it/s] 27%|██▋       | 162/605 [01:01<02:04,  3.55it/s] 27%|██▋       | 163/605 [01:01<02:04,  3.55it/s] 27%|██▋       | 164/605 [01:02<02:03,  3.56it/s] 27%|██▋       | 165/605 [01:02<02:03,  3.56it/s] 27%|██▋       | 166/605 [01:02<02:03,  3.56it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 28%|██▊       | 167/605 [01:02<02:05,  3.48it/s] 28%|██▊       | 168/605 [01:03<02:05,  3.49it/s] 28%|██▊       | 169/605 [01:03<02:04,  3.51it/s] 28%|██▊       | 170/605 [01:03<02:03,  3.52it/s] 28%|██▊       | 171/605 [01:04<02:02,  3.53it/s] 28%|██▊       | 172/605 [01:04<02:02,  3.54it/s] 29%|██▊       | 173/605 [01:04<02:02,  3.54it/s] 29%|██▉       | 174/605 [01:04<02:01,  3.54it/s] 29%|██▉       | 175/605 [01:05<02:01,  3.55it/s] 29%|██▉       | 176/605 [01:05<02:00,  3.55it/s] 29%|██▉       | 177/605 [01:05<02:00,  3.55it/s] 29%|██▉       | 178/605 [01:06<02:00,  3.55it/s] 30%|██▉       | 179/605 [01:06<02:00,  3.55it/s] 30%|██▉       | 180/605 [01:06<01:59,  3.55it/s] 30%|██▉       | 181/605 [01:06<01:59,  3.55it/s] 30%|███       | 182/605 [01:07<01:59,  3.55it/s] 30%|███       | 183/605 [01:07<01:58,  3.56it/s] 30%|███       | 184/605 [01:07<01:59,  3.53it/s] 31%|███       | 185/605 [01:08<01:58,  3.54it/s] 31%|███       | 186/605 [01:08<01:58,  3.54it/s] 31%|███       | 187/605 [01:08<01:57,  3.54it/s] 31%|███       | 188/605 [01:08<01:57,  3.54it/s] 31%|███       | 189/605 [01:09<01:57,  3.54it/s] 31%|███▏      | 190/605 [01:09<01:57,  3.55it/s] 32%|███▏      | 191/605 [01:09<01:56,  3.55it/s] 32%|███▏      | 192/605 [01:09<01:56,  3.55it/s] 32%|███▏      | 193/605 [01:10<01:55,  3.56it/s] 32%|███▏      | 194/605 [01:10<01:55,  3.56it/s] 32%|███▏      | 195/605 [01:10<01:55,  3.54it/s] 32%|███▏      | 196/605 [01:11<01:55,  3.54it/s] 33%|███▎      | 197/605 [01:11<01:54,  3.55it/s] 33%|███▎      | 198/605 [01:11<01:54,  3.55it/s] 33%|███▎      | 199/605 [01:11<01:54,  3.55it/s] 33%|███▎      | 200/605 [01:12<01:54,  3.55it/s] 33%|███▎      | 201/605 [01:12<01:53,  3.55it/s] 33%|███▎      | 202/605 [01:12<01:53,  3.55it/s] 34%|███▎      | 203/605 [01:13<01:53,  3.55it/s] 34%|███▎      | 204/605 [01:13<01:52,  3.56it/s] 34%|███▍      | 205/605 [01:13<01:52,  3.56it/s] 34%|███▍      | 206/605 [01:13<01:52,  3.54it/s] 34%|███▍      | 207/605 [01:14<01:52,  3.55it/s] 34%|███▍      | 208/605 [01:14<01:51,  3.55it/s] 35%|███▍      | 209/605 [01:14<01:51,  3.55it/s] 35%|███▍      | 210/605 [01:15<01:51,  3.55it/s] 35%|███▍      | 211/605 [01:15<01:51,  3.55it/s] 35%|███▌      | 212/605 [01:15<01:50,  3.55it/s] 35%|███▌      | 213/605 [01:15<01:50,  3.55it/s] 35%|███▌      | 214/605 [01:16<01:50,  3.55it/s] 36%|███▌      | 215/605 [01:16<01:49,  3.56it/s] 36%|███▌      | 216/605 [01:16<01:49,  3.56it/s] 36%|███▌      | 217/605 [01:17<01:49,  3.55it/s] 36%|███▌      | 218/605 [01:17<01:48,  3.55it/s] 36%|███▌      | 219/605 [01:17<01:48,  3.56it/s] 36%|███▋      | 220/605 [01:17<01:48,  3.56it/s] 37%|███▋      | 221/605 [01:18<01:48,  3.55it/s] 37%|███▋      | 222/605 [01:18<01:47,  3.55it/s] 37%|███▋      | 223/605 [01:18<01:47,  3.55it/s] 37%|███▋      | 224/605 [01:19<01:47,  3.55it/s] 37%|███▋      | 225/605 [01:19<01:47,  3.55it/s] 37%|███▋      | 226/605 [01:19<01:46,  3.55it/s] 38%|███▊      | 227/605 [01:19<01:46,  3.55it/s] 38%|███▊      | 228/605 [01:20<01:46,  3.54it/s] 38%|███▊      | 229/605 [01:20<01:46,  3.54it/s] 38%|███▊      | 230/605 [01:20<01:45,  3.55it/s] 38%|███▊      | 231/605 [01:20<01:45,  3.55it/s] 38%|███▊      | 232/605 [01:21<01:45,  3.55it/s] 39%|███▊      | 233/605 [01:21<01:44,  3.55it/s] 39%|███▊      | 234/605 [01:21<01:44,  3.55it/s] 39%|███▉      | 235/605 [01:22<01:44,  3.55it/s] 39%|███▉      | 236/605 [01:22<01:43,  3.55it/s] 39%|███▉      | 237/605 [01:22<01:43,  3.55it/s] 39%|███▉      | 238/605 [01:22<01:43,  3.55it/s] 40%|███▉      | 239/605 [01:23<01:42,  3.56it/s] 40%|███▉      | 240/605 [01:23<01:42,  3.56it/s] 40%|███▉      | 241/605 [01:23<01:42,  3.56it/s] 40%|████      | 242/605 [01:24<01:40,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 01:40:31,432 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:40:31,432 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 01:40:31,432 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.7959, 'eval_samples_per_second': 353.873, 'eval_steps_per_second': 44.288, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.09it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.41it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.67it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.60it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.06it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.49it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.28it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.15it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.12it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.28it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.43it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.53it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.46it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.34it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.15it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.00it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.99it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.14it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.28it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.35it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.32it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.36it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.27it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.14it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.98it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.08it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.14it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.37it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.42it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.37it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.36it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.26it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.10it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.98it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.06it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.09it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.33it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.34it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.39it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.27it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.30it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.07it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.12it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.10it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.13it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.31it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.43it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.40it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.30it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.14it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.10it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.10it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.12it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.17it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.26it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.45it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.38it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.25it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.13it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.04it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.09it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.12it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.25it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.34it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.43it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.38it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.26it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.13it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.02it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.04it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.09it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.10it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.23it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.37it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.37it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.28it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.10it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.04it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.02it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.05it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.22it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.28it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.27it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.27it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.25it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.23it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.15it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.15it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.19it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.28it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.33it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.37it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.33it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.29it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.15it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.10it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.12it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.07it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.21it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.26it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.31it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.28it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.19it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.22it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.20it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.18it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.11it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.18it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.26it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.35it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.32it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.18it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.22it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.23it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.12it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.12it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.17it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.17it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.30it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.30it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.26it/s][A                                                 
                                                 [A 40%|████      | 242/605 [01:37<01:40,  3.61it/s]
100%|██████████| 611/611 [00:13<00:00, 44.26it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:40:45,270 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242
[INFO|configuration_utils.py:351] 2023-08-28 01:40:45,289 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:40:47,865 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:40:47,884 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:40:47,893 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242/special_tokens_map.json
 40%|████      | 243/605 [01:41<32:06,  5.32s/it] 40%|████      | 244/605 [01:41<22:55,  3.81s/it] 40%|████      | 245/605 [01:41<16:30,  2.75s/it] 41%|████      | 246/605 [01:41<12:02,  2.01s/it] 41%|████      | 247/605 [01:42<08:54,  1.49s/it] 41%|████      | 248/605 [01:42<06:42,  1.13s/it] 41%|████      | 249/605 [01:42<05:11,  1.14it/s] 41%|████▏     | 250/605 [01:43<04:07,  1.44it/s] 41%|████▏     | 251/605 [01:43<03:22,  1.75it/s] 42%|████▏     | 252/605 [01:43<02:50,  2.07it/s] 42%|████▏     | 253/605 [01:43<02:28,  2.36it/s] 42%|████▏     | 254/605 [01:44<02:13,  2.63it/s] 42%|████▏     | 255/605 [01:44<02:02,  2.85it/s] 42%|████▏     | 256/605 [01:44<01:55,  3.03it/s] 42%|████▏     | 257/605 [01:45<01:49,  3.17it/s] 43%|████▎     | 258/605 [01:45<01:45,  3.28it/s] 43%|████▎     | 259/605 [01:45<01:43,  3.35it/s] 43%|████▎     | 260/605 [01:45<01:41,  3.41it/s] 43%|████▎     | 261/605 [01:46<01:39,  3.45it/s] 43%|████▎     | 262/605 [01:46<01:38,  3.48it/s] 43%|████▎     | 263/605 [01:46<01:37,  3.51it/s] 44%|████▎     | 264/605 [01:47<01:36,  3.52it/s] 44%|████▍     | 265/605 [01:47<01:36,  3.54it/s] 44%|████▍     | 266/605 [01:47<01:36,  3.53it/s] 44%|████▍     | 267/605 [01:47<01:35,  3.54it/s] 44%|████▍     | 268/605 [01:48<01:35,  3.55it/s] 44%|████▍     | 269/605 [01:48<01:34,  3.55it/s] 45%|████▍     | 270/605 [01:48<01:34,  3.55it/s] 45%|████▍     | 271/605 [01:49<01:34,  3.55it/s] 45%|████▍     | 272/605 [01:49<01:33,  3.55it/s] 45%|████▌     | 273/605 [01:49<01:33,  3.56it/s] 45%|████▌     | 274/605 [01:49<01:33,  3.55it/s] 45%|████▌     | 275/605 [01:50<01:32,  3.55it/s] 46%|████▌     | 276/605 [01:50<01:32,  3.55it/s] 46%|████▌     | 277/605 [01:50<01:32,  3.54it/s] 46%|████▌     | 278/605 [01:50<01:32,  3.55it/s] 46%|████▌     | 279/605 [01:51<01:31,  3.56it/s] 46%|████▋     | 280/605 [01:51<01:31,  3.56it/s] 46%|████▋     | 281/605 [01:51<01:30,  3.56it/s] 47%|████▋     | 282/605 [01:52<01:30,  3.57it/s] 47%|████▋     | 283/605 [01:52<01:29,  3.59it/s] 47%|████▋     | 284/605 [01:52<01:29,  3.60it/s] 47%|████▋     | 285/605 [01:52<01:28,  3.60it/s] 47%|████▋     | 286/605 [01:53<01:28,  3.60it/s] 47%|████▋     | 287/605 [01:53<01:28,  3.60it/s] 48%|████▊     | 288/605 [01:53<01:27,  3.61it/s] 48%|████▊     | 289/605 [01:54<01:27,  3.61it/s] 48%|████▊     | 290/605 [01:54<01:27,  3.61it/s] 48%|████▊     | 291/605 [01:54<01:26,  3.61it/s] 48%|████▊     | 292/605 [01:54<01:26,  3.61it/s] 48%|████▊     | 293/605 [01:55<01:26,  3.61it/s] 49%|████▊     | 294/605 [01:55<01:26,  3.62it/s] 49%|████▉     | 295/605 [01:55<01:25,  3.61it/s] 49%|████▉     | 296/605 [01:55<01:25,  3.62it/s] 49%|████▉     | 297/605 [01:56<01:25,  3.61it/s] 49%|████▉     | 298/605 [01:56<01:24,  3.61it/s] 49%|████▉     | 299/605 [01:56<01:25,  3.59it/s] 50%|████▉     | 300/605 [01:57<01:24,  3.60it/s] 50%|████▉     | 301/605 [01:57<01:24,  3.60it/s] 50%|████▉     | 302/605 [01:57<01:24,  3.61it/s] 50%|█████     | 303/605 [01:57<01:23,  3.61it/s] 50%|█████     | 304/605 [01:58<01:23,  3.61it/s] 50%|█████     | 305/605 [01:58<01:23,  3.61it/s] 51%|█████     | 306/605 [01:58<01:22,  3.61it/s] 51%|█████     | 307/605 [01:59<01:22,  3.60it/s] 51%|█████     | 308/605 [01:59<01:22,  3.60it/s] 51%|█████     | 309/605 [01:59<01:22,  3.61it/s] 51%|█████     | 310/605 [01:59<01:22,  3.59it/s] 51%|█████▏    | 311/605 [02:00<01:21,  3.60it/s] 52%|█████▏    | 312/605 [02:00<01:21,  3.61it/s] 52%|█████▏    | 313/605 [02:00<01:20,  3.61it/s] 52%|█████▏    | 314/605 [02:00<01:20,  3.61it/s] 52%|█████▏    | 315/605 [02:01<01:20,  3.61it/s] 52%|█████▏    | 316/605 [02:01<01:20,  3.61it/s] 52%|█████▏    | 317/605 [02:01<01:19,  3.61it/s] 53%|█████▎    | 318/605 [02:02<01:19,  3.62it/s] 53%|█████▎    | 319/605 [02:02<01:19,  3.62it/s] 53%|█████▎    | 320/605 [02:02<01:18,  3.61it/s] 53%|█████▎    | 321/605 [02:02<01:18,  3.60it/s] 53%|█████▎    | 322/605 [02:03<01:18,  3.60it/s] 53%|█████▎    | 323/605 [02:03<01:18,  3.60it/s] 54%|█████▎    | 324/605 [02:03<01:17,  3.61it/s] 54%|█████▎    | 325/605 [02:04<01:17,  3.61it/s] 54%|█████▍    | 326/605 [02:04<01:17,  3.61it/s] 54%|█████▍    | 327/605 [02:04<01:17,  3.61it/s] 54%|█████▍    | 328/605 [02:04<01:16,  3.61it/s] 54%|█████▍    | 329/605 [02:05<01:16,  3.61it/s] 55%|█████▍    | 330/605 [02:05<01:16,  3.61it/s] 55%|█████▍    | 331/605 [02:05<01:15,  3.61it/s] 55%|█████▍    | 332/605 [02:05<01:15,  3.60it/s] 55%|█████▌    | 333/605 [02:06<01:15,  3.60it/s] 55%|█████▌    | 334/605 [02:06<01:15,  3.61it/s] 55%|█████▌    | 335/605 [02:06<01:14,  3.61it/s] 56%|█████▌    | 336/605 [02:07<01:14,  3.61it/s] 56%|█████▌    | 337/605 [02:07<01:14,  3.61it/s] 56%|█████▌    | 338/605 [02:07<01:14,  3.61it/s] 56%|█████▌    | 339/605 [02:07<01:13,  3.61it/s] 56%|█████▌    | 340/605 [02:08<01:13,  3.61it/s] 56%|█████▋    | 341/605 [02:08<01:13,  3.61it/s] 57%|█████▋    | 342/605 [02:08<01:12,  3.61it/s] 57%|█████▋    | 343/605 [02:09<01:12,  3.60it/s] 57%|█████▋    | 344/605 [02:09<01:12,  3.60it/s] 57%|█████▋    | 345/605 [02:09<01:12,  3.60it/s] 57%|█████▋    | 346/605 [02:09<01:11,  3.60it/s] 57%|█████▋    | 347/605 [02:10<01:11,  3.60it/s] 58%|█████▊    | 348/605 [02:10<01:11,  3.61it/s] 58%|█████▊    | 349/605 [02:10<01:10,  3.61it/s] 58%|█████▊    | 350/605 [02:10<01:10,  3.61it/s] 58%|█████▊    | 351/605 [02:11<01:10,  3.61it/s] 58%|█████▊    | 352/605 [02:11<01:10,  3.61it/s] 58%|█████▊    | 353/605 [02:11<01:09,  3.61it/s] 59%|█████▊    | 354/605 [02:12<01:09,  3.59it/s] 59%|█████▊    | 355/605 [02:12<01:09,  3.60it/s] 59%|█████▉    | 356/605 [02:12<01:09,  3.61it/s] 59%|█████▉    | 357/605 [02:12<01:08,  3.61it/s] 59%|█████▉    | 358/605 [02:13<01:08,  3.61it/s] 59%|█████▉    | 359/605 [02:13<01:08,  3.61it/s] 60%|█████▉    | 360/605 [02:13<01:07,  3.61it/s] 60%|█████▉    | 361/605 [02:14<01:07,  3.61it/s] 60%|█████▉    | 362/605 [02:14<01:07,  3.61it/s] 60%|██████    | 363/605 [02:14<01:06,  3.66it/s][INFO|trainer.py:2140] 2023-08-28 01:41:21,932 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:41:21,932 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 01:41:21,932 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8078, 'eval_samples_per_second': 353.568, 'eval_steps_per_second': 44.25, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.43it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.83it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.58it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.53it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.93it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.49it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.33it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.04it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.13it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.42it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.54it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.54it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.40it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.23it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.15it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.99it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.95it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.11it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.31it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.46it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.41it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.06it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.31it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.20it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.94it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.11it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.34it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.38it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.42it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.30it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.20it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.16it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.96it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.96it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.10it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.30it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.39it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.40it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.36it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.29it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.19it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.06it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.05it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.10it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.27it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.33it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.45it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.37it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.27it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.18it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.13it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.07it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.10it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.19it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.37it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.43it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.38it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.26it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.15it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.08it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.11it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.16it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.27it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.33it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.43it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.35it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.26it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.08it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.11it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.15it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.30it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.38it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.40it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.35it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.29it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.08it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.96it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.12it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.29it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.33it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.35it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.31it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.27it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.25it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.09it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.04it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.14it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.25it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.35it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.38it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.33it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.32it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.19it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.11it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.04it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.12it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.20it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.42it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.37it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.26it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.32it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.24it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.01it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.10it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.12it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.28it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.41it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.33it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.30it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.26it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.21it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.14it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.98it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.07it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.34it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.23it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.28it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.30it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.27it/s][A                                                 
                                                 [A 60%|██████    | 363/605 [02:28<01:06,  3.66it/s]
100%|██████████| 611/611 [00:13<00:00, 44.27it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:41:35,761 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363
[INFO|configuration_utils.py:351] 2023-08-28 01:41:35,782 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:41:37,882 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:41:38,009 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:41:38,069 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363/special_tokens_map.json
 60%|██████    | 364/605 [02:31<21:18,  5.31s/it] 60%|██████    | 365/605 [02:31<15:11,  3.80s/it] 60%|██████    | 366/605 [02:32<10:55,  2.74s/it] 61%|██████    | 367/605 [02:32<07:56,  2.00s/it] 61%|██████    | 368/605 [02:32<05:52,  1.49s/it] 61%|██████    | 369/605 [02:33<04:25,  1.13s/it] 61%|██████    | 370/605 [02:33<03:24,  1.15it/s] 61%|██████▏   | 371/605 [02:33<02:42,  1.44it/s] 61%|██████▏   | 372/605 [02:33<02:13,  1.75it/s] 62%|██████▏   | 373/605 [02:34<01:52,  2.06it/s] 62%|██████▏   | 374/605 [02:34<01:37,  2.36it/s] 62%|██████▏   | 375/605 [02:34<01:27,  2.62it/s] 62%|██████▏   | 376/605 [02:34<01:20,  2.85it/s] 62%|██████▏   | 377/605 [02:35<01:15,  3.03it/s] 62%|██████▏   | 378/605 [02:35<01:11,  3.17it/s] 63%|██████▎   | 379/605 [02:35<01:08,  3.28it/s] 63%|██████▎   | 380/605 [02:36<01:06,  3.36it/s] 63%|██████▎   | 381/605 [02:36<01:05,  3.44it/s] 63%|██████▎   | 382/605 [02:36<01:03,  3.49it/s] 63%|██████▎   | 383/605 [02:36<01:03,  3.50it/s] 63%|██████▎   | 384/605 [02:37<01:02,  3.53it/s] 64%|██████▎   | 385/605 [02:37<01:01,  3.55it/s] 64%|██████▍   | 386/605 [02:37<01:01,  3.57it/s] 64%|██████▍   | 387/605 [02:38<01:00,  3.58it/s] 64%|██████▍   | 388/605 [02:38<01:00,  3.59it/s] 64%|██████▍   | 389/605 [02:38<01:00,  3.60it/s] 64%|██████▍   | 390/605 [02:38<00:59,  3.60it/s] 65%|██████▍   | 391/605 [02:39<00:59,  3.59it/s] 65%|██████▍   | 392/605 [02:39<00:59,  3.60it/s] 65%|██████▍   | 393/605 [02:39<00:58,  3.60it/s] 65%|██████▌   | 394/605 [02:39<00:58,  3.59it/s] 65%|██████▌   | 395/605 [02:40<00:58,  3.59it/s] 65%|██████▌   | 396/605 [02:40<00:58,  3.60it/s] 66%|██████▌   | 397/605 [02:40<00:57,  3.60it/s] 66%|██████▌   | 398/605 [02:41<00:57,  3.61it/s] 66%|██████▌   | 399/605 [02:41<00:57,  3.61it/s] 66%|██████▌   | 400/605 [02:41<00:56,  3.61it/s] 66%|██████▋   | 401/605 [02:41<00:56,  3.61it/s] 66%|██████▋   | 402/605 [02:42<00:56,  3.61it/s] 67%|██████▋   | 403/605 [02:42<00:55,  3.61it/s] 67%|██████▋   | 404/605 [02:42<00:55,  3.61it/s] 67%|██████▋   | 405/605 [02:43<00:55,  3.60it/s] 67%|██████▋   | 406/605 [02:43<00:55,  3.60it/s] 67%|██████▋   | 407/605 [02:43<00:54,  3.61it/s] 67%|██████▋   | 408/605 [02:43<00:54,  3.61it/s] 68%|██████▊   | 409/605 [02:44<00:54,  3.61it/s] 68%|██████▊   | 410/605 [02:44<00:54,  3.60it/s] 68%|██████▊   | 411/605 [02:44<00:53,  3.60it/s] 68%|██████▊   | 412/605 [02:44<00:53,  3.61it/s] 68%|██████▊   | 413/605 [02:45<00:53,  3.61it/s] 68%|██████▊   | 414/605 [02:45<00:52,  3.61it/s] 69%|██████▊   | 415/605 [02:45<00:52,  3.61it/s] 69%|██████▉   | 416/605 [02:46<00:52,  3.58it/s] 69%|██████▉   | 417/605 [02:46<00:52,  3.59it/s] 69%|██████▉   | 418/605 [02:46<00:51,  3.60it/s] 69%|██████▉   | 419/605 [02:46<00:51,  3.60it/s] 69%|██████▉   | 420/605 [02:47<00:51,  3.61it/s] 70%|██████▉   | 421/605 [02:47<00:50,  3.61it/s] 70%|██████▉   | 422/605 [02:47<00:50,  3.61it/s] 70%|██████▉   | 423/605 [02:48<00:50,  3.61it/s] 70%|███████   | 424/605 [02:48<00:50,  3.61it/s] 70%|███████   | 425/605 [02:48<00:49,  3.61it/s] 70%|███████   | 426/605 [02:48<00:49,  3.60it/s] 71%|███████   | 427/605 [02:49<00:49,  3.59it/s] 71%|███████   | 428/605 [02:49<00:49,  3.59it/s] 71%|███████   | 429/605 [02:49<00:48,  3.60it/s] 71%|███████   | 430/605 [02:49<00:48,  3.60it/s] 71%|███████   | 431/605 [02:50<00:48,  3.61it/s] 71%|███████▏  | 432/605 [02:50<00:47,  3.61it/s] 72%|███████▏  | 433/605 [02:50<00:47,  3.61it/s] 72%|███████▏  | 434/605 [02:51<00:47,  3.61it/s] 72%|███████▏  | 435/605 [02:51<00:47,  3.61it/s] 72%|███████▏  | 436/605 [02:51<00:46,  3.61it/s] 72%|███████▏  | 437/605 [02:51<00:46,  3.61it/s] 72%|███████▏  | 438/605 [02:52<00:46,  3.60it/s] 73%|███████▎  | 439/605 [02:52<00:46,  3.60it/s] 73%|███████▎  | 440/605 [02:52<00:45,  3.61it/s] 73%|███████▎  | 441/605 [02:53<00:45,  3.61it/s] 73%|███████▎  | 442/605 [02:53<00:45,  3.61it/s] 73%|███████▎  | 443/605 [02:53<00:44,  3.61it/s] 73%|███████▎  | 444/605 [02:53<00:44,  3.61it/s] 74%|███████▎  | 445/605 [02:54<00:44,  3.61it/s] 74%|███████▎  | 446/605 [02:54<00:44,  3.61it/s] 74%|███████▍  | 447/605 [02:54<00:43,  3.61it/s] 74%|███████▍  | 448/605 [02:54<00:43,  3.61it/s] 74%|███████▍  | 449/605 [02:55<00:43,  3.61it/s] 74%|███████▍  | 450/605 [02:55<00:42,  3.61it/s] 75%|███████▍  | 451/605 [02:55<00:42,  3.61it/s] 75%|███████▍  | 452/605 [02:56<00:42,  3.61it/s] 75%|███████▍  | 453/605 [02:56<00:42,  3.61it/s] 75%|███████▌  | 454/605 [02:56<00:41,  3.61it/s] 75%|███████▌  | 455/605 [02:56<00:41,  3.61it/s] 75%|███████▌  | 456/605 [02:57<00:41,  3.61it/s] 76%|███████▌  | 457/605 [02:57<00:40,  3.61it/s] 76%|███████▌  | 458/605 [02:57<00:40,  3.61it/s] 76%|███████▌  | 459/605 [02:58<00:40,  3.62it/s] 76%|███████▌  | 460/605 [02:58<00:40,  3.60it/s] 76%|███████▌  | 461/605 [02:58<00:39,  3.60it/s] 76%|███████▋  | 462/605 [02:58<00:39,  3.60it/s] 77%|███████▋  | 463/605 [02:59<00:39,  3.60it/s] 77%|███████▋  | 464/605 [02:59<00:39,  3.61it/s] 77%|███████▋  | 465/605 [02:59<00:38,  3.61it/s] 77%|███████▋  | 466/605 [02:59<00:38,  3.61it/s] 77%|███████▋  | 467/605 [03:00<00:38,  3.61it/s] 77%|███████▋  | 468/605 [03:00<00:37,  3.61it/s] 78%|███████▊  | 469/605 [03:00<00:37,  3.61it/s] 78%|███████▊  | 470/605 [03:01<00:37,  3.61it/s] 78%|███████▊  | 471/605 [03:01<00:37,  3.60it/s] 78%|███████▊  | 472/605 [03:01<00:36,  3.60it/s] 78%|███████▊  | 473/605 [03:01<00:36,  3.61it/s] 78%|███████▊  | 474/605 [03:02<00:36,  3.61it/s] 79%|███████▊  | 475/605 [03:02<00:35,  3.61it/s] 79%|███████▊  | 476/605 [03:02<00:35,  3.61it/s] 79%|███████▉  | 477/605 [03:03<00:35,  3.61it/s] 79%|███████▉  | 478/605 [03:03<00:35,  3.61it/s] 79%|███████▉  | 479/605 [03:03<00:34,  3.61it/s] 79%|███████▉  | 480/605 [03:03<00:34,  3.61it/s] 80%|███████▉  | 481/605 [03:04<00:34,  3.61it/s] 80%|███████▉  | 482/605 [03:04<00:34,  3.60it/s] 80%|███████▉  | 483/605 [03:04<00:33,  3.60it/s] 80%|████████  | 484/605 [03:04<00:33,  3.66it/s][INFO|trainer.py:2140] 2023-08-28 01:42:12,311 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:42:12,311 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 01:42:12,311 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.804, 'eval_samples_per_second': 353.665, 'eval_steps_per_second': 44.262, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.16it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.86it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.97it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.92it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.09it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.63it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.25it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.00it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.20it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.38it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.48it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.49it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.46it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.26it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.13it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.92it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.85it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.11it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.25it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.40it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.37it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.35it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.29it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.07it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.96it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.92it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.09it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.24it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.33it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.44it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.39it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.25it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.08it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.97it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.85it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.03it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.23it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.34it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.44it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.41it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.29it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.16it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.05it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 43.99it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.11it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.25it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.38it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.50it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.37it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.28it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.17it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.10it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.01it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.12it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.23it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.39it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.48it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.40it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.32it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.12it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.11it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.02it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.06it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.14it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.37it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.42it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.34it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.32it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.23it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.17it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.05it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.01it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.13it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.34it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.37it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.36it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.34it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.27it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.13it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.08it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.02it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.18it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.24it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.24it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.35it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.27it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.28it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.15it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 43.99it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 43.93it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.14it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.27it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.33it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.21it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.27it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.16it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.14it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.04it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.05it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.20it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.31it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.32it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.34it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.23it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.24it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.16it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.06it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.09it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.23it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.27it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.30it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.36it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.32it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.28it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.12it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.96it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.02it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.03it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 43.99it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.12it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.12it/s][A                                                 
                                                 [A 80%|████████  | 484/605 [03:18<00:33,  3.66it/s]
100%|██████████| 611/611 [00:13<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:42:26,196 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484
[INFO|configuration_utils.py:351] 2023-08-28 01:42:26,297 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:42:28,159 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:42:28,173 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:42:28,185 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484/special_tokens_map.json
 80%|████████  | 485/605 [03:21<10:14,  5.12s/it] 80%|████████  | 486/605 [03:21<07:16,  3.67s/it] 80%|████████  | 487/605 [03:21<05:12,  2.65s/it] 81%|████████  | 488/605 [03:22<03:47,  1.94s/it] 81%|████████  | 489/605 [03:22<02:47,  1.44s/it] 81%|████████  | 490/605 [03:22<02:05,  1.09s/it] 81%|████████  | 491/605 [03:23<01:36,  1.18it/s] 81%|████████▏ | 492/605 [03:23<01:16,  1.47it/s] 81%|████████▏ | 493/605 [03:23<01:02,  1.79it/s] 82%|████████▏ | 494/605 [03:23<00:52,  2.10it/s] 82%|████████▏ | 495/605 [03:24<00:46,  2.39it/s] 82%|████████▏ | 496/605 [03:24<00:41,  2.65it/s] 82%|████████▏ | 497/605 [03:24<00:37,  2.87it/s] 82%|████████▏ | 498/605 [03:25<00:35,  3.04it/s] 82%|████████▏ | 499/605 [03:25<00:33,  3.18it/s] 83%|████████▎ | 500/605 [03:25<00:31,  3.28it/s]                                                  83%|████████▎ | 500/605 [03:25<00:31,  3.28it/s] 83%|████████▎ | 501/605 [03:25<00:30,  3.36it/s] 83%|████████▎ | 502/605 [03:26<00:30,  3.41it/s] 83%|████████▎ | 503/605 [03:26<00:29,  3.46it/s] 83%|████████▎ | 504/605 [03:26<00:28,  3.49it/s] 83%|████████▎ | 505/605 [03:26<00:28,  3.51it/s] 84%|████████▎ | 506/605 [03:27<00:28,  3.53it/s] 84%|████████▍ | 507/605 [03:27<00:27,  3.54it/s] 84%|████████▍ | 508/605 [03:27<00:27,  3.54it/s] 84%|████████▍ | 509/605 [03:28<00:27,  3.54it/s] 84%|████████▍ | 510/605 [03:28<00:26,  3.55it/s] 84%|████████▍ | 511/605 [03:28<00:26,  3.55it/s] 85%|████████▍ | 512/605 [03:28<00:26,  3.53it/s] 85%|████████▍ | 513/605 [03:29<00:25,  3.54it/s] 85%|████████▍ | 514/605 [03:29<00:25,  3.54it/s] 85%|████████▌ | 515/605 [03:29<00:25,  3.55it/s] 85%|████████▌ | 516/605 [03:30<00:25,  3.55it/s] 85%|████████▌ | 517/605 [03:30<00:24,  3.55it/s] 86%|████████▌ | 518/605 [03:30<00:24,  3.56it/s] 86%|████████▌ | 519/605 [03:30<00:24,  3.56it/s] 86%|████████▌ | 520/605 [03:31<00:23,  3.56it/s] 86%|████████▌ | 521/605 [03:31<00:23,  3.56it/s] 86%|████████▋ | 522/605 [03:31<00:23,  3.56it/s] 86%|████████▋ | 523/605 [03:32<00:23,  3.54it/s] 87%|████████▋ | 524/605 [03:32<00:22,  3.54it/s] 87%|████████▋ | 525/605 [03:32<00:22,  3.55it/s] 87%|████████▋ | 526/605 [03:32<00:22,  3.55it/s] 87%|████████▋ | 527/605 [03:33<00:21,  3.55it/s] 87%|████████▋ | 528/605 [03:33<00:21,  3.55it/s] 87%|████████▋ | 529/605 [03:33<00:21,  3.55it/s] 88%|████████▊ | 530/605 [03:34<00:21,  3.56it/s] 88%|████████▊ | 531/605 [03:34<00:20,  3.56it/s] 88%|████████▊ | 532/605 [03:34<00:20,  3.56it/s] 88%|████████▊ | 533/605 [03:34<00:20,  3.56it/s] 88%|████████▊ | 534/605 [03:35<00:20,  3.52it/s] 88%|████████▊ | 535/605 [03:35<00:19,  3.54it/s] 89%|████████▊ | 536/605 [03:35<00:19,  3.54it/s] 89%|████████▉ | 537/605 [03:36<00:19,  3.55it/s] 89%|████████▉ | 538/605 [03:36<00:18,  3.57it/s] 89%|████████▉ | 539/605 [03:36<00:18,  3.58it/s] 89%|████████▉ | 540/605 [03:36<00:18,  3.59it/s] 89%|████████▉ | 541/605 [03:37<00:17,  3.59it/s] 90%|████████▉ | 542/605 [03:37<00:17,  3.59it/s] 90%|████████▉ | 543/605 [03:37<00:17,  3.60it/s] 90%|████████▉ | 544/605 [03:37<00:16,  3.60it/s] 90%|█████████ | 545/605 [03:38<00:16,  3.60it/s] 90%|█████████ | 546/605 [03:38<00:16,  3.60it/s] 90%|█████████ | 547/605 [03:38<00:16,  3.60it/s] 91%|█████████ | 548/605 [03:39<00:15,  3.60it/s] 91%|█████████ | 549/605 [03:39<00:15,  3.61it/s] 91%|█████████ | 550/605 [03:39<00:15,  3.61it/s] 91%|█████████ | 551/605 [03:39<00:14,  3.61it/s] 91%|█████████ | 552/605 [03:40<00:14,  3.61it/s] 91%|█████████▏| 553/605 [03:40<00:14,  3.61it/s] 92%|█████████▏| 554/605 [03:40<00:14,  3.61it/s] 92%|█████████▏| 555/605 [03:40<00:13,  3.61it/s] 92%|█████████▏| 556/605 [03:41<00:13,  3.59it/s] 92%|█████████▏| 557/605 [03:41<00:13,  3.59it/s] 92%|█████████▏| 558/605 [03:41<00:13,  3.59it/s] 92%|█████████▏| 559/605 [03:42<00:12,  3.60it/s] 93%|█████████▎| 560/605 [03:42<00:12,  3.60it/s] 93%|█████████▎| 561/605 [03:42<00:12,  3.61it/s] 93%|█████████▎| 562/605 [03:42<00:11,  3.61it/s] 93%|█████████▎| 563/605 [03:43<00:11,  3.60it/s] 93%|█████████▎| 564/605 [03:43<00:11,  3.61it/s] 93%|█████████▎| 565/605 [03:43<00:11,  3.61it/s] 94%|█████████▎| 566/605 [03:44<00:10,  3.61it/s] 94%|█████████▎| 567/605 [03:44<00:10,  3.53it/s] 94%|█████████▍| 568/605 [03:44<00:10,  3.55it/s] 94%|█████████▍| 569/605 [03:44<00:10,  3.57it/s] 94%|█████████▍| 570/605 [03:45<00:09,  3.58it/s] 94%|█████████▍| 571/605 [03:45<00:09,  3.59it/s] 95%|█████████▍| 572/605 [03:45<00:09,  3.59it/s] 95%|█████████▍| 573/605 [03:46<00:08,  3.60it/s] 95%|█████████▍| 574/605 [03:46<00:08,  3.60it/s] 95%|█████████▌| 575/605 [03:46<00:08,  3.61it/s] 95%|█████████▌| 576/605 [03:46<00:08,  3.61it/s] 95%|█████████▌| 577/605 [03:47<00:07,  3.61it/s] 96%|█████████▌| 578/605 [03:47<00:07,  3.59it/s] 96%|█████████▌| 579/605 [03:47<00:07,  3.59it/s] 96%|█████████▌| 580/605 [03:47<00:06,  3.60it/s] 96%|█████████▌| 581/605 [03:48<00:06,  3.60it/s] 96%|█████████▌| 582/605 [03:48<00:06,  3.61it/s] 96%|█████████▋| 583/605 [03:48<00:06,  3.61it/s] 97%|█████████▋| 584/605 [03:49<00:05,  3.61it/s] 97%|█████████▋| 585/605 [03:49<00:05,  3.61it/s] 97%|█████████▋| 586/605 [03:49<00:05,  3.61it/s] 97%|█████████▋| 587/605 [03:49<00:04,  3.61it/s] 97%|█████████▋| 588/605 [03:50<00:04,  3.61it/s] 97%|█████████▋| 589/605 [03:50<00:04,  3.59it/s] 98%|█████████▊| 590/605 [03:50<00:04,  3.60it/s] 98%|█████████▊| 591/605 [03:51<00:03,  3.60it/s] 98%|█████████▊| 592/605 [03:51<00:03,  3.61it/s] 98%|█████████▊| 593/605 [03:51<00:03,  3.61it/s] 98%|█████████▊| 594/605 [03:51<00:03,  3.61it/s] 98%|█████████▊| 595/605 [03:52<00:02,  3.60it/s] 99%|█████████▊| 596/605 [03:52<00:02,  3.60it/s] 99%|█████████▊| 597/605 [03:52<00:02,  3.61it/s] 99%|█████████▉| 598/605 [03:52<00:01,  3.61it/s] 99%|█████████▉| 599/605 [03:53<00:01,  3.61it/s] 99%|█████████▉| 600/605 [03:53<00:01,  3.59it/s] 99%|█████████▉| 601/605 [03:53<00:01,  3.60it/s]100%|█████████▉| 602/605 [03:54<00:00,  3.60it/s]100%|█████████▉| 603/605 [03:54<00:00,  3.60it/s]100%|█████████▉| 604/605 [03:54<00:00,  3.60it/s]100%|██████████| 605/605 [03:54<00:00,  3.66it/s][INFO|trainer.py:2140] 2023-08-28 01:43:02,249 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:43:02,249 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 01:43:02,249 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8102, 'eval_samples_per_second': 353.506, 'eval_steps_per_second': 44.243, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.6797520661157023e-05, 'epoch': 4.13}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.24it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.79it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.95it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.84it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.03it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.42it/s][A
  6%|▌         | 37/611 [00:00<00:13, 44.07it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.94it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.05it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.26it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.37it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.53it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.53it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.31it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.07it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.95it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.92it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.01it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.20it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.35it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.46it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.48it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.29it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.06it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.90it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.88it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.01it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.16it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.36it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.52it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.38it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.27it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.06it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.93it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.93it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.04it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.11it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.28it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.45it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.48it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.28it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.00it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.92it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.05it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.12it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.19it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.41it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.44it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.48it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.34it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.11it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.96it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.87it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.07it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.18it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.36it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.46it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.45it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.25it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.04it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.94it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.99it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 43.99it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.17it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.36it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.44it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.41it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.23it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.98it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.00it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.04it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.26it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.42it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.47it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.41it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.24it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.01it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.86it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.00it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.12it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.31it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.44it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.46it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.38it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.24it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.08it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 43.92it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 43.94it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.05it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.32it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.49it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.47it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.29it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.13it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.04it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 43.92it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.91it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.11it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.26it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.22it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.36it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.21it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.08it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.93it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.01it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.01it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.03it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.14it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.29it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.31it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.26it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.09it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 43.91it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.02it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.94it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 43.92it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.12it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.37it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.42it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.43it/s][A                                                 
                                                 [A100%|██████████| 605/605 [04:08<00:00,  3.66it/s]
100%|██████████| 611/611 [00:13<00:00, 44.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:43:16,082 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605
[INFO|configuration_utils.py:351] 2023-08-28 01:43:16,106 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:43:17,989 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:43:18,001 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:43:18,014 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 01:43:18,263 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 01:43:18,263 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121 (score: 0.9401851296424866).
                                                 100%|██████████| 605/605 [04:12<00:00,  3.66it/s]100%|██████████| 605/605 [04:12<00:00,  2.40it/s]
[INFO|trainer.py:1894] 2023-08-28 01:43:19,978 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 01:43:20,001 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:43:21,860 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:43:21,874 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:43:21,883 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 01:43:22,072 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:22,072 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:22,072 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:22,073 >>   train_runtime            = 0:04:12.59
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:22,073 >>   train_samples            =       7740
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:22,073 >>   train_samples_per_second =    153.209
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:22,073 >>   train_steps_per_second   =      2.395
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8169, 'eval_samples_per_second': 353.336, 'eval_steps_per_second': 44.221, 'epoch': 5.0}
{'train_runtime': 252.5962, 'train_samples_per_second': 153.209, 'train_steps_per_second': 2.395, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 01:43:22 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 01:43:22,111 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:43:22,111 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 01:43:22,111 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 56.57it/s]  2%|▏         | 12/611 [00:00<00:12, 48.99it/s]  3%|▎         | 17/611 [00:00<00:12, 47.22it/s]  4%|▎         | 22/611 [00:00<00:12, 46.46it/s]  4%|▍         | 27/611 [00:00<00:12, 45.99it/s]  5%|▌         | 32/611 [00:00<00:12, 45.69it/s]  6%|▌         | 37/611 [00:00<00:12, 45.50it/s]  7%|▋         | 42/611 [00:00<00:12, 44.96it/s]  8%|▊         | 47/611 [00:01<00:12, 44.31it/s]  9%|▊         | 52/611 [00:01<00:12, 43.96it/s]  9%|▉         | 57/611 [00:01<00:12, 43.93it/s] 10%|█         | 62/611 [00:01<00:12, 44.11it/s] 11%|█         | 67/611 [00:01<00:12, 44.37it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.56it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.72it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.74it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.44it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.05it/s] 16%|█▌        | 97/611 [00:02<00:11, 43.83it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.87it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.13it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.30it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.49it/s] 20%|█▉        | 122/611 [00:02<00:10, 44.66it/s] 21%|██        | 127/611 [00:02<00:10, 44.74it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.41it/s] 22%|██▏       | 137/611 [00:03<00:10, 44.13it/s] 23%|██▎       | 142/611 [00:03<00:10, 43.92it/s] 24%|██▍       | 147/611 [00:03<00:10, 44.03it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.14it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.35it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.47it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.65it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.67it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.46it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.21it/s] 31%|███       | 187/611 [00:04<00:09, 43.95it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.01it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.15it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.35it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.50it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.62it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.63it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.49it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.19it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.03it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.07it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.14it/s] 40%|████      | 247/611 [00:05<00:08, 44.27it/s] 41%|████      | 252/611 [00:05<00:08, 44.45it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.55it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.64it/s] 44%|████▎     | 267/611 [00:05<00:07, 44.46it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.28it/s] 45%|████▌     | 277/611 [00:06<00:07, 44.13it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.15it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.21it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.26it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.37it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.52it/s] 50%|█████     | 307/611 [00:06<00:06, 44.53it/s] 51%|█████     | 312/611 [00:07<00:06, 44.44it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.25it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.22it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.17it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.17it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.34it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.45it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.50it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.42it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.37it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.16it/s] 60%|██████    | 367/611 [00:08<00:05, 44.19it/s] 61%|██████    | 372/611 [00:08<00:05, 44.09it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.16it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.33it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.43it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.49it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.43it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.38it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.27it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.18it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.13it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.17it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.33it/s] 71%|███████   | 432/611 [00:09<00:04, 44.42it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.49it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.44it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.27it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.30it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.18it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.14it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.14it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.26it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.45it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.51it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.47it/s] 81%|████████  | 492/611 [00:11<00:02, 44.38it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.28it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.21it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.12it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.21it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.28it/s] 85%|████████▌ | 522/611 [00:11<00:02, 44.48it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.52it/s] 87%|████████▋ | 532/611 [00:11<00:01, 44.41it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.33it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.27it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.16it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.10it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.12it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.22it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.41it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.50it/s] 94%|█████████▍| 577/611 [00:12<00:00, 44.41it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.28it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.17it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.13it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.09it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.13it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.15it/s]100%|██████████| 611/611 [00:13<00:00, 44.41it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 01:43:35,887 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:35,887 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:35,887 >>   eval_loss               =     0.9402
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:35,887 >>   eval_runtime            = 0:00:13.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:35,887 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:35,887 >>   eval_samples_per_second =    354.399
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:35,887 >>   eval_steps_per_second   =     44.354
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:43:35,887 >>   perplexity              =     2.5605
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:41,688 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:41,691 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:41,691 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:41,692 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:41,692 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:43:42,414 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:43:42,415 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:43:42,668 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:43:43,729 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:43:43,729 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:45,888 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:45,889 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:45,890 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:45,890 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:43:45,890 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:43:46,205 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:43:46,206 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:43:46,879 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:43:47,047 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:43:47,047 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 4it [00:02,  1.80it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.70it/s]Extractor Predicting: 8it [00:04,  1.75it/s]Extractor Predicting: 9it [00:05,  1.82it/s]Extractor Predicting: 10it [00:05,  1.91it/s]Extractor Predicting: 11it [00:06,  1.74it/s]Extractor Predicting: 12it [00:06,  1.73it/s]Extractor Predicting: 13it [00:07,  1.78it/s]Extractor Predicting: 14it [00:07,  1.83it/s]Extractor Predicting: 15it [00:08,  1.82it/s]Extractor Predicting: 16it [00:08,  1.85it/s]Extractor Predicting: 17it [00:09,  1.86it/s]Extractor Predicting: 18it [00:10,  1.86it/s]Extractor Predicting: 19it [00:10,  1.91it/s]Extractor Predicting: 20it [00:11,  1.92it/s]Extractor Predicting: 21it [00:11,  1.93it/s]Extractor Predicting: 22it [00:12,  1.99it/s]Extractor Predicting: 23it [00:12,  1.98it/s]Extractor Predicting: 24it [00:13,  1.99it/s]Extractor Predicting: 25it [00:13,  1.94it/s]Extractor Predicting: 26it [00:14,  1.90it/s]Extractor Predicting: 27it [00:14,  1.91it/s]Extractor Predicting: 28it [00:15,  1.90it/s]Extractor Predicting: 29it [00:15,  1.89it/s]Extractor Predicting: 30it [00:16,  1.92it/s]Extractor Predicting: 31it [00:16,  1.93it/s]Extractor Predicting: 32it [00:17,  1.97it/s]Extractor Predicting: 33it [00:17,  1.94it/s]Extractor Predicting: 34it [00:18,  1.91it/s]Extractor Predicting: 35it [00:18,  1.91it/s]Extractor Predicting: 36it [00:19,  1.88it/s]Extractor Predicting: 37it [00:19,  1.93it/s]Extractor Predicting: 38it [00:20,  1.92it/s]Extractor Predicting: 39it [00:20,  1.89it/s]Extractor Predicting: 40it [00:21,  1.92it/s]Extractor Predicting: 41it [00:21,  1.89it/s]Extractor Predicting: 42it [00:22,  1.89it/s]Extractor Predicting: 43it [00:23,  1.89it/s]Extractor Predicting: 44it [00:23,  1.88it/s]Extractor Predicting: 45it [00:24,  1.92it/s]Extractor Predicting: 46it [00:24,  1.91it/s]Extractor Predicting: 47it [00:25,  1.90it/s]Extractor Predicting: 48it [00:25,  1.89it/s]Extractor Predicting: 49it [00:26,  1.89it/s]Extractor Predicting: 50it [00:26,  1.83it/s]Extractor Predicting: 51it [00:27,  1.84it/s]Extractor Predicting: 52it [00:27,  1.89it/s]Extractor Predicting: 53it [00:28,  1.87it/s]Extractor Predicting: 54it [00:28,  1.85it/s]Extractor Predicting: 55it [00:29,  1.78it/s]Extractor Predicting: 56it [00:30,  1.74it/s]Extractor Predicting: 57it [00:30,  1.76it/s]Extractor Predicting: 58it [00:31,  1.76it/s]Extractor Predicting: 59it [00:31,  1.79it/s]Extractor Predicting: 60it [00:32,  1.77it/s]Extractor Predicting: 61it [00:32,  1.77it/s]Extractor Predicting: 62it [00:33,  1.73it/s]Extractor Predicting: 63it [00:34,  1.74it/s]Extractor Predicting: 64it [00:34,  1.76it/s]Extractor Predicting: 65it [00:35,  1.74it/s]Extractor Predicting: 66it [00:35,  1.70it/s]Extractor Predicting: 67it [00:36,  1.73it/s]Extractor Predicting: 68it [00:36,  1.77it/s]Extractor Predicting: 69it [00:37,  1.76it/s]Extractor Predicting: 70it [00:38,  1.73it/s]Extractor Predicting: 71it [00:38,  1.74it/s]Extractor Predicting: 72it [00:39,  1.71it/s]Extractor Predicting: 73it [00:39,  1.74it/s]Extractor Predicting: 74it [00:40,  1.74it/s]Extractor Predicting: 75it [00:40,  1.75it/s]Extractor Predicting: 76it [00:41,  1.76it/s]Extractor Predicting: 77it [00:42,  1.75it/s]Extractor Predicting: 78it [00:42,  1.78it/s]Extractor Predicting: 79it [00:43,  1.80it/s]Extractor Predicting: 80it [00:43,  1.80it/s]Extractor Predicting: 81it [00:44,  1.80it/s]Extractor Predicting: 82it [00:44,  1.76it/s]Extractor Predicting: 83it [00:45,  1.78it/s]Extractor Predicting: 84it [00:46,  1.77it/s]Extractor Predicting: 85it [00:46,  1.76it/s]Extractor Predicting: 86it [00:47,  1.74it/s]Extractor Predicting: 87it [00:47,  1.73it/s]Extractor Predicting: 88it [00:48,  1.69it/s]Extractor Predicting: 89it [00:49,  1.69it/s]Extractor Predicting: 90it [00:49,  1.69it/s]Extractor Predicting: 91it [00:50,  1.70it/s]Extractor Predicting: 92it [00:50,  1.77it/s]Extractor Predicting: 93it [00:51,  1.85it/s]Extractor Predicting: 94it [00:51,  1.66it/s]Extractor Predicting: 95it [00:52,  1.71it/s]Extractor Predicting: 96it [00:53,  1.74it/s]Extractor Predicting: 97it [00:53,  1.80it/s]Extractor Predicting: 98it [00:54,  1.77it/s]Extractor Predicting: 99it [00:54,  1.71it/s]Extractor Predicting: 100it [00:55,  1.75it/s]Extractor Predicting: 101it [00:55,  1.67it/s]Extractor Predicting: 102it [00:56,  1.66it/s]Extractor Predicting: 103it [00:57,  1.69it/s]Extractor Predicting: 104it [00:57,  1.72it/s]Extractor Predicting: 105it [00:58,  1.74it/s]Extractor Predicting: 106it [00:58,  1.80it/s]Extractor Predicting: 107it [00:59,  1.81it/s]Extractor Predicting: 108it [00:59,  1.84it/s]Extractor Predicting: 109it [01:00,  1.83it/s]Extractor Predicting: 110it [01:00,  1.83it/s]Extractor Predicting: 111it [01:01,  1.87it/s]Extractor Predicting: 112it [01:01,  1.87it/s]Extractor Predicting: 113it [01:02,  1.81it/s]Extractor Predicting: 114it [01:03,  1.78it/s]Extractor Predicting: 115it [01:03,  1.78it/s]Extractor Predicting: 116it [01:04,  1.74it/s]Extractor Predicting: 117it [01:04,  1.72it/s]Extractor Predicting: 118it [01:05,  1.72it/s]Extractor Predicting: 119it [01:06,  1.69it/s]Extractor Predicting: 120it [01:06,  1.65it/s]Extractor Predicting: 121it [01:07,  1.67it/s]Extractor Predicting: 122it [01:07,  1.68it/s]Extractor Predicting: 123it [01:08,  1.71it/s]Extractor Predicting: 124it [01:09,  1.72it/s]Extractor Predicting: 125it [01:09,  1.74it/s]Extractor Predicting: 126it [01:10,  1.74it/s]Extractor Predicting: 127it [01:10,  1.74it/s]Extractor Predicting: 128it [01:11,  1.75it/s]Extractor Predicting: 129it [01:11,  1.78it/s]Extractor Predicting: 130it [01:12,  1.71it/s]Extractor Predicting: 131it [01:13,  1.72it/s]Extractor Predicting: 132it [01:13,  1.75it/s]Extractor Predicting: 133it [01:14,  1.72it/s]Extractor Predicting: 134it [01:14,  1.73it/s]Extractor Predicting: 135it [01:15,  1.72it/s]Extractor Predicting: 136it [01:15,  1.75it/s]Extractor Predicting: 137it [01:16,  1.71it/s]Extractor Predicting: 138it [01:17,  1.72it/s]Extractor Predicting: 139it [01:17,  1.70it/s]Extractor Predicting: 140it [01:18,  1.69it/s]Extractor Predicting: 141it [01:18,  1.71it/s]Extractor Predicting: 142it [01:19,  1.72it/s]Extractor Predicting: 143it [01:20,  1.70it/s]Extractor Predicting: 144it [01:20,  1.75it/s]Extractor Predicting: 145it [01:21,  1.80it/s]Extractor Predicting: 146it [01:21,  1.78it/s]Extractor Predicting: 147it [01:22,  1.73it/s]Extractor Predicting: 148it [01:22,  1.75it/s]Extractor Predicting: 149it [01:23,  1.73it/s]Extractor Predicting: 150it [01:24,  1.70it/s]Extractor Predicting: 151it [01:24,  1.69it/s]Extractor Predicting: 152it [01:25,  1.69it/s]Extractor Predicting: 153it [01:25,  1.71it/s]Extractor Predicting: 154it [01:26,  1.72it/s]Extractor Predicting: 155it [01:26,  1.73it/s]Extractor Predicting: 156it [01:27,  1.66it/s]Extractor Predicting: 157it [01:28,  1.62it/s]Extractor Predicting: 158it [01:28,  1.60it/s]Extractor Predicting: 159it [01:29,  1.63it/s]Extractor Predicting: 160it [01:30,  1.66it/s]Extractor Predicting: 161it [01:30,  1.67it/s]Extractor Predicting: 162it [01:31,  1.69it/s]Extractor Predicting: 163it [01:31,  1.70it/s]Extractor Predicting: 164it [01:32,  1.74it/s]Extractor Predicting: 165it [01:32,  1.74it/s]Extractor Predicting: 166it [01:33,  1.77it/s]Extractor Predicting: 167it [01:34,  1.75it/s]Extractor Predicting: 168it [01:34,  1.75it/s]Extractor Predicting: 169it [01:35,  1.75it/s]Extractor Predicting: 170it [01:35,  1.74it/s]Extractor Predicting: 171it [01:36,  1.76it/s]Extractor Predicting: 172it [01:36,  1.80it/s]Extractor Predicting: 173it [01:37,  1.73it/s]Extractor Predicting: 174it [01:38,  1.75it/s]Extractor Predicting: 175it [01:38,  1.72it/s]Extractor Predicting: 176it [01:39,  1.74it/s]Extractor Predicting: 177it [01:39,  1.71it/s]Extractor Predicting: 178it [01:40,  1.71it/s]Extractor Predicting: 179it [01:41,  1.71it/s]Extractor Predicting: 180it [01:41,  1.56it/s]Extractor Predicting: 181it [01:42,  1.61it/s]Extractor Predicting: 182it [01:42,  1.63it/s]Extractor Predicting: 183it [01:43,  1.61it/s]Extractor Predicting: 184it [01:44,  1.64it/s]Extractor Predicting: 185it [01:44,  1.78it/s]Extractor Predicting: 185it [01:44,  1.77it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:40,528 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:40,533 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:40,533 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:40,533 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:40,533 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:45:41,230 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:45:41,231 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:45:41,834 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:45:42,861 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:45:42,861 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:45,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:45,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:45,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:45,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:45,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:45:46,282 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:45:46,283 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:45:46,900 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:45:47,055 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:45:47,055 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.77it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.81it/s]Extractor Predicting: 6it [00:03,  1.81it/s]Extractor Predicting: 7it [00:03,  1.82it/s]Extractor Predicting: 8it [00:04,  1.79it/s]Extractor Predicting: 9it [00:05,  1.80it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.81it/s]Extractor Predicting: 12it [00:06,  1.83it/s]Extractor Predicting: 13it [00:07,  1.82it/s]Extractor Predicting: 14it [00:07,  1.81it/s]Extractor Predicting: 15it [00:08,  1.83it/s]Extractor Predicting: 16it [00:08,  1.82it/s]Extractor Predicting: 17it [00:09,  1.82it/s]Extractor Predicting: 18it [00:10,  1.77it/s]Extractor Predicting: 19it [00:10,  1.79it/s]Extractor Predicting: 20it [00:11,  1.76it/s]Extractor Predicting: 21it [00:11,  1.74it/s]Extractor Predicting: 22it [00:12,  1.75it/s]Extractor Predicting: 23it [00:12,  1.75it/s]Extractor Predicting: 24it [00:13,  1.78it/s]Extractor Predicting: 25it [00:13,  1.79it/s]Extractor Predicting: 26it [00:14,  1.79it/s]Extractor Predicting: 27it [00:15,  1.80it/s]Extractor Predicting: 28it [00:15,  1.81it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:16,  1.87it/s]Extractor Predicting: 31it [00:17,  1.76it/s]Extractor Predicting: 32it [00:17,  1.80it/s]Extractor Predicting: 33it [00:18,  1.83it/s]Extractor Predicting: 34it [00:18,  1.86it/s]Extractor Predicting: 35it [00:19,  1.89it/s]Extractor Predicting: 36it [00:19,  1.87it/s]Extractor Predicting: 37it [00:20,  1.90it/s]Extractor Predicting: 38it [00:21,  1.87it/s]Extractor Predicting: 39it [00:21,  1.81it/s]Extractor Predicting: 40it [00:22,  1.83it/s]Extractor Predicting: 41it [00:22,  1.86it/s]Extractor Predicting: 42it [00:23,  1.88it/s]Extractor Predicting: 43it [00:23,  1.89it/s]Extractor Predicting: 44it [00:24,  1.91it/s]Extractor Predicting: 45it [00:24,  1.93it/s]Extractor Predicting: 46it [00:25,  1.97it/s]Extractor Predicting: 47it [00:25,  1.97it/s]Extractor Predicting: 48it [00:26,  1.96it/s]Extractor Predicting: 49it [00:26,  1.94it/s]Extractor Predicting: 50it [00:27,  1.92it/s]Extractor Predicting: 51it [00:27,  1.94it/s]Extractor Predicting: 52it [00:28,  1.93it/s]Extractor Predicting: 53it [00:28,  1.91it/s]Extractor Predicting: 54it [00:29,  1.87it/s]Extractor Predicting: 55it [00:29,  1.85it/s]Extractor Predicting: 56it [00:30,  1.84it/s]Extractor Predicting: 57it [00:31,  1.87it/s]Extractor Predicting: 58it [00:31,  1.96it/s]Extractor Predicting: 59it [00:32,  1.92it/s]Extractor Predicting: 60it [00:32,  1.89it/s]Extractor Predicting: 61it [00:33,  1.82it/s]Extractor Predicting: 62it [00:33,  1.79it/s]Extractor Predicting: 63it [00:34,  1.74it/s]Extractor Predicting: 64it [00:34,  1.70it/s]Extractor Predicting: 65it [00:35,  1.69it/s]Extractor Predicting: 66it [00:36,  1.68it/s]Extractor Predicting: 67it [00:36,  1.66it/s]Extractor Predicting: 68it [00:37,  1.68it/s]Extractor Predicting: 69it [00:37,  1.69it/s]Extractor Predicting: 70it [00:38,  1.72it/s]Extractor Predicting: 71it [00:39,  1.74it/s]Extractor Predicting: 72it [00:39,  1.76it/s]Extractor Predicting: 73it [00:40,  1.80it/s]Extractor Predicting: 74it [00:40,  1.81it/s]Extractor Predicting: 75it [00:41,  1.82it/s]Extractor Predicting: 76it [00:41,  1.84it/s]Extractor Predicting: 77it [00:42,  1.86it/s]Extractor Predicting: 78it [00:42,  1.84it/s]Extractor Predicting: 79it [00:43,  1.90it/s]Extractor Predicting: 80it [00:43,  1.95it/s]Extractor Predicting: 81it [00:44,  1.89it/s]Extractor Predicting: 82it [00:44,  1.91it/s]Extractor Predicting: 83it [00:45,  1.86it/s]Extractor Predicting: 84it [00:46,  1.84it/s]Extractor Predicting: 85it [00:46,  1.78it/s]Extractor Predicting: 86it [00:47,  1.76it/s]Extractor Predicting: 87it [00:47,  1.75it/s]Extractor Predicting: 88it [00:48,  1.78it/s]Extractor Predicting: 89it [00:48,  1.76it/s]Extractor Predicting: 90it [00:49,  1.79it/s]Extractor Predicting: 91it [00:50,  1.77it/s]Extractor Predicting: 92it [00:50,  1.76it/s]Extractor Predicting: 93it [00:51,  1.79it/s]Extractor Predicting: 94it [00:51,  1.81it/s]Extractor Predicting: 95it [00:52,  1.78it/s]Extractor Predicting: 96it [00:52,  1.78it/s]Extractor Predicting: 97it [00:53,  1.79it/s]Extractor Predicting: 98it [00:53,  1.79it/s]Extractor Predicting: 99it [00:54,  1.79it/s]Extractor Predicting: 100it [00:55,  1.77it/s]Extractor Predicting: 101it [00:55,  1.79it/s]Extractor Predicting: 102it [00:56,  1.80it/s]Extractor Predicting: 103it [00:56,  1.74it/s]Extractor Predicting: 104it [00:57,  1.78it/s]Extractor Predicting: 105it [00:57,  1.81it/s]Extractor Predicting: 106it [00:58,  1.83it/s]Extractor Predicting: 107it [00:58,  1.80it/s]Extractor Predicting: 108it [00:59,  1.83it/s]Extractor Predicting: 109it [01:00,  1.81it/s]Extractor Predicting: 110it [01:00,  1.84it/s]Extractor Predicting: 111it [01:01,  1.83it/s]Extractor Predicting: 112it [01:01,  1.80it/s]Extractor Predicting: 113it [01:02,  1.78it/s]Extractor Predicting: 114it [01:02,  1.78it/s]Extractor Predicting: 115it [01:03,  1.78it/s]Extractor Predicting: 116it [01:03,  1.78it/s]Extractor Predicting: 117it [01:04,  1.82it/s]Extractor Predicting: 118it [01:05,  1.80it/s]Extractor Predicting: 119it [01:05,  1.79it/s]Extractor Predicting: 120it [01:06,  1.82it/s]Extractor Predicting: 121it [01:06,  1.84it/s]Extractor Predicting: 122it [01:07,  1.83it/s]Extractor Predicting: 123it [01:07,  1.80it/s]Extractor Predicting: 124it [01:08,  1.77it/s]Extractor Predicting: 125it [01:08,  1.77it/s]Extractor Predicting: 126it [01:09,  1.76it/s]Extractor Predicting: 127it [01:10,  1.80it/s]Extractor Predicting: 128it [01:10,  1.74it/s]Extractor Predicting: 129it [01:11,  1.76it/s]Extractor Predicting: 130it [01:11,  1.80it/s]Extractor Predicting: 131it [01:12,  1.77it/s]Extractor Predicting: 132it [01:12,  1.77it/s]Extractor Predicting: 133it [01:13,  1.78it/s]Extractor Predicting: 134it [01:14,  1.78it/s]Extractor Predicting: 135it [01:14,  1.80it/s]Extractor Predicting: 136it [01:15,  1.85it/s]Extractor Predicting: 137it [01:15,  1.78it/s]Extractor Predicting: 138it [01:16,  1.60it/s]Extractor Predicting: 139it [01:16,  1.68it/s]Extractor Predicting: 140it [01:17,  1.73it/s]Extractor Predicting: 141it [01:18,  1.74it/s]Extractor Predicting: 142it [01:18,  1.78it/s]Extractor Predicting: 143it [01:19,  1.69it/s]Extractor Predicting: 144it [01:19,  1.76it/s]Extractor Predicting: 145it [01:20,  1.74it/s]Extractor Predicting: 146it [01:20,  1.79it/s]Extractor Predicting: 147it [01:21,  1.86it/s]Extractor Predicting: 148it [01:21,  1.83it/s]Extractor Predicting: 149it [01:22,  1.88it/s]Extractor Predicting: 150it [01:22,  1.90it/s]Extractor Predicting: 151it [01:23,  1.93it/s]Extractor Predicting: 152it [01:24,  1.88it/s]Extractor Predicting: 153it [01:24,  1.88it/s]Extractor Predicting: 154it [01:25,  1.82it/s]Extractor Predicting: 155it [01:25,  1.80it/s]Extractor Predicting: 156it [01:26,  1.86it/s]Extractor Predicting: 157it [01:26,  1.81it/s]Extractor Predicting: 158it [01:27,  1.79it/s]Extractor Predicting: 159it [01:27,  1.81it/s]Extractor Predicting: 160it [01:28,  1.81it/s]Extractor Predicting: 161it [01:28,  1.85it/s]Extractor Predicting: 162it [01:29,  1.83it/s]Extractor Predicting: 163it [01:30,  1.83it/s]Extractor Predicting: 164it [01:30,  1.81it/s]Extractor Predicting: 165it [01:31,  1.75it/s]Extractor Predicting: 166it [01:31,  1.72it/s]Extractor Predicting: 167it [01:32,  1.72it/s]Extractor Predicting: 168it [01:33,  1.73it/s]Extractor Predicting: 169it [01:33,  1.77it/s]Extractor Predicting: 170it [01:34,  1.79it/s]Extractor Predicting: 171it [01:34,  1.80it/s]Extractor Predicting: 172it [01:35,  1.84it/s]Extractor Predicting: 173it [01:35,  1.83it/s]Extractor Predicting: 174it [01:36,  1.82it/s]Extractor Predicting: 175it [01:36,  1.81it/s]Extractor Predicting: 176it [01:37,  1.83it/s]Extractor Predicting: 177it [01:37,  1.80it/s]Extractor Predicting: 178it [01:38,  1.79it/s]Extractor Predicting: 179it [01:39,  1.75it/s]Extractor Predicting: 180it [01:39,  1.80it/s]Extractor Predicting: 181it [01:40,  1.79it/s]Extractor Predicting: 182it [01:40,  1.83it/s]Extractor Predicting: 183it [01:41,  1.86it/s]Extractor Predicting: 184it [01:41,  1.84it/s]Extractor Predicting: 185it [01:42,  1.83it/s]Extractor Predicting: 186it [01:42,  1.78it/s]Extractor Predicting: 187it [01:43,  1.80it/s]Extractor Predicting: 188it [01:44,  1.80it/s]Extractor Predicting: 189it [01:44,  1.80it/s]Extractor Predicting: 190it [01:45,  1.83it/s]Extractor Predicting: 191it [01:45,  1.82it/s]Extractor Predicting: 192it [01:46,  1.87it/s]Extractor Predicting: 193it [01:46,  1.86it/s]Extractor Predicting: 194it [01:47,  1.86it/s]Extractor Predicting: 195it [01:47,  1.84it/s]Extractor Predicting: 196it [01:48,  1.82it/s]Extractor Predicting: 197it [01:48,  1.82it/s]Extractor Predicting: 198it [01:49,  1.79it/s]Extractor Predicting: 199it [01:50,  1.78it/s]Extractor Predicting: 200it [01:50,  1.79it/s]Extractor Predicting: 201it [01:51,  1.81it/s]Extractor Predicting: 202it [01:51,  1.81it/s]Extractor Predicting: 203it [01:52,  1.84it/s]Extractor Predicting: 204it [01:52,  1.82it/s]Extractor Predicting: 205it [01:53,  1.84it/s]Extractor Predicting: 206it [01:53,  1.82it/s]Extractor Predicting: 207it [01:54,  1.84it/s]Extractor Predicting: 208it [01:54,  1.81it/s]Extractor Predicting: 209it [01:55,  1.78it/s]Extractor Predicting: 210it [01:56,  1.85it/s]Extractor Predicting: 211it [01:56,  1.81it/s]Extractor Predicting: 212it [01:57,  1.83it/s]Extractor Predicting: 213it [01:57,  1.84it/s]Extractor Predicting: 214it [01:58,  1.87it/s]Extractor Predicting: 215it [01:58,  1.86it/s]Extractor Predicting: 216it [01:59,  1.81it/s]Extractor Predicting: 217it [01:59,  1.83it/s]Extractor Predicting: 218it [02:00,  1.81it/s]Extractor Predicting: 219it [02:01,  1.81it/s]Extractor Predicting: 220it [02:01,  1.82it/s]Extractor Predicting: 221it [02:02,  1.84it/s]Extractor Predicting: 222it [02:02,  1.77it/s]Extractor Predicting: 223it [02:03,  1.71it/s]Extractor Predicting: 224it [02:03,  1.73it/s]Extractor Predicting: 225it [02:04,  1.76it/s]Extractor Predicting: 226it [02:04,  1.81it/s]Extractor Predicting: 227it [02:05,  1.82it/s]Extractor Predicting: 228it [02:06,  1.84it/s]Extractor Predicting: 229it [02:06,  1.87it/s]Extractor Predicting: 230it [02:07,  1.89it/s]Extractor Predicting: 231it [02:07,  1.89it/s]Extractor Predicting: 232it [02:08,  1.88it/s]Extractor Predicting: 233it [02:08,  1.87it/s]Extractor Predicting: 234it [02:09,  1.87it/s]Extractor Predicting: 235it [02:09,  1.82it/s]Extractor Predicting: 236it [02:10,  1.82it/s]Extractor Predicting: 237it [02:10,  1.83it/s]Extractor Predicting: 238it [02:11,  1.79it/s]Extractor Predicting: 239it [02:11,  1.82it/s]Extractor Predicting: 240it [02:12,  1.81it/s]Extractor Predicting: 241it [02:13,  1.82it/s]Extractor Predicting: 242it [02:13,  1.78it/s]Extractor Predicting: 243it [02:14,  1.74it/s]Extractor Predicting: 244it [02:14,  1.75it/s]Extractor Predicting: 245it [02:15,  1.78it/s]Extractor Predicting: 246it [02:15,  1.79it/s]Extractor Predicting: 247it [02:16,  1.83it/s]Extractor Predicting: 248it [02:17,  1.76it/s]Extractor Predicting: 249it [02:17,  1.75it/s]Extractor Predicting: 250it [02:18,  1.78it/s]Extractor Predicting: 251it [02:18,  1.79it/s]Extractor Predicting: 252it [02:19,  1.77it/s]Extractor Predicting: 253it [02:19,  1.73it/s]Extractor Predicting: 254it [02:20,  1.71it/s]Extractor Predicting: 255it [02:21,  1.73it/s]Extractor Predicting: 256it [02:21,  1.75it/s]Extractor Predicting: 257it [02:22,  1.76it/s]Extractor Predicting: 258it [02:22,  1.77it/s]Extractor Predicting: 259it [02:23,  1.57it/s]Extractor Predicting: 260it [02:24,  1.59it/s]Extractor Predicting: 261it [02:24,  1.65it/s]Extractor Predicting: 262it [02:25,  1.68it/s]Extractor Predicting: 263it [02:25,  1.71it/s]Extractor Predicting: 264it [02:26,  1.73it/s]Extractor Predicting: 265it [02:26,  1.76it/s]Extractor Predicting: 266it [02:27,  1.70it/s]Extractor Predicting: 267it [02:28,  1.71it/s]Extractor Predicting: 268it [02:28,  1.70it/s]Extractor Predicting: 269it [02:29,  1.72it/s]Extractor Predicting: 270it [02:29,  1.71it/s]Extractor Predicting: 271it [02:30,  1.70it/s]Extractor Predicting: 272it [02:31,  1.70it/s]Extractor Predicting: 273it [02:31,  1.73it/s]Extractor Predicting: 274it [02:32,  1.70it/s]Extractor Predicting: 275it [02:32,  1.71it/s]Extractor Predicting: 276it [02:33,  1.72it/s]Extractor Predicting: 277it [02:34,  1.74it/s]Extractor Predicting: 278it [02:34,  1.77it/s]Extractor Predicting: 279it [02:35,  1.73it/s]Extractor Predicting: 280it [02:35,  1.75it/s]Extractor Predicting: 281it [02:36,  1.70it/s]Extractor Predicting: 282it [02:36,  1.71it/s]Extractor Predicting: 283it [02:37,  1.73it/s]Extractor Predicting: 284it [02:38,  1.76it/s]Extractor Predicting: 285it [02:38,  1.75it/s]Extractor Predicting: 286it [02:39,  1.79it/s]Extractor Predicting: 287it [02:39,  1.73it/s]Extractor Predicting: 288it [02:40,  1.75it/s]Extractor Predicting: 289it [02:40,  1.76it/s]Extractor Predicting: 290it [02:41,  1.71it/s]Extractor Predicting: 291it [02:42,  1.67it/s]Extractor Predicting: 292it [02:42,  1.72it/s]Extractor Predicting: 293it [02:43,  1.72it/s]Extractor Predicting: 294it [02:43,  1.70it/s]Extractor Predicting: 295it [02:44,  1.70it/s]Extractor Predicting: 296it [02:44,  1.74it/s]Extractor Predicting: 297it [02:45,  1.75it/s]Extractor Predicting: 298it [02:46,  1.71it/s]Extractor Predicting: 299it [02:46,  1.72it/s]Extractor Predicting: 300it [02:47,  1.70it/s]Extractor Predicting: 301it [02:47,  1.72it/s]Extractor Predicting: 302it [02:48,  1.68it/s]Extractor Predicting: 303it [02:49,  1.73it/s]Extractor Predicting: 304it [02:49,  1.75it/s]Extractor Predicting: 305it [02:50,  1.79it/s]Extractor Predicting: 306it [02:50,  1.82it/s]Extractor Predicting: 307it [02:51,  1.78it/s]Extractor Predicting: 308it [02:51,  1.81it/s]Extractor Predicting: 309it [02:52,  1.77it/s]Extractor Predicting: 310it [02:52,  1.77it/s]Extractor Predicting: 311it [02:53,  1.72it/s]Extractor Predicting: 312it [02:54,  1.75it/s]Extractor Predicting: 313it [02:54,  1.64it/s]Extractor Predicting: 314it [02:55,  1.65it/s]Extractor Predicting: 315it [02:56,  1.70it/s]Extractor Predicting: 316it [02:56,  1.75it/s]Extractor Predicting: 317it [02:57,  1.77it/s]Extractor Predicting: 318it [02:57,  1.81it/s]Extractor Predicting: 319it [02:58,  1.77it/s]Extractor Predicting: 320it [02:58,  1.73it/s]Extractor Predicting: 321it [02:59,  1.73it/s]Extractor Predicting: 322it [02:59,  1.76it/s]Extractor Predicting: 323it [03:00,  1.70it/s]Extractor Predicting: 324it [03:01,  1.72it/s]Extractor Predicting: 325it [03:01,  1.75it/s]Extractor Predicting: 326it [03:02,  1.75it/s]Extractor Predicting: 327it [03:02,  1.76it/s]Extractor Predicting: 328it [03:03,  1.72it/s]Extractor Predicting: 329it [03:03,  1.74it/s]Extractor Predicting: 330it [03:04,  1.72it/s]Extractor Predicting: 331it [03:05,  1.74it/s]Extractor Predicting: 332it [03:05,  1.73it/s]Extractor Predicting: 333it [03:06,  1.72it/s]Extractor Predicting: 334it [03:06,  1.74it/s]Extractor Predicting: 335it [03:07,  1.72it/s]Extractor Predicting: 336it [03:08,  1.63it/s]Extractor Predicting: 337it [03:08,  1.64it/s]Extractor Predicting: 338it [03:09,  1.65it/s]Extractor Predicting: 339it [03:09,  1.67it/s]Extractor Predicting: 340it [03:10,  1.68it/s]Extractor Predicting: 341it [03:11,  1.67it/s]Extractor Predicting: 342it [03:11,  1.68it/s]Extractor Predicting: 343it [03:12,  1.71it/s]Extractor Predicting: 344it [03:12,  1.71it/s]Extractor Predicting: 345it [03:13,  1.66it/s]Extractor Predicting: 346it [03:14,  1.67it/s]Extractor Predicting: 347it [03:14,  1.69it/s]Extractor Predicting: 348it [03:15,  1.71it/s]Extractor Predicting: 349it [03:15,  1.72it/s]Extractor Predicting: 350it [03:16,  1.71it/s]Extractor Predicting: 351it [03:16,  1.75it/s]Extractor Predicting: 352it [03:17,  1.69it/s]Extractor Predicting: 353it [03:18,  1.66it/s]Extractor Predicting: 354it [03:18,  1.71it/s]Extractor Predicting: 355it [03:19,  1.74it/s]Extractor Predicting: 356it [03:19,  1.73it/s]Extractor Predicting: 357it [03:20,  1.74it/s]Extractor Predicting: 358it [03:21,  1.75it/s]Extractor Predicting: 359it [03:21,  1.73it/s]Extractor Predicting: 360it [03:22,  1.69it/s]Extractor Predicting: 361it [03:22,  1.69it/s]Extractor Predicting: 362it [03:23,  1.71it/s]Extractor Predicting: 363it [03:23,  1.76it/s]Extractor Predicting: 364it [03:24,  1.77it/s]Extractor Predicting: 365it [03:25,  1.73it/s]Extractor Predicting: 366it [03:25,  1.68it/s]Extractor Predicting: 367it [03:26,  1.68it/s]Extractor Predicting: 368it [03:26,  1.66it/s]Extractor Predicting: 369it [03:27,  1.61it/s]Extractor Predicting: 370it [03:28,  1.61it/s]Extractor Predicting: 371it [03:29,  1.47it/s]Extractor Predicting: 372it [03:29,  1.81it/s]Extractor Predicting: 372it [03:29,  1.78it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:24,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:24,652 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:24,652 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:24,652 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:24,652 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:49:24,930 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:49:24,931 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:49:25,182 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:49:26,250 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:49:26,250 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:28,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:28,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:28,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:28,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:49:28,383 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:49:28,679 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:49:28,680 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:49:29,341 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:49:29,512 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:49:29,512 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.74it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.84it/s]Extractor Predicting: 7it [00:03,  1.80it/s]Extractor Predicting: 8it [00:04,  1.81it/s]Extractor Predicting: 9it [00:05,  1.83it/s]Extractor Predicting: 10it [00:05,  1.86it/s]Extractor Predicting: 11it [00:06,  1.85it/s]Extractor Predicting: 12it [00:06,  1.88it/s]Extractor Predicting: 13it [00:07,  1.88it/s]Extractor Predicting: 14it [00:07,  1.86it/s]Extractor Predicting: 15it [00:08,  1.77it/s]Extractor Predicting: 16it [00:08,  1.79it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:09,  1.82it/s]Extractor Predicting: 19it [00:10,  1.83it/s]Extractor Predicting: 20it [00:11,  1.82it/s]Extractor Predicting: 21it [00:11,  1.77it/s]Extractor Predicting: 22it [00:12,  1.76it/s]Extractor Predicting: 23it [00:12,  1.76it/s]Extractor Predicting: 24it [00:13,  1.72it/s]Extractor Predicting: 25it [00:13,  1.75it/s]Extractor Predicting: 26it [00:14,  1.73it/s]Extractor Predicting: 27it [00:15,  1.71it/s]Extractor Predicting: 28it [00:15,  1.70it/s]Extractor Predicting: 29it [00:16,  1.74it/s]Extractor Predicting: 30it [00:16,  1.75it/s]Extractor Predicting: 31it [00:17,  1.78it/s]Extractor Predicting: 32it [00:17,  1.75it/s]Extractor Predicting: 33it [00:18,  1.77it/s]Extractor Predicting: 34it [00:19,  1.78it/s]Extractor Predicting: 35it [00:19,  1.74it/s]Extractor Predicting: 36it [00:20,  1.75it/s]Extractor Predicting: 37it [00:20,  1.73it/s]Extractor Predicting: 38it [00:21,  1.67it/s]Extractor Predicting: 39it [00:22,  1.62it/s]Extractor Predicting: 40it [00:22,  1.62it/s]Extractor Predicting: 41it [00:23,  1.62it/s]Extractor Predicting: 42it [00:23,  1.65it/s]Extractor Predicting: 43it [00:24,  1.65it/s]Extractor Predicting: 44it [00:25,  1.67it/s]Extractor Predicting: 45it [00:25,  1.64it/s]Extractor Predicting: 46it [00:26,  1.67it/s]Extractor Predicting: 47it [00:26,  1.66it/s]Extractor Predicting: 48it [00:27,  1.52it/s]Extractor Predicting: 49it [00:28,  1.56it/s]Extractor Predicting: 50it [00:28,  1.58it/s]Extractor Predicting: 51it [00:29,  1.57it/s]Extractor Predicting: 52it [00:30,  1.58it/s]Extractor Predicting: 53it [00:30,  1.61it/s]Extractor Predicting: 54it [00:31,  1.60it/s]Extractor Predicting: 55it [00:32,  1.65it/s]Extractor Predicting: 56it [00:32,  1.66it/s]Extractor Predicting: 57it [00:33,  1.66it/s]Extractor Predicting: 58it [00:33,  1.64it/s]Extractor Predicting: 59it [00:34,  1.64it/s]Extractor Predicting: 60it [00:35,  1.65it/s]Extractor Predicting: 61it [00:35,  1.60it/s]Extractor Predicting: 62it [00:36,  1.60it/s]Extractor Predicting: 63it [00:36,  1.76it/s]Extractor Predicting: 63it [00:36,  1.71it/s]
[INFO|configuration_utils.py:515] 2023-08-28 01:50:07,278 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:50:07,281 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 01:50:07,285 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:50:07,286 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 01:50:07,288 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 01:50:10,540 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 01:50:10,540 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 01:50:10,553 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:50:10,553 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 01:50:10,561 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:50:10,567 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:50:10,567 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:50:10,567 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:50:10,567 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:50:10,567 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:50:10,567 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 01:50:10,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:11,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:11,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:12,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:13,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:13,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:14,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:14,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:15,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:16,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:16,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:17,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:17,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:18,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:19,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:19,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:20,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:21,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:21,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:22,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:22,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:23,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:24,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:24,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:25,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:26,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:26,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:51, 16.53s/it][WARNING|generation_utils.py:914] 2023-08-28 01:50:27,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:27,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:28,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:28,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:29,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:30,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:30,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:31,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:31,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:32,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:32,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:33,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:34,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:34,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:35,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:35,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:36,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:36,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:37,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:37,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:38,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:39,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:40,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:40,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:41,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:30<03:19, 15.31s/it][WARNING|generation_utils.py:914] 2023-08-28 01:50:41,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:42,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:42,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:43,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:43,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:44,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:44,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:45,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:46,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:46,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:47,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:47,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:48,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:48,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:49,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:49,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:50,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:50,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:51,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:51,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:52,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:40, 13.41s/it][WARNING|generation_utils.py:914] 2023-08-28 01:50:52,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:53,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:54,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:54,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:55,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:56,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:56,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:57,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:58,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:58,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:59,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:50:59,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:00,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:01,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:01,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:02,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:02,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:03,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:04,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:04,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:05,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:05,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:06,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:07,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:56<02:33, 13.93s/it][WARNING|generation_utils.py:914] 2023-08-28 01:51:07,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:08,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:08,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:09,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:09,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:10,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:11,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:11,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:12,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:13,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:13,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:14,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:15,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:15,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:16,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:16,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:17,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:18,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:18,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:19,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:19,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:20,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:21,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:21,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:22,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:24, 14.41s/it][WARNING|generation_utils.py:914] 2023-08-28 01:51:22,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:23,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:24,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:24,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:25,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:25,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:26,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:27,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:27,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:28,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:29,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:29,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:30,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:31,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:31,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:32,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:32,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:33,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:34,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:35,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:35,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:36,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:26<02:09, 14.33s/it][WARNING|generation_utils.py:914] 2023-08-28 01:51:37,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:37,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:38,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:38,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:39,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:40,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:40,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:41,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:41,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:42,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:43,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:43,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:44,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:44,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:45,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:46,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:46,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:47,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:47,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:48,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:49,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:49,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:39<01:51, 13.97s/it][WARNING|generation_utils.py:914] 2023-08-28 01:51:50,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:50,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:51,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:51,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:52,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:53,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:53,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:54,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:54,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:55,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:55,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:56,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:57,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:57,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:58,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:58,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:59,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:59,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:00,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:01,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:01,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:02,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:52<01:34, 13.52s/it][WARNING|generation_utils.py:914] 2023-08-28 01:52:02,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:03,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:04,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:04,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:05,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:06,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:06,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:07,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:07,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:08,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:09,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:09,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:10,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:10,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:11,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:11,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:12,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:13,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:13,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:14,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:14,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:15,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:15,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:16,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:17,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:17,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:07<01:24, 14.14s/it][WARNING|generation_utils.py:914] 2023-08-28 01:52:18,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:18,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:19,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:20,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:20,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:21,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:21,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:22,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:22,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:23,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:24,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:24,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:25,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:25,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:26,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:27,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:28,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:28,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:29,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:29,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:30,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:31,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:31,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:21<01:10, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 01:52:32,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:32,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:33,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:33,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:34,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:35,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:35,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:36,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:36,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:37,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:37,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:38,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:38,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:39,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:39,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:40,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:40,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:41,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:41,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:42,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:43,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:43,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:44,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:33<00:53, 13.50s/it][WARNING|generation_utils.py:914] 2023-08-28 01:52:44,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:45,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:45,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:46,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:46,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:47,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:47,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:48,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:49,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:49,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:50,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:50,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:51,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:52,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:53,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:53,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:54,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:54,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:55,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:56,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:56,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:57,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:57,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:58,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:59,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:59,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:00,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:01,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:50<00:43, 14.60s/it][WARNING|generation_utils.py:914] 2023-08-28 01:53:01,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:02,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:02,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:03,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:04,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:04,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:05,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:05,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:06,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:07,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:07,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:08,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:09,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:09,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:10,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:10,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:11,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:12,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:12,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:13,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:14,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:14,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:15,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:15,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:05<00:29, 14.67s/it][WARNING|generation_utils.py:914] 2023-08-28 01:53:16,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:17,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:17,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:18,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:18,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:19,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:19,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:20,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:20,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:21,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:21,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:22,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:22,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:23,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:24,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:24,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:25,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:25,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:26,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:26,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:27,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:27,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:28,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:18<00:14, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-28 01:53:29,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:29,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:30,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:31,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:31,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:32,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:32,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:33,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:34,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:34,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:35,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:36,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:37,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:37,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:38,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:38,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:39,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:40,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:41,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:41,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:42,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:43,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:43,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:33<00:00, 14.38s/it]Generating: 100%|██████████| 15/15 [03:33<00:00, 14.23s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:50,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:50,588 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:50,588 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:50,588 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:50,588 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:53:51,202 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:53:51,202 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:53:51,786 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:53:52,858 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:53:52,859 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:55,718 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:55,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:55,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:55,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:53:55,722 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:53:56,350 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:53:56,351 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:53:56,927 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:53:57,103 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:53:57,103 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 469, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : developer .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 626, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.7825, 'errors': {'', "('London', 'work location', '', 'The New York Times ( July 18 , 1923 October 6 , 1977 ) reported that the first and only English book on the topic appeared in London during the reigns of William Shakespeare and William Shakespeare .')"}}
['Relation : composer . Context : Later in the year ( 1143 ) , he composed The Seven Kingdoms , the first play in the epic of the Old Kingdom . Head Entity : The Seven Kingdoms , Tail Entity : Robert I .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best Rap Album at the 2004 MTV Video Music Awards and had two acts on the chart . Head Entity : music , Tail Entity : Rap Album .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 590, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7319711538461539, 'errors': {'', "('James S. Cain', 'field of work', '', 'He is also known for his work with James S. Cain , and for his work on the painting of the Seven Wonders of Oz by William S. Burroughs .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 194, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 298, 'raw': 448}
{'target': 600, 'success': 322, 'raw': 480}
{'target': 600, 'success': 346, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 387, 'raw': 576}
{'target': 600, 'success': 408, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 518, 'raw': 768}
{'target': 600, 'success': 543, 'raw': 800}
{'target': 600, 'success': 563, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 606, 'raw': 896}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6763392857142857, 'errors': {'', "('John Lennon', 'occupation', '', 'The band released their debut album In Search of a Way ( 2000 ) , which featured a cover from John Lennon and Steve Dizzy .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 13757
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13857, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.43it/s]Extractor Estimating: 2it [00:01,  1.44it/s]Extractor Estimating: 3it [00:01,  1.55it/s]Extractor Estimating: 4it [00:02,  1.67it/s]Extractor Estimating: 5it [00:03,  1.70it/s]Extractor Estimating: 6it [00:03,  1.72it/s]Extractor Estimating: 7it [00:04,  1.71it/s]Extractor Estimating: 8it [00:04,  1.62it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:07,  1.63it/s]Extractor Estimating: 13it [00:07,  1.64it/s]Extractor Estimating: 14it [00:08,  1.66it/s]Extractor Estimating: 15it [00:09,  1.69it/s]Extractor Estimating: 16it [00:09,  1.70it/s]Extractor Estimating: 17it [00:10,  1.66it/s]Extractor Estimating: 18it [00:10,  1.67it/s]Extractor Estimating: 19it [00:11,  1.65it/s]Extractor Estimating: 20it [00:12,  1.66it/s]Extractor Estimating: 21it [00:12,  1.71it/s]Extractor Estimating: 22it [00:13,  1.75it/s]Extractor Estimating: 23it [00:13,  1.74it/s]Extractor Estimating: 24it [00:14,  1.70it/s]Extractor Estimating: 25it [00:15,  1.72it/s]Extractor Estimating: 26it [00:15,  1.78it/s]Extractor Estimating: 27it [00:16,  1.73it/s]Extractor Estimating: 28it [00:16,  1.79it/s]Extractor Estimating: 29it [00:17,  1.77it/s]Extractor Estimating: 30it [00:17,  1.67it/s]Extractor Estimating: 31it [00:18,  1.64it/s]Extractor Estimating: 32it [00:19,  1.74it/s]Extractor Estimating: 33it [00:19,  1.71it/s]Extractor Estimating: 34it [00:20,  1.73it/s]Extractor Estimating: 35it [00:20,  1.68it/s]Extractor Estimating: 36it [00:21,  1.68it/s]Extractor Estimating: 37it [00:22,  1.70it/s]Extractor Estimating: 38it [00:22,  1.67it/s]Extractor Estimating: 39it [00:23,  1.69it/s]Extractor Estimating: 40it [00:23,  1.72it/s]Extractor Estimating: 41it [00:24,  1.68it/s]Extractor Estimating: 42it [00:24,  1.73it/s]Extractor Estimating: 43it [00:25,  1.76it/s]Extractor Estimating: 44it [00:26,  1.75it/s]Extractor Estimating: 45it [00:26,  1.77it/s]Extractor Estimating: 46it [00:27,  1.76it/s]Extractor Estimating: 47it [00:27,  1.76it/s]Extractor Estimating: 48it [00:28,  1.75it/s]Extractor Estimating: 49it [00:28,  1.71it/s]Extractor Estimating: 50it [00:29,  1.78it/s]Extractor Estimating: 51it [00:29,  1.81it/s]Extractor Estimating: 52it [00:30,  1.85it/s]Extractor Estimating: 53it [00:31,  1.88it/s]Extractor Estimating: 54it [00:31,  1.92it/s]Extractor Estimating: 55it [00:32,  1.92it/s]Extractor Estimating: 56it [00:32,  2.01it/s]Extractor Estimating: 57it [00:32,  1.99it/s]Extractor Estimating: 58it [00:33,  2.01it/s]Extractor Estimating: 59it [00:33,  2.06it/s]Extractor Estimating: 60it [00:34,  1.99it/s]Extractor Estimating: 61it [00:35,  1.95it/s]Extractor Estimating: 62it [00:35,  2.00it/s]Extractor Estimating: 63it [00:35,  2.05it/s]Extractor Estimating: 64it [00:36,  1.99it/s]Extractor Estimating: 65it [00:37,  1.94it/s]Extractor Estimating: 66it [00:37,  2.02it/s]Extractor Estimating: 67it [00:37,  2.02it/s]Extractor Estimating: 68it [00:38,  1.96it/s]Extractor Estimating: 69it [00:38,  2.01it/s]Extractor Estimating: 70it [00:39,  1.98it/s]Extractor Estimating: 71it [00:39,  2.02it/s]Extractor Estimating: 72it [00:40,  2.01it/s]Extractor Estimating: 73it [00:40,  2.03it/s]Extractor Estimating: 74it [00:41,  2.04it/s]Extractor Estimating: 75it [00:41,  2.01it/s]Extractor Estimating: 76it [00:42,  1.92it/s]Extractor Estimating: 77it [00:43,  1.77it/s]Extractor Estimating: 78it [00:43,  1.75it/s]Extractor Estimating: 79it [00:44,  1.71it/s]Extractor Estimating: 80it [00:45,  1.62it/s]Extractor Estimating: 81it [00:45,  1.48it/s]Extractor Estimating: 82it [00:46,  1.48it/s]Extractor Estimating: 83it [00:47,  1.52it/s]Extractor Estimating: 84it [00:47,  1.51it/s]Extractor Estimating: 85it [00:48,  1.52it/s]Extractor Estimating: 86it [00:49,  1.55it/s]Extractor Estimating: 87it [00:49,  1.54it/s]Extractor Estimating: 88it [00:50,  1.57it/s]Extractor Estimating: 89it [00:50,  1.63it/s]Extractor Estimating: 90it [00:51,  1.66it/s]Extractor Estimating: 91it [00:52,  1.66it/s]Extractor Estimating: 92it [00:52,  1.68it/s]Extractor Estimating: 93it [00:53,  1.69it/s]Extractor Estimating: 94it [00:53,  1.69it/s]Extractor Estimating: 95it [00:54,  1.65it/s]Extractor Estimating: 96it [00:55,  1.67it/s]Extractor Estimating: 97it [00:55,  1.66it/s]Extractor Estimating: 98it [00:56,  1.66it/s]Extractor Estimating: 99it [00:56,  1.66it/s]Extractor Estimating: 100it [00:57,  1.63it/s]Extractor Estimating: 101it [00:58,  1.63it/s]Extractor Estimating: 102it [00:58,  1.64it/s]Extractor Estimating: 103it [00:59,  1.66it/s]Extractor Estimating: 104it [00:59,  1.69it/s]Extractor Estimating: 105it [01:00,  1.69it/s]Extractor Estimating: 106it [01:01,  1.67it/s]Extractor Estimating: 107it [01:01,  1.65it/s]Extractor Estimating: 108it [01:02,  1.66it/s]Extractor Estimating: 109it [01:03,  1.61it/s]Extractor Estimating: 110it [01:03,  1.62it/s]Extractor Estimating: 111it [01:04,  1.58it/s]Extractor Estimating: 112it [01:05,  1.52it/s]Extractor Estimating: 113it [01:05,  1.54it/s]Extractor Estimating: 114it [01:06,  1.54it/s]Extractor Estimating: 115it [01:06,  1.52it/s]Extractor Estimating: 116it [01:07,  1.54it/s]Extractor Estimating: 117it [01:08,  1.54it/s]Extractor Estimating: 118it [01:08,  1.61it/s]Extractor Estimating: 119it [01:09,  1.61it/s]Extractor Estimating: 120it [01:10,  1.58it/s]Extractor Estimating: 121it [01:10,  1.60it/s]Extractor Estimating: 122it [01:11,  1.59it/s]Extractor Estimating: 123it [01:11,  1.58it/s]Extractor Estimating: 124it [01:12,  1.61it/s]Extractor Estimating: 125it [01:13,  1.63it/s]Extractor Estimating: 126it [01:13,  1.68it/s]Extractor Estimating: 127it [01:14,  1.62it/s]Extractor Estimating: 128it [01:14,  1.65it/s]Extractor Estimating: 129it [01:15,  1.64it/s]Extractor Estimating: 130it [01:16,  1.66it/s]Extractor Estimating: 131it [01:16,  1.63it/s]Extractor Estimating: 132it [01:17,  1.58it/s]Extractor Estimating: 133it [01:18,  1.56it/s]Extractor Estimating: 134it [01:18,  1.59it/s]Extractor Estimating: 135it [01:19,  1.61it/s]Extractor Estimating: 136it [01:19,  1.60it/s]Extractor Estimating: 137it [01:20,  1.52it/s]Extractor Estimating: 138it [01:21,  1.54it/s]Extractor Estimating: 139it [01:21,  1.54it/s]Extractor Estimating: 140it [01:22,  1.57it/s]Extractor Estimating: 141it [01:23,  1.60it/s]Extractor Estimating: 142it [01:23,  1.64it/s]Extractor Estimating: 143it [01:24,  1.67it/s]Extractor Estimating: 144it [01:24,  1.66it/s]Extractor Estimating: 145it [01:25,  1.70it/s]Extractor Estimating: 146it [01:26,  1.65it/s]Extractor Estimating: 147it [01:26,  1.66it/s]Extractor Estimating: 148it [01:27,  1.63it/s]Extractor Estimating: 149it [01:27,  1.69it/s]Extractor Estimating: 150it [01:28,  1.53it/s]Extractor Estimating: 151it [01:29,  1.57it/s]Extractor Estimating: 152it [01:29,  1.63it/s]Extractor Estimating: 153it [01:30,  1.68it/s]Extractor Estimating: 154it [01:31,  1.71it/s]Extractor Estimating: 155it [01:31,  1.73it/s]Extractor Estimating: 156it [01:32,  1.70it/s]Extractor Estimating: 157it [01:32,  1.69it/s]Extractor Estimating: 158it [01:33,  1.68it/s]Extractor Estimating: 159it [01:33,  1.68it/s]Extractor Estimating: 160it [01:34,  1.71it/s]Extractor Estimating: 161it [01:35,  1.61it/s]Extractor Estimating: 162it [01:35,  1.61it/s]Extractor Estimating: 163it [01:36,  1.67it/s]Extractor Estimating: 164it [01:36,  1.70it/s]Extractor Estimating: 165it [01:37,  1.74it/s]Extractor Estimating: 166it [01:38,  1.74it/s]Extractor Estimating: 167it [01:38,  1.78it/s]Extractor Estimating: 168it [01:39,  1.79it/s]Extractor Estimating: 169it [01:39,  1.82it/s]Extractor Estimating: 170it [01:40,  1.71it/s]Extractor Estimating: 171it [01:40,  1.71it/s]Extractor Estimating: 172it [01:41,  1.74it/s]Extractor Estimating: 173it [01:42,  1.77it/s]Extractor Estimating: 174it [01:42,  1.71it/s]Extractor Estimating: 175it [01:43,  1.65it/s]Extractor Estimating: 176it [01:43,  1.64it/s]Extractor Estimating: 177it [01:44,  1.67it/s]Extractor Estimating: 178it [01:45,  1.67it/s]Extractor Estimating: 179it [01:45,  1.67it/s]Extractor Estimating: 180it [01:46,  1.65it/s]Extractor Estimating: 181it [01:46,  1.66it/s]Extractor Estimating: 182it [01:47,  1.67it/s]Extractor Estimating: 183it [01:48,  1.67it/s]Extractor Estimating: 184it [01:48,  1.69it/s]Extractor Estimating: 185it [01:49,  1.67it/s]Extractor Estimating: 186it [01:49,  1.66it/s]Extractor Estimating: 187it [01:50,  1.69it/s]Extractor Estimating: 188it [01:51,  1.67it/s]Extractor Estimating: 189it [01:51,  1.67it/s]Extractor Estimating: 190it [01:52,  1.63it/s]Extractor Estimating: 191it [01:53,  1.60it/s]Extractor Estimating: 192it [01:53,  1.62it/s]Extractor Estimating: 193it [01:54,  1.59it/s]Extractor Estimating: 194it [01:54,  1.63it/s]Extractor Estimating: 195it [01:55,  1.64it/s]Extractor Estimating: 196it [01:56,  1.60it/s]Extractor Estimating: 197it [01:56,  1.60it/s]Extractor Estimating: 198it [01:57,  1.59it/s]Extractor Estimating: 199it [01:57,  1.63it/s]Extractor Estimating: 200it [01:58,  1.62it/s]Extractor Estimating: 201it [01:59,  1.61it/s]Extractor Estimating: 202it [01:59,  1.64it/s]Extractor Estimating: 203it [02:00,  1.61it/s]Extractor Estimating: 204it [02:01,  1.62it/s]Extractor Estimating: 205it [02:01,  1.57it/s]Extractor Estimating: 206it [02:02,  1.62it/s]Extractor Estimating: 207it [02:02,  1.64it/s]Extractor Estimating: 208it [02:03,  1.64it/s]Extractor Estimating: 209it [02:04,  1.66it/s]Extractor Estimating: 210it [02:04,  1.64it/s]Extractor Estimating: 211it [02:05,  1.67it/s]Extractor Estimating: 212it [02:05,  1.67it/s]Extractor Estimating: 213it [02:06,  1.68it/s]Extractor Estimating: 214it [02:07,  1.66it/s]Extractor Estimating: 215it [02:07,  1.64it/s]Extractor Estimating: 216it [02:08,  1.60it/s]Extractor Estimating: 217it [02:08,  1.62it/s]Extractor Estimating: 218it [02:09,  1.62it/s]Extractor Estimating: 219it [02:10,  1.68it/s]Extractor Estimating: 220it [02:10,  1.68it/s]Extractor Estimating: 221it [02:11,  1.67it/s]Extractor Estimating: 222it [02:11,  1.66it/s]Extractor Estimating: 223it [02:12,  1.68it/s]Extractor Estimating: 224it [02:13,  1.65it/s]Extractor Estimating: 225it [02:13,  1.62it/s]Extractor Estimating: 226it [02:14,  1.61it/s]Extractor Estimating: 227it [02:15,  1.56it/s]Extractor Estimating: 228it [02:15,  1.55it/s]Extractor Estimating: 229it [02:16,  1.57it/s]Extractor Estimating: 230it [02:17,  1.45it/s]Extractor Estimating: 231it [02:17,  1.50it/s]Extractor Estimating: 232it [02:18,  1.56it/s]Extractor Estimating: 233it [02:19,  1.60it/s]Extractor Estimating: 234it [02:19,  1.58it/s]Extractor Estimating: 235it [02:20,  1.55it/s]Extractor Estimating: 236it [02:20,  1.55it/s]Extractor Estimating: 237it [02:21,  1.56it/s]Extractor Estimating: 238it [02:22,  1.60it/s]Extractor Estimating: 239it [02:22,  1.62it/s]Extractor Estimating: 240it [02:23,  1.62it/s]Extractor Estimating: 241it [02:24,  1.59it/s]Extractor Estimating: 242it [02:24,  1.55it/s]Extractor Estimating: 243it [02:25,  1.55it/s]Extractor Estimating: 244it [02:25,  1.59it/s]Extractor Estimating: 245it [02:26,  1.57it/s]Extractor Estimating: 246it [02:27,  1.56it/s]Extractor Estimating: 247it [02:27,  1.56it/s]Extractor Estimating: 248it [02:28,  1.60it/s]Extractor Estimating: 249it [02:29,  1.62it/s]Extractor Estimating: 250it [02:29,  1.57it/s]Extractor Estimating: 251it [02:30,  1.59it/s]Extractor Estimating: 252it [02:31,  1.57it/s]Extractor Estimating: 253it [02:31,  1.58it/s]Extractor Estimating: 254it [02:32,  1.60it/s]Extractor Estimating: 255it [02:32,  1.63it/s]Extractor Estimating: 256it [02:33,  1.63it/s]Extractor Estimating: 257it [02:34,  1.67it/s]Extractor Estimating: 258it [02:34,  1.70it/s]Extractor Estimating: 259it [02:35,  1.67it/s]Extractor Estimating: 260it [02:35,  1.71it/s]Extractor Estimating: 261it [02:36,  1.66it/s]Extractor Estimating: 262it [02:37,  1.65it/s]Extractor Estimating: 263it [02:37,  1.66it/s]Extractor Estimating: 264it [02:38,  1.69it/s]Extractor Estimating: 265it [02:38,  1.69it/s]Extractor Estimating: 266it [02:39,  1.71it/s]Extractor Estimating: 267it [02:39,  1.69it/s]Extractor Estimating: 268it [02:40,  1.63it/s]Extractor Estimating: 269it [02:41,  1.66it/s]Extractor Estimating: 270it [02:41,  1.65it/s]Extractor Estimating: 271it [02:42,  1.62it/s]Extractor Estimating: 272it [02:43,  1.57it/s]Extractor Estimating: 273it [02:43,  1.62it/s]Extractor Estimating: 274it [02:44,  1.63it/s]Extractor Estimating: 275it [02:44,  1.65it/s]Extractor Estimating: 276it [02:45,  1.60it/s]Extractor Estimating: 277it [02:46,  1.63it/s]Extractor Estimating: 278it [02:46,  1.66it/s]Extractor Estimating: 279it [02:47,  1.63it/s]Extractor Estimating: 280it [02:48,  1.64it/s]Extractor Estimating: 281it [02:48,  1.63it/s]Extractor Estimating: 282it [02:49,  1.67it/s]Extractor Estimating: 283it [02:49,  1.60it/s]Extractor Estimating: 284it [02:50,  1.60it/s]Extractor Estimating: 285it [02:51,  1.59it/s]Extractor Estimating: 286it [02:51,  1.60it/s]Extractor Estimating: 287it [02:52,  1.57it/s]Extractor Estimating: 288it [02:53,  1.61it/s]Extractor Estimating: 289it [02:53,  1.59it/s]Extractor Estimating: 290it [02:54,  1.64it/s]Extractor Estimating: 291it [02:54,  1.65it/s]Extractor Estimating: 292it [02:55,  1.61it/s]Extractor Estimating: 293it [02:56,  1.59it/s]Extractor Estimating: 294it [02:56,  1.63it/s]Extractor Estimating: 295it [02:57,  1.62it/s]Extractor Estimating: 296it [02:57,  1.62it/s]Extractor Estimating: 297it [02:58,  1.66it/s]Extractor Estimating: 298it [02:59,  1.71it/s]Extractor Estimating: 299it [02:59,  1.65it/s]Extractor Estimating: 300it [03:00,  1.64it/s]Extractor Estimating: 301it [03:00,  1.63it/s]Extractor Estimating: 302it [03:01,  1.57it/s]Extractor Estimating: 303it [03:02,  1.58it/s]Extractor Estimating: 304it [03:02,  1.56it/s]Extractor Estimating: 305it [03:03,  1.55it/s]Extractor Estimating: 306it [03:04,  1.54it/s]Extractor Estimating: 307it [03:04,  1.59it/s]Extractor Estimating: 308it [03:05,  1.59it/s]Extractor Estimating: 309it [03:06,  1.58it/s]Extractor Estimating: 310it [03:06,  1.57it/s]Extractor Estimating: 311it [03:07,  1.57it/s]Extractor Estimating: 312it [03:08,  1.43it/s]Extractor Estimating: 313it [03:08,  1.48it/s]Extractor Estimating: 314it [03:09,  1.52it/s]Extractor Estimating: 315it [03:10,  1.54it/s]Extractor Estimating: 316it [03:10,  1.55it/s]Extractor Estimating: 317it [03:11,  1.59it/s]Extractor Estimating: 318it [03:12,  1.54it/s]Extractor Estimating: 319it [03:12,  1.56it/s]Extractor Estimating: 320it [03:13,  1.56it/s]Extractor Estimating: 321it [03:13,  1.54it/s]Extractor Estimating: 322it [03:14,  1.56it/s]Extractor Estimating: 323it [03:15,  1.57it/s]Extractor Estimating: 324it [03:15,  1.61it/s]Extractor Estimating: 325it [03:16,  1.62it/s]Extractor Estimating: 326it [03:17,  1.61it/s]Extractor Estimating: 327it [03:17,  1.64it/s]Extractor Estimating: 328it [03:18,  1.64it/s]Extractor Estimating: 329it [03:18,  1.67it/s]Extractor Estimating: 330it [03:19,  1.62it/s]Extractor Estimating: 331it [03:20,  1.63it/s]Extractor Estimating: 332it [03:20,  1.66it/s]Extractor Estimating: 333it [03:21,  1.68it/s]Extractor Estimating: 334it [03:21,  1.66it/s]Extractor Estimating: 335it [03:22,  1.64it/s]Extractor Estimating: 336it [03:23,  1.66it/s]Extractor Estimating: 337it [03:23,  1.56it/s]Extractor Estimating: 338it [03:24,  1.61it/s]Extractor Estimating: 339it [03:25,  1.57it/s]Extractor Estimating: 340it [03:25,  1.62it/s]Extractor Estimating: 341it [03:26,  1.62it/s]Extractor Estimating: 342it [03:26,  1.61it/s]Extractor Estimating: 343it [03:27,  1.65it/s]Extractor Estimating: 344it [03:28,  1.63it/s]Extractor Estimating: 345it [03:28,  1.59it/s]Extractor Estimating: 346it [03:29,  1.59it/s]Extractor Estimating: 347it [03:29,  1.62it/s]Extractor Estimating: 348it [03:30,  1.65it/s]Extractor Estimating: 349it [03:31,  1.67it/s]Extractor Estimating: 350it [03:31,  1.64it/s]Extractor Estimating: 351it [03:32,  1.66it/s]Extractor Estimating: 352it [03:32,  1.64it/s]Extractor Estimating: 353it [03:33,  1.67it/s]Extractor Estimating: 354it [03:34,  1.66it/s]Extractor Estimating: 355it [03:34,  1.59it/s]Extractor Estimating: 356it [03:35,  1.55it/s]Extractor Estimating: 357it [03:36,  1.57it/s]Extractor Estimating: 358it [03:36,  1.65it/s]Extractor Estimating: 359it [03:37,  1.64it/s]Extractor Estimating: 360it [03:37,  1.66it/s]Extractor Estimating: 361it [03:38,  1.68it/s]Extractor Estimating: 362it [03:39,  1.64it/s]Extractor Estimating: 363it [03:39,  1.62it/s]Extractor Estimating: 364it [03:40,  1.59it/s]Extractor Estimating: 365it [03:40,  1.60it/s]Extractor Estimating: 366it [03:41,  1.67it/s]Extractor Estimating: 367it [03:42,  1.64it/s]Extractor Estimating: 368it [03:42,  1.63it/s]Extractor Estimating: 369it [03:43,  1.64it/s]Extractor Estimating: 370it [03:43,  1.64it/s]Extractor Estimating: 371it [03:44,  1.61it/s]Extractor Estimating: 372it [03:45,  1.65it/s]Extractor Estimating: 373it [03:45,  1.65it/s]Extractor Estimating: 374it [03:46,  1.62it/s]Extractor Estimating: 375it [03:46,  1.86it/s]Extractor Estimating: 375it [03:46,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:00,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:00,722 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:00,722 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:00,722 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:00,722 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:58:01,442 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:58:01,442 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:58:01,697 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:58:02,774 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:58:02,775 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:04,959 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:04,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:04,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:04,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:58:04,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:58:05,557 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:58:05,558 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:58:06,235 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:58:06,407 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:58:06,408 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 04:12:47,195 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 04:12:47,218 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7815 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 26094
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26194, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26194, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.962, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.980, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.985, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 74, avg_time 0.976, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 174, avg_time 0.977, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 274, avg_time 2.293, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 48, avg_time 0.979, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 148, avg_time 0.975, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 248, avg_time 0.989, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 22, avg_time 0.978, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 122, avg_time 2.285, loss:nan
g_step 1200, step 222, avg_time 0.984, loss:nan
g_step 1300, step 322, avg_time 0.978, loss:nan
g_step 1400, step 96, avg_time 0.980, loss:nan
g_step 1500, step 196, avg_time 0.977, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 296, avg_time 2.287, loss:nan
g_step 1700, step 70, avg_time 0.980, loss:nan
g_step 1800, step 170, avg_time 0.987, loss:nan
g_step 1900, step 270, avg_time 0.964, loss:nan
g_step 2000, step 44, avg_time 0.990, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 144, avg_time 2.301, loss:nan
g_step 2200, step 244, avg_time 0.984, loss:nan
g_step 2300, step 18, avg_time 0.966, loss:nan
g_step 2400, step 118, avg_time 0.992, loss:nan
g_step 2500, step 218, avg_time 0.973, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 318, avg_time 2.290, loss:nan
g_step 2700, step 92, avg_time 0.977, loss:nan
g_step 2800, step 192, avg_time 0.980, loss:nan
g_step 2900, step 292, avg_time 0.980, loss:nan
g_step 3000, step 66, avg_time 0.975, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 166, avg_time 2.297, loss:nan
g_step 3200, step 266, avg_time 0.978, loss:nan
g_step 3300, step 40, avg_time 0.981, loss:nan
g_step 3400, step 140, avg_time 0.969, loss:nan
g_step 3500, step 240, avg_time 0.976, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 14, avg_time 2.287, loss:nan
g_step 3700, step 114, avg_time 0.986, loss:nan
g_step 3800, step 214, avg_time 0.974, loss:nan
g_step 3900, step 314, avg_time 0.971, loss:nan
g_step 4000, step 88, avg_time 0.982, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 188, avg_time 2.276, loss:nan
g_step 4200, step 288, avg_time 0.989, loss:nan
g_step 4300, step 62, avg_time 0.964, loss:nan
g_step 4400, step 162, avg_time 0.983, loss:nan
g_step 4500, step 262, avg_time 0.981, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 36, avg_time 2.273, loss:nan
g_step 4700, step 136, avg_time 0.978, loss:nan
g_step 4800, step 236, avg_time 0.965, loss:nan
g_step 4900, step 10, avg_time 0.978, loss:nan
g_step 5000, step 110, avg_time 0.974, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 210, avg_time 2.289, loss:nan
g_step 5200, step 310, avg_time 0.971, loss:nan
g_step 5300, step 84, avg_time 0.976, loss:nan
g_step 5400, step 184, avg_time 0.977, loss:nan
g_step 5500, step 284, avg_time 0.976, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 58, avg_time 2.287, loss:nan
g_step 5700, step 158, avg_time 0.978, loss:nan
g_step 5800, step 258, avg_time 0.975, loss:nan
g_step 5900, step 32, avg_time 0.977, loss:nan
g_step 6000, step 132, avg_time 0.983, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 232, avg_time 2.288, loss:nan
g_step 6200, step 6, avg_time 0.978, loss:nan
g_step 6300, step 106, avg_time 0.978, loss:nan
g_step 6400, step 206, avg_time 0.972, loss:nan
g_step 6500, step 306, avg_time 0.978, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 04:12:47 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 04:12:47 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_04-12-47_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 04:12:48 - WARNING - datasets.builder -   Using custom data configuration default-17dcc15e8dc8af81
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-17dcc15e8dc8af81/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 04:12:48,476 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:12:48,477 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:12:48,478 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:12:48,479 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:12:48,490 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:12:48,493 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:12:48,494 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:12:48,494 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:12:48,494 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:12:48,494 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:12:48,494 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 04:12:48,628 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:12:51,685 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 04:12:51,687 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-17dcc15e8dc8af81/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  3.79ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.31ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.48ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.57ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.60ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.63ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  3.89ba/s]100%|██████████| 8/8 [00:01<00:00,  4.23ba/s]100%|██████████| 8/8 [00:01<00:00,  4.30ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.21ba/s] 40%|████      | 2/5 [00:00<00:00,  4.44ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.51ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.56ba/s]100%|██████████| 5/5 [00:01<00:00,  4.77ba/s]100%|██████████| 5/5 [00:01<00:00,  4.62ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.21ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.32ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.72ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.93ba/s]100%|██████████| 8/8 [00:00<00:00, 10.94ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.98ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.32ba/s]100%|██████████| 5/5 [00:00<00:00, 11.03ba/s]100%|██████████| 5/5 [00:00<00:00, 10.76ba/s]
[INFO|trainer.py:414] 2023-08-28 04:12:56,245 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 04:12:56,261 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 04:12:56,261 >>   Num examples = 7900
[INFO|trainer.py:1149] 2023-08-28 04:12:56,261 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 04:12:56,261 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 04:12:56,261 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 04:12:56,262 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 04:12:56,262 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:00<02:54,  3.52it/s]  0%|          | 2/615 [00:00<02:51,  3.58it/s]  0%|          | 3/615 [00:00<02:49,  3.60it/s]  1%|          | 4/615 [00:01<02:49,  3.61it/s]  1%|          | 5/615 [00:01<02:48,  3.62it/s]  1%|          | 6/615 [00:01<02:47,  3.63it/s]  1%|          | 7/615 [00:01<02:48,  3.61it/s]  1%|▏         | 8/615 [00:02<02:47,  3.62it/s]  1%|▏         | 9/615 [00:02<02:47,  3.62it/s]  2%|▏         | 10/615 [00:02<02:46,  3.63it/s]  2%|▏         | 11/615 [00:03<02:46,  3.63it/s]  2%|▏         | 12/615 [00:03<02:46,  3.63it/s]  2%|▏         | 13/615 [00:03<02:45,  3.63it/s]  2%|▏         | 14/615 [00:03<02:45,  3.63it/s]  2%|▏         | 15/615 [00:04<02:45,  3.63it/s]  3%|▎         | 16/615 [00:04<02:44,  3.64it/s]  3%|▎         | 17/615 [00:04<02:44,  3.63it/s]  3%|▎         | 18/615 [00:04<02:45,  3.61it/s]  3%|▎         | 19/615 [00:05<02:45,  3.61it/s]  3%|▎         | 20/615 [00:05<02:44,  3.62it/s]  3%|▎         | 21/615 [00:05<02:43,  3.62it/s]  4%|▎         | 22/615 [00:06<02:43,  3.63it/s]  4%|▎         | 23/615 [00:06<02:43,  3.63it/s]  4%|▍         | 24/615 [00:06<02:42,  3.63it/s]  4%|▍         | 25/615 [00:06<02:42,  3.63it/s]  4%|▍         | 26/615 [00:07<02:42,  3.63it/s]  4%|▍         | 27/615 [00:07<02:42,  3.63it/s]  5%|▍         | 28/615 [00:07<02:41,  3.63it/s]  5%|▍         | 29/615 [00:08<02:43,  3.59it/s]  5%|▍         | 30/615 [00:08<02:42,  3.60it/s]  5%|▌         | 31/615 [00:08<02:41,  3.61it/s]  5%|▌         | 32/615 [00:08<02:41,  3.62it/s]  5%|▌         | 33/615 [00:09<02:40,  3.62it/s]  6%|▌         | 34/615 [00:09<02:40,  3.62it/s]  6%|▌         | 35/615 [00:09<02:40,  3.62it/s]  6%|▌         | 36/615 [00:09<02:39,  3.63it/s]  6%|▌         | 37/615 [00:10<02:39,  3.63it/s]  6%|▌         | 38/615 [00:10<02:38,  3.63it/s]  6%|▋         | 39/615 [00:10<02:38,  3.63it/s]  7%|▋         | 40/615 [00:11<02:39,  3.61it/s]  7%|▋         | 41/615 [00:11<02:38,  3.61it/s]  7%|▋         | 42/615 [00:11<02:38,  3.62it/s]  7%|▋         | 43/615 [00:11<02:37,  3.62it/s]  7%|▋         | 44/615 [00:12<02:37,  3.62it/s]  7%|▋         | 45/615 [00:12<02:37,  3.61it/s]  7%|▋         | 46/615 [00:12<02:38,  3.60it/s]  8%|▊         | 47/615 [00:12<02:38,  3.59it/s]  8%|▊         | 48/615 [00:13<02:38,  3.58it/s]  8%|▊         | 49/615 [00:13<02:38,  3.58it/s]  8%|▊         | 50/615 [00:13<02:38,  3.57it/s]  8%|▊         | 51/615 [00:14<02:38,  3.56it/s]  8%|▊         | 52/615 [00:14<02:38,  3.56it/s]  9%|▊         | 53/615 [00:14<02:37,  3.57it/s]  9%|▉         | 54/615 [00:14<02:36,  3.59it/s]  9%|▉         | 55/615 [00:15<02:35,  3.60it/s]  9%|▉         | 56/615 [00:15<02:35,  3.61it/s]  9%|▉         | 57/615 [00:15<02:34,  3.61it/s]  9%|▉         | 58/615 [00:16<02:34,  3.61it/s] 10%|▉         | 59/615 [00:16<02:33,  3.61it/s] 10%|▉         | 60/615 [00:16<02:33,  3.61it/s] 10%|▉         | 61/615 [00:16<02:33,  3.62it/s] 10%|█         | 62/615 [00:17<02:34,  3.59it/s] 10%|█         | 63/615 [00:17<02:33,  3.60it/s] 10%|█         | 64/615 [00:17<02:32,  3.61it/s] 11%|█         | 65/615 [00:17<02:32,  3.61it/s] 11%|█         | 66/615 [00:18<02:31,  3.61it/s] 11%|█         | 67/615 [00:18<02:31,  3.61it/s] 11%|█         | 68/615 [00:18<02:31,  3.61it/s] 11%|█         | 69/615 [00:19<02:30,  3.62it/s] 11%|█▏        | 70/615 [00:19<02:30,  3.62it/s] 12%|█▏        | 71/615 [00:19<02:30,  3.62it/s] 12%|█▏        | 72/615 [00:19<02:29,  3.62it/s] 12%|█▏        | 73/615 [00:20<02:30,  3.60it/s] 12%|█▏        | 74/615 [00:20<02:29,  3.61it/s] 12%|█▏        | 75/615 [00:20<02:29,  3.61it/s] 12%|█▏        | 76/615 [00:21<02:29,  3.61it/s] 13%|█▎        | 77/615 [00:21<02:28,  3.62it/s] 13%|█▎        | 78/615 [00:21<02:28,  3.62it/s] 13%|█▎        | 79/615 [00:21<02:28,  3.62it/s] 13%|█▎        | 80/615 [00:22<02:27,  3.62it/s] 13%|█▎        | 81/615 [00:22<02:27,  3.62it/s] 13%|█▎        | 82/615 [00:22<02:27,  3.62it/s] 13%|█▎        | 83/615 [00:22<02:26,  3.62it/s] 14%|█▎        | 84/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 85/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 86/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 87/615 [00:24<02:25,  3.62it/s] 14%|█▍        | 88/615 [00:24<02:25,  3.61it/s] 14%|█▍        | 89/615 [00:24<02:25,  3.62it/s] 15%|█▍        | 90/615 [00:24<02:25,  3.60it/s] 15%|█▍        | 91/615 [00:25<02:25,  3.61it/s] 15%|█▍        | 92/615 [00:25<02:24,  3.61it/s] 15%|█▌        | 93/615 [00:25<02:24,  3.61it/s] 15%|█▌        | 94/615 [00:26<02:24,  3.61it/s] 15%|█▌        | 95/615 [00:26<02:23,  3.62it/s] 16%|█▌        | 96/615 [00:26<02:23,  3.62it/s] 16%|█▌        | 97/615 [00:26<02:23,  3.61it/s] 16%|█▌        | 98/615 [00:27<02:22,  3.62it/s] 16%|█▌        | 99/615 [00:27<02:22,  3.62it/s] 16%|█▋        | 100/615 [00:27<02:22,  3.62it/s] 16%|█▋        | 101/615 [00:27<02:23,  3.58it/s] 17%|█▋        | 102/615 [00:28<02:22,  3.59it/s] 17%|█▋        | 103/615 [00:28<02:22,  3.60it/s] 17%|█▋        | 104/615 [00:28<02:21,  3.61it/s] 17%|█▋        | 105/615 [00:29<02:21,  3.61it/s] 17%|█▋        | 106/615 [00:29<02:20,  3.62it/s] 17%|█▋        | 107/615 [00:29<02:20,  3.62it/s] 18%|█▊        | 108/615 [00:29<02:20,  3.62it/s] 18%|█▊        | 109/615 [00:30<02:19,  3.62it/s] 18%|█▊        | 110/615 [00:30<02:19,  3.62it/s] 18%|█▊        | 111/615 [00:30<02:19,  3.62it/s] 18%|█▊        | 112/615 [00:31<02:19,  3.60it/s] 18%|█▊        | 113/615 [00:31<02:19,  3.61it/s] 19%|█▊        | 114/615 [00:31<02:18,  3.61it/s] 19%|█▊        | 115/615 [00:31<02:18,  3.62it/s] 19%|█▉        | 116/615 [00:32<02:17,  3.62it/s] 19%|█▉        | 117/615 [00:32<02:17,  3.62it/s] 19%|█▉        | 118/615 [00:32<02:17,  3.62it/s] 19%|█▉        | 119/615 [00:32<02:17,  3.62it/s] 20%|█▉        | 120/615 [00:33<02:16,  3.62it/s] 20%|█▉        | 121/615 [00:33<02:16,  3.62it/s] 20%|█▉        | 122/615 [00:33<02:16,  3.62it/s] 20%|██        | 123/615 [00:34<02:16,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 04:13:30,436 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:13:30,437 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 04:13:30,437 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.12it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.99it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.16it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.99it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.13it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.67it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.37it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.12it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.28it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.46it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.63it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.74it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.54it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.26it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.16it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.03it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.01it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.17it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.36it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.59it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.66it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.50it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.37it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.16it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.09it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.10it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.22it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.40it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.57it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.52it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.48it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.30it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.17it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.10it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.08it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.19it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.38it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.51it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.61it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.52it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.32it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.20it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.18it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.09it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.24it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.37it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.43it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.58it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.45it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.35it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.18it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.15it/s][A
 44%|████▎     | 267/611 [00:05<00:07, 44.19it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.11it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.20it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.44it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.44it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.55it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.36it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.28it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.17it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.18it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.20it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.36it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.47it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.52it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.41it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.36it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.27it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.11it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.13it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.29it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.41it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.53it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.45it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.39it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.42it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.26it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.21it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.17it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.36it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.49it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.48it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.43it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.37it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.27it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.18it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.12it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.14it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.34it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.44it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.43it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.41it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.33it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.24it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.14it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.09it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.10it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.32it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.48it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.52it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.44it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.34it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.25it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.21it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.12it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.21it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.32it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.49it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.52it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.45it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.29it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.24it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.19it/s][A
 94%|█████████▍| 577/611 [00:12<00:00, 44.17it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.19it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.30it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.29it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.44it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.35it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.31it/s][A                                                 
                                                 [A 20%|██        | 123/615 [00:47<02:16,  3.60it/s]
100%|██████████| 611/611 [00:13<00:00, 44.31it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:13:44,226 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-28 04:13:44,254 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:13:45,798 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:13:45,822 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:13:45,830 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123/special_tokens_map.json
 20%|██        | 124/615 [00:50<41:25,  5.06s/it] 20%|██        | 125/615 [00:50<29:37,  3.63s/it] 20%|██        | 126/615 [00:50<21:23,  2.62s/it] 21%|██        | 127/615 [00:51<15:40,  1.93s/it] 21%|██        | 128/615 [00:51<11:38,  1.43s/it] 21%|██        | 129/615 [00:51<08:48,  1.09s/it] 21%|██        | 130/615 [00:51<06:50,  1.18it/s] 21%|██▏       | 131/615 [00:52<05:27,  1.48it/s] 21%|██▏       | 132/615 [00:52<04:29,  1.79it/s] 22%|██▏       | 133/615 [00:52<03:48,  2.11it/s] 22%|██▏       | 134/615 [00:53<03:20,  2.40it/s] 22%|██▏       | 135/615 [00:53<03:00,  2.66it/s] 22%|██▏       | 136/615 [00:53<02:46,  2.88it/s] 22%|██▏       | 137/615 [00:53<02:36,  3.06it/s] 22%|██▏       | 138/615 [00:54<02:29,  3.19it/s] 23%|██▎       | 139/615 [00:54<02:24,  3.30it/s] 23%|██▎       | 140/615 [00:54<02:20,  3.37it/s] 23%|██▎       | 141/615 [00:55<02:18,  3.43it/s] 23%|██▎       | 142/615 [00:55<02:16,  3.47it/s] 23%|██▎       | 143/615 [00:55<02:16,  3.47it/s] 23%|██▎       | 144/615 [00:55<02:14,  3.49it/s] 24%|██▎       | 145/615 [00:56<02:13,  3.51it/s] 24%|██▎       | 146/615 [00:56<02:13,  3.53it/s] 24%|██▍       | 147/615 [00:56<02:12,  3.53it/s] 24%|██▍       | 148/615 [00:57<02:11,  3.54it/s] 24%|██▍       | 149/615 [00:57<02:11,  3.55it/s] 24%|██▍       | 150/615 [00:57<02:10,  3.55it/s] 25%|██▍       | 151/615 [00:57<02:10,  3.56it/s] 25%|██▍       | 152/615 [00:58<02:10,  3.56it/s] 25%|██▍       | 153/615 [00:58<02:09,  3.56it/s] 25%|██▌       | 154/615 [00:58<02:10,  3.54it/s] 25%|██▌       | 155/615 [00:59<02:09,  3.55it/s] 25%|██▌       | 156/615 [00:59<02:09,  3.55it/s] 26%|██▌       | 157/615 [00:59<02:09,  3.55it/s] 26%|██▌       | 158/615 [00:59<02:08,  3.55it/s] 26%|██▌       | 159/615 [01:00<02:08,  3.55it/s] 26%|██▌       | 160/615 [01:00<02:08,  3.55it/s] 26%|██▌       | 161/615 [01:00<02:07,  3.55it/s] 26%|██▋       | 162/615 [01:00<02:07,  3.56it/s] 27%|██▋       | 163/615 [01:01<02:07,  3.56it/s] 27%|██▋       | 164/615 [01:01<02:06,  3.56it/s] 27%|██▋       | 165/615 [01:01<02:06,  3.57it/s] 27%|██▋       | 166/615 [01:02<02:05,  3.58it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/615 [01:02<02:04,  3.59it/s] 27%|██▋       | 168/615 [01:02<02:04,  3.59it/s] 27%|██▋       | 169/615 [01:02<02:04,  3.60it/s] 28%|██▊       | 170/615 [01:03<02:03,  3.60it/s] 28%|██▊       | 171/615 [01:03<02:03,  3.60it/s] 28%|██▊       | 172/615 [01:03<02:02,  3.61it/s] 28%|██▊       | 173/615 [01:04<02:02,  3.61it/s] 28%|██▊       | 174/615 [01:04<02:02,  3.61it/s] 28%|██▊       | 175/615 [01:04<02:01,  3.61it/s] 29%|██▊       | 176/615 [01:04<02:02,  3.59it/s] 29%|██▉       | 177/615 [01:05<02:02,  3.59it/s] 29%|██▉       | 178/615 [01:05<02:01,  3.59it/s] 29%|██▉       | 179/615 [01:05<02:01,  3.60it/s] 29%|██▉       | 180/615 [01:05<02:00,  3.60it/s] 29%|██▉       | 181/615 [01:06<02:00,  3.61it/s] 30%|██▉       | 182/615 [01:06<02:00,  3.61it/s] 30%|██▉       | 183/615 [01:06<01:59,  3.61it/s] 30%|██▉       | 184/615 [01:07<01:59,  3.61it/s] 30%|███       | 185/615 [01:07<01:59,  3.61it/s] 30%|███       | 186/615 [01:07<01:58,  3.61it/s] 30%|███       | 187/615 [01:07<01:59,  3.60it/s] 31%|███       | 188/615 [01:08<01:58,  3.60it/s] 31%|███       | 189/615 [01:08<01:58,  3.60it/s] 31%|███       | 190/615 [01:08<01:57,  3.61it/s] 31%|███       | 191/615 [01:09<01:57,  3.61it/s] 31%|███       | 192/615 [01:09<01:57,  3.61it/s] 31%|███▏      | 193/615 [01:09<01:57,  3.61it/s] 32%|███▏      | 194/615 [01:09<01:56,  3.61it/s] 32%|███▏      | 195/615 [01:10<01:56,  3.61it/s] 32%|███▏      | 196/615 [01:10<01:55,  3.61it/s] 32%|███▏      | 197/615 [01:10<01:55,  3.61it/s] 32%|███▏      | 198/615 [01:10<01:55,  3.60it/s] 32%|███▏      | 199/615 [01:11<01:55,  3.61it/s] 33%|███▎      | 200/615 [01:11<01:55,  3.60it/s] 33%|███▎      | 201/615 [01:11<01:54,  3.61it/s] 33%|███▎      | 202/615 [01:12<01:54,  3.61it/s] 33%|███▎      | 203/615 [01:12<01:54,  3.61it/s] 33%|███▎      | 204/615 [01:12<01:53,  3.61it/s] 33%|███▎      | 205/615 [01:12<01:53,  3.61it/s] 33%|███▎      | 206/615 [01:13<01:53,  3.61it/s] 34%|███▎      | 207/615 [01:13<01:53,  3.61it/s] 34%|███▍      | 208/615 [01:13<01:52,  3.61it/s] 34%|███▍      | 209/615 [01:14<01:53,  3.59it/s] 34%|███▍      | 210/615 [01:14<01:52,  3.60it/s] 34%|███▍      | 211/615 [01:14<01:52,  3.60it/s] 34%|███▍      | 212/615 [01:14<01:51,  3.61it/s] 35%|███▍      | 213/615 [01:15<01:51,  3.61it/s] 35%|███▍      | 214/615 [01:15<01:51,  3.61it/s] 35%|███▍      | 215/615 [01:15<01:50,  3.61it/s] 35%|███▌      | 216/615 [01:15<01:50,  3.61it/s] 35%|███▌      | 217/615 [01:16<01:50,  3.61it/s] 35%|███▌      | 218/615 [01:16<01:49,  3.61it/s] 36%|███▌      | 219/615 [01:16<01:49,  3.61it/s] 36%|███▌      | 220/615 [01:17<01:49,  3.60it/s] 36%|███▌      | 221/615 [01:17<01:49,  3.60it/s] 36%|███▌      | 222/615 [01:17<01:49,  3.60it/s] 36%|███▋      | 223/615 [01:17<01:48,  3.60it/s] 36%|███▋      | 224/615 [01:18<01:48,  3.61it/s] 37%|███▋      | 225/615 [01:18<01:48,  3.61it/s] 37%|███▋      | 226/615 [01:18<01:47,  3.61it/s] 37%|███▋      | 227/615 [01:19<01:47,  3.61it/s] 37%|███▋      | 228/615 [01:19<01:47,  3.61it/s] 37%|███▋      | 229/615 [01:19<01:46,  3.61it/s] 37%|███▋      | 230/615 [01:19<01:46,  3.61it/s] 38%|███▊      | 231/615 [01:20<01:47,  3.59it/s] 38%|███▊      | 232/615 [01:20<01:46,  3.59it/s] 38%|███▊      | 233/615 [01:20<01:46,  3.60it/s] 38%|███▊      | 234/615 [01:20<01:45,  3.60it/s] 38%|███▊      | 235/615 [01:21<01:45,  3.60it/s] 38%|███▊      | 236/615 [01:21<01:45,  3.61it/s] 39%|███▊      | 237/615 [01:21<01:44,  3.61it/s] 39%|███▊      | 238/615 [01:22<01:44,  3.61it/s] 39%|███▉      | 239/615 [01:22<01:44,  3.61it/s] 39%|███▉      | 240/615 [01:22<01:43,  3.61it/s] 39%|███▉      | 241/615 [01:22<01:43,  3.61it/s] 39%|███▉      | 242/615 [01:23<01:43,  3.60it/s] 40%|███▉      | 243/615 [01:23<01:43,  3.61it/s] 40%|███▉      | 244/615 [01:23<01:42,  3.61it/s] 40%|███▉      | 245/615 [01:24<01:42,  3.61it/s] 40%|████      | 246/615 [01:24<01:42,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 04:14:20,676 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:14:20,676 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 04:14:20,676 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.775, 'eval_samples_per_second': 354.41, 'eval_steps_per_second': 44.356, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.11it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.70it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.99it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.89it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.16it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.47it/s][A
  6%|▌         | 37/611 [00:00<00:13, 44.12it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.97it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.19it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.30it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.43it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.54it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.51it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.38it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.12it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.88it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.93it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.12it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.13it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.39it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.49it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.45it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.22it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.06it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.99it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.03it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.19it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.27it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.36it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.47it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.40it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.34it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.11it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.05it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.12it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.25it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.44it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.49it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.45it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.34it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.30it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.15it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.02it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 43.98it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.13it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.24it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.45it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.45it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.26it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.19it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.14it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.04it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.06it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.20it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.31it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.46it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.51it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.31it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.22it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.23it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.11it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.01it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.01it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.30it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.37it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.34it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.28it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.13it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.07it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.01it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.05it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.32it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.44it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.38it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.16it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.19it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.13it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.02it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.11it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.17it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.31it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.48it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.24it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.13it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.23it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.16it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.11it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.09it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.16it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.32it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.43it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.27it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.21it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.32it/s][A
 79%|███████▉  | 482/611 [00:11<00:02, 44.20it/s][A
 80%|███████▉  | 487/611 [00:11<00:05, 24.61it/s][A
 81%|████████  | 492/611 [00:11<00:04, 28.51it/s][A
 81%|████████▏ | 497/611 [00:11<00:03, 31.97it/s][A
 82%|████████▏ | 502/611 [00:11<00:03, 35.00it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 37.53it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 39.53it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 41.04it/s][A
 85%|████████▌ | 522/611 [00:12<00:02, 41.83it/s][A
 86%|████████▋ | 527/611 [00:12<00:01, 42.09it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 42.39it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 42.57it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 43.09it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 43.59it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 43.92it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.18it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.35it/s][A
 93%|█████████▎| 567/611 [00:13<00:00, 44.17it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 44.10it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.87it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.67it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 43.90it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.10it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.31it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.42it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.50it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:14<00:00, 44.50it/s][A 40%|████      | 246/615 [01:38<01:42,  3.61it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:14:34,794 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 04:14:34,813 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:14:36,659 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:14:36,675 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:14:36,685 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246/special_tokens_map.json
 40%|████      | 247/615 [01:40<31:55,  5.21s/it] 40%|████      | 248/615 [01:41<22:48,  3.73s/it] 40%|████      | 249/615 [01:41<16:25,  2.69s/it] 41%|████      | 250/615 [01:41<11:59,  1.97s/it] 41%|████      | 251/615 [01:42<08:52,  1.46s/it] 41%|████      | 252/615 [01:42<06:42,  1.11s/it] 41%|████      | 253/615 [01:42<05:11,  1.16it/s] 41%|████▏     | 254/615 [01:42<04:07,  1.46it/s] 41%|████▏     | 255/615 [01:43<03:23,  1.77it/s] 42%|████▏     | 256/615 [01:43<02:52,  2.08it/s] 42%|████▏     | 257/615 [01:43<02:30,  2.38it/s] 42%|████▏     | 258/615 [01:44<02:15,  2.64it/s] 42%|████▏     | 259/615 [01:44<02:04,  2.86it/s] 42%|████▏     | 260/615 [01:44<01:56,  3.04it/s] 42%|████▏     | 261/615 [01:44<01:51,  3.17it/s] 43%|████▎     | 262/615 [01:45<01:47,  3.28it/s] 43%|████▎     | 263/615 [01:45<01:44,  3.36it/s] 43%|████▎     | 264/615 [01:45<01:42,  3.41it/s] 43%|████▎     | 265/615 [01:46<01:41,  3.46it/s] 43%|████▎     | 266/615 [01:46<01:40,  3.48it/s] 43%|████▎     | 267/615 [01:46<01:39,  3.50it/s] 44%|████▎     | 268/615 [01:46<01:38,  3.52it/s] 44%|████▎     | 269/615 [01:47<01:38,  3.53it/s] 44%|████▍     | 270/615 [01:47<01:37,  3.53it/s] 44%|████▍     | 271/615 [01:47<01:37,  3.54it/s] 44%|████▍     | 272/615 [01:48<01:36,  3.54it/s] 44%|████▍     | 273/615 [01:48<01:36,  3.55it/s] 45%|████▍     | 274/615 [01:48<01:36,  3.55it/s] 45%|████▍     | 275/615 [01:48<01:35,  3.56it/s] 45%|████▍     | 276/615 [01:49<01:35,  3.56it/s] 45%|████▌     | 277/615 [01:49<01:35,  3.56it/s] 45%|████▌     | 278/615 [01:49<01:34,  3.55it/s] 45%|████▌     | 279/615 [01:49<01:34,  3.55it/s] 46%|████▌     | 280/615 [01:50<01:34,  3.55it/s] 46%|████▌     | 281/615 [01:50<01:34,  3.55it/s] 46%|████▌     | 282/615 [01:50<01:33,  3.55it/s] 46%|████▌     | 283/615 [01:51<01:36,  3.46it/s] 46%|████▌     | 284/615 [01:51<01:35,  3.48it/s] 46%|████▋     | 285/615 [01:51<01:34,  3.50it/s] 47%|████▋     | 286/615 [01:51<01:33,  3.52it/s] 47%|████▋     | 287/615 [01:52<01:32,  3.53it/s] 47%|████▋     | 288/615 [01:52<01:32,  3.54it/s] 47%|████▋     | 289/615 [01:52<01:31,  3.54it/s] 47%|████▋     | 290/615 [01:53<01:31,  3.55it/s] 47%|████▋     | 291/615 [01:53<01:31,  3.55it/s] 47%|████▋     | 292/615 [01:53<01:31,  3.55it/s] 48%|████▊     | 293/615 [01:53<01:30,  3.55it/s] 48%|████▊     | 294/615 [01:54<01:30,  3.55it/s] 48%|████▊     | 295/615 [01:54<01:30,  3.55it/s] 48%|████▊     | 296/615 [01:54<01:29,  3.56it/s] 48%|████▊     | 297/615 [01:55<01:29,  3.56it/s] 48%|████▊     | 298/615 [01:55<01:28,  3.56it/s] 49%|████▊     | 299/615 [01:55<01:28,  3.56it/s] 49%|████▉     | 300/615 [01:55<01:28,  3.56it/s] 49%|████▉     | 301/615 [01:56<01:28,  3.56it/s] 49%|████▉     | 302/615 [01:56<01:27,  3.56it/s] 49%|████▉     | 303/615 [01:56<01:27,  3.56it/s] 49%|████▉     | 304/615 [01:57<01:27,  3.56it/s] 50%|████▉     | 305/615 [01:57<01:27,  3.54it/s] 50%|████▉     | 306/615 [01:57<01:27,  3.55it/s] 50%|████▉     | 307/615 [01:57<01:26,  3.55it/s] 50%|█████     | 308/615 [01:58<01:26,  3.55it/s] 50%|█████     | 309/615 [01:58<01:26,  3.55it/s] 50%|█████     | 310/615 [01:58<01:25,  3.55it/s] 51%|█████     | 311/615 [01:59<01:25,  3.56it/s] 51%|█████     | 312/615 [01:59<01:25,  3.56it/s] 51%|█████     | 313/615 [01:59<01:24,  3.56it/s] 51%|█████     | 314/615 [01:59<01:24,  3.56it/s] 51%|█████     | 315/615 [02:00<01:24,  3.56it/s] 51%|█████▏    | 316/615 [02:00<01:24,  3.54it/s] 52%|█████▏    | 317/615 [02:00<01:24,  3.54it/s] 52%|█████▏    | 318/615 [02:00<01:23,  3.55it/s] 52%|█████▏    | 319/615 [02:01<01:23,  3.55it/s] 52%|█████▏    | 320/615 [02:01<01:23,  3.55it/s] 52%|█████▏    | 321/615 [02:01<01:22,  3.55it/s] 52%|█████▏    | 322/615 [02:02<01:22,  3.55it/s] 53%|█████▎    | 323/615 [02:02<01:22,  3.55it/s] 53%|█████▎    | 324/615 [02:02<01:21,  3.56it/s] 53%|█████▎    | 325/615 [02:02<01:21,  3.56it/s] 53%|█████▎    | 326/615 [02:03<01:21,  3.56it/s] 53%|█████▎    | 327/615 [02:03<01:21,  3.53it/s] 53%|█████▎    | 328/615 [02:03<01:21,  3.54it/s] 53%|█████▎    | 329/615 [02:04<01:20,  3.54it/s] 54%|█████▎    | 330/615 [02:04<01:20,  3.54it/s] 54%|█████▍    | 331/615 [02:04<01:20,  3.55it/s] 54%|█████▍    | 332/615 [02:04<01:19,  3.55it/s] 54%|█████▍    | 333/615 [02:05<01:19,  3.55it/s] 54%|█████▍    | 334/615 [02:05<01:19,  3.55it/s] 54%|█████▍    | 335/615 [02:05<01:18,  3.56it/s] 55%|█████▍    | 336/615 [02:06<01:18,  3.56it/s] 55%|█████▍    | 337/615 [02:06<01:17,  3.56it/s] 55%|█████▍    | 338/615 [02:06<01:18,  3.54it/s] 55%|█████▌    | 339/615 [02:06<01:17,  3.55it/s] 55%|█████▌    | 340/615 [02:07<01:17,  3.55it/s] 55%|█████▌    | 341/615 [02:07<01:17,  3.55it/s] 56%|█████▌    | 342/615 [02:07<01:16,  3.55it/s] 56%|█████▌    | 343/615 [02:08<01:16,  3.55it/s] 56%|█████▌    | 344/615 [02:08<01:16,  3.55it/s] 56%|█████▌    | 345/615 [02:08<01:15,  3.55it/s] 56%|█████▋    | 346/615 [02:08<01:15,  3.56it/s] 56%|█████▋    | 347/615 [02:09<01:15,  3.56it/s] 57%|█████▋    | 348/615 [02:09<01:15,  3.56it/s] 57%|█████▋    | 349/615 [02:09<01:14,  3.55it/s] 57%|█████▋    | 350/615 [02:10<01:14,  3.55it/s] 57%|█████▋    | 351/615 [02:10<01:14,  3.55it/s] 57%|█████▋    | 352/615 [02:10<01:13,  3.56it/s] 57%|█████▋    | 353/615 [02:10<01:13,  3.56it/s] 58%|█████▊    | 354/615 [02:11<01:12,  3.58it/s] 58%|█████▊    | 355/615 [02:11<01:12,  3.59it/s] 58%|█████▊    | 356/615 [02:11<01:12,  3.60it/s] 58%|█████▊    | 357/615 [02:11<01:11,  3.60it/s] 58%|█████▊    | 358/615 [02:12<01:11,  3.60it/s] 58%|█████▊    | 359/615 [02:12<01:11,  3.60it/s] 59%|█████▊    | 360/615 [02:12<01:10,  3.59it/s] 59%|█████▊    | 361/615 [02:13<01:10,  3.60it/s] 59%|█████▉    | 362/615 [02:13<01:10,  3.60it/s] 59%|█████▉    | 363/615 [02:13<01:09,  3.61it/s] 59%|█████▉    | 364/615 [02:13<01:09,  3.61it/s] 59%|█████▉    | 365/615 [02:14<01:09,  3.61it/s] 60%|█████▉    | 366/615 [02:14<01:08,  3.61it/s] 60%|█████▉    | 367/615 [02:14<01:08,  3.60it/s] 60%|█████▉    | 368/615 [02:15<01:08,  3.60it/s] 60%|██████    | 369/615 [02:15<01:08,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 04:15:11,674 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:15:11,674 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 04:15:11,674 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 14.0999, 'eval_samples_per_second': 346.244, 'eval_steps_per_second': 43.334, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.45it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.74it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.75it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.64it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.88it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.49it/s][A
  6%|▌         | 37/611 [00:00<00:13, 44.06it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.03it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.25it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.35it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.32it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.25it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.34it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.16it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.98it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.91it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.97it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.15it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.03it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.40it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.44it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.36it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.33it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.12it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.08it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.25it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.26it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.38it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.33it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.31it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.22it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.09it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.03it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.89it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.17it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.27it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.19it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.27it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.21it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.21it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.13it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.05it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.09it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.24it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.39it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.32it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.34it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.31it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.23it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.12it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.90it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.14it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.28it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.42it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.34it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.33it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.32it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.25it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.13it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.97it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.07it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.26it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.41it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.31it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.41it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.36it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.26it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.11it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.95it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.96it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.26it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.31it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.38it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.36it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.34it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.25it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.11it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.96it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.07it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.27it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.38it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.43it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.35it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.38it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.26it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.08it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 43.95it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.04it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.21it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.32it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.42it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.35it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.30it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.28it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.20it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.09it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.19it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.29it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.25it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.41it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.34it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.32it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.23it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.09it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.02it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.19it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.25it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.33it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.37it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.49it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.32it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.25it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.05it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.99it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.12it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.22it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.39it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.35it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.42it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.32it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.32it/s][A 60%|██████    | 369/615 [02:29<01:08,  3.61it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:15:25,512 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-28 04:15:25,532 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:15:27,692 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:15:27,710 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:15:27,724 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369/special_tokens_map.json
 60%|██████    | 370/615 [02:32<21:23,  5.24s/it] 60%|██████    | 371/615 [02:32<15:15,  3.75s/it] 60%|██████    | 372/615 [02:32<10:58,  2.71s/it] 61%|██████    | 373/615 [02:32<07:59,  1.98s/it] 61%|██████    | 374/615 [02:33<05:54,  1.47s/it] 61%|██████    | 375/615 [02:33<04:26,  1.11s/it] 61%|██████    | 376/615 [02:33<03:25,  1.16it/s] 61%|██████▏   | 377/615 [02:34<02:43,  1.46it/s] 61%|██████▏   | 378/615 [02:34<02:13,  1.78it/s] 62%|██████▏   | 379/615 [02:34<01:52,  2.10it/s] 62%|██████▏   | 380/615 [02:34<01:37,  2.40it/s] 62%|██████▏   | 381/615 [02:35<01:27,  2.67it/s] 62%|██████▏   | 382/615 [02:35<01:20,  2.89it/s] 62%|██████▏   | 383/615 [02:35<01:15,  3.08it/s] 62%|██████▏   | 384/615 [02:35<01:12,  3.21it/s] 63%|██████▎   | 385/615 [02:36<01:09,  3.32it/s] 63%|██████▎   | 386/615 [02:36<01:07,  3.40it/s] 63%|██████▎   | 387/615 [02:36<01:05,  3.46it/s] 63%|██████▎   | 388/615 [02:37<01:04,  3.51it/s] 63%|██████▎   | 389/615 [02:37<01:03,  3.54it/s] 63%|██████▎   | 390/615 [02:37<01:03,  3.56it/s] 64%|██████▎   | 391/615 [02:37<01:02,  3.57it/s] 64%|██████▎   | 392/615 [02:38<01:02,  3.58it/s] 64%|██████▍   | 393/615 [02:38<01:01,  3.59it/s] 64%|██████▍   | 394/615 [02:38<01:01,  3.60it/s] 64%|██████▍   | 395/615 [02:39<01:01,  3.60it/s] 64%|██████▍   | 396/615 [02:39<01:00,  3.60it/s] 65%|██████▍   | 397/615 [02:39<01:00,  3.60it/s] 65%|██████▍   | 398/615 [02:39<01:00,  3.60it/s] 65%|██████▍   | 399/615 [02:40<00:59,  3.60it/s] 65%|██████▌   | 400/615 [02:40<00:59,  3.61it/s] 65%|██████▌   | 401/615 [02:40<00:59,  3.61it/s] 65%|██████▌   | 402/615 [02:40<00:58,  3.61it/s] 66%|██████▌   | 403/615 [02:41<00:58,  3.61it/s] 66%|██████▌   | 404/615 [02:41<00:58,  3.61it/s] 66%|██████▌   | 405/615 [02:41<00:58,  3.61it/s] 66%|██████▌   | 406/615 [02:42<00:58,  3.59it/s] 66%|██████▌   | 407/615 [02:42<00:57,  3.60it/s] 66%|██████▋   | 408/615 [02:42<00:57,  3.60it/s] 67%|██████▋   | 409/615 [02:42<00:57,  3.61it/s] 67%|██████▋   | 410/615 [02:43<00:56,  3.61it/s] 67%|██████▋   | 411/615 [02:43<00:56,  3.59it/s] 67%|██████▋   | 412/615 [02:43<00:56,  3.58it/s] 67%|██████▋   | 413/615 [02:44<00:56,  3.58it/s] 67%|██████▋   | 414/615 [02:44<00:56,  3.57it/s] 67%|██████▋   | 415/615 [02:44<00:55,  3.57it/s] 68%|██████▊   | 416/615 [02:44<00:55,  3.57it/s] 68%|██████▊   | 417/615 [02:45<00:55,  3.54it/s] 68%|██████▊   | 418/615 [02:45<00:55,  3.54it/s] 68%|██████▊   | 419/615 [02:45<00:55,  3.55it/s] 68%|██████▊   | 420/615 [02:46<00:54,  3.55it/s] 68%|██████▊   | 421/615 [02:46<00:54,  3.55it/s] 69%|██████▊   | 422/615 [02:46<00:54,  3.55it/s] 69%|██████▉   | 423/615 [02:46<00:53,  3.56it/s] 69%|██████▉   | 424/615 [02:47<00:53,  3.56it/s] 69%|██████▉   | 425/615 [02:47<00:53,  3.56it/s] 69%|██████▉   | 426/615 [02:47<00:53,  3.56it/s] 69%|██████▉   | 427/615 [02:47<00:52,  3.57it/s] 70%|██████▉   | 428/615 [02:48<00:52,  3.56it/s] 70%|██████▉   | 429/615 [02:48<00:52,  3.55it/s] 70%|██████▉   | 430/615 [02:48<00:52,  3.56it/s] 70%|███████   | 431/615 [02:49<00:51,  3.55it/s] 70%|███████   | 432/615 [02:49<00:51,  3.55it/s] 70%|███████   | 433/615 [02:49<00:51,  3.55it/s] 71%|███████   | 434/615 [02:49<00:51,  3.54it/s] 71%|███████   | 435/615 [02:50<00:50,  3.54it/s] 71%|███████   | 436/615 [02:50<00:50,  3.55it/s] 71%|███████   | 437/615 [02:50<00:50,  3.55it/s] 71%|███████   | 438/615 [02:51<00:49,  3.56it/s] 71%|███████▏  | 439/615 [02:51<00:51,  3.40it/s] 72%|███████▏  | 440/615 [02:51<00:50,  3.44it/s] 72%|███████▏  | 441/615 [02:51<00:50,  3.48it/s] 72%|███████▏  | 442/615 [02:52<00:49,  3.50it/s] 72%|███████▏  | 443/615 [02:52<00:48,  3.51it/s] 72%|███████▏  | 444/615 [02:52<00:48,  3.52it/s] 72%|███████▏  | 445/615 [02:53<00:48,  3.53it/s] 73%|███████▎  | 446/615 [02:53<00:47,  3.54it/s] 73%|███████▎  | 447/615 [02:53<00:47,  3.54it/s] 73%|███████▎  | 448/615 [02:53<00:47,  3.55it/s] 73%|███████▎  | 449/615 [02:54<00:46,  3.55it/s] 73%|███████▎  | 450/615 [02:54<00:46,  3.51it/s] 73%|███████▎  | 451/615 [02:54<00:46,  3.52it/s] 73%|███████▎  | 452/615 [02:55<00:46,  3.53it/s] 74%|███████▎  | 453/615 [02:55<00:45,  3.54it/s] 74%|███████▍  | 454/615 [02:55<00:45,  3.54it/s] 74%|███████▍  | 455/615 [02:55<00:45,  3.55it/s] 74%|███████▍  | 456/615 [02:56<00:44,  3.55it/s] 74%|███████▍  | 457/615 [02:56<00:44,  3.55it/s] 74%|███████▍  | 458/615 [02:56<00:44,  3.56it/s] 75%|███████▍  | 459/615 [02:57<00:43,  3.56it/s] 75%|███████▍  | 460/615 [02:57<00:43,  3.56it/s] 75%|███████▍  | 461/615 [02:57<00:43,  3.51it/s] 75%|███████▌  | 462/615 [02:57<00:43,  3.53it/s] 75%|███████▌  | 463/615 [02:58<00:42,  3.54it/s] 75%|███████▌  | 464/615 [02:58<00:42,  3.55it/s] 76%|███████▌  | 465/615 [02:58<00:42,  3.55it/s] 76%|███████▌  | 466/615 [02:59<00:41,  3.56it/s] 76%|███████▌  | 467/615 [02:59<00:41,  3.56it/s] 76%|███████▌  | 468/615 [02:59<00:41,  3.56it/s] 76%|███████▋  | 469/615 [02:59<00:41,  3.56it/s] 76%|███████▋  | 470/615 [03:00<00:40,  3.56it/s] 77%|███████▋  | 471/615 [03:00<00:40,  3.56it/s] 77%|███████▋  | 472/615 [03:00<00:40,  3.55it/s] 77%|███████▋  | 473/615 [03:00<00:39,  3.55it/s] 77%|███████▋  | 474/615 [03:01<00:39,  3.55it/s] 77%|███████▋  | 475/615 [03:01<00:39,  3.55it/s] 77%|███████▋  | 476/615 [03:01<00:39,  3.55it/s] 78%|███████▊  | 477/615 [03:02<00:38,  3.55it/s] 78%|███████▊  | 478/615 [03:02<00:38,  3.56it/s] 78%|███████▊  | 479/615 [03:02<00:38,  3.56it/s] 78%|███████▊  | 480/615 [03:02<00:37,  3.56it/s] 78%|███████▊  | 481/615 [03:03<00:37,  3.56it/s] 78%|███████▊  | 482/615 [03:03<00:37,  3.56it/s] 79%|███████▊  | 483/615 [03:03<00:37,  3.55it/s] 79%|███████▊  | 484/615 [03:04<00:36,  3.55it/s] 79%|███████▉  | 485/615 [03:04<00:36,  3.55it/s] 79%|███████▉  | 486/615 [03:04<00:36,  3.55it/s] 79%|███████▉  | 487/615 [03:04<00:36,  3.55it/s] 79%|███████▉  | 488/615 [03:05<00:35,  3.55it/s] 80%|███████▉  | 489/615 [03:05<00:35,  3.55it/s] 80%|███████▉  | 490/615 [03:05<00:35,  3.55it/s] 80%|███████▉  | 491/615 [03:06<00:34,  3.55it/s] 80%|████████  | 492/615 [03:06<00:34,  3.56it/s][INFO|trainer.py:2140] 2023-08-28 04:16:02,716 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:16:02,716 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 04:16:02,716 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8063, 'eval_samples_per_second': 353.608, 'eval_steps_per_second': 44.255, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.07it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.67it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.92it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.90it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.91it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.58it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.23it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.06it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.17it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.33it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.48it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.58it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.39it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.25it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.03it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.98it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.01it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.14it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.30it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.45it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.51it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.41it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.25it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.11it/s][A
 21%|██        | 127/611 [00:02<00:11, 44.00it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.02it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.01it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.14it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.28it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.42it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.33it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.24it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.11it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.02it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.02it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.82it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.18it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.40it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.49it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.40it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.29it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.18it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.13it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.14it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.12it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.24it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.44it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.43it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.34it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.19it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.22it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.12it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.20it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.16it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.25it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.40it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.39it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.30it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.23it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.28it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.21it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.15it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.22it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.24it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.40it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.40it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.33it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.21it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.30it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.21it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.17it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.16it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.28it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.42it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.34it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.29it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.25it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.15it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.15it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.13it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.27it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.43it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.39it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.37it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.29it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.23it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.20it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.20it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.08it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.17it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.38it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.42it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.32it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.28it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.11it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.31it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.25it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.14it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.20it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.37it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.37it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.37it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.27it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.26it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.24it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.24it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.18it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.26it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.39it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.32it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.31it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.24it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.28it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.20it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.20it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.11it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.26it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.32it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.39it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.29it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.20it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.20it/s][A 80%|████████  | 492/615 [03:20<00:34,  3.56it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:16:16,533 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 04:16:16,559 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:16:18,847 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:16:18,866 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:16:18,877 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492/special_tokens_map.json
 80%|████████  | 493/615 [03:23<10:41,  5.26s/it] 80%|████████  | 494/615 [03:23<07:35,  3.77s/it] 80%|████████  | 495/615 [03:23<05:26,  2.72s/it] 81%|████████  | 496/615 [03:24<03:56,  1.99s/it] 81%|████████  | 497/615 [03:24<02:54,  1.48s/it] 81%|████████  | 498/615 [03:24<02:10,  1.12s/it] 81%|████████  | 499/615 [03:24<01:40,  1.15it/s] 81%|████████▏ | 500/615 [03:25<01:19,  1.45it/s]                                                  81%|████████▏ | 500/615 [03:25<01:19,  1.45it/s] 81%|████████▏ | 501/615 [03:25<01:04,  1.76it/s] 82%|████████▏ | 502/615 [03:25<00:54,  2.07it/s] 82%|████████▏ | 503/615 [03:26<00:47,  2.37it/s] 82%|████████▏ | 504/615 [03:26<00:42,  2.63it/s] 82%|████████▏ | 505/615 [03:26<00:38,  2.85it/s] 82%|████████▏ | 506/615 [03:26<00:35,  3.03it/s] 82%|████████▏ | 507/615 [03:27<00:34,  3.18it/s] 83%|████████▎ | 508/615 [03:27<00:32,  3.26it/s] 83%|████████▎ | 509/615 [03:27<00:31,  3.35it/s] 83%|████████▎ | 510/615 [03:27<00:30,  3.40it/s] 83%|████████▎ | 511/615 [03:28<00:30,  3.45it/s] 83%|████████▎ | 512/615 [03:28<00:29,  3.48it/s] 83%|████████▎ | 513/615 [03:28<00:29,  3.50it/s] 84%|████████▎ | 514/615 [03:29<00:28,  3.52it/s] 84%|████████▎ | 515/615 [03:29<00:28,  3.55it/s] 84%|████████▍ | 516/615 [03:29<00:27,  3.56it/s] 84%|████████▍ | 517/615 [03:29<00:27,  3.58it/s] 84%|████████▍ | 518/615 [03:30<00:27,  3.59it/s] 84%|████████▍ | 519/615 [03:30<00:27,  3.53it/s] 85%|████████▍ | 520/615 [03:30<00:26,  3.55it/s] 85%|████████▍ | 521/615 [03:31<00:26,  3.57it/s] 85%|████████▍ | 522/615 [03:31<00:25,  3.58it/s] 85%|████████▌ | 523/615 [03:31<00:25,  3.59it/s] 85%|████████▌ | 524/615 [03:31<00:25,  3.60it/s] 85%|████████▌ | 525/615 [03:32<00:24,  3.60it/s] 86%|████████▌ | 526/615 [03:32<00:24,  3.61it/s] 86%|████████▌ | 527/615 [03:32<00:24,  3.61it/s] 86%|████████▌ | 528/615 [03:33<00:24,  3.61it/s] 86%|████████▌ | 529/615 [03:33<00:23,  3.60it/s] 86%|████████▌ | 530/615 [03:33<00:23,  3.59it/s] 86%|████████▋ | 531/615 [03:33<00:23,  3.59it/s] 87%|████████▋ | 532/615 [03:34<00:23,  3.60it/s] 87%|████████▋ | 533/615 [03:34<00:22,  3.60it/s] 87%|████████▋ | 534/615 [03:34<00:22,  3.61it/s] 87%|████████▋ | 535/615 [03:34<00:22,  3.61it/s] 87%|████████▋ | 536/615 [03:35<00:21,  3.61it/s] 87%|████████▋ | 537/615 [03:35<00:21,  3.61it/s] 87%|████████▋ | 538/615 [03:35<00:21,  3.61it/s] 88%|████████▊ | 539/615 [03:36<00:21,  3.61it/s] 88%|████████▊ | 540/615 [03:36<00:20,  3.61it/s] 88%|████████▊ | 541/615 [03:36<00:20,  3.55it/s] 88%|████████▊ | 542/615 [03:36<00:20,  3.56it/s] 88%|████████▊ | 543/615 [03:37<00:20,  3.58it/s] 88%|████████▊ | 544/615 [03:37<00:19,  3.58it/s] 89%|████████▊ | 545/615 [03:37<00:19,  3.59it/s] 89%|████████▉ | 546/615 [03:38<00:19,  3.60it/s] 89%|████████▉ | 547/615 [03:38<00:18,  3.60it/s] 89%|████████▉ | 548/615 [03:38<00:18,  3.61it/s] 89%|████████▉ | 549/615 [03:38<00:18,  3.61it/s] 89%|████████▉ | 550/615 [03:39<00:18,  3.61it/s] 90%|████████▉ | 551/615 [03:39<00:17,  3.61it/s] 90%|████████▉ | 552/615 [03:39<00:17,  3.59it/s] 90%|████████▉ | 553/615 [03:39<00:17,  3.60it/s] 90%|█████████ | 554/615 [03:40<00:16,  3.60it/s] 90%|█████████ | 555/615 [03:40<00:16,  3.60it/s] 90%|█████████ | 556/615 [03:40<00:16,  3.61it/s] 91%|█████████ | 557/615 [03:41<00:16,  3.61it/s] 91%|█████████ | 558/615 [03:41<00:15,  3.61it/s] 91%|█████████ | 559/615 [03:41<00:15,  3.61it/s] 91%|█████████ | 560/615 [03:41<00:15,  3.61it/s] 91%|█████████ | 561/615 [03:42<00:14,  3.61it/s] 91%|█████████▏| 562/615 [03:42<00:14,  3.61it/s] 92%|█████████▏| 563/615 [03:42<00:14,  3.58it/s] 92%|█████████▏| 564/615 [03:43<00:14,  3.59it/s] 92%|█████████▏| 565/615 [03:43<00:13,  3.60it/s] 92%|█████████▏| 566/615 [03:43<00:13,  3.60it/s] 92%|█████████▏| 567/615 [03:43<00:13,  3.60it/s] 92%|█████████▏| 568/615 [03:44<00:13,  3.61it/s] 93%|█████████▎| 569/615 [03:44<00:12,  3.61it/s] 93%|█████████▎| 570/615 [03:44<00:12,  3.61it/s] 93%|█████████▎| 571/615 [03:44<00:12,  3.61it/s] 93%|█████████▎| 572/615 [03:45<00:11,  3.61it/s] 93%|█████████▎| 573/615 [03:45<00:11,  3.61it/s] 93%|█████████▎| 574/615 [03:45<00:11,  3.60it/s] 93%|█████████▎| 575/615 [03:46<00:11,  3.60it/s] 94%|█████████▎| 576/615 [03:46<00:10,  3.60it/s] 94%|█████████▍| 577/615 [03:46<00:10,  3.60it/s] 94%|█████████▍| 578/615 [03:46<00:10,  3.61it/s] 94%|█████████▍| 579/615 [03:47<00:09,  3.61it/s] 94%|█████████▍| 580/615 [03:47<00:09,  3.61it/s] 94%|█████████▍| 581/615 [03:47<00:09,  3.61it/s] 95%|█████████▍| 582/615 [03:48<00:09,  3.61it/s] 95%|█████████▍| 583/615 [03:48<00:08,  3.61it/s] 95%|█████████▍| 584/615 [03:48<00:08,  3.61it/s] 95%|█████████▌| 585/615 [03:48<00:08,  3.59it/s] 95%|█████████▌| 586/615 [03:49<00:08,  3.59it/s] 95%|█████████▌| 587/615 [03:49<00:07,  3.60it/s] 96%|█████████▌| 588/615 [03:49<00:07,  3.60it/s] 96%|█████████▌| 589/615 [03:49<00:07,  3.59it/s] 96%|█████████▌| 590/615 [03:50<00:06,  3.60it/s] 96%|█████████▌| 591/615 [03:50<00:06,  3.60it/s] 96%|█████████▋| 592/615 [03:50<00:06,  3.60it/s] 96%|█████████▋| 593/615 [03:51<00:06,  3.60it/s] 97%|█████████▋| 594/615 [03:51<00:05,  3.61it/s] 97%|█████████▋| 595/615 [03:51<00:05,  3.52it/s] 97%|█████████▋| 596/615 [03:51<00:05,  3.53it/s] 97%|█████████▋| 597/615 [03:52<00:05,  3.56it/s] 97%|█████████▋| 598/615 [03:52<00:04,  3.57it/s] 97%|█████████▋| 599/615 [03:52<00:04,  3.58it/s] 98%|█████████▊| 600/615 [03:53<00:04,  3.59it/s] 98%|█████████▊| 601/615 [03:53<00:03,  3.60it/s] 98%|█████████▊| 602/615 [03:53<00:03,  3.60it/s] 98%|█████████▊| 603/615 [03:53<00:03,  3.61it/s] 98%|█████████▊| 604/615 [03:54<00:03,  3.61it/s] 98%|█████████▊| 605/615 [03:54<00:02,  3.61it/s] 99%|█████████▊| 606/615 [03:54<00:02,  3.61it/s] 99%|█████████▊| 607/615 [03:54<00:02,  3.59it/s] 99%|█████████▉| 608/615 [03:55<00:01,  3.60it/s] 99%|█████████▉| 609/615 [03:55<00:01,  3.60it/s] 99%|█████████▉| 610/615 [03:55<00:01,  3.60it/s] 99%|█████████▉| 611/615 [03:56<00:01,  3.60it/s]100%|█████████▉| 612/615 [03:56<00:00,  3.60it/s]100%|█████████▉| 613/615 [03:56<00:00,  3.61it/s]100%|█████████▉| 614/615 [03:56<00:00,  3.61it/s]100%|██████████| 615/615 [03:57<00:00,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 04:16:53,451 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:16:53,451 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 04:16:53,451 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.7984, 'eval_samples_per_second': 353.808, 'eval_steps_per_second': 44.28, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.64it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.56it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.63it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.70it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.11it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.70it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.35it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.18it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.38it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.44it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.55it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.49it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.35it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.26it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.15it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.07it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.05it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.15it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.31it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.44it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.52it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.36it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.25it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.09it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.06it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.13it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.17it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.33it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.41it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.47it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.37it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.30it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.16it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.09it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.03it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.18it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.26it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.44it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.43it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.43it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.30it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.18it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.12it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.09it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.21it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.34it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.40it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.38it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.36it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.26it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.12it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.04it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.07it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.14it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.32it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.45it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.44it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.35it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.28it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.16it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.08it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.16it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.23it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.28it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.31it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.42it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.42it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.33it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.10it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.17it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.22it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.32it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.23it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.30it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.35it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.36it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.28it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.15it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.13it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.20it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.31it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.40it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.33it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.37it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.32it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.29it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.20it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.17it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.26it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.26it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.31it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.31it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.39it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.32it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.26it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.17it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.27it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.21it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.24it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.35it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.27it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.36it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.33it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.29it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.24it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.20it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.18it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.17it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.32it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.27it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.36it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.37it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.27it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.23it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.17it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.19it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 43.97it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.27it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.26it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.36it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.38it/s][A                                                 
                                                 [A100%|██████████| 615/615 [04:10<00:00,  3.62it/s]
100%|██████████| 611/611 [00:13<00:00, 44.38it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:17:07,262 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-28 04:17:07,279 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:17:08,811 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:17:08,827 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:17:08,839 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 04:17:09,070 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 04:17:09,070 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123 (score: 0.9401851296424866).
                                                 100%|██████████| 615/615 [04:14<00:00,  3.62it/s]100%|██████████| 615/615 [04:14<00:00,  2.42it/s]
[INFO|trainer.py:1894] 2023-08-28 04:17:10,696 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 04:17:10,714 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:17:12,713 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:17:12,729 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:17:12,739 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:17:12,939 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:12,940 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:12,940 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:12,940 >>   train_runtime            = 0:04:14.42
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:12,940 >>   train_samples            =       7900
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:12,940 >>   train_samples_per_second =    155.251
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:12,940 >>   train_steps_per_second   =      2.417
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.7915, 'eval_samples_per_second': 353.987, 'eval_steps_per_second': 44.303, 'epoch': 5.0}
{'train_runtime': 254.426, 'train_samples_per_second': 155.251, 'train_steps_per_second': 2.417, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 04:17:12 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 04:17:12,978 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:17:12,978 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 04:17:12,978 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.55it/s]  2%|▏         | 12/611 [00:00<00:12, 48.81it/s]  3%|▎         | 17/611 [00:00<00:12, 47.05it/s]  4%|▎         | 22/611 [00:00<00:12, 46.39it/s]  4%|▍         | 27/611 [00:00<00:12, 45.95it/s]  5%|▌         | 32/611 [00:00<00:12, 45.61it/s]  6%|▌         | 37/611 [00:00<00:12, 45.45it/s]  7%|▋         | 42/611 [00:00<00:12, 44.91it/s]  8%|▊         | 47/611 [00:01<00:12, 44.31it/s]  9%|▊         | 52/611 [00:01<00:12, 43.96it/s]  9%|▉         | 57/611 [00:01<00:12, 44.12it/s] 10%|█         | 62/611 [00:01<00:12, 44.18it/s] 11%|█         | 67/611 [00:01<00:12, 44.44it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.64it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.75it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.78it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.43it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.13it/s] 16%|█▌        | 97/611 [00:02<00:11, 43.93it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.89it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.09it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.37it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.49it/s] 20%|█▉        | 122/611 [00:02<00:10, 44.68it/s] 21%|██        | 127/611 [00:02<00:10, 44.72it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.55it/s] 22%|██▏       | 137/611 [00:03<00:10, 44.28it/s] 23%|██▎       | 142/611 [00:03<00:10, 44.08it/s] 24%|██▍       | 147/611 [00:03<00:10, 44.09it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.13it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.33it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.45it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.63it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.66it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.51it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.23it/s] 31%|███       | 187/611 [00:04<00:09, 44.14it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.13it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.23it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.21it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.45it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.60it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.63it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.33it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.22it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.19it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.18it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.24it/s] 40%|████      | 247/611 [00:05<00:08, 44.26it/s] 41%|████      | 252/611 [00:05<00:08, 44.49it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.63it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.66it/s] 44%|████▎     | 267/611 [00:05<00:07, 44.45it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.26it/s] 45%|████▌     | 277/611 [00:06<00:07, 44.17it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.20it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.21it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.38it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.42it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.64it/s] 50%|█████     | 307/611 [00:06<00:06, 44.55it/s] 51%|█████     | 312/611 [00:07<00:06, 44.47it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.29it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.18it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.21it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.27it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.29it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.36it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.52it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.54it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.40it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.27it/s] 60%|██████    | 367/611 [00:08<00:05, 44.18it/s] 61%|██████    | 372/611 [00:08<00:05, 44.28it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.25it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.23it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.35it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.49it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.54it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.35it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.30it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.27it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.29it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.20it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.43it/s] 71%|███████   | 432/611 [00:09<00:04, 44.47it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.58it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.46it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.36it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.32it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.18it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.23it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.39it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.42it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.44it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.58it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.40it/s] 81%|████████  | 492/611 [00:11<00:02, 44.34it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.21it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.19it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.22it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.32it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.40it/s] 85%|████████▌ | 522/611 [00:11<00:02, 44.47it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.52it/s] 87%|████████▋ | 532/611 [00:11<00:01, 44.42it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.28it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.11it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.14it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.14it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.25it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.37it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.49it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.45it/s] 94%|█████████▍| 577/611 [00:12<00:00, 44.40it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.24it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.08it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.13it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.10it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.24it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.30it/s]100%|██████████| 611/611 [00:13<00:00, 44.44it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:17:26,743 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:26,743 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:26,743 >>   eval_loss               =     0.9402
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:26,743 >>   eval_runtime            = 0:00:13.76
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:26,744 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:26,744 >>   eval_samples_per_second =    354.664
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:26,744 >>   eval_steps_per_second   =     44.388
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:17:26,744 >>   perplexity              =     2.5605
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:33,757 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:33,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:33,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:33,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:33,762 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:17:34,396 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:17:34,397 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:17:34,986 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:17:36,025 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:17:36,025 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:38,837 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:38,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:38,843 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:38,843 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:17:38,843 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:17:39,510 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:17:39,511 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:17:40,085 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:17:40,255 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:17:40,255 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.70it/s]Extractor Predicting: 8it [00:04,  1.74it/s]Extractor Predicting: 9it [00:05,  1.82it/s]Extractor Predicting: 10it [00:05,  1.90it/s]Extractor Predicting: 11it [00:06,  1.87it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.84it/s]Extractor Predicting: 14it [00:07,  1.86it/s]Extractor Predicting: 15it [00:08,  1.84it/s]Extractor Predicting: 16it [00:08,  1.87it/s]Extractor Predicting: 17it [00:09,  1.86it/s]Extractor Predicting: 18it [00:09,  1.87it/s]Extractor Predicting: 19it [00:10,  1.91it/s]Extractor Predicting: 20it [00:10,  1.92it/s]Extractor Predicting: 21it [00:11,  1.92it/s]Extractor Predicting: 22it [00:11,  1.98it/s]Extractor Predicting: 23it [00:12,  1.98it/s]Extractor Predicting: 24it [00:12,  1.98it/s]Extractor Predicting: 25it [00:13,  1.95it/s]Extractor Predicting: 26it [00:14,  1.91it/s]Extractor Predicting: 27it [00:14,  1.92it/s]Extractor Predicting: 28it [00:15,  1.91it/s]Extractor Predicting: 29it [00:15,  1.91it/s]Extractor Predicting: 30it [00:16,  1.93it/s]Extractor Predicting: 31it [00:16,  1.94it/s]Extractor Predicting: 32it [00:17,  1.97it/s]Extractor Predicting: 33it [00:17,  1.94it/s]Extractor Predicting: 34it [00:18,  1.91it/s]Extractor Predicting: 35it [00:18,  1.91it/s]Extractor Predicting: 36it [00:19,  1.88it/s]Extractor Predicting: 37it [00:19,  1.92it/s]Extractor Predicting: 38it [00:20,  1.92it/s]Extractor Predicting: 39it [00:20,  1.88it/s]Extractor Predicting: 40it [00:21,  1.92it/s]Extractor Predicting: 41it [00:21,  1.88it/s]Extractor Predicting: 42it [00:22,  1.89it/s]Extractor Predicting: 43it [00:22,  1.88it/s]Extractor Predicting: 44it [00:23,  1.88it/s]Extractor Predicting: 45it [00:23,  1.91it/s]Extractor Predicting: 46it [00:24,  1.90it/s]Extractor Predicting: 47it [00:25,  1.89it/s]Extractor Predicting: 48it [00:25,  1.88it/s]Extractor Predicting: 49it [00:26,  1.88it/s]Extractor Predicting: 50it [00:26,  1.84it/s]Extractor Predicting: 51it [00:27,  1.85it/s]Extractor Predicting: 52it [00:27,  1.88it/s]Extractor Predicting: 53it [00:28,  1.86it/s]Extractor Predicting: 54it [00:28,  1.85it/s]Extractor Predicting: 55it [00:29,  1.78it/s]Extractor Predicting: 56it [00:30,  1.73it/s]Extractor Predicting: 57it [00:30,  1.76it/s]Extractor Predicting: 58it [00:31,  1.75it/s]Extractor Predicting: 59it [00:31,  1.79it/s]Extractor Predicting: 60it [00:32,  1.78it/s]Extractor Predicting: 61it [00:32,  1.78it/s]Extractor Predicting: 62it [00:33,  1.74it/s]Extractor Predicting: 63it [00:34,  1.75it/s]Extractor Predicting: 64it [00:34,  1.76it/s]Extractor Predicting: 65it [00:35,  1.74it/s]Extractor Predicting: 66it [00:35,  1.70it/s]Extractor Predicting: 67it [00:36,  1.74it/s]Extractor Predicting: 68it [00:36,  1.78it/s]Extractor Predicting: 69it [00:37,  1.76it/s]Extractor Predicting: 70it [00:38,  1.73it/s]Extractor Predicting: 71it [00:38,  1.75it/s]Extractor Predicting: 72it [00:39,  1.71it/s]Extractor Predicting: 73it [00:39,  1.75it/s]Extractor Predicting: 74it [00:40,  1.74it/s]Extractor Predicting: 75it [00:40,  1.74it/s]Extractor Predicting: 76it [00:41,  1.76it/s]Extractor Predicting: 77it [00:42,  1.75it/s]Extractor Predicting: 78it [00:42,  1.78it/s]Extractor Predicting: 79it [00:43,  1.81it/s]Extractor Predicting: 80it [00:43,  1.80it/s]Extractor Predicting: 81it [00:44,  1.80it/s]Extractor Predicting: 82it [00:44,  1.77it/s]Extractor Predicting: 83it [00:45,  1.78it/s]Extractor Predicting: 84it [00:45,  1.77it/s]Extractor Predicting: 85it [00:46,  1.76it/s]Extractor Predicting: 86it [00:47,  1.74it/s]Extractor Predicting: 87it [00:47,  1.74it/s]Extractor Predicting: 88it [00:48,  1.69it/s]Extractor Predicting: 89it [00:48,  1.70it/s]Extractor Predicting: 90it [00:49,  1.70it/s]Extractor Predicting: 91it [00:50,  1.70it/s]Extractor Predicting: 92it [00:50,  1.77it/s]Extractor Predicting: 93it [00:51,  1.85it/s]Extractor Predicting: 94it [00:51,  1.84it/s]Extractor Predicting: 95it [00:52,  1.83it/s]Extractor Predicting: 96it [00:52,  1.83it/s]Extractor Predicting: 97it [00:53,  1.86it/s]Extractor Predicting: 98it [00:53,  1.82it/s]Extractor Predicting: 99it [00:54,  1.74it/s]Extractor Predicting: 100it [00:55,  1.61it/s]Extractor Predicting: 101it [00:55,  1.57it/s]Extractor Predicting: 102it [00:56,  1.59it/s]Extractor Predicting: 103it [00:57,  1.64it/s]Extractor Predicting: 104it [00:57,  1.68it/s]Extractor Predicting: 105it [00:58,  1.72it/s]Extractor Predicting: 106it [00:58,  1.79it/s]Extractor Predicting: 107it [00:59,  1.79it/s]Extractor Predicting: 108it [00:59,  1.83it/s]Extractor Predicting: 109it [01:00,  1.82it/s]Extractor Predicting: 110it [01:00,  1.82it/s]Extractor Predicting: 111it [01:01,  1.85it/s]Extractor Predicting: 112it [01:01,  1.86it/s]Extractor Predicting: 113it [01:02,  1.79it/s]Extractor Predicting: 114it [01:03,  1.77it/s]Extractor Predicting: 115it [01:03,  1.79it/s]Extractor Predicting: 116it [01:04,  1.75it/s]Extractor Predicting: 117it [01:04,  1.73it/s]Extractor Predicting: 118it [01:05,  1.74it/s]Extractor Predicting: 119it [01:05,  1.71it/s]Extractor Predicting: 120it [01:06,  1.68it/s]Extractor Predicting: 121it [01:07,  1.69it/s]Extractor Predicting: 122it [01:07,  1.70it/s]Extractor Predicting: 123it [01:08,  1.74it/s]Extractor Predicting: 124it [01:08,  1.74it/s]Extractor Predicting: 125it [01:09,  1.75it/s]Extractor Predicting: 126it [01:10,  1.74it/s]Extractor Predicting: 127it [01:10,  1.74it/s]Extractor Predicting: 128it [01:11,  1.74it/s]Extractor Predicting: 129it [01:11,  1.78it/s]Extractor Predicting: 130it [01:12,  1.69it/s]Extractor Predicting: 131it [01:12,  1.71it/s]Extractor Predicting: 132it [01:13,  1.76it/s]Extractor Predicting: 133it [01:14,  1.73it/s]Extractor Predicting: 134it [01:14,  1.74it/s]Extractor Predicting: 135it [01:15,  1.72it/s]Extractor Predicting: 136it [01:15,  1.74it/s]Extractor Predicting: 137it [01:16,  1.70it/s]Extractor Predicting: 138it [01:16,  1.73it/s]Extractor Predicting: 139it [01:17,  1.70it/s]Extractor Predicting: 140it [01:18,  1.69it/s]Extractor Predicting: 141it [01:18,  1.71it/s]Extractor Predicting: 142it [01:19,  1.73it/s]Extractor Predicting: 143it [01:19,  1.70it/s]Extractor Predicting: 144it [01:20,  1.74it/s]Extractor Predicting: 145it [01:20,  1.79it/s]Extractor Predicting: 146it [01:21,  1.77it/s]Extractor Predicting: 147it [01:22,  1.73it/s]Extractor Predicting: 148it [01:22,  1.75it/s]Extractor Predicting: 149it [01:23,  1.72it/s]Extractor Predicting: 150it [01:23,  1.71it/s]Extractor Predicting: 151it [01:24,  1.70it/s]Extractor Predicting: 152it [01:25,  1.70it/s]Extractor Predicting: 153it [01:25,  1.70it/s]Extractor Predicting: 154it [01:26,  1.71it/s]Extractor Predicting: 155it [01:26,  1.70it/s]Extractor Predicting: 156it [01:27,  1.64it/s]Extractor Predicting: 157it [01:28,  1.60it/s]Extractor Predicting: 158it [01:28,  1.57it/s]Extractor Predicting: 159it [01:29,  1.61it/s]Extractor Predicting: 160it [01:30,  1.64it/s]Extractor Predicting: 161it [01:30,  1.67it/s]Extractor Predicting: 162it [01:31,  1.67it/s]Extractor Predicting: 163it [01:31,  1.68it/s]Extractor Predicting: 164it [01:32,  1.71it/s]Extractor Predicting: 165it [01:32,  1.70it/s]Extractor Predicting: 166it [01:33,  1.74it/s]Extractor Predicting: 167it [01:34,  1.73it/s]Extractor Predicting: 168it [01:34,  1.73it/s]Extractor Predicting: 169it [01:35,  1.74it/s]Extractor Predicting: 170it [01:35,  1.74it/s]Extractor Predicting: 171it [01:36,  1.76it/s]Extractor Predicting: 172it [01:36,  1.79it/s]Extractor Predicting: 173it [01:37,  1.73it/s]Extractor Predicting: 174it [01:38,  1.75it/s]Extractor Predicting: 175it [01:38,  1.71it/s]Extractor Predicting: 176it [01:39,  1.74it/s]Extractor Predicting: 177it [01:39,  1.71it/s]Extractor Predicting: 178it [01:40,  1.71it/s]Extractor Predicting: 179it [01:41,  1.71it/s]Extractor Predicting: 180it [01:41,  1.72it/s]Extractor Predicting: 181it [01:42,  1.72it/s]Extractor Predicting: 182it [01:42,  1.55it/s]Extractor Predicting: 183it [01:43,  1.55it/s]Extractor Predicting: 184it [01:44,  1.60it/s]Extractor Predicting: 185it [01:44,  1.75it/s]Extractor Predicting: 185it [01:44,  1.77it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:33,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:33,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:33,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:33,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:33,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:19:34,674 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:19:34,674 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:19:35,428 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:19:36,426 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:19:36,426 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:38,725 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:38,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:38,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:38,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:19:38,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:19:39,518 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:19:39,519 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:19:40,121 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:19:40,277 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:19:40,277 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 4it [00:02,  1.78it/s]Extractor Predicting: 5it [00:02,  1.80it/s]Extractor Predicting: 6it [00:03,  1.80it/s]Extractor Predicting: 7it [00:03,  1.81it/s]Extractor Predicting: 8it [00:04,  1.78it/s]Extractor Predicting: 9it [00:05,  1.79it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.80it/s]Extractor Predicting: 12it [00:06,  1.82it/s]Extractor Predicting: 13it [00:07,  1.80it/s]Extractor Predicting: 14it [00:07,  1.80it/s]Extractor Predicting: 15it [00:08,  1.82it/s]Extractor Predicting: 16it [00:08,  1.82it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:10,  1.74it/s]Extractor Predicting: 19it [00:10,  1.77it/s]Extractor Predicting: 20it [00:11,  1.74it/s]Extractor Predicting: 21it [00:11,  1.74it/s]Extractor Predicting: 22it [00:12,  1.74it/s]Extractor Predicting: 23it [00:12,  1.74it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.78it/s]Extractor Predicting: 26it [00:14,  1.77it/s]Extractor Predicting: 27it [00:15,  1.65it/s]Extractor Predicting: 28it [00:15,  1.70it/s]Extractor Predicting: 29it [00:16,  1.71it/s]Extractor Predicting: 30it [00:16,  1.81it/s]Extractor Predicting: 31it [00:17,  1.88it/s]Extractor Predicting: 32it [00:17,  1.90it/s]Extractor Predicting: 33it [00:18,  1.90it/s]Extractor Predicting: 34it [00:18,  1.91it/s]Extractor Predicting: 35it [00:19,  1.91it/s]Extractor Predicting: 36it [00:20,  1.89it/s]Extractor Predicting: 37it [00:20,  1.91it/s]Extractor Predicting: 38it [00:21,  1.90it/s]Extractor Predicting: 39it [00:21,  1.83it/s]Extractor Predicting: 40it [00:22,  1.85it/s]Extractor Predicting: 41it [00:22,  1.86it/s]Extractor Predicting: 42it [00:23,  1.87it/s]Extractor Predicting: 43it [00:23,  1.87it/s]Extractor Predicting: 44it [00:24,  1.90it/s]Extractor Predicting: 45it [00:24,  1.91it/s]Extractor Predicting: 46it [00:25,  1.95it/s]Extractor Predicting: 47it [00:25,  1.96it/s]Extractor Predicting: 48it [00:26,  1.96it/s]Extractor Predicting: 49it [00:26,  1.93it/s]Extractor Predicting: 50it [00:27,  1.91it/s]Extractor Predicting: 51it [00:27,  1.93it/s]Extractor Predicting: 52it [00:28,  1.92it/s]Extractor Predicting: 53it [00:28,  1.90it/s]Extractor Predicting: 54it [00:29,  1.86it/s]Extractor Predicting: 55it [00:30,  1.84it/s]Extractor Predicting: 56it [00:30,  1.80it/s]Extractor Predicting: 57it [00:31,  1.83it/s]Extractor Predicting: 58it [00:31,  1.92it/s]Extractor Predicting: 59it [00:32,  1.88it/s]Extractor Predicting: 60it [00:32,  1.86it/s]Extractor Predicting: 61it [00:33,  1.80it/s]Extractor Predicting: 62it [00:33,  1.78it/s]Extractor Predicting: 63it [00:34,  1.73it/s]Extractor Predicting: 64it [00:35,  1.70it/s]Extractor Predicting: 65it [00:35,  1.69it/s]Extractor Predicting: 66it [00:36,  1.68it/s]Extractor Predicting: 67it [00:36,  1.66it/s]Extractor Predicting: 68it [00:37,  1.67it/s]Extractor Predicting: 69it [00:38,  1.68it/s]Extractor Predicting: 70it [00:38,  1.71it/s]Extractor Predicting: 71it [00:39,  1.73it/s]Extractor Predicting: 72it [00:39,  1.75it/s]Extractor Predicting: 73it [00:40,  1.80it/s]Extractor Predicting: 74it [00:40,  1.80it/s]Extractor Predicting: 75it [00:41,  1.81it/s]Extractor Predicting: 76it [00:41,  1.83it/s]Extractor Predicting: 77it [00:42,  1.85it/s]Extractor Predicting: 78it [00:43,  1.84it/s]Extractor Predicting: 79it [00:43,  1.90it/s]Extractor Predicting: 80it [00:44,  1.95it/s]Extractor Predicting: 81it [00:44,  1.88it/s]Extractor Predicting: 82it [00:45,  1.90it/s]Extractor Predicting: 83it [00:45,  1.84it/s]Extractor Predicting: 84it [00:46,  1.83it/s]Extractor Predicting: 85it [00:46,  1.78it/s]Extractor Predicting: 86it [00:47,  1.75it/s]Extractor Predicting: 87it [00:48,  1.74it/s]Extractor Predicting: 88it [00:48,  1.78it/s]Extractor Predicting: 89it [00:49,  1.76it/s]Extractor Predicting: 90it [00:49,  1.78it/s]Extractor Predicting: 91it [00:50,  1.76it/s]Extractor Predicting: 92it [00:50,  1.76it/s]Extractor Predicting: 93it [00:51,  1.78it/s]Extractor Predicting: 94it [00:51,  1.81it/s]Extractor Predicting: 95it [00:52,  1.79it/s]Extractor Predicting: 96it [00:53,  1.79it/s]Extractor Predicting: 97it [00:53,  1.80it/s]Extractor Predicting: 98it [00:54,  1.79it/s]Extractor Predicting: 99it [00:54,  1.78it/s]Extractor Predicting: 100it [00:55,  1.76it/s]Extractor Predicting: 101it [00:55,  1.78it/s]Extractor Predicting: 102it [00:56,  1.80it/s]Extractor Predicting: 103it [00:57,  1.75it/s]Extractor Predicting: 104it [00:57,  1.79it/s]Extractor Predicting: 105it [00:58,  1.81it/s]Extractor Predicting: 106it [00:58,  1.83it/s]Extractor Predicting: 107it [00:59,  1.81it/s]Extractor Predicting: 108it [00:59,  1.83it/s]Extractor Predicting: 109it [01:00,  1.81it/s]Extractor Predicting: 110it [01:00,  1.83it/s]Extractor Predicting: 111it [01:01,  1.82it/s]Extractor Predicting: 112it [01:01,  1.80it/s]Extractor Predicting: 113it [01:02,  1.78it/s]Extractor Predicting: 114it [01:03,  1.78it/s]Extractor Predicting: 115it [01:03,  1.78it/s]Extractor Predicting: 116it [01:04,  1.78it/s]Extractor Predicting: 117it [01:04,  1.82it/s]Extractor Predicting: 118it [01:05,  1.80it/s]Extractor Predicting: 119it [01:05,  1.79it/s]Extractor Predicting: 120it [01:06,  1.82it/s]Extractor Predicting: 121it [01:06,  1.86it/s]Extractor Predicting: 122it [01:07,  1.84it/s]Extractor Predicting: 123it [01:08,  1.59it/s]Extractor Predicting: 124it [01:08,  1.63it/s]Extractor Predicting: 125it [01:09,  1.67it/s]Extractor Predicting: 126it [01:10,  1.68it/s]Extractor Predicting: 127it [01:10,  1.74it/s]Extractor Predicting: 128it [01:11,  1.70it/s]Extractor Predicting: 129it [01:11,  1.72it/s]Extractor Predicting: 130it [01:12,  1.77it/s]Extractor Predicting: 131it [01:12,  1.75it/s]Extractor Predicting: 132it [01:13,  1.75it/s]Extractor Predicting: 133it [01:13,  1.76it/s]Extractor Predicting: 134it [01:14,  1.76it/s]Extractor Predicting: 135it [01:15,  1.77it/s]Extractor Predicting: 136it [01:15,  1.81it/s]Extractor Predicting: 137it [01:16,  1.74it/s]Extractor Predicting: 138it [01:16,  1.77it/s]Extractor Predicting: 139it [01:17,  1.80it/s]Extractor Predicting: 140it [01:17,  1.79it/s]Extractor Predicting: 141it [01:18,  1.78it/s]Extractor Predicting: 142it [01:18,  1.81it/s]Extractor Predicting: 143it [01:19,  1.69it/s]Extractor Predicting: 144it [01:20,  1.77it/s]Extractor Predicting: 145it [01:20,  1.74it/s]Extractor Predicting: 146it [01:21,  1.78it/s]Extractor Predicting: 147it [01:21,  1.84it/s]Extractor Predicting: 148it [01:22,  1.81it/s]Extractor Predicting: 149it [01:22,  1.86it/s]Extractor Predicting: 150it [01:23,  1.88it/s]Extractor Predicting: 151it [01:23,  1.90it/s]Extractor Predicting: 152it [01:24,  1.85it/s]Extractor Predicting: 153it [01:25,  1.86it/s]Extractor Predicting: 154it [01:25,  1.79it/s]Extractor Predicting: 155it [01:26,  1.79it/s]Extractor Predicting: 156it [01:26,  1.86it/s]Extractor Predicting: 157it [01:27,  1.81it/s]Extractor Predicting: 158it [01:27,  1.79it/s]Extractor Predicting: 159it [01:28,  1.82it/s]Extractor Predicting: 160it [01:28,  1.81it/s]Extractor Predicting: 161it [01:29,  1.86it/s]Extractor Predicting: 162it [01:29,  1.83it/s]Extractor Predicting: 163it [01:30,  1.83it/s]Extractor Predicting: 164it [01:31,  1.80it/s]Extractor Predicting: 165it [01:31,  1.75it/s]Extractor Predicting: 166it [01:32,  1.71it/s]Extractor Predicting: 167it [01:32,  1.72it/s]Extractor Predicting: 168it [01:33,  1.74it/s]Extractor Predicting: 169it [01:34,  1.78it/s]Extractor Predicting: 170it [01:34,  1.80it/s]Extractor Predicting: 171it [01:35,  1.81it/s]Extractor Predicting: 172it [01:35,  1.81it/s]Extractor Predicting: 173it [01:36,  1.81it/s]Extractor Predicting: 174it [01:36,  1.80it/s]Extractor Predicting: 175it [01:37,  1.80it/s]Extractor Predicting: 176it [01:37,  1.82it/s]Extractor Predicting: 177it [01:38,  1.79it/s]Extractor Predicting: 178it [01:38,  1.79it/s]Extractor Predicting: 179it [01:39,  1.75it/s]Extractor Predicting: 180it [01:40,  1.80it/s]Extractor Predicting: 181it [01:40,  1.80it/s]Extractor Predicting: 182it [01:41,  1.83it/s]Extractor Predicting: 183it [01:41,  1.86it/s]Extractor Predicting: 184it [01:42,  1.84it/s]Extractor Predicting: 185it [01:42,  1.84it/s]Extractor Predicting: 186it [01:43,  1.78it/s]Extractor Predicting: 187it [01:43,  1.80it/s]Extractor Predicting: 188it [01:44,  1.81it/s]Extractor Predicting: 189it [01:45,  1.81it/s]Extractor Predicting: 190it [01:45,  1.83it/s]Extractor Predicting: 191it [01:46,  1.83it/s]Extractor Predicting: 192it [01:46,  1.87it/s]Extractor Predicting: 193it [01:47,  1.85it/s]Extractor Predicting: 194it [01:47,  1.85it/s]Extractor Predicting: 195it [01:48,  1.83it/s]Extractor Predicting: 196it [01:48,  1.79it/s]Extractor Predicting: 197it [01:49,  1.76it/s]Extractor Predicting: 198it [01:50,  1.75it/s]Extractor Predicting: 199it [01:50,  1.76it/s]Extractor Predicting: 200it [01:51,  1.77it/s]Extractor Predicting: 201it [01:51,  1.80it/s]Extractor Predicting: 202it [01:52,  1.81it/s]Extractor Predicting: 203it [01:52,  1.82it/s]Extractor Predicting: 204it [01:53,  1.81it/s]Extractor Predicting: 205it [01:53,  1.83it/s]Extractor Predicting: 206it [01:54,  1.82it/s]Extractor Predicting: 207it [01:54,  1.83it/s]Extractor Predicting: 208it [01:55,  1.81it/s]Extractor Predicting: 209it [01:56,  1.77it/s]Extractor Predicting: 210it [01:56,  1.86it/s]Extractor Predicting: 211it [01:57,  1.83it/s]Extractor Predicting: 212it [01:57,  1.85it/s]Extractor Predicting: 213it [01:58,  1.85it/s]Extractor Predicting: 214it [01:58,  1.88it/s]Extractor Predicting: 215it [01:59,  1.86it/s]Extractor Predicting: 216it [01:59,  1.81it/s]Extractor Predicting: 217it [02:00,  1.82it/s]Extractor Predicting: 218it [02:01,  1.80it/s]Extractor Predicting: 219it [02:01,  1.81it/s]Extractor Predicting: 220it [02:02,  1.81it/s]Extractor Predicting: 221it [02:02,  1.82it/s]Extractor Predicting: 222it [02:03,  1.76it/s]Extractor Predicting: 223it [02:03,  1.70it/s]Extractor Predicting: 224it [02:04,  1.73it/s]Extractor Predicting: 225it [02:04,  1.76it/s]Extractor Predicting: 226it [02:05,  1.80it/s]Extractor Predicting: 227it [02:06,  1.81it/s]Extractor Predicting: 228it [02:06,  1.84it/s]Extractor Predicting: 229it [02:07,  1.87it/s]Extractor Predicting: 230it [02:07,  1.89it/s]Extractor Predicting: 231it [02:08,  1.89it/s]Extractor Predicting: 232it [02:08,  1.88it/s]Extractor Predicting: 233it [02:09,  1.86it/s]Extractor Predicting: 234it [02:09,  1.86it/s]Extractor Predicting: 235it [02:10,  1.82it/s]Extractor Predicting: 236it [02:10,  1.83it/s]Extractor Predicting: 237it [02:11,  1.84it/s]Extractor Predicting: 238it [02:12,  1.80it/s]Extractor Predicting: 239it [02:12,  1.82it/s]Extractor Predicting: 240it [02:13,  1.81it/s]Extractor Predicting: 241it [02:13,  1.83it/s]Extractor Predicting: 242it [02:14,  1.80it/s]Extractor Predicting: 243it [02:14,  1.75it/s]Extractor Predicting: 244it [02:15,  1.77it/s]Extractor Predicting: 245it [02:15,  1.81it/s]Extractor Predicting: 246it [02:16,  1.80it/s]Extractor Predicting: 247it [02:16,  1.84it/s]Extractor Predicting: 248it [02:17,  1.77it/s]Extractor Predicting: 249it [02:18,  1.76it/s]Extractor Predicting: 250it [02:18,  1.58it/s]Extractor Predicting: 251it [02:19,  1.64it/s]Extractor Predicting: 252it [02:20,  1.66it/s]Extractor Predicting: 253it [02:20,  1.65it/s]Extractor Predicting: 254it [02:21,  1.65it/s]Extractor Predicting: 255it [02:21,  1.69it/s]Extractor Predicting: 256it [02:22,  1.71it/s]Extractor Predicting: 257it [02:22,  1.74it/s]Extractor Predicting: 258it [02:23,  1.75it/s]Extractor Predicting: 259it [02:24,  1.75it/s]Extractor Predicting: 260it [02:24,  1.72it/s]Extractor Predicting: 261it [02:25,  1.75it/s]Extractor Predicting: 262it [02:25,  1.74it/s]Extractor Predicting: 263it [02:26,  1.75it/s]Extractor Predicting: 264it [02:26,  1.75it/s]Extractor Predicting: 265it [02:27,  1.77it/s]Extractor Predicting: 266it [02:28,  1.72it/s]Extractor Predicting: 267it [02:28,  1.72it/s]Extractor Predicting: 268it [02:29,  1.71it/s]Extractor Predicting: 269it [02:29,  1.73it/s]Extractor Predicting: 270it [02:30,  1.71it/s]Extractor Predicting: 271it [02:31,  1.70it/s]Extractor Predicting: 272it [02:31,  1.71it/s]Extractor Predicting: 273it [02:32,  1.72it/s]Extractor Predicting: 274it [02:32,  1.69it/s]Extractor Predicting: 275it [02:33,  1.70it/s]Extractor Predicting: 276it [02:34,  1.72it/s]Extractor Predicting: 277it [02:34,  1.72it/s]Extractor Predicting: 278it [02:35,  1.75it/s]Extractor Predicting: 279it [02:35,  1.73it/s]Extractor Predicting: 280it [02:36,  1.74it/s]Extractor Predicting: 281it [02:36,  1.68it/s]Extractor Predicting: 282it [02:37,  1.66it/s]Extractor Predicting: 283it [02:38,  1.66it/s]Extractor Predicting: 284it [02:38,  1.70it/s]Extractor Predicting: 285it [02:39,  1.71it/s]Extractor Predicting: 286it [02:39,  1.74it/s]Extractor Predicting: 287it [02:40,  1.69it/s]Extractor Predicting: 288it [02:41,  1.73it/s]Extractor Predicting: 289it [02:41,  1.74it/s]Extractor Predicting: 290it [02:42,  1.70it/s]Extractor Predicting: 291it [02:42,  1.67it/s]Extractor Predicting: 292it [02:43,  1.71it/s]Extractor Predicting: 293it [02:43,  1.71it/s]Extractor Predicting: 294it [02:44,  1.69it/s]Extractor Predicting: 295it [02:45,  1.70it/s]Extractor Predicting: 296it [02:45,  1.74it/s]Extractor Predicting: 297it [02:46,  1.75it/s]Extractor Predicting: 298it [02:46,  1.72it/s]Extractor Predicting: 299it [02:47,  1.72it/s]Extractor Predicting: 300it [02:48,  1.70it/s]Extractor Predicting: 301it [02:48,  1.72it/s]Extractor Predicting: 302it [02:49,  1.68it/s]Extractor Predicting: 303it [02:49,  1.73it/s]Extractor Predicting: 304it [02:50,  1.75it/s]Extractor Predicting: 305it [02:50,  1.78it/s]Extractor Predicting: 306it [02:51,  1.82it/s]Extractor Predicting: 307it [02:52,  1.78it/s]Extractor Predicting: 308it [02:52,  1.80it/s]Extractor Predicting: 309it [02:53,  1.77it/s]Extractor Predicting: 310it [02:53,  1.77it/s]Extractor Predicting: 311it [02:54,  1.72it/s]Extractor Predicting: 312it [02:54,  1.74it/s]Extractor Predicting: 313it [02:55,  1.67it/s]Extractor Predicting: 314it [02:56,  1.67it/s]Extractor Predicting: 315it [02:56,  1.71it/s]Extractor Predicting: 316it [02:57,  1.76it/s]Extractor Predicting: 317it [02:57,  1.77it/s]Extractor Predicting: 318it [02:58,  1.81it/s]Extractor Predicting: 319it [02:58,  1.77it/s]Extractor Predicting: 320it [02:59,  1.74it/s]Extractor Predicting: 321it [03:00,  1.74it/s]Extractor Predicting: 322it [03:00,  1.76it/s]Extractor Predicting: 323it [03:01,  1.70it/s]Extractor Predicting: 324it [03:01,  1.71it/s]Extractor Predicting: 325it [03:02,  1.75it/s]Extractor Predicting: 326it [03:02,  1.75it/s]Extractor Predicting: 327it [03:03,  1.77it/s]Extractor Predicting: 328it [03:04,  1.73it/s]Extractor Predicting: 329it [03:04,  1.74it/s]Extractor Predicting: 330it [03:05,  1.72it/s]Extractor Predicting: 331it [03:05,  1.74it/s]Extractor Predicting: 332it [03:06,  1.73it/s]Extractor Predicting: 333it [03:06,  1.71it/s]Extractor Predicting: 334it [03:07,  1.74it/s]Extractor Predicting: 335it [03:08,  1.72it/s]Extractor Predicting: 336it [03:08,  1.62it/s]Extractor Predicting: 337it [03:09,  1.64it/s]Extractor Predicting: 338it [03:10,  1.64it/s]Extractor Predicting: 339it [03:10,  1.67it/s]Extractor Predicting: 340it [03:11,  1.67it/s]Extractor Predicting: 341it [03:11,  1.66it/s]Extractor Predicting: 342it [03:12,  1.67it/s]Extractor Predicting: 343it [03:12,  1.71it/s]Extractor Predicting: 344it [03:13,  1.71it/s]Extractor Predicting: 345it [03:14,  1.66it/s]Extractor Predicting: 346it [03:14,  1.67it/s]Extractor Predicting: 347it [03:15,  1.69it/s]Extractor Predicting: 348it [03:15,  1.72it/s]Extractor Predicting: 349it [03:16,  1.73it/s]Extractor Predicting: 350it [03:17,  1.72it/s]Extractor Predicting: 351it [03:17,  1.75it/s]Extractor Predicting: 352it [03:18,  1.50it/s]Extractor Predicting: 353it [03:19,  1.53it/s]Extractor Predicting: 354it [03:19,  1.60it/s]Extractor Predicting: 355it [03:20,  1.66it/s]Extractor Predicting: 356it [03:20,  1.67it/s]Extractor Predicting: 357it [03:21,  1.69it/s]Extractor Predicting: 358it [03:21,  1.71it/s]Extractor Predicting: 359it [03:22,  1.71it/s]Extractor Predicting: 360it [03:23,  1.68it/s]Extractor Predicting: 361it [03:23,  1.68it/s]Extractor Predicting: 362it [03:24,  1.70it/s]Extractor Predicting: 363it [03:24,  1.74it/s]Extractor Predicting: 364it [03:25,  1.76it/s]Extractor Predicting: 365it [03:26,  1.71it/s]Extractor Predicting: 366it [03:26,  1.66it/s]Extractor Predicting: 367it [03:27,  1.67it/s]Extractor Predicting: 368it [03:27,  1.65it/s]Extractor Predicting: 369it [03:28,  1.60it/s]Extractor Predicting: 370it [03:29,  1.61it/s]Extractor Predicting: 371it [03:29,  1.65it/s]Extractor Predicting: 372it [03:30,  1.99it/s]Extractor Predicting: 372it [03:30,  1.77it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:18,766 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:18,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:18,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:18,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:18,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:23:19,081 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:23:19,082 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:23:19,375 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:23:20,412 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:23:20,412 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:22,495 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:22,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:22,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:22,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:23:22,500 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:23:23,222 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:23:23,223 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:23:23,900 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:23:24,070 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:23:24,070 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.76it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 7it [00:03,  1.80it/s]Extractor Predicting: 8it [00:04,  1.80it/s]Extractor Predicting: 9it [00:05,  1.83it/s]Extractor Predicting: 10it [00:05,  1.83it/s]Extractor Predicting: 11it [00:06,  1.82it/s]Extractor Predicting: 12it [00:06,  1.87it/s]Extractor Predicting: 13it [00:07,  1.87it/s]Extractor Predicting: 14it [00:07,  1.85it/s]Extractor Predicting: 15it [00:08,  1.77it/s]Extractor Predicting: 16it [00:08,  1.79it/s]Extractor Predicting: 17it [00:09,  1.78it/s]Extractor Predicting: 18it [00:10,  1.81it/s]Extractor Predicting: 19it [00:10,  1.82it/s]Extractor Predicting: 20it [00:11,  1.82it/s]Extractor Predicting: 21it [00:11,  1.64it/s]Extractor Predicting: 22it [00:12,  1.66it/s]Extractor Predicting: 23it [00:13,  1.68it/s]Extractor Predicting: 24it [00:13,  1.66it/s]Extractor Predicting: 25it [00:14,  1.71it/s]Extractor Predicting: 26it [00:14,  1.69it/s]Extractor Predicting: 27it [00:15,  1.70it/s]Extractor Predicting: 28it [00:16,  1.67it/s]Extractor Predicting: 29it [00:16,  1.72it/s]Extractor Predicting: 30it [00:17,  1.73it/s]Extractor Predicting: 31it [00:17,  1.76it/s]Extractor Predicting: 32it [00:18,  1.74it/s]Extractor Predicting: 33it [00:18,  1.77it/s]Extractor Predicting: 34it [00:19,  1.77it/s]Extractor Predicting: 35it [00:19,  1.73it/s]Extractor Predicting: 36it [00:20,  1.75it/s]Extractor Predicting: 37it [00:21,  1.72it/s]Extractor Predicting: 38it [00:21,  1.66it/s]Extractor Predicting: 39it [00:22,  1.62it/s]Extractor Predicting: 40it [00:23,  1.62it/s]Extractor Predicting: 41it [00:23,  1.62it/s]Extractor Predicting: 42it [00:24,  1.65it/s]Extractor Predicting: 43it [00:24,  1.65it/s]Extractor Predicting: 44it [00:25,  1.67it/s]Extractor Predicting: 45it [00:26,  1.64it/s]Extractor Predicting: 46it [00:26,  1.67it/s]Extractor Predicting: 47it [00:27,  1.65it/s]Extractor Predicting: 48it [00:27,  1.65it/s]Extractor Predicting: 49it [00:28,  1.66it/s]Extractor Predicting: 50it [00:29,  1.64it/s]Extractor Predicting: 51it [00:29,  1.61it/s]Extractor Predicting: 52it [00:30,  1.61it/s]Extractor Predicting: 53it [00:30,  1.63it/s]Extractor Predicting: 54it [00:31,  1.61it/s]Extractor Predicting: 55it [00:32,  1.65it/s]Extractor Predicting: 56it [00:32,  1.66it/s]Extractor Predicting: 57it [00:33,  1.66it/s]Extractor Predicting: 58it [00:33,  1.63it/s]Extractor Predicting: 59it [00:34,  1.63it/s]Extractor Predicting: 60it [00:35,  1.64it/s]Extractor Predicting: 61it [00:35,  1.59it/s]Extractor Predicting: 62it [00:36,  1.59it/s]Extractor Predicting: 63it [00:36,  1.74it/s]Extractor Predicting: 63it [00:36,  1.70it/s]
[INFO|configuration_utils.py:515] 2023-08-28 04:24:02,118 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:24:02,119 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:24:02,124 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:24:02,125 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 04:24:02,129 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:24:05,380 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 04:24:05,385 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 04:24:05,393 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:24:05,394 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:24:05,403 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:24:05,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:24:05,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:24:05,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:24:05,406 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:24:05,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:24:05,406 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 04:24:05,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:06,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:06,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:07,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:08,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:08,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:09,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:09,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:10,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:10,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:11,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:12,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:12,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:13,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:13,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:14,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:15,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:15,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:16,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:17,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:17,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:18,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:19,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:19,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:20,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:20,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:21,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:52, 16.58s/it][WARNING|generation_utils.py:914] 2023-08-28 04:24:22,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:22,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:23,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:23,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:24,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:25,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:25,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:26,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:26,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:27,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:27,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:28,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:29,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:29,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:30,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:30,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:31,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:31,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:32,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:33,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:33,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:34,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:35,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:35,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:36,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:20, 15.46s/it][WARNING|generation_utils.py:914] 2023-08-28 04:24:36,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:37,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:37,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:38,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:39,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:39,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:40,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:40,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:41,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:41,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:42,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:42,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:43,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:43,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:44,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:44,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:45,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:46,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:46,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:47,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:47,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:43, 13.59s/it][WARNING|generation_utils.py:914] 2023-08-28 04:24:48,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:48,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:49,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:50,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:50,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:51,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:52,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:52,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:53,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:54,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:54,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:55,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:55,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:56,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:57,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:57,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:58,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:58,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:24:59,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:00,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:00,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:01,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:01,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:02,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:57<02:35, 14.10s/it][WARNING|generation_utils.py:914] 2023-08-28 04:25:03,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:03,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:04,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:04,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:05,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:06,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:06,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:07,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:07,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:08,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:09,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:09,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:10,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:10,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:11,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:12,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:12,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:13,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:13,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:14,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:15,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:15,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:16,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:17,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:17,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:24, 14.43s/it][WARNING|generation_utils.py:914] 2023-08-28 04:25:18,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:18,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:19,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:19,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:20,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:21,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:21,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:22,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:23,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:23,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:24,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:25,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:25,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:26,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:27,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:27,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:28,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:28,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:29,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:30,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:31,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:31,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:26<02:09, 14.35s/it][WARNING|generation_utils.py:914] 2023-08-28 04:25:32,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:32,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:33,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:34,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:34,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:35,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:35,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:36,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:37,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:37,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:38,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:38,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:39,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:40,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:40,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:41,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:41,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:42,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:43,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:43,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:44,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:44,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:39<01:51, 13.99s/it][WARNING|generation_utils.py:914] 2023-08-28 04:25:45,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:46,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:46,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:47,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:47,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:48,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:48,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:49,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:50,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:50,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:51,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:51,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:52,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:53,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:53,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:54,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:54,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:55,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:56,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:56,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:57,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:57,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:52<01:35, 13.62s/it][WARNING|generation_utils.py:914] 2023-08-28 04:25:58,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:59,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:25:59,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:00,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:00,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:01,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:02,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:02,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:03,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:03,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:04,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:05,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:05,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:06,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:06,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:07,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:07,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:08,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:09,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:09,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:10,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:10,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:11,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:11,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:12,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:13,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:08<01:25, 14.17s/it][WARNING|generation_utils.py:914] 2023-08-28 04:26:13,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:14,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:14,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:15,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:16,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:16,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:17,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:17,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:18,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:19,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:19,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:20,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:20,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:21,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:22,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:22,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:23,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:24,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:24,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:25,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:25,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:26,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:27,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:22<01:10, 14.08s/it][WARNING|generation_utils.py:914] 2023-08-28 04:26:27,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:28,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:28,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:29,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:29,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:30,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:30,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:31,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:31,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:32,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:33,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:33,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:33,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:34,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:35,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:35,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:36,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:36,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:37,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:37,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:38,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:38,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:39,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:34<00:53, 13.43s/it][WARNING|generation_utils.py:914] 2023-08-28 04:26:39,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:40,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:40,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:41,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:41,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:42,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:43,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:43,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:44,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:44,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:45,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:46,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:46,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:47,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:48,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:48,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:49,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:50,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:50,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:51,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:51,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:52,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:53,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:53,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:54,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:54,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:55,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:56,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:50<00:43, 14.52s/it][WARNING|generation_utils.py:914] 2023-08-28 04:26:56,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:57,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:57,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:58,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:59,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:26:59,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:00,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:00,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:01,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:02,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:02,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:03,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:03,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:04,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:05,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:05,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:06,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:06,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:07,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:07,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:08,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:09,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:09,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:10,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:05<00:28, 14.48s/it][WARNING|generation_utils.py:914] 2023-08-28 04:27:11,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:11,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:12,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:12,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:13,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:13,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:14,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:14,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:15,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:15,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:16,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:16,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:17,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:18,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:18,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:19,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:19,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:20,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:20,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:21,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:22,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:22,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:23,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:17<00:13, 13.91s/it][WARNING|generation_utils.py:914] 2023-08-28 04:27:23,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:24,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:24,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:25,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:26,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:26,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:27,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:28,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:28,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:29,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:30,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:30,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:31,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:32,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:32,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:33,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:34,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:34,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:35,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:36,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:37,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:37,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:27:38,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:33<00:00, 14.28s/it]Generating: 100%|██████████| 15/15 [03:33<00:00, 14.21s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:45,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:45,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:45,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:45,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:45,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:27:45,870 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:27:45,871 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:27:46,437 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:27:47,512 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:27:47,512 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:50,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:50,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:50,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:50,331 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:50,331 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:27:50,950 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:27:50,951 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:27:51,537 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:27:51,707 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:27:51,707 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 469, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : developer .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 626, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.7825, 'errors': {'', "('London', 'work location', '', 'The New York Times ( July 18 , 1923 October 6 , 1977 ) reported that the first and only English book on the topic appeared in London during the reigns of William Shakespeare and William Shakespeare .')"}}
['Relation : composer . Context : Later in the year ( 1143 ) , he composed The Seven Kingdoms , the first play in the epic of the Old Kingdom . Head Entity : The Seven Kingdoms , Tail Entity : Robert I .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best Rap Album at the 2004 MTV Video Music Awards and had two acts on the chart . Head Entity : music , Tail Entity : Rap Album .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 590, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7319711538461539, 'errors': {'', "('James S. Cain', 'field of work', '', 'He is also known for his work with James S. Cain , and for his work on the painting of the Seven Wonders of Oz by William S. Burroughs .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 194, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 298, 'raw': 448}
{'target': 600, 'success': 322, 'raw': 480}
{'target': 600, 'success': 346, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 387, 'raw': 576}
{'target': 600, 'success': 408, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 518, 'raw': 768}
{'target': 600, 'success': 543, 'raw': 800}
{'target': 600, 'success': 563, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 606, 'raw': 896}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6763392857142857, 'errors': {'', "('John Lennon', 'occupation', '', 'The band released their debut album In Search of a Way ( 2000 ) , which featured a cover from John Lennon and Steve Dizzy .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 13757
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13857, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.42it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:01,  1.54it/s]Extractor Estimating: 4it [00:02,  1.66it/s]Extractor Estimating: 5it [00:03,  1.68it/s]Extractor Estimating: 6it [00:03,  1.71it/s]Extractor Estimating: 7it [00:04,  1.69it/s]Extractor Estimating: 8it [00:04,  1.68it/s]Extractor Estimating: 9it [00:05,  1.64it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.62it/s]Extractor Estimating: 12it [00:07,  1.64it/s]Extractor Estimating: 13it [00:07,  1.64it/s]Extractor Estimating: 14it [00:08,  1.66it/s]Extractor Estimating: 15it [00:09,  1.68it/s]Extractor Estimating: 16it [00:09,  1.69it/s]Extractor Estimating: 17it [00:10,  1.66it/s]Extractor Estimating: 18it [00:10,  1.66it/s]Extractor Estimating: 19it [00:11,  1.65it/s]Extractor Estimating: 20it [00:12,  1.65it/s]Extractor Estimating: 21it [00:12,  1.71it/s]Extractor Estimating: 22it [00:13,  1.74it/s]Extractor Estimating: 23it [00:13,  1.73it/s]Extractor Estimating: 24it [00:14,  1.68it/s]Extractor Estimating: 25it [00:15,  1.72it/s]Extractor Estimating: 26it [00:15,  1.78it/s]Extractor Estimating: 27it [00:16,  1.73it/s]Extractor Estimating: 28it [00:16,  1.78it/s]Extractor Estimating: 29it [00:17,  1.76it/s]Extractor Estimating: 30it [00:17,  1.65it/s]Extractor Estimating: 31it [00:18,  1.64it/s]Extractor Estimating: 32it [00:19,  1.73it/s]Extractor Estimating: 33it [00:19,  1.71it/s]Extractor Estimating: 34it [00:20,  1.72it/s]Extractor Estimating: 35it [00:20,  1.68it/s]Extractor Estimating: 36it [00:21,  1.68it/s]Extractor Estimating: 37it [00:22,  1.69it/s]Extractor Estimating: 38it [00:22,  1.66it/s]Extractor Estimating: 39it [00:23,  1.70it/s]Extractor Estimating: 40it [00:23,  1.73it/s]Extractor Estimating: 41it [00:24,  1.68it/s]Extractor Estimating: 42it [00:24,  1.72it/s]Extractor Estimating: 43it [00:25,  1.75it/s]Extractor Estimating: 44it [00:26,  1.75it/s]Extractor Estimating: 45it [00:26,  1.77it/s]Extractor Estimating: 46it [00:27,  1.76it/s]Extractor Estimating: 47it [00:27,  1.76it/s]Extractor Estimating: 48it [00:28,  1.63it/s]Extractor Estimating: 49it [00:29,  1.63it/s]Extractor Estimating: 50it [00:29,  1.72it/s]Extractor Estimating: 51it [00:30,  1.76it/s]Extractor Estimating: 52it [00:30,  1.81it/s]Extractor Estimating: 53it [00:31,  1.85it/s]Extractor Estimating: 54it [00:31,  1.89it/s]Extractor Estimating: 55it [00:32,  1.91it/s]Extractor Estimating: 56it [00:32,  2.00it/s]Extractor Estimating: 57it [00:33,  1.98it/s]Extractor Estimating: 58it [00:33,  2.00it/s]Extractor Estimating: 59it [00:34,  2.06it/s]Extractor Estimating: 60it [00:34,  1.98it/s]Extractor Estimating: 61it [00:35,  1.94it/s]Extractor Estimating: 62it [00:35,  1.99it/s]Extractor Estimating: 63it [00:36,  2.05it/s]Extractor Estimating: 64it [00:36,  1.99it/s]Extractor Estimating: 65it [00:37,  1.94it/s]Extractor Estimating: 66it [00:37,  2.02it/s]Extractor Estimating: 67it [00:38,  2.01it/s]Extractor Estimating: 68it [00:38,  1.97it/s]Extractor Estimating: 69it [00:39,  2.01it/s]Extractor Estimating: 70it [00:39,  1.99it/s]Extractor Estimating: 71it [00:40,  2.02it/s]Extractor Estimating: 72it [00:40,  2.01it/s]Extractor Estimating: 73it [00:41,  2.03it/s]Extractor Estimating: 74it [00:41,  2.03it/s]Extractor Estimating: 75it [00:42,  2.00it/s]Extractor Estimating: 76it [00:42,  1.92it/s]Extractor Estimating: 77it [00:43,  1.77it/s]Extractor Estimating: 78it [00:43,  1.75it/s]Extractor Estimating: 79it [00:44,  1.71it/s]Extractor Estimating: 80it [00:45,  1.60it/s]Extractor Estimating: 81it [00:45,  1.60it/s]Extractor Estimating: 82it [00:46,  1.56it/s]Extractor Estimating: 83it [00:47,  1.58it/s]Extractor Estimating: 84it [00:47,  1.55it/s]Extractor Estimating: 85it [00:48,  1.54it/s]Extractor Estimating: 86it [00:49,  1.56it/s]Extractor Estimating: 87it [00:49,  1.55it/s]Extractor Estimating: 88it [00:50,  1.58it/s]Extractor Estimating: 89it [00:50,  1.63it/s]Extractor Estimating: 90it [00:51,  1.65it/s]Extractor Estimating: 91it [00:52,  1.65it/s]Extractor Estimating: 92it [00:52,  1.67it/s]Extractor Estimating: 93it [00:53,  1.69it/s]Extractor Estimating: 94it [00:53,  1.69it/s]Extractor Estimating: 95it [00:54,  1.65it/s]Extractor Estimating: 96it [00:55,  1.67it/s]Extractor Estimating: 97it [00:55,  1.65it/s]Extractor Estimating: 98it [00:56,  1.65it/s]Extractor Estimating: 99it [00:56,  1.65it/s]Extractor Estimating: 100it [00:57,  1.63it/s]Extractor Estimating: 101it [00:58,  1.62it/s]Extractor Estimating: 102it [00:58,  1.63it/s]Extractor Estimating: 103it [00:59,  1.65it/s]Extractor Estimating: 104it [01:00,  1.69it/s]Extractor Estimating: 105it [01:00,  1.68it/s]Extractor Estimating: 106it [01:01,  1.67it/s]Extractor Estimating: 107it [01:01,  1.64it/s]Extractor Estimating: 108it [01:02,  1.66it/s]Extractor Estimating: 109it [01:03,  1.61it/s]Extractor Estimating: 110it [01:03,  1.61it/s]Extractor Estimating: 111it [01:04,  1.58it/s]Extractor Estimating: 112it [01:05,  1.51it/s]Extractor Estimating: 113it [01:05,  1.53it/s]Extractor Estimating: 114it [01:06,  1.54it/s]Extractor Estimating: 115it [01:07,  1.52it/s]Extractor Estimating: 116it [01:07,  1.53it/s]Extractor Estimating: 117it [01:08,  1.54it/s]Extractor Estimating: 118it [01:08,  1.60it/s]Extractor Estimating: 119it [01:09,  1.61it/s]Extractor Estimating: 120it [01:10,  1.57it/s]Extractor Estimating: 121it [01:10,  1.60it/s]Extractor Estimating: 122it [01:11,  1.58it/s]Extractor Estimating: 123it [01:12,  1.58it/s]Extractor Estimating: 124it [01:12,  1.61it/s]Extractor Estimating: 125it [01:13,  1.65it/s]Extractor Estimating: 126it [01:13,  1.69it/s]Extractor Estimating: 127it [01:14,  1.63it/s]Extractor Estimating: 128it [01:15,  1.66it/s]Extractor Estimating: 129it [01:15,  1.65it/s]Extractor Estimating: 130it [01:16,  1.66it/s]Extractor Estimating: 131it [01:16,  1.62it/s]Extractor Estimating: 132it [01:17,  1.58it/s]Extractor Estimating: 133it [01:18,  1.56it/s]Extractor Estimating: 134it [01:18,  1.57it/s]Extractor Estimating: 135it [01:19,  1.47it/s]Extractor Estimating: 136it [01:20,  1.49it/s]Extractor Estimating: 137it [01:21,  1.45it/s]Extractor Estimating: 138it [01:21,  1.48it/s]Extractor Estimating: 139it [01:22,  1.50it/s]Extractor Estimating: 140it [01:22,  1.53it/s]Extractor Estimating: 141it [01:23,  1.58it/s]Extractor Estimating: 142it [01:24,  1.61it/s]Extractor Estimating: 143it [01:24,  1.66it/s]Extractor Estimating: 144it [01:25,  1.65it/s]Extractor Estimating: 145it [01:25,  1.69it/s]Extractor Estimating: 146it [01:26,  1.63it/s]Extractor Estimating: 147it [01:27,  1.63it/s]Extractor Estimating: 148it [01:27,  1.61it/s]Extractor Estimating: 149it [01:28,  1.66it/s]Extractor Estimating: 150it [01:28,  1.64it/s]Extractor Estimating: 151it [01:29,  1.64it/s]Extractor Estimating: 152it [01:30,  1.68it/s]Extractor Estimating: 153it [01:30,  1.70it/s]Extractor Estimating: 154it [01:31,  1.73it/s]Extractor Estimating: 155it [01:31,  1.72it/s]Extractor Estimating: 156it [01:32,  1.68it/s]Extractor Estimating: 157it [01:33,  1.67it/s]Extractor Estimating: 158it [01:33,  1.67it/s]Extractor Estimating: 159it [01:34,  1.67it/s]Extractor Estimating: 160it [01:34,  1.71it/s]Extractor Estimating: 161it [01:35,  1.60it/s]Extractor Estimating: 162it [01:36,  1.61it/s]Extractor Estimating: 163it [01:36,  1.67it/s]Extractor Estimating: 164it [01:37,  1.69it/s]Extractor Estimating: 165it [01:37,  1.74it/s]Extractor Estimating: 166it [01:38,  1.75it/s]Extractor Estimating: 167it [01:38,  1.78it/s]Extractor Estimating: 168it [01:39,  1.80it/s]Extractor Estimating: 169it [01:39,  1.82it/s]Extractor Estimating: 170it [01:40,  1.71it/s]Extractor Estimating: 171it [01:41,  1.71it/s]Extractor Estimating: 172it [01:41,  1.73it/s]Extractor Estimating: 173it [01:42,  1.76it/s]Extractor Estimating: 174it [01:42,  1.70it/s]Extractor Estimating: 175it [01:43,  1.64it/s]Extractor Estimating: 176it [01:44,  1.63it/s]Extractor Estimating: 177it [01:44,  1.65it/s]Extractor Estimating: 178it [01:45,  1.66it/s]Extractor Estimating: 179it [01:46,  1.66it/s]Extractor Estimating: 180it [01:46,  1.63it/s]Extractor Estimating: 181it [01:47,  1.65it/s]Extractor Estimating: 182it [01:47,  1.65it/s]Extractor Estimating: 183it [01:48,  1.65it/s]Extractor Estimating: 184it [01:49,  1.67it/s]Extractor Estimating: 185it [01:49,  1.64it/s]Extractor Estimating: 186it [01:50,  1.63it/s]Extractor Estimating: 187it [01:50,  1.67it/s]Extractor Estimating: 188it [01:51,  1.66it/s]Extractor Estimating: 189it [01:52,  1.65it/s]Extractor Estimating: 190it [01:52,  1.60it/s]Extractor Estimating: 191it [01:53,  1.58it/s]Extractor Estimating: 192it [01:54,  1.60it/s]Extractor Estimating: 193it [01:54,  1.57it/s]Extractor Estimating: 194it [01:55,  1.61it/s]Extractor Estimating: 195it [01:55,  1.63it/s]Extractor Estimating: 196it [01:56,  1.59it/s]Extractor Estimating: 197it [01:57,  1.60it/s]Extractor Estimating: 198it [01:57,  1.59it/s]Extractor Estimating: 199it [01:58,  1.63it/s]Extractor Estimating: 200it [01:59,  1.47it/s]Extractor Estimating: 201it [01:59,  1.50it/s]Extractor Estimating: 202it [02:00,  1.56it/s]Extractor Estimating: 203it [02:01,  1.55it/s]Extractor Estimating: 204it [02:01,  1.58it/s]Extractor Estimating: 205it [02:02,  1.54it/s]Extractor Estimating: 206it [02:02,  1.59it/s]Extractor Estimating: 207it [02:03,  1.62it/s]Extractor Estimating: 208it [02:04,  1.63it/s]Extractor Estimating: 209it [02:04,  1.65it/s]Extractor Estimating: 210it [02:05,  1.63it/s]Extractor Estimating: 211it [02:05,  1.66it/s]Extractor Estimating: 212it [02:06,  1.67it/s]Extractor Estimating: 213it [02:07,  1.68it/s]Extractor Estimating: 214it [02:07,  1.66it/s]Extractor Estimating: 215it [02:08,  1.64it/s]Extractor Estimating: 216it [02:09,  1.59it/s]Extractor Estimating: 217it [02:09,  1.61it/s]Extractor Estimating: 218it [02:10,  1.62it/s]Extractor Estimating: 219it [02:10,  1.68it/s]Extractor Estimating: 220it [02:11,  1.68it/s]Extractor Estimating: 221it [02:12,  1.67it/s]Extractor Estimating: 222it [02:12,  1.66it/s]Extractor Estimating: 223it [02:13,  1.69it/s]Extractor Estimating: 224it [02:13,  1.65it/s]Extractor Estimating: 225it [02:14,  1.64it/s]Extractor Estimating: 226it [02:15,  1.62it/s]Extractor Estimating: 227it [02:15,  1.57it/s]Extractor Estimating: 228it [02:16,  1.56it/s]Extractor Estimating: 229it [02:17,  1.57it/s]Extractor Estimating: 230it [02:17,  1.58it/s]Extractor Estimating: 231it [02:18,  1.60it/s]Extractor Estimating: 232it [02:18,  1.62it/s]Extractor Estimating: 233it [02:19,  1.65it/s]Extractor Estimating: 234it [02:20,  1.61it/s]Extractor Estimating: 235it [02:20,  1.56it/s]Extractor Estimating: 236it [02:21,  1.56it/s]Extractor Estimating: 237it [02:22,  1.57it/s]Extractor Estimating: 238it [02:22,  1.60it/s]Extractor Estimating: 239it [02:23,  1.63it/s]Extractor Estimating: 240it [02:23,  1.63it/s]Extractor Estimating: 241it [02:24,  1.60it/s]Extractor Estimating: 242it [02:25,  1.56it/s]Extractor Estimating: 243it [02:25,  1.56it/s]Extractor Estimating: 244it [02:26,  1.59it/s]Extractor Estimating: 245it [02:27,  1.57it/s]Extractor Estimating: 246it [02:27,  1.56it/s]Extractor Estimating: 247it [02:28,  1.55it/s]Extractor Estimating: 248it [02:28,  1.59it/s]Extractor Estimating: 249it [02:29,  1.61it/s]Extractor Estimating: 250it [02:30,  1.57it/s]Extractor Estimating: 251it [02:30,  1.59it/s]Extractor Estimating: 252it [02:31,  1.58it/s]Extractor Estimating: 253it [02:32,  1.59it/s]Extractor Estimating: 254it [02:32,  1.61it/s]Extractor Estimating: 255it [02:33,  1.64it/s]Extractor Estimating: 256it [02:33,  1.65it/s]Extractor Estimating: 257it [02:34,  1.69it/s]Extractor Estimating: 258it [02:35,  1.72it/s]Extractor Estimating: 259it [02:35,  1.69it/s]Extractor Estimating: 260it [02:36,  1.72it/s]Extractor Estimating: 261it [02:36,  1.67it/s]Extractor Estimating: 262it [02:37,  1.66it/s]Extractor Estimating: 263it [02:38,  1.68it/s]Extractor Estimating: 264it [02:38,  1.72it/s]Extractor Estimating: 265it [02:39,  1.71it/s]Extractor Estimating: 266it [02:39,  1.73it/s]Extractor Estimating: 267it [02:40,  1.72it/s]Extractor Estimating: 268it [02:40,  1.65it/s]Extractor Estimating: 269it [02:41,  1.67it/s]Extractor Estimating: 270it [02:42,  1.65it/s]Extractor Estimating: 271it [02:42,  1.62it/s]Extractor Estimating: 272it [02:43,  1.57it/s]Extractor Estimating: 273it [02:44,  1.62it/s]Extractor Estimating: 274it [02:44,  1.63it/s]Extractor Estimating: 275it [02:45,  1.64it/s]Extractor Estimating: 276it [02:45,  1.59it/s]Extractor Estimating: 277it [02:46,  1.62it/s]Extractor Estimating: 278it [02:47,  1.65it/s]Extractor Estimating: 279it [02:47,  1.63it/s]Extractor Estimating: 280it [02:48,  1.63it/s]Extractor Estimating: 281it [02:49,  1.63it/s]Extractor Estimating: 282it [02:49,  1.51it/s]Extractor Estimating: 283it [02:50,  1.50it/s]Extractor Estimating: 284it [02:51,  1.52it/s]Extractor Estimating: 285it [02:51,  1.54it/s]Extractor Estimating: 286it [02:52,  1.56it/s]Extractor Estimating: 287it [02:53,  1.54it/s]Extractor Estimating: 288it [02:53,  1.59it/s]Extractor Estimating: 289it [02:54,  1.58it/s]Extractor Estimating: 290it [02:54,  1.63it/s]Extractor Estimating: 291it [02:55,  1.64it/s]Extractor Estimating: 292it [02:56,  1.60it/s]Extractor Estimating: 293it [02:56,  1.58it/s]Extractor Estimating: 294it [02:57,  1.62it/s]Extractor Estimating: 295it [02:57,  1.66it/s]Extractor Estimating: 296it [02:58,  1.64it/s]Extractor Estimating: 297it [02:59,  1.68it/s]Extractor Estimating: 298it [02:59,  1.73it/s]Extractor Estimating: 299it [03:00,  1.66it/s]Extractor Estimating: 300it [03:00,  1.65it/s]Extractor Estimating: 301it [03:01,  1.62it/s]Extractor Estimating: 302it [03:02,  1.57it/s]Extractor Estimating: 303it [03:02,  1.57it/s]Extractor Estimating: 304it [03:03,  1.56it/s]Extractor Estimating: 305it [03:04,  1.56it/s]Extractor Estimating: 306it [03:04,  1.54it/s]Extractor Estimating: 307it [03:05,  1.60it/s]Extractor Estimating: 308it [03:05,  1.59it/s]Extractor Estimating: 309it [03:06,  1.59it/s]Extractor Estimating: 310it [03:07,  1.58it/s]Extractor Estimating: 311it [03:07,  1.57it/s]Extractor Estimating: 312it [03:08,  1.57it/s]Extractor Estimating: 313it [03:09,  1.58it/s]Extractor Estimating: 314it [03:09,  1.60it/s]Extractor Estimating: 315it [03:10,  1.39it/s]Extractor Estimating: 316it [03:11,  1.44it/s]Extractor Estimating: 317it [03:11,  1.50it/s]Extractor Estimating: 318it [03:12,  1.49it/s]Extractor Estimating: 319it [03:13,  1.52it/s]Extractor Estimating: 320it [03:13,  1.53it/s]Extractor Estimating: 321it [03:14,  1.51it/s]Extractor Estimating: 322it [03:15,  1.54it/s]Extractor Estimating: 323it [03:15,  1.56it/s]Extractor Estimating: 324it [03:16,  1.60it/s]Extractor Estimating: 325it [03:17,  1.61it/s]Extractor Estimating: 326it [03:17,  1.61it/s]Extractor Estimating: 327it [03:18,  1.64it/s]Extractor Estimating: 328it [03:18,  1.64it/s]Extractor Estimating: 329it [03:19,  1.67it/s]Extractor Estimating: 330it [03:20,  1.63it/s]Extractor Estimating: 331it [03:20,  1.63it/s]Extractor Estimating: 332it [03:21,  1.63it/s]Extractor Estimating: 333it [03:21,  1.65it/s]Extractor Estimating: 334it [03:22,  1.64it/s]Extractor Estimating: 335it [03:23,  1.62it/s]Extractor Estimating: 336it [03:23,  1.65it/s]Extractor Estimating: 337it [03:24,  1.56it/s]Extractor Estimating: 338it [03:25,  1.60it/s]Extractor Estimating: 339it [03:25,  1.56it/s]Extractor Estimating: 340it [03:26,  1.62it/s]Extractor Estimating: 341it [03:26,  1.62it/s]Extractor Estimating: 342it [03:27,  1.61it/s]Extractor Estimating: 343it [03:28,  1.64it/s]Extractor Estimating: 344it [03:28,  1.62it/s]Extractor Estimating: 345it [03:29,  1.59it/s]Extractor Estimating: 346it [03:30,  1.58it/s]Extractor Estimating: 347it [03:30,  1.62it/s]Extractor Estimating: 348it [03:31,  1.64it/s]Extractor Estimating: 349it [03:31,  1.66it/s]Extractor Estimating: 350it [03:32,  1.64it/s]Extractor Estimating: 351it [03:33,  1.66it/s]Extractor Estimating: 352it [03:33,  1.64it/s]Extractor Estimating: 353it [03:34,  1.67it/s]Extractor Estimating: 354it [03:34,  1.66it/s]Extractor Estimating: 355it [03:35,  1.59it/s]Extractor Estimating: 356it [03:36,  1.56it/s]Extractor Estimating: 357it [03:36,  1.58it/s]Extractor Estimating: 358it [03:37,  1.65it/s]Extractor Estimating: 359it [03:38,  1.49it/s]Extractor Estimating: 360it [03:38,  1.55it/s]Extractor Estimating: 361it [03:39,  1.60it/s]Extractor Estimating: 362it [03:39,  1.59it/s]Extractor Estimating: 363it [03:40,  1.57it/s]Extractor Estimating: 364it [03:41,  1.56it/s]Extractor Estimating: 365it [03:41,  1.58it/s]Extractor Estimating: 366it [03:42,  1.65it/s]Extractor Estimating: 367it [03:43,  1.63it/s]Extractor Estimating: 368it [03:43,  1.61it/s]Extractor Estimating: 369it [03:44,  1.63it/s]Extractor Estimating: 370it [03:44,  1.63it/s]Extractor Estimating: 371it [03:45,  1.59it/s]Extractor Estimating: 372it [03:46,  1.63it/s]Extractor Estimating: 373it [03:46,  1.64it/s]Extractor Estimating: 374it [03:47,  1.62it/s]Extractor Estimating: 375it [03:47,  1.85it/s]Extractor Estimating: 375it [03:47,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:31:56,867 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:31:56,872 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:31:56,872 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:31:56,872 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:31:56,872 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:31:57,484 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:31:57,484 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:31:58,066 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:31:59,138 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:31:59,138 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:32:01,984 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:32:01,993 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:32:01,993 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:32:01,993 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:32:01,993 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:32:02,656 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:32:02,657 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:32:03,234 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:32:03,411 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:32:03,412 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:49:51,238 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:49:51,265 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7868 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 24562
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24662, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24662, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.000, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.005, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.990, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 72, avg_time 1.006, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 172, avg_time 1.010, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 272, avg_time 2.313, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 44, avg_time 1.006, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 144, avg_time 0.994, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 244, avg_time 1.002, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 16, avg_time 1.006, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 116, avg_time 2.308, loss:nan
g_step 1200, step 216, avg_time 1.002, loss:nan
g_step 1300, step 316, avg_time 1.001, loss:nan
g_step 1400, step 88, avg_time 0.991, loss:nan
g_step 1500, step 188, avg_time 1.001, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 288, avg_time 2.320, loss:nan
g_step 1700, step 60, avg_time 1.003, loss:nan
g_step 1800, step 160, avg_time 1.001, loss:nan
g_step 1900, step 260, avg_time 1.005, loss:nan
g_step 2000, step 32, avg_time 0.999, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 132, avg_time 2.310, loss:nan
g_step 2200, step 232, avg_time 1.004, loss:nan
g_step 2300, step 4, avg_time 1.011, loss:nan
g_step 2400, step 104, avg_time 1.004, loss:nan
g_step 2500, step 204, avg_time 0.992, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 304, avg_time 2.312, loss:nan
g_step 2700, step 76, avg_time 1.025, loss:nan
g_step 2800, step 176, avg_time 0.992, loss:nan
g_step 2900, step 276, avg_time 1.000, loss:nan
g_step 3000, step 48, avg_time 1.005, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 148, avg_time 2.310, loss:nan
g_step 3200, step 248, avg_time 0.997, loss:nan
g_step 3300, step 20, avg_time 1.002, loss:nan
g_step 3400, step 120, avg_time 0.995, loss:nan
g_step 3500, step 220, avg_time 1.002, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 320, avg_time 2.305, loss:nan
g_step 3700, step 92, avg_time 0.991, loss:nan
g_step 3800, step 192, avg_time 1.001, loss:nan
g_step 3900, step 292, avg_time 1.000, loss:nan
g_step 4000, step 64, avg_time 1.005, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 164, avg_time 2.295, loss:nan
g_step 4200, step 264, avg_time 1.010, loss:nan
g_step 4300, step 36, avg_time 0.991, loss:nan
g_step 4400, step 136, avg_time 1.004, loss:nan
g_step 4500, step 236, avg_time 0.996, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 8, avg_time 2.309, loss:nan
g_step 4700, step 108, avg_time 1.004, loss:nan
g_step 4800, step 208, avg_time 1.003, loss:nan
g_step 4900, step 308, avg_time 0.987, loss:nan
g_step 5000, step 80, avg_time 0.991, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 180, avg_time 2.310, loss:nan
g_step 5200, step 280, avg_time 1.003, loss:nan
g_step 5300, step 52, avg_time 1.001, loss:nan
g_step 5400, step 152, avg_time 1.013, loss:nan
g_step 5500, step 252, avg_time 0.995, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 24, avg_time 2.307, loss:nan
g_step 5700, step 124, avg_time 0.998, loss:nan
g_step 5800, step 224, avg_time 0.996, loss:nan
g_step 5900, step 324, avg_time 1.006, loss:nan
g_step 6000, step 96, avg_time 0.990, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 196, avg_time 2.320, loss:nan
g_step 6200, step 296, avg_time 0.993, loss:nan
g_step 6300, step 68, avg_time 0.980, loss:nan
g_step 6400, step 168, avg_time 1.006, loss:nan
g_step 6500, step 268, avg_time 1.004, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:49:51 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:49:51 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-49-51_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:49:52 - WARNING - datasets.builder -   Using custom data configuration default-a5122d943ed0cc85
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a5122d943ed0cc85/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:49:52,511 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:49:52,512 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:49:52,512 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:49:52,513 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:49:52,523 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:52,527 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:52,527 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:52,527 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:52,527 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:52,527 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:49:52,527 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:49:52,662 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:49:55,716 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:49:55,718 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a5122d943ed0cc85/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.19ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.00ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.32ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.64ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.95ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.17ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.32ba/s]100%|██████████| 8/8 [00:01<00:00,  4.54ba/s]100%|██████████| 8/8 [00:01<00:00,  4.19ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.11ba/s] 40%|████      | 2/5 [00:00<00:00,  4.38ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.46ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.51ba/s]100%|██████████| 5/5 [00:01<00:00,  4.74ba/s]100%|██████████| 5/5 [00:01<00:00,  4.58ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.51ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.57ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.81ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.79ba/s]100%|██████████| 8/8 [00:00<00:00, 10.78ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.64ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.38ba/s]100%|██████████| 5/5 [00:00<00:00, 10.90ba/s]100%|██████████| 5/5 [00:00<00:00, 10.64ba/s]
[INFO|trainer.py:414] 2023-08-28 06:50:00,426 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:50:00,438 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:50:00,438 >>   Num examples = 7920
[INFO|trainer.py:1149] 2023-08-28 06:50:00,438 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:50:00,438 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:50:00,438 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:50:00,438 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:50:00,438 >>   Total optimization steps = 620
  0%|          | 0/620 [00:00<?, ?it/s]  0%|          | 1/620 [00:00<02:57,  3.49it/s]  0%|          | 2/620 [00:00<02:53,  3.57it/s]  0%|          | 3/620 [00:00<02:52,  3.58it/s]  1%|          | 4/620 [00:01<02:51,  3.60it/s]  1%|          | 5/620 [00:01<02:51,  3.59it/s]  1%|          | 6/620 [00:01<02:52,  3.56it/s]  1%|          | 7/620 [00:01<02:52,  3.56it/s]  1%|▏         | 8/620 [00:02<02:51,  3.56it/s]  1%|▏         | 9/620 [00:02<02:51,  3.56it/s]  2%|▏         | 10/620 [00:02<02:51,  3.56it/s]  2%|▏         | 11/620 [00:03<02:50,  3.56it/s]  2%|▏         | 12/620 [00:03<02:50,  3.57it/s]  2%|▏         | 13/620 [00:03<02:50,  3.57it/s]  2%|▏         | 14/620 [00:03<02:49,  3.57it/s]  2%|▏         | 15/620 [00:04<02:49,  3.57it/s]  3%|▎         | 16/620 [00:04<02:49,  3.57it/s]  3%|▎         | 17/620 [00:04<02:50,  3.54it/s]  3%|▎         | 18/620 [00:05<02:49,  3.55it/s]  3%|▎         | 19/620 [00:05<02:49,  3.55it/s]  3%|▎         | 20/620 [00:05<02:49,  3.55it/s]  3%|▎         | 21/620 [00:05<02:48,  3.55it/s]  4%|▎         | 22/620 [00:06<02:48,  3.55it/s]  4%|▎         | 23/620 [00:06<02:48,  3.55it/s]  4%|▍         | 24/620 [00:06<02:47,  3.56it/s]  4%|▍         | 25/620 [00:07<02:47,  3.56it/s]  4%|▍         | 26/620 [00:07<02:46,  3.56it/s]  4%|▍         | 27/620 [00:07<02:46,  3.56it/s]  5%|▍         | 28/620 [00:07<02:47,  3.54it/s]  5%|▍         | 29/620 [00:08<02:46,  3.54it/s]  5%|▍         | 30/620 [00:08<02:46,  3.55it/s]  5%|▌         | 31/620 [00:08<02:45,  3.55it/s]  5%|▌         | 32/620 [00:08<02:45,  3.55it/s]  5%|▌         | 33/620 [00:09<02:45,  3.55it/s]  5%|▌         | 34/620 [00:09<02:44,  3.55it/s]  6%|▌         | 35/620 [00:09<02:44,  3.56it/s]  6%|▌         | 36/620 [00:10<02:44,  3.56it/s]  6%|▌         | 37/620 [00:10<02:43,  3.56it/s]  6%|▌         | 38/620 [00:10<02:43,  3.56it/s]  6%|▋         | 39/620 [00:10<02:43,  3.55it/s]  6%|▋         | 40/620 [00:11<02:43,  3.56it/s]  7%|▋         | 41/620 [00:11<02:42,  3.56it/s]  7%|▋         | 42/620 [00:11<02:42,  3.56it/s]  7%|▋         | 43/620 [00:12<02:42,  3.56it/s]  7%|▋         | 44/620 [00:12<02:41,  3.56it/s]  7%|▋         | 45/620 [00:12<02:41,  3.56it/s]  7%|▋         | 46/620 [00:12<02:41,  3.55it/s]  8%|▊         | 47/620 [00:13<02:41,  3.55it/s]  8%|▊         | 48/620 [00:13<02:41,  3.55it/s]  8%|▊         | 49/620 [00:13<02:40,  3.55it/s]  8%|▊         | 50/620 [00:14<02:41,  3.54it/s]  8%|▊         | 51/620 [00:14<02:40,  3.54it/s]  8%|▊         | 52/620 [00:14<02:40,  3.55it/s]  9%|▊         | 53/620 [00:14<02:39,  3.55it/s]  9%|▊         | 54/620 [00:15<02:39,  3.55it/s]  9%|▉         | 55/620 [00:15<02:39,  3.55it/s]  9%|▉         | 56/620 [00:15<02:38,  3.55it/s]  9%|▉         | 57/620 [00:16<02:38,  3.55it/s]  9%|▉         | 58/620 [00:16<02:38,  3.55it/s] 10%|▉         | 59/620 [00:16<02:38,  3.55it/s] 10%|▉         | 60/620 [00:16<02:37,  3.55it/s] 10%|▉         | 61/620 [00:17<02:38,  3.53it/s] 10%|█         | 62/620 [00:17<02:37,  3.54it/s] 10%|█         | 63/620 [00:17<02:37,  3.54it/s] 10%|█         | 64/620 [00:18<02:36,  3.55it/s] 10%|█         | 65/620 [00:18<02:36,  3.55it/s] 11%|█         | 66/620 [00:18<02:36,  3.55it/s] 11%|█         | 67/620 [00:18<02:35,  3.55it/s] 11%|█         | 68/620 [00:19<02:35,  3.55it/s] 11%|█         | 69/620 [00:19<02:35,  3.55it/s] 11%|█▏        | 70/620 [00:19<02:34,  3.55it/s] 11%|█▏        | 71/620 [00:19<02:34,  3.55it/s] 12%|█▏        | 72/620 [00:20<02:34,  3.54it/s] 12%|█▏        | 73/620 [00:20<02:34,  3.55it/s] 12%|█▏        | 74/620 [00:20<02:33,  3.55it/s] 12%|█▏        | 75/620 [00:21<02:33,  3.55it/s] 12%|█▏        | 76/620 [00:21<02:33,  3.55it/s] 12%|█▏        | 77/620 [00:21<02:32,  3.55it/s] 13%|█▎        | 78/620 [00:21<02:32,  3.55it/s] 13%|█▎        | 79/620 [00:22<02:32,  3.55it/s] 13%|█▎        | 80/620 [00:22<02:32,  3.55it/s] 13%|█▎        | 81/620 [00:22<02:31,  3.55it/s] 13%|█▎        | 82/620 [00:23<02:31,  3.55it/s] 13%|█▎        | 83/620 [00:23<02:31,  3.55it/s] 14%|█▎        | 84/620 [00:23<02:30,  3.56it/s] 14%|█▎        | 85/620 [00:23<02:29,  3.57it/s] 14%|█▍        | 86/620 [00:24<02:29,  3.58it/s] 14%|█▍        | 87/620 [00:24<02:28,  3.59it/s] 14%|█▍        | 88/620 [00:24<02:28,  3.58it/s] 14%|█▍        | 89/620 [00:25<02:28,  3.59it/s] 15%|█▍        | 90/620 [00:25<02:27,  3.59it/s] 15%|█▍        | 91/620 [00:25<02:27,  3.59it/s] 15%|█▍        | 92/620 [00:25<02:26,  3.60it/s] 15%|█▌        | 93/620 [00:26<02:26,  3.60it/s] 15%|█▌        | 94/620 [00:26<02:26,  3.60it/s] 15%|█▌        | 95/620 [00:26<02:25,  3.60it/s] 15%|█▌        | 96/620 [00:26<02:25,  3.60it/s] 16%|█▌        | 97/620 [00:27<02:25,  3.60it/s] 16%|█▌        | 98/620 [00:27<02:24,  3.60it/s] 16%|█▌        | 99/620 [00:27<02:25,  3.58it/s] 16%|█▌        | 100/620 [00:28<02:25,  3.58it/s] 16%|█▋        | 101/620 [00:28<02:24,  3.59it/s] 16%|█▋        | 102/620 [00:28<02:24,  3.59it/s] 17%|█▋        | 103/620 [00:28<02:23,  3.60it/s] 17%|█▋        | 104/620 [00:29<02:23,  3.60it/s] 17%|█▋        | 105/620 [00:29<02:22,  3.60it/s] 17%|█▋        | 106/620 [00:29<02:22,  3.61it/s] 17%|█▋        | 107/620 [00:30<02:22,  3.60it/s] 17%|█▋        | 108/620 [00:30<02:22,  3.60it/s] 18%|█▊        | 109/620 [00:30<02:21,  3.60it/s] 18%|█▊        | 110/620 [00:30<02:22,  3.57it/s] 18%|█▊        | 111/620 [00:31<02:22,  3.58it/s] 18%|█▊        | 112/620 [00:31<02:21,  3.59it/s] 18%|█▊        | 113/620 [00:31<02:21,  3.59it/s] 18%|█▊        | 114/620 [00:31<02:20,  3.59it/s] 19%|█▊        | 115/620 [00:32<02:20,  3.60it/s] 19%|█▊        | 116/620 [00:32<02:19,  3.60it/s] 19%|█▉        | 117/620 [00:32<02:19,  3.61it/s] 19%|█▉        | 118/620 [00:33<02:19,  3.61it/s] 19%|█▉        | 119/620 [00:33<02:19,  3.60it/s] 19%|█▉        | 120/620 [00:33<02:18,  3.61it/s] 20%|█▉        | 121/620 [00:33<02:18,  3.59it/s] 20%|█▉        | 122/620 [00:34<02:18,  3.59it/s] 20%|█▉        | 123/620 [00:34<02:18,  3.60it/s] 20%|██        | 124/620 [00:34<02:08,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 06:50:35,141 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:50:35,141 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 06:50:35,141 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.30it/s][A
  2%|▏         | 12/611 [00:00<00:12, 49.05it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.93it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.62it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.93it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.46it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.23it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.08it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.14it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.35it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.44it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.41it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.41it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.28it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.08it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.02it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.04it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.14it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.33it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.40it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.47it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.38it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.32it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.14it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.94it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.05it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.26it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.40it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.42it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.32it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.24it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.12it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.05it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.00it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.09it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.20it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.34it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.41it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.51it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.33it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.18it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.07it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.01it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.13it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.24it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.38it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.40it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.40it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.30it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.17it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.96it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.96it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.12it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.29it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.39it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.42it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.38it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.36it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.18it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.96it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.98it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.08it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.29it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.39it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.41it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.34it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.30it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.17it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.93it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.06it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.25it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.40it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.38it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.38it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.31it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.18it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.05it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 43.93it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.09it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.28it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.41it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.40it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.38it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.33it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.17it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.04it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 43.98it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.15it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.34it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.34it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.40it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.38it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.34it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.17it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.00it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.94it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.01it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.27it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.41it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.37it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.36it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.30it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.07it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.11it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.07it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.19it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.29it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.47it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.35it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.33it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.25it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.00it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.06it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.07it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.21it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.31it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.42it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.41it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.35it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.35it/s][A 20%|██        | 124/620 [00:48<02:08,  3.86it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:50:48,965 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124
[INFO|configuration_utils.py:351] 2023-08-28 06:50:48,984 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:50:50,730 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:50:50,751 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:50:50,760 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124/special_tokens_map.json
 20%|██        | 125/620 [00:50<41:35,  5.04s/it] 20%|██        | 126/620 [00:51<29:45,  3.62s/it] 20%|██        | 127/620 [00:51<21:29,  2.62s/it] 21%|██        | 128/620 [00:51<15:42,  1.92s/it] 21%|██        | 129/620 [00:52<11:39,  1.42s/it] 21%|██        | 130/620 [00:52<08:50,  1.08s/it] 21%|██        | 131/620 [00:52<06:51,  1.19it/s] 21%|██▏       | 132/620 [00:52<05:28,  1.49it/s] 21%|██▏       | 133/620 [00:53<04:30,  1.80it/s] 22%|██▏       | 134/620 [00:53<03:49,  2.11it/s] 22%|██▏       | 135/620 [00:53<03:21,  2.41it/s] 22%|██▏       | 136/620 [00:53<03:01,  2.66it/s] 22%|██▏       | 137/620 [00:54<02:47,  2.88it/s] 22%|██▏       | 138/620 [00:54<02:38,  3.05it/s] 22%|██▏       | 139/620 [00:54<02:31,  3.18it/s] 23%|██▎       | 140/620 [00:55<02:26,  3.29it/s] 23%|██▎       | 141/620 [00:55<02:23,  3.34it/s] 23%|██▎       | 142/620 [00:55<02:20,  3.40it/s] 23%|██▎       | 143/620 [00:55<02:18,  3.44it/s] 23%|██▎       | 144/620 [00:56<02:17,  3.47it/s] 23%|██▎       | 145/620 [00:56<02:15,  3.49it/s] 24%|██▎       | 146/620 [00:56<02:15,  3.51it/s] 24%|██▎       | 147/620 [00:57<02:14,  3.52it/s] 24%|██▍       | 148/620 [00:57<02:13,  3.53it/s] 24%|██▍       | 149/620 [00:57<02:13,  3.54it/s] 24%|██▍       | 150/620 [00:57<02:12,  3.54it/s] 24%|██▍       | 151/620 [00:58<02:12,  3.54it/s] 25%|██▍       | 152/620 [00:58<02:12,  3.52it/s] 25%|██▍       | 153/620 [00:58<02:12,  3.53it/s] 25%|██▍       | 154/620 [00:59<02:11,  3.53it/s] 25%|██▌       | 155/620 [00:59<02:11,  3.54it/s] 25%|██▌       | 156/620 [00:59<02:10,  3.54it/s] 25%|██▌       | 157/620 [00:59<02:10,  3.55it/s] 25%|██▌       | 158/620 [01:00<02:10,  3.55it/s] 26%|██▌       | 159/620 [01:00<02:09,  3.55it/s] 26%|██▌       | 160/620 [01:00<02:09,  3.55it/s] 26%|██▌       | 161/620 [01:01<02:09,  3.55it/s] 26%|██▌       | 162/620 [01:01<02:08,  3.55it/s] 26%|██▋       | 163/620 [01:01<02:09,  3.54it/s] 26%|██▋       | 164/620 [01:01<02:08,  3.54it/s] 27%|██▋       | 165/620 [01:02<02:08,  3.55it/s] 27%|██▋       | 166/620 [01:02<02:08,  3.55it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/620 [01:02<02:07,  3.55it/s] 27%|██▋       | 168/620 [01:03<02:07,  3.55it/s] 27%|██▋       | 169/620 [01:03<02:06,  3.55it/s] 27%|██▋       | 170/620 [01:03<02:06,  3.55it/s] 28%|██▊       | 171/620 [01:03<02:06,  3.55it/s] 28%|██▊       | 172/620 [01:04<02:06,  3.55it/s] 28%|██▊       | 173/620 [01:04<02:05,  3.55it/s] 28%|██▊       | 174/620 [01:04<02:06,  3.53it/s] 28%|██▊       | 175/620 [01:05<02:05,  3.54it/s] 28%|██▊       | 176/620 [01:05<02:05,  3.55it/s] 29%|██▊       | 177/620 [01:05<02:04,  3.55it/s] 29%|██▊       | 178/620 [01:05<02:04,  3.55it/s] 29%|██▉       | 179/620 [01:06<02:04,  3.55it/s] 29%|██▉       | 180/620 [01:06<02:04,  3.54it/s] 29%|██▉       | 181/620 [01:06<02:03,  3.54it/s] 29%|██▉       | 182/620 [01:06<02:03,  3.54it/s] 30%|██▉       | 183/620 [01:07<02:03,  3.55it/s] 30%|██▉       | 184/620 [01:07<02:03,  3.54it/s] 30%|██▉       | 185/620 [01:07<02:03,  3.53it/s] 30%|███       | 186/620 [01:08<02:02,  3.54it/s] 30%|███       | 187/620 [01:08<02:02,  3.54it/s] 30%|███       | 188/620 [01:08<02:01,  3.54it/s] 30%|███       | 189/620 [01:08<02:01,  3.54it/s] 31%|███       | 190/620 [01:09<02:01,  3.55it/s] 31%|███       | 191/620 [01:09<02:01,  3.54it/s] 31%|███       | 192/620 [01:09<02:00,  3.55it/s] 31%|███       | 193/620 [01:10<02:00,  3.55it/s] 31%|███▏      | 194/620 [01:10<02:00,  3.55it/s] 31%|███▏      | 195/620 [01:10<01:59,  3.55it/s] 32%|███▏      | 196/620 [01:10<01:59,  3.54it/s] 32%|███▏      | 197/620 [01:11<01:59,  3.55it/s] 32%|███▏      | 198/620 [01:11<01:59,  3.54it/s] 32%|███▏      | 199/620 [01:11<01:58,  3.55it/s] 32%|███▏      | 200/620 [01:12<01:58,  3.55it/s] 32%|███▏      | 201/620 [01:12<01:58,  3.55it/s] 33%|███▎      | 202/620 [01:12<01:57,  3.55it/s] 33%|███▎      | 203/620 [01:12<01:57,  3.55it/s] 33%|███▎      | 204/620 [01:13<01:57,  3.55it/s] 33%|███▎      | 205/620 [01:13<01:56,  3.55it/s] 33%|███▎      | 206/620 [01:13<01:56,  3.55it/s] 33%|███▎      | 207/620 [01:14<01:56,  3.53it/s] 34%|███▎      | 208/620 [01:14<01:56,  3.54it/s] 34%|███▎      | 209/620 [01:14<01:56,  3.54it/s] 34%|███▍      | 210/620 [01:14<01:55,  3.54it/s] 34%|███▍      | 211/620 [01:15<01:54,  3.56it/s] 34%|███▍      | 212/620 [01:15<01:54,  3.57it/s] 34%|███▍      | 213/620 [01:15<01:53,  3.58it/s] 35%|███▍      | 214/620 [01:15<01:53,  3.59it/s] 35%|███▍      | 215/620 [01:16<01:52,  3.59it/s] 35%|███▍      | 216/620 [01:16<01:52,  3.60it/s] 35%|███▌      | 217/620 [01:16<01:51,  3.60it/s] 35%|███▌      | 218/620 [01:17<01:51,  3.59it/s] 35%|███▌      | 219/620 [01:17<01:51,  3.59it/s] 35%|███▌      | 220/620 [01:17<01:51,  3.60it/s] 36%|███▌      | 221/620 [01:17<01:50,  3.60it/s] 36%|███▌      | 222/620 [01:18<01:50,  3.60it/s] 36%|███▌      | 223/620 [01:18<01:50,  3.60it/s] 36%|███▌      | 224/620 [01:18<01:49,  3.60it/s] 36%|███▋      | 225/620 [01:19<01:49,  3.60it/s] 36%|███▋      | 226/620 [01:19<01:49,  3.60it/s] 37%|███▋      | 227/620 [01:19<01:49,  3.60it/s] 37%|███▋      | 228/620 [01:19<01:48,  3.60it/s] 37%|███▋      | 229/620 [01:20<01:48,  3.59it/s] 37%|███▋      | 230/620 [01:20<01:48,  3.59it/s] 37%|███▋      | 231/620 [01:20<01:48,  3.59it/s] 37%|███▋      | 232/620 [01:20<01:47,  3.59it/s] 38%|███▊      | 233/620 [01:21<01:47,  3.59it/s] 38%|███▊      | 234/620 [01:21<01:47,  3.59it/s] 38%|███▊      | 235/620 [01:21<01:46,  3.60it/s] 38%|███▊      | 236/620 [01:22<01:46,  3.60it/s] 38%|███▊      | 237/620 [01:22<01:46,  3.60it/s] 38%|███▊      | 238/620 [01:22<01:46,  3.60it/s] 39%|███▊      | 239/620 [01:22<01:45,  3.60it/s] 39%|███▊      | 240/620 [01:23<01:45,  3.60it/s] 39%|███▉      | 241/620 [01:23<01:45,  3.60it/s] 39%|███▉      | 242/620 [01:23<01:44,  3.60it/s] 39%|███▉      | 243/620 [01:24<01:44,  3.60it/s] 39%|███▉      | 244/620 [01:24<01:44,  3.60it/s] 40%|███▉      | 245/620 [01:24<01:44,  3.60it/s] 40%|███▉      | 246/620 [01:24<01:43,  3.60it/s] 40%|███▉      | 247/620 [01:25<01:43,  3.60it/s] 40%|████      | 248/620 [01:25<01:36,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 06:51:25,813 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:51:25,813 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 06:51:25,813 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8035, 'eval_samples_per_second': 353.678, 'eval_steps_per_second': 44.264, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.18it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.83it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.75it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.50it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.94it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.59it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.25it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.98it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.04it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.36it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.51it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.51it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.37it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.17it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.11it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.01it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.94it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.10it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.29it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.39it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.45it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.38it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.22it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.08it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.98it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.12it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.18it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.38it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.43it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.38it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.11it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.06it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.98it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.11it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.19it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.26it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.32it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.32it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.32it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.14it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.03it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.06it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.09it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.16it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.13it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.34it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.32it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.21it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.07it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.99it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.06it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.12it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.19it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.15it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.32it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.35it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.24it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.10it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.09it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.03it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.16it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.14it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.20it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.24it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.26it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.28it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.11it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.16it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.08it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.10it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.11it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.24it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.24it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.30it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.28it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.23it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.10it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.18it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.11it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.13it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.17it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.32it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.30it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.26it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.27it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.11it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.15it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.11it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.16it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.16it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.28it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.26it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.31it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.12it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.18it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.02it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.13it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.09it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.28it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.35it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.29it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.26it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.04it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.23it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.17it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.12it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.15it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.25it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.30it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.29it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.23it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.15it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.11it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.12it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.18it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.18it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.22it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.29it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.22it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.26it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.26it/s][A 40%|████      | 248/620 [01:39<01:36,  3.86it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:51:39,649 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248
[INFO|configuration_utils.py:351] 2023-08-28 06:51:39,666 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:51:42,923 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:51:42,937 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:51:42,945 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248/special_tokens_map.json
 40%|████      | 249/620 [01:43<34:03,  5.51s/it] 40%|████      | 250/620 [01:43<24:17,  3.94s/it] 40%|████      | 251/620 [01:43<17:28,  2.84s/it] 41%|████      | 252/620 [01:43<12:43,  2.07s/it] 41%|████      | 253/620 [01:44<09:23,  1.54s/it] 41%|████      | 254/620 [01:44<07:04,  1.16s/it] 41%|████      | 255/620 [01:44<05:27,  1.12it/s] 41%|████▏     | 256/620 [01:45<04:19,  1.41it/s] 41%|████▏     | 257/620 [01:45<03:31,  1.72it/s] 42%|████▏     | 258/620 [01:45<02:58,  2.03it/s] 42%|████▏     | 259/620 [01:45<02:35,  2.32it/s] 42%|████▏     | 260/620 [01:46<02:18,  2.59it/s] 42%|████▏     | 261/620 [01:46<02:07,  2.82it/s] 42%|████▏     | 262/620 [01:46<01:59,  3.00it/s] 42%|████▏     | 263/620 [01:47<01:53,  3.15it/s] 43%|████▎     | 264/620 [01:47<01:49,  3.26it/s] 43%|████▎     | 265/620 [01:47<01:46,  3.34it/s] 43%|████▎     | 266/620 [01:47<01:43,  3.41it/s] 43%|████▎     | 267/620 [01:48<01:42,  3.45it/s] 43%|████▎     | 268/620 [01:48<01:41,  3.48it/s] 43%|████▎     | 269/620 [01:48<01:40,  3.50it/s] 44%|████▎     | 270/620 [01:49<01:40,  3.50it/s] 44%|████▎     | 271/620 [01:49<01:39,  3.52it/s] 44%|████▍     | 272/620 [01:49<01:38,  3.53it/s] 44%|████▍     | 273/620 [01:49<01:38,  3.54it/s] 44%|████▍     | 274/620 [01:50<01:37,  3.54it/s] 44%|████▍     | 275/620 [01:50<01:37,  3.53it/s] 45%|████▍     | 276/620 [01:50<01:37,  3.54it/s] 45%|████▍     | 277/620 [01:51<01:39,  3.44it/s] 45%|████▍     | 278/620 [01:51<01:38,  3.46it/s] 45%|████▌     | 279/620 [01:51<01:37,  3.49it/s] 45%|████▌     | 280/620 [01:51<01:36,  3.51it/s] 45%|████▌     | 281/620 [01:52<01:36,  3.51it/s] 45%|████▌     | 282/620 [01:52<01:35,  3.52it/s] 46%|████▌     | 283/620 [01:52<01:35,  3.53it/s] 46%|████▌     | 284/620 [01:53<01:35,  3.53it/s] 46%|████▌     | 285/620 [01:53<01:34,  3.54it/s] 46%|████▌     | 286/620 [01:53<01:34,  3.54it/s] 46%|████▋     | 287/620 [01:53<01:33,  3.55it/s] 46%|████▋     | 288/620 [01:54<01:33,  3.55it/s] 47%|████▋     | 289/620 [01:54<01:33,  3.55it/s] 47%|████▋     | 290/620 [01:54<01:32,  3.55it/s] 47%|████▋     | 291/620 [01:54<01:32,  3.56it/s] 47%|████▋     | 292/620 [01:55<01:32,  3.55it/s] 47%|████▋     | 293/620 [01:55<01:32,  3.55it/s] 47%|████▋     | 294/620 [01:55<01:31,  3.56it/s] 48%|████▊     | 295/620 [01:56<01:31,  3.57it/s] 48%|████▊     | 296/620 [01:56<01:30,  3.58it/s] 48%|████▊     | 297/620 [01:56<01:30,  3.59it/s] 48%|████▊     | 298/620 [01:56<01:29,  3.59it/s] 48%|████▊     | 299/620 [01:57<01:29,  3.59it/s] 48%|████▊     | 300/620 [01:57<01:28,  3.60it/s] 49%|████▊     | 301/620 [01:57<01:28,  3.60it/s] 49%|████▊     | 302/620 [01:58<01:28,  3.60it/s] 49%|████▉     | 303/620 [01:58<01:28,  3.60it/s] 49%|████▉     | 304/620 [01:58<01:27,  3.60it/s] 49%|████▉     | 305/620 [01:58<01:27,  3.60it/s] 49%|████▉     | 306/620 [01:59<01:27,  3.61it/s] 50%|████▉     | 307/620 [01:59<01:26,  3.61it/s] 50%|████▉     | 308/620 [01:59<01:26,  3.60it/s] 50%|████▉     | 309/620 [02:00<01:26,  3.59it/s] 50%|█████     | 310/620 [02:00<01:26,  3.59it/s] 50%|█████     | 311/620 [02:00<01:25,  3.60it/s] 50%|█████     | 312/620 [02:00<01:25,  3.60it/s] 50%|█████     | 313/620 [02:01<01:25,  3.60it/s] 51%|█████     | 314/620 [02:01<01:24,  3.60it/s] 51%|█████     | 315/620 [02:01<01:24,  3.60it/s] 51%|█████     | 316/620 [02:01<01:24,  3.60it/s] 51%|█████     | 317/620 [02:02<01:24,  3.60it/s] 51%|█████▏    | 318/620 [02:02<01:23,  3.60it/s] 51%|█████▏    | 319/620 [02:02<01:23,  3.60it/s] 52%|█████▏    | 320/620 [02:03<01:23,  3.59it/s] 52%|█████▏    | 321/620 [02:03<01:23,  3.59it/s] 52%|█████▏    | 322/620 [02:03<01:22,  3.60it/s] 52%|█████▏    | 323/620 [02:03<01:22,  3.60it/s] 52%|█████▏    | 324/620 [02:04<01:22,  3.60it/s] 52%|█████▏    | 325/620 [02:04<01:21,  3.60it/s] 53%|█████▎    | 326/620 [02:04<01:21,  3.61it/s] 53%|█████▎    | 327/620 [02:05<01:21,  3.60it/s] 53%|█████▎    | 328/620 [02:05<01:21,  3.60it/s] 53%|█████▎    | 329/620 [02:05<01:20,  3.60it/s] 53%|█████▎    | 330/620 [02:05<01:20,  3.60it/s] 53%|█████▎    | 331/620 [02:06<01:20,  3.58it/s] 54%|█████▎    | 332/620 [02:06<01:20,  3.58it/s] 54%|█████▎    | 333/620 [02:06<01:19,  3.59it/s] 54%|█████▍    | 334/620 [02:06<01:19,  3.59it/s] 54%|█████▍    | 335/620 [02:07<01:19,  3.59it/s] 54%|█████▍    | 336/620 [02:07<01:18,  3.60it/s] 54%|█████▍    | 337/620 [02:07<01:18,  3.60it/s] 55%|█████▍    | 338/620 [02:08<01:18,  3.60it/s] 55%|█████▍    | 339/620 [02:08<01:17,  3.60it/s] 55%|█████▍    | 340/620 [02:08<01:17,  3.60it/s] 55%|█████▌    | 341/620 [02:08<01:17,  3.60it/s] 55%|█████▌    | 342/620 [02:09<01:17,  3.57it/s] 55%|█████▌    | 343/620 [02:09<01:17,  3.58it/s] 55%|█████▌    | 344/620 [02:09<01:16,  3.59it/s] 56%|█████▌    | 345/620 [02:10<01:16,  3.59it/s] 56%|█████▌    | 346/620 [02:10<01:16,  3.59it/s] 56%|█████▌    | 347/620 [02:10<01:15,  3.60it/s] 56%|█████▌    | 348/620 [02:10<01:15,  3.60it/s] 56%|█████▋    | 349/620 [02:11<01:15,  3.60it/s] 56%|█████▋    | 350/620 [02:11<01:14,  3.60it/s] 57%|█████▋    | 351/620 [02:11<01:14,  3.60it/s] 57%|█████▋    | 352/620 [02:11<01:14,  3.60it/s] 57%|█████▋    | 353/620 [02:12<01:34,  2.83it/s] 57%|█████▋    | 354/620 [02:12<01:28,  3.02it/s] 57%|█████▋    | 355/620 [02:13<01:23,  3.17it/s] 57%|█████▋    | 356/620 [02:13<01:20,  3.29it/s] 58%|█████▊    | 357/620 [02:13<01:17,  3.38it/s] 58%|█████▊    | 358/620 [02:13<01:16,  3.44it/s] 58%|█████▊    | 359/620 [02:14<01:14,  3.48it/s] 58%|█████▊    | 360/620 [02:14<01:13,  3.52it/s] 58%|█████▊    | 361/620 [02:14<01:13,  3.54it/s] 58%|█████▊    | 362/620 [02:14<01:12,  3.55it/s] 59%|█████▊    | 363/620 [02:15<01:12,  3.56it/s] 59%|█████▊    | 364/620 [02:15<01:11,  3.57it/s] 59%|█████▉    | 365/620 [02:15<01:11,  3.58it/s] 59%|█████▉    | 366/620 [02:16<01:10,  3.59it/s] 59%|█████▉    | 367/620 [02:16<01:10,  3.59it/s] 59%|█████▉    | 368/620 [02:16<01:10,  3.60it/s] 60%|█████▉    | 369/620 [02:16<01:09,  3.60it/s] 60%|█████▉    | 370/620 [02:17<01:09,  3.60it/s] 60%|█████▉    | 371/620 [02:17<01:09,  3.60it/s] 60%|██████    | 372/620 [02:17<01:04,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 06:52:18,150 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:52:18,150 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 06:52:18,151 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8146, 'eval_samples_per_second': 353.393, 'eval_steps_per_second': 44.228, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.94it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.92it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.09it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.99it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.09it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.49it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.26it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.01it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.16it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.42it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.54it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.63it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.42it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.18it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.98it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.83it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.87it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.01it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.28it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.40it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.56it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.48it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.20it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.08it/s][A
 21%|██        | 127/611 [00:02<00:11, 44.00it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.02it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.16it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.28it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.44it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.49it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.46it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.25it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.01it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.95it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.98it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.12it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.22it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.30it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.48it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.50it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.36it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.12it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.87it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.10it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.19it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.18it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.37it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.48it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.46it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.31it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.20it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.14it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.15it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.21it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.18it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.37it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.49it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.43it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.29it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.20it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.09it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.13it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.16it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.17it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.38it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.44it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.38it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.30it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.21it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.13it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.07it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.13it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.33it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.39it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.43it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.36it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.24it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.13it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.16it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.16it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.10it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.30it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.35it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.48it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.41it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.24it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.26it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.10it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.10it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.13it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.33it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.31it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.45it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.36it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.32it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.20it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.06it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.01it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.13it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.30it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.41it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.42it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.38it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.28it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.21it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.96it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 43.96it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.10it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.31it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.45it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.43it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.41it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.30it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.19it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.10it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.08it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.06it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.25it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.39it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.34it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.29it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.29it/s][A 60%|██████    | 372/620 [02:31<01:04,  3.86it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:52:31,970 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372
[INFO|configuration_utils.py:351] 2023-08-28 06:52:31,993 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:52:34,239 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:52:34,259 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:52:34,272 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372/special_tokens_map.json
 60%|██████    | 373/620 [02:34<21:22,  5.19s/it] 60%|██████    | 374/620 [02:34<15:14,  3.72s/it] 60%|██████    | 375/620 [02:34<10:58,  2.69s/it] 61%|██████    | 376/620 [02:35<07:59,  1.97s/it] 61%|██████    | 377/620 [02:35<05:54,  1.46s/it] 61%|██████    | 378/620 [02:35<04:27,  1.11s/it] 61%|██████    | 379/620 [02:36<03:27,  1.16it/s] 61%|██████▏   | 380/620 [02:36<02:44,  1.46it/s] 61%|██████▏   | 381/620 [02:36<02:14,  1.77it/s] 62%|██████▏   | 382/620 [02:36<01:54,  2.09it/s] 62%|██████▏   | 383/620 [02:37<01:39,  2.38it/s] 62%|██████▏   | 384/620 [02:37<01:29,  2.65it/s] 62%|██████▏   | 385/620 [02:37<01:21,  2.88it/s] 62%|██████▏   | 386/620 [02:38<01:16,  3.06it/s] 62%|██████▏   | 387/620 [02:38<01:12,  3.21it/s] 63%|██████▎   | 388/620 [02:38<01:09,  3.32it/s] 63%|██████▎   | 389/620 [02:38<01:08,  3.40it/s] 63%|██████▎   | 390/620 [02:39<01:06,  3.46it/s] 63%|██████▎   | 391/620 [02:39<01:05,  3.50it/s] 63%|██████▎   | 392/620 [02:39<01:04,  3.53it/s] 63%|██████▎   | 393/620 [02:39<01:03,  3.56it/s] 64%|██████▎   | 394/620 [02:40<01:03,  3.56it/s] 64%|██████▎   | 395/620 [02:40<01:02,  3.58it/s] 64%|██████▍   | 396/620 [02:40<01:02,  3.59it/s] 64%|██████▍   | 397/620 [02:41<01:02,  3.59it/s] 64%|██████▍   | 398/620 [02:41<01:01,  3.59it/s] 64%|██████▍   | 399/620 [02:41<01:01,  3.60it/s] 65%|██████▍   | 400/620 [02:41<01:01,  3.60it/s] 65%|██████▍   | 401/620 [02:42<01:00,  3.60it/s] 65%|██████▍   | 402/620 [02:42<01:00,  3.61it/s] 65%|██████▌   | 403/620 [02:42<01:00,  3.61it/s] 65%|██████▌   | 404/620 [02:43<00:59,  3.60it/s] 65%|██████▌   | 405/620 [02:43<00:59,  3.59it/s] 65%|██████▌   | 406/620 [02:43<00:59,  3.59it/s] 66%|██████▌   | 407/620 [02:43<00:59,  3.60it/s] 66%|██████▌   | 408/620 [02:44<00:58,  3.60it/s] 66%|██████▌   | 409/620 [02:44<00:58,  3.60it/s] 66%|██████▌   | 410/620 [02:44<00:58,  3.60it/s] 66%|██████▋   | 411/620 [02:44<00:57,  3.60it/s] 66%|██████▋   | 412/620 [02:45<00:57,  3.60it/s] 67%|██████▋   | 413/620 [02:45<00:57,  3.60it/s] 67%|██████▋   | 414/620 [02:45<00:57,  3.60it/s] 67%|██████▋   | 415/620 [02:46<00:56,  3.61it/s] 67%|██████▋   | 416/620 [02:46<00:56,  3.59it/s] 67%|██████▋   | 417/620 [02:46<00:56,  3.59it/s] 67%|██████▋   | 418/620 [02:46<00:56,  3.59it/s] 68%|██████▊   | 419/620 [02:47<00:55,  3.60it/s] 68%|██████▊   | 420/620 [02:47<00:55,  3.60it/s] 68%|██████▊   | 421/620 [02:47<00:55,  3.60it/s] 68%|██████▊   | 422/620 [02:48<00:54,  3.60it/s] 68%|██████▊   | 423/620 [02:48<00:54,  3.60it/s] 68%|██████▊   | 424/620 [02:48<00:54,  3.60it/s] 69%|██████▊   | 425/620 [02:48<00:54,  3.60it/s] 69%|██████▊   | 426/620 [02:49<00:53,  3.60it/s] 69%|██████▉   | 427/620 [02:49<00:53,  3.59it/s] 69%|██████▉   | 428/620 [02:49<00:53,  3.60it/s] 69%|██████▉   | 429/620 [02:49<00:53,  3.60it/s] 69%|██████▉   | 430/620 [02:50<00:52,  3.60it/s] 70%|██████▉   | 431/620 [02:50<00:52,  3.59it/s] 70%|██████▉   | 432/620 [02:50<00:52,  3.59it/s] 70%|██████▉   | 433/620 [02:51<00:53,  3.52it/s] 70%|███████   | 434/620 [02:51<00:52,  3.53it/s] 70%|███████   | 435/620 [02:51<00:52,  3.56it/s] 70%|███████   | 436/620 [02:51<00:51,  3.57it/s] 70%|███████   | 437/620 [02:52<00:51,  3.58it/s] 71%|███████   | 438/620 [02:52<00:51,  3.57it/s] 71%|███████   | 439/620 [02:52<00:50,  3.58it/s] 71%|███████   | 440/620 [02:53<00:50,  3.58it/s] 71%|███████   | 441/620 [02:53<00:49,  3.59it/s] 71%|███████▏  | 442/620 [02:53<00:49,  3.59it/s] 71%|███████▏  | 443/620 [02:53<00:49,  3.60it/s] 72%|███████▏  | 444/620 [02:54<00:48,  3.60it/s] 72%|███████▏  | 445/620 [02:54<00:48,  3.60it/s] 72%|███████▏  | 446/620 [02:54<00:48,  3.60it/s] 72%|███████▏  | 447/620 [02:55<00:48,  3.60it/s] 72%|███████▏  | 448/620 [02:55<00:47,  3.60it/s] 72%|███████▏  | 449/620 [02:55<00:47,  3.60it/s] 73%|███████▎  | 450/620 [02:55<00:47,  3.60it/s] 73%|███████▎  | 451/620 [02:56<00:46,  3.60it/s] 73%|███████▎  | 452/620 [02:56<00:46,  3.60it/s] 73%|███████▎  | 453/620 [02:56<00:46,  3.61it/s] 73%|███████▎  | 454/620 [02:56<00:46,  3.61it/s] 73%|███████▎  | 455/620 [02:57<00:45,  3.61it/s] 74%|███████▎  | 456/620 [02:57<00:45,  3.60it/s] 74%|███████▎  | 457/620 [02:57<00:45,  3.60it/s] 74%|███████▍  | 458/620 [02:58<00:44,  3.60it/s] 74%|███████▍  | 459/620 [02:58<00:45,  3.57it/s] 74%|███████▍  | 460/620 [02:58<00:44,  3.59it/s] 74%|███████▍  | 461/620 [02:58<00:44,  3.59it/s] 75%|███████▍  | 462/620 [02:59<00:43,  3.59it/s] 75%|███████▍  | 463/620 [02:59<00:43,  3.59it/s] 75%|███████▍  | 464/620 [02:59<00:43,  3.60it/s] 75%|███████▌  | 465/620 [03:00<00:43,  3.60it/s] 75%|███████▌  | 466/620 [03:00<00:42,  3.60it/s] 75%|███████▌  | 467/620 [03:00<00:42,  3.60it/s] 75%|███████▌  | 468/620 [03:00<00:42,  3.60it/s] 76%|███████▌  | 469/620 [03:01<00:41,  3.60it/s] 76%|███████▌  | 470/620 [03:01<00:41,  3.59it/s] 76%|███████▌  | 471/620 [03:01<00:41,  3.59it/s] 76%|███████▌  | 472/620 [03:01<00:41,  3.60it/s] 76%|███████▋  | 473/620 [03:02<00:40,  3.60it/s] 76%|███████▋  | 474/620 [03:02<00:40,  3.60it/s] 77%|███████▋  | 475/620 [03:02<00:40,  3.60it/s] 77%|███████▋  | 476/620 [03:03<00:40,  3.60it/s] 77%|███████▋  | 477/620 [03:03<00:39,  3.60it/s] 77%|███████▋  | 478/620 [03:03<00:39,  3.60it/s] 77%|███████▋  | 479/620 [03:03<00:39,  3.59it/s] 77%|███████▋  | 480/620 [03:04<00:38,  3.59it/s] 78%|███████▊  | 481/620 [03:04<00:38,  3.59it/s] 78%|███████▊  | 482/620 [03:04<00:38,  3.59it/s] 78%|███████▊  | 483/620 [03:05<00:38,  3.59it/s] 78%|███████▊  | 484/620 [03:05<00:37,  3.60it/s] 78%|███████▊  | 485/620 [03:05<00:37,  3.60it/s] 78%|███████▊  | 486/620 [03:05<00:37,  3.60it/s] 79%|███████▊  | 487/620 [03:06<00:36,  3.60it/s] 79%|███████▊  | 488/620 [03:06<00:36,  3.60it/s] 79%|███████▉  | 489/620 [03:06<00:36,  3.60it/s] 79%|███████▉  | 490/620 [03:06<00:36,  3.60it/s] 79%|███████▉  | 491/620 [03:07<00:35,  3.59it/s] 79%|███████▉  | 492/620 [03:07<00:35,  3.58it/s] 80%|███████▉  | 493/620 [03:07<00:35,  3.58it/s] 80%|███████▉  | 494/620 [03:08<00:35,  3.59it/s] 80%|███████▉  | 495/620 [03:08<00:34,  3.59it/s] 80%|████████  | 496/620 [03:08<00:32,  3.85it/s][INFO|trainer.py:2140] 2023-08-28 06:53:09,031 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:53:09,031 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 06:53:09,031 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.797, 'eval_samples_per_second': 353.845, 'eval_steps_per_second': 44.285, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.21it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.85it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.98it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.62it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.97it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.53it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.22it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.08it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.25it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.49it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.58it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.55it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.46it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.30it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.17it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.98it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.99it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.24it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.46it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.54it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.40it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.28it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.24it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.06it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.93it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.03it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.22it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.32it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.59it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.50it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.37it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.28it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.15it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.00it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.05it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.22it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.38it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.45it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.53it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.32it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.15it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.03it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.93it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.03it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.15it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.35it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.48it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.49it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.35it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.23it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.12it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.06it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.06it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.13it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.25it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.49it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.49it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.34it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.28it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.12it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.03it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.02it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.19it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.36it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.51it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.37it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.36it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.24it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.04it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.00it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.11it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.27it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.45it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.47it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.47it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.21it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.11it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.00it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.02it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.13it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.29it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.47it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.54it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.36it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.27it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.14it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.00it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 43.98it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.12it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.25it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.41it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.54it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.52it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.23it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.11it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.02it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.97it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.12it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.24it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.46it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.53it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.42it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.26it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.15it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.05it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.07it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.15it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.30it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.55it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.51it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.26it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.14it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.01it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.04it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.01it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.19it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.37it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.52it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.48it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.34it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.34it/s][A 80%|████████  | 496/620 [03:22<00:32,  3.85it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:53:22,844 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496
[INFO|configuration_utils.py:351] 2023-08-28 06:53:22,871 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:53:25,197 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:53:25,213 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:53:25,222 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496/special_tokens_map.json
 80%|████████  | 497/620 [03:25<10:40,  5.21s/it] 80%|████████  | 498/620 [03:25<07:35,  3.73s/it] 80%|████████  | 499/620 [03:25<05:26,  2.70s/it] 81%|████████  | 500/620 [03:26<03:56,  1.97s/it]                                                  81%|████████  | 500/620 [03:26<03:56,  1.97s/it] 81%|████████  | 501/620 [03:26<02:54,  1.46s/it] 81%|████████  | 502/620 [03:26<02:10,  1.11s/it] 81%|████████  | 503/620 [03:27<01:40,  1.16it/s] 81%|████████▏ | 504/620 [03:27<01:19,  1.46it/s] 81%|████████▏ | 505/620 [03:27<01:05,  1.77it/s] 82%|████████▏ | 506/620 [03:27<00:54,  2.08it/s] 82%|████████▏ | 507/620 [03:28<00:47,  2.38it/s] 82%|████████▏ | 508/620 [03:28<00:42,  2.64it/s] 82%|████████▏ | 509/620 [03:28<00:38,  2.86it/s] 82%|████████▏ | 510/620 [03:29<00:36,  3.03it/s] 82%|████████▏ | 511/620 [03:29<00:34,  3.17it/s] 83%|████████▎ | 512/620 [03:29<00:32,  3.28it/s] 83%|████████▎ | 513/620 [03:29<00:31,  3.35it/s] 83%|████████▎ | 514/620 [03:30<00:31,  3.41it/s] 83%|████████▎ | 515/620 [03:30<00:30,  3.45it/s] 83%|████████▎ | 516/620 [03:30<00:29,  3.48it/s] 83%|████████▎ | 517/620 [03:30<00:29,  3.50it/s] 84%|████████▎ | 518/620 [03:31<00:29,  3.52it/s] 84%|████████▎ | 519/620 [03:31<00:28,  3.53it/s] 84%|████████▍ | 520/620 [03:31<00:28,  3.54it/s] 84%|████████▍ | 521/620 [03:32<00:28,  3.51it/s] 84%|████████▍ | 522/620 [03:32<00:27,  3.52it/s] 84%|████████▍ | 523/620 [03:32<00:27,  3.53it/s] 85%|████████▍ | 524/620 [03:32<00:27,  3.54it/s] 85%|████████▍ | 525/620 [03:33<00:26,  3.54it/s] 85%|████████▍ | 526/620 [03:33<00:26,  3.54it/s] 85%|████████▌ | 527/620 [03:33<00:26,  3.55it/s] 85%|████████▌ | 528/620 [03:34<00:25,  3.55it/s] 85%|████████▌ | 529/620 [03:34<00:25,  3.56it/s] 85%|████████▌ | 530/620 [03:34<00:25,  3.56it/s] 86%|████████▌ | 531/620 [03:34<00:25,  3.56it/s] 86%|████████▌ | 532/620 [03:35<00:24,  3.53it/s] 86%|████████▌ | 533/620 [03:35<00:24,  3.54it/s] 86%|████████▌ | 534/620 [03:35<00:24,  3.54it/s] 86%|████████▋ | 535/620 [03:36<00:23,  3.54it/s] 86%|████████▋ | 536/620 [03:36<00:23,  3.55it/s] 87%|████████▋ | 537/620 [03:36<00:23,  3.55it/s] 87%|████████▋ | 538/620 [03:36<00:23,  3.55it/s] 87%|████████▋ | 539/620 [03:37<00:22,  3.56it/s] 87%|████████▋ | 540/620 [03:37<00:22,  3.56it/s] 87%|████████▋ | 541/620 [03:37<00:22,  3.56it/s] 87%|████████▋ | 542/620 [03:38<00:21,  3.56it/s] 88%|████████▊ | 543/620 [03:38<00:21,  3.54it/s] 88%|████████▊ | 544/620 [03:38<00:21,  3.54it/s] 88%|████████▊ | 545/620 [03:38<00:21,  3.54it/s] 88%|████████▊ | 546/620 [03:39<00:20,  3.54it/s] 88%|████████▊ | 547/620 [03:39<00:20,  3.55it/s] 88%|████████▊ | 548/620 [03:39<00:20,  3.55it/s] 89%|████████▊ | 549/620 [03:39<00:19,  3.55it/s] 89%|████████▊ | 550/620 [03:40<00:19,  3.56it/s] 89%|████████▉ | 551/620 [03:40<00:19,  3.56it/s] 89%|████████▉ | 552/620 [03:40<00:19,  3.56it/s] 89%|████████▉ | 553/620 [03:41<00:18,  3.56it/s] 89%|████████▉ | 554/620 [03:41<00:18,  3.54it/s] 90%|████████▉ | 555/620 [03:41<00:18,  3.55it/s] 90%|████████▉ | 556/620 [03:41<00:18,  3.55it/s] 90%|████████▉ | 557/620 [03:42<00:17,  3.55it/s] 90%|█████████ | 558/620 [03:42<00:17,  3.55it/s] 90%|█████████ | 559/620 [03:42<00:17,  3.55it/s] 90%|█████████ | 560/620 [03:43<00:16,  3.55it/s] 90%|█████████ | 561/620 [03:43<00:16,  3.55it/s] 91%|█████████ | 562/620 [03:43<00:16,  3.56it/s] 91%|█████████ | 563/620 [03:43<00:16,  3.55it/s] 91%|█████████ | 564/620 [03:44<00:15,  3.55it/s] 91%|█████████ | 565/620 [03:44<00:15,  3.53it/s] 91%|█████████▏| 566/620 [03:44<00:15,  3.54it/s] 91%|█████████▏| 567/620 [03:45<00:14,  3.54it/s] 92%|█████████▏| 568/620 [03:45<00:14,  3.54it/s] 92%|█████████▏| 569/620 [03:45<00:14,  3.54it/s] 92%|█████████▏| 570/620 [03:45<00:14,  3.55it/s] 92%|█████████▏| 571/620 [03:46<00:13,  3.55it/s] 92%|█████████▏| 572/620 [03:46<00:13,  3.56it/s] 92%|█████████▏| 573/620 [03:46<00:13,  3.56it/s] 93%|█████████▎| 574/620 [03:47<00:12,  3.55it/s] 93%|█████████▎| 575/620 [03:47<00:12,  3.56it/s] 93%|█████████▎| 576/620 [03:47<00:12,  3.53it/s] 93%|█████████▎| 577/620 [03:47<00:12,  3.54it/s] 93%|█████████▎| 578/620 [03:48<00:11,  3.54it/s] 93%|█████████▎| 579/620 [03:48<00:11,  3.55it/s] 94%|█████████▎| 580/620 [03:48<00:11,  3.55it/s] 94%|█████████▎| 581/620 [03:49<00:10,  3.55it/s] 94%|█████████▍| 582/620 [03:49<00:10,  3.56it/s] 94%|█████████▍| 583/620 [03:49<00:10,  3.56it/s] 94%|█████████▍| 584/620 [03:49<00:10,  3.56it/s] 94%|█████████▍| 585/620 [03:50<00:09,  3.56it/s] 95%|█████████▍| 586/620 [03:50<00:09,  3.55it/s] 95%|█████████▍| 587/620 [03:50<00:09,  3.52it/s] 95%|█████████▍| 588/620 [03:50<00:09,  3.53it/s] 95%|█████████▌| 589/620 [03:51<00:08,  3.46it/s] 95%|█████████▌| 590/620 [03:51<00:08,  3.47it/s] 95%|█████████▌| 591/620 [03:51<00:08,  3.49it/s] 95%|█████████▌| 592/620 [03:52<00:07,  3.51it/s] 96%|█████████▌| 593/620 [03:52<00:07,  3.52it/s] 96%|█████████▌| 594/620 [03:52<00:07,  3.53it/s] 96%|█████████▌| 595/620 [03:52<00:07,  3.53it/s] 96%|█████████▌| 596/620 [03:53<00:06,  3.54it/s] 96%|█████████▋| 597/620 [03:53<00:06,  3.55it/s] 96%|█████████▋| 598/620 [03:53<00:06,  3.54it/s] 97%|█████████▋| 599/620 [03:54<00:05,  3.54it/s] 97%|█████████▋| 600/620 [03:54<00:05,  3.54it/s] 97%|█████████▋| 601/620 [03:54<00:05,  3.54it/s] 97%|█████████▋| 602/620 [03:54<00:05,  3.54it/s] 97%|█████████▋| 603/620 [03:55<00:04,  3.54it/s] 97%|█████████▋| 604/620 [03:55<00:04,  3.55it/s] 98%|█████████▊| 605/620 [03:55<00:04,  3.55it/s] 98%|█████████▊| 606/620 [03:56<00:03,  3.55it/s] 98%|█████████▊| 607/620 [03:56<00:03,  3.56it/s] 98%|█████████▊| 608/620 [03:56<00:03,  3.55it/s] 98%|█████████▊| 609/620 [03:56<00:03,  3.55it/s] 98%|█████████▊| 610/620 [03:57<00:02,  3.55it/s] 99%|█████████▊| 611/620 [03:57<00:02,  3.55it/s] 99%|█████████▊| 612/620 [03:57<00:02,  3.55it/s] 99%|█████████▉| 613/620 [03:58<00:01,  3.54it/s] 99%|█████████▉| 614/620 [03:58<00:01,  3.55it/s] 99%|█████████▉| 615/620 [03:58<00:01,  3.55it/s] 99%|█████████▉| 616/620 [03:58<00:01,  3.55it/s]100%|█████████▉| 617/620 [03:59<00:00,  3.55it/s]100%|█████████▉| 618/620 [03:59<00:00,  3.55it/s]100%|█████████▉| 619/620 [03:59<00:00,  3.52it/s]100%|██████████| 620/620 [03:59<00:00,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 06:54:00,426 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:54:00,426 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 06:54:00,426 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.7971, 'eval_samples_per_second': 353.842, 'eval_steps_per_second': 44.285, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7298387096774197e-05, 'epoch': 4.03}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.59it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.62it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.17it/s][A
  4%|▎         | 22/611 [00:00<00:12, 46.24it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.49it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.77it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.26it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.88it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.09it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.29it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.44it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.56it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.61it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.52it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.24it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.83it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.81it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.94it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.04it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.41it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.57it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.61it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.39it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.15it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.93it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.90it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.93it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.11it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.36it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.51it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.46it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.45it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.19it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.09it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.05it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.09it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.19it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.26it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.38it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.53it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.39it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.22it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.16it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.15it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.10it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.19it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.31it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.42it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.40it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.36it/s][A
 42%|████▏     | 257/611 [00:05<00:07, 44.26it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.18it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.15it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.89it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 43.99it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.26it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.36it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.42it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.35it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.25it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.14it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.01it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.01it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.20it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.30it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.47it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.45it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.41it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.39it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.29it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.14it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.00it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.13it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.12it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.28it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.34it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.39it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.32it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.25it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.21it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.13it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.14it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.18it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.27it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.33it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.35it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.28it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.23it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.15it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.15it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.15it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.23it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.34it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.26it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.30it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.25it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.10it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.25it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.18it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.17it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.22it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.19it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.22it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.28it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.26it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.23it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.11it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.12it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.13it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.19it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.31it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.30it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.33it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.23it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.23it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.13it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.07it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.07it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.18it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.34it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.39it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.39it/s][A100%|██████████| 620/620 [04:13<00:00,  3.78it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:54:14,246 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620
[INFO|configuration_utils.py:351] 2023-08-28 06:54:14,262 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:54:16,674 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:54:16,692 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:54:16,702 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:54:16,964 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:54:16,964 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124 (score: 0.9401851296424866).
                                                 100%|██████████| 620/620 [04:18<00:00,  3.78it/s]100%|██████████| 620/620 [04:18<00:00,  2.40it/s]
[INFO|trainer.py:1894] 2023-08-28 06:54:18,644 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 06:54:18,664 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:54:22,249 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:54:22,272 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:54:22,280 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:54:22,487 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:22,488 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:22,488 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:22,488 >>   train_runtime            = 0:04:18.20
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:22,488 >>   train_samples            =       7920
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:22,488 >>   train_samples_per_second =    153.369
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:22,488 >>   train_steps_per_second   =      2.401
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8007, 'eval_samples_per_second': 353.751, 'eval_steps_per_second': 44.273, 'epoch': 5.0}
{'train_runtime': 258.2004, 'train_samples_per_second': 153.369, 'train_steps_per_second': 2.401, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:54:22 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:54:22,527 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:54:22,527 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 06:54:22,527 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.13it/s]  2%|▏         | 12/611 [00:00<00:12, 48.76it/s]  3%|▎         | 17/611 [00:00<00:12, 47.22it/s]  4%|▎         | 22/611 [00:00<00:12, 46.39it/s]  4%|▍         | 27/611 [00:00<00:12, 45.85it/s]  5%|▌         | 32/611 [00:00<00:12, 45.62it/s]  6%|▌         | 37/611 [00:00<00:12, 45.42it/s]  7%|▋         | 42/611 [00:00<00:12, 44.80it/s]  8%|▊         | 47/611 [00:01<00:12, 44.18it/s]  9%|▊         | 52/611 [00:01<00:12, 43.80it/s]  9%|▉         | 57/611 [00:01<00:12, 43.69it/s] 10%|█         | 62/611 [00:01<00:12, 43.92it/s] 11%|█         | 67/611 [00:01<00:12, 44.16it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.35it/s] 13%|█▎        | 77/611 [00:01<00:12, 44.39it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.62it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.42it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.16it/s] 16%|█▌        | 97/611 [00:02<00:11, 43.85it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.83it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.02it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.21it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.38it/s] 20%|█▉        | 122/611 [00:02<00:10, 44.49it/s] 21%|██        | 127/611 [00:02<00:10, 44.52it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.36it/s] 22%|██▏       | 137/611 [00:03<00:10, 44.12it/s] 23%|██▎       | 142/611 [00:03<00:10, 43.96it/s] 24%|██▍       | 147/611 [00:03<00:10, 43.93it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.11it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.23it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.31it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.44it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.39it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.20it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.05it/s] 31%|███       | 187/611 [00:04<00:09, 43.95it/s] 31%|███▏      | 192/611 [00:04<00:09, 43.98it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.15it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.24it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.26it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.37it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.35it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.21it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.08it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.04it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.06it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.13it/s] 40%|████      | 247/611 [00:05<00:08, 44.22it/s] 41%|████      | 252/611 [00:05<00:08, 44.30it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.37it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.26it/s] 44%|████▎     | 267/611 [00:06<00:07, 44.23it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.11it/s] 45%|████▌     | 277/611 [00:06<00:07, 43.98it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.04it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.16it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.25it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.33it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.30it/s] 50%|█████     | 307/611 [00:06<00:06, 44.31it/s] 51%|█████     | 312/611 [00:07<00:06, 44.17it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.09it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.00it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.09it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.21it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.30it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.31it/s] 57%|█████▋    | 347/611 [00:07<00:06, 43.83it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.45it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.33it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.17it/s] 60%|██████    | 367/611 [00:08<00:05, 44.12it/s] 61%|██████    | 372/611 [00:08<00:05, 44.12it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.21it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.30it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.33it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.24it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.30it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.16it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.07it/s] 67%|██████▋   | 412/611 [00:09<00:04, 43.98it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.10it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.26it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.22it/s] 71%|███████   | 432/611 [00:09<00:04, 44.24it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.30it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.24it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.17it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.01it/s] 75%|███████▍  | 457/611 [00:10<00:03, 43.97it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.10it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.29it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.37it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.37it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.34it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.38it/s] 81%|████████  | 492/611 [00:11<00:02, 44.34it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.22it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.12it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.26it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.50it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.54it/s] 85%|████████▌ | 522/611 [00:11<00:01, 44.53it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.38it/s] 87%|████████▋ | 532/611 [00:11<00:01, 44.35it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.23it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.27it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.30it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.32it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.46it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.49it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.54it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.40it/s] 94%|█████████▍| 577/611 [00:13<00:00, 44.31it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.26it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.27it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.32it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.30it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.42it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.49it/s]100%|██████████| 611/611 [00:13<00:00, 44.34it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:54:36,324 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:36,324 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:36,324 >>   eval_loss               =     0.9402
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:36,324 >>   eval_runtime            = 0:00:13.79
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:36,324 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:36,324 >>   eval_samples_per_second =    353.869
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:36,324 >>   eval_steps_per_second   =     44.288
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:54:36,324 >>   perplexity              =     2.5605
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:42,604 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:42,608 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:42,608 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:42,608 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:42,608 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:54:42,917 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:54:42,918 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:54:43,180 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:54:44,233 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:54:44,233 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:46,752 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:46,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:46,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:46,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:46,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:54:47,106 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:54:47,107 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:54:47,363 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:54:47,533 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:54:47,533 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.75it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.73it/s]Extractor Predicting: 9it [00:05,  1.80it/s]Extractor Predicting: 10it [00:05,  1.88it/s]Extractor Predicting: 11it [00:06,  1.86it/s]Extractor Predicting: 12it [00:06,  1.80it/s]Extractor Predicting: 13it [00:07,  1.83it/s]Extractor Predicting: 14it [00:07,  1.86it/s]Extractor Predicting: 15it [00:08,  1.84it/s]Extractor Predicting: 16it [00:08,  1.87it/s]Extractor Predicting: 17it [00:09,  1.86it/s]Extractor Predicting: 18it [00:10,  1.86it/s]Extractor Predicting: 19it [00:10,  1.92it/s]Extractor Predicting: 20it [00:11,  1.92it/s]Extractor Predicting: 21it [00:11,  1.93it/s]Extractor Predicting: 22it [00:12,  1.99it/s]Extractor Predicting: 23it [00:12,  1.98it/s]Extractor Predicting: 24it [00:13,  1.99it/s]Extractor Predicting: 25it [00:13,  1.94it/s]Extractor Predicting: 26it [00:14,  1.91it/s]Extractor Predicting: 27it [00:14,  1.92it/s]Extractor Predicting: 28it [00:15,  1.91it/s]Extractor Predicting: 29it [00:15,  1.91it/s]Extractor Predicting: 30it [00:16,  1.93it/s]Extractor Predicting: 31it [00:16,  1.91it/s]Extractor Predicting: 32it [00:17,  1.95it/s]Extractor Predicting: 33it [00:17,  1.92it/s]Extractor Predicting: 34it [00:18,  1.90it/s]Extractor Predicting: 35it [00:18,  1.90it/s]Extractor Predicting: 36it [00:19,  1.88it/s]Extractor Predicting: 37it [00:19,  1.91it/s]Extractor Predicting: 38it [00:20,  1.91it/s]Extractor Predicting: 39it [00:21,  1.74it/s]Extractor Predicting: 40it [00:21,  1.81it/s]Extractor Predicting: 41it [00:22,  1.80it/s]Extractor Predicting: 42it [00:22,  1.81it/s]Extractor Predicting: 43it [00:23,  1.81it/s]Extractor Predicting: 44it [00:23,  1.78it/s]Extractor Predicting: 45it [00:24,  1.83it/s]Extractor Predicting: 46it [00:24,  1.78it/s]Extractor Predicting: 47it [00:25,  1.80it/s]Extractor Predicting: 48it [00:26,  1.82it/s]Extractor Predicting: 49it [00:26,  1.81it/s]Extractor Predicting: 50it [00:27,  1.79it/s]Extractor Predicting: 51it [00:27,  1.80it/s]Extractor Predicting: 52it [00:28,  1.87it/s]Extractor Predicting: 53it [00:28,  1.85it/s]Extractor Predicting: 54it [00:29,  1.84it/s]Extractor Predicting: 55it [00:29,  1.77it/s]Extractor Predicting: 56it [00:30,  1.73it/s]Extractor Predicting: 57it [00:31,  1.75it/s]Extractor Predicting: 58it [00:31,  1.75it/s]Extractor Predicting: 59it [00:32,  1.79it/s]Extractor Predicting: 60it [00:32,  1.77it/s]Extractor Predicting: 61it [00:33,  1.76it/s]Extractor Predicting: 62it [00:33,  1.73it/s]Extractor Predicting: 63it [00:34,  1.75it/s]Extractor Predicting: 64it [00:35,  1.76it/s]Extractor Predicting: 65it [00:35,  1.74it/s]Extractor Predicting: 66it [00:36,  1.70it/s]Extractor Predicting: 67it [00:36,  1.73it/s]Extractor Predicting: 68it [00:37,  1.77it/s]Extractor Predicting: 69it [00:37,  1.75it/s]Extractor Predicting: 70it [00:38,  1.70it/s]Extractor Predicting: 71it [00:39,  1.73it/s]Extractor Predicting: 72it [00:39,  1.70it/s]Extractor Predicting: 73it [00:40,  1.73it/s]Extractor Predicting: 74it [00:40,  1.73it/s]Extractor Predicting: 75it [00:41,  1.75it/s]Extractor Predicting: 76it [00:41,  1.76it/s]Extractor Predicting: 77it [00:42,  1.75it/s]Extractor Predicting: 78it [00:43,  1.79it/s]Extractor Predicting: 79it [00:43,  1.80it/s]Extractor Predicting: 80it [00:44,  1.78it/s]Extractor Predicting: 81it [00:44,  1.79it/s]Extractor Predicting: 82it [00:45,  1.75it/s]Extractor Predicting: 83it [00:45,  1.77it/s]Extractor Predicting: 84it [00:46,  1.76it/s]Extractor Predicting: 85it [00:47,  1.75it/s]Extractor Predicting: 86it [00:47,  1.73it/s]Extractor Predicting: 87it [00:48,  1.72it/s]Extractor Predicting: 88it [00:48,  1.68it/s]Extractor Predicting: 89it [00:49,  1.69it/s]Extractor Predicting: 90it [00:50,  1.69it/s]Extractor Predicting: 91it [00:50,  1.69it/s]Extractor Predicting: 92it [00:51,  1.76it/s]Extractor Predicting: 93it [00:51,  1.84it/s]Extractor Predicting: 94it [00:52,  1.83it/s]Extractor Predicting: 95it [00:52,  1.83it/s]Extractor Predicting: 96it [00:53,  1.83it/s]Extractor Predicting: 97it [00:53,  1.86it/s]Extractor Predicting: 98it [00:54,  1.82it/s]Extractor Predicting: 99it [00:54,  1.73it/s]Extractor Predicting: 100it [00:55,  1.77it/s]Extractor Predicting: 101it [00:56,  1.68it/s]Extractor Predicting: 102it [00:56,  1.67it/s]Extractor Predicting: 103it [00:57,  1.70it/s]Extractor Predicting: 104it [00:57,  1.73it/s]Extractor Predicting: 105it [00:58,  1.75it/s]Extractor Predicting: 106it [00:58,  1.81it/s]Extractor Predicting: 107it [00:59,  1.81it/s]Extractor Predicting: 108it [01:00,  1.84it/s]Extractor Predicting: 109it [01:00,  1.83it/s]Extractor Predicting: 110it [01:01,  1.82it/s]Extractor Predicting: 111it [01:01,  1.86it/s]Extractor Predicting: 112it [01:02,  1.86it/s]Extractor Predicting: 113it [01:02,  1.81it/s]Extractor Predicting: 114it [01:03,  1.78it/s]Extractor Predicting: 115it [01:03,  1.80it/s]Extractor Predicting: 116it [01:04,  1.76it/s]Extractor Predicting: 117it [01:05,  1.73it/s]Extractor Predicting: 118it [01:05,  1.74it/s]Extractor Predicting: 119it [01:06,  1.71it/s]Extractor Predicting: 120it [01:06,  1.68it/s]Extractor Predicting: 121it [01:07,  1.68it/s]Extractor Predicting: 122it [01:08,  1.69it/s]Extractor Predicting: 123it [01:08,  1.73it/s]Extractor Predicting: 124it [01:09,  1.74it/s]Extractor Predicting: 125it [01:09,  1.75it/s]Extractor Predicting: 126it [01:10,  1.74it/s]Extractor Predicting: 127it [01:10,  1.73it/s]Extractor Predicting: 128it [01:11,  1.74it/s]Extractor Predicting: 129it [01:12,  1.77it/s]Extractor Predicting: 130it [01:12,  1.70it/s]Extractor Predicting: 131it [01:13,  1.54it/s]Extractor Predicting: 132it [01:14,  1.61it/s]Extractor Predicting: 133it [01:14,  1.60it/s]Extractor Predicting: 134it [01:15,  1.64it/s]Extractor Predicting: 135it [01:15,  1.64it/s]Extractor Predicting: 136it [01:16,  1.68it/s]Extractor Predicting: 137it [01:17,  1.66it/s]Extractor Predicting: 138it [01:17,  1.69it/s]Extractor Predicting: 139it [01:18,  1.67it/s]Extractor Predicting: 140it [01:18,  1.66it/s]Extractor Predicting: 141it [01:19,  1.69it/s]Extractor Predicting: 142it [01:19,  1.71it/s]Extractor Predicting: 143it [01:20,  1.69it/s]Extractor Predicting: 144it [01:21,  1.74it/s]Extractor Predicting: 145it [01:21,  1.78it/s]Extractor Predicting: 146it [01:22,  1.77it/s]Extractor Predicting: 147it [01:22,  1.73it/s]Extractor Predicting: 148it [01:23,  1.74it/s]Extractor Predicting: 149it [01:23,  1.73it/s]Extractor Predicting: 150it [01:24,  1.71it/s]Extractor Predicting: 151it [01:25,  1.70it/s]Extractor Predicting: 152it [01:25,  1.70it/s]Extractor Predicting: 153it [01:26,  1.71it/s]Extractor Predicting: 154it [01:26,  1.71it/s]Extractor Predicting: 155it [01:27,  1.71it/s]Extractor Predicting: 156it [01:28,  1.65it/s]Extractor Predicting: 157it [01:28,  1.60it/s]Extractor Predicting: 158it [01:29,  1.57it/s]Extractor Predicting: 159it [01:30,  1.61it/s]Extractor Predicting: 160it [01:30,  1.65it/s]Extractor Predicting: 161it [01:31,  1.67it/s]Extractor Predicting: 162it [01:31,  1.68it/s]Extractor Predicting: 163it [01:32,  1.68it/s]Extractor Predicting: 164it [01:32,  1.71it/s]Extractor Predicting: 165it [01:33,  1.71it/s]Extractor Predicting: 166it [01:34,  1.74it/s]Extractor Predicting: 167it [01:34,  1.73it/s]Extractor Predicting: 168it [01:35,  1.73it/s]Extractor Predicting: 169it [01:35,  1.75it/s]Extractor Predicting: 170it [01:36,  1.74it/s]Extractor Predicting: 171it [01:36,  1.76it/s]Extractor Predicting: 172it [01:37,  1.79it/s]Extractor Predicting: 173it [01:38,  1.73it/s]Extractor Predicting: 174it [01:38,  1.75it/s]Extractor Predicting: 175it [01:39,  1.72it/s]Extractor Predicting: 176it [01:39,  1.74it/s]Extractor Predicting: 177it [01:40,  1.71it/s]Extractor Predicting: 178it [01:41,  1.72it/s]Extractor Predicting: 179it [01:41,  1.72it/s]Extractor Predicting: 180it [01:42,  1.72it/s]Extractor Predicting: 181it [01:42,  1.72it/s]Extractor Predicting: 182it [01:43,  1.72it/s]Extractor Predicting: 183it [01:43,  1.67it/s]Extractor Predicting: 184it [01:44,  1.69it/s]Extractor Predicting: 185it [01:45,  1.82it/s]Extractor Predicting: 185it [01:45,  1.76it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:41,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:41,015 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:41,015 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:41,015 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:41,016 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:56:41,325 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:56:41,326 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:56:41,593 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:56:42,628 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:56:42,628 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:45,476 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:45,482 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:45,482 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:45,482 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:56:45,482 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:56:46,116 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:56:46,117 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:56:46,698 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:56:46,850 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:56:46,850 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.79it/s]Extractor Predicting: 6it [00:03,  1.79it/s]Extractor Predicting: 7it [00:03,  1.79it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.75it/s]Extractor Predicting: 11it [00:06,  1.79it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.60it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.75it/s]Extractor Predicting: 17it [00:09,  1.76it/s]Extractor Predicting: 18it [00:10,  1.72it/s]Extractor Predicting: 19it [00:10,  1.75it/s]Extractor Predicting: 20it [00:11,  1.73it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:12,  1.73it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.77it/s]Extractor Predicting: 26it [00:14,  1.77it/s]Extractor Predicting: 27it [00:15,  1.79it/s]Extractor Predicting: 28it [00:15,  1.80it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:17,  1.87it/s]Extractor Predicting: 31it [00:17,  1.93it/s]Extractor Predicting: 32it [00:18,  1.93it/s]Extractor Predicting: 33it [00:18,  1.92it/s]Extractor Predicting: 34it [00:19,  1.92it/s]Extractor Predicting: 35it [00:19,  1.93it/s]Extractor Predicting: 36it [00:20,  1.90it/s]Extractor Predicting: 37it [00:20,  1.93it/s]Extractor Predicting: 38it [00:21,  1.91it/s]Extractor Predicting: 39it [00:21,  1.84it/s]Extractor Predicting: 40it [00:22,  1.85it/s]Extractor Predicting: 41it [00:22,  1.87it/s]Extractor Predicting: 42it [00:23,  1.89it/s]Extractor Predicting: 43it [00:23,  1.89it/s]Extractor Predicting: 44it [00:24,  1.92it/s]Extractor Predicting: 45it [00:24,  1.93it/s]Extractor Predicting: 46it [00:25,  1.96it/s]Extractor Predicting: 47it [00:25,  1.95it/s]Extractor Predicting: 48it [00:26,  1.94it/s]Extractor Predicting: 49it [00:26,  1.92it/s]Extractor Predicting: 50it [00:27,  1.90it/s]Extractor Predicting: 51it [00:28,  1.87it/s]Extractor Predicting: 52it [00:28,  1.84it/s]Extractor Predicting: 53it [00:29,  1.84it/s]Extractor Predicting: 54it [00:29,  1.81it/s]Extractor Predicting: 55it [00:30,  1.80it/s]Extractor Predicting: 56it [00:30,  1.80it/s]Extractor Predicting: 57it [00:31,  1.85it/s]Extractor Predicting: 58it [00:31,  1.94it/s]Extractor Predicting: 59it [00:32,  1.90it/s]Extractor Predicting: 60it [00:32,  1.88it/s]Extractor Predicting: 61it [00:33,  1.81it/s]Extractor Predicting: 62it [00:34,  1.78it/s]Extractor Predicting: 63it [00:34,  1.73it/s]Extractor Predicting: 64it [00:35,  1.70it/s]Extractor Predicting: 65it [00:35,  1.69it/s]Extractor Predicting: 66it [00:36,  1.68it/s]Extractor Predicting: 67it [00:37,  1.67it/s]Extractor Predicting: 68it [00:37,  1.68it/s]Extractor Predicting: 69it [00:38,  1.69it/s]Extractor Predicting: 70it [00:38,  1.72it/s]Extractor Predicting: 71it [00:39,  1.73it/s]Extractor Predicting: 72it [00:39,  1.76it/s]Extractor Predicting: 73it [00:40,  1.62it/s]Extractor Predicting: 74it [00:41,  1.66it/s]Extractor Predicting: 75it [00:41,  1.71it/s]Extractor Predicting: 76it [00:42,  1.75it/s]Extractor Predicting: 77it [00:42,  1.79it/s]Extractor Predicting: 78it [00:43,  1.78it/s]Extractor Predicting: 79it [00:43,  1.85it/s]Extractor Predicting: 80it [00:44,  1.90it/s]Extractor Predicting: 81it [00:44,  1.85it/s]Extractor Predicting: 82it [00:45,  1.88it/s]Extractor Predicting: 83it [00:46,  1.82it/s]Extractor Predicting: 84it [00:46,  1.82it/s]Extractor Predicting: 85it [00:47,  1.78it/s]Extractor Predicting: 86it [00:47,  1.76it/s]Extractor Predicting: 87it [00:48,  1.75it/s]Extractor Predicting: 88it [00:48,  1.79it/s]Extractor Predicting: 89it [00:49,  1.78it/s]Extractor Predicting: 90it [00:50,  1.80it/s]Extractor Predicting: 91it [00:50,  1.78it/s]Extractor Predicting: 92it [00:51,  1.77it/s]Extractor Predicting: 93it [00:51,  1.80it/s]Extractor Predicting: 94it [00:52,  1.82it/s]Extractor Predicting: 95it [00:52,  1.81it/s]Extractor Predicting: 96it [00:53,  1.81it/s]Extractor Predicting: 97it [00:53,  1.82it/s]Extractor Predicting: 98it [00:54,  1.80it/s]Extractor Predicting: 99it [00:55,  1.79it/s]Extractor Predicting: 100it [00:55,  1.77it/s]Extractor Predicting: 101it [00:56,  1.79it/s]Extractor Predicting: 102it [00:56,  1.80it/s]Extractor Predicting: 103it [00:57,  1.75it/s]Extractor Predicting: 104it [00:57,  1.79it/s]Extractor Predicting: 105it [00:58,  1.81it/s]Extractor Predicting: 106it [00:58,  1.83it/s]Extractor Predicting: 107it [00:59,  1.80it/s]Extractor Predicting: 108it [01:00,  1.82it/s]Extractor Predicting: 109it [01:00,  1.81it/s]Extractor Predicting: 110it [01:01,  1.83it/s]Extractor Predicting: 111it [01:01,  1.83it/s]Extractor Predicting: 112it [01:02,  1.72it/s]Extractor Predicting: 113it [01:02,  1.73it/s]Extractor Predicting: 114it [01:03,  1.74it/s]Extractor Predicting: 115it [01:04,  1.75it/s]Extractor Predicting: 116it [01:04,  1.76it/s]Extractor Predicting: 117it [01:05,  1.80it/s]Extractor Predicting: 118it [01:05,  1.78it/s]Extractor Predicting: 119it [01:06,  1.77it/s]Extractor Predicting: 120it [01:06,  1.81it/s]Extractor Predicting: 121it [01:07,  1.85it/s]Extractor Predicting: 122it [01:07,  1.84it/s]Extractor Predicting: 123it [01:08,  1.80it/s]Extractor Predicting: 124it [01:09,  1.77it/s]Extractor Predicting: 125it [01:09,  1.78it/s]Extractor Predicting: 126it [01:10,  1.77it/s]Extractor Predicting: 127it [01:10,  1.81it/s]Extractor Predicting: 128it [01:11,  1.75it/s]Extractor Predicting: 129it [01:11,  1.77it/s]Extractor Predicting: 130it [01:12,  1.80it/s]Extractor Predicting: 131it [01:12,  1.77it/s]Extractor Predicting: 132it [01:13,  1.77it/s]Extractor Predicting: 133it [01:14,  1.77it/s]Extractor Predicting: 134it [01:14,  1.77it/s]Extractor Predicting: 135it [01:15,  1.78it/s]Extractor Predicting: 136it [01:15,  1.81it/s]Extractor Predicting: 137it [01:16,  1.75it/s]Extractor Predicting: 138it [01:16,  1.77it/s]Extractor Predicting: 139it [01:17,  1.80it/s]Extractor Predicting: 140it [01:17,  1.80it/s]Extractor Predicting: 141it [01:18,  1.79it/s]Extractor Predicting: 142it [01:19,  1.81it/s]Extractor Predicting: 143it [01:19,  1.69it/s]Extractor Predicting: 144it [01:20,  1.76it/s]Extractor Predicting: 145it [01:20,  1.74it/s]Extractor Predicting: 146it [01:21,  1.77it/s]Extractor Predicting: 147it [01:21,  1.83it/s]Extractor Predicting: 148it [01:22,  1.81it/s]Extractor Predicting: 149it [01:23,  1.85it/s]Extractor Predicting: 150it [01:23,  1.86it/s]Extractor Predicting: 151it [01:24,  1.89it/s]Extractor Predicting: 152it [01:24,  1.84it/s]Extractor Predicting: 153it [01:25,  1.85it/s]Extractor Predicting: 154it [01:25,  1.79it/s]Extractor Predicting: 155it [01:26,  1.79it/s]Extractor Predicting: 156it [01:26,  1.86it/s]Extractor Predicting: 157it [01:27,  1.81it/s]Extractor Predicting: 158it [01:27,  1.79it/s]Extractor Predicting: 159it [01:28,  1.81it/s]Extractor Predicting: 160it [01:29,  1.80it/s]Extractor Predicting: 161it [01:29,  1.84it/s]Extractor Predicting: 162it [01:30,  1.60it/s]Extractor Predicting: 163it [01:30,  1.65it/s]Extractor Predicting: 164it [01:31,  1.67it/s]Extractor Predicting: 165it [01:32,  1.65it/s]Extractor Predicting: 166it [01:32,  1.64it/s]Extractor Predicting: 167it [01:33,  1.65it/s]Extractor Predicting: 168it [01:33,  1.69it/s]Extractor Predicting: 169it [01:34,  1.74it/s]Extractor Predicting: 170it [01:35,  1.76it/s]Extractor Predicting: 171it [01:35,  1.78it/s]Extractor Predicting: 172it [01:36,  1.81it/s]Extractor Predicting: 173it [01:36,  1.81it/s]Extractor Predicting: 174it [01:37,  1.80it/s]Extractor Predicting: 175it [01:37,  1.79it/s]Extractor Predicting: 176it [01:38,  1.81it/s]Extractor Predicting: 177it [01:38,  1.78it/s]Extractor Predicting: 178it [01:39,  1.78it/s]Extractor Predicting: 179it [01:40,  1.74it/s]Extractor Predicting: 180it [01:40,  1.79it/s]Extractor Predicting: 181it [01:41,  1.79it/s]Extractor Predicting: 182it [01:41,  1.82it/s]Extractor Predicting: 183it [01:42,  1.85it/s]Extractor Predicting: 184it [01:42,  1.83it/s]Extractor Predicting: 185it [01:43,  1.84it/s]Extractor Predicting: 186it [01:43,  1.77it/s]Extractor Predicting: 187it [01:44,  1.80it/s]Extractor Predicting: 188it [01:45,  1.80it/s]Extractor Predicting: 189it [01:45,  1.80it/s]Extractor Predicting: 190it [01:46,  1.78it/s]Extractor Predicting: 191it [01:46,  1.79it/s]Extractor Predicting: 192it [01:47,  1.84it/s]Extractor Predicting: 193it [01:47,  1.83it/s]Extractor Predicting: 194it [01:48,  1.83it/s]Extractor Predicting: 195it [01:48,  1.82it/s]Extractor Predicting: 196it [01:49,  1.80it/s]Extractor Predicting: 197it [01:49,  1.80it/s]Extractor Predicting: 198it [01:50,  1.78it/s]Extractor Predicting: 199it [01:51,  1.78it/s]Extractor Predicting: 200it [01:51,  1.78it/s]Extractor Predicting: 201it [01:52,  1.80it/s]Extractor Predicting: 202it [01:52,  1.80it/s]Extractor Predicting: 203it [01:53,  1.83it/s]Extractor Predicting: 204it [01:53,  1.82it/s]Extractor Predicting: 205it [01:54,  1.83it/s]Extractor Predicting: 206it [01:54,  1.82it/s]Extractor Predicting: 207it [01:55,  1.84it/s]Extractor Predicting: 208it [01:56,  1.81it/s]Extractor Predicting: 209it [01:56,  1.78it/s]Extractor Predicting: 210it [01:57,  1.85it/s]Extractor Predicting: 211it [01:57,  1.82it/s]Extractor Predicting: 212it [01:58,  1.84it/s]Extractor Predicting: 213it [01:58,  1.84it/s]Extractor Predicting: 214it [01:59,  1.88it/s]Extractor Predicting: 215it [01:59,  1.86it/s]Extractor Predicting: 216it [02:00,  1.81it/s]Extractor Predicting: 217it [02:00,  1.83it/s]Extractor Predicting: 218it [02:01,  1.81it/s]Extractor Predicting: 219it [02:02,  1.81it/s]Extractor Predicting: 220it [02:02,  1.82it/s]Extractor Predicting: 221it [02:03,  1.83it/s]Extractor Predicting: 222it [02:03,  1.76it/s]Extractor Predicting: 223it [02:04,  1.70it/s]Extractor Predicting: 224it [02:04,  1.72it/s]Extractor Predicting: 225it [02:05,  1.76it/s]Extractor Predicting: 226it [02:06,  1.80it/s]Extractor Predicting: 227it [02:06,  1.81it/s]Extractor Predicting: 228it [02:07,  1.83it/s]Extractor Predicting: 229it [02:07,  1.86it/s]Extractor Predicting: 230it [02:08,  1.89it/s]Extractor Predicting: 231it [02:08,  1.89it/s]Extractor Predicting: 232it [02:09,  1.88it/s]Extractor Predicting: 233it [02:09,  1.86it/s]Extractor Predicting: 234it [02:10,  1.86it/s]Extractor Predicting: 235it [02:10,  1.83it/s]Extractor Predicting: 236it [02:11,  1.83it/s]Extractor Predicting: 237it [02:11,  1.84it/s]Extractor Predicting: 238it [02:12,  1.80it/s]Extractor Predicting: 239it [02:13,  1.83it/s]Extractor Predicting: 240it [02:13,  1.81it/s]Extractor Predicting: 241it [02:14,  1.83it/s]Extractor Predicting: 242it [02:14,  1.80it/s]Extractor Predicting: 243it [02:15,  1.75it/s]Extractor Predicting: 244it [02:15,  1.77it/s]Extractor Predicting: 245it [02:16,  1.81it/s]Extractor Predicting: 246it [02:16,  1.80it/s]Extractor Predicting: 247it [02:17,  1.84it/s]Extractor Predicting: 248it [02:18,  1.76it/s]Extractor Predicting: 249it [02:18,  1.75it/s]Extractor Predicting: 250it [02:19,  1.78it/s]Extractor Predicting: 251it [02:19,  1.78it/s]Extractor Predicting: 252it [02:20,  1.77it/s]Extractor Predicting: 253it [02:20,  1.73it/s]Extractor Predicting: 254it [02:21,  1.71it/s]Extractor Predicting: 255it [02:22,  1.73it/s]Extractor Predicting: 256it [02:22,  1.75it/s]Extractor Predicting: 257it [02:23,  1.76it/s]Extractor Predicting: 258it [02:23,  1.77it/s]Extractor Predicting: 259it [02:24,  1.76it/s]Extractor Predicting: 260it [02:24,  1.73it/s]Extractor Predicting: 261it [02:25,  1.76it/s]Extractor Predicting: 262it [02:26,  1.75it/s]Extractor Predicting: 263it [02:26,  1.76it/s]Extractor Predicting: 264it [02:27,  1.76it/s]Extractor Predicting: 265it [02:27,  1.79it/s]Extractor Predicting: 266it [02:28,  1.72it/s]Extractor Predicting: 267it [02:28,  1.73it/s]Extractor Predicting: 268it [02:29,  1.73it/s]Extractor Predicting: 269it [02:30,  1.75it/s]Extractor Predicting: 270it [02:30,  1.74it/s]Extractor Predicting: 271it [02:31,  1.73it/s]Extractor Predicting: 272it [02:31,  1.73it/s]Extractor Predicting: 273it [02:32,  1.75it/s]Extractor Predicting: 274it [02:33,  1.71it/s]Extractor Predicting: 275it [02:33,  1.72it/s]Extractor Predicting: 276it [02:34,  1.73it/s]Extractor Predicting: 277it [02:35,  1.52it/s]Extractor Predicting: 278it [02:35,  1.60it/s]Extractor Predicting: 279it [02:36,  1.63it/s]Extractor Predicting: 280it [02:36,  1.67it/s]Extractor Predicting: 281it [02:37,  1.64it/s]Extractor Predicting: 282it [02:37,  1.66it/s]Extractor Predicting: 283it [02:38,  1.68it/s]Extractor Predicting: 284it [02:39,  1.70it/s]Extractor Predicting: 285it [02:39,  1.70it/s]Extractor Predicting: 286it [02:40,  1.73it/s]Extractor Predicting: 287it [02:40,  1.67it/s]Extractor Predicting: 288it [02:41,  1.71it/s]Extractor Predicting: 289it [02:41,  1.73it/s]Extractor Predicting: 290it [02:42,  1.69it/s]Extractor Predicting: 291it [02:43,  1.66it/s]Extractor Predicting: 292it [02:43,  1.71it/s]Extractor Predicting: 293it [02:44,  1.71it/s]Extractor Predicting: 294it [02:44,  1.69it/s]Extractor Predicting: 295it [02:45,  1.70it/s]Extractor Predicting: 296it [02:46,  1.74it/s]Extractor Predicting: 297it [02:46,  1.74it/s]Extractor Predicting: 298it [02:47,  1.72it/s]Extractor Predicting: 299it [02:47,  1.73it/s]Extractor Predicting: 300it [02:48,  1.69it/s]Extractor Predicting: 301it [02:49,  1.72it/s]Extractor Predicting: 302it [02:49,  1.69it/s]Extractor Predicting: 303it [02:50,  1.73it/s]Extractor Predicting: 304it [02:50,  1.75it/s]Extractor Predicting: 305it [02:51,  1.79it/s]Extractor Predicting: 306it [02:51,  1.82it/s]Extractor Predicting: 307it [02:52,  1.79it/s]Extractor Predicting: 308it [02:52,  1.81it/s]Extractor Predicting: 309it [02:53,  1.77it/s]Extractor Predicting: 310it [02:54,  1.77it/s]Extractor Predicting: 311it [02:54,  1.72it/s]Extractor Predicting: 312it [02:55,  1.74it/s]Extractor Predicting: 313it [02:55,  1.67it/s]Extractor Predicting: 314it [02:56,  1.67it/s]Extractor Predicting: 315it [02:57,  1.71it/s]Extractor Predicting: 316it [02:57,  1.76it/s]Extractor Predicting: 317it [02:58,  1.77it/s]Extractor Predicting: 318it [02:58,  1.81it/s]Extractor Predicting: 319it [02:59,  1.77it/s]Extractor Predicting: 320it [02:59,  1.73it/s]Extractor Predicting: 321it [03:00,  1.73it/s]Extractor Predicting: 322it [03:01,  1.75it/s]Extractor Predicting: 323it [03:01,  1.70it/s]Extractor Predicting: 324it [03:02,  1.71it/s]Extractor Predicting: 325it [03:02,  1.75it/s]Extractor Predicting: 326it [03:03,  1.74it/s]Extractor Predicting: 327it [03:03,  1.76it/s]Extractor Predicting: 328it [03:04,  1.72it/s]Extractor Predicting: 329it [03:05,  1.74it/s]Extractor Predicting: 330it [03:05,  1.71it/s]Extractor Predicting: 331it [03:06,  1.73it/s]Extractor Predicting: 332it [03:06,  1.73it/s]Extractor Predicting: 333it [03:07,  1.71it/s]Extractor Predicting: 334it [03:07,  1.73it/s]Extractor Predicting: 335it [03:08,  1.72it/s]Extractor Predicting: 336it [03:09,  1.62it/s]Extractor Predicting: 337it [03:09,  1.63it/s]Extractor Predicting: 338it [03:10,  1.64it/s]Extractor Predicting: 339it [03:11,  1.67it/s]Extractor Predicting: 340it [03:11,  1.68it/s]Extractor Predicting: 341it [03:12,  1.66it/s]Extractor Predicting: 342it [03:12,  1.67it/s]Extractor Predicting: 343it [03:13,  1.71it/s]Extractor Predicting: 344it [03:13,  1.71it/s]Extractor Predicting: 345it [03:14,  1.66it/s]Extractor Predicting: 346it [03:15,  1.67it/s]Extractor Predicting: 347it [03:15,  1.68it/s]Extractor Predicting: 348it [03:16,  1.71it/s]Extractor Predicting: 349it [03:16,  1.72it/s]Extractor Predicting: 350it [03:17,  1.71it/s]Extractor Predicting: 351it [03:18,  1.74it/s]Extractor Predicting: 352it [03:18,  1.69it/s]Extractor Predicting: 353it [03:19,  1.66it/s]Extractor Predicting: 354it [03:19,  1.70it/s]Extractor Predicting: 355it [03:20,  1.74it/s]Extractor Predicting: 356it [03:21,  1.73it/s]Extractor Predicting: 357it [03:21,  1.74it/s]Extractor Predicting: 358it [03:22,  1.75it/s]Extractor Predicting: 359it [03:22,  1.74it/s]Extractor Predicting: 360it [03:23,  1.69it/s]Extractor Predicting: 361it [03:23,  1.66it/s]Extractor Predicting: 362it [03:24,  1.68it/s]Extractor Predicting: 363it [03:25,  1.73it/s]Extractor Predicting: 364it [03:25,  1.75it/s]Extractor Predicting: 365it [03:26,  1.71it/s]Extractor Predicting: 366it [03:26,  1.67it/s]Extractor Predicting: 367it [03:27,  1.67it/s]Extractor Predicting: 368it [03:28,  1.65it/s]Extractor Predicting: 369it [03:28,  1.60it/s]Extractor Predicting: 370it [03:29,  1.61it/s]Extractor Predicting: 371it [03:29,  1.65it/s]Extractor Predicting: 372it [03:30,  1.98it/s]Extractor Predicting: 372it [03:30,  1.77it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:28,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:28,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:28,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:28,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:28,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:00:28,834 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:00:28,835 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:00:29,397 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:00:30,442 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:00:30,442 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:33,336 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:33,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:33,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:33,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:00:33,341 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:00:33,957 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:00:33,959 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:00:34,526 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:00:34,693 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:00:34,693 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.75it/s]Extractor Predicting: 6it [00:03,  1.81it/s]Extractor Predicting: 7it [00:04,  1.78it/s]Extractor Predicting: 8it [00:04,  1.79it/s]Extractor Predicting: 9it [00:05,  1.80it/s]Extractor Predicting: 10it [00:05,  1.82it/s]Extractor Predicting: 11it [00:06,  1.82it/s]Extractor Predicting: 12it [00:06,  1.86it/s]Extractor Predicting: 13it [00:07,  1.86it/s]Extractor Predicting: 14it [00:07,  1.84it/s]Extractor Predicting: 15it [00:08,  1.76it/s]Extractor Predicting: 16it [00:08,  1.78it/s]Extractor Predicting: 17it [00:09,  1.77it/s]Extractor Predicting: 18it [00:10,  1.80it/s]Extractor Predicting: 19it [00:10,  1.81it/s]Extractor Predicting: 20it [00:11,  1.81it/s]Extractor Predicting: 21it [00:11,  1.77it/s]Extractor Predicting: 22it [00:12,  1.75it/s]Extractor Predicting: 23it [00:12,  1.74it/s]Extractor Predicting: 24it [00:13,  1.70it/s]Extractor Predicting: 25it [00:14,  1.74it/s]Extractor Predicting: 26it [00:14,  1.72it/s]Extractor Predicting: 27it [00:15,  1.71it/s]Extractor Predicting: 28it [00:15,  1.70it/s]Extractor Predicting: 29it [00:16,  1.73it/s]Extractor Predicting: 30it [00:17,  1.74it/s]Extractor Predicting: 31it [00:17,  1.77it/s]Extractor Predicting: 32it [00:18,  1.74it/s]Extractor Predicting: 33it [00:18,  1.77it/s]Extractor Predicting: 34it [00:19,  1.78it/s]Extractor Predicting: 35it [00:19,  1.73it/s]Extractor Predicting: 36it [00:20,  1.75it/s]Extractor Predicting: 37it [00:21,  1.72it/s]Extractor Predicting: 38it [00:21,  1.66it/s]Extractor Predicting: 39it [00:22,  1.62it/s]Extractor Predicting: 40it [00:22,  1.61it/s]Extractor Predicting: 41it [00:23,  1.49it/s]Extractor Predicting: 42it [00:24,  1.55it/s]Extractor Predicting: 43it [00:24,  1.58it/s]Extractor Predicting: 44it [00:25,  1.61it/s]Extractor Predicting: 45it [00:26,  1.59it/s]Extractor Predicting: 46it [00:26,  1.64it/s]Extractor Predicting: 47it [00:27,  1.62it/s]Extractor Predicting: 48it [00:27,  1.63it/s]Extractor Predicting: 49it [00:28,  1.63it/s]Extractor Predicting: 50it [00:29,  1.63it/s]Extractor Predicting: 51it [00:29,  1.60it/s]Extractor Predicting: 52it [00:30,  1.60it/s]Extractor Predicting: 53it [00:31,  1.63it/s]Extractor Predicting: 54it [00:31,  1.61it/s]Extractor Predicting: 55it [00:32,  1.66it/s]Extractor Predicting: 56it [00:32,  1.68it/s]Extractor Predicting: 57it [00:33,  1.67it/s]Extractor Predicting: 58it [00:34,  1.65it/s]Extractor Predicting: 59it [00:34,  1.64it/s]Extractor Predicting: 60it [00:35,  1.64it/s]Extractor Predicting: 61it [00:35,  1.59it/s]Extractor Predicting: 62it [00:36,  1.59it/s]Extractor Predicting: 63it [00:37,  1.74it/s]Extractor Predicting: 63it [00:37,  1.70it/s]
[INFO|configuration_utils.py:515] 2023-08-28 07:01:12,726 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:01:12,727 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:01:12,735 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:01:12,736 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 07:01:12,738 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:01:16,277 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 07:01:16,279 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 07:01:16,290 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:01:16,291 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:01:16,296 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:16,301 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:16,302 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:16,302 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:16,302 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:16,302 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:01:16,302 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 07:01:16,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:17,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:17,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:18,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:18,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:19,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:20,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:20,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:21,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:21,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:22,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:23,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:23,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:24,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:24,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:25,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:26,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:26,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:27,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:27,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:28,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:29,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:29,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:30,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:31,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:31,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:32,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:49, 16.41s/it][WARNING|generation_utils.py:914] 2023-08-28 07:01:32,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:33,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:34,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:34,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:35,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:35,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:36,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:36,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:37,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:38,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:38,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:39,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:39,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:40,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:40,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:41,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:41,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:42,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:43,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:43,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:44,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:44,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:45,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:46,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:46,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:19, 15.37s/it][WARNING|generation_utils.py:914] 2023-08-28 07:01:47,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:48,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:48,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:49,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:49,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:50,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:50,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:51,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:51,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:52,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:53,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:53,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:54,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:54,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:55,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:55,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:56,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:56,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:57,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:57,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:58,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:41, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-28 07:01:58,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:01:59,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:00,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:00,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:01,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:02,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:02,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:03,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:03,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:04,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:05,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:05,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:06,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:06,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:07,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:08,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:08,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:09,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:09,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:10,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:11,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:11,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:12,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:12,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:56<02:33, 13.95s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:13,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:14,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:14,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:15,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:15,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:16,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:17,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:17,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:18,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:18,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:19,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:20,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:20,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:21,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:22,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:22,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:23,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:23,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:24,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:25,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:25,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:26,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:26,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:27,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:28,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:24, 14.42s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:28,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:29,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:29,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:30,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:31,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:31,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:32,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:33,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:33,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:34,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:35,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:35,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:36,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:37,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:37,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:38,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:38,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:39,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:40,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:40,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:41,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:42,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:26<02:09, 14.36s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:42,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:43,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:44,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:44,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:45,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:46,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:46,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:47,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:47,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:48,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:49,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:49,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:50,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:50,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:51,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:51,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:52,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:53,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:53,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:54,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:54,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:55,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:39<01:52, 14.02s/it][WARNING|generation_utils.py:914] 2023-08-28 07:02:56,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:56,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:57,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:57,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:58,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:59,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:02:59,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:00,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:01,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:01,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:02,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:02,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:03,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:04,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:04,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:05,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:05,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:06,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:07,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:07,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:08,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:08,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:52<01:35, 13.71s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:09,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:09,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:10,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:11,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:11,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:12,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:13,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:13,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:14,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:14,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:15,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:15,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:16,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:17,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:17,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:18,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:18,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:19,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:20,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:20,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:21,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:21,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:22,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:22,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:23,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:24,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:08<01:25, 14.25s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:24,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:25,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:25,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:26,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:27,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:27,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:28,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:28,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:29,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:30,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:30,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:31,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:31,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:32,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:33,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:33,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:34,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:35,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:35,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:36,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:36,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:37,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:38,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:22<01:10, 14.13s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:38,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:39,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:39,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:40,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:40,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:41,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:41,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:42,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:42,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:43,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:43,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:44,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:44,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:45,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:45,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:46,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:47,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:47,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:48,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:48,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:49,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:49,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:50,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:34<00:53, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-28 07:03:50,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:51,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:51,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:52,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:52,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:53,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:54,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:54,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:55,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:56,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:56,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:57,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:57,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:58,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:03:59,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:00,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:00,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:01,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:01,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:02,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:03,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:03,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:04,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:04,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:05,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:06,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:06,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:07,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:51<00:43, 14.63s/it][WARNING|generation_utils.py:914] 2023-08-28 07:04:07,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:08,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:09,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:09,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:10,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:11,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:11,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:12,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:12,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:13,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:14,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:14,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:15,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:15,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:16,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:17,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:17,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:18,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:18,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:19,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:20,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:20,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:21,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:22,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:06<00:29, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-28 07:04:22,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:23,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:23,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:24,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:24,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:25,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:25,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:26,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:26,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:27,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:27,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:28,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:29,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:29,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:30,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:30,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:31,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:31,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:32,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:33,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:33,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:34,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:34,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:18<00:14, 14.03s/it][WARNING|generation_utils.py:914] 2023-08-28 07:04:35,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:35,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:36,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:37,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:37,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:38,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:39,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:39,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:40,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:41,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:41,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:42,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:43,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:43,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:44,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:45,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:45,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:46,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:47,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:47,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:48,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:49,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:04:49,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:33<00:00, 14.36s/it]Generating: 100%|██████████| 15/15 [03:33<00:00, 14.25s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:57,065 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:57,070 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:57,070 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:57,070 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:57,070 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:04:57,821 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:04:57,822 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:04:58,395 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:04:59,461 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:04:59,461 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:05:02,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:05:02,472 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:05:02,472 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:05:02,472 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:05:02,472 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:05:03,122 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:05:03,123 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:05:03,693 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:05:03,865 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:05:03,865 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 469, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : developer .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 626, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.7825, 'errors': {'', "('London', 'work location', '', 'The New York Times ( July 18 , 1923 October 6 , 1977 ) reported that the first and only English book on the topic appeared in London during the reigns of William Shakespeare and William Shakespeare .')"}}
['Relation : composer . Context : Later in the year ( 1143 ) , he composed The Seven Kingdoms , the first play in the epic of the Old Kingdom . Head Entity : The Seven Kingdoms , Tail Entity : Robert I .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best Rap Album at the 2004 MTV Video Music Awards and had two acts on the chart . Head Entity : music , Tail Entity : Rap Album .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 590, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7319711538461539, 'errors': {'', "('James S. Cain', 'field of work', '', 'He is also known for his work with James S. Cain , and for his work on the painting of the Seven Wonders of Oz by William S. Burroughs .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 194, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 298, 'raw': 448}
{'target': 600, 'success': 322, 'raw': 480}
{'target': 600, 'success': 346, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 387, 'raw': 576}
{'target': 600, 'success': 408, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 518, 'raw': 768}
{'target': 600, 'success': 543, 'raw': 800}
{'target': 600, 'success': 563, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 606, 'raw': 896}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6763392857142857, 'errors': {'', "('John Lennon', 'occupation', '', 'The band released their debut album In Search of a Way ( 2000 ) , which featured a cover from John Lennon and Steve Dizzy .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 13757
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13857, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.42it/s]Extractor Estimating: 2it [00:01,  1.42it/s]Extractor Estimating: 3it [00:02,  1.52it/s]Extractor Estimating: 4it [00:02,  1.64it/s]Extractor Estimating: 5it [00:03,  1.67it/s]Extractor Estimating: 6it [00:03,  1.70it/s]Extractor Estimating: 7it [00:04,  1.68it/s]Extractor Estimating: 8it [00:04,  1.67it/s]Extractor Estimating: 9it [00:05,  1.63it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.60it/s]Extractor Estimating: 12it [00:07,  1.61it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.65it/s]Extractor Estimating: 15it [00:09,  1.58it/s]Extractor Estimating: 16it [00:09,  1.62it/s]Extractor Estimating: 17it [00:10,  1.60it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:11,  1.61it/s]Extractor Estimating: 20it [00:12,  1.62it/s]Extractor Estimating: 21it [00:12,  1.68it/s]Extractor Estimating: 22it [00:13,  1.73it/s]Extractor Estimating: 23it [00:14,  1.74it/s]Extractor Estimating: 24it [00:14,  1.69it/s]Extractor Estimating: 25it [00:15,  1.73it/s]Extractor Estimating: 26it [00:15,  1.79it/s]Extractor Estimating: 27it [00:16,  1.74it/s]Extractor Estimating: 28it [00:16,  1.79it/s]Extractor Estimating: 29it [00:17,  1.77it/s]Extractor Estimating: 30it [00:18,  1.66it/s]Extractor Estimating: 31it [00:18,  1.65it/s]Extractor Estimating: 32it [00:19,  1.74it/s]Extractor Estimating: 33it [00:19,  1.73it/s]Extractor Estimating: 34it [00:20,  1.74it/s]Extractor Estimating: 35it [00:21,  1.71it/s]Extractor Estimating: 36it [00:21,  1.71it/s]Extractor Estimating: 37it [00:22,  1.72it/s]Extractor Estimating: 38it [00:22,  1.67it/s]Extractor Estimating: 39it [00:23,  1.70it/s]Extractor Estimating: 40it [00:23,  1.72it/s]Extractor Estimating: 41it [00:24,  1.69it/s]Extractor Estimating: 42it [00:25,  1.72it/s]Extractor Estimating: 43it [00:25,  1.75it/s]Extractor Estimating: 44it [00:26,  1.74it/s]Extractor Estimating: 45it [00:26,  1.77it/s]Extractor Estimating: 46it [00:27,  1.76it/s]Extractor Estimating: 47it [00:27,  1.76it/s]Extractor Estimating: 48it [00:28,  1.74it/s]Extractor Estimating: 49it [00:29,  1.70it/s]Extractor Estimating: 50it [00:29,  1.77it/s]Extractor Estimating: 51it [00:30,  1.80it/s]Extractor Estimating: 52it [00:30,  1.85it/s]Extractor Estimating: 53it [00:31,  1.87it/s]Extractor Estimating: 54it [00:31,  1.90it/s]Extractor Estimating: 55it [00:32,  1.91it/s]Extractor Estimating: 56it [00:32,  2.00it/s]Extractor Estimating: 57it [00:33,  1.98it/s]Extractor Estimating: 58it [00:33,  1.99it/s]Extractor Estimating: 59it [00:34,  2.05it/s]Extractor Estimating: 60it [00:34,  1.98it/s]Extractor Estimating: 61it [00:35,  1.95it/s]Extractor Estimating: 62it [00:35,  1.99it/s]Extractor Estimating: 63it [00:36,  2.04it/s]Extractor Estimating: 64it [00:36,  1.98it/s]Extractor Estimating: 65it [00:37,  1.93it/s]Extractor Estimating: 66it [00:37,  1.99it/s]Extractor Estimating: 67it [00:38,  1.98it/s]Extractor Estimating: 68it [00:38,  1.95it/s]Extractor Estimating: 69it [00:39,  1.99it/s]Extractor Estimating: 70it [00:39,  1.98it/s]Extractor Estimating: 71it [00:40,  2.02it/s]Extractor Estimating: 72it [00:40,  2.00it/s]Extractor Estimating: 73it [00:41,  2.02it/s]Extractor Estimating: 74it [00:41,  2.02it/s]Extractor Estimating: 75it [00:42,  2.00it/s]Extractor Estimating: 76it [00:42,  1.92it/s]Extractor Estimating: 77it [00:43,  1.77it/s]Extractor Estimating: 78it [00:44,  1.75it/s]Extractor Estimating: 79it [00:44,  1.72it/s]Extractor Estimating: 80it [00:45,  1.62it/s]Extractor Estimating: 81it [00:45,  1.61it/s]Extractor Estimating: 82it [00:46,  1.56it/s]Extractor Estimating: 83it [00:47,  1.58it/s]Extractor Estimating: 84it [00:47,  1.55it/s]Extractor Estimating: 85it [00:48,  1.42it/s]Extractor Estimating: 86it [00:49,  1.47it/s]Extractor Estimating: 87it [00:50,  1.48it/s]Extractor Estimating: 88it [00:50,  1.52it/s]Extractor Estimating: 89it [00:51,  1.58it/s]Extractor Estimating: 90it [00:51,  1.62it/s]Extractor Estimating: 91it [00:52,  1.62it/s]Extractor Estimating: 92it [00:53,  1.65it/s]Extractor Estimating: 93it [00:53,  1.67it/s]Extractor Estimating: 94it [00:54,  1.68it/s]Extractor Estimating: 95it [00:54,  1.63it/s]Extractor Estimating: 96it [00:55,  1.66it/s]Extractor Estimating: 97it [00:56,  1.64it/s]Extractor Estimating: 98it [00:56,  1.65it/s]Extractor Estimating: 99it [00:57,  1.65it/s]Extractor Estimating: 100it [00:57,  1.63it/s]Extractor Estimating: 101it [00:58,  1.62it/s]Extractor Estimating: 102it [00:59,  1.63it/s]Extractor Estimating: 103it [00:59,  1.65it/s]Extractor Estimating: 104it [01:00,  1.68it/s]Extractor Estimating: 105it [01:00,  1.68it/s]Extractor Estimating: 106it [01:01,  1.66it/s]Extractor Estimating: 107it [01:02,  1.64it/s]Extractor Estimating: 108it [01:02,  1.65it/s]Extractor Estimating: 109it [01:03,  1.60it/s]Extractor Estimating: 110it [01:04,  1.61it/s]Extractor Estimating: 111it [01:04,  1.58it/s]Extractor Estimating: 112it [01:05,  1.51it/s]Extractor Estimating: 113it [01:06,  1.54it/s]Extractor Estimating: 114it [01:06,  1.54it/s]Extractor Estimating: 115it [01:07,  1.52it/s]Extractor Estimating: 116it [01:07,  1.53it/s]Extractor Estimating: 117it [01:08,  1.54it/s]Extractor Estimating: 118it [01:09,  1.61it/s]Extractor Estimating: 119it [01:09,  1.61it/s]Extractor Estimating: 120it [01:10,  1.58it/s]Extractor Estimating: 121it [01:11,  1.60it/s]Extractor Estimating: 122it [01:11,  1.59it/s]Extractor Estimating: 123it [01:12,  1.59it/s]Extractor Estimating: 124it [01:12,  1.61it/s]Extractor Estimating: 125it [01:13,  1.65it/s]Extractor Estimating: 126it [01:14,  1.70it/s]Extractor Estimating: 127it [01:14,  1.63it/s]Extractor Estimating: 128it [01:15,  1.65it/s]Extractor Estimating: 129it [01:15,  1.64it/s]Extractor Estimating: 130it [01:16,  1.65it/s]Extractor Estimating: 131it [01:17,  1.63it/s]Extractor Estimating: 132it [01:17,  1.58it/s]Extractor Estimating: 133it [01:18,  1.56it/s]Extractor Estimating: 134it [01:19,  1.58it/s]Extractor Estimating: 135it [01:19,  1.61it/s]Extractor Estimating: 136it [01:20,  1.60it/s]Extractor Estimating: 137it [01:21,  1.50it/s]Extractor Estimating: 138it [01:21,  1.53it/s]Extractor Estimating: 139it [01:22,  1.53it/s]Extractor Estimating: 140it [01:22,  1.56it/s]Extractor Estimating: 141it [01:23,  1.60it/s]Extractor Estimating: 142it [01:24,  1.64it/s]Extractor Estimating: 143it [01:24,  1.68it/s]Extractor Estimating: 144it [01:25,  1.67it/s]Extractor Estimating: 145it [01:25,  1.70it/s]Extractor Estimating: 146it [01:26,  1.63it/s]Extractor Estimating: 147it [01:27,  1.63it/s]Extractor Estimating: 148it [01:27,  1.59it/s]Extractor Estimating: 149it [01:28,  1.65it/s]Extractor Estimating: 150it [01:29,  1.64it/s]Extractor Estimating: 151it [01:29,  1.65it/s]Extractor Estimating: 152it [01:30,  1.68it/s]Extractor Estimating: 153it [01:30,  1.71it/s]Extractor Estimating: 154it [01:31,  1.73it/s]Extractor Estimating: 155it [01:31,  1.73it/s]Extractor Estimating: 156it [01:32,  1.69it/s]Extractor Estimating: 157it [01:33,  1.68it/s]Extractor Estimating: 158it [01:33,  1.67it/s]Extractor Estimating: 159it [01:34,  1.67it/s]Extractor Estimating: 160it [01:34,  1.72it/s]Extractor Estimating: 161it [01:35,  1.61it/s]Extractor Estimating: 162it [01:36,  1.61it/s]Extractor Estimating: 163it [01:36,  1.67it/s]Extractor Estimating: 164it [01:37,  1.70it/s]Extractor Estimating: 165it [01:37,  1.74it/s]Extractor Estimating: 166it [01:38,  1.75it/s]Extractor Estimating: 167it [01:39,  1.62it/s]Extractor Estimating: 168it [01:39,  1.67it/s]Extractor Estimating: 169it [01:40,  1.72it/s]Extractor Estimating: 170it [01:40,  1.65it/s]Extractor Estimating: 171it [01:41,  1.66it/s]Extractor Estimating: 172it [01:42,  1.70it/s]Extractor Estimating: 173it [01:42,  1.72it/s]Extractor Estimating: 174it [01:43,  1.68it/s]Extractor Estimating: 175it [01:43,  1.63it/s]Extractor Estimating: 176it [01:44,  1.61it/s]Extractor Estimating: 177it [01:45,  1.64it/s]Extractor Estimating: 178it [01:45,  1.65it/s]Extractor Estimating: 179it [01:46,  1.64it/s]Extractor Estimating: 180it [01:46,  1.62it/s]Extractor Estimating: 181it [01:47,  1.63it/s]Extractor Estimating: 182it [01:48,  1.64it/s]Extractor Estimating: 183it [01:48,  1.64it/s]Extractor Estimating: 184it [01:49,  1.66it/s]Extractor Estimating: 185it [01:49,  1.64it/s]Extractor Estimating: 186it [01:50,  1.63it/s]Extractor Estimating: 187it [01:51,  1.68it/s]Extractor Estimating: 188it [01:51,  1.66it/s]Extractor Estimating: 189it [01:52,  1.65it/s]Extractor Estimating: 190it [01:53,  1.62it/s]Extractor Estimating: 191it [01:53,  1.59it/s]Extractor Estimating: 192it [01:54,  1.61it/s]Extractor Estimating: 193it [01:54,  1.58it/s]Extractor Estimating: 194it [01:55,  1.61it/s]Extractor Estimating: 195it [01:56,  1.63it/s]Extractor Estimating: 196it [01:56,  1.59it/s]Extractor Estimating: 197it [01:57,  1.60it/s]Extractor Estimating: 198it [01:58,  1.60it/s]Extractor Estimating: 199it [01:58,  1.63it/s]Extractor Estimating: 200it [01:59,  1.62it/s]Extractor Estimating: 201it [01:59,  1.61it/s]Extractor Estimating: 202it [02:00,  1.65it/s]Extractor Estimating: 203it [02:01,  1.61it/s]Extractor Estimating: 204it [02:01,  1.63it/s]Extractor Estimating: 205it [02:02,  1.57it/s]Extractor Estimating: 206it [02:02,  1.62it/s]Extractor Estimating: 207it [02:03,  1.65it/s]Extractor Estimating: 208it [02:04,  1.66it/s]Extractor Estimating: 209it [02:04,  1.67it/s]Extractor Estimating: 210it [02:05,  1.65it/s]Extractor Estimating: 211it [02:05,  1.68it/s]Extractor Estimating: 212it [02:06,  1.68it/s]Extractor Estimating: 213it [02:07,  1.70it/s]Extractor Estimating: 214it [02:07,  1.67it/s]Extractor Estimating: 215it [02:08,  1.64it/s]Extractor Estimating: 216it [02:09,  1.60it/s]Extractor Estimating: 217it [02:09,  1.62it/s]Extractor Estimating: 218it [02:10,  1.63it/s]Extractor Estimating: 219it [02:10,  1.69it/s]Extractor Estimating: 220it [02:11,  1.69it/s]Extractor Estimating: 221it [02:11,  1.68it/s]Extractor Estimating: 222it [02:12,  1.67it/s]Extractor Estimating: 223it [02:13,  1.68it/s]Extractor Estimating: 224it [02:13,  1.65it/s]Extractor Estimating: 225it [02:14,  1.64it/s]Extractor Estimating: 226it [02:15,  1.62it/s]Extractor Estimating: 227it [02:15,  1.55it/s]Extractor Estimating: 228it [02:16,  1.54it/s]Extractor Estimating: 229it [02:17,  1.56it/s]Extractor Estimating: 230it [02:17,  1.58it/s]Extractor Estimating: 231it [02:18,  1.60it/s]Extractor Estimating: 232it [02:18,  1.63it/s]Extractor Estimating: 233it [02:19,  1.66it/s]Extractor Estimating: 234it [02:20,  1.62it/s]Extractor Estimating: 235it [02:20,  1.57it/s]Extractor Estimating: 236it [02:21,  1.57it/s]Extractor Estimating: 237it [02:22,  1.58it/s]Extractor Estimating: 238it [02:22,  1.61it/s]Extractor Estimating: 239it [02:23,  1.64it/s]Extractor Estimating: 240it [02:23,  1.63it/s]Extractor Estimating: 241it [02:24,  1.60it/s]Extractor Estimating: 242it [02:25,  1.56it/s]Extractor Estimating: 243it [02:25,  1.56it/s]Extractor Estimating: 244it [02:26,  1.59it/s]Extractor Estimating: 245it [02:27,  1.57it/s]Extractor Estimating: 246it [02:27,  1.43it/s]Extractor Estimating: 247it [02:28,  1.46it/s]Extractor Estimating: 248it [02:29,  1.52it/s]Extractor Estimating: 249it [02:29,  1.56it/s]Extractor Estimating: 250it [02:30,  1.53it/s]Extractor Estimating: 251it [02:31,  1.56it/s]Extractor Estimating: 252it [02:31,  1.54it/s]Extractor Estimating: 253it [02:32,  1.56it/s]Extractor Estimating: 254it [02:32,  1.58it/s]Extractor Estimating: 255it [02:33,  1.61it/s]Extractor Estimating: 256it [02:34,  1.61it/s]Extractor Estimating: 257it [02:34,  1.65it/s]Extractor Estimating: 258it [02:35,  1.69it/s]Extractor Estimating: 259it [02:35,  1.66it/s]Extractor Estimating: 260it [02:36,  1.70it/s]Extractor Estimating: 261it [02:37,  1.66it/s]Extractor Estimating: 262it [02:37,  1.64it/s]Extractor Estimating: 263it [02:38,  1.66it/s]Extractor Estimating: 264it [02:38,  1.69it/s]Extractor Estimating: 265it [02:39,  1.68it/s]Extractor Estimating: 266it [02:40,  1.70it/s]Extractor Estimating: 267it [02:40,  1.69it/s]Extractor Estimating: 268it [02:41,  1.62it/s]Extractor Estimating: 269it [02:41,  1.65it/s]Extractor Estimating: 270it [02:42,  1.64it/s]Extractor Estimating: 271it [02:43,  1.61it/s]Extractor Estimating: 272it [02:43,  1.57it/s]Extractor Estimating: 273it [02:44,  1.62it/s]Extractor Estimating: 274it [02:45,  1.63it/s]Extractor Estimating: 275it [02:45,  1.65it/s]Extractor Estimating: 276it [02:46,  1.60it/s]Extractor Estimating: 277it [02:46,  1.62it/s]Extractor Estimating: 278it [02:47,  1.65it/s]Extractor Estimating: 279it [02:48,  1.63it/s]Extractor Estimating: 280it [02:48,  1.64it/s]Extractor Estimating: 281it [02:49,  1.64it/s]Extractor Estimating: 282it [02:49,  1.66it/s]Extractor Estimating: 283it [02:50,  1.60it/s]Extractor Estimating: 284it [02:51,  1.60it/s]Extractor Estimating: 285it [02:51,  1.60it/s]Extractor Estimating: 286it [02:52,  1.60it/s]Extractor Estimating: 287it [02:53,  1.57it/s]Extractor Estimating: 288it [02:53,  1.62it/s]Extractor Estimating: 289it [02:54,  1.60it/s]Extractor Estimating: 290it [02:54,  1.65it/s]Extractor Estimating: 291it [02:55,  1.65it/s]Extractor Estimating: 292it [02:56,  1.61it/s]Extractor Estimating: 293it [02:56,  1.59it/s]Extractor Estimating: 294it [02:57,  1.63it/s]Extractor Estimating: 295it [02:57,  1.66it/s]Extractor Estimating: 296it [02:58,  1.65it/s]Extractor Estimating: 297it [02:59,  1.68it/s]Extractor Estimating: 298it [02:59,  1.73it/s]Extractor Estimating: 299it [03:00,  1.66it/s]Extractor Estimating: 300it [03:00,  1.66it/s]Extractor Estimating: 301it [03:01,  1.63it/s]Extractor Estimating: 302it [03:02,  1.58it/s]Extractor Estimating: 303it [03:02,  1.57it/s]Extractor Estimating: 304it [03:03,  1.55it/s]Extractor Estimating: 305it [03:04,  1.56it/s]Extractor Estimating: 306it [03:04,  1.53it/s]Extractor Estimating: 307it [03:05,  1.56it/s]Extractor Estimating: 308it [03:06,  1.57it/s]Extractor Estimating: 309it [03:06,  1.58it/s]Extractor Estimating: 310it [03:07,  1.59it/s]Extractor Estimating: 311it [03:08,  1.57it/s]Extractor Estimating: 312it [03:08,  1.55it/s]Extractor Estimating: 313it [03:09,  1.57it/s]Extractor Estimating: 314it [03:09,  1.58it/s]Extractor Estimating: 315it [03:10,  1.57it/s]Extractor Estimating: 316it [03:11,  1.57it/s]Extractor Estimating: 317it [03:11,  1.60it/s]Extractor Estimating: 318it [03:12,  1.44it/s]Extractor Estimating: 319it [03:13,  1.48it/s]Extractor Estimating: 320it [03:13,  1.50it/s]Extractor Estimating: 321it [03:14,  1.49it/s]Extractor Estimating: 322it [03:15,  1.52it/s]Extractor Estimating: 323it [03:15,  1.53it/s]Extractor Estimating: 324it [03:16,  1.57it/s]Extractor Estimating: 325it [03:17,  1.58it/s]Extractor Estimating: 326it [03:17,  1.58it/s]Extractor Estimating: 327it [03:18,  1.61it/s]Extractor Estimating: 328it [03:18,  1.62it/s]Extractor Estimating: 329it [03:19,  1.65it/s]Extractor Estimating: 330it [03:20,  1.62it/s]Extractor Estimating: 331it [03:20,  1.62it/s]Extractor Estimating: 332it [03:21,  1.65it/s]Extractor Estimating: 333it [03:21,  1.67it/s]Extractor Estimating: 334it [03:22,  1.66it/s]Extractor Estimating: 335it [03:23,  1.63it/s]Extractor Estimating: 336it [03:23,  1.65it/s]Extractor Estimating: 337it [03:24,  1.55it/s]Extractor Estimating: 338it [03:25,  1.60it/s]Extractor Estimating: 339it [03:25,  1.56it/s]Extractor Estimating: 340it [03:26,  1.62it/s]Extractor Estimating: 341it [03:26,  1.61it/s]Extractor Estimating: 342it [03:27,  1.62it/s]Extractor Estimating: 343it [03:28,  1.66it/s]Extractor Estimating: 344it [03:28,  1.64it/s]Extractor Estimating: 345it [03:29,  1.61it/s]Extractor Estimating: 346it [03:30,  1.62it/s]Extractor Estimating: 347it [03:30,  1.65it/s]Extractor Estimating: 348it [03:31,  1.67it/s]Extractor Estimating: 349it [03:31,  1.67it/s]Extractor Estimating: 350it [03:32,  1.65it/s]Extractor Estimating: 351it [03:32,  1.69it/s]Extractor Estimating: 352it [03:33,  1.66it/s]Extractor Estimating: 353it [03:34,  1.69it/s]Extractor Estimating: 354it [03:34,  1.69it/s]Extractor Estimating: 355it [03:35,  1.60it/s]Extractor Estimating: 356it [03:36,  1.56it/s]Extractor Estimating: 357it [03:36,  1.57it/s]Extractor Estimating: 358it [03:37,  1.65it/s]Extractor Estimating: 359it [03:37,  1.64it/s]Extractor Estimating: 360it [03:38,  1.66it/s]Extractor Estimating: 361it [03:39,  1.69it/s]Extractor Estimating: 362it [03:39,  1.64it/s]Extractor Estimating: 363it [03:40,  1.61it/s]Extractor Estimating: 364it [03:41,  1.59it/s]Extractor Estimating: 365it [03:41,  1.60it/s]Extractor Estimating: 366it [03:42,  1.67it/s]Extractor Estimating: 367it [03:42,  1.64it/s]Extractor Estimating: 368it [03:43,  1.63it/s]Extractor Estimating: 369it [03:44,  1.64it/s]Extractor Estimating: 370it [03:44,  1.64it/s]Extractor Estimating: 371it [03:45,  1.61it/s]Extractor Estimating: 372it [03:45,  1.65it/s]Extractor Estimating: 373it [03:46,  1.65it/s]Extractor Estimating: 374it [03:47,  1.61it/s]Extractor Estimating: 375it [03:47,  1.83it/s]Extractor Estimating: 375it [03:47,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:07,923 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:07,927 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:07,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:07,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:07,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:09:08,212 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:09:08,213 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:09:08,874 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:09:09,930 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:09:09,930 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:12,006 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:12,011 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:12,011 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:12,011 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:09:12,011 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:09:12,309 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:09:12,311 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:09:12,988 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:09:13,152 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:09:13,152 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 09:28:12,137 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 09:28:12,162 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7872 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 22784
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22884, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22884, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.010, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.020, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 72, avg_time 1.015, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 172, avg_time 1.017, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 272, avg_time 2.320, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 44, avg_time 1.003, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 144, avg_time 1.003, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 244, avg_time 1.012, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 16, avg_time 1.016, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 116, avg_time 2.313, loss:nan
g_step 1200, step 216, avg_time 1.009, loss:nan
g_step 1300, step 316, avg_time 1.013, loss:nan
g_step 1400, step 88, avg_time 1.004, loss:nan
g_step 1500, step 188, avg_time 1.018, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 288, avg_time 2.318, loss:nan
g_step 1700, step 60, avg_time 1.013, loss:nan
g_step 1800, step 160, avg_time 1.010, loss:nan
g_step 1900, step 260, avg_time 1.010, loss:nan
g_step 2000, step 32, avg_time 1.004, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 132, avg_time 2.321, loss:nan
g_step 2200, step 232, avg_time 1.005, loss:nan
g_step 2300, step 4, avg_time 1.012, loss:nan
g_step 2400, step 104, avg_time 1.014, loss:nan
g_step 2500, step 204, avg_time 0.998, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 304, avg_time 2.326, loss:nan
g_step 2700, step 76, avg_time 0.997, loss:nan
g_step 2800, step 176, avg_time 1.016, loss:nan
g_step 2900, step 276, avg_time 1.013, loss:nan
g_step 3000, step 48, avg_time 1.005, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 148, avg_time 2.332, loss:nan
g_step 3200, step 248, avg_time 1.004, loss:nan
g_step 3300, step 20, avg_time 1.014, loss:nan
g_step 3400, step 120, avg_time 1.021, loss:nan
g_step 3500, step 220, avg_time 1.018, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 320, avg_time 2.300, loss:nan
g_step 3700, step 92, avg_time 1.008, loss:nan
g_step 3800, step 192, avg_time 1.008, loss:nan
g_step 3900, step 292, avg_time 1.015, loss:nan
g_step 4000, step 64, avg_time 1.015, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 164, avg_time 2.315, loss:nan
g_step 4200, step 264, avg_time 1.016, loss:nan
g_step 4300, step 36, avg_time 1.007, loss:nan
g_step 4400, step 136, avg_time 1.000, loss:nan
g_step 4500, step 236, avg_time 1.013, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 8, avg_time 2.319, loss:nan
g_step 4700, step 108, avg_time 1.021, loss:nan
g_step 4800, step 208, avg_time 0.998, loss:nan
g_step 4900, step 308, avg_time 1.014, loss:nan
g_step 5000, step 80, avg_time 1.019, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 180, avg_time 2.313, loss:nan
g_step 5200, step 280, avg_time 1.016, loss:nan
g_step 5300, step 52, avg_time 1.010, loss:nan
g_step 5400, step 152, avg_time 1.004, loss:nan
g_step 5500, step 252, avg_time 1.019, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 24, avg_time 2.336, loss:nan
g_step 5700, step 124, avg_time 1.003, loss:nan
g_step 5800, step 224, avg_time 1.018, loss:nan
g_step 5900, step 324, avg_time 1.012, loss:nan
g_step 6000, step 96, avg_time 1.024, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 196, avg_time 2.308, loss:nan
g_step 6200, step 296, avg_time 1.011, loss:nan
g_step 6300, step 68, avg_time 1.003, loss:nan
g_step 6400, step 168, avg_time 1.027, loss:nan
g_step 6500, step 268, avg_time 1.014, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 09:28:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 09:28:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_09-28-12_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 09:28:13 - WARNING - datasets.builder -   Using custom data configuration default-61f5b8d959b78e38
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-61f5b8d959b78e38/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 09:28:13,418 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:28:13,419 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 09:28:13,420 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:28:13,421 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 09:28:13,431 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:28:13,436 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:28:13,436 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:28:13,436 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:28:13,436 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:28:13,436 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:28:13,436 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 09:28:13,585 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 09:28:16,646 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 09:28:16,654 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-61f5b8d959b78e38/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.09ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.94ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.30ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.47ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.56ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.63ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.66ba/s]100%|██████████| 8/8 [00:01<00:00,  4.83ba/s]100%|██████████| 8/8 [00:01<00:00,  4.52ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.17ba/s] 40%|████      | 2/5 [00:00<00:00,  4.40ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.48ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.54ba/s]100%|██████████| 5/5 [00:01<00:00,  4.74ba/s]100%|██████████| 5/5 [00:01<00:00,  4.60ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.82ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.25ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.55ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.72ba/s]100%|██████████| 8/8 [00:00<00:00, 10.74ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.56ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.65ba/s]100%|██████████| 5/5 [00:00<00:00, 11.19ba/s]100%|██████████| 5/5 [00:00<00:00, 10.98ba/s]
[INFO|trainer.py:414] 2023-08-28 09:28:21,108 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 09:28:21,127 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 09:28:21,127 >>   Num examples = 7899
[INFO|trainer.py:1149] 2023-08-28 09:28:21,127 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 09:28:21,127 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 09:28:21,127 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 09:28:21,127 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 09:28:21,128 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:00<02:54,  3.51it/s]  0%|          | 2/615 [00:00<02:51,  3.57it/s]  0%|          | 3/615 [00:00<02:50,  3.60it/s]  1%|          | 4/615 [00:01<02:49,  3.61it/s]  1%|          | 5/615 [00:01<02:48,  3.61it/s]  1%|          | 6/615 [00:01<02:48,  3.61it/s]  1%|          | 7/615 [00:01<02:49,  3.58it/s]  1%|▏         | 8/615 [00:02<02:49,  3.58it/s]  1%|▏         | 9/615 [00:02<02:48,  3.59it/s]  2%|▏         | 10/615 [00:02<02:48,  3.59it/s]  2%|▏         | 11/615 [00:03<02:47,  3.60it/s]  2%|▏         | 12/615 [00:03<02:47,  3.61it/s]  2%|▏         | 13/615 [00:03<02:46,  3.61it/s]  2%|▏         | 14/615 [00:03<02:46,  3.61it/s]  2%|▏         | 15/615 [00:04<02:46,  3.61it/s]  3%|▎         | 16/615 [00:04<02:45,  3.61it/s]  3%|▎         | 17/615 [00:04<02:45,  3.61it/s]  3%|▎         | 18/615 [00:05<02:45,  3.60it/s]  3%|▎         | 19/615 [00:05<02:45,  3.60it/s]  3%|▎         | 20/615 [00:05<02:44,  3.61it/s]  3%|▎         | 21/615 [00:05<02:44,  3.60it/s]  4%|▎         | 22/615 [00:06<02:44,  3.61it/s]  4%|▎         | 23/615 [00:06<02:44,  3.61it/s]  4%|▍         | 24/615 [00:06<02:43,  3.61it/s]  4%|▍         | 25/615 [00:06<02:43,  3.61it/s]  4%|▍         | 26/615 [00:07<02:43,  3.61it/s]  4%|▍         | 27/615 [00:07<02:42,  3.62it/s]  5%|▍         | 28/615 [00:07<02:42,  3.62it/s]  5%|▍         | 29/615 [00:08<02:42,  3.61it/s]  5%|▍         | 30/615 [00:08<02:42,  3.61it/s]  5%|▌         | 31/615 [00:08<02:41,  3.61it/s]  5%|▌         | 32/615 [00:08<02:41,  3.61it/s]  5%|▌         | 33/615 [00:09<02:41,  3.61it/s]  6%|▌         | 34/615 [00:09<02:40,  3.61it/s]  6%|▌         | 35/615 [00:09<02:40,  3.61it/s]  6%|▌         | 36/615 [00:09<02:40,  3.61it/s]  6%|▌         | 37/615 [00:10<02:40,  3.61it/s]  6%|▌         | 38/615 [00:10<02:39,  3.61it/s]  6%|▋         | 39/615 [00:10<02:39,  3.61it/s]  7%|▋         | 40/615 [00:11<02:42,  3.55it/s]  7%|▋         | 41/615 [00:11<02:41,  3.56it/s]  7%|▋         | 42/615 [00:11<02:40,  3.58it/s]  7%|▋         | 43/615 [00:11<02:39,  3.59it/s]  7%|▋         | 44/615 [00:12<02:38,  3.59it/s]  7%|▋         | 45/615 [00:12<02:38,  3.60it/s]  7%|▋         | 46/615 [00:12<02:38,  3.60it/s]  8%|▊         | 47/615 [00:13<02:37,  3.60it/s]  8%|▊         | 48/615 [00:13<02:37,  3.60it/s]  8%|▊         | 49/615 [00:13<02:36,  3.61it/s]  8%|▊         | 50/615 [00:13<02:36,  3.60it/s]  8%|▊         | 51/615 [00:14<02:37,  3.58it/s]  8%|▊         | 52/615 [00:14<02:36,  3.59it/s]  9%|▊         | 53/615 [00:14<02:36,  3.60it/s]  9%|▉         | 54/615 [00:14<02:35,  3.60it/s]  9%|▉         | 55/615 [00:15<02:35,  3.60it/s]  9%|▉         | 56/615 [00:15<02:35,  3.61it/s]  9%|▉         | 57/615 [00:15<02:34,  3.61it/s]  9%|▉         | 58/615 [00:16<02:34,  3.60it/s] 10%|▉         | 59/615 [00:16<02:34,  3.60it/s] 10%|▉         | 60/615 [00:16<02:34,  3.60it/s] 10%|▉         | 61/615 [00:16<02:33,  3.60it/s] 10%|█         | 62/615 [00:17<02:34,  3.59it/s] 10%|█         | 63/615 [00:17<02:33,  3.59it/s] 10%|█         | 64/615 [00:17<02:33,  3.60it/s] 11%|█         | 65/615 [00:18<02:32,  3.60it/s] 11%|█         | 66/615 [00:18<02:32,  3.61it/s] 11%|█         | 67/615 [00:18<02:31,  3.61it/s] 11%|█         | 68/615 [00:18<02:32,  3.60it/s] 11%|█         | 69/615 [00:19<02:31,  3.60it/s] 11%|█▏        | 70/615 [00:19<02:31,  3.60it/s] 12%|█▏        | 71/615 [00:19<02:31,  3.60it/s] 12%|█▏        | 72/615 [00:19<02:30,  3.60it/s] 12%|█▏        | 73/615 [00:20<02:31,  3.58it/s] 12%|█▏        | 74/615 [00:20<02:30,  3.58it/s] 12%|█▏        | 75/615 [00:20<02:30,  3.59it/s] 12%|█▏        | 76/615 [00:21<02:29,  3.60it/s] 13%|█▎        | 77/615 [00:21<02:29,  3.60it/s] 13%|█▎        | 78/615 [00:21<02:29,  3.60it/s] 13%|█▎        | 79/615 [00:21<02:28,  3.60it/s] 13%|█▎        | 80/615 [00:22<02:28,  3.60it/s] 13%|█▎        | 81/615 [00:22<02:28,  3.60it/s] 13%|█▎        | 82/615 [00:22<02:28,  3.60it/s] 13%|█▎        | 83/615 [00:23<02:27,  3.60it/s] 14%|█▎        | 84/615 [00:23<02:27,  3.60it/s] 14%|█▍        | 85/615 [00:23<02:27,  3.60it/s] 14%|█▍        | 86/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 87/615 [00:24<02:26,  3.61it/s] 14%|█▍        | 88/615 [00:24<02:26,  3.61it/s] 14%|█▍        | 89/615 [00:24<02:25,  3.61it/s] 15%|█▍        | 90/615 [00:25<02:26,  3.59it/s] 15%|█▍        | 91/615 [00:25<02:25,  3.59it/s] 15%|█▍        | 92/615 [00:25<02:25,  3.59it/s] 15%|█▌        | 93/615 [00:25<02:25,  3.60it/s] 15%|█▌        | 94/615 [00:26<02:24,  3.60it/s] 15%|█▌        | 95/615 [00:26<02:24,  3.60it/s] 16%|█▌        | 96/615 [00:26<02:23,  3.61it/s] 16%|█▌        | 97/615 [00:26<02:23,  3.61it/s] 16%|█▌        | 98/615 [00:27<02:23,  3.60it/s] 16%|█▌        | 99/615 [00:27<02:23,  3.60it/s] 16%|█▋        | 100/615 [00:27<02:23,  3.60it/s] 16%|█▋        | 101/615 [00:28<02:23,  3.58it/s] 17%|█▋        | 102/615 [00:28<02:23,  3.58it/s] 17%|█▋        | 103/615 [00:28<02:22,  3.59it/s] 17%|█▋        | 104/615 [00:28<02:22,  3.59it/s] 17%|█▋        | 105/615 [00:29<02:21,  3.60it/s] 17%|█▋        | 106/615 [00:29<02:21,  3.60it/s] 17%|█▋        | 107/615 [00:29<02:21,  3.60it/s] 18%|█▊        | 108/615 [00:30<02:20,  3.60it/s] 18%|█▊        | 109/615 [00:30<02:20,  3.60it/s] 18%|█▊        | 110/615 [00:30<02:20,  3.60it/s] 18%|█▊        | 111/615 [00:30<02:20,  3.60it/s] 18%|█▊        | 112/615 [00:31<02:20,  3.59it/s] 18%|█▊        | 113/615 [00:31<02:19,  3.59it/s] 19%|█▊        | 114/615 [00:31<02:19,  3.59it/s] 19%|█▊        | 115/615 [00:31<02:19,  3.60it/s] 19%|█▉        | 116/615 [00:32<02:18,  3.60it/s] 19%|█▉        | 117/615 [00:32<02:18,  3.59it/s] 19%|█▉        | 118/615 [00:32<02:18,  3.60it/s] 19%|█▉        | 119/615 [00:33<02:17,  3.60it/s] 20%|█▉        | 120/615 [00:33<02:17,  3.60it/s] 20%|█▉        | 121/615 [00:33<02:17,  3.60it/s] 20%|█▉        | 122/615 [00:33<02:16,  3.60it/s] 20%|██        | 123/615 [00:34<02:17,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 09:28:55,425 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:28:55,426 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 09:28:55,426 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.36it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.97it/s][A
  3%|▎         | 17/611 [00:00<00:12, 47.14it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.96it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.13it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.54it/s][A
  6%|▌         | 37/611 [00:00<00:13, 44.09it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.97it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.09it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.31it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.53it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.63it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.67it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.40it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.09it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.87it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.83it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.02it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.24it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.37it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.48it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.60it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.47it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.19it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.91it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.90it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.93it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.22it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.33it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.36it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.52it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.47it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.31it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.04it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.93it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.99it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.23it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.37it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.48it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.53it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.36it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.30it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.10it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.03it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.18it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.29it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.35it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.43it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.37it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.31it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.18it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.11it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.12it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.16it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.26it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.43it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.48it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.41it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.25it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.17it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.11it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.08it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.17it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.16it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.48it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.49it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.31it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.21it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.07it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.12it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.15it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.24it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.35it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.46it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.44it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.31it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.12it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.96it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.10it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.28it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.31it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.36it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.31it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.36it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.22it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.11it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.05it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.16it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.16it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.26it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.32it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.40it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.40it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.25it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.18it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.15it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.08it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.24it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.20it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.26it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.47it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.39it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.30it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.25it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.15it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.17it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.25it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.30it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.33it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.40it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.32it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.27it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.21it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.18it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.13it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.28it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.25it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.32it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.33it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.25it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.25it/s][A 20%|██        | 123/615 [00:48<02:17,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:29:09,237 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-28 09:29:09,261 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:29:10,911 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:29:10,937 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:29:10,948 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123/special_tokens_map.json
 20%|██        | 124/615 [00:50<41:23,  5.06s/it] 20%|██        | 125/615 [00:50<29:36,  3.62s/it] 20%|██        | 126/615 [00:50<21:22,  2.62s/it] 21%|██        | 127/615 [00:51<15:39,  1.93s/it] 21%|██        | 128/615 [00:51<11:37,  1.43s/it] 21%|██        | 129/615 [00:51<08:48,  1.09s/it] 21%|██        | 130/615 [00:52<06:50,  1.18it/s] 21%|██▏       | 131/615 [00:52<05:27,  1.48it/s] 21%|██▏       | 132/615 [00:52<04:29,  1.79it/s] 22%|██▏       | 133/615 [00:52<03:48,  2.11it/s] 22%|██▏       | 134/615 [00:53<03:20,  2.40it/s] 22%|██▏       | 135/615 [00:53<03:00,  2.66it/s] 22%|██▏       | 136/615 [00:53<02:47,  2.87it/s] 22%|██▏       | 137/615 [00:54<02:37,  3.04it/s] 22%|██▏       | 138/615 [00:54<02:30,  3.17it/s] 23%|██▎       | 139/615 [00:54<02:25,  3.28it/s] 23%|██▎       | 140/615 [00:54<02:21,  3.36it/s] 23%|██▎       | 141/615 [00:55<02:18,  3.42it/s] 23%|██▎       | 142/615 [00:55<02:16,  3.45it/s] 23%|██▎       | 143/615 [00:55<02:15,  3.47it/s] 23%|██▎       | 144/615 [00:56<02:14,  3.49it/s] 24%|██▎       | 145/615 [00:56<02:13,  3.51it/s] 24%|██▎       | 146/615 [00:56<02:13,  3.52it/s] 24%|██▍       | 147/615 [00:56<02:12,  3.53it/s] 24%|██▍       | 148/615 [00:57<02:12,  3.54it/s] 24%|██▍       | 149/615 [00:57<02:11,  3.54it/s] 24%|██▍       | 150/615 [00:57<02:11,  3.54it/s] 25%|██▍       | 151/615 [00:58<02:10,  3.55it/s] 25%|██▍       | 152/615 [00:58<02:10,  3.55it/s] 25%|██▍       | 153/615 [00:58<02:10,  3.55it/s] 25%|██▌       | 154/615 [00:58<02:10,  3.53it/s] 25%|██▌       | 155/615 [00:59<02:10,  3.54it/s] 25%|██▌       | 156/615 [00:59<02:09,  3.54it/s] 26%|██▌       | 157/615 [00:59<02:09,  3.54it/s] 26%|██▌       | 158/615 [00:59<02:08,  3.54it/s] 26%|██▌       | 159/615 [01:00<02:08,  3.55it/s] 26%|██▌       | 160/615 [01:00<02:08,  3.55it/s] 26%|██▌       | 161/615 [01:00<02:08,  3.54it/s] 26%|██▋       | 162/615 [01:01<02:07,  3.55it/s] 27%|██▋       | 163/615 [01:01<02:07,  3.55it/s] 27%|██▋       | 164/615 [01:01<02:07,  3.55it/s] 27%|██▋       | 165/615 [01:01<02:07,  3.53it/s] 27%|██▋       | 166/615 [01:02<02:06,  3.54it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/615 [01:02<02:06,  3.54it/s] 27%|██▋       | 168/615 [01:02<02:06,  3.54it/s] 27%|██▋       | 169/615 [01:03<02:05,  3.55it/s] 28%|██▊       | 170/615 [01:03<02:05,  3.55it/s] 28%|██▊       | 171/615 [01:03<02:04,  3.55it/s] 28%|██▊       | 172/615 [01:03<02:04,  3.55it/s] 28%|██▊       | 173/615 [01:04<02:04,  3.55it/s] 28%|██▊       | 174/615 [01:04<02:04,  3.55it/s] 28%|██▊       | 175/615 [01:04<02:04,  3.55it/s] 29%|██▊       | 176/615 [01:05<02:05,  3.49it/s] 29%|██▉       | 177/615 [01:05<02:04,  3.51it/s] 29%|██▉       | 178/615 [01:05<02:04,  3.52it/s] 29%|██▉       | 179/615 [01:05<02:03,  3.53it/s] 29%|██▉       | 180/615 [01:06<02:03,  3.53it/s] 29%|██▉       | 181/615 [01:06<02:02,  3.54it/s] 30%|██▉       | 182/615 [01:06<02:02,  3.54it/s] 30%|██▉       | 183/615 [01:07<02:02,  3.54it/s] 30%|██▉       | 184/615 [01:07<02:01,  3.54it/s] 30%|███       | 185/615 [01:07<02:01,  3.54it/s] 30%|███       | 186/615 [01:07<02:01,  3.54it/s] 30%|███       | 187/615 [01:08<02:01,  3.53it/s] 31%|███       | 188/615 [01:08<02:00,  3.54it/s] 31%|███       | 189/615 [01:08<02:00,  3.54it/s] 31%|███       | 190/615 [01:09<01:59,  3.54it/s] 31%|███       | 191/615 [01:09<01:59,  3.55it/s] 31%|███       | 192/615 [01:09<01:59,  3.55it/s] 31%|███▏      | 193/615 [01:09<01:58,  3.55it/s] 32%|███▏      | 194/615 [01:10<01:58,  3.55it/s] 32%|███▏      | 195/615 [01:10<01:57,  3.56it/s] 32%|███▏      | 196/615 [01:10<01:57,  3.57it/s] 32%|███▏      | 197/615 [01:10<01:56,  3.58it/s] 32%|███▏      | 198/615 [01:11<01:56,  3.58it/s] 32%|███▏      | 199/615 [01:11<01:55,  3.59it/s] 33%|███▎      | 200/615 [01:11<01:55,  3.59it/s] 33%|███▎      | 201/615 [01:12<01:55,  3.59it/s] 33%|███▎      | 202/615 [01:12<01:54,  3.59it/s] 33%|███▎      | 203/615 [01:12<01:54,  3.60it/s] 33%|███▎      | 204/615 [01:12<01:54,  3.60it/s] 33%|███▎      | 205/615 [01:13<01:53,  3.60it/s] 33%|███▎      | 206/615 [01:13<01:53,  3.60it/s] 34%|███▎      | 207/615 [01:13<01:53,  3.60it/s] 34%|███▍      | 208/615 [01:14<01:52,  3.60it/s] 34%|███▍      | 209/615 [01:14<01:52,  3.59it/s] 34%|███▍      | 210/615 [01:14<01:52,  3.60it/s] 34%|███▍      | 211/615 [01:14<01:52,  3.60it/s] 34%|███▍      | 212/615 [01:15<01:51,  3.60it/s] 35%|███▍      | 213/615 [01:15<01:51,  3.60it/s] 35%|███▍      | 214/615 [01:15<01:51,  3.60it/s] 35%|███▍      | 215/615 [01:15<01:51,  3.59it/s] 35%|███▌      | 216/615 [01:16<01:50,  3.60it/s] 35%|███▌      | 217/615 [01:16<01:50,  3.60it/s] 35%|███▌      | 218/615 [01:16<01:50,  3.60it/s] 36%|███▌      | 219/615 [01:17<01:49,  3.60it/s] 36%|███▌      | 220/615 [01:17<01:50,  3.59it/s] 36%|███▌      | 221/615 [01:17<01:49,  3.59it/s] 36%|███▌      | 222/615 [01:17<01:49,  3.60it/s] 36%|███▋      | 223/615 [01:18<01:48,  3.60it/s] 36%|███▋      | 224/615 [01:18<01:48,  3.60it/s] 37%|███▋      | 225/615 [01:18<01:48,  3.60it/s] 37%|███▋      | 226/615 [01:19<01:48,  3.60it/s] 37%|███▋      | 227/615 [01:19<01:47,  3.60it/s] 37%|███▋      | 228/615 [01:19<01:47,  3.60it/s] 37%|███▋      | 229/615 [01:19<01:47,  3.60it/s] 37%|███▋      | 230/615 [01:20<01:46,  3.60it/s] 38%|███▊      | 231/615 [01:20<01:46,  3.59it/s] 38%|███▊      | 232/615 [01:20<01:46,  3.59it/s] 38%|███▊      | 233/615 [01:21<01:46,  3.59it/s] 38%|███▊      | 234/615 [01:21<01:45,  3.60it/s] 38%|███▊      | 235/615 [01:21<01:45,  3.60it/s] 38%|███▊      | 236/615 [01:21<01:45,  3.60it/s] 39%|███▊      | 237/615 [01:22<01:45,  3.60it/s] 39%|███▊      | 238/615 [01:22<01:44,  3.60it/s] 39%|███▉      | 239/615 [01:22<01:44,  3.60it/s] 39%|███▉      | 240/615 [01:22<01:44,  3.60it/s] 39%|███▉      | 241/615 [01:23<01:43,  3.60it/s] 39%|███▉      | 242/615 [01:23<01:43,  3.60it/s] 40%|███▉      | 243/615 [01:23<01:43,  3.60it/s] 40%|███▉      | 244/615 [01:24<01:43,  3.60it/s] 40%|███▉      | 245/615 [01:24<01:42,  3.60it/s] 40%|████      | 246/615 [01:24<01:42,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 09:29:45,861 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:29:45,862 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 09:29:45,862 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.7961, 'eval_samples_per_second': 353.869, 'eval_steps_per_second': 44.288, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.18it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.88it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.86it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.69it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.93it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.49it/s][A
  6%|▌         | 37/611 [00:00<00:13, 44.13it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.07it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.16it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.44it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.49it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.42it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.27it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.14it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.97it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.86it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.82it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.06it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.24it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.36it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.48it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.43it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.14it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.00it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.90it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.88it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.05it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.21it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.33it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.44it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.40it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.27it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.18it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.99it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.04it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.18it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.26it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.43it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.44it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.35it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.27it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.08it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.05it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.03it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.16it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.30it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.42it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.41it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.36it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.06it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.12it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.91it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.97it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.18it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.32it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.31it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.42it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.23it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.18it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.06it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.94it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.01it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.23it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.27it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.41it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.36it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.25it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.17it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.02it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.96it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.01it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.19it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.34it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.47it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.30it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.22it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.15it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.09it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.02it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.05it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.19it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.35it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.38it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.39it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.30it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.19it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.05it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.11it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.21it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.26it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.27it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.36it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.32it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.20it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.14it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.08it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.12it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.24it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.25it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.26it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.38it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.27it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.15it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.25it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.10it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.17it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.24it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.23it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.30it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.31it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.22it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.33it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.25it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.11it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.21it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.25it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.20it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.29it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.22it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.24it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.20it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.20it/s][A 40%|████      | 246/615 [01:38<01:42,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:29:59,694 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 09:29:59,718 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:30:01,544 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:30:01,555 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:30:01,565 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246/special_tokens_map.json
 40%|████      | 247/615 [01:41<31:22,  5.11s/it] 40%|████      | 248/615 [01:41<22:24,  3.66s/it] 40%|████      | 249/615 [01:41<16:10,  2.65s/it] 41%|████      | 250/615 [01:41<11:48,  1.94s/it] 41%|████      | 251/615 [01:42<08:45,  1.44s/it] 41%|████      | 252/615 [01:42<06:36,  1.09s/it] 41%|████      | 253/615 [01:42<05:07,  1.18it/s] 41%|████▏     | 254/615 [01:42<04:04,  1.48it/s] 41%|████▏     | 255/615 [01:43<03:20,  1.79it/s] 42%|████▏     | 256/615 [01:43<02:49,  2.11it/s] 42%|████▏     | 257/615 [01:43<02:28,  2.41it/s] 42%|████▏     | 258/615 [01:44<02:13,  2.68it/s] 42%|████▏     | 259/615 [01:44<02:02,  2.90it/s] 42%|████▏     | 260/615 [01:44<01:55,  3.07it/s] 42%|████▏     | 261/615 [01:44<01:50,  3.21it/s] 43%|████▎     | 262/615 [01:45<01:46,  3.32it/s] 43%|████▎     | 263/615 [01:45<01:43,  3.40it/s] 43%|████▎     | 264/615 [01:45<01:41,  3.46it/s] 43%|████▎     | 265/615 [01:46<01:40,  3.50it/s] 43%|████▎     | 266/615 [01:46<01:38,  3.53it/s] 43%|████▎     | 267/615 [01:46<01:38,  3.55it/s] 44%|████▎     | 268/615 [01:46<01:37,  3.56it/s] 44%|████▎     | 269/615 [01:47<01:36,  3.58it/s] 44%|████▍     | 270/615 [01:47<01:36,  3.59it/s] 44%|████▍     | 271/615 [01:47<01:36,  3.58it/s] 44%|████▍     | 272/615 [01:47<01:35,  3.59it/s] 44%|████▍     | 273/615 [01:48<01:35,  3.59it/s] 45%|████▍     | 274/615 [01:48<01:34,  3.60it/s] 45%|████▍     | 275/615 [01:48<01:34,  3.60it/s] 45%|████▍     | 276/615 [01:49<01:34,  3.60it/s] 45%|████▌     | 277/615 [01:49<01:33,  3.60it/s] 45%|████▌     | 278/615 [01:49<01:33,  3.60it/s] 45%|████▌     | 279/615 [01:49<01:33,  3.59it/s] 46%|████▌     | 280/615 [01:50<01:33,  3.59it/s] 46%|████▌     | 281/615 [01:50<01:32,  3.59it/s] 46%|████▌     | 282/615 [01:50<01:33,  3.57it/s] 46%|████▌     | 283/615 [01:51<01:32,  3.58it/s] 46%|████▌     | 284/615 [01:51<01:34,  3.49it/s] 46%|████▋     | 285/615 [01:51<01:34,  3.51it/s] 47%|████▋     | 286/615 [01:51<01:33,  3.53it/s] 47%|████▋     | 287/615 [01:52<01:32,  3.54it/s] 47%|████▋     | 288/615 [01:52<01:32,  3.55it/s] 47%|████▋     | 289/615 [01:52<01:31,  3.57it/s] 47%|████▋     | 290/615 [01:53<01:30,  3.57it/s] 47%|████▋     | 291/615 [01:53<01:30,  3.58it/s] 47%|████▋     | 292/615 [01:53<01:30,  3.59it/s] 48%|████▊     | 293/615 [01:53<01:30,  3.56it/s] 48%|████▊     | 294/615 [01:54<01:29,  3.57it/s] 48%|████▊     | 295/615 [01:54<01:29,  3.58it/s] 48%|████▊     | 296/615 [01:54<01:28,  3.59it/s] 48%|████▊     | 297/615 [01:54<01:28,  3.59it/s] 48%|████▊     | 298/615 [01:55<01:28,  3.60it/s] 49%|████▊     | 299/615 [01:55<01:27,  3.60it/s] 49%|████▉     | 300/615 [01:55<01:27,  3.59it/s] 49%|████▉     | 301/615 [01:56<01:27,  3.60it/s] 49%|████▉     | 302/615 [01:56<01:27,  3.60it/s] 49%|████▉     | 303/615 [01:56<01:26,  3.60it/s] 49%|████▉     | 304/615 [01:56<01:26,  3.60it/s] 50%|████▉     | 305/615 [01:57<01:26,  3.58it/s] 50%|████▉     | 306/615 [01:57<01:26,  3.58it/s] 50%|████▉     | 307/615 [01:57<01:25,  3.59it/s] 50%|█████     | 308/615 [01:58<01:25,  3.59it/s] 50%|█████     | 309/615 [01:58<01:25,  3.60it/s] 50%|█████     | 310/615 [01:58<01:24,  3.60it/s] 51%|█████     | 311/615 [01:58<01:24,  3.60it/s] 51%|█████     | 312/615 [01:59<01:24,  3.60it/s] 51%|█████     | 313/615 [01:59<01:23,  3.60it/s] 51%|█████     | 314/615 [01:59<01:23,  3.60it/s] 51%|█████     | 315/615 [01:59<01:23,  3.59it/s] 51%|█████▏    | 316/615 [02:00<01:23,  3.57it/s] 52%|█████▏    | 317/615 [02:00<01:23,  3.58it/s] 52%|█████▏    | 318/615 [02:00<01:22,  3.59it/s] 52%|█████▏    | 319/615 [02:01<01:22,  3.59it/s] 52%|█████▏    | 320/615 [02:01<01:22,  3.59it/s] 52%|█████▏    | 321/615 [02:01<01:21,  3.60it/s] 52%|█████▏    | 322/615 [02:01<01:21,  3.60it/s] 53%|█████▎    | 323/615 [02:02<01:21,  3.60it/s] 53%|█████▎    | 324/615 [02:02<01:20,  3.60it/s] 53%|█████▎    | 325/615 [02:02<01:20,  3.60it/s] 53%|█████▎    | 326/615 [02:03<01:20,  3.60it/s] 53%|█████▎    | 327/615 [02:03<01:20,  3.57it/s] 53%|█████▎    | 328/615 [02:03<01:20,  3.58it/s] 53%|█████▎    | 329/615 [02:03<01:19,  3.58it/s] 54%|█████▎    | 330/615 [02:04<01:19,  3.58it/s] 54%|█████▍    | 331/615 [02:04<01:19,  3.59it/s] 54%|█████▍    | 332/615 [02:04<01:18,  3.59it/s] 54%|█████▍    | 333/615 [02:05<01:18,  3.57it/s] 54%|█████▍    | 334/615 [02:05<01:18,  3.56it/s] 54%|█████▍    | 335/615 [02:05<01:18,  3.56it/s] 55%|█████▍    | 336/615 [02:05<01:18,  3.55it/s] 55%|█████▍    | 337/615 [02:06<01:18,  3.55it/s] 55%|█████▍    | 338/615 [02:06<01:18,  3.54it/s] 55%|█████▌    | 339/615 [02:06<01:17,  3.54it/s] 55%|█████▌    | 340/615 [02:06<01:17,  3.54it/s] 55%|█████▌    | 341/615 [02:07<01:17,  3.54it/s] 56%|█████▌    | 342/615 [02:07<01:17,  3.54it/s] 56%|█████▌    | 343/615 [02:07<01:16,  3.54it/s] 56%|█████▌    | 344/615 [02:08<01:16,  3.55it/s] 56%|█████▌    | 345/615 [02:08<01:16,  3.55it/s] 56%|█████▋    | 346/615 [02:08<01:15,  3.55it/s] 56%|█████▋    | 347/615 [02:08<01:15,  3.55it/s] 57%|█████▋    | 348/615 [02:09<01:15,  3.55it/s] 57%|█████▋    | 349/615 [02:09<01:15,  3.53it/s] 57%|█████▋    | 350/615 [02:09<01:14,  3.53it/s] 57%|█████▋    | 351/615 [02:10<01:14,  3.54it/s] 57%|█████▋    | 352/615 [02:10<01:14,  3.54it/s] 57%|█████▋    | 353/615 [02:10<01:13,  3.54it/s] 58%|█████▊    | 354/615 [02:10<01:13,  3.55it/s] 58%|█████▊    | 355/615 [02:11<01:13,  3.55it/s] 58%|█████▊    | 356/615 [02:11<01:13,  3.55it/s] 58%|█████▊    | 357/615 [02:11<01:12,  3.54it/s] 58%|█████▊    | 358/615 [02:12<01:12,  3.54it/s] 58%|█████▊    | 359/615 [02:12<01:12,  3.54it/s] 59%|█████▊    | 360/615 [02:12<01:12,  3.53it/s] 59%|█████▊    | 361/615 [02:12<01:11,  3.53it/s] 59%|█████▉    | 362/615 [02:13<01:11,  3.54it/s] 59%|█████▉    | 363/615 [02:13<01:11,  3.54it/s] 59%|█████▉    | 364/615 [02:13<01:10,  3.54it/s] 59%|█████▉    | 365/615 [02:14<01:10,  3.54it/s] 60%|█████▉    | 366/615 [02:14<01:10,  3.54it/s] 60%|█████▉    | 367/615 [02:14<01:10,  3.54it/s] 60%|█████▉    | 368/615 [02:14<01:09,  3.54it/s] 60%|██████    | 369/615 [02:15<01:09,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 09:30:36,423 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:30:36,423 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 09:30:36,423 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8117, 'eval_samples_per_second': 353.469, 'eval_steps_per_second': 44.238, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.44it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.36it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.69it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.97it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.05it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.67it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.31it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.01it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.18it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.41it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.49it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.53it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.43it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.26it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.14it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.96it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.97it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.06it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.27it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.36it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.52it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.39it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.17it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.06it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.98it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.91it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.06it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.23it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.48it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.48it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.46it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.20it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.12it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.95it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.92it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.08it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.23it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.38it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.50it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.38it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.30it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.13it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.95it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 43.95it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.13it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.34it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.40it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.50it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.40it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.33it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.10it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.01it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.97it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.17it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.35it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.50it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.37it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.41it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.26it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.14it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.02it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.90it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.09it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.31it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.44it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.41it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.42it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.21it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.15it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.98it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.00it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.11it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.33it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.40it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.46it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.38it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.25it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.06it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.03it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.07it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.16it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.27it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.43it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.44it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.35it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.31it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.09it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.02it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.05it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.17it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.32it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.39it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.37it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.29it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.31it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.15it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.06it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.02it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.13it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.32it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.41it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.44it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.34it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.32it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.15it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.03it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.03it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.18it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.29it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.33it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.37it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.38it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.27it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.16it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.94it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.98it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.12it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.31it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.30it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.40it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.41it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.41it/s][A 60%|██████    | 369/615 [02:29<01:09,  3.55it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:30:50,250 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-28 09:30:50,269 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:30:52,091 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:30:52,114 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:30:52,129 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369/special_tokens_map.json
 60%|██████    | 370/615 [02:31<20:54,  5.12s/it] 60%|██████    | 371/615 [02:31<14:55,  3.67s/it] 60%|██████    | 372/615 [02:32<10:44,  2.65s/it] 61%|██████    | 373/615 [02:32<07:49,  1.94s/it] 61%|██████    | 374/615 [02:32<05:47,  1.44s/it] 61%|██████    | 375/615 [02:32<04:22,  1.09s/it] 61%|██████    | 376/615 [02:33<03:23,  1.18it/s] 61%|██████▏   | 377/615 [02:33<02:41,  1.47it/s] 61%|██████▏   | 378/615 [02:33<02:12,  1.78it/s] 62%|██████▏   | 379/615 [02:34<01:52,  2.10it/s] 62%|██████▏   | 380/615 [02:34<01:38,  2.39it/s] 62%|██████▏   | 381/615 [02:34<01:28,  2.65it/s] 62%|██████▏   | 382/615 [02:34<01:21,  2.87it/s] 62%|██████▏   | 383/615 [02:35<01:16,  3.04it/s] 62%|██████▏   | 384/615 [02:35<01:12,  3.18it/s] 63%|██████▎   | 385/615 [02:35<01:10,  3.28it/s] 63%|██████▎   | 386/615 [02:36<01:08,  3.35it/s] 63%|██████▎   | 387/615 [02:36<01:06,  3.40it/s] 63%|██████▎   | 388/615 [02:36<01:05,  3.45it/s] 63%|██████▎   | 389/615 [02:36<01:05,  3.48it/s] 63%|██████▎   | 390/615 [02:37<01:04,  3.49it/s] 64%|██████▎   | 391/615 [02:37<01:03,  3.51it/s] 64%|██████▎   | 392/615 [02:37<01:03,  3.52it/s] 64%|██████▍   | 393/615 [02:38<01:02,  3.53it/s] 64%|██████▍   | 394/615 [02:38<01:02,  3.54it/s] 64%|██████▍   | 395/615 [02:38<01:02,  3.54it/s] 64%|██████▍   | 396/615 [02:38<01:01,  3.55it/s] 65%|██████▍   | 397/615 [02:39<01:01,  3.54it/s] 65%|██████▍   | 398/615 [02:39<01:01,  3.55it/s] 65%|██████▍   | 399/615 [02:39<01:00,  3.55it/s] 65%|██████▌   | 400/615 [02:40<01:00,  3.55it/s] 65%|██████▌   | 401/615 [02:40<01:00,  3.55it/s] 65%|██████▌   | 402/615 [02:40<01:00,  3.55it/s] 66%|██████▌   | 403/615 [02:40<00:59,  3.55it/s] 66%|██████▌   | 404/615 [02:41<00:59,  3.55it/s] 66%|██████▌   | 405/615 [02:41<00:59,  3.55it/s] 66%|██████▌   | 406/615 [02:41<00:58,  3.56it/s] 66%|██████▌   | 407/615 [02:42<00:58,  3.55it/s] 66%|██████▋   | 408/615 [02:42<00:58,  3.51it/s] 67%|██████▋   | 409/615 [02:42<00:58,  3.53it/s] 67%|██████▋   | 410/615 [02:42<00:57,  3.53it/s] 67%|██████▋   | 411/615 [02:43<00:57,  3.54it/s] 67%|██████▋   | 412/615 [02:43<00:57,  3.54it/s] 67%|██████▋   | 413/615 [02:43<00:56,  3.55it/s] 67%|██████▋   | 414/615 [02:43<00:56,  3.55it/s] 67%|██████▋   | 415/615 [02:44<00:56,  3.56it/s] 68%|██████▊   | 416/615 [02:44<00:56,  3.55it/s] 68%|██████▊   | 417/615 [02:44<00:55,  3.55it/s] 68%|██████▊   | 418/615 [02:45<00:55,  3.55it/s] 68%|██████▊   | 419/615 [02:45<00:55,  3.53it/s] 68%|██████▊   | 420/615 [02:45<00:55,  3.53it/s] 68%|██████▊   | 421/615 [02:45<00:54,  3.54it/s] 69%|██████▊   | 422/615 [02:46<00:54,  3.54it/s] 69%|██████▉   | 423/615 [02:46<00:54,  3.55it/s] 69%|██████▉   | 424/615 [02:46<00:53,  3.55it/s] 69%|██████▉   | 425/615 [02:47<00:53,  3.55it/s] 69%|██████▉   | 426/615 [02:47<00:53,  3.55it/s] 69%|██████▉   | 427/615 [02:47<00:52,  3.55it/s] 70%|██████▉   | 428/615 [02:47<00:52,  3.55it/s] 70%|██████▉   | 429/615 [02:48<00:52,  3.55it/s] 70%|██████▉   | 430/615 [02:48<00:52,  3.54it/s] 70%|███████   | 431/615 [02:48<00:51,  3.54it/s] 70%|███████   | 432/615 [02:49<00:51,  3.55it/s] 70%|███████   | 433/615 [02:49<00:51,  3.55it/s] 71%|███████   | 434/615 [02:49<00:50,  3.55it/s] 71%|███████   | 435/615 [02:49<00:50,  3.55it/s] 71%|███████   | 436/615 [02:50<00:50,  3.54it/s] 71%|███████   | 437/615 [02:50<00:50,  3.54it/s] 71%|███████   | 438/615 [02:50<00:49,  3.54it/s] 71%|███████▏  | 439/615 [02:51<00:49,  3.54it/s] 72%|███████▏  | 440/615 [02:51<00:49,  3.53it/s] 72%|███████▏  | 441/615 [02:51<00:50,  3.46it/s] 72%|███████▏  | 442/615 [02:51<00:49,  3.47it/s] 72%|███████▏  | 443/615 [02:52<00:49,  3.49it/s] 72%|███████▏  | 444/615 [02:52<00:48,  3.51it/s] 72%|███████▏  | 445/615 [02:52<00:48,  3.52it/s] 73%|███████▎  | 446/615 [02:53<00:47,  3.53it/s] 73%|███████▎  | 447/615 [02:53<00:47,  3.54it/s] 73%|███████▎  | 448/615 [02:53<00:47,  3.54it/s] 73%|███████▎  | 449/615 [02:53<00:46,  3.55it/s] 73%|███████▎  | 450/615 [02:54<00:46,  3.53it/s] 73%|███████▎  | 451/615 [02:54<00:46,  3.54it/s] 73%|███████▎  | 452/615 [02:54<00:46,  3.53it/s] 74%|███████▎  | 453/615 [02:55<00:45,  3.54it/s] 74%|███████▍  | 454/615 [02:55<00:45,  3.56it/s] 74%|███████▍  | 455/615 [02:55<00:44,  3.57it/s] 74%|███████▍  | 456/615 [02:55<00:44,  3.58it/s] 74%|███████▍  | 457/615 [02:56<00:44,  3.58it/s] 74%|███████▍  | 458/615 [02:56<00:43,  3.59it/s] 75%|███████▍  | 459/615 [02:56<00:43,  3.60it/s] 75%|███████▍  | 460/615 [02:56<00:43,  3.60it/s] 75%|███████▍  | 461/615 [02:57<00:42,  3.60it/s] 75%|███████▌  | 462/615 [02:57<00:42,  3.60it/s] 75%|███████▌  | 463/615 [02:57<00:42,  3.58it/s] 75%|███████▌  | 464/615 [02:58<00:42,  3.59it/s] 76%|███████▌  | 465/615 [02:58<00:41,  3.59it/s] 76%|███████▌  | 466/615 [02:58<00:41,  3.59it/s] 76%|███████▌  | 467/615 [02:58<00:41,  3.59it/s] 76%|███████▌  | 468/615 [02:59<00:40,  3.60it/s] 76%|███████▋  | 469/615 [02:59<00:40,  3.60it/s] 76%|███████▋  | 470/615 [02:59<00:40,  3.60it/s] 77%|███████▋  | 471/615 [03:00<00:39,  3.60it/s] 77%|███████▋  | 472/615 [03:00<00:39,  3.60it/s] 77%|███████▋  | 473/615 [03:00<00:39,  3.61it/s] 77%|███████▋  | 474/615 [03:00<00:39,  3.60it/s] 77%|███████▋  | 475/615 [03:01<00:38,  3.60it/s] 77%|███████▋  | 476/615 [03:01<00:38,  3.60it/s] 78%|███████▊  | 477/615 [03:01<00:38,  3.60it/s] 78%|███████▊  | 478/615 [03:01<00:38,  3.60it/s] 78%|███████▊  | 479/615 [03:02<00:37,  3.61it/s] 78%|███████▊  | 480/615 [03:02<00:37,  3.61it/s] 78%|███████▊  | 481/615 [03:02<00:37,  3.61it/s] 78%|███████▊  | 482/615 [03:03<00:36,  3.61it/s] 79%|███████▊  | 483/615 [03:03<00:36,  3.60it/s] 79%|███████▊  | 484/615 [03:03<00:36,  3.59it/s] 79%|███████▉  | 485/615 [03:03<00:36,  3.60it/s] 79%|███████▉  | 486/615 [03:04<00:35,  3.60it/s] 79%|███████▉  | 487/615 [03:04<00:35,  3.60it/s] 79%|███████▉  | 488/615 [03:04<00:35,  3.60it/s] 80%|███████▉  | 489/615 [03:05<00:34,  3.60it/s] 80%|███████▉  | 490/615 [03:05<00:34,  3.60it/s] 80%|███████▉  | 491/615 [03:05<00:34,  3.60it/s] 80%|████████  | 492/615 [03:05<00:34,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 09:31:27,094 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:31:27,095 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 09:31:27,095 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8041, 'eval_samples_per_second': 353.663, 'eval_steps_per_second': 44.262, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.08it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.76it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.85it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.64it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.92it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.46it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.19it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.05it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.21it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.34it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.48it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.52it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.45it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.34it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.12it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.97it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.01it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.11it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.20it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.38it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.41it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.43it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.28it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.06it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.02it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.03it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.20it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.35it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.39it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.43it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.35it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.25it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.10it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.02it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.00it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.23it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.36it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.44it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.41it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.19it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.34it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.16it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.96it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 43.82it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.14it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.35it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.37it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.48it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.32it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.33it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.15it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.03it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.93it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.21it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.36it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.47it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.44it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.37it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.28it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.08it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.02it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.98it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 43.99it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.26it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.36it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.34it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.29it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.20it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.00it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.95it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.96it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.22it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.36it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.49it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.43it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.26it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.22it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.02it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.96it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.05it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.26it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.40it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.46it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.19it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.19it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.05it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.95it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.00it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.14it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.23it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.38it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.47it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.45it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.38it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.26it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.05it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 43.85it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.10it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.21it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.40it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.43it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.37it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.29it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.28it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.13it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.03it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.10it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.20it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.41it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.44it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.37it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.29it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.23it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.13it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.97it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.06it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.10it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.36it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.42it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.46it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.27it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.27it/s][A 80%|████████  | 492/615 [03:19<00:34,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:31:40,910 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 09:31:40,929 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:31:42,728 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:31:42,739 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:31:42,747 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492/special_tokens_map.json
 80%|████████  | 493/615 [03:22<10:22,  5.11s/it] 80%|████████  | 494/615 [03:22<07:22,  3.66s/it] 80%|████████  | 495/615 [03:22<05:17,  2.65s/it] 81%|████████  | 496/615 [03:23<03:50,  1.94s/it] 81%|████████  | 497/615 [03:23<02:49,  1.44s/it] 81%|████████  | 498/615 [03:23<02:07,  1.09s/it] 81%|████████  | 499/615 [03:23<01:38,  1.18it/s] 81%|████████▏ | 500/615 [03:24<01:17,  1.48it/s]                                                  81%|████████▏ | 500/615 [03:24<01:17,  1.48it/s] 81%|████████▏ | 501/615 [03:24<01:03,  1.80it/s] 82%|████████▏ | 502/615 [03:24<00:53,  2.11it/s] 82%|████████▏ | 503/615 [03:25<00:46,  2.41it/s] 82%|████████▏ | 504/615 [03:25<00:41,  2.68it/s] 82%|████████▏ | 505/615 [03:25<00:38,  2.89it/s] 82%|████████▏ | 506/615 [03:25<00:35,  3.08it/s] 82%|████████▏ | 507/615 [03:26<00:33,  3.22it/s] 83%|████████▎ | 508/615 [03:26<00:32,  3.33it/s] 83%|████████▎ | 509/615 [03:26<00:31,  3.40it/s] 83%|████████▎ | 510/615 [03:26<00:30,  3.46it/s] 83%|████████▎ | 511/615 [03:27<00:29,  3.50it/s] 83%|████████▎ | 512/615 [03:27<00:29,  3.53it/s] 83%|████████▎ | 513/615 [03:27<00:28,  3.55it/s] 84%|████████▎ | 514/615 [03:28<00:28,  3.57it/s] 84%|████████▎ | 515/615 [03:28<00:27,  3.58it/s] 84%|████████▍ | 516/615 [03:28<00:27,  3.56it/s] 84%|████████▍ | 517/615 [03:28<00:27,  3.57it/s] 84%|████████▍ | 518/615 [03:29<00:27,  3.58it/s] 84%|████████▍ | 519/615 [03:29<00:26,  3.59it/s] 85%|████████▍ | 520/615 [03:29<00:26,  3.59it/s] 85%|████████▍ | 521/615 [03:30<00:26,  3.59it/s] 85%|████████▍ | 522/615 [03:30<00:25,  3.59it/s] 85%|████████▌ | 523/615 [03:30<00:25,  3.60it/s] 85%|████████▌ | 524/615 [03:30<00:25,  3.60it/s] 85%|████████▌ | 525/615 [03:31<00:24,  3.60it/s] 86%|████████▌ | 526/615 [03:31<00:24,  3.60it/s] 86%|████████▌ | 527/615 [03:31<00:24,  3.61it/s] 86%|████████▌ | 528/615 [03:31<00:24,  3.61it/s] 86%|████████▌ | 529/615 [03:32<00:23,  3.61it/s] 86%|████████▌ | 530/615 [03:32<00:23,  3.60it/s] 86%|████████▋ | 531/615 [03:32<00:23,  3.60it/s] 87%|████████▋ | 532/615 [03:33<00:23,  3.60it/s] 87%|████████▋ | 533/615 [03:33<00:22,  3.60it/s] 87%|████████▋ | 534/615 [03:33<00:22,  3.60it/s] 87%|████████▋ | 535/615 [03:33<00:22,  3.61it/s] 87%|████████▋ | 536/615 [03:34<00:21,  3.60it/s] 87%|████████▋ | 537/615 [03:34<00:21,  3.58it/s] 87%|████████▋ | 538/615 [03:34<00:21,  3.59it/s] 88%|████████▊ | 539/615 [03:35<00:21,  3.59it/s] 88%|████████▊ | 540/615 [03:35<00:20,  3.59it/s] 88%|████████▊ | 541/615 [03:35<00:20,  3.60it/s] 88%|████████▊ | 542/615 [03:35<00:20,  3.60it/s] 88%|████████▊ | 543/615 [03:36<00:19,  3.60it/s] 88%|████████▊ | 544/615 [03:36<00:19,  3.60it/s] 89%|████████▊ | 545/615 [03:36<00:19,  3.60it/s] 89%|████████▉ | 546/615 [03:36<00:19,  3.61it/s] 89%|████████▉ | 547/615 [03:37<00:18,  3.60it/s] 89%|████████▉ | 548/615 [03:37<00:18,  3.59it/s] 89%|████████▉ | 549/615 [03:37<00:18,  3.60it/s] 89%|████████▉ | 550/615 [03:38<00:18,  3.60it/s] 90%|████████▉ | 551/615 [03:38<00:17,  3.60it/s] 90%|████████▉ | 552/615 [03:38<00:17,  3.60it/s] 90%|████████▉ | 553/615 [03:38<00:17,  3.60it/s] 90%|█████████ | 554/615 [03:39<00:16,  3.60it/s] 90%|█████████ | 555/615 [03:39<00:16,  3.60it/s] 90%|█████████ | 556/615 [03:39<00:16,  3.60it/s] 91%|█████████ | 557/615 [03:40<00:16,  3.60it/s] 91%|█████████ | 558/615 [03:40<00:15,  3.60it/s] 91%|█████████ | 559/615 [03:40<00:15,  3.59it/s] 91%|█████████ | 560/615 [03:40<00:15,  3.59it/s] 91%|█████████ | 561/615 [03:41<00:15,  3.60it/s] 91%|█████████▏| 562/615 [03:41<00:14,  3.60it/s] 92%|█████████▏| 563/615 [03:41<00:14,  3.60it/s] 92%|█████████▏| 564/615 [03:41<00:14,  3.60it/s] 92%|█████████▏| 565/615 [03:42<00:13,  3.60it/s] 92%|█████████▏| 566/615 [03:42<00:13,  3.60it/s] 92%|█████████▏| 567/615 [03:42<00:13,  3.60it/s] 92%|█████████▏| 568/615 [03:43<00:13,  3.60it/s] 93%|█████████▎| 569/615 [03:43<00:12,  3.60it/s] 93%|█████████▎| 570/615 [03:43<00:12,  3.57it/s] 93%|█████████▎| 571/615 [03:43<00:12,  3.58it/s] 93%|█████████▎| 572/615 [03:44<00:11,  3.59it/s] 93%|█████████▎| 573/615 [03:44<00:11,  3.59it/s] 93%|█████████▎| 574/615 [03:44<00:11,  3.59it/s] 93%|█████████▎| 575/615 [03:45<00:11,  3.60it/s] 94%|█████████▎| 576/615 [03:45<00:10,  3.60it/s] 94%|█████████▍| 577/615 [03:45<00:10,  3.60it/s] 94%|█████████▍| 578/615 [03:45<00:10,  3.60it/s] 94%|█████████▍| 579/615 [03:46<00:10,  3.60it/s] 94%|█████████▍| 580/615 [03:46<00:09,  3.60it/s] 94%|█████████▍| 581/615 [03:46<00:09,  3.58it/s] 95%|█████████▍| 582/615 [03:46<00:09,  3.59it/s] 95%|█████████▍| 583/615 [03:47<00:08,  3.59it/s] 95%|█████████▍| 584/615 [03:47<00:08,  3.59it/s] 95%|█████████▌| 585/615 [03:47<00:08,  3.60it/s] 95%|█████████▌| 586/615 [03:48<00:08,  3.60it/s] 95%|█████████▌| 587/615 [03:48<00:07,  3.60it/s] 96%|█████████▌| 588/615 [03:48<00:07,  3.60it/s] 96%|█████████▌| 589/615 [03:48<00:07,  3.60it/s] 96%|█████████▌| 590/615 [03:49<00:06,  3.60it/s] 96%|█████████▌| 591/615 [03:49<00:06,  3.60it/s] 96%|█████████▋| 592/615 [03:49<00:06,  3.58it/s] 96%|█████████▋| 593/615 [03:50<00:06,  3.58it/s] 97%|█████████▋| 594/615 [03:50<00:05,  3.57it/s] 97%|█████████▋| 595/615 [03:50<00:05,  3.57it/s] 97%|█████████▋| 596/615 [03:50<00:05,  3.58it/s] 97%|█████████▋| 597/615 [03:51<00:05,  3.59it/s] 97%|█████████▋| 598/615 [03:51<00:04,  3.57it/s] 97%|█████████▋| 599/615 [03:51<00:04,  3.46it/s] 98%|█████████▊| 600/615 [03:52<00:04,  3.48it/s] 98%|█████████▊| 601/615 [03:52<00:03,  3.51it/s] 98%|█████████▊| 602/615 [03:52<00:03,  3.54it/s] 98%|█████████▊| 603/615 [03:52<00:03,  3.54it/s] 98%|█████████▊| 604/615 [03:53<00:03,  3.56it/s] 98%|█████████▊| 605/615 [03:53<00:02,  3.57it/s] 99%|█████████▊| 606/615 [03:53<00:02,  3.58it/s] 99%|█████████▊| 607/615 [03:53<00:02,  3.59it/s] 99%|█████████▉| 608/615 [03:54<00:01,  3.59it/s] 99%|█████████▉| 609/615 [03:54<00:01,  3.59it/s] 99%|█████████▉| 610/615 [03:54<00:01,  3.59it/s] 99%|█████████▉| 611/615 [03:55<00:01,  3.59it/s]100%|█████████▉| 612/615 [03:55<00:00,  3.60it/s]100%|█████████▉| 613/615 [03:55<00:00,  3.60it/s]100%|█████████▉| 614/615 [03:55<00:00,  3.59it/s]100%|██████████| 615/615 [03:56<00:00,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 09:32:17,336 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:32:17,336 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 09:32:17,336 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8047, 'eval_samples_per_second': 353.648, 'eval_steps_per_second': 44.26, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.19it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.61it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.91it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.95it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.31it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.64it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.31it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.00it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.19it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.38it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.40it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.55it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.52it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.38it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.18it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.06it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.07it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.13it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.25it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.38it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.51it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.49it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.33it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.21it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.05it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.05it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.15it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.32it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.42it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.48it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.41it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.29it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.14it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.12it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.08it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.14it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.21it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.36it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.49it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.36it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.33it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.16it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.07it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.18it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.15it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.27it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.40it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.47it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.33it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.27it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.14it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.18it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.20it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.20it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.17it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.36it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.41it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.28it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.21it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.20it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.17it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.23it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.25it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.25it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.34it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.40it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.28it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.22it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.15it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.24it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.27it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.28it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.30it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.37it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.35it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.32it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.26it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.22it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.16it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.26it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.23it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.28it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.27it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.37it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.27it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.21it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.12it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.15it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.25it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.23it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.20it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.32it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.40it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.34it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.27it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.12it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.07it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.19it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.29it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.30it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.43it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.37it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.29it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.18it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.10it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.02it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.19it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.30it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.37it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.48it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.41it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.38it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.13it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.10it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.15it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.20it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.32it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.37it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.51it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.38it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.32it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.32it/s][A100%|██████████| 615/615 [04:09<00:00,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:32:31,166 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-28 09:32:31,185 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:32:32,911 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:32:32,929 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:32:32,937 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 09:32:33,219 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 09:32:33,219 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123 (score: 0.9401851296424866).
                                                 100%|██████████| 615/615 [04:14<00:00,  3.59it/s]100%|██████████| 615/615 [04:14<00:00,  2.42it/s]
[INFO|trainer.py:1894] 2023-08-28 09:32:35,134 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 09:32:35,152 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:32:36,802 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:32:36,818 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:32:36,829 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:32:37,036 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:37,037 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:37,037 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:37,037 >>   train_runtime            = 0:04:14.00
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:37,037 >>   train_samples            =       7899
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:37,037 >>   train_samples_per_second =     155.49
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:37,037 >>   train_steps_per_second   =      2.421
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.7917, 'eval_samples_per_second': 353.981, 'eval_steps_per_second': 44.302, 'epoch': 5.0}
{'train_runtime': 254.0035, 'train_samples_per_second': 155.49, 'train_steps_per_second': 2.421, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 09:32:37 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 09:32:37,079 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:32:37,080 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 09:32:37,080 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 56.34it/s]  2%|▏         | 12/611 [00:00<00:12, 48.97it/s]  3%|▎         | 17/611 [00:00<00:12, 47.30it/s]  4%|▎         | 22/611 [00:00<00:12, 46.33it/s]  4%|▍         | 27/611 [00:00<00:12, 45.89it/s]  5%|▌         | 32/611 [00:00<00:12, 45.70it/s]  6%|▌         | 37/611 [00:00<00:12, 45.62it/s]  7%|▋         | 42/611 [00:00<00:12, 45.03it/s]  8%|▊         | 47/611 [00:01<00:12, 44.30it/s]  9%|▊         | 52/611 [00:01<00:12, 43.91it/s]  9%|▉         | 57/611 [00:01<00:12, 44.04it/s] 10%|█         | 62/611 [00:01<00:12, 44.16it/s] 11%|█         | 67/611 [00:01<00:12, 44.36it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.47it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.62it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.76it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.66it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.27it/s] 16%|█▌        | 97/611 [00:02<00:11, 44.03it/s] 17%|█▋        | 102/611 [00:02<00:11, 44.09it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.18it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.27it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.47it/s] 20%|█▉        | 122/611 [00:02<00:10, 44.65it/s] 21%|██        | 127/611 [00:02<00:10, 44.59it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.52it/s] 22%|██▏       | 137/611 [00:03<00:10, 44.32it/s] 23%|██▎       | 142/611 [00:03<00:10, 44.14it/s] 24%|██▍       | 147/611 [00:03<00:10, 44.09it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.21it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.28it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.51it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.65it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.59it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.50it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.23it/s] 31%|███       | 187/611 [00:04<00:09, 44.13it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.06it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.15it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.30it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.52it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.55it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.62it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.48it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.29it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.20it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.05it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.12it/s] 40%|████      | 247/611 [00:05<00:08, 44.47it/s] 41%|████      | 252/611 [00:05<00:08, 44.56it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.52it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.49it/s] 44%|████▎     | 267/611 [00:05<00:07, 44.43it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.34it/s] 45%|████▌     | 277/611 [00:06<00:07, 44.14it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.07it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.18it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.47it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.56it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.58it/s] 50%|█████     | 307/611 [00:06<00:06, 44.48it/s] 51%|█████     | 312/611 [00:07<00:06, 44.33it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.30it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.19it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.07it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.19it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.38it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.55it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.56it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.47it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.40it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.01it/s] 60%|██████    | 367/611 [00:08<00:05, 44.05it/s] 61%|██████    | 372/611 [00:08<00:05, 44.03it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.09it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.28it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.48it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.55it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.50it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.31it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.25it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.22it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.20it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.28it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.43it/s] 71%|███████   | 432/611 [00:09<00:04, 44.51it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.48it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.52it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.35it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.26it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.13it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.17it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.20it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.39it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.50it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.61it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.44it/s] 81%|████████  | 492/611 [00:11<00:02, 44.38it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.26it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.21it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.22it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.29it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.38it/s] 85%|████████▌ | 522/611 [00:11<00:02, 44.46it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.58it/s] 87%|████████▋ | 532/611 [00:11<00:01, 44.46it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.39it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.25it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.20it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.21it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.25it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.35it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.43it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.54it/s] 94%|█████████▍| 577/611 [00:12<00:00, 44.43it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.44it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.35it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.26it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.21it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.24it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.42it/s]100%|██████████| 611/611 [00:13<00:00, 44.45it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:32:50,842 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:50,843 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:50,843 >>   eval_loss               =     0.9402
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:50,843 >>   eval_runtime            = 0:00:13.76
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:50,843 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:50,843 >>   eval_samples_per_second =    354.726
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:50,843 >>   eval_steps_per_second   =     44.395
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:32:50,843 >>   perplexity              =     2.5605
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:57,675 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:57,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:57,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:57,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:32:57,679 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:32:58,315 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:32:58,315 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:32:58,895 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:32:59,934 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:32:59,935 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:02,768 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:02,776 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:02,776 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:02,776 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:33:02,776 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:33:03,423 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:33:03,424 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:33:03,998 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:33:04,174 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:33:04,174 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.74it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.72it/s]Extractor Predicting: 9it [00:05,  1.80it/s]Extractor Predicting: 10it [00:05,  1.88it/s]Extractor Predicting: 11it [00:06,  1.85it/s]Extractor Predicting: 12it [00:06,  1.79it/s]Extractor Predicting: 13it [00:07,  1.82it/s]Extractor Predicting: 14it [00:07,  1.85it/s]Extractor Predicting: 15it [00:08,  1.83it/s]Extractor Predicting: 16it [00:08,  1.85it/s]Extractor Predicting: 17it [00:09,  1.86it/s]Extractor Predicting: 18it [00:10,  1.74it/s]Extractor Predicting: 19it [00:10,  1.81it/s]Extractor Predicting: 20it [00:11,  1.84it/s]Extractor Predicting: 21it [00:11,  1.86it/s]Extractor Predicting: 22it [00:12,  1.94it/s]Extractor Predicting: 23it [00:12,  1.94it/s]Extractor Predicting: 24it [00:13,  1.95it/s]Extractor Predicting: 25it [00:13,  1.91it/s]Extractor Predicting: 26it [00:14,  1.88it/s]Extractor Predicting: 27it [00:14,  1.89it/s]Extractor Predicting: 28it [00:15,  1.88it/s]Extractor Predicting: 29it [00:15,  1.88it/s]Extractor Predicting: 30it [00:16,  1.89it/s]Extractor Predicting: 31it [00:16,  1.90it/s]Extractor Predicting: 32it [00:17,  1.94it/s]Extractor Predicting: 33it [00:17,  1.91it/s]Extractor Predicting: 34it [00:18,  1.88it/s]Extractor Predicting: 35it [00:19,  1.88it/s]Extractor Predicting: 36it [00:19,  1.85it/s]Extractor Predicting: 37it [00:20,  1.90it/s]Extractor Predicting: 38it [00:20,  1.89it/s]Extractor Predicting: 39it [00:21,  1.87it/s]Extractor Predicting: 40it [00:21,  1.90it/s]Extractor Predicting: 41it [00:22,  1.87it/s]Extractor Predicting: 42it [00:22,  1.87it/s]Extractor Predicting: 43it [00:23,  1.86it/s]Extractor Predicting: 44it [00:23,  1.86it/s]Extractor Predicting: 45it [00:24,  1.90it/s]Extractor Predicting: 46it [00:24,  1.88it/s]Extractor Predicting: 47it [00:25,  1.88it/s]Extractor Predicting: 48it [00:25,  1.88it/s]Extractor Predicting: 49it [00:26,  1.88it/s]Extractor Predicting: 50it [00:27,  1.83it/s]Extractor Predicting: 51it [00:27,  1.84it/s]Extractor Predicting: 52it [00:28,  1.89it/s]Extractor Predicting: 53it [00:28,  1.86it/s]Extractor Predicting: 54it [00:29,  1.84it/s]Extractor Predicting: 55it [00:29,  1.77it/s]Extractor Predicting: 56it [00:30,  1.73it/s]Extractor Predicting: 57it [00:30,  1.76it/s]Extractor Predicting: 58it [00:31,  1.75it/s]Extractor Predicting: 59it [00:32,  1.78it/s]Extractor Predicting: 60it [00:32,  1.77it/s]Extractor Predicting: 61it [00:33,  1.77it/s]Extractor Predicting: 62it [00:33,  1.73it/s]Extractor Predicting: 63it [00:34,  1.75it/s]Extractor Predicting: 64it [00:34,  1.75it/s]Extractor Predicting: 65it [00:35,  1.73it/s]Extractor Predicting: 66it [00:36,  1.68it/s]Extractor Predicting: 67it [00:36,  1.71it/s]Extractor Predicting: 68it [00:37,  1.76it/s]Extractor Predicting: 69it [00:37,  1.75it/s]Extractor Predicting: 70it [00:38,  1.72it/s]Extractor Predicting: 71it [00:39,  1.74it/s]Extractor Predicting: 72it [00:39,  1.70it/s]Extractor Predicting: 73it [00:40,  1.74it/s]Extractor Predicting: 74it [00:40,  1.74it/s]Extractor Predicting: 75it [00:41,  1.75it/s]Extractor Predicting: 76it [00:41,  1.76it/s]Extractor Predicting: 77it [00:42,  1.75it/s]Extractor Predicting: 78it [00:43,  1.79it/s]Extractor Predicting: 79it [00:43,  1.81it/s]Extractor Predicting: 80it [00:44,  1.80it/s]Extractor Predicting: 81it [00:44,  1.80it/s]Extractor Predicting: 82it [00:45,  1.76it/s]Extractor Predicting: 83it [00:45,  1.77it/s]Extractor Predicting: 84it [00:46,  1.76it/s]Extractor Predicting: 85it [00:46,  1.74it/s]Extractor Predicting: 86it [00:47,  1.72it/s]Extractor Predicting: 87it [00:48,  1.72it/s]Extractor Predicting: 88it [00:48,  1.69it/s]Extractor Predicting: 89it [00:49,  1.69it/s]Extractor Predicting: 90it [00:49,  1.69it/s]Extractor Predicting: 91it [00:50,  1.70it/s]Extractor Predicting: 92it [00:51,  1.77it/s]Extractor Predicting: 93it [00:51,  1.85it/s]Extractor Predicting: 94it [00:52,  1.84it/s]Extractor Predicting: 95it [00:52,  1.83it/s]Extractor Predicting: 96it [00:53,  1.83it/s]Extractor Predicting: 97it [00:53,  1.85it/s]Extractor Predicting: 98it [00:54,  1.81it/s]Extractor Predicting: 99it [00:54,  1.74it/s]Extractor Predicting: 100it [00:55,  1.78it/s]Extractor Predicting: 101it [00:56,  1.68it/s]Extractor Predicting: 102it [00:56,  1.67it/s]Extractor Predicting: 103it [00:57,  1.70it/s]Extractor Predicting: 104it [00:57,  1.73it/s]Extractor Predicting: 105it [00:58,  1.75it/s]Extractor Predicting: 106it [00:58,  1.81it/s]Extractor Predicting: 107it [00:59,  1.81it/s]Extractor Predicting: 108it [00:59,  1.84it/s]Extractor Predicting: 109it [01:00,  1.83it/s]Extractor Predicting: 110it [01:01,  1.83it/s]Extractor Predicting: 111it [01:01,  1.86it/s]Extractor Predicting: 112it [01:02,  1.86it/s]Extractor Predicting: 113it [01:02,  1.80it/s]Extractor Predicting: 114it [01:03,  1.60it/s]Extractor Predicting: 115it [01:04,  1.66it/s]Extractor Predicting: 116it [01:04,  1.66it/s]Extractor Predicting: 117it [01:05,  1.66it/s]Extractor Predicting: 118it [01:05,  1.68it/s]Extractor Predicting: 119it [01:06,  1.67it/s]Extractor Predicting: 120it [01:07,  1.64it/s]Extractor Predicting: 121it [01:07,  1.66it/s]Extractor Predicting: 122it [01:08,  1.67it/s]Extractor Predicting: 123it [01:08,  1.71it/s]Extractor Predicting: 124it [01:09,  1.71it/s]Extractor Predicting: 125it [01:09,  1.73it/s]Extractor Predicting: 126it [01:10,  1.71it/s]Extractor Predicting: 127it [01:11,  1.70it/s]Extractor Predicting: 128it [01:11,  1.70it/s]Extractor Predicting: 129it [01:12,  1.74it/s]Extractor Predicting: 130it [01:12,  1.69it/s]Extractor Predicting: 131it [01:13,  1.70it/s]Extractor Predicting: 132it [01:14,  1.75it/s]Extractor Predicting: 133it [01:14,  1.72it/s]Extractor Predicting: 134it [01:15,  1.73it/s]Extractor Predicting: 135it [01:15,  1.70it/s]Extractor Predicting: 136it [01:16,  1.72it/s]Extractor Predicting: 137it [01:17,  1.69it/s]Extractor Predicting: 138it [01:17,  1.72it/s]Extractor Predicting: 139it [01:18,  1.69it/s]Extractor Predicting: 140it [01:18,  1.68it/s]Extractor Predicting: 141it [01:19,  1.71it/s]Extractor Predicting: 142it [01:19,  1.72it/s]Extractor Predicting: 143it [01:20,  1.70it/s]Extractor Predicting: 144it [01:21,  1.75it/s]Extractor Predicting: 145it [01:21,  1.80it/s]Extractor Predicting: 146it [01:22,  1.78it/s]Extractor Predicting: 147it [01:22,  1.74it/s]Extractor Predicting: 148it [01:23,  1.75it/s]Extractor Predicting: 149it [01:23,  1.73it/s]Extractor Predicting: 150it [01:24,  1.71it/s]Extractor Predicting: 151it [01:25,  1.70it/s]Extractor Predicting: 152it [01:25,  1.71it/s]Extractor Predicting: 153it [01:26,  1.71it/s]Extractor Predicting: 154it [01:26,  1.71it/s]Extractor Predicting: 155it [01:27,  1.72it/s]Extractor Predicting: 156it [01:28,  1.65it/s]Extractor Predicting: 157it [01:28,  1.60it/s]Extractor Predicting: 158it [01:29,  1.57it/s]Extractor Predicting: 159it [01:30,  1.61it/s]Extractor Predicting: 160it [01:30,  1.64it/s]Extractor Predicting: 161it [01:31,  1.66it/s]Extractor Predicting: 162it [01:31,  1.67it/s]Extractor Predicting: 163it [01:32,  1.67it/s]Extractor Predicting: 164it [01:32,  1.71it/s]Extractor Predicting: 165it [01:33,  1.71it/s]Extractor Predicting: 166it [01:34,  1.74it/s]Extractor Predicting: 167it [01:34,  1.73it/s]Extractor Predicting: 168it [01:35,  1.73it/s]Extractor Predicting: 169it [01:35,  1.75it/s]Extractor Predicting: 170it [01:36,  1.74it/s]Extractor Predicting: 171it [01:36,  1.76it/s]Extractor Predicting: 172it [01:37,  1.79it/s]Extractor Predicting: 173it [01:38,  1.73it/s]Extractor Predicting: 174it [01:38,  1.75it/s]Extractor Predicting: 175it [01:39,  1.72it/s]Extractor Predicting: 176it [01:39,  1.74it/s]Extractor Predicting: 177it [01:40,  1.71it/s]Extractor Predicting: 178it [01:40,  1.72it/s]Extractor Predicting: 179it [01:41,  1.71it/s]Extractor Predicting: 180it [01:42,  1.72it/s]Extractor Predicting: 181it [01:42,  1.73it/s]Extractor Predicting: 182it [01:43,  1.71it/s]Extractor Predicting: 183it [01:43,  1.67it/s]Extractor Predicting: 184it [01:44,  1.69it/s]Extractor Predicting: 185it [01:45,  1.80it/s]Extractor Predicting: 185it [01:45,  1.76it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:34:58,162 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:34:58,163 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:34:58,163 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:34:58,164 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:34:58,164 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:34:58,791 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:34:58,791 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:34:59,067 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:35:00,092 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:35:00,094 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:35:01,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:35:01,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:35:01,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:35:01,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:35:01,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:35:01,788 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:35:01,789 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:35:02,050 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:35:02,184 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:35:02,184 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.75it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.78it/s]Extractor Predicting: 7it [00:03,  1.79it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.76it/s]Extractor Predicting: 11it [00:06,  1.79it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.81it/s]Extractor Predicting: 14it [00:07,  1.80it/s]Extractor Predicting: 15it [00:08,  1.82it/s]Extractor Predicting: 16it [00:08,  1.80it/s]Extractor Predicting: 17it [00:09,  1.80it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:10,  1.78it/s]Extractor Predicting: 20it [00:11,  1.75it/s]Extractor Predicting: 21it [00:11,  1.74it/s]Extractor Predicting: 22it [00:12,  1.75it/s]Extractor Predicting: 23it [00:13,  1.74it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.78it/s]Extractor Predicting: 26it [00:14,  1.77it/s]Extractor Predicting: 27it [00:15,  1.79it/s]Extractor Predicting: 28it [00:15,  1.79it/s]Extractor Predicting: 29it [00:16,  1.78it/s]Extractor Predicting: 30it [00:16,  1.87it/s]Extractor Predicting: 31it [00:17,  1.75it/s]Extractor Predicting: 32it [00:17,  1.80it/s]Extractor Predicting: 33it [00:18,  1.82it/s]Extractor Predicting: 34it [00:19,  1.85it/s]Extractor Predicting: 35it [00:19,  1.87it/s]Extractor Predicting: 36it [00:20,  1.85it/s]Extractor Predicting: 37it [00:20,  1.88it/s]Extractor Predicting: 38it [00:21,  1.87it/s]Extractor Predicting: 39it [00:21,  1.81it/s]Extractor Predicting: 40it [00:22,  1.83it/s]Extractor Predicting: 41it [00:22,  1.86it/s]Extractor Predicting: 42it [00:23,  1.87it/s]Extractor Predicting: 43it [00:23,  1.88it/s]Extractor Predicting: 44it [00:24,  1.90it/s]Extractor Predicting: 45it [00:24,  1.92it/s]Extractor Predicting: 46it [00:25,  1.95it/s]Extractor Predicting: 47it [00:25,  1.96it/s]Extractor Predicting: 48it [00:26,  1.95it/s]Extractor Predicting: 49it [00:26,  1.93it/s]Extractor Predicting: 50it [00:27,  1.91it/s]Extractor Predicting: 51it [00:27,  1.93it/s]Extractor Predicting: 52it [00:28,  1.91it/s]Extractor Predicting: 53it [00:29,  1.90it/s]Extractor Predicting: 54it [00:29,  1.85it/s]Extractor Predicting: 55it [00:30,  1.84it/s]Extractor Predicting: 56it [00:30,  1.84it/s]Extractor Predicting: 57it [00:31,  1.87it/s]Extractor Predicting: 58it [00:31,  1.95it/s]Extractor Predicting: 59it [00:32,  1.90it/s]Extractor Predicting: 60it [00:32,  1.87it/s]Extractor Predicting: 61it [00:33,  1.82it/s]Extractor Predicting: 62it [00:33,  1.79it/s]Extractor Predicting: 63it [00:34,  1.74it/s]Extractor Predicting: 64it [00:35,  1.70it/s]Extractor Predicting: 65it [00:35,  1.69it/s]Extractor Predicting: 66it [00:36,  1.67it/s]Extractor Predicting: 67it [00:37,  1.65it/s]Extractor Predicting: 68it [00:37,  1.67it/s]Extractor Predicting: 69it [00:38,  1.68it/s]Extractor Predicting: 70it [00:38,  1.71it/s]Extractor Predicting: 71it [00:39,  1.73it/s]Extractor Predicting: 72it [00:39,  1.75it/s]Extractor Predicting: 73it [00:40,  1.80it/s]Extractor Predicting: 74it [00:40,  1.81it/s]Extractor Predicting: 75it [00:41,  1.82it/s]Extractor Predicting: 76it [00:42,  1.83it/s]Extractor Predicting: 77it [00:42,  1.85it/s]Extractor Predicting: 78it [00:43,  1.83it/s]Extractor Predicting: 79it [00:43,  1.90it/s]Extractor Predicting: 80it [00:44,  1.94it/s]Extractor Predicting: 81it [00:44,  1.89it/s]Extractor Predicting: 82it [00:45,  1.91it/s]Extractor Predicting: 83it [00:45,  1.86it/s]Extractor Predicting: 84it [00:46,  1.83it/s]Extractor Predicting: 85it [00:46,  1.77it/s]Extractor Predicting: 86it [00:47,  1.75it/s]Extractor Predicting: 87it [00:48,  1.75it/s]Extractor Predicting: 88it [00:48,  1.78it/s]Extractor Predicting: 89it [00:49,  1.76it/s]Extractor Predicting: 90it [00:49,  1.78it/s]Extractor Predicting: 91it [00:50,  1.76it/s]Extractor Predicting: 92it [00:50,  1.76it/s]Extractor Predicting: 93it [00:51,  1.79it/s]Extractor Predicting: 94it [00:51,  1.81it/s]Extractor Predicting: 95it [00:52,  1.79it/s]Extractor Predicting: 96it [00:53,  1.78it/s]Extractor Predicting: 97it [00:53,  1.79it/s]Extractor Predicting: 98it [00:54,  1.78it/s]Extractor Predicting: 99it [00:54,  1.78it/s]Extractor Predicting: 100it [00:55,  1.77it/s]Extractor Predicting: 101it [00:55,  1.79it/s]Extractor Predicting: 102it [00:56,  1.80it/s]Extractor Predicting: 103it [00:57,  1.75it/s]Extractor Predicting: 104it [00:57,  1.79it/s]Extractor Predicting: 105it [00:58,  1.81it/s]Extractor Predicting: 106it [00:58,  1.83it/s]Extractor Predicting: 107it [00:59,  1.81it/s]Extractor Predicting: 108it [00:59,  1.83it/s]Extractor Predicting: 109it [01:00,  1.81it/s]Extractor Predicting: 110it [01:00,  1.83it/s]Extractor Predicting: 111it [01:01,  1.83it/s]Extractor Predicting: 112it [01:01,  1.80it/s]Extractor Predicting: 113it [01:02,  1.78it/s]Extractor Predicting: 114it [01:03,  1.78it/s]Extractor Predicting: 115it [01:03,  1.77it/s]Extractor Predicting: 116it [01:04,  1.78it/s]Extractor Predicting: 117it [01:04,  1.82it/s]Extractor Predicting: 118it [01:05,  1.80it/s]Extractor Predicting: 119it [01:05,  1.78it/s]Extractor Predicting: 120it [01:06,  1.82it/s]Extractor Predicting: 121it [01:06,  1.85it/s]Extractor Predicting: 122it [01:07,  1.83it/s]Extractor Predicting: 123it [01:08,  1.80it/s]Extractor Predicting: 124it [01:08,  1.77it/s]Extractor Predicting: 125it [01:09,  1.78it/s]Extractor Predicting: 126it [01:09,  1.77it/s]Extractor Predicting: 127it [01:10,  1.81it/s]Extractor Predicting: 128it [01:10,  1.75it/s]Extractor Predicting: 129it [01:11,  1.77it/s]Extractor Predicting: 130it [01:12,  1.81it/s]Extractor Predicting: 131it [01:12,  1.78it/s]Extractor Predicting: 132it [01:13,  1.78it/s]Extractor Predicting: 133it [01:13,  1.78it/s]Extractor Predicting: 134it [01:14,  1.76it/s]Extractor Predicting: 135it [01:14,  1.76it/s]Extractor Predicting: 136it [01:15,  1.81it/s]Extractor Predicting: 137it [01:16,  1.74it/s]Extractor Predicting: 138it [01:16,  1.56it/s]Extractor Predicting: 139it [01:17,  1.63it/s]Extractor Predicting: 140it [01:17,  1.68it/s]Extractor Predicting: 141it [01:18,  1.70it/s]Extractor Predicting: 142it [01:19,  1.74it/s]Extractor Predicting: 143it [01:19,  1.65it/s]Extractor Predicting: 144it [01:20,  1.72it/s]Extractor Predicting: 145it [01:20,  1.70it/s]Extractor Predicting: 146it [01:21,  1.75it/s]Extractor Predicting: 147it [01:21,  1.81it/s]Extractor Predicting: 148it [01:22,  1.79it/s]Extractor Predicting: 149it [01:22,  1.84it/s]Extractor Predicting: 150it [01:23,  1.85it/s]Extractor Predicting: 151it [01:24,  1.87it/s]Extractor Predicting: 152it [01:24,  1.82it/s]Extractor Predicting: 153it [01:25,  1.84it/s]Extractor Predicting: 154it [01:25,  1.79it/s]Extractor Predicting: 155it [01:26,  1.79it/s]Extractor Predicting: 156it [01:26,  1.85it/s]Extractor Predicting: 157it [01:27,  1.81it/s]Extractor Predicting: 158it [01:27,  1.78it/s]Extractor Predicting: 159it [01:28,  1.81it/s]Extractor Predicting: 160it [01:29,  1.81it/s]Extractor Predicting: 161it [01:29,  1.85it/s]Extractor Predicting: 162it [01:30,  1.83it/s]Extractor Predicting: 163it [01:30,  1.82it/s]Extractor Predicting: 164it [01:31,  1.79it/s]Extractor Predicting: 165it [01:31,  1.74it/s]Extractor Predicting: 166it [01:32,  1.70it/s]Extractor Predicting: 167it [01:33,  1.71it/s]Extractor Predicting: 168it [01:33,  1.74it/s]Extractor Predicting: 169it [01:34,  1.78it/s]Extractor Predicting: 170it [01:34,  1.80it/s]Extractor Predicting: 171it [01:35,  1.81it/s]Extractor Predicting: 172it [01:35,  1.85it/s]Extractor Predicting: 173it [01:36,  1.84it/s]Extractor Predicting: 174it [01:36,  1.82it/s]Extractor Predicting: 175it [01:37,  1.82it/s]Extractor Predicting: 176it [01:37,  1.82it/s]Extractor Predicting: 177it [01:38,  1.79it/s]Extractor Predicting: 178it [01:39,  1.79it/s]Extractor Predicting: 179it [01:39,  1.75it/s]Extractor Predicting: 180it [01:40,  1.80it/s]Extractor Predicting: 181it [01:40,  1.79it/s]Extractor Predicting: 182it [01:41,  1.82it/s]Extractor Predicting: 183it [01:41,  1.85it/s]Extractor Predicting: 184it [01:42,  1.84it/s]Extractor Predicting: 185it [01:42,  1.84it/s]Extractor Predicting: 186it [01:43,  1.78it/s]Extractor Predicting: 187it [01:44,  1.80it/s]Extractor Predicting: 188it [01:44,  1.80it/s]Extractor Predicting: 189it [01:45,  1.80it/s]Extractor Predicting: 190it [01:45,  1.83it/s]Extractor Predicting: 191it [01:46,  1.83it/s]Extractor Predicting: 192it [01:46,  1.88it/s]Extractor Predicting: 193it [01:47,  1.85it/s]Extractor Predicting: 194it [01:47,  1.84it/s]Extractor Predicting: 195it [01:48,  1.83it/s]Extractor Predicting: 196it [01:48,  1.82it/s]Extractor Predicting: 197it [01:49,  1.81it/s]Extractor Predicting: 198it [01:50,  1.79it/s]Extractor Predicting: 199it [01:50,  1.79it/s]Extractor Predicting: 200it [01:51,  1.79it/s]Extractor Predicting: 201it [01:51,  1.80it/s]Extractor Predicting: 202it [01:52,  1.79it/s]Extractor Predicting: 203it [01:52,  1.83it/s]Extractor Predicting: 204it [01:53,  1.82it/s]Extractor Predicting: 205it [01:53,  1.84it/s]Extractor Predicting: 206it [01:54,  1.81it/s]Extractor Predicting: 207it [01:55,  1.83it/s]Extractor Predicting: 208it [01:55,  1.81it/s]Extractor Predicting: 209it [01:56,  1.78it/s]Extractor Predicting: 210it [01:56,  1.86it/s]Extractor Predicting: 211it [01:57,  1.83it/s]Extractor Predicting: 212it [01:57,  1.85it/s]Extractor Predicting: 213it [01:58,  1.85it/s]Extractor Predicting: 214it [01:58,  1.87it/s]Extractor Predicting: 215it [01:59,  1.86it/s]Extractor Predicting: 216it [01:59,  1.81it/s]Extractor Predicting: 217it [02:00,  1.84it/s]Extractor Predicting: 218it [02:01,  1.80it/s]Extractor Predicting: 219it [02:01,  1.80it/s]Extractor Predicting: 220it [02:02,  1.81it/s]Extractor Predicting: 221it [02:02,  1.83it/s]Extractor Predicting: 222it [02:03,  1.77it/s]Extractor Predicting: 223it [02:03,  1.70it/s]Extractor Predicting: 224it [02:04,  1.73it/s]Extractor Predicting: 225it [02:05,  1.76it/s]Extractor Predicting: 226it [02:05,  1.80it/s]Extractor Predicting: 227it [02:06,  1.81it/s]Extractor Predicting: 228it [02:06,  1.84it/s]Extractor Predicting: 229it [02:07,  1.87it/s]Extractor Predicting: 230it [02:07,  1.89it/s]Extractor Predicting: 231it [02:08,  1.89it/s]Extractor Predicting: 232it [02:08,  1.88it/s]Extractor Predicting: 233it [02:09,  1.86it/s]Extractor Predicting: 234it [02:09,  1.87it/s]Extractor Predicting: 235it [02:10,  1.83it/s]Extractor Predicting: 236it [02:10,  1.84it/s]Extractor Predicting: 237it [02:11,  1.84it/s]Extractor Predicting: 238it [02:12,  1.80it/s]Extractor Predicting: 239it [02:12,  1.83it/s]Extractor Predicting: 240it [02:13,  1.82it/s]Extractor Predicting: 241it [02:13,  1.83it/s]Extractor Predicting: 242it [02:14,  1.80it/s]Extractor Predicting: 243it [02:14,  1.75it/s]Extractor Predicting: 244it [02:15,  1.77it/s]Extractor Predicting: 245it [02:15,  1.81it/s]Extractor Predicting: 246it [02:16,  1.80it/s]Extractor Predicting: 247it [02:16,  1.84it/s]Extractor Predicting: 248it [02:17,  1.76it/s]Extractor Predicting: 249it [02:18,  1.75it/s]Extractor Predicting: 250it [02:18,  1.78it/s]Extractor Predicting: 251it [02:19,  1.78it/s]Extractor Predicting: 252it [02:19,  1.77it/s]Extractor Predicting: 253it [02:20,  1.73it/s]Extractor Predicting: 254it [02:21,  1.71it/s]Extractor Predicting: 255it [02:21,  1.73it/s]Extractor Predicting: 256it [02:22,  1.74it/s]Extractor Predicting: 257it [02:22,  1.76it/s]Extractor Predicting: 258it [02:23,  1.77it/s]Extractor Predicting: 259it [02:24,  1.55it/s]Extractor Predicting: 260it [02:24,  1.57it/s]Extractor Predicting: 261it [02:25,  1.64it/s]Extractor Predicting: 262it [02:25,  1.66it/s]Extractor Predicting: 263it [02:26,  1.69it/s]Extractor Predicting: 264it [02:27,  1.71it/s]Extractor Predicting: 265it [02:27,  1.74it/s]Extractor Predicting: 266it [02:28,  1.69it/s]Extractor Predicting: 267it [02:28,  1.69it/s]Extractor Predicting: 268it [02:29,  1.67it/s]Extractor Predicting: 269it [02:30,  1.68it/s]Extractor Predicting: 270it [02:30,  1.68it/s]Extractor Predicting: 271it [02:31,  1.67it/s]Extractor Predicting: 272it [02:31,  1.67it/s]Extractor Predicting: 273it [02:32,  1.67it/s]Extractor Predicting: 274it [02:33,  1.64it/s]Extractor Predicting: 275it [02:33,  1.65it/s]Extractor Predicting: 276it [02:34,  1.67it/s]Extractor Predicting: 277it [02:34,  1.69it/s]Extractor Predicting: 278it [02:35,  1.73it/s]Extractor Predicting: 279it [02:35,  1.70it/s]Extractor Predicting: 280it [02:36,  1.71it/s]Extractor Predicting: 281it [02:37,  1.67it/s]Extractor Predicting: 282it [02:37,  1.67it/s]Extractor Predicting: 283it [02:38,  1.70it/s]Extractor Predicting: 284it [02:38,  1.73it/s]Extractor Predicting: 285it [02:39,  1.72it/s]Extractor Predicting: 286it [02:40,  1.76it/s]Extractor Predicting: 287it [02:40,  1.70it/s]Extractor Predicting: 288it [02:41,  1.74it/s]Extractor Predicting: 289it [02:41,  1.76it/s]Extractor Predicting: 290it [02:42,  1.72it/s]Extractor Predicting: 291it [02:42,  1.68it/s]Extractor Predicting: 292it [02:43,  1.72it/s]Extractor Predicting: 293it [02:44,  1.72it/s]Extractor Predicting: 294it [02:44,  1.70it/s]Extractor Predicting: 295it [02:45,  1.70it/s]Extractor Predicting: 296it [02:45,  1.75it/s]Extractor Predicting: 297it [02:46,  1.74it/s]Extractor Predicting: 298it [02:47,  1.71it/s]Extractor Predicting: 299it [02:47,  1.72it/s]Extractor Predicting: 300it [02:48,  1.70it/s]Extractor Predicting: 301it [02:48,  1.72it/s]Extractor Predicting: 302it [02:49,  1.68it/s]Extractor Predicting: 303it [02:49,  1.72it/s]Extractor Predicting: 304it [02:50,  1.74it/s]Extractor Predicting: 305it [02:51,  1.78it/s]Extractor Predicting: 306it [02:51,  1.82it/s]Extractor Predicting: 307it [02:52,  1.80it/s]Extractor Predicting: 308it [02:52,  1.82it/s]Extractor Predicting: 309it [02:53,  1.78it/s]Extractor Predicting: 310it [02:53,  1.78it/s]Extractor Predicting: 311it [02:54,  1.74it/s]Extractor Predicting: 312it [02:54,  1.77it/s]Extractor Predicting: 313it [02:55,  1.68it/s]Extractor Predicting: 314it [02:56,  1.68it/s]Extractor Predicting: 315it [02:56,  1.72it/s]Extractor Predicting: 316it [02:57,  1.77it/s]Extractor Predicting: 317it [02:57,  1.79it/s]Extractor Predicting: 318it [02:58,  1.83it/s]Extractor Predicting: 319it [02:58,  1.79it/s]Extractor Predicting: 320it [02:59,  1.75it/s]Extractor Predicting: 321it [03:00,  1.75it/s]Extractor Predicting: 322it [03:00,  1.78it/s]Extractor Predicting: 323it [03:01,  1.71it/s]Extractor Predicting: 324it [03:01,  1.72it/s]Extractor Predicting: 325it [03:02,  1.75it/s]Extractor Predicting: 326it [03:02,  1.75it/s]Extractor Predicting: 327it [03:03,  1.76it/s]Extractor Predicting: 328it [03:04,  1.72it/s]Extractor Predicting: 329it [03:04,  1.74it/s]Extractor Predicting: 330it [03:05,  1.71it/s]Extractor Predicting: 331it [03:05,  1.74it/s]Extractor Predicting: 332it [03:06,  1.73it/s]Extractor Predicting: 333it [03:07,  1.71it/s]Extractor Predicting: 334it [03:07,  1.73it/s]Extractor Predicting: 335it [03:08,  1.72it/s]Extractor Predicting: 336it [03:08,  1.62it/s]Extractor Predicting: 337it [03:09,  1.63it/s]Extractor Predicting: 338it [03:10,  1.64it/s]Extractor Predicting: 339it [03:10,  1.67it/s]Extractor Predicting: 340it [03:11,  1.67it/s]Extractor Predicting: 341it [03:11,  1.66it/s]Extractor Predicting: 342it [03:12,  1.67it/s]Extractor Predicting: 343it [03:13,  1.71it/s]Extractor Predicting: 344it [03:13,  1.70it/s]Extractor Predicting: 345it [03:14,  1.65it/s]Extractor Predicting: 346it [03:14,  1.66it/s]Extractor Predicting: 347it [03:15,  1.68it/s]Extractor Predicting: 348it [03:16,  1.71it/s]Extractor Predicting: 349it [03:16,  1.72it/s]Extractor Predicting: 350it [03:17,  1.70it/s]Extractor Predicting: 351it [03:17,  1.74it/s]Extractor Predicting: 352it [03:18,  1.69it/s]Extractor Predicting: 353it [03:18,  1.66it/s]Extractor Predicting: 354it [03:19,  1.71it/s]Extractor Predicting: 355it [03:20,  1.74it/s]Extractor Predicting: 356it [03:20,  1.73it/s]Extractor Predicting: 357it [03:21,  1.74it/s]Extractor Predicting: 358it [03:21,  1.75it/s]Extractor Predicting: 359it [03:22,  1.73it/s]Extractor Predicting: 360it [03:23,  1.70it/s]Extractor Predicting: 361it [03:23,  1.69it/s]Extractor Predicting: 362it [03:24,  1.71it/s]Extractor Predicting: 363it [03:24,  1.76it/s]Extractor Predicting: 364it [03:25,  1.77it/s]Extractor Predicting: 365it [03:25,  1.73it/s]Extractor Predicting: 366it [03:26,  1.68it/s]Extractor Predicting: 367it [03:27,  1.68it/s]Extractor Predicting: 368it [03:27,  1.66it/s]Extractor Predicting: 369it [03:28,  1.61it/s]Extractor Predicting: 370it [03:29,  1.62it/s]Extractor Predicting: 371it [03:29,  1.46it/s]Extractor Predicting: 372it [03:30,  1.78it/s]Extractor Predicting: 372it [03:30,  1.77it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:40,281 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:40,287 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:40,287 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:40,287 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:40,287 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:38:40,578 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:38:40,579 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:38:40,835 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:38:41,884 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:38:41,884 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:43,191 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:43,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:43,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:43,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:38:43,193 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:38:43,506 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:38:43,511 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:38:43,765 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:38:43,903 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:38:43,904 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.75it/s]Extractor Predicting: 6it [00:03,  1.82it/s]Extractor Predicting: 7it [00:04,  1.78it/s]Extractor Predicting: 8it [00:04,  1.79it/s]Extractor Predicting: 9it [00:05,  1.81it/s]Extractor Predicting: 10it [00:05,  1.83it/s]Extractor Predicting: 11it [00:06,  1.82it/s]Extractor Predicting: 12it [00:06,  1.86it/s]Extractor Predicting: 13it [00:07,  1.85it/s]Extractor Predicting: 14it [00:07,  1.83it/s]Extractor Predicting: 15it [00:08,  1.75it/s]Extractor Predicting: 16it [00:08,  1.77it/s]Extractor Predicting: 17it [00:09,  1.77it/s]Extractor Predicting: 18it [00:10,  1.79it/s]Extractor Predicting: 19it [00:10,  1.80it/s]Extractor Predicting: 20it [00:11,  1.80it/s]Extractor Predicting: 21it [00:11,  1.75it/s]Extractor Predicting: 22it [00:12,  1.75it/s]Extractor Predicting: 23it [00:12,  1.73it/s]Extractor Predicting: 24it [00:13,  1.70it/s]Extractor Predicting: 25it [00:14,  1.74it/s]Extractor Predicting: 26it [00:14,  1.72it/s]Extractor Predicting: 27it [00:15,  1.71it/s]Extractor Predicting: 28it [00:15,  1.69it/s]Extractor Predicting: 29it [00:16,  1.72it/s]Extractor Predicting: 30it [00:17,  1.74it/s]Extractor Predicting: 31it [00:17,  1.76it/s]Extractor Predicting: 32it [00:18,  1.73it/s]Extractor Predicting: 33it [00:18,  1.77it/s]Extractor Predicting: 34it [00:19,  1.78it/s]Extractor Predicting: 35it [00:19,  1.74it/s]Extractor Predicting: 36it [00:20,  1.74it/s]Extractor Predicting: 37it [00:21,  1.72it/s]Extractor Predicting: 38it [00:21,  1.66it/s]Extractor Predicting: 39it [00:22,  1.62it/s]Extractor Predicting: 40it [00:23,  1.61it/s]Extractor Predicting: 41it [00:23,  1.62it/s]Extractor Predicting: 42it [00:24,  1.65it/s]Extractor Predicting: 43it [00:24,  1.65it/s]Extractor Predicting: 44it [00:25,  1.67it/s]Extractor Predicting: 45it [00:26,  1.64it/s]Extractor Predicting: 46it [00:26,  1.67it/s]Extractor Predicting: 47it [00:27,  1.66it/s]Extractor Predicting: 48it [00:27,  1.53it/s]Extractor Predicting: 49it [00:28,  1.56it/s]Extractor Predicting: 50it [00:29,  1.58it/s]Extractor Predicting: 51it [00:29,  1.57it/s]Extractor Predicting: 52it [00:30,  1.57it/s]Extractor Predicting: 53it [00:31,  1.60it/s]Extractor Predicting: 54it [00:31,  1.59it/s]Extractor Predicting: 55it [00:32,  1.64it/s]Extractor Predicting: 56it [00:32,  1.64it/s]Extractor Predicting: 57it [00:33,  1.63it/s]Extractor Predicting: 58it [00:34,  1.61it/s]Extractor Predicting: 59it [00:34,  1.61it/s]Extractor Predicting: 60it [00:35,  1.61it/s]Extractor Predicting: 61it [00:36,  1.57it/s]Extractor Predicting: 62it [00:36,  1.57it/s]Extractor Predicting: 63it [00:37,  1.71it/s]Extractor Predicting: 63it [00:37,  1.70it/s]
[INFO|configuration_utils.py:515] 2023-08-28 09:39:21,986 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:39:21,987 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 09:39:21,993 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:39:21,994 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 09:39:21,997 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 09:39:25,054 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 09:39:25,056 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 09:39:25,064 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:39:25,065 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 09:39:25,070 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:39:25,073 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:39:25,074 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:39:25,074 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:39:25,074 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:39:25,074 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:39:25,074 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 09:39:25,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:25,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:26,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:27,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:27,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:28,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:28,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:29,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:30,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:30,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:31,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:31,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:32,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:32,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:33,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:34,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:35,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:35,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:36,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:36,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:37,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:38,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:38,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:39,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:39,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:40,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:41,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:50, 16.43s/it][WARNING|generation_utils.py:914] 2023-08-28 09:39:41,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:42,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:42,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:43,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:43,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:44,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:45,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:45,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:46,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:46,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:47,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:48,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:48,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:49,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:49,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:50,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:50,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:51,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:51,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:52,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:53,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:53,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:54,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:55,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:55,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:19, 15.36s/it][WARNING|generation_utils.py:914] 2023-08-28 09:39:56,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:56,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:57,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:57,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:58,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:58,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:39:59,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:00,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:00,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:01,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:01,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:02,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:02,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:03,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:03,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:04,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:04,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:05,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:05,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:06,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:06,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:42<02:41, 13.44s/it][WARNING|generation_utils.py:914] 2023-08-28 09:40:07,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:08,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:08,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:09,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:10,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:10,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:11,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:12,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:12,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:13,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:13,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:14,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:15,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:15,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:16,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:16,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:17,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:18,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:18,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:19,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:19,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:20,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:20,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:21,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:56<02:32, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-28 09:40:22,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:22,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:23,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:23,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:24,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:25,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:25,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:26,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:26,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:27,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:28,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:28,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:29,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:30,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:30,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:31,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:31,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:32,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:33,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:33,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:34,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:34,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:35,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:36,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:36,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:23, 14.39s/it][WARNING|generation_utils.py:914] 2023-08-28 09:40:37,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:37,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:38,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:39,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:39,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:40,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:41,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:41,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:42,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:43,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:43,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:44,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:44,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:45,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:46,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:46,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:47,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:48,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:48,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:49,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:50,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:50,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:26<02:08, 14.29s/it][WARNING|generation_utils.py:914] 2023-08-28 09:40:51,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:52,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:52,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:53,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:53,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:54,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:55,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:55,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:56,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:56,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:57,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:58,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:58,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:59,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:40:59,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:00,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:00,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:01,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:02,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:02,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:03,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:03,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:39<01:51, 13.92s/it][WARNING|generation_utils.py:914] 2023-08-28 09:41:04,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:05,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:05,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:06,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:06,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:07,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:08,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:08,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:09,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:09,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:10,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:10,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:11,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:12,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:12,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:13,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:13,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:14,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:15,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:15,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:16,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:16,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:52<01:35, 13.62s/it][WARNING|generation_utils.py:914] 2023-08-28 09:41:17,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:18,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:18,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:19,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:20,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:20,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:21,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:21,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:22,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:23,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:23,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:24,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:24,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:25,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:26,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:26,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:27,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:27,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:28,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:28,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:29,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:30,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:30,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:31,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:31,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:32,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:07<01:25, 14.24s/it][WARNING|generation_utils.py:914] 2023-08-28 09:41:33,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:33,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:34,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:34,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:35,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:36,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:36,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:37,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:37,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:38,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:39,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:39,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:40,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:40,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:41,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:42,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:42,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:43,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:44,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:44,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:45,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:45,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:46,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:21<01:10, 14.14s/it][WARNING|generation_utils.py:914] 2023-08-28 09:41:47,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:47,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:48,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:48,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:49,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:49,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:50,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:50,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:51,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:51,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:52,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:53,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:53,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:53,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:54,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:54,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:55,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:56,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:56,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:57,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:57,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:58,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:58,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:33<00:53, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-28 09:41:59,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:41:59,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:00,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:00,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:01,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:01,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:02,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:03,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:03,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:04,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:04,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:05,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:06,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:06,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:07,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:08,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:09,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:09,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:10,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:10,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:11,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:11,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:12,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:13,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:13,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:14,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:15,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:15,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:50<00:43, 14.58s/it][WARNING|generation_utils.py:914] 2023-08-28 09:42:16,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:16,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:17,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:18,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:18,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:19,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:19,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:20,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:21,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:21,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:22,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:23,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:23,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:24,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:24,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:25,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:26,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:26,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:27,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:28,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:28,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:29,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:29,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:30,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:05<00:29, 14.69s/it][WARNING|generation_utils.py:914] 2023-08-28 09:42:31,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:31,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:32,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:32,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:33,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:33,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:34,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:34,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:35,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:35,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:36,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:36,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:37,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:38,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:38,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:39,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:39,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:40,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:40,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:41,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:42,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:42,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:43,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:18<00:14, 14.04s/it][WARNING|generation_utils.py:914] 2023-08-28 09:42:43,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:44,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:44,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:45,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:46,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:46,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:47,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:48,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:48,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:49,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:50,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:50,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:51,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:52,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:52,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:53,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:54,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:54,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:55,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:56,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:57,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:57,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:42:58,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:33<00:00, 14.37s/it]Generating: 100%|██████████| 15/15 [03:33<00:00, 14.23s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:05,328 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:05,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:05,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:05,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:05,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:43:05,918 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:43:05,919 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:43:06,522 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:43:07,586 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:43:07,586 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:10,506 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:10,510 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:10,510 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:10,510 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:43:10,510 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:43:11,139 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:43:11,140 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:43:11,703 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:43:11,871 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:43:11,872 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 469, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : developer .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 626, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.7825, 'errors': {'', "('London', 'work location', '', 'The New York Times ( July 18 , 1923 October 6 , 1977 ) reported that the first and only English book on the topic appeared in London during the reigns of William Shakespeare and William Shakespeare .')"}}
['Relation : composer . Context : Later in the year ( 1143 ) , he composed The Seven Kingdoms , the first play in the epic of the Old Kingdom . Head Entity : The Seven Kingdoms , Tail Entity : Robert I .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best Rap Album at the 2004 MTV Video Music Awards and had two acts on the chart . Head Entity : music , Tail Entity : Rap Album .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 590, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7319711538461539, 'errors': {'', "('James S. Cain', 'field of work', '', 'He is also known for his work with James S. Cain , and for his work on the painting of the Seven Wonders of Oz by William S. Burroughs .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 194, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 298, 'raw': 448}
{'target': 600, 'success': 322, 'raw': 480}
{'target': 600, 'success': 346, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 387, 'raw': 576}
{'target': 600, 'success': 408, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 518, 'raw': 768}
{'target': 600, 'success': 543, 'raw': 800}
{'target': 600, 'success': 563, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 606, 'raw': 896}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6763392857142857, 'errors': {'', "('John Lennon', 'occupation', '', 'The band released their debut album In Search of a Way ( 2000 ) , which featured a cover from John Lennon and Steve Dizzy .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 13757
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13857, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.42it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:01,  1.53it/s]Extractor Estimating: 4it [00:02,  1.65it/s]Extractor Estimating: 5it [00:03,  1.68it/s]Extractor Estimating: 6it [00:03,  1.70it/s]Extractor Estimating: 7it [00:04,  1.68it/s]Extractor Estimating: 8it [00:04,  1.59it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.60it/s]Extractor Estimating: 11it [00:06,  1.58it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.63it/s]Extractor Estimating: 14it [00:08,  1.64it/s]Extractor Estimating: 15it [00:09,  1.67it/s]Extractor Estimating: 16it [00:09,  1.67it/s]Extractor Estimating: 17it [00:10,  1.64it/s]Extractor Estimating: 18it [00:11,  1.65it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:12,  1.69it/s]Extractor Estimating: 22it [00:13,  1.73it/s]Extractor Estimating: 23it [00:14,  1.72it/s]Extractor Estimating: 24it [00:14,  1.68it/s]Extractor Estimating: 25it [00:15,  1.71it/s]Extractor Estimating: 26it [00:15,  1.77it/s]Extractor Estimating: 27it [00:16,  1.72it/s]Extractor Estimating: 28it [00:16,  1.77it/s]Extractor Estimating: 29it [00:17,  1.76it/s]Extractor Estimating: 30it [00:18,  1.64it/s]Extractor Estimating: 31it [00:18,  1.62it/s]Extractor Estimating: 32it [00:19,  1.71it/s]Extractor Estimating: 33it [00:19,  1.69it/s]Extractor Estimating: 34it [00:20,  1.71it/s]Extractor Estimating: 35it [00:21,  1.67it/s]Extractor Estimating: 36it [00:21,  1.67it/s]Extractor Estimating: 37it [00:22,  1.68it/s]Extractor Estimating: 38it [00:22,  1.65it/s]Extractor Estimating: 39it [00:23,  1.68it/s]Extractor Estimating: 40it [00:24,  1.66it/s]Extractor Estimating: 41it [00:24,  1.64it/s]Extractor Estimating: 42it [00:25,  1.69it/s]Extractor Estimating: 43it [00:25,  1.72it/s]Extractor Estimating: 44it [00:26,  1.72it/s]Extractor Estimating: 45it [00:26,  1.76it/s]Extractor Estimating: 46it [00:27,  1.75it/s]Extractor Estimating: 47it [00:28,  1.75it/s]Extractor Estimating: 48it [00:28,  1.74it/s]Extractor Estimating: 49it [00:29,  1.70it/s]Extractor Estimating: 50it [00:29,  1.77it/s]Extractor Estimating: 51it [00:30,  1.81it/s]Extractor Estimating: 52it [00:30,  1.85it/s]Extractor Estimating: 53it [00:31,  1.88it/s]Extractor Estimating: 54it [00:31,  1.92it/s]Extractor Estimating: 55it [00:32,  1.93it/s]Extractor Estimating: 56it [00:32,  2.01it/s]Extractor Estimating: 57it [00:33,  1.99it/s]Extractor Estimating: 58it [00:33,  2.01it/s]Extractor Estimating: 59it [00:34,  2.06it/s]Extractor Estimating: 60it [00:34,  1.99it/s]Extractor Estimating: 61it [00:35,  1.95it/s]Extractor Estimating: 62it [00:35,  2.00it/s]Extractor Estimating: 63it [00:36,  2.05it/s]Extractor Estimating: 64it [00:36,  1.99it/s]Extractor Estimating: 65it [00:37,  1.95it/s]Extractor Estimating: 66it [00:37,  2.02it/s]Extractor Estimating: 67it [00:38,  2.02it/s]Extractor Estimating: 68it [00:38,  1.97it/s]Extractor Estimating: 69it [00:39,  2.01it/s]Extractor Estimating: 70it [00:39,  1.99it/s]Extractor Estimating: 71it [00:40,  2.03it/s]Extractor Estimating: 72it [00:40,  2.01it/s]Extractor Estimating: 73it [00:41,  2.03it/s]Extractor Estimating: 74it [00:41,  2.04it/s]Extractor Estimating: 75it [00:42,  2.01it/s]Extractor Estimating: 76it [00:42,  1.93it/s]Extractor Estimating: 77it [00:43,  1.78it/s]Extractor Estimating: 78it [00:44,  1.76it/s]Extractor Estimating: 79it [00:44,  1.71it/s]Extractor Estimating: 80it [00:45,  1.62it/s]Extractor Estimating: 81it [00:46,  1.48it/s]Extractor Estimating: 82it [00:46,  1.47it/s]Extractor Estimating: 83it [00:47,  1.51it/s]Extractor Estimating: 84it [00:48,  1.50it/s]Extractor Estimating: 85it [00:48,  1.50it/s]Extractor Estimating: 86it [00:49,  1.53it/s]Extractor Estimating: 87it [00:50,  1.52it/s]Extractor Estimating: 88it [00:50,  1.55it/s]Extractor Estimating: 89it [00:51,  1.61it/s]Extractor Estimating: 90it [00:51,  1.64it/s]Extractor Estimating: 91it [00:52,  1.64it/s]Extractor Estimating: 92it [00:53,  1.66it/s]Extractor Estimating: 93it [00:53,  1.68it/s]Extractor Estimating: 94it [00:54,  1.68it/s]Extractor Estimating: 95it [00:54,  1.63it/s]Extractor Estimating: 96it [00:55,  1.65it/s]Extractor Estimating: 97it [00:56,  1.64it/s]Extractor Estimating: 98it [00:56,  1.64it/s]Extractor Estimating: 99it [00:57,  1.65it/s]Extractor Estimating: 100it [00:58,  1.63it/s]Extractor Estimating: 101it [00:58,  1.63it/s]Extractor Estimating: 102it [00:59,  1.63it/s]Extractor Estimating: 103it [00:59,  1.65it/s]Extractor Estimating: 104it [01:00,  1.69it/s]Extractor Estimating: 105it [01:00,  1.69it/s]Extractor Estimating: 106it [01:01,  1.67it/s]Extractor Estimating: 107it [01:02,  1.65it/s]Extractor Estimating: 108it [01:02,  1.67it/s]Extractor Estimating: 109it [01:03,  1.61it/s]Extractor Estimating: 110it [01:04,  1.62it/s]Extractor Estimating: 111it [01:04,  1.58it/s]Extractor Estimating: 112it [01:05,  1.49it/s]Extractor Estimating: 113it [01:06,  1.52it/s]Extractor Estimating: 114it [01:06,  1.53it/s]Extractor Estimating: 115it [01:07,  1.51it/s]Extractor Estimating: 116it [01:08,  1.52it/s]Extractor Estimating: 117it [01:08,  1.53it/s]Extractor Estimating: 118it [01:09,  1.60it/s]Extractor Estimating: 119it [01:09,  1.61it/s]Extractor Estimating: 120it [01:10,  1.57it/s]Extractor Estimating: 121it [01:11,  1.59it/s]Extractor Estimating: 122it [01:11,  1.59it/s]Extractor Estimating: 123it [01:12,  1.58it/s]Extractor Estimating: 124it [01:13,  1.61it/s]Extractor Estimating: 125it [01:13,  1.65it/s]Extractor Estimating: 126it [01:14,  1.70it/s]Extractor Estimating: 127it [01:14,  1.64it/s]Extractor Estimating: 128it [01:15,  1.66it/s]Extractor Estimating: 129it [01:16,  1.66it/s]Extractor Estimating: 130it [01:16,  1.67it/s]Extractor Estimating: 131it [01:17,  1.64it/s]Extractor Estimating: 132it [01:17,  1.59it/s]Extractor Estimating: 133it [01:18,  1.57it/s]Extractor Estimating: 134it [01:19,  1.59it/s]Extractor Estimating: 135it [01:19,  1.62it/s]Extractor Estimating: 136it [01:20,  1.61it/s]Extractor Estimating: 137it [01:21,  1.52it/s]Extractor Estimating: 138it [01:21,  1.54it/s]Extractor Estimating: 139it [01:22,  1.55it/s]Extractor Estimating: 140it [01:23,  1.57it/s]Extractor Estimating: 141it [01:23,  1.60it/s]Extractor Estimating: 142it [01:24,  1.63it/s]Extractor Estimating: 143it [01:24,  1.68it/s]Extractor Estimating: 144it [01:25,  1.66it/s]Extractor Estimating: 145it [01:25,  1.69it/s]Extractor Estimating: 146it [01:26,  1.64it/s]Extractor Estimating: 147it [01:27,  1.64it/s]Extractor Estimating: 148it [01:27,  1.62it/s]Extractor Estimating: 149it [01:28,  1.68it/s]Extractor Estimating: 150it [01:29,  1.50it/s]Extractor Estimating: 151it [01:29,  1.54it/s]Extractor Estimating: 152it [01:30,  1.59it/s]Extractor Estimating: 153it [01:31,  1.64it/s]Extractor Estimating: 154it [01:31,  1.68it/s]Extractor Estimating: 155it [01:32,  1.69it/s]Extractor Estimating: 156it [01:32,  1.66it/s]Extractor Estimating: 157it [01:33,  1.65it/s]Extractor Estimating: 158it [01:34,  1.65it/s]Extractor Estimating: 159it [01:34,  1.66it/s]Extractor Estimating: 160it [01:35,  1.71it/s]Extractor Estimating: 161it [01:35,  1.60it/s]Extractor Estimating: 162it [01:36,  1.61it/s]Extractor Estimating: 163it [01:37,  1.66it/s]Extractor Estimating: 164it [01:37,  1.69it/s]Extractor Estimating: 165it [01:38,  1.73it/s]Extractor Estimating: 166it [01:38,  1.74it/s]Extractor Estimating: 167it [01:39,  1.77it/s]Extractor Estimating: 168it [01:39,  1.79it/s]Extractor Estimating: 169it [01:40,  1.81it/s]Extractor Estimating: 170it [01:41,  1.70it/s]Extractor Estimating: 171it [01:41,  1.70it/s]Extractor Estimating: 172it [01:42,  1.73it/s]Extractor Estimating: 173it [01:42,  1.76it/s]Extractor Estimating: 174it [01:43,  1.71it/s]Extractor Estimating: 175it [01:44,  1.63it/s]Extractor Estimating: 176it [01:44,  1.62it/s]Extractor Estimating: 177it [01:45,  1.65it/s]Extractor Estimating: 178it [01:45,  1.66it/s]Extractor Estimating: 179it [01:46,  1.65it/s]Extractor Estimating: 180it [01:47,  1.63it/s]Extractor Estimating: 181it [01:47,  1.64it/s]Extractor Estimating: 182it [01:48,  1.64it/s]Extractor Estimating: 183it [01:48,  1.65it/s]Extractor Estimating: 184it [01:49,  1.67it/s]Extractor Estimating: 185it [01:50,  1.64it/s]Extractor Estimating: 186it [01:50,  1.63it/s]Extractor Estimating: 187it [01:51,  1.67it/s]Extractor Estimating: 188it [01:51,  1.66it/s]Extractor Estimating: 189it [01:52,  1.65it/s]Extractor Estimating: 190it [01:53,  1.61it/s]Extractor Estimating: 191it [01:53,  1.58it/s]Extractor Estimating: 192it [01:54,  1.60it/s]Extractor Estimating: 193it [01:55,  1.57it/s]Extractor Estimating: 194it [01:55,  1.61it/s]Extractor Estimating: 195it [01:56,  1.62it/s]Extractor Estimating: 196it [01:56,  1.57it/s]Extractor Estimating: 197it [01:57,  1.59it/s]Extractor Estimating: 198it [01:58,  1.57it/s]Extractor Estimating: 199it [01:58,  1.61it/s]Extractor Estimating: 200it [01:59,  1.60it/s]Extractor Estimating: 201it [02:00,  1.59it/s]Extractor Estimating: 202it [02:00,  1.63it/s]Extractor Estimating: 203it [02:01,  1.58it/s]Extractor Estimating: 204it [02:01,  1.61it/s]Extractor Estimating: 205it [02:02,  1.56it/s]Extractor Estimating: 206it [02:03,  1.61it/s]Extractor Estimating: 207it [02:03,  1.64it/s]Extractor Estimating: 208it [02:04,  1.64it/s]Extractor Estimating: 209it [02:04,  1.66it/s]Extractor Estimating: 210it [02:05,  1.64it/s]Extractor Estimating: 211it [02:06,  1.67it/s]Extractor Estimating: 212it [02:06,  1.67it/s]Extractor Estimating: 213it [02:07,  1.69it/s]Extractor Estimating: 214it [02:07,  1.67it/s]Extractor Estimating: 215it [02:08,  1.64it/s]Extractor Estimating: 216it [02:09,  1.61it/s]Extractor Estimating: 217it [02:09,  1.62it/s]Extractor Estimating: 218it [02:10,  1.62it/s]Extractor Estimating: 219it [02:11,  1.68it/s]Extractor Estimating: 220it [02:11,  1.68it/s]Extractor Estimating: 221it [02:12,  1.67it/s]Extractor Estimating: 222it [02:12,  1.66it/s]Extractor Estimating: 223it [02:13,  1.68it/s]Extractor Estimating: 224it [02:14,  1.66it/s]Extractor Estimating: 225it [02:14,  1.64it/s]Extractor Estimating: 226it [02:15,  1.62it/s]Extractor Estimating: 227it [02:15,  1.56it/s]Extractor Estimating: 228it [02:16,  1.55it/s]Extractor Estimating: 229it [02:17,  1.57it/s]Extractor Estimating: 230it [02:18,  1.44it/s]Extractor Estimating: 231it [02:18,  1.50it/s]Extractor Estimating: 232it [02:19,  1.55it/s]Extractor Estimating: 233it [02:19,  1.60it/s]Extractor Estimating: 234it [02:20,  1.57it/s]Extractor Estimating: 235it [02:21,  1.54it/s]Extractor Estimating: 236it [02:21,  1.53it/s]Extractor Estimating: 237it [02:22,  1.55it/s]Extractor Estimating: 238it [02:23,  1.58it/s]Extractor Estimating: 239it [02:23,  1.60it/s]Extractor Estimating: 240it [02:25,  1.07it/s]Extractor Estimating: 241it [02:26,  1.17it/s]Extractor Estimating: 242it [02:26,  1.25it/s]Extractor Estimating: 243it [02:27,  1.32it/s]Extractor Estimating: 244it [02:27,  1.41it/s]Extractor Estimating: 245it [02:28,  1.44it/s]Extractor Estimating: 246it [02:29,  1.47it/s]Extractor Estimating: 247it [02:29,  1.49it/s]Extractor Estimating: 248it [02:30,  1.55it/s]Extractor Estimating: 249it [02:31,  1.58it/s]Extractor Estimating: 250it [02:31,  1.55it/s]Extractor Estimating: 251it [02:32,  1.57it/s]Extractor Estimating: 252it [02:33,  1.55it/s]Extractor Estimating: 253it [02:33,  1.57it/s]Extractor Estimating: 254it [02:34,  1.59it/s]Extractor Estimating: 255it [02:34,  1.61it/s]Extractor Estimating: 256it [02:35,  1.62it/s]Extractor Estimating: 257it [02:36,  1.65it/s]Extractor Estimating: 258it [02:36,  1.69it/s]Extractor Estimating: 259it [02:37,  1.64it/s]Extractor Estimating: 260it [02:37,  1.69it/s]Extractor Estimating: 261it [02:38,  1.64it/s]Extractor Estimating: 262it [02:39,  1.63it/s]Extractor Estimating: 263it [02:39,  1.65it/s]Extractor Estimating: 264it [02:40,  1.69it/s]Extractor Estimating: 265it [02:40,  1.68it/s]Extractor Estimating: 266it [02:41,  1.70it/s]Extractor Estimating: 267it [02:42,  1.69it/s]Extractor Estimating: 268it [02:42,  1.62it/s]Extractor Estimating: 269it [02:43,  1.65it/s]Extractor Estimating: 270it [02:43,  1.64it/s]Extractor Estimating: 271it [02:44,  1.61it/s]Extractor Estimating: 272it [02:45,  1.56it/s]Extractor Estimating: 273it [02:45,  1.62it/s]Extractor Estimating: 274it [02:46,  1.63it/s]Extractor Estimating: 275it [02:46,  1.65it/s]Extractor Estimating: 276it [02:47,  1.59it/s]Extractor Estimating: 277it [02:48,  1.63it/s]Extractor Estimating: 278it [02:48,  1.67it/s]Extractor Estimating: 279it [02:49,  1.65it/s]Extractor Estimating: 280it [02:50,  1.66it/s]Extractor Estimating: 281it [02:50,  1.64it/s]Extractor Estimating: 282it [02:51,  1.67it/s]Extractor Estimating: 283it [02:51,  1.62it/s]Extractor Estimating: 284it [02:52,  1.62it/s]Extractor Estimating: 285it [02:53,  1.60it/s]Extractor Estimating: 286it [02:53,  1.61it/s]Extractor Estimating: 287it [02:54,  1.57it/s]Extractor Estimating: 288it [02:55,  1.62it/s]Extractor Estimating: 289it [02:55,  1.61it/s]Extractor Estimating: 290it [02:56,  1.66it/s]Extractor Estimating: 291it [02:56,  1.66it/s]Extractor Estimating: 292it [02:57,  1.63it/s]Extractor Estimating: 293it [02:58,  1.61it/s]Extractor Estimating: 294it [02:58,  1.65it/s]Extractor Estimating: 295it [02:59,  1.69it/s]Extractor Estimating: 296it [02:59,  1.66it/s]Extractor Estimating: 297it [03:00,  1.69it/s]Extractor Estimating: 298it [03:00,  1.72it/s]Extractor Estimating: 299it [03:01,  1.65it/s]Extractor Estimating: 300it [03:02,  1.65it/s]Extractor Estimating: 301it [03:02,  1.62it/s]Extractor Estimating: 302it [03:03,  1.57it/s]Extractor Estimating: 303it [03:04,  1.57it/s]Extractor Estimating: 304it [03:04,  1.56it/s]Extractor Estimating: 305it [03:05,  1.56it/s]Extractor Estimating: 306it [03:06,  1.54it/s]Extractor Estimating: 307it [03:06,  1.59it/s]Extractor Estimating: 308it [03:07,  1.59it/s]Extractor Estimating: 309it [03:08,  1.58it/s]Extractor Estimating: 310it [03:08,  1.58it/s]Extractor Estimating: 311it [03:09,  1.57it/s]Extractor Estimating: 312it [03:10,  1.42it/s]Extractor Estimating: 313it [03:10,  1.48it/s]Extractor Estimating: 314it [03:11,  1.51it/s]Extractor Estimating: 315it [03:12,  1.53it/s]Extractor Estimating: 316it [03:12,  1.54it/s]Extractor Estimating: 317it [03:13,  1.56it/s]Extractor Estimating: 318it [03:13,  1.53it/s]Extractor Estimating: 319it [03:14,  1.54it/s]Extractor Estimating: 320it [03:15,  1.54it/s]Extractor Estimating: 321it [03:15,  1.52it/s]Extractor Estimating: 322it [03:16,  1.54it/s]Extractor Estimating: 323it [03:17,  1.55it/s]Extractor Estimating: 324it [03:17,  1.60it/s]Extractor Estimating: 325it [03:18,  1.60it/s]Extractor Estimating: 326it [03:19,  1.60it/s]Extractor Estimating: 327it [03:19,  1.63it/s]Extractor Estimating: 328it [03:20,  1.63it/s]Extractor Estimating: 329it [03:20,  1.66it/s]Extractor Estimating: 330it [03:21,  1.62it/s]Extractor Estimating: 331it [03:22,  1.62it/s]Extractor Estimating: 332it [03:22,  1.65it/s]Extractor Estimating: 333it [03:23,  1.67it/s]Extractor Estimating: 334it [03:23,  1.65it/s]Extractor Estimating: 335it [03:24,  1.62it/s]Extractor Estimating: 336it [03:25,  1.65it/s]Extractor Estimating: 337it [03:25,  1.54it/s]Extractor Estimating: 338it [03:26,  1.59it/s]Extractor Estimating: 339it [03:27,  1.55it/s]Extractor Estimating: 340it [03:27,  1.61it/s]Extractor Estimating: 341it [03:28,  1.61it/s]Extractor Estimating: 342it [03:28,  1.60it/s]Extractor Estimating: 343it [03:29,  1.64it/s]Extractor Estimating: 344it [03:30,  1.61it/s]Extractor Estimating: 345it [03:30,  1.58it/s]Extractor Estimating: 346it [03:31,  1.58it/s]Extractor Estimating: 347it [03:32,  1.62it/s]Extractor Estimating: 348it [03:32,  1.64it/s]Extractor Estimating: 349it [03:33,  1.66it/s]Extractor Estimating: 350it [03:33,  1.63it/s]Extractor Estimating: 351it [03:34,  1.66it/s]Extractor Estimating: 352it [03:35,  1.64it/s]Extractor Estimating: 353it [03:35,  1.67it/s]Extractor Estimating: 354it [03:36,  1.66it/s]Extractor Estimating: 355it [03:36,  1.57it/s]Extractor Estimating: 356it [03:37,  1.54it/s]Extractor Estimating: 357it [03:38,  1.57it/s]Extractor Estimating: 358it [03:38,  1.64it/s]Extractor Estimating: 359it [03:39,  1.64it/s]Extractor Estimating: 360it [03:39,  1.65it/s]Extractor Estimating: 361it [03:40,  1.68it/s]Extractor Estimating: 362it [03:41,  1.64it/s]Extractor Estimating: 363it [03:41,  1.62it/s]Extractor Estimating: 364it [03:42,  1.59it/s]Extractor Estimating: 365it [03:43,  1.60it/s]Extractor Estimating: 366it [03:43,  1.66it/s]Extractor Estimating: 367it [03:44,  1.64it/s]Extractor Estimating: 368it [03:44,  1.62it/s]Extractor Estimating: 369it [03:45,  1.63it/s]Extractor Estimating: 370it [03:46,  1.64it/s]Extractor Estimating: 371it [03:46,  1.61it/s]Extractor Estimating: 372it [03:47,  1.65it/s]Extractor Estimating: 373it [03:47,  1.66it/s]Extractor Estimating: 374it [03:48,  1.62it/s]Extractor Estimating: 375it [03:48,  1.83it/s]Extractor Estimating: 375it [03:48,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:18,063 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:18,067 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:18,067 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:18,067 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:18,067 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:47:18,649 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:47:18,650 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:47:19,229 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:47:20,282 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:47:20,282 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:22,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:22,655 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:22,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:22,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:22,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:47:23,299 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:47:23,300 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:47:23,869 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:47:24,035 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:47:24,035 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 12:03:52,705 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 12:03:52,710 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7687 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 20348
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20448, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20448, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.018, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.026, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 79, avg_time 1.033, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 179, avg_time 1.034, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 279, avg_time 2.351, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 58, avg_time 1.015, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 158, avg_time 1.030, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 258, avg_time 1.029, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 37, avg_time 1.023, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 137, avg_time 2.334, loss:nan
g_step 1200, step 237, avg_time 1.020, loss:nan
g_step 1300, step 16, avg_time 1.025, loss:nan
g_step 1400, step 116, avg_time 1.039, loss:nan
g_step 1500, step 216, avg_time 1.024, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 316, avg_time 2.343, loss:nan
g_step 1700, step 95, avg_time 1.024, loss:nan
g_step 1800, step 195, avg_time 1.015, loss:nan
g_step 1900, step 295, avg_time 1.027, loss:nan
g_step 2000, step 74, avg_time 1.016, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 174, avg_time 2.347, loss:nan
g_step 2200, step 274, avg_time 1.027, loss:nan
g_step 2300, step 53, avg_time 1.034, loss:nan
g_step 2400, step 153, avg_time 1.021, loss:nan
g_step 2500, step 253, avg_time 1.035, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 32, avg_time 2.311, loss:nan
g_step 2700, step 132, avg_time 1.019, loss:nan
g_step 2800, step 232, avg_time 1.034, loss:nan
g_step 2900, step 11, avg_time 1.041, loss:nan
g_step 3000, step 111, avg_time 1.021, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 211, avg_time 2.340, loss:nan
g_step 3200, step 311, avg_time 1.042, loss:nan
g_step 3300, step 90, avg_time 1.026, loss:nan
g_step 3400, step 190, avg_time 1.031, loss:nan
g_step 3500, step 290, avg_time 1.023, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 69, avg_time 2.348, loss:nan
g_step 3700, step 169, avg_time 1.026, loss:nan
g_step 3800, step 269, avg_time 1.044, loss:nan
g_step 3900, step 48, avg_time 1.018, loss:nan
g_step 4000, step 148, avg_time 1.034, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 248, avg_time 2.349, loss:nan
g_step 4200, step 27, avg_time 1.028, loss:nan
g_step 4300, step 127, avg_time 1.032, loss:nan
g_step 4400, step 227, avg_time 1.034, loss:nan
g_step 4500, step 6, avg_time 1.027, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 106, avg_time 2.342, loss:nan
g_step 4700, step 206, avg_time 1.041, loss:nan
g_step 4800, step 306, avg_time 1.025, loss:nan
g_step 4900, step 85, avg_time 1.038, loss:nan
g_step 5000, step 185, avg_time 1.029, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 285, avg_time 2.344, loss:nan
g_step 5200, step 64, avg_time 1.035, loss:nan
g_step 5300, step 164, avg_time 1.034, loss:nan
g_step 5400, step 264, avg_time 1.044, loss:nan
g_step 5500, step 43, avg_time 1.021, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 143, avg_time 2.352, loss:nan
g_step 5700, step 243, avg_time 1.037, loss:nan
g_step 5800, step 22, avg_time 1.032, loss:nan
g_step 5900, step 122, avg_time 1.047, loss:nan
g_step 6000, step 222, avg_time 1.032, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 1, avg_time 2.348, loss:nan
g_step 6200, step 101, avg_time 1.034, loss:nan
g_step 6300, step 201, avg_time 1.034, loss:nan
g_step 6400, step 301, avg_time 1.037, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 12:03:52 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 12:03:52 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_12-03-52_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 12:03:53 - WARNING - datasets.builder -   Using custom data configuration default-bdb8f1563568fc3d
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-bdb8f1563568fc3d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 12:03:54,012 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:03:54,014 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:03:54,014 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:03:54,015 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:03:54,023 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:54,028 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:54,028 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:54,028 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:54,029 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:54,029 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:03:54,029 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 12:03:54,177 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:03:57,279 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 12:03:57,284 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-bdb8f1563568fc3d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.07ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.91ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.31ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.48ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.58ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.64ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.68ba/s]100%|██████████| 8/8 [00:01<00:00,  5.18ba/s]100%|██████████| 8/8 [00:01<00:00,  4.64ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.14ba/s] 40%|████      | 2/5 [00:00<00:00,  4.38ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.49ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.68ba/s]100%|██████████| 5/5 [00:01<00:00,  4.11ba/s]100%|██████████| 5/5 [00:01<00:00,  4.11ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.26ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.41ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.58ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.66ba/s]100%|██████████| 8/8 [00:00<00:00, 11.00ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.27ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.52ba/s]100%|██████████| 5/5 [00:00<00:00, 11.19ba/s]100%|██████████| 5/5 [00:00<00:00, 10.94ba/s]
[INFO|trainer.py:414] 2023-08-28 12:04:01,847 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 12:04:01,864 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 12:04:01,864 >>   Num examples = 7700
[INFO|trainer.py:1149] 2023-08-28 12:04:01,864 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 12:04:01,864 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 12:04:01,864 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 12:04:01,864 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 12:04:01,864 >>   Total optimization steps = 600
  0%|          | 0/600 [00:00<?, ?it/s]  0%|          | 1/600 [00:00<02:48,  3.56it/s]  0%|          | 2/600 [00:00<02:46,  3.59it/s]  0%|          | 3/600 [00:00<02:45,  3.60it/s]  1%|          | 4/600 [00:01<02:45,  3.61it/s]  1%|          | 5/600 [00:01<02:44,  3.61it/s]  1%|          | 6/600 [00:01<02:44,  3.60it/s]  1%|          | 7/600 [00:01<02:46,  3.57it/s]  1%|▏         | 8/600 [00:02<02:45,  3.57it/s]  2%|▏         | 9/600 [00:02<02:45,  3.57it/s]  2%|▏         | 10/600 [00:02<02:44,  3.58it/s]  2%|▏         | 11/600 [00:03<02:43,  3.60it/s]  2%|▏         | 12/600 [00:03<02:43,  3.60it/s]  2%|▏         | 13/600 [00:03<02:42,  3.61it/s]  2%|▏         | 14/600 [00:03<02:42,  3.61it/s]  2%|▎         | 15/600 [00:04<02:41,  3.61it/s]  3%|▎         | 16/600 [00:04<02:41,  3.61it/s]  3%|▎         | 17/600 [00:04<02:41,  3.62it/s]  3%|▎         | 18/600 [00:04<02:40,  3.62it/s]  3%|▎         | 19/600 [00:05<02:40,  3.62it/s]  3%|▎         | 20/600 [00:05<02:40,  3.61it/s]  4%|▎         | 21/600 [00:05<02:40,  3.61it/s]  4%|▎         | 22/600 [00:06<02:39,  3.61it/s]  4%|▍         | 23/600 [00:06<02:39,  3.62it/s]  4%|▍         | 24/600 [00:06<02:39,  3.62it/s]  4%|▍         | 25/600 [00:06<02:39,  3.62it/s]  4%|▍         | 26/600 [00:07<02:38,  3.61it/s]  4%|▍         | 27/600 [00:07<02:38,  3.61it/s]  5%|▍         | 28/600 [00:07<02:38,  3.61it/s]  5%|▍         | 29/600 [00:08<02:38,  3.59it/s]  5%|▌         | 30/600 [00:08<02:38,  3.59it/s]  5%|▌         | 31/600 [00:08<02:38,  3.60it/s]  5%|▌         | 32/600 [00:08<02:37,  3.61it/s]  6%|▌         | 33/600 [00:09<02:37,  3.61it/s]  6%|▌         | 34/600 [00:09<02:36,  3.61it/s]  6%|▌         | 35/600 [00:09<02:36,  3.61it/s]  6%|▌         | 36/600 [00:09<02:36,  3.61it/s]  6%|▌         | 37/600 [00:10<02:36,  3.61it/s]  6%|▋         | 38/600 [00:10<02:35,  3.61it/s]  6%|▋         | 39/600 [00:10<02:35,  3.61it/s]  7%|▋         | 40/600 [00:11<02:35,  3.61it/s]  7%|▋         | 41/600 [00:11<02:34,  3.61it/s]  7%|▋         | 42/600 [00:11<02:34,  3.61it/s]  7%|▋         | 43/600 [00:11<02:34,  3.61it/s]  7%|▋         | 44/600 [00:12<02:34,  3.61it/s]  8%|▊         | 45/600 [00:12<02:33,  3.61it/s]  8%|▊         | 46/600 [00:12<02:33,  3.61it/s]  8%|▊         | 47/600 [00:13<02:33,  3.61it/s]  8%|▊         | 48/600 [00:13<02:32,  3.61it/s]  8%|▊         | 49/600 [00:13<02:32,  3.61it/s]  8%|▊         | 50/600 [00:13<02:32,  3.61it/s]  8%|▊         | 51/600 [00:14<02:32,  3.60it/s]  9%|▊         | 52/600 [00:14<02:32,  3.60it/s]  9%|▉         | 53/600 [00:14<02:31,  3.61it/s]  9%|▉         | 54/600 [00:14<02:31,  3.61it/s]  9%|▉         | 55/600 [00:15<02:31,  3.61it/s]  9%|▉         | 56/600 [00:15<02:30,  3.61it/s] 10%|▉         | 57/600 [00:15<02:30,  3.61it/s] 10%|▉         | 58/600 [00:16<02:30,  3.61it/s] 10%|▉         | 59/600 [00:16<02:29,  3.61it/s] 10%|█         | 60/600 [00:16<02:29,  3.61it/s] 10%|█         | 61/600 [00:16<02:29,  3.61it/s] 10%|█         | 62/600 [00:17<02:29,  3.61it/s] 10%|█         | 63/600 [00:17<02:28,  3.61it/s] 11%|█         | 64/600 [00:17<02:28,  3.61it/s] 11%|█         | 65/600 [00:18<02:28,  3.61it/s] 11%|█         | 66/600 [00:18<02:27,  3.61it/s] 11%|█         | 67/600 [00:18<02:27,  3.61it/s] 11%|█▏        | 68/600 [00:18<02:27,  3.61it/s] 12%|█▏        | 69/600 [00:19<02:27,  3.61it/s] 12%|█▏        | 70/600 [00:19<02:26,  3.61it/s] 12%|█▏        | 71/600 [00:19<02:26,  3.61it/s] 12%|█▏        | 72/600 [00:19<02:26,  3.60it/s] 12%|█▏        | 73/600 [00:20<02:26,  3.61it/s] 12%|█▏        | 74/600 [00:20<02:25,  3.61it/s] 12%|█▎        | 75/600 [00:20<02:25,  3.61it/s] 13%|█▎        | 76/600 [00:21<02:25,  3.61it/s] 13%|█▎        | 77/600 [00:21<02:24,  3.61it/s] 13%|█▎        | 78/600 [00:21<02:24,  3.60it/s] 13%|█▎        | 79/600 [00:21<02:24,  3.60it/s] 13%|█▎        | 80/600 [00:22<02:24,  3.60it/s] 14%|█▎        | 81/600 [00:22<02:24,  3.60it/s] 14%|█▎        | 82/600 [00:22<02:23,  3.61it/s] 14%|█▍        | 83/600 [00:23<02:23,  3.61it/s] 14%|█▍        | 84/600 [00:23<02:23,  3.60it/s] 14%|█▍        | 85/600 [00:23<02:22,  3.60it/s] 14%|█▍        | 86/600 [00:23<02:22,  3.60it/s] 14%|█▍        | 87/600 [00:24<02:22,  3.60it/s] 15%|█▍        | 88/600 [00:24<02:22,  3.60it/s] 15%|█▍        | 89/600 [00:24<02:21,  3.60it/s] 15%|█▌        | 90/600 [00:24<02:21,  3.61it/s] 15%|█▌        | 91/600 [00:25<02:20,  3.61it/s] 15%|█▌        | 92/600 [00:25<02:20,  3.61it/s] 16%|█▌        | 93/600 [00:25<02:20,  3.61it/s] 16%|█▌        | 94/600 [00:26<02:20,  3.60it/s] 16%|█▌        | 95/600 [00:26<02:20,  3.60it/s] 16%|█▌        | 96/600 [00:26<02:19,  3.60it/s] 16%|█▌        | 97/600 [00:26<02:19,  3.60it/s] 16%|█▋        | 98/600 [00:27<02:19,  3.61it/s] 16%|█▋        | 99/600 [00:27<02:18,  3.61it/s] 17%|█▋        | 100/600 [00:27<02:18,  3.61it/s] 17%|█▋        | 101/600 [00:28<02:18,  3.60it/s] 17%|█▋        | 102/600 [00:28<02:18,  3.60it/s] 17%|█▋        | 103/600 [00:28<02:17,  3.60it/s] 17%|█▋        | 104/600 [00:28<02:17,  3.61it/s] 18%|█▊        | 105/600 [00:29<02:17,  3.61it/s] 18%|█▊        | 106/600 [00:29<02:16,  3.61it/s] 18%|█▊        | 107/600 [00:29<02:16,  3.61it/s] 18%|█▊        | 108/600 [00:29<02:16,  3.61it/s] 18%|█▊        | 109/600 [00:30<02:16,  3.61it/s] 18%|█▊        | 110/600 [00:30<02:15,  3.60it/s] 18%|█▊        | 111/600 [00:30<02:15,  3.60it/s] 19%|█▊        | 112/600 [00:31<02:15,  3.60it/s] 19%|█▉        | 113/600 [00:31<02:15,  3.61it/s] 19%|█▉        | 114/600 [00:31<02:14,  3.61it/s] 19%|█▉        | 115/600 [00:31<02:14,  3.61it/s] 19%|█▉        | 116/600 [00:32<02:14,  3.61it/s] 20%|█▉        | 117/600 [00:32<02:14,  3.60it/s] 20%|█▉        | 118/600 [00:32<02:13,  3.60it/s] 20%|█▉        | 119/600 [00:33<02:13,  3.60it/s] 20%|██        | 120/600 [00:33<02:13,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 12:04:35,185 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:04:35,185 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 12:04:35,185 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.90it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.70it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.51it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.55it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.84it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.49it/s][A
  6%|▌         | 37/611 [00:00<00:13, 44.13it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.08it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.22it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.37it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.55it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.43it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.31it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.19it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.99it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.80it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.96it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.14it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.28it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.47it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.54it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.34it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.18it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.04it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.93it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.03it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.19it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.16it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.47it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.46it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.42it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.25it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.18it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.06it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.02it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.18it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.37it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.40it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.43it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.32it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.25it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.15it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.08it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.08it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.21it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.43it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.40it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.37it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.31it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.20it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.16it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.13it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.11it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.20it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.36it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.40it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.46it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.30it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.11it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.04it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.05it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.17it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.26it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.40it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.35it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.38it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.36it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.10it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.92it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.08it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.20it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.37it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.47it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.42it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.35it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.27it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.07it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.91it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.00it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.17it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.34it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.36it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.38it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.36it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.23it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.00it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.98it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.00it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.07it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.20it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.37it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.42it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.46it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.28it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.13it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.06it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.02it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.14it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.22it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.34it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.39it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.38it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.21it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.11it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.04it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.99it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.04it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.30it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.42it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.43it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.38it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.25it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.11it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.03it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.00it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.08it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.30it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.43it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.43it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.31it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.17it/s][A                                                 
                                                 [A 20%|██        | 120/600 [00:47<02:13,  3.60it/s]
100%|██████████| 611/611 [00:13<00:00, 44.17it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:04:49,053 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 12:04:49,074 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:04:51,070 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:04:51,093 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:04:51,104 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120/special_tokens_map.json
 20%|██        | 121/600 [00:49<41:25,  5.19s/it] 20%|██        | 122/600 [00:50<29:36,  3.72s/it] 20%|██        | 123/600 [00:50<21:21,  2.69s/it] 21%|██        | 124/600 [00:50<15:35,  1.97s/it] 21%|██        | 125/600 [00:51<11:34,  1.46s/it] 21%|██        | 126/600 [00:51<08:44,  1.11s/it] 21%|██        | 127/600 [00:51<06:46,  1.16it/s] 21%|██▏       | 128/600 [00:51<05:23,  1.46it/s] 22%|██▏       | 129/600 [00:52<04:25,  1.78it/s] 22%|██▏       | 130/600 [00:52<03:44,  2.09it/s] 22%|██▏       | 131/600 [00:52<03:15,  2.39it/s] 22%|██▏       | 132/600 [00:53<02:55,  2.66it/s] 22%|██▏       | 133/600 [00:53<02:41,  2.89it/s] 22%|██▏       | 134/600 [00:53<02:32,  3.05it/s] 22%|██▎       | 135/600 [00:53<02:25,  3.20it/s] 23%|██▎       | 136/600 [00:54<02:20,  3.31it/s] 23%|██▎       | 137/600 [00:54<02:16,  3.39it/s] 23%|██▎       | 138/600 [00:54<02:13,  3.45it/s] 23%|██▎       | 139/600 [00:54<02:11,  3.50it/s] 23%|██▎       | 140/600 [00:55<02:11,  3.51it/s] 24%|██▎       | 141/600 [00:55<02:10,  3.53it/s] 24%|██▎       | 142/600 [00:55<02:08,  3.55it/s] 24%|██▍       | 143/600 [00:56<02:08,  3.57it/s] 24%|██▍       | 144/600 [00:56<02:07,  3.58it/s] 24%|██▍       | 145/600 [00:56<02:07,  3.58it/s] 24%|██▍       | 146/600 [00:56<02:06,  3.59it/s] 24%|██▍       | 147/600 [00:57<02:07,  3.56it/s] 25%|██▍       | 148/600 [00:57<02:06,  3.57it/s] 25%|██▍       | 149/600 [00:57<02:05,  3.58it/s] 25%|██▌       | 150/600 [00:58<02:05,  3.58it/s] 25%|██▌       | 151/600 [00:58<02:05,  3.58it/s] 25%|██▌       | 152/600 [00:58<02:04,  3.59it/s] 26%|██▌       | 153/600 [00:58<02:04,  3.59it/s] 26%|██▌       | 154/600 [00:59<02:04,  3.59it/s] 26%|██▌       | 155/600 [00:59<02:03,  3.60it/s] 26%|██▌       | 156/600 [00:59<02:03,  3.60it/s] 26%|██▌       | 157/600 [00:59<02:04,  3.55it/s] 26%|██▋       | 158/600 [01:00<02:05,  3.53it/s] 26%|██▋       | 159/600 [01:00<02:04,  3.55it/s] 27%|██▋       | 160/600 [01:00<02:03,  3.56it/s] 27%|██▋       | 161/600 [01:01<02:02,  3.57it/s] 27%|██▋       | 162/600 [01:01<02:02,  3.58it/s] 27%|██▋       | 163/600 [01:01<02:01,  3.59it/s] 27%|██▋       | 164/600 [01:01<02:01,  3.59it/s] 28%|██▊       | 165/600 [01:02<02:00,  3.60it/s] 28%|██▊       | 166/600 [01:02<02:00,  3.60it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 28%|██▊       | 167/600 [01:02<02:00,  3.60it/s] 28%|██▊       | 168/600 [01:03<02:00,  3.60it/s] 28%|██▊       | 169/600 [01:03<02:00,  3.58it/s] 28%|██▊       | 170/600 [01:03<02:00,  3.58it/s] 28%|██▊       | 171/600 [01:03<01:59,  3.59it/s] 29%|██▊       | 172/600 [01:04<01:59,  3.59it/s] 29%|██▉       | 173/600 [01:04<01:58,  3.59it/s] 29%|██▉       | 174/600 [01:04<01:58,  3.59it/s] 29%|██▉       | 175/600 [01:05<01:58,  3.59it/s] 29%|██▉       | 176/600 [01:05<01:58,  3.59it/s] 30%|██▉       | 177/600 [01:05<01:57,  3.60it/s] 30%|██▉       | 178/600 [01:05<01:57,  3.60it/s] 30%|██▉       | 179/600 [01:06<01:56,  3.60it/s] 30%|███       | 180/600 [01:06<01:57,  3.58it/s] 30%|███       | 181/600 [01:06<01:56,  3.59it/s] 30%|███       | 182/600 [01:06<01:56,  3.59it/s] 30%|███       | 183/600 [01:07<01:56,  3.59it/s] 31%|███       | 184/600 [01:07<01:55,  3.59it/s] 31%|███       | 185/600 [01:07<01:55,  3.60it/s] 31%|███       | 186/600 [01:08<01:55,  3.58it/s] 31%|███       | 187/600 [01:08<01:55,  3.58it/s] 31%|███▏      | 188/600 [01:08<01:55,  3.58it/s] 32%|███▏      | 189/600 [01:08<01:54,  3.59it/s] 32%|███▏      | 190/600 [01:09<01:54,  3.59it/s] 32%|███▏      | 191/600 [01:09<01:54,  3.58it/s] 32%|███▏      | 192/600 [01:09<01:53,  3.59it/s] 32%|███▏      | 193/600 [01:10<01:53,  3.59it/s] 32%|███▏      | 194/600 [01:10<01:52,  3.59it/s] 32%|███▎      | 195/600 [01:10<01:52,  3.59it/s] 33%|███▎      | 196/600 [01:10<01:52,  3.60it/s] 33%|███▎      | 197/600 [01:11<01:52,  3.60it/s] 33%|███▎      | 198/600 [01:11<01:51,  3.60it/s] 33%|███▎      | 199/600 [01:11<01:51,  3.60it/s] 33%|███▎      | 200/600 [01:11<01:51,  3.60it/s] 34%|███▎      | 201/600 [01:12<01:50,  3.60it/s] 34%|███▎      | 202/600 [01:12<01:51,  3.58it/s] 34%|███▍      | 203/600 [01:12<01:50,  3.58it/s] 34%|███▍      | 204/600 [01:13<01:50,  3.59it/s] 34%|███▍      | 205/600 [01:13<01:50,  3.59it/s] 34%|███▍      | 206/600 [01:13<01:49,  3.59it/s] 34%|███▍      | 207/600 [01:13<01:49,  3.59it/s] 35%|███▍      | 208/600 [01:14<01:48,  3.60it/s] 35%|███▍      | 209/600 [01:14<01:48,  3.60it/s] 35%|███▌      | 210/600 [01:14<01:48,  3.60it/s] 35%|███▌      | 211/600 [01:15<01:48,  3.60it/s] 35%|███▌      | 212/600 [01:15<01:47,  3.60it/s] 36%|███▌      | 213/600 [01:15<01:48,  3.58it/s] 36%|███▌      | 214/600 [01:15<01:47,  3.58it/s] 36%|███▌      | 215/600 [01:16<01:47,  3.59it/s] 36%|███▌      | 216/600 [01:16<01:46,  3.59it/s] 36%|███▌      | 217/600 [01:16<01:46,  3.59it/s] 36%|███▋      | 218/600 [01:16<01:46,  3.59it/s] 36%|███▋      | 219/600 [01:17<01:45,  3.60it/s] 37%|███▋      | 220/600 [01:17<01:45,  3.60it/s] 37%|███▋      | 221/600 [01:17<01:45,  3.60it/s] 37%|███▋      | 222/600 [01:18<01:44,  3.60it/s] 37%|███▋      | 223/600 [01:18<01:44,  3.60it/s] 37%|███▋      | 224/600 [01:18<01:44,  3.58it/s] 38%|███▊      | 225/600 [01:18<01:44,  3.59it/s] 38%|███▊      | 226/600 [01:19<01:44,  3.59it/s] 38%|███▊      | 227/600 [01:19<01:43,  3.59it/s] 38%|███▊      | 228/600 [01:19<01:43,  3.59it/s] 38%|███▊      | 229/600 [01:20<01:43,  3.58it/s] 38%|███▊      | 230/600 [01:20<01:43,  3.58it/s] 38%|███▊      | 231/600 [01:20<01:42,  3.59it/s] 39%|███▊      | 232/600 [01:20<01:42,  3.59it/s] 39%|███▉      | 233/600 [01:21<01:42,  3.59it/s] 39%|███▉      | 234/600 [01:21<01:41,  3.60it/s] 39%|███▉      | 235/600 [01:21<01:41,  3.58it/s] 39%|███▉      | 236/600 [01:21<01:41,  3.59it/s] 40%|███▉      | 237/600 [01:22<01:41,  3.59it/s] 40%|███▉      | 238/600 [01:22<01:40,  3.60it/s] 40%|███▉      | 239/600 [01:22<01:40,  3.60it/s] 40%|████      | 240/600 [01:23<01:40,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 12:05:25,015 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:05:25,015 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 12:05:25,015 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8523, 'eval_samples_per_second': 352.432, 'eval_steps_per_second': 44.108, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.74it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.71it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.63it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.52it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.79it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.44it/s][A
  6%|▌         | 37/611 [00:00<00:13, 44.05it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.98it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.07it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.27it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.36it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.55it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.42it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.08it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.05it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.81it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.86it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.96it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.10it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.25it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.43it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.40it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.32it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.06it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.91it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.87it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.01it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.09it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.27it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.42it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.37it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.14it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.04it/s][A
 28%|██▊       | 172/611 [00:03<00:10, 43.85it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.88it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.00it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.18it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.25it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.46it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.34it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.28it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.07it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.95it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.92it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 43.86it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.18it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.25it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.37it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.20it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.11it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.01it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.96it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.97it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.99it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.19it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.30it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.24it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.24it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.12it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.11it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.00it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.06it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.14it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.28it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.32it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.32it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.23it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.12it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.08it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.02it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.04it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.15it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.34it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.27it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.22it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.16it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.16it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.19it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.11it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.14it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.20it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.36it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.34it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.21it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.12it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.10it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.13it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.12it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.12it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.27it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.35it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.16it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.11it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.04it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.05it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.04it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.05it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.21it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.34it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.16it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.14it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.17it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.09it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.14it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.08it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.13it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.27it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.30it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.15it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.20it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.20it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.19it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.13it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.15it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.14it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.28it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.20it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.22it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.22it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.04it/s][A                                                 
                                                 [A 40%|████      | 240/600 [01:37<01:40,  3.60it/s]
100%|██████████| 611/611 [00:13<00:00, 44.04it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:05:38,910 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 12:05:38,928 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:05:40,904 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:05:40,920 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:05:40,929 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240/special_tokens_map.json
 40%|████      | 241/600 [01:39<30:50,  5.15s/it] 40%|████      | 242/600 [01:39<22:02,  3.69s/it] 40%|████      | 243/600 [01:40<15:53,  2.67s/it] 41%|████      | 244/600 [01:40<11:35,  1.95s/it] 41%|████      | 245/600 [01:40<08:35,  1.45s/it] 41%|████      | 246/600 [01:41<06:29,  1.10s/it] 41%|████      | 247/600 [01:41<05:01,  1.17it/s] 41%|████▏     | 248/600 [01:41<04:00,  1.46it/s] 42%|████▏     | 249/600 [01:41<03:17,  1.78it/s] 42%|████▏     | 250/600 [01:42<02:47,  2.09it/s] 42%|████▏     | 251/600 [01:42<02:26,  2.38it/s] 42%|████▏     | 252/600 [01:42<02:11,  2.64it/s] 42%|████▏     | 253/600 [01:43<02:01,  2.86it/s] 42%|████▏     | 254/600 [01:43<01:54,  3.03it/s] 42%|████▎     | 255/600 [01:43<01:48,  3.17it/s] 43%|████▎     | 256/600 [01:43<01:45,  3.27it/s] 43%|████▎     | 257/600 [01:44<01:42,  3.36it/s] 43%|████▎     | 258/600 [01:44<01:40,  3.41it/s] 43%|████▎     | 259/600 [01:44<01:38,  3.45it/s] 43%|████▎     | 260/600 [01:44<01:37,  3.48it/s] 44%|████▎     | 261/600 [01:45<01:36,  3.50it/s] 44%|████▎     | 262/600 [01:45<01:36,  3.51it/s] 44%|████▍     | 263/600 [01:45<01:35,  3.52it/s] 44%|████▍     | 264/600 [01:46<01:35,  3.53it/s] 44%|████▍     | 265/600 [01:46<01:35,  3.52it/s] 44%|████▍     | 266/600 [01:46<01:34,  3.53it/s] 44%|████▍     | 267/600 [01:46<01:34,  3.54it/s] 45%|████▍     | 268/600 [01:47<01:33,  3.54it/s] 45%|████▍     | 269/600 [01:47<01:33,  3.55it/s] 45%|████▌     | 270/600 [01:47<01:33,  3.55it/s] 45%|████▌     | 271/600 [01:48<01:32,  3.54it/s] 45%|████▌     | 272/600 [01:48<01:32,  3.54it/s] 46%|████▌     | 273/600 [01:48<01:32,  3.54it/s] 46%|████▌     | 274/600 [01:48<01:31,  3.55it/s] 46%|████▌     | 275/600 [01:49<01:31,  3.55it/s] 46%|████▌     | 276/600 [01:49<01:31,  3.53it/s] 46%|████▌     | 277/600 [01:49<01:31,  3.53it/s] 46%|████▋     | 278/600 [01:50<01:31,  3.53it/s] 46%|████▋     | 279/600 [01:50<01:30,  3.54it/s] 47%|████▋     | 280/600 [01:50<01:30,  3.54it/s] 47%|████▋     | 281/600 [01:50<01:29,  3.54it/s] 47%|████▋     | 282/600 [01:51<01:30,  3.53it/s] 47%|████▋     | 283/600 [01:51<01:29,  3.53it/s] 47%|████▋     | 284/600 [01:51<01:29,  3.53it/s] 48%|████▊     | 285/600 [01:52<01:28,  3.55it/s] 48%|████▊     | 286/600 [01:52<01:27,  3.57it/s] 48%|████▊     | 287/600 [01:52<01:27,  3.56it/s] 48%|████▊     | 288/600 [01:52<01:27,  3.57it/s] 48%|████▊     | 289/600 [01:53<01:26,  3.58it/s] 48%|████▊     | 290/600 [01:53<01:26,  3.59it/s] 48%|████▊     | 291/600 [01:53<01:25,  3.59it/s] 49%|████▊     | 292/600 [01:54<01:25,  3.60it/s] 49%|████▉     | 293/600 [01:54<01:25,  3.60it/s] 49%|████▉     | 294/600 [01:54<01:25,  3.60it/s] 49%|████▉     | 295/600 [01:54<01:24,  3.60it/s] 49%|████▉     | 296/600 [01:55<01:24,  3.60it/s] 50%|████▉     | 297/600 [01:55<01:24,  3.58it/s] 50%|████▉     | 298/600 [01:55<01:24,  3.58it/s] 50%|████▉     | 299/600 [01:55<01:23,  3.59it/s] 50%|█████     | 300/600 [01:56<01:23,  3.59it/s] 50%|█████     | 301/600 [01:56<01:23,  3.59it/s] 50%|█████     | 302/600 [01:56<01:22,  3.60it/s] 50%|█████     | 303/600 [01:57<01:22,  3.60it/s] 51%|█████     | 304/600 [01:57<01:22,  3.60it/s] 51%|█████     | 305/600 [01:57<01:21,  3.60it/s] 51%|█████     | 306/600 [01:57<01:21,  3.60it/s] 51%|█████     | 307/600 [01:58<01:21,  3.60it/s] 51%|█████▏    | 308/600 [01:58<01:21,  3.60it/s] 52%|█████▏    | 309/600 [01:58<01:21,  3.58it/s] 52%|█████▏    | 310/600 [01:59<01:20,  3.58it/s] 52%|█████▏    | 311/600 [01:59<01:20,  3.59it/s] 52%|█████▏    | 312/600 [01:59<01:20,  3.59it/s] 52%|█████▏    | 313/600 [01:59<01:19,  3.59it/s] 52%|█████▏    | 314/600 [02:00<01:21,  3.50it/s] 52%|█████▎    | 315/600 [02:00<01:21,  3.50it/s] 53%|█████▎    | 316/600 [02:00<01:20,  3.53it/s] 53%|█████▎    | 317/600 [02:00<01:19,  3.55it/s] 53%|█████▎    | 318/600 [02:01<01:19,  3.57it/s] 53%|█████▎    | 319/600 [02:01<01:18,  3.58it/s] 53%|█████▎    | 320/600 [02:01<01:18,  3.57it/s] 54%|█████▎    | 321/600 [02:02<01:18,  3.58it/s] 54%|█████▎    | 322/600 [02:02<01:17,  3.59it/s] 54%|█████▍    | 323/600 [02:02<01:17,  3.59it/s] 54%|█████▍    | 324/600 [02:02<01:16,  3.59it/s] 54%|█████▍    | 325/600 [02:03<01:16,  3.60it/s] 54%|█████▍    | 326/600 [02:03<01:16,  3.60it/s] 55%|█████▍    | 327/600 [02:03<01:15,  3.60it/s] 55%|█████▍    | 328/600 [02:04<01:15,  3.59it/s] 55%|█████▍    | 329/600 [02:04<01:15,  3.60it/s] 55%|█████▌    | 330/600 [02:04<01:15,  3.60it/s] 55%|█████▌    | 331/600 [02:04<01:15,  3.57it/s] 55%|█████▌    | 332/600 [02:05<01:14,  3.58it/s] 56%|█████▌    | 333/600 [02:05<01:14,  3.58it/s] 56%|█████▌    | 334/600 [02:05<01:14,  3.59it/s] 56%|█████▌    | 335/600 [02:06<01:13,  3.59it/s] 56%|█████▌    | 336/600 [02:06<01:13,  3.60it/s] 56%|█████▌    | 337/600 [02:06<01:13,  3.60it/s] 56%|█████▋    | 338/600 [02:06<01:12,  3.60it/s] 56%|█████▋    | 339/600 [02:07<01:12,  3.60it/s] 57%|█████▋    | 340/600 [02:07<01:12,  3.60it/s] 57%|█████▋    | 341/600 [02:07<01:11,  3.60it/s] 57%|█████▋    | 342/600 [02:07<01:11,  3.58it/s] 57%|█████▋    | 343/600 [02:08<01:12,  3.57it/s] 57%|█████▋    | 344/600 [02:08<01:11,  3.57it/s] 57%|█████▊    | 345/600 [02:08<01:11,  3.58it/s] 58%|█████▊    | 346/600 [02:09<01:10,  3.58it/s] 58%|█████▊    | 347/600 [02:09<01:10,  3.59it/s] 58%|█████▊    | 348/600 [02:09<01:10,  3.59it/s] 58%|█████▊    | 349/600 [02:09<01:09,  3.59it/s] 58%|█████▊    | 350/600 [02:10<01:09,  3.60it/s] 58%|█████▊    | 351/600 [02:10<01:09,  3.60it/s] 59%|█████▊    | 352/600 [02:10<01:08,  3.60it/s] 59%|█████▉    | 353/600 [02:11<01:08,  3.59it/s] 59%|█████▉    | 354/600 [02:11<01:08,  3.60it/s] 59%|█████▉    | 355/600 [02:11<01:08,  3.59it/s] 59%|█████▉    | 356/600 [02:11<01:07,  3.60it/s] 60%|█████▉    | 357/600 [02:12<01:07,  3.60it/s] 60%|█████▉    | 358/600 [02:12<01:07,  3.60it/s] 60%|█████▉    | 359/600 [02:12<01:06,  3.60it/s] 60%|██████    | 360/600 [02:12<01:06,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 12:06:14,870 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:06:14,870 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 12:06:14,870 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8726, 'eval_samples_per_second': 351.917, 'eval_steps_per_second': 44.044, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.91it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.41it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.48it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.43it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.64it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.13it/s][A
  6%|▌         | 37/611 [00:00<00:13, 43.94it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.86it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.19it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.42it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.54it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.40it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.29it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.01it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.02it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.96it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.89it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.10it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.34it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.49it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.36it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.17it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.03it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.97it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.91it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.87it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.10it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.33it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.35it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.33it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.11it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.04it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.91it/s][A
 28%|██▊       | 172/611 [00:03<00:10, 43.15it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.47it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.88it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.10it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.24it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.23it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.13it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.95it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.83it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.95it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.95it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.11it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.21it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.36it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.34it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.20it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.02it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.99it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.16it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.18it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.27it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.36it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.35it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 43.99it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.96it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.04it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.09it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.12it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.19it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.26it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.34it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.37it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.16it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.98it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.85it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.01it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.05it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.20it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.26it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.34it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.19it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.20it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.08it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.93it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.93it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.05it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.14it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.21it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.20it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.30it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.20it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.04it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.99it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 43.84it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.08it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.25it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.28it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.20it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.27it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.18it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.03it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.97it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.02it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.08it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.31it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.28it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.21it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.27it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.11it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.99it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.96it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.03it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.05it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.30it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.29it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.26it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.29it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.19it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.07it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 43.91it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.06it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.20it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.18it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.20it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.31it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.25it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.12it/s][A                                                 
                                                 [A 60%|██████    | 360/600 [02:26<01:06,  3.60it/s]
100%|██████████| 611/611 [00:13<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:06:28,773 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360
[INFO|configuration_utils.py:351] 2023-08-28 12:06:28,795 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:06:30,613 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:06:30,628 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:06:30,639 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360/special_tokens_map.json
 60%|██████    | 361/600 [02:29<20:21,  5.11s/it] 60%|██████    | 362/600 [02:29<14:31,  3.66s/it] 60%|██████    | 363/600 [02:29<10:27,  2.65s/it] 61%|██████    | 364/600 [02:30<07:37,  1.94s/it] 61%|██████    | 365/600 [02:30<05:38,  1.44s/it] 61%|██████    | 366/600 [02:30<04:15,  1.09s/it] 61%|██████    | 367/600 [02:31<03:18,  1.18it/s] 61%|██████▏   | 368/600 [02:31<02:37,  1.47it/s] 62%|██████▏   | 369/600 [02:31<02:09,  1.78it/s] 62%|██████▏   | 370/600 [02:31<01:49,  2.10it/s] 62%|██████▏   | 371/600 [02:32<01:35,  2.40it/s] 62%|██████▏   | 372/600 [02:32<01:25,  2.67it/s] 62%|██████▏   | 373/600 [02:32<01:18,  2.88it/s] 62%|██████▏   | 374/600 [02:33<01:13,  3.06it/s] 62%|██████▎   | 375/600 [02:33<01:10,  3.21it/s] 63%|██████▎   | 376/600 [02:33<01:07,  3.31it/s] 63%|██████▎   | 377/600 [02:33<01:05,  3.40it/s] 63%|██████▎   | 378/600 [02:34<01:04,  3.45it/s] 63%|██████▎   | 379/600 [02:34<01:03,  3.50it/s] 63%|██████▎   | 380/600 [02:34<01:02,  3.53it/s] 64%|██████▎   | 381/600 [02:34<01:01,  3.55it/s] 64%|██████▎   | 382/600 [02:35<01:01,  3.56it/s] 64%|██████▍   | 383/600 [02:35<01:00,  3.57it/s] 64%|██████▍   | 384/600 [02:35<01:00,  3.57it/s] 64%|██████▍   | 385/600 [02:36<01:00,  3.58it/s] 64%|██████▍   | 386/600 [02:36<00:59,  3.58it/s] 64%|██████▍   | 387/600 [02:36<00:59,  3.58it/s] 65%|██████▍   | 388/600 [02:36<00:59,  3.59it/s] 65%|██████▍   | 389/600 [02:37<00:58,  3.60it/s] 65%|██████▌   | 390/600 [02:37<00:58,  3.60it/s] 65%|██████▌   | 391/600 [02:37<00:58,  3.60it/s] 65%|██████▌   | 392/600 [02:38<00:57,  3.60it/s] 66%|██████▌   | 393/600 [02:38<00:57,  3.60it/s] 66%|██████▌   | 394/600 [02:38<00:57,  3.60it/s] 66%|██████▌   | 395/600 [02:38<00:57,  3.58it/s] 66%|██████▌   | 396/600 [02:39<00:56,  3.59it/s] 66%|██████▌   | 397/600 [02:39<00:56,  3.59it/s] 66%|██████▋   | 398/600 [02:39<00:56,  3.59it/s] 66%|██████▋   | 399/600 [02:39<00:55,  3.59it/s] 67%|██████▋   | 400/600 [02:40<00:55,  3.60it/s] 67%|██████▋   | 401/600 [02:40<00:55,  3.60it/s] 67%|██████▋   | 402/600 [02:40<00:54,  3.60it/s] 67%|██████▋   | 403/600 [02:41<00:54,  3.60it/s] 67%|██████▋   | 404/600 [02:41<00:54,  3.60it/s] 68%|██████▊   | 405/600 [02:41<00:54,  3.60it/s] 68%|██████▊   | 406/600 [02:41<00:54,  3.58it/s] 68%|██████▊   | 407/600 [02:42<00:53,  3.59it/s] 68%|██████▊   | 408/600 [02:42<00:53,  3.59it/s] 68%|██████▊   | 409/600 [02:42<00:53,  3.59it/s] 68%|██████▊   | 410/600 [02:43<00:52,  3.60it/s] 68%|██████▊   | 411/600 [02:43<00:52,  3.60it/s] 69%|██████▊   | 412/600 [02:43<00:52,  3.60it/s] 69%|██████▉   | 413/600 [02:43<00:52,  3.59it/s] 69%|██████▉   | 414/600 [02:44<00:51,  3.60it/s] 69%|██████▉   | 415/600 [02:44<00:51,  3.60it/s] 69%|██████▉   | 416/600 [02:44<00:51,  3.60it/s] 70%|██████▉   | 417/600 [02:44<00:51,  3.58it/s] 70%|██████▉   | 418/600 [02:45<00:50,  3.59it/s] 70%|██████▉   | 419/600 [02:45<00:50,  3.59it/s] 70%|███████   | 420/600 [02:45<00:50,  3.59it/s] 70%|███████   | 421/600 [02:46<00:49,  3.59it/s] 70%|███████   | 422/600 [02:46<00:49,  3.59it/s] 70%|███████   | 423/600 [02:46<00:49,  3.59it/s] 71%|███████   | 424/600 [02:46<00:48,  3.60it/s] 71%|███████   | 425/600 [02:47<00:48,  3.60it/s] 71%|███████   | 426/600 [02:47<00:48,  3.60it/s] 71%|███████   | 427/600 [02:47<00:48,  3.60it/s] 71%|███████▏  | 428/600 [02:48<00:47,  3.58it/s] 72%|███████▏  | 429/600 [02:48<00:47,  3.59it/s] 72%|███████▏  | 430/600 [02:48<00:47,  3.59it/s] 72%|███████▏  | 431/600 [02:48<00:47,  3.59it/s] 72%|███████▏  | 432/600 [02:49<00:46,  3.59it/s] 72%|███████▏  | 433/600 [02:49<00:46,  3.59it/s] 72%|███████▏  | 434/600 [02:49<00:46,  3.60it/s] 72%|███████▎  | 435/600 [02:49<00:45,  3.60it/s] 73%|███████▎  | 436/600 [02:50<00:45,  3.60it/s] 73%|███████▎  | 437/600 [02:50<00:45,  3.60it/s] 73%|███████▎  | 438/600 [02:50<00:44,  3.60it/s] 73%|███████▎  | 439/600 [02:51<00:44,  3.58it/s] 73%|███████▎  | 440/600 [02:51<00:44,  3.58it/s] 74%|███████▎  | 441/600 [02:51<00:44,  3.59it/s] 74%|███████▎  | 442/600 [02:51<00:44,  3.59it/s] 74%|███████▍  | 443/600 [02:52<00:43,  3.59it/s] 74%|███████▍  | 444/600 [02:52<00:43,  3.59it/s] 74%|███████▍  | 445/600 [02:52<00:43,  3.60it/s] 74%|███████▍  | 446/600 [02:53<00:42,  3.60it/s] 74%|███████▍  | 447/600 [02:53<00:42,  3.59it/s] 75%|███████▍  | 448/600 [02:53<00:42,  3.59it/s] 75%|███████▍  | 449/600 [02:53<00:42,  3.59it/s] 75%|███████▌  | 450/600 [02:54<00:42,  3.57it/s] 75%|███████▌  | 451/600 [02:54<00:41,  3.58it/s] 75%|███████▌  | 452/600 [02:54<00:41,  3.59it/s] 76%|███████▌  | 453/600 [02:54<00:40,  3.59it/s] 76%|███████▌  | 454/600 [02:55<00:40,  3.60it/s] 76%|███████▌  | 455/600 [02:55<00:40,  3.58it/s] 76%|███████▌  | 456/600 [02:55<00:40,  3.58it/s] 76%|███████▌  | 457/600 [02:56<00:39,  3.59it/s] 76%|███████▋  | 458/600 [02:56<00:39,  3.59it/s] 76%|███████▋  | 459/600 [02:56<00:39,  3.60it/s] 77%|███████▋  | 460/600 [02:56<00:38,  3.60it/s] 77%|███████▋  | 461/600 [02:57<00:38,  3.60it/s] 77%|███████▋  | 462/600 [02:57<00:38,  3.60it/s] 77%|███████▋  | 463/600 [02:57<00:38,  3.60it/s] 77%|███████▋  | 464/600 [02:58<00:37,  3.60it/s] 78%|███████▊  | 465/600 [02:58<00:37,  3.60it/s] 78%|███████▊  | 466/600 [02:58<00:37,  3.60it/s] 78%|███████▊  | 467/600 [02:58<00:36,  3.60it/s] 78%|███████▊  | 468/600 [02:59<00:36,  3.60it/s] 78%|███████▊  | 469/600 [02:59<00:36,  3.60it/s] 78%|███████▊  | 470/600 [02:59<00:36,  3.60it/s] 78%|███████▊  | 471/600 [03:00<00:36,  3.58it/s] 79%|███████▊  | 472/600 [03:00<00:36,  3.52it/s] 79%|███████▉  | 473/600 [03:00<00:35,  3.55it/s] 79%|███████▉  | 474/600 [03:00<00:35,  3.56it/s] 79%|███████▉  | 475/600 [03:01<00:34,  3.57it/s] 79%|███████▉  | 476/600 [03:01<00:34,  3.59it/s] 80%|███████▉  | 477/600 [03:01<00:34,  3.59it/s] 80%|███████▉  | 478/600 [03:01<00:33,  3.59it/s] 80%|███████▉  | 479/600 [03:02<00:33,  3.59it/s] 80%|████████  | 480/600 [03:02<00:33,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 12:07:04,424 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:07:04,424 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 12:07:04,424 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8883, 'eval_samples_per_second': 351.519, 'eval_steps_per_second': 43.994, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.62it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.56it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.83it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.64it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.68it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.33it/s][A
  6%|▌         | 37/611 [00:00<00:13, 44.09it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.84it/s][A
  8%|▊         | 47/611 [00:01<00:12, 43.97it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.26it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.38it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.42it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.49it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.26it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.08it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.88it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.69it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.86it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.07it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.22it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.35it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.43it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.21it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.02it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.77it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.84it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.89it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.22it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.32it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.45it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.39it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.17it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.10it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.93it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.93it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.11it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.21it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.29it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.41it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.22it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.06it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.97it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.85it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.97it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.08it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.23it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.36it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.38it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.25it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.11it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.05it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.84it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.89it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.08it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.31it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.32it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.34it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.28it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.05it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.05it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.02it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.05it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.17it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.31it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.26it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.25it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.25it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.10it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.88it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.18it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.24it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.26it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.29it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.20it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.10it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.93it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.88it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.97it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.15it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.23it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.34it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.31it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.17it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.11it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.04it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.91it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 43.90it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.10it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.21it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.29it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.21it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.24it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.17it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.94it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.89it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.10it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.24it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.31it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.35it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.20it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.25it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.15it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.95it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.86it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.97it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.12it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.27it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.30it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.25it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.17it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.01it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.93it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 43.90it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.11it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.13it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.39it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.25it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.30it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.21it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.05it/s][A                                                 
                                                 [A 80%|████████  | 480/600 [03:16<00:33,  3.59it/s]
100%|██████████| 611/611 [00:13<00:00, 44.05it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:07:18,317 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480
[INFO|configuration_utils.py:351] 2023-08-28 12:07:18,333 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:07:20,593 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:07:20,615 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:07:20,627 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480/special_tokens_map.json
 80%|████████  | 481/600 [03:19<10:23,  5.24s/it] 80%|████████  | 482/600 [03:19<07:22,  3.75s/it] 80%|████████  | 483/600 [03:19<05:17,  2.71s/it] 81%|████████  | 484/600 [03:20<03:50,  1.98s/it] 81%|████████  | 485/600 [03:20<02:49,  1.47s/it] 81%|████████  | 486/600 [03:20<02:07,  1.11s/it] 81%|████████  | 487/600 [03:21<01:37,  1.16it/s] 81%|████████▏ | 488/600 [03:21<01:17,  1.45it/s] 82%|████████▏ | 489/600 [03:21<01:02,  1.77it/s] 82%|████████▏ | 490/600 [03:21<00:52,  2.09it/s] 82%|████████▏ | 491/600 [03:22<00:45,  2.38it/s] 82%|████████▏ | 492/600 [03:22<00:40,  2.65it/s] 82%|████████▏ | 493/600 [03:22<00:37,  2.88it/s] 82%|████████▏ | 494/600 [03:22<00:34,  3.07it/s] 82%|████████▎ | 495/600 [03:23<00:32,  3.21it/s] 83%|████████▎ | 496/600 [03:23<00:31,  3.31it/s] 83%|████████▎ | 497/600 [03:23<00:30,  3.39it/s] 83%|████████▎ | 498/600 [03:24<00:29,  3.45it/s] 83%|████████▎ | 499/600 [03:24<00:28,  3.49it/s] 83%|████████▎ | 500/600 [03:24<00:28,  3.52it/s]                                                  83%|████████▎ | 500/600 [03:24<00:28,  3.52it/s] 84%|████████▎ | 501/600 [03:24<00:27,  3.54it/s] 84%|████████▎ | 502/600 [03:25<00:27,  3.53it/s] 84%|████████▍ | 503/600 [03:25<00:27,  3.54it/s] 84%|████████▍ | 504/600 [03:25<00:26,  3.56it/s] 84%|████████▍ | 505/600 [03:26<00:26,  3.57it/s] 84%|████████▍ | 506/600 [03:26<00:26,  3.58it/s] 84%|████████▍ | 507/600 [03:26<00:25,  3.59it/s] 85%|████████▍ | 508/600 [03:26<00:25,  3.59it/s] 85%|████████▍ | 509/600 [03:27<00:25,  3.60it/s] 85%|████████▌ | 510/600 [03:27<00:25,  3.60it/s] 85%|████████▌ | 511/600 [03:27<00:24,  3.59it/s] 85%|████████▌ | 512/600 [03:27<00:24,  3.59it/s] 86%|████████▌ | 513/600 [03:28<00:24,  3.59it/s] 86%|████████▌ | 514/600 [03:28<00:23,  3.60it/s] 86%|████████▌ | 515/600 [03:28<00:23,  3.60it/s] 86%|████████▌ | 516/600 [03:29<00:23,  3.60it/s] 86%|████████▌ | 517/600 [03:29<00:23,  3.60it/s] 86%|████████▋ | 518/600 [03:29<00:22,  3.60it/s] 86%|████████▋ | 519/600 [03:29<00:22,  3.60it/s] 87%|████████▋ | 520/600 [03:30<00:22,  3.60it/s] 87%|████████▋ | 521/600 [03:30<00:21,  3.60it/s] 87%|████████▋ | 522/600 [03:30<00:21,  3.58it/s] 87%|████████▋ | 523/600 [03:31<00:21,  3.59it/s] 87%|████████▋ | 524/600 [03:31<00:21,  3.59it/s] 88%|████████▊ | 525/600 [03:31<00:20,  3.59it/s] 88%|████████▊ | 526/600 [03:31<00:20,  3.59it/s] 88%|████████▊ | 527/600 [03:32<00:20,  3.60it/s] 88%|████████▊ | 528/600 [03:32<00:20,  3.60it/s] 88%|████████▊ | 529/600 [03:32<00:19,  3.59it/s] 88%|████████▊ | 530/600 [03:32<00:19,  3.60it/s] 88%|████████▊ | 531/600 [03:33<00:19,  3.60it/s] 89%|████████▊ | 532/600 [03:33<00:18,  3.60it/s] 89%|████████▉ | 533/600 [03:33<00:18,  3.59it/s] 89%|████████▉ | 534/600 [03:34<00:18,  3.59it/s] 89%|████████▉ | 535/600 [03:34<00:18,  3.59it/s] 89%|████████▉ | 536/600 [03:34<00:17,  3.59it/s] 90%|████████▉ | 537/600 [03:34<00:17,  3.59it/s] 90%|████████▉ | 538/600 [03:35<00:17,  3.58it/s] 90%|████████▉ | 539/600 [03:35<00:16,  3.59it/s] 90%|█████████ | 540/600 [03:35<00:16,  3.59it/s] 90%|█████████ | 541/600 [03:36<00:16,  3.59it/s] 90%|█████████ | 542/600 [03:36<00:16,  3.60it/s] 90%|█████████ | 543/600 [03:36<00:15,  3.60it/s] 91%|█████████ | 544/600 [03:36<00:15,  3.58it/s] 91%|█████████ | 545/600 [03:37<00:15,  3.58it/s] 91%|█████████ | 546/600 [03:37<00:15,  3.59it/s] 91%|█████████ | 547/600 [03:37<00:14,  3.59it/s] 91%|█████████▏| 548/600 [03:38<00:14,  3.59it/s] 92%|█████████▏| 549/600 [03:38<00:14,  3.59it/s] 92%|█████████▏| 550/600 [03:38<00:13,  3.59it/s] 92%|█████████▏| 551/600 [03:38<00:13,  3.59it/s] 92%|█████████▏| 552/600 [03:39<00:13,  3.59it/s] 92%|█████████▏| 553/600 [03:39<00:13,  3.60it/s] 92%|█████████▏| 554/600 [03:39<00:12,  3.60it/s] 92%|█████████▎| 555/600 [03:39<00:12,  3.59it/s] 93%|█████████▎| 556/600 [03:40<00:12,  3.59it/s] 93%|█████████▎| 557/600 [03:40<00:11,  3.59it/s] 93%|█████████▎| 558/600 [03:40<00:11,  3.60it/s] 93%|█████████▎| 559/600 [03:41<00:11,  3.60it/s] 93%|█████████▎| 560/600 [03:41<00:11,  3.60it/s] 94%|█████████▎| 561/600 [03:41<00:10,  3.60it/s] 94%|█████████▎| 562/600 [03:41<00:10,  3.59it/s] 94%|█████████▍| 563/600 [03:42<00:10,  3.59it/s] 94%|█████████▍| 564/600 [03:42<00:10,  3.59it/s] 94%|█████████▍| 565/600 [03:42<00:09,  3.60it/s] 94%|█████████▍| 566/600 [03:43<00:09,  3.58it/s] 94%|█████████▍| 567/600 [03:43<00:09,  3.59it/s] 95%|█████████▍| 568/600 [03:43<00:08,  3.59it/s] 95%|█████████▍| 569/600 [03:43<00:08,  3.60it/s] 95%|█████████▌| 570/600 [03:44<00:08,  3.60it/s] 95%|█████████▌| 571/600 [03:44<00:08,  3.60it/s] 95%|█████████▌| 572/600 [03:44<00:07,  3.60it/s] 96%|█████████▌| 573/600 [03:44<00:07,  3.60it/s] 96%|█████████▌| 574/600 [03:45<00:07,  3.60it/s] 96%|█████████▌| 575/600 [03:45<00:06,  3.60it/s] 96%|█████████▌| 576/600 [03:45<00:06,  3.59it/s] 96%|█████████▌| 577/600 [03:46<00:06,  3.57it/s] 96%|█████████▋| 578/600 [03:46<00:06,  3.58it/s] 96%|█████████▋| 579/600 [03:46<00:05,  3.59it/s] 97%|█████████▋| 580/600 [03:46<00:05,  3.59it/s] 97%|█████████▋| 581/600 [03:47<00:05,  3.60it/s] 97%|█████████▋| 582/600 [03:47<00:05,  3.60it/s] 97%|█████████▋| 583/600 [03:47<00:04,  3.60it/s] 97%|█████████▋| 584/600 [03:48<00:04,  3.60it/s] 98%|█████████▊| 585/600 [03:48<00:04,  3.60it/s] 98%|█████████▊| 586/600 [03:48<00:03,  3.60it/s] 98%|█████████▊| 587/600 [03:48<00:03,  3.60it/s] 98%|█████████▊| 588/600 [03:49<00:03,  3.57it/s] 98%|█████████▊| 589/600 [03:49<00:03,  3.58it/s] 98%|█████████▊| 590/600 [03:49<00:02,  3.58it/s] 98%|█████████▊| 591/600 [03:49<00:02,  3.59it/s] 99%|█████████▊| 592/600 [03:50<00:02,  3.59it/s] 99%|█████████▉| 593/600 [03:50<00:01,  3.59it/s] 99%|█████████▉| 594/600 [03:50<00:01,  3.60it/s] 99%|█████████▉| 595/600 [03:51<00:01,  3.60it/s] 99%|█████████▉| 596/600 [03:51<00:01,  3.60it/s]100%|█████████▉| 597/600 [03:51<00:00,  3.59it/s]100%|█████████▉| 598/600 [03:51<00:00,  3.59it/s]100%|█████████▉| 599/600 [03:52<00:00,  3.57it/s]100%|██████████| 600/600 [03:52<00:00,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 12:07:54,354 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:07:54,354 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 12:07:54,354 >>   Batch size = 8
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8806, 'eval_samples_per_second': 351.715, 'eval_steps_per_second': 44.018, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.6625e-05, 'epoch': 4.17}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.51it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.70it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.70it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.71it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.03it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.17it/s][A
  6%|▌         | 37/611 [00:00<00:13, 43.99it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.84it/s][A
  8%|▊         | 47/611 [00:01<00:12, 43.89it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.14it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.33it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.43it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.44it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.26it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.09it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.87it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.83it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.95it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.05it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.40it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.39it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.34it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.23it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.16it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.88it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.99it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.18it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.25it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.33it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.19it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.14it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.08it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.08it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.89it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.09it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.19it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.29it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.29it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.21it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.17it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.08it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.06it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.04it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.16it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.26it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.32it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.31it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.15it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.15it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.11it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.09it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.12it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.19it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.19it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.24it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.35it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.29it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.15it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.08it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.02it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.09it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.13it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.25it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.21it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.29it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.26it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.19it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.17it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.64it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.81it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.04it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.09it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.10it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.17it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.18it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.01it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.92it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.08it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.17it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.23it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.29it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.20it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.21it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.19it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.11it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.97it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.01it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.19it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.26it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.30it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.26it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.28it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.06it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.97it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.00it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.12it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.11it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.27it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.34it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.34it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.31it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.07it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.00it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.14it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.19it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.19it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 43.99it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.40it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.24it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.19it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.09it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.97it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.16it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.24it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.27it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.29it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.34it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.21it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.07it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.09it/s][A                                                 
                                                 [A100%|██████████| 600/600 [04:06<00:00,  3.58it/s]
100%|██████████| 611/611 [00:13<00:00, 44.09it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:08:08,205 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600
[INFO|configuration_utils.py:351] 2023-08-28 12:08:08,224 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:08:10,332 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:08:10,347 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:08:10,359 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 12:08:10,631 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 12:08:10,632 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120 (score: 0.9401851296424866).
                                                 100%|██████████| 600/600 [04:10<00:00,  3.58it/s]100%|██████████| 600/600 [04:10<00:00,  2.40it/s]
[INFO|trainer.py:1894] 2023-08-28 12:08:12,365 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 12:08:12,379 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:08:14,493 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:08:14,517 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:08:14,526 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:08:14,704 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:14,705 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:14,705 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:14,705 >>   train_runtime            = 0:04:10.49
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:14,705 >>   train_samples            =       7700
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:14,705 >>   train_samples_per_second =    153.697
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:14,705 >>   train_steps_per_second   =      2.395
{'eval_loss': 0.9401851296424866, 'eval_runtime': 13.8319, 'eval_samples_per_second': 352.953, 'eval_steps_per_second': 44.173, 'epoch': 5.0}
{'train_runtime': 250.4933, 'train_samples_per_second': 153.697, 'train_steps_per_second': 2.395, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 12:08:14 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 12:08:14,739 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:08:14,739 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 12:08:14,739 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.39it/s]  2%|▏         | 12/611 [00:00<00:12, 48.46it/s]  3%|▎         | 17/611 [00:00<00:12, 47.03it/s]  4%|▎         | 22/611 [00:00<00:12, 46.27it/s]  4%|▍         | 27/611 [00:00<00:12, 45.87it/s]  5%|▌         | 32/611 [00:00<00:12, 45.57it/s]  6%|▌         | 37/611 [00:00<00:12, 45.34it/s]  7%|▋         | 42/611 [00:00<00:12, 44.82it/s]  8%|▊         | 47/611 [00:01<00:12, 44.16it/s]  9%|▊         | 52/611 [00:01<00:12, 44.00it/s]  9%|▉         | 57/611 [00:01<00:12, 44.15it/s] 10%|█         | 62/611 [00:01<00:12, 44.24it/s] 11%|█         | 67/611 [00:01<00:12, 44.46it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.67it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.78it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.60it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.16it/s] 15%|█▌        | 92/611 [00:02<00:11, 43.97it/s] 16%|█▌        | 97/611 [00:02<00:11, 43.74it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.93it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.16it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.30it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.59it/s] 20%|█▉        | 122/611 [00:02<00:10, 44.67it/s] 21%|██        | 127/611 [00:02<00:10, 44.54it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.17it/s] 22%|██▏       | 137/611 [00:03<00:10, 43.86it/s] 23%|██▎       | 142/611 [00:03<00:10, 43.85it/s] 24%|██▍       | 147/611 [00:03<00:10, 43.98it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.19it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.43it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.36it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.67it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.45it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.35it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.07it/s] 31%|███       | 187/611 [00:04<00:09, 43.91it/s] 31%|███▏      | 192/611 [00:04<00:09, 43.98it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.22it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.31it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.54it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.56it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.46it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.30it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.08it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.03it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.16it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.24it/s] 40%|████      | 247/611 [00:05<00:08, 44.33it/s] 41%|████      | 252/611 [00:05<00:08, 44.45it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.67it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.55it/s] 44%|████▎     | 267/611 [00:05<00:07, 44.28it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.14it/s] 45%|████▌     | 277/611 [00:06<00:07, 44.18it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.18it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.15it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.35it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.44it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.48it/s] 50%|█████     | 307/611 [00:06<00:06, 44.38it/s] 51%|█████     | 312/611 [00:07<00:06, 44.22it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.14it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.22it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.16it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.27it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.38it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.51it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.56it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.32it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.20it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.18it/s] 60%|██████    | 367/611 [00:08<00:05, 44.07it/s] 61%|██████    | 372/611 [00:08<00:05, 44.14it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.14it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.37it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.55it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.54it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.32it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.08it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.18it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.24it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.09it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.10it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.40it/s] 71%|███████   | 432/611 [00:09<00:04, 44.51it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.49it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.26it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.12it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.19it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.17it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.10it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.23it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.39it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.56it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.45it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.30it/s] 81%|████████  | 492/611 [00:11<00:02, 44.21it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.26it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.12it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.13it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.26it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.33it/s] 85%|████████▌ | 522/611 [00:11<00:02, 44.47it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.34it/s] 87%|████████▋ | 532/611 [00:11<00:01, 44.28it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.26it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.22it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.16it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.27it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.28it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.41it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.48it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.34it/s] 94%|█████████▍| 577/611 [00:12<00:00, 44.28it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.21it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.17it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.13it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.21it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.28it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.46it/s]100%|██████████| 611/611 [00:13<00:00, 44.39it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:08:28,521 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:28,521 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:28,521 >>   eval_loss               =     0.9402
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:28,521 >>   eval_runtime            = 0:00:13.78
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:28,521 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:28,521 >>   eval_samples_per_second =    354.236
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:28,521 >>   eval_steps_per_second   =     44.334
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:08:28,521 >>   perplexity              =     2.5605
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:33,534 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:33,536 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:33,536 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:33,536 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:33,536 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:08:34,160 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:08:34,163 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:08:34,730 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:08:35,766 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:08:35,766 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:38,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:38,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:38,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:38,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:08:38,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:08:39,281 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:08:39,282 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:08:39,843 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:08:40,014 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:08:40,014 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.73it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.83it/s]Extractor Predicting: 11it [00:06,  1.80it/s]Extractor Predicting: 12it [00:07,  1.76it/s]Extractor Predicting: 13it [00:07,  1.78it/s]Extractor Predicting: 14it [00:08,  1.81it/s]Extractor Predicting: 15it [00:08,  1.80it/s]Extractor Predicting: 16it [00:09,  1.83it/s]Extractor Predicting: 17it [00:09,  1.83it/s]Extractor Predicting: 18it [00:10,  1.83it/s]Extractor Predicting: 19it [00:10,  1.87it/s]Extractor Predicting: 20it [00:11,  1.88it/s]Extractor Predicting: 21it [00:11,  1.89it/s]Extractor Predicting: 22it [00:12,  1.95it/s]Extractor Predicting: 23it [00:12,  1.95it/s]Extractor Predicting: 24it [00:13,  1.95it/s]Extractor Predicting: 25it [00:13,  1.91it/s]Extractor Predicting: 26it [00:14,  1.87it/s]Extractor Predicting: 27it [00:14,  1.88it/s]Extractor Predicting: 28it [00:15,  1.88it/s]Extractor Predicting: 29it [00:16,  1.87it/s]Extractor Predicting: 30it [00:16,  1.89it/s]Extractor Predicting: 31it [00:17,  1.89it/s]Extractor Predicting: 32it [00:17,  1.94it/s]Extractor Predicting: 33it [00:18,  1.90it/s]Extractor Predicting: 34it [00:18,  1.88it/s]Extractor Predicting: 35it [00:19,  1.88it/s]Extractor Predicting: 36it [00:19,  1.85it/s]Extractor Predicting: 37it [00:20,  1.89it/s]Extractor Predicting: 38it [00:20,  1.89it/s]Extractor Predicting: 39it [00:21,  1.85it/s]Extractor Predicting: 40it [00:21,  1.89it/s]Extractor Predicting: 41it [00:22,  1.86it/s]Extractor Predicting: 42it [00:22,  1.87it/s]Extractor Predicting: 43it [00:23,  1.85it/s]Extractor Predicting: 44it [00:24,  1.85it/s]Extractor Predicting: 45it [00:24,  1.90it/s]Extractor Predicting: 46it [00:25,  1.88it/s]Extractor Predicting: 47it [00:25,  1.87it/s]Extractor Predicting: 48it [00:26,  1.86it/s]Extractor Predicting: 49it [00:26,  1.86it/s]Extractor Predicting: 50it [00:27,  1.81it/s]Extractor Predicting: 51it [00:27,  1.82it/s]Extractor Predicting: 52it [00:28,  1.88it/s]Extractor Predicting: 53it [00:28,  1.85it/s]Extractor Predicting: 54it [00:29,  1.84it/s]Extractor Predicting: 55it [00:30,  1.77it/s]Extractor Predicting: 56it [00:30,  1.73it/s]Extractor Predicting: 57it [00:31,  1.75it/s]Extractor Predicting: 58it [00:31,  1.74it/s]Extractor Predicting: 59it [00:32,  1.77it/s]Extractor Predicting: 60it [00:32,  1.76it/s]Extractor Predicting: 61it [00:33,  1.76it/s]Extractor Predicting: 62it [00:34,  1.73it/s]Extractor Predicting: 63it [00:34,  1.74it/s]Extractor Predicting: 64it [00:35,  1.75it/s]Extractor Predicting: 65it [00:35,  1.73it/s]Extractor Predicting: 66it [00:36,  1.70it/s]Extractor Predicting: 67it [00:36,  1.72it/s]Extractor Predicting: 68it [00:37,  1.77it/s]Extractor Predicting: 69it [00:38,  1.76it/s]Extractor Predicting: 70it [00:38,  1.73it/s]Extractor Predicting: 71it [00:39,  1.74it/s]Extractor Predicting: 72it [00:39,  1.70it/s]Extractor Predicting: 73it [00:40,  1.73it/s]Extractor Predicting: 74it [00:40,  1.73it/s]Extractor Predicting: 75it [00:41,  1.75it/s]Extractor Predicting: 76it [00:42,  1.76it/s]Extractor Predicting: 77it [00:42,  1.75it/s]Extractor Predicting: 78it [00:43,  1.78it/s]Extractor Predicting: 79it [00:43,  1.80it/s]Extractor Predicting: 80it [00:44,  1.79it/s]Extractor Predicting: 81it [00:44,  1.79it/s]Extractor Predicting: 82it [00:45,  1.76it/s]Extractor Predicting: 83it [00:46,  1.77it/s]Extractor Predicting: 84it [00:46,  1.76it/s]Extractor Predicting: 85it [00:47,  1.75it/s]Extractor Predicting: 86it [00:47,  1.73it/s]Extractor Predicting: 87it [00:48,  1.73it/s]Extractor Predicting: 88it [00:48,  1.69it/s]Extractor Predicting: 89it [00:49,  1.70it/s]Extractor Predicting: 90it [00:50,  1.70it/s]Extractor Predicting: 91it [00:50,  1.70it/s]Extractor Predicting: 92it [00:51,  1.77it/s]Extractor Predicting: 93it [00:51,  1.85it/s]Extractor Predicting: 94it [00:52,  1.84it/s]Extractor Predicting: 95it [00:52,  1.84it/s]Extractor Predicting: 96it [00:53,  1.84it/s]Extractor Predicting: 97it [00:53,  1.86it/s]Extractor Predicting: 98it [00:54,  1.81it/s]Extractor Predicting: 99it [00:55,  1.73it/s]Extractor Predicting: 100it [00:55,  1.77it/s]Extractor Predicting: 101it [00:56,  1.52it/s]Extractor Predicting: 102it [00:57,  1.55it/s]Extractor Predicting: 103it [00:57,  1.60it/s]Extractor Predicting: 104it [00:58,  1.65it/s]Extractor Predicting: 105it [00:58,  1.68it/s]Extractor Predicting: 106it [00:59,  1.75it/s]Extractor Predicting: 107it [00:59,  1.76it/s]Extractor Predicting: 108it [01:00,  1.80it/s]Extractor Predicting: 109it [01:01,  1.79it/s]Extractor Predicting: 110it [01:01,  1.79it/s]Extractor Predicting: 111it [01:02,  1.83it/s]Extractor Predicting: 112it [01:02,  1.83it/s]Extractor Predicting: 113it [01:03,  1.77it/s]Extractor Predicting: 114it [01:03,  1.75it/s]Extractor Predicting: 115it [01:04,  1.77it/s]Extractor Predicting: 116it [01:05,  1.73it/s]Extractor Predicting: 117it [01:05,  1.70it/s]Extractor Predicting: 118it [01:06,  1.71it/s]Extractor Predicting: 119it [01:06,  1.68it/s]Extractor Predicting: 120it [01:07,  1.65it/s]Extractor Predicting: 121it [01:08,  1.67it/s]Extractor Predicting: 122it [01:08,  1.68it/s]Extractor Predicting: 123it [01:09,  1.71it/s]Extractor Predicting: 124it [01:09,  1.71it/s]Extractor Predicting: 125it [01:10,  1.73it/s]Extractor Predicting: 126it [01:10,  1.73it/s]Extractor Predicting: 127it [01:11,  1.72it/s]Extractor Predicting: 128it [01:12,  1.71it/s]Extractor Predicting: 129it [01:12,  1.75it/s]Extractor Predicting: 130it [01:13,  1.68it/s]Extractor Predicting: 131it [01:13,  1.70it/s]Extractor Predicting: 132it [01:14,  1.74it/s]Extractor Predicting: 133it [01:14,  1.71it/s]Extractor Predicting: 134it [01:15,  1.72it/s]Extractor Predicting: 135it [01:16,  1.70it/s]Extractor Predicting: 136it [01:16,  1.73it/s]Extractor Predicting: 137it [01:17,  1.69it/s]Extractor Predicting: 138it [01:17,  1.72it/s]Extractor Predicting: 139it [01:18,  1.70it/s]Extractor Predicting: 140it [01:19,  1.68it/s]Extractor Predicting: 141it [01:19,  1.71it/s]Extractor Predicting: 142it [01:20,  1.72it/s]Extractor Predicting: 143it [01:20,  1.70it/s]Extractor Predicting: 144it [01:21,  1.75it/s]Extractor Predicting: 145it [01:21,  1.79it/s]Extractor Predicting: 146it [01:22,  1.76it/s]Extractor Predicting: 147it [01:23,  1.73it/s]Extractor Predicting: 148it [01:23,  1.74it/s]Extractor Predicting: 149it [01:24,  1.72it/s]Extractor Predicting: 150it [01:24,  1.70it/s]Extractor Predicting: 151it [01:25,  1.67it/s]Extractor Predicting: 152it [01:26,  1.68it/s]Extractor Predicting: 153it [01:26,  1.69it/s]Extractor Predicting: 154it [01:27,  1.70it/s]Extractor Predicting: 155it [01:27,  1.70it/s]Extractor Predicting: 156it [01:28,  1.64it/s]Extractor Predicting: 157it [01:29,  1.60it/s]Extractor Predicting: 158it [01:29,  1.57it/s]Extractor Predicting: 159it [01:30,  1.61it/s]Extractor Predicting: 160it [01:31,  1.63it/s]Extractor Predicting: 161it [01:31,  1.65it/s]Extractor Predicting: 162it [01:32,  1.65it/s]Extractor Predicting: 163it [01:32,  1.66it/s]Extractor Predicting: 164it [01:33,  1.70it/s]Extractor Predicting: 165it [01:33,  1.70it/s]Extractor Predicting: 166it [01:34,  1.73it/s]Extractor Predicting: 167it [01:35,  1.73it/s]Extractor Predicting: 168it [01:35,  1.72it/s]Extractor Predicting: 169it [01:36,  1.74it/s]Extractor Predicting: 170it [01:36,  1.73it/s]Extractor Predicting: 171it [01:37,  1.76it/s]Extractor Predicting: 172it [01:37,  1.79it/s]Extractor Predicting: 173it [01:38,  1.73it/s]Extractor Predicting: 174it [01:39,  1.74it/s]Extractor Predicting: 175it [01:39,  1.72it/s]Extractor Predicting: 176it [01:40,  1.74it/s]Extractor Predicting: 177it [01:40,  1.70it/s]Extractor Predicting: 178it [01:41,  1.71it/s]Extractor Predicting: 179it [01:42,  1.71it/s]Extractor Predicting: 180it [01:42,  1.70it/s]Extractor Predicting: 181it [01:43,  1.71it/s]Extractor Predicting: 182it [01:43,  1.70it/s]Extractor Predicting: 183it [01:44,  1.66it/s]Extractor Predicting: 184it [01:45,  1.68it/s]Extractor Predicting: 185it [01:45,  1.79it/s]Extractor Predicting: 185it [01:45,  1.75it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:34,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:34,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:34,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:34,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:34,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:10:35,235 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:10:35,235 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:10:35,806 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:10:36,837 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:10:36,840 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:39,679 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:39,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:39,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:39,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:10:39,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:10:40,320 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:10:40,329 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:10:40,892 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:10:41,037 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:10:41,038 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.78it/s]Extractor Predicting: 7it [00:03,  1.79it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.77it/s]Extractor Predicting: 10it [00:05,  1.75it/s]Extractor Predicting: 11it [00:06,  1.77it/s]Extractor Predicting: 12it [00:06,  1.79it/s]Extractor Predicting: 13it [00:07,  1.79it/s]Extractor Predicting: 14it [00:07,  1.79it/s]Extractor Predicting: 15it [00:08,  1.81it/s]Extractor Predicting: 16it [00:08,  1.79it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:10,  1.76it/s]Extractor Predicting: 20it [00:11,  1.72it/s]Extractor Predicting: 21it [00:11,  1.71it/s]Extractor Predicting: 22it [00:12,  1.72it/s]Extractor Predicting: 23it [00:13,  1.72it/s]Extractor Predicting: 24it [00:13,  1.75it/s]Extractor Predicting: 25it [00:14,  1.76it/s]Extractor Predicting: 26it [00:14,  1.75it/s]Extractor Predicting: 27it [00:15,  1.77it/s]Extractor Predicting: 28it [00:15,  1.77it/s]Extractor Predicting: 29it [00:16,  1.76it/s]Extractor Predicting: 30it [00:16,  1.84it/s]Extractor Predicting: 31it [00:17,  1.91it/s]Extractor Predicting: 32it [00:17,  1.91it/s]Extractor Predicting: 33it [00:18,  1.89it/s]Extractor Predicting: 34it [00:19,  1.90it/s]Extractor Predicting: 35it [00:19,  1.90it/s]Extractor Predicting: 36it [00:20,  1.88it/s]Extractor Predicting: 37it [00:20,  1.90it/s]Extractor Predicting: 38it [00:21,  1.89it/s]Extractor Predicting: 39it [00:21,  1.82it/s]Extractor Predicting: 40it [00:22,  1.83it/s]Extractor Predicting: 41it [00:22,  1.86it/s]Extractor Predicting: 42it [00:23,  1.86it/s]Extractor Predicting: 43it [00:23,  1.88it/s]Extractor Predicting: 44it [00:24,  1.89it/s]Extractor Predicting: 45it [00:24,  1.92it/s]Extractor Predicting: 46it [00:25,  1.95it/s]Extractor Predicting: 47it [00:25,  1.95it/s]Extractor Predicting: 48it [00:26,  1.95it/s]Extractor Predicting: 49it [00:26,  1.92it/s]Extractor Predicting: 50it [00:27,  1.90it/s]Extractor Predicting: 51it [00:27,  1.92it/s]Extractor Predicting: 52it [00:28,  1.91it/s]Extractor Predicting: 53it [00:29,  1.89it/s]Extractor Predicting: 54it [00:29,  1.84it/s]Extractor Predicting: 55it [00:30,  1.84it/s]Extractor Predicting: 56it [00:30,  1.83it/s]Extractor Predicting: 57it [00:31,  1.87it/s]Extractor Predicting: 58it [00:31,  1.95it/s]Extractor Predicting: 59it [00:32,  1.90it/s]Extractor Predicting: 60it [00:32,  1.87it/s]Extractor Predicting: 61it [00:33,  1.63it/s]Extractor Predicting: 62it [00:34,  1.65it/s]Extractor Predicting: 63it [00:34,  1.63it/s]Extractor Predicting: 64it [00:35,  1.62it/s]Extractor Predicting: 65it [00:36,  1.63it/s]Extractor Predicting: 66it [00:36,  1.63it/s]Extractor Predicting: 67it [00:37,  1.63it/s]Extractor Predicting: 68it [00:37,  1.64it/s]Extractor Predicting: 69it [00:38,  1.66it/s]Extractor Predicting: 70it [00:39,  1.68it/s]Extractor Predicting: 71it [00:39,  1.70it/s]Extractor Predicting: 72it [00:40,  1.73it/s]Extractor Predicting: 73it [00:40,  1.78it/s]Extractor Predicting: 74it [00:41,  1.78it/s]Extractor Predicting: 75it [00:41,  1.79it/s]Extractor Predicting: 76it [00:42,  1.80it/s]Extractor Predicting: 77it [00:42,  1.83it/s]Extractor Predicting: 78it [00:43,  1.81it/s]Extractor Predicting: 79it [00:43,  1.87it/s]Extractor Predicting: 80it [00:44,  1.92it/s]Extractor Predicting: 81it [00:44,  1.86it/s]Extractor Predicting: 82it [00:45,  1.89it/s]Extractor Predicting: 83it [00:46,  1.83it/s]Extractor Predicting: 84it [00:46,  1.82it/s]Extractor Predicting: 85it [00:47,  1.76it/s]Extractor Predicting: 86it [00:47,  1.73it/s]Extractor Predicting: 87it [00:48,  1.72it/s]Extractor Predicting: 88it [00:48,  1.76it/s]Extractor Predicting: 89it [00:49,  1.75it/s]Extractor Predicting: 90it [00:50,  1.77it/s]Extractor Predicting: 91it [00:50,  1.75it/s]Extractor Predicting: 92it [00:51,  1.74it/s]Extractor Predicting: 93it [00:51,  1.77it/s]Extractor Predicting: 94it [00:52,  1.79it/s]Extractor Predicting: 95it [00:52,  1.77it/s]Extractor Predicting: 96it [00:53,  1.77it/s]Extractor Predicting: 97it [00:54,  1.78it/s]Extractor Predicting: 98it [00:54,  1.77it/s]Extractor Predicting: 99it [00:55,  1.77it/s]Extractor Predicting: 100it [00:55,  1.76it/s]Extractor Predicting: 101it [00:56,  1.77it/s]Extractor Predicting: 102it [00:56,  1.79it/s]Extractor Predicting: 103it [00:57,  1.74it/s]Extractor Predicting: 104it [00:58,  1.79it/s]Extractor Predicting: 105it [00:58,  1.80it/s]Extractor Predicting: 106it [00:59,  1.82it/s]Extractor Predicting: 107it [00:59,  1.80it/s]Extractor Predicting: 108it [01:00,  1.83it/s]Extractor Predicting: 109it [01:00,  1.80it/s]Extractor Predicting: 110it [01:01,  1.82it/s]Extractor Predicting: 111it [01:01,  1.82it/s]Extractor Predicting: 112it [01:02,  1.80it/s]Extractor Predicting: 113it [01:03,  1.78it/s]Extractor Predicting: 114it [01:03,  1.77it/s]Extractor Predicting: 115it [01:04,  1.77it/s]Extractor Predicting: 116it [01:04,  1.78it/s]Extractor Predicting: 117it [01:05,  1.81it/s]Extractor Predicting: 118it [01:05,  1.79it/s]Extractor Predicting: 119it [01:06,  1.77it/s]Extractor Predicting: 120it [01:06,  1.81it/s]Extractor Predicting: 121it [01:07,  1.85it/s]Extractor Predicting: 122it [01:07,  1.84it/s]Extractor Predicting: 123it [01:08,  1.80it/s]Extractor Predicting: 124it [01:09,  1.77it/s]Extractor Predicting: 125it [01:09,  1.77it/s]Extractor Predicting: 126it [01:10,  1.76it/s]Extractor Predicting: 127it [01:10,  1.80it/s]Extractor Predicting: 128it [01:11,  1.74it/s]Extractor Predicting: 129it [01:11,  1.76it/s]Extractor Predicting: 130it [01:12,  1.80it/s]Extractor Predicting: 131it [01:13,  1.77it/s]Extractor Predicting: 132it [01:13,  1.77it/s]Extractor Predicting: 133it [01:14,  1.77it/s]Extractor Predicting: 134it [01:14,  1.76it/s]Extractor Predicting: 135it [01:15,  1.77it/s]Extractor Predicting: 136it [01:15,  1.82it/s]Extractor Predicting: 137it [01:16,  1.74it/s]Extractor Predicting: 138it [01:17,  1.77it/s]Extractor Predicting: 139it [01:17,  1.78it/s]Extractor Predicting: 140it [01:18,  1.79it/s]Extractor Predicting: 141it [01:18,  1.78it/s]Extractor Predicting: 142it [01:19,  1.81it/s]Extractor Predicting: 143it [01:19,  1.67it/s]Extractor Predicting: 144it [01:20,  1.74it/s]Extractor Predicting: 145it [01:21,  1.72it/s]Extractor Predicting: 146it [01:21,  1.76it/s]Extractor Predicting: 147it [01:22,  1.82it/s]Extractor Predicting: 148it [01:22,  1.80it/s]Extractor Predicting: 149it [01:23,  1.85it/s]Extractor Predicting: 150it [01:23,  1.86it/s]Extractor Predicting: 151it [01:24,  1.89it/s]Extractor Predicting: 152it [01:24,  1.83it/s]Extractor Predicting: 153it [01:25,  1.85it/s]Extractor Predicting: 154it [01:25,  1.79it/s]Extractor Predicting: 155it [01:26,  1.79it/s]Extractor Predicting: 156it [01:26,  1.85it/s]Extractor Predicting: 157it [01:27,  1.80it/s]Extractor Predicting: 158it [01:28,  1.79it/s]Extractor Predicting: 159it [01:28,  1.81it/s]Extractor Predicting: 160it [01:29,  1.81it/s]Extractor Predicting: 161it [01:29,  1.85it/s]Extractor Predicting: 162it [01:30,  1.82it/s]Extractor Predicting: 163it [01:30,  1.82it/s]Extractor Predicting: 164it [01:31,  1.80it/s]Extractor Predicting: 165it [01:32,  1.73it/s]Extractor Predicting: 166it [01:32,  1.71it/s]Extractor Predicting: 167it [01:33,  1.71it/s]Extractor Predicting: 168it [01:33,  1.73it/s]Extractor Predicting: 169it [01:34,  1.77it/s]Extractor Predicting: 170it [01:34,  1.80it/s]Extractor Predicting: 171it [01:35,  1.81it/s]Extractor Predicting: 172it [01:35,  1.85it/s]Extractor Predicting: 173it [01:36,  1.84it/s]Extractor Predicting: 174it [01:37,  1.81it/s]Extractor Predicting: 175it [01:37,  1.81it/s]Extractor Predicting: 176it [01:38,  1.82it/s]Extractor Predicting: 177it [01:38,  1.79it/s]Extractor Predicting: 178it [01:39,  1.78it/s]Extractor Predicting: 179it [01:39,  1.75it/s]Extractor Predicting: 180it [01:40,  1.79it/s]Extractor Predicting: 181it [01:40,  1.79it/s]Extractor Predicting: 182it [01:41,  1.82it/s]Extractor Predicting: 183it [01:42,  1.86it/s]Extractor Predicting: 184it [01:42,  1.83it/s]Extractor Predicting: 185it [01:43,  1.61it/s]Extractor Predicting: 186it [01:44,  1.61it/s]Extractor Predicting: 187it [01:44,  1.67it/s]Extractor Predicting: 188it [01:45,  1.71it/s]Extractor Predicting: 189it [01:45,  1.73it/s]Extractor Predicting: 190it [01:46,  1.77it/s]Extractor Predicting: 191it [01:46,  1.77it/s]Extractor Predicting: 192it [01:47,  1.83it/s]Extractor Predicting: 193it [01:47,  1.81it/s]Extractor Predicting: 194it [01:48,  1.82it/s]Extractor Predicting: 195it [01:48,  1.80it/s]Extractor Predicting: 196it [01:49,  1.80it/s]Extractor Predicting: 197it [01:50,  1.79it/s]Extractor Predicting: 198it [01:50,  1.76it/s]Extractor Predicting: 199it [01:51,  1.76it/s]Extractor Predicting: 200it [01:51,  1.77it/s]Extractor Predicting: 201it [01:52,  1.78it/s]Extractor Predicting: 202it [01:52,  1.79it/s]Extractor Predicting: 203it [01:53,  1.82it/s]Extractor Predicting: 204it [01:54,  1.79it/s]Extractor Predicting: 205it [01:54,  1.81it/s]Extractor Predicting: 206it [01:55,  1.78it/s]Extractor Predicting: 207it [01:55,  1.81it/s]Extractor Predicting: 208it [01:56,  1.78it/s]Extractor Predicting: 209it [01:56,  1.75it/s]Extractor Predicting: 210it [01:57,  1.82it/s]Extractor Predicting: 211it [01:57,  1.80it/s]Extractor Predicting: 212it [01:58,  1.81it/s]Extractor Predicting: 213it [01:59,  1.82it/s]Extractor Predicting: 214it [01:59,  1.86it/s]Extractor Predicting: 215it [02:00,  1.83it/s]Extractor Predicting: 216it [02:00,  1.79it/s]Extractor Predicting: 217it [02:01,  1.81it/s]Extractor Predicting: 218it [02:01,  1.79it/s]Extractor Predicting: 219it [02:02,  1.79it/s]Extractor Predicting: 220it [02:02,  1.80it/s]Extractor Predicting: 221it [02:03,  1.81it/s]Extractor Predicting: 222it [02:04,  1.75it/s]Extractor Predicting: 223it [02:04,  1.69it/s]Extractor Predicting: 224it [02:05,  1.72it/s]Extractor Predicting: 225it [02:05,  1.74it/s]Extractor Predicting: 226it [02:06,  1.79it/s]Extractor Predicting: 227it [02:06,  1.79it/s]Extractor Predicting: 228it [02:07,  1.82it/s]Extractor Predicting: 229it [02:07,  1.85it/s]Extractor Predicting: 230it [02:08,  1.87it/s]Extractor Predicting: 231it [02:08,  1.87it/s]Extractor Predicting: 232it [02:09,  1.86it/s]Extractor Predicting: 233it [02:10,  1.85it/s]Extractor Predicting: 234it [02:10,  1.85it/s]Extractor Predicting: 235it [02:11,  1.82it/s]Extractor Predicting: 236it [02:11,  1.82it/s]Extractor Predicting: 237it [02:12,  1.83it/s]Extractor Predicting: 238it [02:12,  1.79it/s]Extractor Predicting: 239it [02:13,  1.82it/s]Extractor Predicting: 240it [02:13,  1.81it/s]Extractor Predicting: 241it [02:14,  1.83it/s]Extractor Predicting: 242it [02:15,  1.80it/s]Extractor Predicting: 243it [02:15,  1.76it/s]Extractor Predicting: 244it [02:16,  1.77it/s]Extractor Predicting: 245it [02:16,  1.81it/s]Extractor Predicting: 246it [02:17,  1.79it/s]Extractor Predicting: 247it [02:17,  1.83it/s]Extractor Predicting: 248it [02:18,  1.76it/s]Extractor Predicting: 249it [02:19,  1.75it/s]Extractor Predicting: 250it [02:19,  1.78it/s]Extractor Predicting: 251it [02:20,  1.78it/s]Extractor Predicting: 252it [02:20,  1.76it/s]Extractor Predicting: 253it [02:21,  1.72it/s]Extractor Predicting: 254it [02:21,  1.70it/s]Extractor Predicting: 255it [02:22,  1.72it/s]Extractor Predicting: 256it [02:23,  1.73it/s]Extractor Predicting: 257it [02:23,  1.75it/s]Extractor Predicting: 258it [02:24,  1.74it/s]Extractor Predicting: 259it [02:24,  1.74it/s]Extractor Predicting: 260it [02:25,  1.72it/s]Extractor Predicting: 261it [02:25,  1.75it/s]Extractor Predicting: 262it [02:26,  1.74it/s]Extractor Predicting: 263it [02:27,  1.75it/s]Extractor Predicting: 264it [02:27,  1.76it/s]Extractor Predicting: 265it [02:28,  1.78it/s]Extractor Predicting: 266it [02:28,  1.72it/s]Extractor Predicting: 267it [02:29,  1.72it/s]Extractor Predicting: 268it [02:29,  1.71it/s]Extractor Predicting: 269it [02:30,  1.72it/s]Extractor Predicting: 270it [02:31,  1.71it/s]Extractor Predicting: 271it [02:31,  1.69it/s]Extractor Predicting: 272it [02:32,  1.70it/s]Extractor Predicting: 273it [02:32,  1.72it/s]Extractor Predicting: 274it [02:33,  1.69it/s]Extractor Predicting: 275it [02:34,  1.70it/s]Extractor Predicting: 276it [02:34,  1.71it/s]Extractor Predicting: 277it [02:35,  1.72it/s]Extractor Predicting: 278it [02:35,  1.75it/s]Extractor Predicting: 279it [02:36,  1.72it/s]Extractor Predicting: 280it [02:36,  1.74it/s]Extractor Predicting: 281it [02:37,  1.68it/s]Extractor Predicting: 282it [02:38,  1.69it/s]Extractor Predicting: 283it [02:38,  1.70it/s]Extractor Predicting: 284it [02:39,  1.73it/s]Extractor Predicting: 285it [02:39,  1.72it/s]Extractor Predicting: 286it [02:40,  1.76it/s]Extractor Predicting: 287it [02:41,  1.70it/s]Extractor Predicting: 288it [02:41,  1.74it/s]Extractor Predicting: 289it [02:42,  1.75it/s]Extractor Predicting: 290it [02:42,  1.71it/s]Extractor Predicting: 291it [02:43,  1.67it/s]Extractor Predicting: 292it [02:43,  1.72it/s]Extractor Predicting: 293it [02:44,  1.72it/s]Extractor Predicting: 294it [02:45,  1.69it/s]Extractor Predicting: 295it [02:45,  1.69it/s]Extractor Predicting: 296it [02:46,  1.73it/s]Extractor Predicting: 297it [02:46,  1.74it/s]Extractor Predicting: 298it [02:47,  1.72it/s]Extractor Predicting: 299it [02:48,  1.72it/s]Extractor Predicting: 300it [02:48,  1.70it/s]Extractor Predicting: 301it [02:49,  1.73it/s]Extractor Predicting: 302it [02:49,  1.69it/s]Extractor Predicting: 303it [02:50,  1.73it/s]Extractor Predicting: 304it [02:50,  1.75it/s]Extractor Predicting: 305it [02:51,  1.79it/s]Extractor Predicting: 306it [02:52,  1.82it/s]Extractor Predicting: 307it [02:52,  1.78it/s]Extractor Predicting: 308it [02:53,  1.80it/s]Extractor Predicting: 309it [02:53,  1.77it/s]Extractor Predicting: 310it [02:54,  1.77it/s]Extractor Predicting: 311it [02:54,  1.72it/s]Extractor Predicting: 312it [02:55,  1.74it/s]Extractor Predicting: 313it [02:56,  1.47it/s]Extractor Predicting: 314it [02:56,  1.52it/s]Extractor Predicting: 315it [02:57,  1.60it/s]Extractor Predicting: 316it [02:58,  1.67it/s]Extractor Predicting: 317it [02:58,  1.70it/s]Extractor Predicting: 318it [02:59,  1.75it/s]Extractor Predicting: 319it [02:59,  1.72it/s]Extractor Predicting: 320it [03:00,  1.69it/s]Extractor Predicting: 321it [03:00,  1.70it/s]Extractor Predicting: 322it [03:01,  1.72it/s]Extractor Predicting: 323it [03:02,  1.67it/s]Extractor Predicting: 324it [03:02,  1.68it/s]Extractor Predicting: 325it [03:03,  1.73it/s]Extractor Predicting: 326it [03:03,  1.73it/s]Extractor Predicting: 327it [03:04,  1.74it/s]Extractor Predicting: 328it [03:05,  1.70it/s]Extractor Predicting: 329it [03:05,  1.72it/s]Extractor Predicting: 330it [03:06,  1.69it/s]Extractor Predicting: 331it [03:06,  1.71it/s]Extractor Predicting: 332it [03:07,  1.70it/s]Extractor Predicting: 333it [03:08,  1.68it/s]Extractor Predicting: 334it [03:08,  1.71it/s]Extractor Predicting: 335it [03:09,  1.69it/s]Extractor Predicting: 336it [03:09,  1.60it/s]Extractor Predicting: 337it [03:10,  1.61it/s]Extractor Predicting: 338it [03:11,  1.63it/s]Extractor Predicting: 339it [03:11,  1.65it/s]Extractor Predicting: 340it [03:12,  1.66it/s]Extractor Predicting: 341it [03:12,  1.64it/s]Extractor Predicting: 342it [03:13,  1.66it/s]Extractor Predicting: 343it [03:14,  1.70it/s]Extractor Predicting: 344it [03:14,  1.70it/s]Extractor Predicting: 345it [03:15,  1.65it/s]Extractor Predicting: 346it [03:15,  1.66it/s]Extractor Predicting: 347it [03:16,  1.68it/s]Extractor Predicting: 348it [03:17,  1.70it/s]Extractor Predicting: 349it [03:17,  1.71it/s]Extractor Predicting: 350it [03:18,  1.71it/s]Extractor Predicting: 351it [03:18,  1.74it/s]Extractor Predicting: 352it [03:19,  1.68it/s]Extractor Predicting: 353it [03:20,  1.66it/s]Extractor Predicting: 354it [03:20,  1.70it/s]Extractor Predicting: 355it [03:21,  1.73it/s]Extractor Predicting: 356it [03:21,  1.73it/s]Extractor Predicting: 357it [03:22,  1.73it/s]Extractor Predicting: 358it [03:22,  1.73it/s]Extractor Predicting: 359it [03:23,  1.72it/s]Extractor Predicting: 360it [03:24,  1.68it/s]Extractor Predicting: 361it [03:24,  1.68it/s]Extractor Predicting: 362it [03:25,  1.70it/s]Extractor Predicting: 363it [03:25,  1.74it/s]Extractor Predicting: 364it [03:26,  1.75it/s]Extractor Predicting: 365it [03:26,  1.72it/s]Extractor Predicting: 366it [03:27,  1.67it/s]Extractor Predicting: 367it [03:28,  1.68it/s]Extractor Predicting: 368it [03:28,  1.66it/s]Extractor Predicting: 369it [03:29,  1.60it/s]Extractor Predicting: 370it [03:30,  1.61it/s]Extractor Predicting: 371it [03:30,  1.65it/s]Extractor Predicting: 372it [03:30,  1.97it/s]Extractor Predicting: 372it [03:30,  1.76it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:19,377 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:19,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:19,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:19,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:19,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:14:19,700 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:14:19,701 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:14:19,971 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:14:21,002 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:14:21,003 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:23,401 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:23,405 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:23,405 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:23,405 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:14:23,405 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:14:24,038 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:14:24,039 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:14:24,609 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:14:24,777 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:14:24,777 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.69it/s]Extractor Predicting: 6it [00:03,  1.77it/s]Extractor Predicting: 7it [00:04,  1.75it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.80it/s]Extractor Predicting: 11it [00:06,  1.80it/s]Extractor Predicting: 12it [00:06,  1.84it/s]Extractor Predicting: 13it [00:07,  1.84it/s]Extractor Predicting: 14it [00:07,  1.83it/s]Extractor Predicting: 15it [00:08,  1.74it/s]Extractor Predicting: 16it [00:09,  1.76it/s]Extractor Predicting: 17it [00:09,  1.75it/s]Extractor Predicting: 18it [00:10,  1.79it/s]Extractor Predicting: 19it [00:10,  1.79it/s]Extractor Predicting: 20it [00:11,  1.79it/s]Extractor Predicting: 21it [00:11,  1.73it/s]Extractor Predicting: 22it [00:12,  1.73it/s]Extractor Predicting: 23it [00:13,  1.73it/s]Extractor Predicting: 24it [00:13,  1.69it/s]Extractor Predicting: 25it [00:14,  1.72it/s]Extractor Predicting: 26it [00:14,  1.70it/s]Extractor Predicting: 27it [00:15,  1.69it/s]Extractor Predicting: 28it [00:16,  1.68it/s]Extractor Predicting: 29it [00:16,  1.72it/s]Extractor Predicting: 30it [00:17,  1.73it/s]Extractor Predicting: 31it [00:17,  1.73it/s]Extractor Predicting: 32it [00:18,  1.71it/s]Extractor Predicting: 33it [00:18,  1.74it/s]Extractor Predicting: 34it [00:19,  1.75it/s]Extractor Predicting: 35it [00:20,  1.72it/s]Extractor Predicting: 36it [00:20,  1.72it/s]Extractor Predicting: 37it [00:21,  1.70it/s]Extractor Predicting: 38it [00:22,  1.64it/s]Extractor Predicting: 39it [00:22,  1.60it/s]Extractor Predicting: 40it [00:23,  1.60it/s]Extractor Predicting: 41it [00:23,  1.61it/s]Extractor Predicting: 42it [00:24,  1.63it/s]Extractor Predicting: 43it [00:25,  1.64it/s]Extractor Predicting: 44it [00:25,  1.66it/s]Extractor Predicting: 45it [00:26,  1.62it/s]Extractor Predicting: 46it [00:26,  1.67it/s]Extractor Predicting: 47it [00:27,  1.65it/s]Extractor Predicting: 48it [00:28,  1.65it/s]Extractor Predicting: 49it [00:28,  1.65it/s]Extractor Predicting: 50it [00:29,  1.64it/s]Extractor Predicting: 51it [00:29,  1.62it/s]Extractor Predicting: 52it [00:30,  1.61it/s]Extractor Predicting: 53it [00:31,  1.63it/s]Extractor Predicting: 54it [00:31,  1.61it/s]Extractor Predicting: 55it [00:32,  1.66it/s]Extractor Predicting: 56it [00:33,  1.67it/s]Extractor Predicting: 57it [00:33,  1.66it/s]Extractor Predicting: 58it [00:34,  1.62it/s]Extractor Predicting: 59it [00:34,  1.62it/s]Extractor Predicting: 60it [00:35,  1.62it/s]Extractor Predicting: 61it [00:36,  1.58it/s]Extractor Predicting: 62it [00:36,  1.58it/s]Extractor Predicting: 63it [00:37,  1.71it/s]Extractor Predicting: 63it [00:37,  1.69it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_1', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:17<04:05, 17.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:32<03:28, 16.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:43<02:46, 13.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:58<02:37, 14.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:14<02:27, 14.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:28<02:11, 14.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:42<01:54, 14.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:55<01:37, 13.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:11<01:26, 14.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:25<01:11, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:37<00:55, 13.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:56<00:45, 15.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:11<00:30, 15.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:24<00:14, 14.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:39<00:00, 14.80s/it]Generating: 100%|██████████| 15/15 [03:39<00:00, 14.66s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 152, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 205, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 316, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 469, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 511, 'raw': 736}
{'target': 600, 'success': 533, 'raw': 768}
{'target': 600, 'success': 558, 'raw': 800}
{'target': 600, 'success': 579, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.6944444444444444, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : developer .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 482, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 626, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.7825, 'errors': {'', "('London', 'work location', '', 'The New York Times ( July 18 , 1923 October 6 , 1977 ) reported that the first and only English book on the topic appeared in London during the reigns of William Shakespeare and William Shakespeare .')"}}
['Relation : composer . Context : Later in the year ( 1143 ) , he composed The Seven Kingdoms , the first play in the epic of the Old Kingdom . Head Entity : The Seven Kingdoms , Tail Entity : Robert I .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best Rap Album at the 2004 MTV Video Music Awards and had two acts on the chart . Head Entity : music , Tail Entity : Rap Album .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 284, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 590, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7319711538461539, 'errors': {'', "('James S. Cain', 'field of work', '', 'He is also known for his work with James S. Cain , and for his work on the painting of the Seven Wonders of Oz by William S. Burroughs .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 83, 'raw': 128}
{'target': 600, 'success': 107, 'raw': 160}
{'target': 600, 'success': 130, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 194, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 254, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 298, 'raw': 448}
{'target': 600, 'success': 322, 'raw': 480}
{'target': 600, 'success': 346, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 387, 'raw': 576}
{'target': 600, 'success': 408, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 456, 'raw': 672}
{'target': 600, 'success': 478, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 518, 'raw': 768}
{'target': 600, 'success': 543, 'raw': 800}
{'target': 600, 'success': 563, 'raw': 832}
{'target': 600, 'success': 588, 'raw': 864}
{'target': 600, 'success': 606, 'raw': 896}
{'prompt': 'Relation : occupation .', 'success_rate': 0.6763392857142857, 'errors': {'', "('John Lennon', 'occupation', '', 'The band released their debut album In Search of a Way ( 2000 ) , which featured a cover from John Lennon and Steve Dizzy .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 13757
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13857, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.50s/it]Extractor Estimating: 2it [00:17,  7.54s/it]Extractor Estimating: 3it [00:18,  4.37s/it]Extractor Estimating: 4it [00:18,  2.85s/it]Extractor Estimating: 5it [00:19,  2.03s/it]Extractor Estimating: 6it [00:20,  1.53s/it]Extractor Estimating: 7it [00:20,  1.23s/it]Extractor Estimating: 8it [00:21,  1.03s/it]Extractor Estimating: 9it [00:22,  1.07s/it]Extractor Estimating: 10it [00:23,  1.08it/s]Extractor Estimating: 11it [00:23,  1.19it/s]Extractor Estimating: 12it [00:24,  1.30it/s]Extractor Estimating: 13it [00:24,  1.34it/s]Extractor Estimating: 14it [00:25,  1.43it/s]Extractor Estimating: 15it [00:26,  1.51it/s]Extractor Estimating: 16it [00:26,  1.56it/s]Extractor Estimating: 17it [00:27,  1.56it/s]Extractor Estimating: 18it [00:27,  1.59it/s]Extractor Estimating: 19it [00:28,  1.60it/s]Extractor Estimating: 20it [00:29,  1.62it/s]Extractor Estimating: 21it [00:29,  1.67it/s]Extractor Estimating: 22it [00:30,  1.71it/s]Extractor Estimating: 23it [00:30,  1.70it/s]Extractor Estimating: 24it [00:31,  1.67it/s]Extractor Estimating: 25it [00:32,  1.70it/s]Extractor Estimating: 26it [00:32,  1.76it/s]Extractor Estimating: 27it [00:33,  1.72it/s]Extractor Estimating: 28it [00:33,  1.76it/s]Extractor Estimating: 29it [00:34,  1.75it/s]Extractor Estimating: 30it [00:36,  1.08it/s]Extractor Estimating: 31it [00:36,  1.19it/s]Extractor Estimating: 32it [00:37,  1.35it/s]Extractor Estimating: 33it [00:37,  1.43it/s]Extractor Estimating: 34it [00:38,  1.50it/s]Extractor Estimating: 35it [00:39,  1.52it/s]Extractor Estimating: 36it [00:39,  1.55it/s]Extractor Estimating: 37it [00:40,  1.59it/s]Extractor Estimating: 38it [00:40,  1.56it/s]Extractor Estimating: 39it [00:41,  1.62it/s]Extractor Estimating: 40it [00:42,  1.67it/s]Extractor Estimating: 41it [00:42,  1.65it/s]Extractor Estimating: 42it [00:43,  1.70it/s]Extractor Estimating: 43it [00:43,  1.73it/s]Extractor Estimating: 44it [00:44,  1.73it/s]Extractor Estimating: 45it [00:44,  1.75it/s]Extractor Estimating: 46it [00:45,  1.74it/s]Extractor Estimating: 47it [00:46,  1.75it/s]Extractor Estimating: 48it [00:46,  1.73it/s]Extractor Estimating: 49it [00:47,  1.69it/s]Extractor Estimating: 50it [00:47,  1.76it/s]Extractor Estimating: 51it [00:48,  1.79it/s]Extractor Estimating: 52it [00:48,  1.83it/s]Extractor Estimating: 53it [00:49,  1.87it/s]Extractor Estimating: 54it [00:49,  1.91it/s]Extractor Estimating: 55it [00:50,  1.92it/s]Extractor Estimating: 56it [00:50,  2.01it/s]Extractor Estimating: 57it [00:51,  1.98it/s]Extractor Estimating: 58it [00:51,  1.99it/s]Extractor Estimating: 59it [00:52,  2.05it/s]Extractor Estimating: 60it [00:52,  1.98it/s]Extractor Estimating: 61it [00:53,  1.94it/s]Extractor Estimating: 62it [00:53,  1.99it/s]Extractor Estimating: 63it [00:54,  2.04it/s]Extractor Estimating: 64it [00:54,  1.98it/s]Extractor Estimating: 65it [00:55,  1.92it/s]Extractor Estimating: 66it [00:55,  2.00it/s]Extractor Estimating: 67it [00:56,  2.00it/s]Extractor Estimating: 68it [00:56,  1.96it/s]Extractor Estimating: 69it [00:57,  2.00it/s]Extractor Estimating: 70it [00:57,  1.98it/s]Extractor Estimating: 71it [00:58,  2.01it/s]Extractor Estimating: 72it [00:58,  1.99it/s]Extractor Estimating: 73it [00:59,  2.02it/s]Extractor Estimating: 74it [00:59,  2.02it/s]Extractor Estimating: 75it [01:00,  1.99it/s]Extractor Estimating: 76it [01:00,  1.91it/s]Extractor Estimating: 77it [01:01,  1.76it/s]Extractor Estimating: 78it [01:02,  1.74it/s]Extractor Estimating: 79it [01:02,  1.71it/s]Extractor Estimating: 80it [01:03,  1.61it/s]Extractor Estimating: 81it [01:04,  1.61it/s]Extractor Estimating: 82it [01:04,  1.56it/s]Extractor Estimating: 83it [01:05,  1.58it/s]Extractor Estimating: 84it [01:06,  1.45it/s]Extractor Estimating: 85it [01:06,  1.47it/s]Extractor Estimating: 86it [01:07,  1.51it/s]Extractor Estimating: 87it [01:08,  1.51it/s]Extractor Estimating: 88it [01:08,  1.55it/s]Extractor Estimating: 89it [01:09,  1.61it/s]Extractor Estimating: 90it [01:09,  1.64it/s]Extractor Estimating: 91it [01:10,  1.64it/s]Extractor Estimating: 92it [01:11,  1.67it/s]Extractor Estimating: 93it [01:11,  1.68it/s]Extractor Estimating: 94it [01:12,  1.69it/s]Extractor Estimating: 95it [01:12,  1.64it/s]Extractor Estimating: 96it [01:13,  1.67it/s]Extractor Estimating: 97it [01:14,  1.65it/s]Extractor Estimating: 98it [01:14,  1.65it/s]Extractor Estimating: 99it [01:15,  1.65it/s]Extractor Estimating: 100it [01:15,  1.63it/s]Extractor Estimating: 101it [01:16,  1.62it/s]Extractor Estimating: 102it [01:17,  1.63it/s]Extractor Estimating: 103it [01:17,  1.65it/s]Extractor Estimating: 104it [01:18,  1.69it/s]Extractor Estimating: 105it [01:18,  1.68it/s]Extractor Estimating: 106it [01:19,  1.67it/s]Extractor Estimating: 107it [01:20,  1.64it/s]Extractor Estimating: 108it [01:20,  1.66it/s]Extractor Estimating: 109it [01:21,  1.60it/s]Extractor Estimating: 110it [01:22,  1.61it/s]Extractor Estimating: 111it [01:22,  1.58it/s]Extractor Estimating: 112it [01:23,  1.50it/s]Extractor Estimating: 113it [01:24,  1.53it/s]Extractor Estimating: 114it [01:24,  1.53it/s]Extractor Estimating: 115it [01:25,  1.52it/s]Extractor Estimating: 116it [01:26,  1.53it/s]Extractor Estimating: 117it [01:26,  1.54it/s]Extractor Estimating: 118it [01:27,  1.60it/s]Extractor Estimating: 119it [01:27,  1.61it/s]Extractor Estimating: 120it [01:28,  1.57it/s]Extractor Estimating: 121it [01:29,  1.60it/s]Extractor Estimating: 122it [01:29,  1.58it/s]Extractor Estimating: 123it [01:30,  1.57it/s]Extractor Estimating: 124it [01:31,  1.60it/s]Extractor Estimating: 125it [01:31,  1.64it/s]Extractor Estimating: 126it [01:32,  1.69it/s]Extractor Estimating: 127it [01:32,  1.62it/s]Extractor Estimating: 128it [01:33,  1.64it/s]Extractor Estimating: 129it [01:34,  1.64it/s]Extractor Estimating: 130it [01:34,  1.66it/s]Extractor Estimating: 131it [01:35,  1.63it/s]Extractor Estimating: 132it [01:35,  1.58it/s]Extractor Estimating: 133it [01:36,  1.56it/s]Extractor Estimating: 134it [01:37,  1.58it/s]Extractor Estimating: 135it [01:37,  1.61it/s]Extractor Estimating: 136it [01:38,  1.59it/s]Extractor Estimating: 137it [01:39,  1.22it/s]Extractor Estimating: 138it [01:40,  1.30it/s]Extractor Estimating: 139it [01:41,  1.35it/s]Extractor Estimating: 140it [01:41,  1.42it/s]Extractor Estimating: 141it [01:42,  1.49it/s]Extractor Estimating: 142it [01:42,  1.54it/s]Extractor Estimating: 143it [01:43,  1.59it/s]Extractor Estimating: 144it [01:44,  1.59it/s]Extractor Estimating: 145it [01:44,  1.63it/s]Extractor Estimating: 146it [01:45,  1.59it/s]Extractor Estimating: 147it [01:45,  1.60it/s]Extractor Estimating: 148it [01:46,  1.59it/s]Extractor Estimating: 149it [01:47,  1.65it/s]Extractor Estimating: 150it [01:47,  1.63it/s]Extractor Estimating: 151it [01:48,  1.64it/s]Extractor Estimating: 152it [01:48,  1.68it/s]Extractor Estimating: 153it [01:49,  1.70it/s]Extractor Estimating: 154it [01:50,  1.73it/s]Extractor Estimating: 155it [01:50,  1.72it/s]Extractor Estimating: 156it [01:51,  1.69it/s]Extractor Estimating: 157it [01:51,  1.68it/s]Extractor Estimating: 158it [01:52,  1.67it/s]Extractor Estimating: 159it [01:53,  1.68it/s]Extractor Estimating: 160it [01:53,  1.72it/s]Extractor Estimating: 161it [01:54,  1.48it/s]Extractor Estimating: 162it [01:55,  1.52it/s]Extractor Estimating: 163it [01:55,  1.60it/s]Extractor Estimating: 164it [01:56,  1.63it/s]Extractor Estimating: 165it [01:56,  1.69it/s]Extractor Estimating: 166it [01:57,  1.72it/s]Extractor Estimating: 167it [01:57,  1.75it/s]Extractor Estimating: 168it [01:58,  1.78it/s]Extractor Estimating: 169it [01:58,  1.80it/s]Extractor Estimating: 170it [01:59,  1.71it/s]Extractor Estimating: 171it [02:00,  1.70it/s]Extractor Estimating: 172it [02:00,  1.73it/s]Extractor Estimating: 173it [02:01,  1.76it/s]Extractor Estimating: 174it [02:01,  1.70it/s]Extractor Estimating: 175it [02:02,  1.64it/s]Extractor Estimating: 176it [02:03,  1.63it/s]Extractor Estimating: 177it [02:03,  1.64it/s]Extractor Estimating: 178it [02:04,  1.66it/s]Extractor Estimating: 179it [02:05,  1.65it/s]Extractor Estimating: 180it [02:05,  1.63it/s]Extractor Estimating: 181it [02:06,  1.64it/s]Extractor Estimating: 182it [02:06,  1.65it/s]Extractor Estimating: 183it [02:07,  1.65it/s]Extractor Estimating: 184it [02:08,  1.67it/s]Extractor Estimating: 185it [02:08,  1.64it/s]Extractor Estimating: 186it [02:09,  1.63it/s]Extractor Estimating: 187it [02:09,  1.68it/s]Extractor Estimating: 188it [02:10,  1.67it/s]Extractor Estimating: 189it [02:11,  1.65it/s]Extractor Estimating: 190it [02:11,  1.62it/s]Extractor Estimating: 191it [02:12,  1.59it/s]Extractor Estimating: 192it [02:13,  1.59it/s]Extractor Estimating: 193it [02:13,  1.57it/s]Extractor Estimating: 194it [02:14,  1.60it/s]Extractor Estimating: 195it [02:14,  1.62it/s]Extractor Estimating: 196it [02:15,  1.58it/s]Extractor Estimating: 197it [02:16,  1.60it/s]Extractor Estimating: 198it [02:16,  1.59it/s]Extractor Estimating: 199it [02:17,  1.63it/s]Extractor Estimating: 200it [02:18,  1.61it/s]Extractor Estimating: 201it [02:18,  1.60it/s]Extractor Estimating: 202it [02:19,  1.64it/s]Extractor Estimating: 203it [02:19,  1.61it/s]Extractor Estimating: 204it [02:20,  1.63it/s]Extractor Estimating: 205it [02:21,  1.56it/s]Extractor Estimating: 206it [02:21,  1.62it/s]Extractor Estimating: 207it [02:22,  1.64it/s]Extractor Estimating: 208it [02:22,  1.65it/s]Extractor Estimating: 209it [02:23,  1.65it/s]Extractor Estimating: 210it [02:24,  1.64it/s]Extractor Estimating: 211it [02:24,  1.66it/s]Extractor Estimating: 212it [02:25,  1.67it/s]Extractor Estimating: 213it [02:25,  1.68it/s]Extractor Estimating: 214it [02:26,  1.66it/s]Extractor Estimating: 215it [02:27,  1.63it/s]Extractor Estimating: 216it [02:27,  1.60it/s]Extractor Estimating: 217it [02:28,  1.62it/s]Extractor Estimating: 218it [02:29,  1.62it/s]Extractor Estimating: 219it [02:29,  1.68it/s]Extractor Estimating: 220it [02:30,  1.68it/s]Extractor Estimating: 221it [02:30,  1.66it/s]Extractor Estimating: 222it [02:31,  1.66it/s]Extractor Estimating: 223it [02:31,  1.68it/s]Extractor Estimating: 224it [02:32,  1.65it/s]Extractor Estimating: 225it [02:33,  1.64it/s]Extractor Estimating: 226it [02:33,  1.61it/s]Extractor Estimating: 227it [02:34,  1.56it/s]Extractor Estimating: 228it [02:35,  1.55it/s]Extractor Estimating: 229it [02:35,  1.57it/s]Extractor Estimating: 230it [02:36,  1.58it/s]Extractor Estimating: 231it [02:37,  1.60it/s]Extractor Estimating: 232it [02:37,  1.63it/s]Extractor Estimating: 233it [02:38,  1.65it/s]Extractor Estimating: 234it [02:38,  1.61it/s]Extractor Estimating: 235it [02:39,  1.56it/s]Extractor Estimating: 236it [02:40,  1.56it/s]Extractor Estimating: 237it [02:40,  1.57it/s]Extractor Estimating: 238it [02:41,  1.60it/s]Extractor Estimating: 239it [02:42,  1.63it/s]Extractor Estimating: 240it [02:42,  1.62it/s]Extractor Estimating: 241it [02:43,  1.59it/s]Extractor Estimating: 242it [02:44,  1.55it/s]Extractor Estimating: 243it [02:44,  1.55it/s]Extractor Estimating: 244it [02:45,  1.45it/s]Extractor Estimating: 245it [02:46,  1.48it/s]Extractor Estimating: 246it [02:46,  1.49it/s]Extractor Estimating: 247it [02:47,  1.51it/s]Extractor Estimating: 248it [02:48,  1.55it/s]Extractor Estimating: 249it [02:48,  1.55it/s]Extractor Estimating: 250it [02:49,  1.52it/s]Extractor Estimating: 251it [02:49,  1.56it/s]Extractor Estimating: 252it [02:50,  1.53it/s]Extractor Estimating: 253it [02:51,  1.54it/s]Extractor Estimating: 254it [02:51,  1.54it/s]Extractor Estimating: 255it [02:52,  1.58it/s]Extractor Estimating: 256it [02:53,  1.59it/s]Extractor Estimating: 257it [02:53,  1.63it/s]Extractor Estimating: 258it [02:54,  1.68it/s]Extractor Estimating: 259it [02:54,  1.65it/s]Extractor Estimating: 260it [02:55,  1.69it/s]Extractor Estimating: 261it [02:56,  1.65it/s]Extractor Estimating: 262it [02:56,  1.63it/s]Extractor Estimating: 263it [02:57,  1.65it/s]Extractor Estimating: 264it [02:57,  1.68it/s]Extractor Estimating: 265it [02:58,  1.68it/s]Extractor Estimating: 266it [02:59,  1.70it/s]Extractor Estimating: 267it [02:59,  1.68it/s]Extractor Estimating: 268it [03:00,  1.62it/s]Extractor Estimating: 269it [03:00,  1.65it/s]Extractor Estimating: 270it [03:01,  1.64it/s]Extractor Estimating: 271it [03:02,  1.61it/s]Extractor Estimating: 272it [03:02,  1.57it/s]Extractor Estimating: 273it [03:03,  1.62it/s]Extractor Estimating: 274it [03:04,  1.63it/s]Extractor Estimating: 275it [03:04,  1.64it/s]Extractor Estimating: 276it [03:05,  1.60it/s]Extractor Estimating: 277it [03:05,  1.62it/s]Extractor Estimating: 278it [03:06,  1.65it/s]Extractor Estimating: 279it [03:07,  1.63it/s]Extractor Estimating: 280it [03:07,  1.64it/s]Extractor Estimating: 281it [03:08,  1.64it/s]Extractor Estimating: 282it [03:08,  1.67it/s]Extractor Estimating: 283it [03:09,  1.60it/s]Extractor Estimating: 284it [03:10,  1.60it/s]Extractor Estimating: 285it [03:10,  1.59it/s]Extractor Estimating: 286it [03:11,  1.59it/s]Extractor Estimating: 287it [03:12,  1.57it/s]Extractor Estimating: 288it [03:12,  1.61it/s]Extractor Estimating: 289it [03:13,  1.60it/s]Extractor Estimating: 290it [03:13,  1.64it/s]Extractor Estimating: 291it [03:14,  1.64it/s]Extractor Estimating: 292it [03:15,  1.60it/s]Extractor Estimating: 293it [03:15,  1.59it/s]Extractor Estimating: 294it [03:16,  1.62it/s]Extractor Estimating: 295it [03:16,  1.66it/s]Extractor Estimating: 296it [03:17,  1.65it/s]Extractor Estimating: 297it [03:18,  1.68it/s]Extractor Estimating: 298it [03:18,  1.73it/s]Extractor Estimating: 299it [03:19,  1.66it/s]Extractor Estimating: 300it [03:19,  1.65it/s]Extractor Estimating: 301it [03:20,  1.63it/s]Extractor Estimating: 302it [03:21,  1.58it/s]Extractor Estimating: 303it [03:21,  1.58it/s]Extractor Estimating: 304it [03:22,  1.56it/s]Extractor Estimating: 305it [03:23,  1.56it/s]Extractor Estimating: 306it [03:23,  1.54it/s]Extractor Estimating: 307it [03:24,  1.59it/s]Extractor Estimating: 308it [03:25,  1.58it/s]Extractor Estimating: 309it [03:25,  1.58it/s]Extractor Estimating: 310it [03:26,  1.58it/s]Extractor Estimating: 311it [03:27,  1.56it/s]Extractor Estimating: 312it [03:27,  1.56it/s]Extractor Estimating: 313it [03:28,  1.58it/s]Extractor Estimating: 314it [03:28,  1.59it/s]Extractor Estimating: 315it [03:29,  1.59it/s]Extractor Estimating: 316it [03:30,  1.58it/s]Extractor Estimating: 317it [03:30,  1.60it/s]Extractor Estimating: 318it [03:31,  1.55it/s]Extractor Estimating: 319it [03:32,  1.56it/s]Extractor Estimating: 320it [03:32,  1.55it/s]Extractor Estimating: 321it [03:33,  1.54it/s]Extractor Estimating: 322it [03:34,  1.56it/s]Extractor Estimating: 323it [03:34,  1.57it/s]Extractor Estimating: 324it [03:35,  1.60it/s]Extractor Estimating: 325it [03:35,  1.61it/s]Extractor Estimating: 326it [03:36,  1.47it/s]Extractor Estimating: 327it [03:37,  1.53it/s]Extractor Estimating: 328it [03:37,  1.56it/s]Extractor Estimating: 329it [03:38,  1.61it/s]Extractor Estimating: 330it [03:39,  1.58it/s]Extractor Estimating: 331it [03:39,  1.60it/s]Extractor Estimating: 332it [03:40,  1.64it/s]Extractor Estimating: 333it [03:40,  1.65it/s]Extractor Estimating: 334it [03:41,  1.63it/s]Extractor Estimating: 335it [03:42,  1.61it/s]Extractor Estimating: 336it [03:42,  1.64it/s]Extractor Estimating: 337it [03:43,  1.55it/s]Extractor Estimating: 338it [03:44,  1.60it/s]Extractor Estimating: 339it [03:44,  1.55it/s]Extractor Estimating: 340it [03:45,  1.61it/s]Extractor Estimating: 341it [03:45,  1.61it/s]Extractor Estimating: 342it [03:46,  1.60it/s]Extractor Estimating: 343it [03:47,  1.64it/s]Extractor Estimating: 344it [03:47,  1.61it/s]Extractor Estimating: 345it [03:48,  1.58it/s]Extractor Estimating: 346it [03:49,  1.58it/s]Extractor Estimating: 347it [03:49,  1.61it/s]Extractor Estimating: 348it [03:50,  1.64it/s]Extractor Estimating: 349it [03:50,  1.66it/s]Extractor Estimating: 350it [03:51,  1.63it/s]Extractor Estimating: 351it [03:52,  1.66it/s]Extractor Estimating: 352it [03:52,  1.64it/s]Extractor Estimating: 353it [03:53,  1.66it/s]Extractor Estimating: 354it [03:53,  1.66it/s]Extractor Estimating: 355it [03:54,  1.59it/s]Extractor Estimating: 356it [03:55,  1.56it/s]Extractor Estimating: 357it [03:55,  1.56it/s]Extractor Estimating: 358it [03:56,  1.61it/s]Extractor Estimating: 359it [03:57,  1.61it/s]Extractor Estimating: 360it [03:57,  1.64it/s]Extractor Estimating: 361it [03:58,  1.66it/s]Extractor Estimating: 362it [03:58,  1.61it/s]Extractor Estimating: 363it [03:59,  1.56it/s]Extractor Estimating: 364it [04:00,  1.54it/s]Extractor Estimating: 365it [04:00,  1.57it/s]Extractor Estimating: 366it [04:01,  1.65it/s]Extractor Estimating: 367it [04:02,  1.63it/s]Extractor Estimating: 368it [04:02,  1.61it/s]Extractor Estimating: 369it [04:03,  1.63it/s]Extractor Estimating: 370it [04:03,  1.64it/s]Extractor Estimating: 371it [04:04,  1.61it/s]Extractor Estimating: 372it [04:05,  1.65it/s]Extractor Estimating: 373it [04:05,  1.65it/s]Extractor Estimating: 374it [04:06,  1.62it/s]Extractor Estimating: 375it [04:06,  1.62it/s]Extractor Estimating: 375it [04:06,  1.52it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7416 mean pseudo reward: 0.9195481172946914
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 27238
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27338, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_filtered_large/unseen_10_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27338, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.227, loss:1269.3860
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.955, loss:1201.3013
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.956, loss:1159.6865
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 91, avg_time 0.966, loss:1129.8248
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 191, avg_time 0.952, loss:1127.0611
>> valid entity prec:0.5517, rec:0.5510, f1:0.5514
>> valid relation prec:0.1912, rec:0.0027, f1:0.0053
>> valid relation with NER prec:0.1912, rec:0.0027, f1:0.0053
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 291, avg_time 2.571, loss:1113.1919
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 82, avg_time 0.957, loss:1087.4606
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 182, avg_time 0.956, loss:1093.2998
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 282, avg_time 0.958, loss:1106.1920
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 73, avg_time 0.966, loss:1028.6736
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5627, rec:0.3462, f1:0.4286
>> valid relation prec:0.2857, rec:0.0074, f1:0.0144
>> valid relation with NER prec:0.2857, rec:0.0074, f1:0.0144
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 173, avg_time 2.531, loss:1064.4763
g_step 1200, step 273, avg_time 0.958, loss:1054.6566
g_step 1300, step 64, avg_time 0.961, loss:996.9935
g_step 1400, step 164, avg_time 0.953, loss:1008.3483
g_step 1500, step 264, avg_time 0.956, loss:977.8710
>> valid entity prec:0.6584, rec:0.3605, f1:0.4659
>> valid relation prec:0.1769, rec:0.0053, f1:0.0103
>> valid relation with NER prec:0.1769, rec:0.0053, f1:0.0103
g_step 1600, step 55, avg_time 2.518, loss:958.7528
g_step 1700, step 155, avg_time 0.951, loss:975.3453
g_step 1800, step 255, avg_time 0.969, loss:967.9306
g_step 1900, step 46, avg_time 0.972, loss:926.8288
g_step 2000, step 146, avg_time 0.953, loss:899.6521
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5368, rec:0.5214, f1:0.5290
>> valid relation prec:0.0611, rec:0.0023, f1:0.0043
>> valid relation with NER prec:0.0611, rec:0.0023, f1:0.0043
g_step 2100, step 246, avg_time 2.554, loss:969.9004
g_step 2200, step 37, avg_time 0.964, loss:888.8066
g_step 2300, step 137, avg_time 0.960, loss:878.5655
g_step 2400, step 237, avg_time 0.956, loss:893.5713
g_step 2500, step 28, avg_time 0.958, loss:889.7052
>> valid entity prec:0.5327, rec:0.5050, f1:0.5185
>> valid relation prec:0.1741, rec:0.0143, f1:0.0265
>> valid relation with NER prec:0.1741, rec:0.0143, f1:0.0265
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 128, avg_time 2.545, loss:819.3125
g_step 2700, step 228, avg_time 0.966, loss:877.3545
g_step 2800, step 19, avg_time 0.952, loss:866.6925
g_step 2900, step 119, avg_time 0.963, loss:832.1785
g_step 3000, step 219, avg_time 0.952, loss:831.1846
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4930, rec:0.6110, f1:0.5457
>> valid relation prec:0.1010, rec:0.0160, f1:0.0276
>> valid relation with NER prec:0.1010, rec:0.0160, f1:0.0276
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 10, avg_time 2.558, loss:835.4261
g_step 3200, step 110, avg_time 0.953, loss:788.3671
g_step 3300, step 210, avg_time 0.960, loss:820.2363
g_step 3400, step 1, avg_time 0.964, loss:810.3278
g_step 3500, step 101, avg_time 0.961, loss:773.0883
>> valid entity prec:0.5611, rec:0.3846, f1:0.4564
>> valid relation prec:0.1202, rec:0.0084, f1:0.0157
>> valid relation with NER prec:0.1202, rec:0.0084, f1:0.0157
g_step 3600, step 201, avg_time 2.538, loss:775.6227
g_step 3700, step 301, avg_time 0.969, loss:792.4794
g_step 3800, step 92, avg_time 0.961, loss:748.7003
g_step 3900, step 192, avg_time 0.961, loss:753.0724
g_step 4000, step 292, avg_time 0.971, loss:781.2868
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5422, rec:0.4592, f1:0.4972
>> valid relation prec:0.1268, rec:0.0143, f1:0.0258
>> valid relation with NER prec:0.1268, rec:0.0143, f1:0.0258
g_step 4100, step 83, avg_time 2.535, loss:705.3737
g_step 4200, step 183, avg_time 0.965, loss:738.4148
g_step 4300, step 283, avg_time 0.972, loss:762.3152
g_step 4400, step 74, avg_time 0.960, loss:707.8538
g_step 4500, step 174, avg_time 0.963, loss:711.1786
>> valid entity prec:0.5265, rec:0.5106, f1:0.5184
>> valid relation prec:0.0636, rec:0.0115, f1:0.0194
>> valid relation with NER prec:0.0636, rec:0.0115, f1:0.0194
g_step 4600, step 274, avg_time 2.548, loss:723.0791
g_step 4700, step 65, avg_time 0.954, loss:687.2046
g_step 4800, step 165, avg_time 0.958, loss:691.6691
g_step 4900, step 265, avg_time 0.954, loss:708.9081
g_step 5000, step 56, avg_time 0.952, loss:675.8639
learning rate was adjusted to 0.0008
>> valid entity prec:0.5277, rec:0.5381, f1:0.5328
>> valid relation prec:0.0679, rec:0.0131, f1:0.0220
>> valid relation with NER prec:0.0679, rec:0.0131, f1:0.0220
g_step 5100, step 156, avg_time 2.562, loss:678.6630
g_step 5200, step 256, avg_time 0.957, loss:670.3756
g_step 5300, step 47, avg_time 0.962, loss:668.7083
g_step 5400, step 147, avg_time 0.951, loss:652.3041
g_step 5500, step 247, avg_time 0.958, loss:638.8305
>> valid entity prec:0.4797, rec:0.4314, f1:0.4543
>> valid relation prec:0.0679, rec:0.0096, f1:0.0169
>> valid relation with NER prec:0.0679, rec:0.0096, f1:0.0169
g_step 5600, step 38, avg_time 2.547, loss:648.0632
g_step 5700, step 138, avg_time 0.954, loss:619.1994
g_step 5800, step 238, avg_time 0.967, loss:645.8261
g_step 5900, step 29, avg_time 0.954, loss:642.6089
g_step 6000, step 129, avg_time 0.970, loss:599.1154
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5328, rec:0.5113, f1:0.5218
>> valid relation prec:0.0541, rec:0.0133, f1:0.0214
>> valid relation with NER prec:0.0541, rec:0.0133, f1:0.0214
g_step 6100, step 229, avg_time 2.535, loss:611.1603
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:35:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:35:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-35-12_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:35:13 - WARNING - datasets.builder -   Using custom data configuration default-f667322ded20a3dc
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f667322ded20a3dc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:35:13,404 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:35:13,405 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:35:13,405 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:35:13,406 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:35:13,412 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:13,416 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:13,416 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:13,417 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:13,417 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:13,417 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:35:13,417 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:35:13,524 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:35:16,597 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:35:16,608 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f667322ded20a3dc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 14:35:16 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x152e1c4d90e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.19ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.94ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.21ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.37ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.45ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.51ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.55ba/s]100%|██████████| 8/8 [00:01<00:00,  5.27ba/s]100%|██████████| 8/8 [00:01<00:00,  4.61ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.20ba/s] 40%|████      | 2/5 [00:00<00:00,  4.38ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.52ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.87ba/s]100%|██████████| 5/5 [00:01<00:00,  4.24ba/s]100%|██████████| 5/5 [00:01<00:00,  4.07ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.16ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.49ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.78ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.79ba/s]100%|██████████| 8/8 [00:00<00:00, 11.32ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.19ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.50ba/s]100%|██████████| 5/5 [00:00<00:00, 11.03ba/s]100%|██████████| 5/5 [00:00<00:00, 10.81ba/s]
[INFO|trainer.py:414] 2023-08-28 14:35:21,235 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:35:21,242 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:35:21,242 >>   Num examples = 7556
[INFO|trainer.py:1149] 2023-08-28 14:35:21,242 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:35:21,242 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:35:21,243 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:35:21,243 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:35:21,243 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:55,  3.35it/s]  0%|          | 2/590 [00:00<02:52,  3.40it/s]  1%|          | 3/590 [00:00<02:51,  3.42it/s]  1%|          | 4/590 [00:01<02:50,  3.43it/s]  1%|          | 5/590 [00:01<02:50,  3.44it/s]  1%|          | 6/590 [00:01<02:50,  3.42it/s]  1%|          | 7/590 [00:02<02:51,  3.41it/s]  1%|▏         | 8/590 [00:02<02:50,  3.41it/s]  2%|▏         | 9/590 [00:02<02:50,  3.40it/s]  2%|▏         | 10/590 [00:02<02:50,  3.40it/s]  2%|▏         | 11/590 [00:03<02:50,  3.40it/s]  2%|▏         | 12/590 [00:03<02:50,  3.39it/s]  2%|▏         | 13/590 [00:03<02:50,  3.39it/s]  2%|▏         | 14/590 [00:04<02:49,  3.39it/s]  3%|▎         | 15/590 [00:04<02:49,  3.39it/s]  3%|▎         | 16/590 [00:04<02:49,  3.39it/s]  3%|▎         | 17/590 [00:05<02:49,  3.38it/s]  3%|▎         | 18/590 [00:05<02:49,  3.38it/s]  3%|▎         | 19/590 [00:05<02:48,  3.38it/s]  3%|▎         | 20/590 [00:05<02:48,  3.39it/s]  4%|▎         | 21/590 [00:06<02:47,  3.39it/s]  4%|▎         | 22/590 [00:06<02:47,  3.39it/s]  4%|▍         | 23/590 [00:06<02:47,  3.39it/s]  4%|▍         | 24/590 [00:07<02:47,  3.39it/s]  4%|▍         | 25/590 [00:07<02:46,  3.39it/s]  4%|▍         | 26/590 [00:07<02:46,  3.39it/s]  5%|▍         | 27/590 [00:07<02:46,  3.39it/s]  5%|▍         | 28/590 [00:08<02:46,  3.38it/s]  5%|▍         | 29/590 [00:08<02:45,  3.38it/s]  5%|▌         | 30/590 [00:08<02:45,  3.39it/s]  5%|▌         | 31/590 [00:09<02:45,  3.39it/s]  5%|▌         | 32/590 [00:09<02:44,  3.39it/s]  6%|▌         | 33/590 [00:09<02:44,  3.39it/s]  6%|▌         | 34/590 [00:10<02:43,  3.39it/s]  6%|▌         | 35/590 [00:10<02:43,  3.39it/s]  6%|▌         | 36/590 [00:10<02:43,  3.39it/s]  6%|▋         | 37/590 [00:10<02:43,  3.39it/s]  6%|▋         | 38/590 [00:11<02:42,  3.39it/s]  7%|▋         | 39/590 [00:11<02:43,  3.38it/s]  7%|▋         | 40/590 [00:11<02:42,  3.38it/s]  7%|▋         | 41/590 [00:12<02:42,  3.38it/s]  7%|▋         | 42/590 [00:12<02:41,  3.39it/s]  7%|▋         | 43/590 [00:12<02:41,  3.39it/s]  7%|▋         | 44/590 [00:12<02:41,  3.39it/s]  8%|▊         | 45/590 [00:13<02:40,  3.39it/s]  8%|▊         | 46/590 [00:13<02:40,  3.39it/s]  8%|▊         | 47/590 [00:13<02:40,  3.39it/s]  8%|▊         | 48/590 [00:14<02:39,  3.39it/s]  8%|▊         | 49/590 [00:14<02:39,  3.39it/s]  8%|▊         | 50/590 [00:14<02:39,  3.38it/s]  9%|▊         | 51/590 [00:15<02:39,  3.38it/s]  9%|▉         | 52/590 [00:15<02:38,  3.38it/s]  9%|▉         | 53/590 [00:15<02:38,  3.38it/s]  9%|▉         | 54/590 [00:15<02:38,  3.38it/s]  9%|▉         | 55/590 [00:16<02:37,  3.39it/s]  9%|▉         | 56/590 [00:16<02:37,  3.39it/s] 10%|▉         | 57/590 [00:16<02:37,  3.39it/s] 10%|▉         | 58/590 [00:17<02:36,  3.39it/s] 10%|█         | 59/590 [00:17<02:36,  3.39it/s] 10%|█         | 60/590 [00:17<02:36,  3.39it/s] 10%|█         | 61/590 [00:17<02:36,  3.38it/s] 11%|█         | 62/590 [00:18<02:36,  3.38it/s] 11%|█         | 63/590 [00:18<02:35,  3.38it/s] 11%|█         | 64/590 [00:18<02:35,  3.39it/s] 11%|█         | 65/590 [00:19<02:35,  3.38it/s] 11%|█         | 66/590 [00:19<02:34,  3.38it/s] 11%|█▏        | 67/590 [00:19<02:34,  3.39it/s] 12%|█▏        | 68/590 [00:20<02:34,  3.39it/s] 12%|█▏        | 69/590 [00:20<02:33,  3.39it/s] 12%|█▏        | 70/590 [00:20<02:33,  3.39it/s] 12%|█▏        | 71/590 [00:20<02:33,  3.39it/s] 12%|█▏        | 72/590 [00:21<02:33,  3.38it/s] 12%|█▏        | 73/590 [00:21<02:32,  3.38it/s] 13%|█▎        | 74/590 [00:21<02:32,  3.38it/s] 13%|█▎        | 75/590 [00:22<02:32,  3.38it/s] 13%|█▎        | 76/590 [00:22<02:31,  3.38it/s] 13%|█▎        | 77/590 [00:22<02:31,  3.39it/s] 13%|█▎        | 78/590 [00:23<02:31,  3.39it/s] 13%|█▎        | 79/590 [00:23<02:30,  3.38it/s] 14%|█▎        | 80/590 [00:23<02:30,  3.39it/s] 14%|█▎        | 81/590 [00:23<02:30,  3.39it/s] 14%|█▍        | 82/590 [00:24<02:29,  3.39it/s] 14%|█▍        | 83/590 [00:24<02:30,  3.36it/s] 14%|█▍        | 84/590 [00:24<02:30,  3.37it/s] 14%|█▍        | 85/590 [00:25<02:29,  3.37it/s] 15%|█▍        | 86/590 [00:25<02:29,  3.38it/s] 15%|█▍        | 87/590 [00:25<02:28,  3.38it/s] 15%|█▍        | 88/590 [00:25<02:28,  3.38it/s] 15%|█▌        | 89/590 [00:26<02:28,  3.38it/s] 15%|█▌        | 90/590 [00:26<02:27,  3.38it/s] 15%|█▌        | 91/590 [00:26<02:27,  3.39it/s] 16%|█▌        | 92/590 [00:27<02:27,  3.39it/s] 16%|█▌        | 93/590 [00:27<02:26,  3.38it/s] 16%|█▌        | 94/590 [00:27<02:26,  3.38it/s] 16%|█▌        | 95/590 [00:28<02:27,  3.36it/s] 16%|█▋        | 96/590 [00:28<02:26,  3.37it/s] 16%|█▋        | 97/590 [00:28<02:26,  3.38it/s] 17%|█▋        | 98/590 [00:28<02:25,  3.38it/s] 17%|█▋        | 99/590 [00:29<02:25,  3.38it/s] 17%|█▋        | 100/590 [00:29<02:24,  3.39it/s] 17%|█▋        | 101/590 [00:29<02:23,  3.41it/s] 17%|█▋        | 102/590 [00:30<02:22,  3.41it/s] 17%|█▋        | 103/590 [00:30<02:22,  3.42it/s] 18%|█▊        | 104/590 [00:30<02:21,  3.42it/s] 18%|█▊        | 105/590 [00:30<02:21,  3.43it/s] 18%|█▊        | 106/590 [00:31<02:21,  3.42it/s] 18%|█▊        | 107/590 [00:31<02:20,  3.43it/s] 18%|█▊        | 108/590 [00:31<02:20,  3.43it/s] 18%|█▊        | 109/590 [00:32<02:20,  3.43it/s] 19%|█▊        | 110/590 [00:32<02:19,  3.43it/s] 19%|█▉        | 111/590 [00:32<02:19,  3.43it/s] 19%|█▉        | 112/590 [00:33<02:19,  3.43it/s] 19%|█▉        | 113/590 [00:33<02:18,  3.43it/s] 19%|█▉        | 114/590 [00:33<02:18,  3.43it/s] 19%|█▉        | 115/590 [00:33<02:18,  3.44it/s] 20%|█▉        | 116/590 [00:34<02:17,  3.44it/s] 20%|█▉        | 117/590 [00:34<02:18,  3.40it/s] 20%|██        | 118/590 [00:34<02:18,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 14:35:56,055 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:35:56,055 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 14:35:56,055 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.38it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.41it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.29it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.40it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.85it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.50it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.35it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.12it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.15it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.17it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.37it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.33it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.20it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.13it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.93it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.83it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.69it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.77it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.09it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.25it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.19it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 41.89it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 42.62it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.00it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.29it/s][A
 22%|██▏       | 132/611 [00:02<00:11, 43.38it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.65it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 43.92it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.11it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.08it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.05it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.22it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.21it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.06it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 43.99it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.90it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.01it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 43.99it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.01it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.95it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.94it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.76it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.84it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.92it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.01it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.01it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.05it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.13it/s][A
 40%|████      | 247/611 [00:05<00:08, 43.33it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.60it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.70it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.80it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.96it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.97it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.06it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.07it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 43.90it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.11it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.13it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 42.21it/s][A
 50%|█████     | 307/611 [00:06<00:07, 42.93it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.34it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 43.39it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 43.58it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 43.73it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 43.85it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.06it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.85it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.84it/s][A
 58%|█████▊    | 352/611 [00:08<00:05, 44.04it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.18it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.27it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.21it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.14it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.12it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.04it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.98it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.96it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 44.15it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.31it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.31it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.22it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.17it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.06it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.97it/s][A
 71%|███████   | 432/611 [00:09<00:04, 43.92it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.02it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.18it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.28it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.40it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.29it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.08it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.08it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.03it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.92it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.94it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.24it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.35it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.40it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.33it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.15it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.13it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.11it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.05it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.99it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.17it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.31it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.18it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.26it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.11it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.95it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.06it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.03it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.07it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.18it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.19it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.23it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.15it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.16it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.11it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.10it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:48<02:18,  3.41it/s]
100%|██████████| 611/611 [00:13<00:00, 44.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:36:09,975 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-28 14:36:10,002 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:36:12,243 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:36:12,272 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:36:12,285 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [00:56<51:41,  6.59s/it] 20%|██        | 120/590 [00:56<36:48,  4.70s/it] 21%|██        | 121/590 [00:56<26:23,  3.38s/it] 21%|██        | 122/590 [00:56<19:07,  2.45s/it] 21%|██        | 123/590 [00:57<14:03,  1.81s/it] 21%|██        | 124/590 [00:57<10:29,  1.35s/it] 21%|██        | 125/590 [00:57<08:00,  1.03s/it] 21%|██▏       | 126/590 [00:58<06:16,  1.23it/s] 22%|██▏       | 127/590 [00:58<05:03,  1.53it/s] 22%|██▏       | 128/590 [00:58<04:12,  1.83it/s] 22%|██▏       | 129/590 [00:58<03:36,  2.13it/s] 22%|██▏       | 130/590 [00:59<03:11,  2.40it/s] 22%|██▏       | 131/590 [00:59<02:53,  2.64it/s] 22%|██▏       | 132/590 [00:59<02:41,  2.84it/s] 23%|██▎       | 133/590 [01:00<02:32,  2.99it/s] 23%|██▎       | 134/590 [01:00<02:26,  3.11it/s] 23%|██▎       | 135/590 [01:00<02:22,  3.20it/s] 23%|██▎       | 136/590 [01:01<02:18,  3.27it/s] 23%|██▎       | 137/590 [01:01<02:16,  3.32it/s] 23%|██▎       | 138/590 [01:01<02:14,  3.35it/s] 24%|██▎       | 139/590 [01:01<02:13,  3.37it/s] 24%|██▎       | 140/590 [01:02<02:12,  3.39it/s] 24%|██▍       | 141/590 [01:02<02:12,  3.40it/s] 24%|██▍       | 142/590 [01:02<02:11,  3.41it/s] 24%|██▍       | 143/590 [01:03<02:10,  3.41it/s] 24%|██▍       | 144/590 [01:03<02:10,  3.42it/s] 25%|██▍       | 145/590 [01:03<02:10,  3.42it/s] 25%|██▍       | 146/590 [01:03<02:09,  3.43it/s] 25%|██▍       | 147/590 [01:04<02:09,  3.43it/s] 25%|██▌       | 148/590 [01:04<02:08,  3.43it/s] 25%|██▌       | 149/590 [01:04<02:08,  3.43it/s] 25%|██▌       | 150/590 [01:05<02:08,  3.42it/s] 26%|██▌       | 151/590 [01:05<02:08,  3.42it/s] 26%|██▌       | 152/590 [01:05<02:07,  3.42it/s] 26%|██▌       | 153/590 [01:05<02:07,  3.42it/s] 26%|██▌       | 154/590 [01:06<02:07,  3.43it/s] 26%|██▋       | 155/590 [01:06<02:06,  3.43it/s] 26%|██▋       | 156/590 [01:06<02:06,  3.43it/s] 27%|██▋       | 157/590 [01:07<02:06,  3.43it/s] 27%|██▋       | 158/590 [01:07<02:05,  3.43it/s] 27%|██▋       | 159/590 [01:07<02:05,  3.43it/s] 27%|██▋       | 160/590 [01:08<02:05,  3.43it/s] 27%|██▋       | 161/590 [01:08<02:05,  3.42it/s] 27%|██▋       | 162/590 [01:08<02:04,  3.42it/s] 28%|██▊       | 163/590 [01:08<02:04,  3.43it/s] 28%|██▊       | 164/590 [01:09<02:04,  3.43it/s] 28%|██▊       | 165/590 [01:09<02:03,  3.43it/s] 28%|██▊       | 166/590 [01:09<02:03,  3.43it/s] 28%|██▊       | 167/590 [01:10<02:03,  3.43it/s] 28%|██▊       | 168/590 [01:10<02:03,  3.43it/s] 29%|██▊       | 169/590 [01:10<02:02,  3.43it/s] 29%|██▉       | 170/590 [01:10<02:02,  3.43it/s] 29%|██▉       | 171/590 [01:11<02:02,  3.43it/s] 29%|██▉       | 172/590 [01:11<02:02,  3.42it/s] 29%|██▉       | 173/590 [01:11<02:01,  3.43it/s] 29%|██▉       | 174/590 [01:12<02:01,  3.43it/s] 30%|██▉       | 175/590 [01:12<02:01,  3.43it/s] 30%|██▉       | 176/590 [01:12<02:00,  3.43it/s] 30%|███       | 177/590 [01:12<02:00,  3.43it/s] 30%|███       | 178/590 [01:13<02:00,  3.43it/s] 30%|███       | 179/590 [01:13<01:59,  3.43it/s] 31%|███       | 180/590 [01:13<01:59,  3.43it/s] 31%|███       | 181/590 [01:14<01:59,  3.43it/s] 31%|███       | 182/590 [01:14<01:58,  3.44it/s] 31%|███       | 183/590 [01:14<01:59,  3.42it/s] 31%|███       | 184/590 [01:15<01:58,  3.42it/s] 31%|███▏      | 185/590 [01:15<01:58,  3.43it/s] 32%|███▏      | 186/590 [01:15<01:57,  3.43it/s] 32%|███▏      | 187/590 [01:15<01:57,  3.43it/s] 32%|███▏      | 188/590 [01:16<01:57,  3.43it/s] 32%|███▏      | 189/590 [01:16<01:56,  3.43it/s] 32%|███▏      | 190/590 [01:16<01:56,  3.43it/s] 32%|███▏      | 191/590 [01:17<01:56,  3.43it/s] 33%|███▎      | 192/590 [01:17<01:56,  3.43it/s] 33%|███▎      | 193/590 [01:17<01:55,  3.43it/s] 33%|███▎      | 194/590 [01:17<01:55,  3.42it/s] 33%|███▎      | 195/590 [01:18<01:55,  3.42it/s] 33%|███▎      | 196/590 [01:18<01:55,  3.42it/s] 33%|███▎      | 197/590 [01:18<01:54,  3.42it/s] 34%|███▎      | 198/590 [01:19<01:54,  3.43it/s] 34%|███▎      | 199/590 [01:19<01:54,  3.43it/s] 34%|███▍      | 200/590 [01:19<01:53,  3.43it/s] 34%|███▍      | 201/590 [01:19<01:53,  3.43it/s] 34%|███▍      | 202/590 [01:20<01:53,  3.43it/s] 34%|███▍      | 203/590 [01:20<01:52,  3.43it/s] 35%|███▍      | 204/590 [01:20<01:52,  3.43it/s] 35%|███▍      | 205/590 [01:21<01:52,  3.42it/s] 35%|███▍      | 206/590 [01:21<01:52,  3.42it/s] 35%|███▌      | 207/590 [01:21<01:51,  3.43it/s] 35%|███▌      | 208/590 [01:22<01:51,  3.43it/s] 35%|███▌      | 209/590 [01:22<01:51,  3.43it/s] 36%|███▌      | 210/590 [01:22<01:50,  3.43it/s] 36%|███▌      | 211/590 [01:22<01:50,  3.43it/s] 36%|███▌      | 212/590 [01:23<01:50,  3.43it/s] 36%|███▌      | 213/590 [01:23<01:49,  3.43it/s] 36%|███▋      | 214/590 [01:23<01:49,  3.43it/s] 36%|███▋      | 215/590 [01:24<01:49,  3.43it/s] 37%|███▋      | 216/590 [01:24<01:49,  3.42it/s] 37%|███▋      | 217/590 [01:24<01:48,  3.43it/s] 37%|███▋      | 218/590 [01:24<01:48,  3.43it/s] 37%|███▋      | 219/590 [01:25<01:48,  3.43it/s] 37%|███▋      | 220/590 [01:25<01:47,  3.43it/s] 37%|███▋      | 221/590 [01:25<01:47,  3.43it/s] 38%|███▊      | 222/590 [01:26<01:47,  3.43it/s] 38%|███▊      | 223/590 [01:26<01:46,  3.43it/s] 38%|███▊      | 224/590 [01:26<01:46,  3.43it/s] 38%|███▊      | 225/590 [01:26<01:46,  3.43it/s] 38%|███▊      | 226/590 [01:27<01:46,  3.43it/s] 38%|███▊      | 227/590 [01:27<01:45,  3.43it/s] 39%|███▊      | 228/590 [01:27<01:45,  3.43it/s] 39%|███▉      | 229/590 [01:28<01:45,  3.43it/s] 39%|███▉      | 230/590 [01:28<01:45,  3.43it/s] 39%|███▉      | 231/590 [01:28<01:44,  3.43it/s] 39%|███▉      | 232/590 [01:29<01:44,  3.43it/s] 39%|███▉      | 233/590 [01:29<01:45,  3.39it/s] 40%|███▉      | 234/590 [01:29<01:44,  3.40it/s] 40%|███▉      | 235/590 [01:29<01:44,  3.41it/s] 40%|████      | 236/590 [01:30<01:43,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 14:36:51,474 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:36:51,474 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 14:36:51,474 >>   Batch size = 8
{'eval_loss': 0.9040324091911316, 'eval_runtime': 13.8852, 'eval_samples_per_second': 351.596, 'eval_steps_per_second': 44.004, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.45it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.43it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.29it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.50it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.34it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.29it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.19it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.91it/s][A
  8%|▊         | 47/611 [00:01<00:12, 43.94it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.06it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.26it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.23it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.05it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.06it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.08it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.97it/s][A
 14%|█▍        | 87/611 [00:01<00:12, 43.12it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.40it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 43.74it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 43.84it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 43.92it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.05it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.17it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.04it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.95it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.98it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.10it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.27it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.23it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.21it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.19it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.10it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.98it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.00it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.06it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.12it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.10it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.24it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.17it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.24it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.17it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.02it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.10it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.07it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.11it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.01it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.12it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.20it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.27it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.13it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.05it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.96it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.14it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.20it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.02it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.15it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.25it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.24it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.15it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.99it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.08it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.08it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.16it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.16it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.18it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.24it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.18it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.16it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.07it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.15it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.13it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.15it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.15it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.19it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.18it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.09it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.12it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.02it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.00it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.01it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.12it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.23it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.20it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.15it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.17it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.10it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 43.99it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.09it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.02it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.16it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.17it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.22it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.92it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.17it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.11it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.06it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.05it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 43.96it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.26it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.29it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.20it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.12it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.04it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.04it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.11it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.01it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.09it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.25it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.33it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.12it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.12it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.04it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.04it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.11it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.03it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.00it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.30it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.26it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.17it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.20it/s][A                                                 
                                                 [A 40%|████      | 236/590 [01:44<01:43,  3.41it/s]
100%|██████████| 611/611 [00:13<00:00, 44.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:37:05,341 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-28 14:37:05,355 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:37:07,462 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:37:07,485 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:37:07,495 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [01:51<38:07,  6.48s/it] 40%|████      | 238/590 [01:51<27:08,  4.63s/it] 41%|████      | 239/590 [01:51<19:27,  3.33s/it] 41%|████      | 240/590 [01:52<14:06,  2.42s/it] 41%|████      | 241/590 [01:52<10:21,  1.78s/it] 41%|████      | 242/590 [01:52<07:44,  1.33s/it] 41%|████      | 243/590 [01:52<05:56,  1.03s/it] 41%|████▏     | 244/590 [01:53<04:39,  1.24it/s] 42%|████▏     | 245/590 [01:53<03:45,  1.53it/s] 42%|████▏     | 246/590 [01:53<03:07,  1.83it/s] 42%|████▏     | 247/590 [01:54<02:41,  2.12it/s] 42%|████▏     | 248/590 [01:54<02:22,  2.39it/s] 42%|████▏     | 249/590 [01:54<02:10,  2.62it/s] 42%|████▏     | 250/590 [01:54<02:01,  2.81it/s] 43%|████▎     | 251/590 [01:55<01:54,  2.96it/s] 43%|████▎     | 252/590 [01:55<01:49,  3.08it/s] 43%|████▎     | 253/590 [01:55<01:46,  3.16it/s] 43%|████▎     | 254/590 [01:56<01:44,  3.23it/s] 43%|████▎     | 255/590 [01:56<01:42,  3.27it/s] 43%|████▎     | 256/590 [01:56<01:41,  3.30it/s] 44%|████▎     | 257/590 [01:57<01:40,  3.33it/s] 44%|████▎     | 258/590 [01:57<01:39,  3.35it/s] 44%|████▍     | 259/590 [01:57<01:38,  3.36it/s] 44%|████▍     | 260/590 [01:57<01:38,  3.36it/s] 44%|████▍     | 261/590 [01:58<01:37,  3.37it/s] 44%|████▍     | 262/590 [01:58<01:37,  3.38it/s] 45%|████▍     | 263/590 [01:58<01:36,  3.38it/s] 45%|████▍     | 264/590 [01:59<01:36,  3.38it/s] 45%|████▍     | 265/590 [01:59<01:35,  3.40it/s] 45%|████▌     | 266/590 [01:59<01:35,  3.41it/s] 45%|████▌     | 267/590 [01:59<01:34,  3.40it/s] 45%|████▌     | 268/590 [02:00<01:34,  3.41it/s] 46%|████▌     | 269/590 [02:00<01:33,  3.42it/s] 46%|████▌     | 270/590 [02:00<01:33,  3.42it/s] 46%|████▌     | 271/590 [02:01<01:33,  3.43it/s] 46%|████▌     | 272/590 [02:01<01:32,  3.43it/s] 46%|████▋     | 273/590 [02:01<01:32,  3.43it/s] 46%|████▋     | 274/590 [02:02<01:32,  3.43it/s] 47%|████▋     | 275/590 [02:02<01:31,  3.43it/s] 47%|████▋     | 276/590 [02:02<01:31,  3.43it/s] 47%|████▋     | 277/590 [02:02<01:31,  3.43it/s] 47%|████▋     | 278/590 [02:03<01:31,  3.43it/s] 47%|████▋     | 279/590 [02:03<01:30,  3.43it/s] 47%|████▋     | 280/590 [02:03<01:30,  3.43it/s] 48%|████▊     | 281/590 [02:04<01:30,  3.43it/s] 48%|████▊     | 282/590 [02:04<01:29,  3.43it/s] 48%|████▊     | 283/590 [02:04<01:29,  3.43it/s] 48%|████▊     | 284/590 [02:04<01:29,  3.43it/s] 48%|████▊     | 285/590 [02:05<01:28,  3.43it/s] 48%|████▊     | 286/590 [02:05<01:28,  3.43it/s] 49%|████▊     | 287/590 [02:05<01:28,  3.44it/s] 49%|████▉     | 288/590 [02:06<01:28,  3.43it/s] 49%|████▉     | 289/590 [02:06<01:28,  3.41it/s] 49%|████▉     | 290/590 [02:06<01:27,  3.42it/s] 49%|████▉     | 291/590 [02:06<01:27,  3.42it/s] 49%|████▉     | 292/590 [02:07<01:27,  3.42it/s] 50%|████▉     | 293/590 [02:07<01:26,  3.43it/s] 50%|████▉     | 294/590 [02:07<01:26,  3.43it/s] 50%|█████     | 295/590 [02:08<01:25,  3.43it/s] 50%|█████     | 296/590 [02:08<01:25,  3.43it/s] 50%|█████     | 297/590 [02:08<01:25,  3.43it/s] 51%|█████     | 298/590 [02:09<01:25,  3.43it/s] 51%|█████     | 299/590 [02:09<01:24,  3.44it/s] 51%|█████     | 300/590 [02:09<01:24,  3.42it/s] 51%|█████     | 301/590 [02:09<01:24,  3.43it/s] 51%|█████     | 302/590 [02:10<01:23,  3.43it/s] 51%|█████▏    | 303/590 [02:10<01:23,  3.43it/s] 52%|█████▏    | 304/590 [02:10<01:23,  3.43it/s] 52%|█████▏    | 305/590 [02:11<01:23,  3.43it/s] 52%|█████▏    | 306/590 [02:11<01:22,  3.43it/s] 52%|█████▏    | 307/590 [02:11<01:22,  3.43it/s] 52%|█████▏    | 308/590 [02:11<01:22,  3.43it/s] 52%|█████▏    | 309/590 [02:12<01:21,  3.43it/s] 53%|█████▎    | 310/590 [02:12<01:21,  3.43it/s] 53%|█████▎    | 311/590 [02:12<01:21,  3.43it/s] 53%|█████▎    | 312/590 [02:13<01:21,  3.43it/s] 53%|█████▎    | 313/590 [02:13<01:20,  3.43it/s] 53%|█████▎    | 314/590 [02:13<01:20,  3.43it/s] 53%|█████▎    | 315/590 [02:13<01:20,  3.43it/s] 54%|█████▎    | 316/590 [02:14<01:19,  3.43it/s] 54%|█████▎    | 317/590 [02:14<01:19,  3.43it/s] 54%|█████▍    | 318/590 [02:14<01:19,  3.43it/s] 54%|█████▍    | 319/590 [02:15<01:19,  3.43it/s] 54%|█████▍    | 320/590 [02:15<01:18,  3.43it/s] 54%|█████▍    | 321/590 [02:15<01:18,  3.43it/s] 55%|█████▍    | 322/590 [02:16<01:20,  3.32it/s] 55%|█████▍    | 323/590 [02:16<01:19,  3.35it/s] 55%|█████▍    | 324/590 [02:16<01:18,  3.37it/s] 55%|█████▌    | 325/590 [02:16<01:18,  3.39it/s] 55%|█████▌    | 326/590 [02:17<01:17,  3.40it/s] 55%|█████▌    | 327/590 [02:17<01:17,  3.40it/s] 56%|█████▌    | 328/590 [02:17<01:16,  3.41it/s] 56%|█████▌    | 329/590 [02:18<01:16,  3.40it/s] 56%|█████▌    | 330/590 [02:18<01:16,  3.41it/s] 56%|█████▌    | 331/590 [02:18<01:15,  3.42it/s] 56%|█████▋    | 332/590 [02:18<01:15,  3.42it/s] 56%|█████▋    | 333/590 [02:19<01:16,  3.34it/s] 57%|█████▋    | 334/590 [02:19<01:16,  3.36it/s] 57%|█████▋    | 335/590 [02:19<01:15,  3.38it/s] 57%|█████▋    | 336/590 [02:20<01:14,  3.40it/s] 57%|█████▋    | 337/590 [02:20<01:14,  3.40it/s] 57%|█████▋    | 338/590 [02:20<01:13,  3.41it/s] 57%|█████▋    | 339/590 [02:21<01:13,  3.42it/s] 58%|█████▊    | 340/590 [02:21<01:13,  3.42it/s] 58%|█████▊    | 341/590 [02:21<01:12,  3.42it/s] 58%|█████▊    | 342/590 [02:21<01:12,  3.42it/s] 58%|█████▊    | 343/590 [02:22<01:12,  3.42it/s] 58%|█████▊    | 344/590 [02:22<01:13,  3.35it/s] 58%|█████▊    | 345/590 [02:22<01:12,  3.37it/s] 59%|█████▊    | 346/590 [02:23<01:12,  3.39it/s] 59%|█████▉    | 347/590 [02:23<01:11,  3.40it/s] 59%|█████▉    | 348/590 [02:23<01:10,  3.41it/s] 59%|█████▉    | 349/590 [02:23<01:10,  3.41it/s] 59%|█████▉    | 350/590 [02:24<01:10,  3.42it/s] 59%|█████▉    | 351/590 [02:24<01:09,  3.42it/s] 60%|█████▉    | 352/590 [02:24<01:09,  3.42it/s] 60%|█████▉    | 353/590 [02:25<01:09,  3.42it/s] 60%|██████    | 354/590 [02:25<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 14:37:46,718 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:37:46,718 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 14:37:46,718 >>   Batch size = 8
{'eval_loss': 0.902677595615387, 'eval_runtime': 13.8502, 'eval_samples_per_second': 352.487, 'eval_steps_per_second': 44.115, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 57.04it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.73it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.48it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.57it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.78it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.57it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.43it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.03it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.07it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.24it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.36it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.29it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.20it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.04it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.00it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.94it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.98it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.04it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.25it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.32it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.28it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.12it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.03it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.99it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.93it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.99it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.16it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.24it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.31it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.24it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.13it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 43.91it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.98it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.95it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.11it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.24it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.18it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.29it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.29it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.11it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.92it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.98it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.01it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.07it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.08it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.18it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.28it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.24it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.09it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.93it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.04it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.06it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.17it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.17it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.22it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.31it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.12it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 43.99it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.99it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.92it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.00it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.19it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.21it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.23it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.27it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.18it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.11it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.07it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.93it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.96it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.13it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.27it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.19it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.26it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.13it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 43.98it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.97it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.98it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.98it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.08it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.17it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.25it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.27it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.09it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.97it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.03it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.06it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 43.96it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.16it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.28it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.31it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.19it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.13it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.97it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.91it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.94it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.03it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.11it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.25it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.24it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.18it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.17it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.09it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.92it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.96it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.09it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.25it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.27it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.28it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.25it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.15it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.03it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.98it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 43.94it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.08it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.99it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.30it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.34it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.14it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.15it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.07it/s][A                                                 
                                                 [A 60%|██████    | 354/590 [02:39<01:08,  3.43it/s]
100%|██████████| 611/611 [00:13<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:38:00,573 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-28 14:38:00,593 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:38:02,615 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:38:02,634 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:38:02,646 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [02:46<25:30,  6.51s/it] 60%|██████    | 356/590 [02:46<18:07,  4.65s/it] 61%|██████    | 357/590 [02:47<12:58,  3.34s/it] 61%|██████    | 358/590 [02:47<09:23,  2.43s/it] 61%|██████    | 359/590 [02:47<06:52,  1.79s/it] 61%|██████    | 360/590 [02:47<05:08,  1.34s/it] 61%|██████    | 361/590 [02:48<03:55,  1.03s/it] 61%|██████▏   | 362/590 [02:48<03:04,  1.24it/s] 62%|██████▏   | 363/590 [02:48<02:28,  1.53it/s] 62%|██████▏   | 364/590 [02:49<02:03,  1.83it/s] 62%|██████▏   | 365/590 [02:49<01:45,  2.12it/s] 62%|██████▏   | 366/590 [02:49<01:33,  2.39it/s] 62%|██████▏   | 367/590 [02:50<01:25,  2.62it/s] 62%|██████▏   | 368/590 [02:50<01:19,  2.81it/s] 63%|██████▎   | 369/590 [02:50<01:14,  2.96it/s] 63%|██████▎   | 370/590 [02:50<01:11,  3.08it/s] 63%|██████▎   | 371/590 [02:51<01:09,  3.16it/s] 63%|██████▎   | 372/590 [02:51<01:07,  3.22it/s] 63%|██████▎   | 373/590 [02:51<01:06,  3.27it/s] 63%|██████▎   | 374/590 [02:52<01:05,  3.31it/s] 64%|██████▎   | 375/590 [02:52<01:04,  3.33it/s] 64%|██████▎   | 376/590 [02:52<01:03,  3.35it/s] 64%|██████▍   | 377/590 [02:52<01:03,  3.36it/s] 64%|██████▍   | 378/590 [02:53<01:02,  3.37it/s] 64%|██████▍   | 379/590 [02:53<01:02,  3.37it/s] 64%|██████▍   | 380/590 [02:53<01:02,  3.37it/s] 65%|██████▍   | 381/590 [02:54<01:01,  3.38it/s] 65%|██████▍   | 382/590 [02:54<01:01,  3.38it/s] 65%|██████▍   | 383/590 [02:54<01:01,  3.39it/s] 65%|██████▌   | 384/590 [02:55<01:00,  3.38it/s] 65%|██████▌   | 385/590 [02:55<01:00,  3.38it/s] 65%|██████▌   | 386/590 [02:55<01:00,  3.39it/s] 66%|██████▌   | 387/590 [02:55<00:59,  3.39it/s] 66%|██████▌   | 388/590 [02:56<00:59,  3.39it/s] 66%|██████▌   | 389/590 [02:56<00:59,  3.37it/s] 66%|██████▌   | 390/590 [02:56<00:59,  3.37it/s] 66%|██████▋   | 391/590 [02:57<00:58,  3.38it/s] 66%|██████▋   | 392/590 [02:57<00:58,  3.38it/s] 67%|██████▋   | 393/590 [02:57<00:58,  3.39it/s] 67%|██████▋   | 394/590 [02:57<00:57,  3.39it/s] 67%|██████▋   | 395/590 [02:58<00:57,  3.41it/s] 67%|██████▋   | 396/590 [02:58<00:56,  3.41it/s] 67%|██████▋   | 397/590 [02:58<00:56,  3.42it/s] 67%|██████▋   | 398/590 [02:59<00:56,  3.43it/s] 68%|██████▊   | 399/590 [02:59<00:55,  3.43it/s] 68%|██████▊   | 400/590 [02:59<00:55,  3.43it/s] 68%|██████▊   | 401/590 [03:00<00:55,  3.44it/s] 68%|██████▊   | 402/590 [03:00<00:54,  3.44it/s] 68%|██████▊   | 403/590 [03:00<00:54,  3.43it/s] 68%|██████▊   | 404/590 [03:00<00:54,  3.44it/s] 69%|██████▊   | 405/590 [03:01<00:53,  3.44it/s] 69%|██████▉   | 406/590 [03:01<00:53,  3.42it/s] 69%|██████▉   | 407/590 [03:01<00:53,  3.43it/s] 69%|██████▉   | 408/590 [03:02<00:53,  3.43it/s] 69%|██████▉   | 409/590 [03:02<00:52,  3.43it/s] 69%|██████▉   | 410/590 [03:02<00:52,  3.43it/s] 70%|██████▉   | 411/590 [03:02<00:52,  3.43it/s] 70%|██████▉   | 412/590 [03:03<00:51,  3.43it/s] 70%|███████   | 413/590 [03:03<00:51,  3.43it/s] 70%|███████   | 414/590 [03:03<00:51,  3.44it/s] 70%|███████   | 415/590 [03:04<00:50,  3.44it/s] 71%|███████   | 416/590 [03:04<00:50,  3.44it/s] 71%|███████   | 417/590 [03:04<00:50,  3.42it/s] 71%|███████   | 418/590 [03:04<00:50,  3.42it/s] 71%|███████   | 419/590 [03:05<00:49,  3.43it/s] 71%|███████   | 420/590 [03:05<00:49,  3.43it/s] 71%|███████▏  | 421/590 [03:05<00:49,  3.43it/s] 72%|███████▏  | 422/590 [03:06<00:48,  3.43it/s] 72%|███████▏  | 423/590 [03:06<00:48,  3.44it/s] 72%|███████▏  | 424/590 [03:06<00:48,  3.43it/s] 72%|███████▏  | 425/590 [03:07<00:48,  3.44it/s] 72%|███████▏  | 426/590 [03:07<00:47,  3.44it/s] 72%|███████▏  | 427/590 [03:07<00:47,  3.44it/s] 73%|███████▎  | 428/590 [03:07<00:47,  3.42it/s] 73%|███████▎  | 429/590 [03:08<00:47,  3.42it/s] 73%|███████▎  | 430/590 [03:08<00:46,  3.43it/s] 73%|███████▎  | 431/590 [03:08<00:46,  3.43it/s] 73%|███████▎  | 432/590 [03:09<00:46,  3.43it/s] 73%|███████▎  | 433/590 [03:09<00:45,  3.43it/s] 74%|███████▎  | 434/590 [03:09<00:45,  3.43it/s] 74%|███████▎  | 435/590 [03:09<00:45,  3.43it/s] 74%|███████▍  | 436/590 [03:10<00:44,  3.43it/s] 74%|███████▍  | 437/590 [03:10<00:44,  3.43it/s] 74%|███████▍  | 438/590 [03:10<00:44,  3.43it/s] 74%|███████▍  | 439/590 [03:11<00:44,  3.42it/s] 75%|███████▍  | 440/590 [03:11<00:43,  3.43it/s] 75%|███████▍  | 441/590 [03:11<00:43,  3.42it/s] 75%|███████▍  | 442/590 [03:11<00:43,  3.43it/s] 75%|███████▌  | 443/590 [03:12<00:42,  3.43it/s] 75%|███████▌  | 444/590 [03:12<00:42,  3.43it/s] 75%|███████▌  | 445/590 [03:12<00:42,  3.43it/s] 76%|███████▌  | 446/590 [03:13<00:41,  3.43it/s] 76%|███████▌  | 447/590 [03:13<00:41,  3.43it/s] 76%|███████▌  | 448/590 [03:13<00:41,  3.43it/s] 76%|███████▌  | 449/590 [03:14<00:41,  3.43it/s] 76%|███████▋  | 450/590 [03:14<00:42,  3.31it/s] 76%|███████▋  | 451/590 [03:14<00:41,  3.34it/s] 77%|███████▋  | 452/590 [03:14<00:41,  3.36it/s] 77%|███████▋  | 453/590 [03:15<00:40,  3.38it/s] 77%|███████▋  | 454/590 [03:15<00:40,  3.40it/s] 77%|███████▋  | 455/590 [03:15<00:39,  3.41it/s] 77%|███████▋  | 456/590 [03:16<00:39,  3.42it/s] 77%|███████▋  | 457/590 [03:16<00:38,  3.42it/s] 78%|███████▊  | 458/590 [03:16<00:38,  3.42it/s] 78%|███████▊  | 459/590 [03:16<00:38,  3.41it/s] 78%|███████▊  | 460/590 [03:17<00:38,  3.40it/s] 78%|███████▊  | 461/590 [03:17<00:38,  3.37it/s] 78%|███████▊  | 462/590 [03:17<00:37,  3.37it/s] 78%|███████▊  | 463/590 [03:18<00:37,  3.37it/s] 79%|███████▊  | 464/590 [03:18<00:37,  3.37it/s] 79%|███████▉  | 465/590 [03:18<00:37,  3.37it/s] 79%|███████▉  | 466/590 [03:19<00:37,  3.35it/s] 79%|███████▉  | 467/590 [03:19<00:36,  3.36it/s] 79%|███████▉  | 468/590 [03:19<00:36,  3.37it/s] 79%|███████▉  | 469/590 [03:19<00:35,  3.37it/s] 80%|███████▉  | 470/590 [03:20<00:35,  3.37it/s] 80%|███████▉  | 471/590 [03:20<00:35,  3.38it/s] 80%|████████  | 472/590 [03:20<00:34,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 14:38:42,111 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:38:42,111 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 14:38:42,111 >>   Batch size = 8
{'eval_loss': 0.9041170477867126, 'eval_runtime': 13.8405, 'eval_samples_per_second': 352.732, 'eval_steps_per_second': 44.146, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.41it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.16it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.12it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.19it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.79it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.56it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.50it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.11it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.18it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.41it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.36it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.18it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.14it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.02it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.06it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.06it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.05it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.09it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.31it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.14it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.12it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.18it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.10it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.05it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.94it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.96it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.03it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.06it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.03it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.04it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.01it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 43.98it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.80it/s][A
 28%|██▊       | 172/611 [00:03<00:10, 43.70it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.88it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.95it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.33it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.30it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.27it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.17it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.08it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.99it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.00it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.04it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.09it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.16it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.13it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.07it/s][A
 40%|████      | 247/611 [00:05<00:08, 43.92it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.92it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.70it/s][A
 43%|████▎     | 262/611 [00:05<00:08, 42.56it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.02it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.34it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 43.72it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 43.81it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 43.80it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 43.85it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.81it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.67it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.79it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.03it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.00it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 43.98it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 43.99it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.10it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.94it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.86it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.85it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.95it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.21it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.27it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.23it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.13it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.10it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.00it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.90it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.96it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 44.11it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.19it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.19it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.17it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.21it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.16it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.02it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.01it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.97it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.02it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.19it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.21it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.17it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.17it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.13it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.10it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.06it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.00it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.12it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.17it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.23it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.19it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.08it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.95it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.04it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.14it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.20it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.07it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.10it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.16it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.26it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.16it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.09it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.03it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.15it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.08it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.16it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.11it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.23it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.08it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.18it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.09it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.05it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:34<00:34,  3.38it/s]
100%|██████████| 611/611 [00:13<00:00, 44.05it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:38:55,990 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-28 14:38:56,023 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:38:58,223 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:38:58,241 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:38:58,251 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [03:41<12:28,  6.39s/it] 80%|████████  | 474/590 [03:41<08:49,  4.57s/it] 81%|████████  | 475/590 [03:42<06:17,  3.29s/it] 81%|████████  | 476/590 [03:42<04:32,  2.39s/it] 81%|████████  | 477/590 [03:42<03:18,  1.76s/it] 81%|████████  | 478/590 [03:42<02:27,  1.32s/it] 81%|████████  | 479/590 [03:43<01:52,  1.01s/it] 81%|████████▏ | 480/590 [03:43<01:27,  1.26it/s] 82%|████████▏ | 481/590 [03:43<01:10,  1.55it/s] 82%|████████▏ | 482/590 [03:44<00:58,  1.86it/s] 82%|████████▏ | 483/590 [03:44<00:49,  2.15it/s] 82%|████████▏ | 484/590 [03:44<00:43,  2.43it/s] 82%|████████▏ | 485/590 [03:44<00:39,  2.65it/s] 82%|████████▏ | 486/590 [03:45<00:36,  2.85it/s] 83%|████████▎ | 487/590 [03:45<00:34,  3.00it/s] 83%|████████▎ | 488/590 [03:45<00:32,  3.12it/s] 83%|████████▎ | 489/590 [03:46<00:31,  3.21it/s] 83%|████████▎ | 490/590 [03:46<00:30,  3.28it/s] 83%|████████▎ | 491/590 [03:46<00:29,  3.32it/s] 83%|████████▎ | 492/590 [03:47<00:29,  3.36it/s] 84%|████████▎ | 493/590 [03:47<00:28,  3.38it/s] 84%|████████▎ | 494/590 [03:47<00:28,  3.40it/s] 84%|████████▍ | 495/590 [03:47<00:27,  3.41it/s] 84%|████████▍ | 496/590 [03:48<00:27,  3.40it/s] 84%|████████▍ | 497/590 [03:48<00:27,  3.41it/s] 84%|████████▍ | 498/590 [03:48<00:26,  3.42it/s] 85%|████████▍ | 499/590 [03:49<00:26,  3.42it/s] 85%|████████▍ | 500/590 [03:49<00:26,  3.43it/s]                                                  85%|████████▍ | 500/590 [03:49<00:26,  3.43it/s] 85%|████████▍ | 501/590 [03:49<00:25,  3.43it/s] 85%|████████▌ | 502/590 [03:49<00:25,  3.43it/s] 85%|████████▌ | 503/590 [03:50<00:25,  3.44it/s] 85%|████████▌ | 504/590 [03:50<00:25,  3.43it/s] 86%|████████▌ | 505/590 [03:50<00:24,  3.44it/s] 86%|████████▌ | 506/590 [03:51<00:24,  3.44it/s] 86%|████████▌ | 507/590 [03:51<00:24,  3.42it/s] 86%|████████▌ | 508/590 [03:51<00:23,  3.43it/s] 86%|████████▋ | 509/590 [03:51<00:23,  3.43it/s] 86%|████████▋ | 510/590 [03:52<00:23,  3.43it/s] 87%|████████▋ | 511/590 [03:52<00:23,  3.43it/s] 87%|████████▋ | 512/590 [03:52<00:22,  3.40it/s] 87%|████████▋ | 513/590 [03:53<00:22,  3.41it/s] 87%|████████▋ | 514/590 [03:53<00:22,  3.42it/s] 87%|████████▋ | 515/590 [03:53<00:21,  3.42it/s] 87%|████████▋ | 516/590 [03:54<00:21,  3.43it/s] 88%|████████▊ | 517/590 [03:54<00:21,  3.43it/s] 88%|████████▊ | 518/590 [03:54<00:21,  3.42it/s] 88%|████████▊ | 519/590 [03:54<00:20,  3.43it/s] 88%|████████▊ | 520/590 [03:55<00:20,  3.43it/s] 88%|████████▊ | 521/590 [03:55<00:20,  3.43it/s] 88%|████████▊ | 522/590 [03:55<00:19,  3.43it/s] 89%|████████▊ | 523/590 [03:56<00:19,  3.44it/s] 89%|████████▉ | 524/590 [03:56<00:19,  3.44it/s] 89%|████████▉ | 525/590 [03:56<00:18,  3.43it/s] 89%|████████▉ | 526/590 [03:56<00:18,  3.43it/s] 89%|████████▉ | 527/590 [03:57<00:18,  3.43it/s] 89%|████████▉ | 528/590 [03:57<00:18,  3.43it/s] 90%|████████▉ | 529/590 [03:57<00:17,  3.41it/s] 90%|████████▉ | 530/590 [03:58<00:17,  3.42it/s] 90%|█████████ | 531/590 [03:58<00:17,  3.43it/s] 90%|█████████ | 532/590 [03:58<00:16,  3.43it/s] 90%|█████████ | 533/590 [03:58<00:16,  3.43it/s] 91%|█████████ | 534/590 [03:59<00:16,  3.43it/s] 91%|█████████ | 535/590 [03:59<00:16,  3.43it/s] 91%|█████████ | 536/590 [03:59<00:15,  3.43it/s] 91%|█████████ | 537/590 [04:00<00:15,  3.43it/s] 91%|█████████ | 538/590 [04:00<00:15,  3.43it/s] 91%|█████████▏| 539/590 [04:00<00:14,  3.44it/s] 92%|█████████▏| 540/590 [04:01<00:14,  3.43it/s] 92%|█████████▏| 541/590 [04:01<00:14,  3.44it/s] 92%|█████████▏| 542/590 [04:01<00:13,  3.44it/s] 92%|█████████▏| 543/590 [04:01<00:13,  3.43it/s] 92%|█████████▏| 544/590 [04:02<00:13,  3.44it/s] 92%|█████████▏| 545/590 [04:02<00:13,  3.43it/s] 93%|█████████▎| 546/590 [04:02<00:12,  3.43it/s] 93%|█████████▎| 547/590 [04:03<00:12,  3.43it/s] 93%|█████████▎| 548/590 [04:03<00:12,  3.43it/s] 93%|█████████▎| 549/590 [04:03<00:11,  3.43it/s] 93%|█████████▎| 550/590 [04:03<00:11,  3.43it/s] 93%|█████████▎| 551/590 [04:04<00:11,  3.43it/s] 94%|█████████▎| 552/590 [04:04<00:11,  3.43it/s] 94%|█████████▎| 553/590 [04:04<00:10,  3.43it/s] 94%|█████████▍| 554/590 [04:05<00:10,  3.43it/s] 94%|█████████▍| 555/590 [04:05<00:10,  3.43it/s] 94%|█████████▍| 556/590 [04:05<00:09,  3.44it/s] 94%|█████████▍| 557/590 [04:05<00:09,  3.43it/s] 95%|█████████▍| 558/590 [04:06<00:09,  3.42it/s] 95%|█████████▍| 559/590 [04:06<00:09,  3.43it/s] 95%|█████████▍| 560/590 [04:06<00:08,  3.43it/s] 95%|█████████▌| 561/590 [04:07<00:08,  3.43it/s] 95%|█████████▌| 562/590 [04:07<00:08,  3.43it/s] 95%|█████████▌| 563/590 [04:07<00:07,  3.43it/s] 96%|█████████▌| 564/590 [04:08<00:07,  3.43it/s] 96%|█████████▌| 565/590 [04:08<00:07,  3.43it/s] 96%|█████████▌| 566/590 [04:08<00:06,  3.43it/s] 96%|█████████▌| 567/590 [04:08<00:06,  3.43it/s] 96%|█████████▋| 568/590 [04:09<00:06,  3.44it/s] 96%|█████████▋| 569/590 [04:09<00:06,  3.42it/s] 97%|█████████▋| 570/590 [04:09<00:05,  3.43it/s] 97%|█████████▋| 571/590 [04:10<00:05,  3.43it/s] 97%|█████████▋| 572/590 [04:10<00:05,  3.43it/s] 97%|█████████▋| 573/590 [04:10<00:04,  3.43it/s] 97%|█████████▋| 574/590 [04:10<00:04,  3.43it/s] 97%|█████████▋| 575/590 [04:11<00:04,  3.43it/s] 98%|█████████▊| 576/590 [04:11<00:04,  3.43it/s] 98%|█████████▊| 577/590 [04:11<00:03,  3.43it/s] 98%|█████████▊| 578/590 [04:12<00:03,  3.43it/s] 98%|█████████▊| 579/590 [04:12<00:03,  3.43it/s] 98%|█████████▊| 580/590 [04:12<00:02,  3.43it/s] 98%|█████████▊| 581/590 [04:12<00:02,  3.42it/s] 99%|█████████▊| 582/590 [04:13<00:02,  3.43it/s] 99%|█████████▉| 583/590 [04:13<00:02,  3.43it/s] 99%|█████████▉| 584/590 [04:13<00:01,  3.43it/s] 99%|█████████▉| 585/590 [04:14<00:01,  3.43it/s] 99%|█████████▉| 586/590 [04:14<00:01,  3.43it/s] 99%|█████████▉| 587/590 [04:14<00:00,  3.43it/s]100%|█████████▉| 588/590 [04:14<00:00,  3.43it/s]100%|█████████▉| 589/590 [04:15<00:00,  3.43it/s]100%|██████████| 590/590 [04:15<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 14:39:36,832 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:39:36,832 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 14:39:36,832 >>   Batch size = 8
{'eval_loss': 0.9058868288993835, 'eval_runtime': 13.8654, 'eval_samples_per_second': 352.099, 'eval_steps_per_second': 44.066, 'epoch': 4.0}
{'loss': 0.7466, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.74it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.46it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.48it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.45it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.77it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.56it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.52it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.21it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.28it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.29it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.36it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.17it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.25it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.09it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.14it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.05it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.04it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.18it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.31it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.28it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.05it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.06it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.12it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.03it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.09it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.04it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.16it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.24it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.21it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.11it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.13it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.08it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.05it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.95it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.02it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.18it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.16it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.20it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.08it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.07it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.06it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.11it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.05it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.06it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.13it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.20it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.20it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.20it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.17it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.01it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.11it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.11it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.09it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.24it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.14it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.18it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.20it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.22it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.16it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.10it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.99it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.08it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.11it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.26it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.19it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.17it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.17it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.18it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.11it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.03it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.18it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.19it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.08it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.13it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.14it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.15it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.05it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.11it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.13it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.22it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.21it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.04it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.12it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.12it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.09it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.08it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.15it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.09it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.18it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.18it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.09it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.21it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.06it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.00it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.10it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.16it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.18it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.15it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.03it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.08it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.18it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.17it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.11it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.11it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.03it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.15it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.06it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.06it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.12it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.02it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.10it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.16it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.13it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.14it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.14it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.14it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.06it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.20it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.09it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.12it/s][A                                                 
                                                 [A100%|██████████| 590/590 [04:29<00:00,  3.43it/s]
100%|██████████| 611/611 [00:13<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:39:50,695 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-28 14:39:50,714 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:39:52,655 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:39:52,676 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:39:52,691 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:39:58,885 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:39:58,906 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236 (score: 0.902677595615387).
                                                 100%|██████████| 590/590 [04:41<00:00,  3.43it/s]100%|██████████| 590/590 [04:41<00:00,  2.10it/s]
[INFO|trainer.py:1894] 2023-08-28 14:40:02,612 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 14:40:02,630 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:40:04,970 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:40:04,989 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:40:04,999 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:40:05,209 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:05,209 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:05,209 >>   train_loss               =     0.7409
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:05,209 >>   train_runtime            = 0:04:41.36
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:05,209 >>   train_samples            =       7556
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:05,209 >>   train_samples_per_second =    134.276
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:05,209 >>   train_steps_per_second   =      2.097
{'eval_loss': 0.9075428247451782, 'eval_runtime': 13.8364, 'eval_samples_per_second': 352.837, 'eval_steps_per_second': 44.159, 'epoch': 5.0}
{'train_runtime': 281.36, 'train_samples_per_second': 134.276, 'train_steps_per_second': 2.097, 'train_loss': 0.7409459647485765, 'epoch': 5.0}
08/28/2023 14:40:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:40:05,260 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:40:05,260 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 14:40:05,261 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.17it/s]  2%|▏         | 12/611 [00:00<00:12, 48.49it/s]  3%|▎         | 17/611 [00:00<00:12, 46.91it/s]  4%|▎         | 22/611 [00:00<00:12, 46.10it/s]  4%|▍         | 27/611 [00:00<00:12, 45.72it/s]  5%|▌         | 32/611 [00:00<00:12, 45.53it/s]  6%|▌         | 37/611 [00:00<00:12, 45.33it/s]  7%|▋         | 42/611 [00:00<00:12, 44.72it/s]  8%|▊         | 47/611 [00:01<00:12, 44.17it/s]  9%|▊         | 52/611 [00:01<00:12, 43.80it/s]  9%|▉         | 57/611 [00:01<00:12, 43.62it/s] 10%|█         | 62/611 [00:01<00:12, 43.98it/s] 11%|█         | 67/611 [00:01<00:12, 44.12it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.32it/s] 13%|█▎        | 77/611 [00:01<00:12, 44.40it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.59it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.40it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.17it/s] 16%|█▌        | 97/611 [00:02<00:11, 43.83it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.80it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.00it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.17it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.27it/s] 20%|█▉        | 122/611 [00:02<00:11, 44.44it/s] 21%|██        | 127/611 [00:02<00:10, 44.44it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.26it/s] 22%|██▏       | 137/611 [00:03<00:10, 44.09it/s] 23%|██▎       | 142/611 [00:03<00:10, 43.91it/s] 24%|██▍       | 147/611 [00:03<00:10, 43.87it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.03it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.18it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.31it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.48it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.41it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.32it/s] 30%|██▉       | 182/611 [00:04<00:09, 43.99it/s] 31%|███       | 187/611 [00:04<00:09, 43.90it/s] 31%|███▏      | 192/611 [00:04<00:09, 43.86it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.04it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.28it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.42it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.39it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.32it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.20it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.06it/s] 38%|███▊      | 232/611 [00:05<00:08, 43.93it/s] 39%|███▉      | 237/611 [00:05<00:08, 43.81it/s] 40%|███▉      | 242/611 [00:05<00:08, 43.97it/s] 40%|████      | 247/611 [00:05<00:08, 44.20it/s] 41%|████      | 252/611 [00:05<00:08, 44.39it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.33it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.25it/s] 44%|████▎     | 267/611 [00:06<00:07, 44.08it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.04it/s] 45%|████▌     | 277/611 [00:06<00:07, 43.94it/s] 46%|████▌     | 282/611 [00:06<00:07, 43.99it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.04it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.26it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.22it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.27it/s] 50%|█████     | 307/611 [00:06<00:06, 44.01it/s] 51%|█████     | 312/611 [00:07<00:06, 44.00it/s] 52%|█████▏    | 317/611 [00:07<00:06, 43.95it/s] 53%|█████▎    | 322/611 [00:07<00:06, 43.99it/s] 54%|█████▎    | 327/611 [00:07<00:06, 43.60it/s] 54%|█████▍    | 332/611 [00:07<00:06, 43.87it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.12it/s] 56%|█████▌    | 342/611 [00:07<00:06, 43.61it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.39it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.21it/s] 58%|█████▊    | 357/611 [00:08<00:05, 43.99it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.00it/s] 60%|██████    | 367/611 [00:08<00:05, 44.02it/s] 61%|██████    | 372/611 [00:08<00:05, 43.89it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.09it/s] 63%|██████▎   | 382/611 [00:08<00:05, 43.81it/s] 63%|██████▎   | 387/611 [00:08<00:05, 43.84it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.35it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.48it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.40it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.20it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.05it/s] 68%|██████▊   | 417/611 [00:09<00:04, 43.97it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.04it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.10it/s] 71%|███████   | 432/611 [00:09<00:04, 44.34it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.25it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.24it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.15it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.06it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.07it/s] 76%|███████▌  | 462/611 [00:10<00:03, 43.95it/s] 76%|███████▋  | 467/611 [00:10<00:03, 43.95it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.17it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.30it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.32it/s] 80%|███████▉  | 487/611 [00:11<00:02, 44.25it/s] 81%|████████  | 492/611 [00:11<00:02, 44.22it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.04it/s] 82%|████████▏ | 502/611 [00:11<00:02, 43.97it/s] 83%|████████▎ | 507/611 [00:11<00:02, 43.84it/s] 84%|████████▍ | 512/611 [00:11<00:02, 43.97it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.14it/s] 85%|████████▌ | 522/611 [00:11<00:02, 44.25it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.26it/s] 87%|████████▋ | 532/611 [00:12<00:01, 44.16it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.17it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.02it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.01it/s] 90%|█████████ | 552/611 [00:12<00:01, 43.92it/s] 91%|█████████ | 557/611 [00:12<00:01, 43.96it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.13it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.23it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.16it/s] 94%|█████████▍| 577/611 [00:13<00:00, 44.17it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.13it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.07it/s] 97%|█████████▋| 592/611 [00:13<00:00, 43.96it/s] 98%|█████████▊| 597/611 [00:13<00:00, 43.91it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.04it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.18it/s]100%|██████████| 611/611 [00:13<00:00, 44.22it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:40:19,095 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:19,095 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:19,095 >>   eval_loss               =     0.9027
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:19,095 >>   eval_runtime            = 0:00:13.83
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:19,095 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:19,095 >>   eval_samples_per_second =    352.891
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:19,095 >>   eval_steps_per_second   =     44.166
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:40:19,095 >>   perplexity              =     2.4662
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:24,331 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:24,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:24,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:24,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:24,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:40:24,625 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:40:24,626 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:40:25,292 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:40:26,299 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:40:26,299 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:29,157 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:29,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:29,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:29,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:29,164 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:40:29,788 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:40:29,789 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:40:30,388 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:40:30,559 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:40:30,559 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-236
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-354
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-472
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:11,  1.67it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:12,  1.71it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:13,  1.76it/s]Extractor Predicting: 23it [00:13,  1.75it/s]Extractor Predicting: 24it [00:14,  1.75it/s]Extractor Predicting: 25it [00:15,  1.72it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:18,  1.72it/s]Extractor Predicting: 31it [00:18,  1.72it/s]Extractor Predicting: 32it [00:19,  1.75it/s]Extractor Predicting: 33it [00:19,  1.73it/s]Extractor Predicting: 34it [00:20,  1.71it/s]Extractor Predicting: 35it [00:20,  1.71it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:22,  1.72it/s]Extractor Predicting: 38it [00:22,  1.71it/s]Extractor Predicting: 39it [00:23,  1.69it/s]Extractor Predicting: 40it [00:23,  1.71it/s]Extractor Predicting: 41it [00:24,  1.68it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:25,  1.68it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:26,  1.71it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.69it/s]Extractor Predicting: 48it [00:28,  1.68it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:29,  1.64it/s]Extractor Predicting: 51it [00:30,  1.65it/s]Extractor Predicting: 52it [00:31,  1.69it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:32,  1.58it/s]Extractor Predicting: 56it [00:33,  1.55it/s]Extractor Predicting: 57it [00:34,  1.57it/s]Extractor Predicting: 58it [00:34,  1.57it/s]Extractor Predicting: 59it [00:35,  1.58it/s]Extractor Predicting: 60it [00:36,  1.57it/s]Extractor Predicting: 61it [00:36,  1.56it/s]Extractor Predicting: 62it [00:37,  1.53it/s]Extractor Predicting: 63it [00:38,  1.55it/s]Extractor Predicting: 64it [00:38,  1.56it/s]Extractor Predicting: 65it [00:39,  1.56it/s]Extractor Predicting: 66it [00:40,  1.53it/s]Extractor Predicting: 67it [00:40,  1.55it/s]Extractor Predicting: 68it [00:41,  1.60it/s]Extractor Predicting: 69it [00:41,  1.59it/s]Extractor Predicting: 70it [00:42,  1.56it/s]Extractor Predicting: 71it [00:43,  1.57it/s]Extractor Predicting: 72it [00:43,  1.54it/s]Extractor Predicting: 73it [00:44,  1.57it/s]Extractor Predicting: 74it [00:45,  1.57it/s]Extractor Predicting: 75it [00:45,  1.58it/s]Extractor Predicting: 76it [00:46,  1.58it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:47,  1.60it/s]Extractor Predicting: 79it [00:48,  1.62it/s]Extractor Predicting: 80it [00:48,  1.62it/s]Extractor Predicting: 81it [00:49,  1.62it/s]Extractor Predicting: 82it [00:50,  1.58it/s]Extractor Predicting: 83it [00:50,  1.59it/s]Extractor Predicting: 84it [00:51,  1.58it/s]Extractor Predicting: 85it [00:52,  1.57it/s]Extractor Predicting: 86it [00:52,  1.56it/s]Extractor Predicting: 87it [00:53,  1.56it/s]Extractor Predicting: 88it [00:54,  1.53it/s]Extractor Predicting: 89it [00:54,  1.53it/s]Extractor Predicting: 90it [00:55,  1.53it/s]Extractor Predicting: 91it [00:55,  1.53it/s]Extractor Predicting: 92it [00:56,  1.59it/s]Extractor Predicting: 93it [00:57,  1.66it/s]Extractor Predicting: 94it [00:57,  1.65it/s]Extractor Predicting: 95it [00:58,  1.65it/s]Extractor Predicting: 96it [00:58,  1.64it/s]Extractor Predicting: 97it [00:59,  1.66it/s]Extractor Predicting: 98it [01:00,  1.63it/s]Extractor Predicting: 99it [01:00,  1.56it/s]Extractor Predicting: 100it [01:01,  1.59it/s]Extractor Predicting: 101it [01:02,  1.43it/s]Extractor Predicting: 102it [01:03,  1.44it/s]Extractor Predicting: 103it [01:03,  1.48it/s]Extractor Predicting: 104it [01:04,  1.52it/s]Extractor Predicting: 105it [01:04,  1.54it/s]Extractor Predicting: 106it [01:05,  1.60it/s]Extractor Predicting: 107it [01:06,  1.60it/s]Extractor Predicting: 108it [01:06,  1.63it/s]Extractor Predicting: 109it [01:07,  1.63it/s]Extractor Predicting: 110it [01:07,  1.63it/s]Extractor Predicting: 111it [01:08,  1.66it/s]Extractor Predicting: 112it [01:09,  1.66it/s]Extractor Predicting: 113it [01:09,  1.61it/s]Extractor Predicting: 114it [01:10,  1.60it/s]Extractor Predicting: 115it [01:10,  1.62it/s]Extractor Predicting: 116it [01:11,  1.58it/s]Extractor Predicting: 117it [01:12,  1.56it/s]Extractor Predicting: 118it [01:12,  1.57it/s]Extractor Predicting: 119it [01:13,  1.55it/s]Extractor Predicting: 120it [01:14,  1.52it/s]Extractor Predicting: 121it [01:14,  1.54it/s]Extractor Predicting: 122it [01:15,  1.54it/s]Extractor Predicting: 123it [01:16,  1.57it/s]Extractor Predicting: 124it [01:16,  1.57it/s]Extractor Predicting: 125it [01:17,  1.58it/s]Extractor Predicting: 126it [01:18,  1.58it/s]Extractor Predicting: 127it [01:18,  1.58it/s]Extractor Predicting: 128it [01:19,  1.58it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:20,  1.55it/s]Extractor Predicting: 131it [01:21,  1.56it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.57it/s]Extractor Predicting: 134it [01:23,  1.58it/s]Extractor Predicting: 135it [01:23,  1.56it/s]Extractor Predicting: 136it [01:24,  1.58it/s]Extractor Predicting: 137it [01:25,  1.55it/s]Extractor Predicting: 138it [01:25,  1.57it/s]Extractor Predicting: 139it [01:26,  1.54it/s]Extractor Predicting: 140it [01:27,  1.53it/s]Extractor Predicting: 141it [01:27,  1.55it/s]Extractor Predicting: 142it [01:28,  1.56it/s]Extractor Predicting: 143it [01:28,  1.54it/s]Extractor Predicting: 144it [01:29,  1.58it/s]Extractor Predicting: 145it [01:30,  1.61it/s]Extractor Predicting: 146it [01:30,  1.59it/s]Extractor Predicting: 147it [01:31,  1.56it/s]Extractor Predicting: 148it [01:32,  1.57it/s]Extractor Predicting: 149it [01:32,  1.56it/s]Extractor Predicting: 150it [01:33,  1.54it/s]Extractor Predicting: 151it [01:34,  1.53it/s]Extractor Predicting: 152it [01:34,  1.53it/s]Extractor Predicting: 153it [01:35,  1.54it/s]Extractor Predicting: 154it [01:36,  1.54it/s]Extractor Predicting: 155it [01:36,  1.54it/s]Extractor Predicting: 156it [01:37,  1.49it/s]Extractor Predicting: 157it [01:38,  1.45it/s]Extractor Predicting: 158it [01:38,  1.42it/s]Extractor Predicting: 159it [01:39,  1.45it/s]Extractor Predicting: 160it [01:40,  1.48it/s]Extractor Predicting: 161it [01:40,  1.50it/s]Extractor Predicting: 162it [01:41,  1.50it/s]Extractor Predicting: 163it [01:42,  1.50it/s]Extractor Predicting: 164it [01:42,  1.52it/s]Extractor Predicting: 165it [01:43,  1.52it/s]Extractor Predicting: 166it [01:44,  1.55it/s]Extractor Predicting: 167it [01:44,  1.53it/s]Extractor Predicting: 168it [01:45,  1.53it/s]Extractor Predicting: 169it [01:46,  1.51it/s]Extractor Predicting: 170it [01:46,  1.52it/s]Extractor Predicting: 171it [01:47,  1.55it/s]Extractor Predicting: 172it [01:47,  1.57it/s]Extractor Predicting: 173it [01:48,  1.53it/s]Extractor Predicting: 174it [01:49,  1.54it/s]Extractor Predicting: 175it [01:49,  1.51it/s]Extractor Predicting: 176it [01:50,  1.54it/s]Extractor Predicting: 177it [01:51,  1.52it/s]Extractor Predicting: 178it [01:51,  1.53it/s]Extractor Predicting: 179it [01:52,  1.53it/s]Extractor Predicting: 180it [01:53,  1.53it/s]Extractor Predicting: 181it [01:53,  1.54it/s]Extractor Predicting: 182it [01:54,  1.54it/s]Extractor Predicting: 183it [01:55,  1.49it/s]Extractor Predicting: 184it [01:55,  1.51it/s]Extractor Predicting: 185it [01:56,  1.64it/s]Extractor Predicting: 185it [01:56,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,007 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:42:35,322 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:42:35,323 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:42:35,578 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:42:36,632 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:42:36,632 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:39,466 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:39,470 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:39,470 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:39,470 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:39,470 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:42:40,084 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:42:40,085 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:42:40,658 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:42:40,828 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:42:40,828 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5025125628140703,
  "recall": 0.020483408439164276,
  "score": 0.03936233024995079,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:12,  1.57it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.47it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:15,  1.55it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:19,  1.72it/s]Extractor Predicting: 32it [00:19,  1.72it/s]Extractor Predicting: 33it [00:20,  1.71it/s]Extractor Predicting: 34it [00:20,  1.71it/s]Extractor Predicting: 35it [00:21,  1.71it/s]Extractor Predicting: 36it [00:22,  1.70it/s]Extractor Predicting: 37it [00:22,  1.72it/s]Extractor Predicting: 38it [00:23,  1.71it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:25,  1.67it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:26,  1.69it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:27,  1.73it/s]Extractor Predicting: 46it [00:28,  1.75it/s]Extractor Predicting: 47it [00:28,  1.75it/s]Extractor Predicting: 48it [00:29,  1.75it/s]Extractor Predicting: 49it [00:29,  1.73it/s]Extractor Predicting: 50it [00:30,  1.71it/s]Extractor Predicting: 51it [00:30,  1.73it/s]Extractor Predicting: 52it [00:31,  1.73it/s]Extractor Predicting: 53it [00:32,  1.70it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.65it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.67it/s]Extractor Predicting: 58it [00:35,  1.73it/s]Extractor Predicting: 59it [00:35,  1.69it/s]Extractor Predicting: 60it [00:36,  1.66it/s]Extractor Predicting: 61it [00:36,  1.62it/s]Extractor Predicting: 62it [00:37,  1.59it/s]Extractor Predicting: 63it [00:38,  1.55it/s]Extractor Predicting: 64it [00:38,  1.53it/s]Extractor Predicting: 65it [00:39,  1.52it/s]Extractor Predicting: 66it [00:40,  1.51it/s]Extractor Predicting: 67it [00:40,  1.50it/s]Extractor Predicting: 68it [00:41,  1.52it/s]Extractor Predicting: 69it [00:42,  1.53it/s]Extractor Predicting: 70it [00:42,  1.55it/s]Extractor Predicting: 71it [00:43,  1.56it/s]Extractor Predicting: 72it [00:44,  1.58it/s]Extractor Predicting: 73it [00:44,  1.63it/s]Extractor Predicting: 74it [00:45,  1.63it/s]Extractor Predicting: 75it [00:45,  1.64it/s]Extractor Predicting: 76it [00:46,  1.65it/s]Extractor Predicting: 77it [00:47,  1.67it/s]Extractor Predicting: 78it [00:47,  1.65it/s]Extractor Predicting: 79it [00:48,  1.70it/s]Extractor Predicting: 80it [00:48,  1.74it/s]Extractor Predicting: 81it [00:49,  1.69it/s]Extractor Predicting: 82it [00:50,  1.71it/s]Extractor Predicting: 83it [00:50,  1.66it/s]Extractor Predicting: 84it [00:51,  1.64it/s]Extractor Predicting: 85it [00:51,  1.60it/s]Extractor Predicting: 86it [00:52,  1.58it/s]Extractor Predicting: 87it [00:53,  1.58it/s]Extractor Predicting: 88it [00:53,  1.60it/s]Extractor Predicting: 89it [00:54,  1.58it/s]Extractor Predicting: 90it [00:55,  1.60it/s]Extractor Predicting: 91it [00:55,  1.59it/s]Extractor Predicting: 92it [00:56,  1.59it/s]Extractor Predicting: 93it [00:56,  1.61it/s]Extractor Predicting: 94it [00:57,  1.62it/s]Extractor Predicting: 95it [00:58,  1.61it/s]Extractor Predicting: 96it [00:58,  1.60it/s]Extractor Predicting: 97it [00:59,  1.62it/s]Extractor Predicting: 98it [01:00,  1.61it/s]Extractor Predicting: 99it [01:00,  1.61it/s]Extractor Predicting: 100it [01:01,  1.59it/s]Extractor Predicting: 101it [01:01,  1.61it/s]Extractor Predicting: 102it [01:02,  1.62it/s]Extractor Predicting: 103it [01:03,  1.58it/s]Extractor Predicting: 104it [01:03,  1.61it/s]Extractor Predicting: 105it [01:04,  1.62it/s]Extractor Predicting: 106it [01:05,  1.64it/s]Extractor Predicting: 107it [01:05,  1.63it/s]Extractor Predicting: 108it [01:06,  1.65it/s]Extractor Predicting: 109it [01:06,  1.63it/s]Extractor Predicting: 110it [01:07,  1.65it/s]Extractor Predicting: 111it [01:08,  1.65it/s]Extractor Predicting: 112it [01:08,  1.62it/s]Extractor Predicting: 113it [01:09,  1.61it/s]Extractor Predicting: 114it [01:09,  1.60it/s]Extractor Predicting: 115it [01:10,  1.59it/s]Extractor Predicting: 116it [01:11,  1.59it/s]Extractor Predicting: 117it [01:11,  1.63it/s]Extractor Predicting: 118it [01:12,  1.61it/s]Extractor Predicting: 119it [01:13,  1.60it/s]Extractor Predicting: 120it [01:13,  1.62it/s]Extractor Predicting: 121it [01:14,  1.66it/s]Extractor Predicting: 122it [01:14,  1.65it/s]Extractor Predicting: 123it [01:15,  1.62it/s]Extractor Predicting: 124it [01:16,  1.59it/s]Extractor Predicting: 125it [01:16,  1.60it/s]Extractor Predicting: 126it [01:17,  1.58it/s]Extractor Predicting: 127it [01:18,  1.62it/s]Extractor Predicting: 128it [01:18,  1.56it/s]Extractor Predicting: 129it [01:19,  1.58it/s]Extractor Predicting: 130it [01:19,  1.56it/s]Extractor Predicting: 131it [01:20,  1.37it/s]Extractor Predicting: 132it [01:21,  1.42it/s]Extractor Predicting: 133it [01:22,  1.46it/s]Extractor Predicting: 134it [01:22,  1.49it/s]Extractor Predicting: 135it [01:23,  1.52it/s]Extractor Predicting: 136it [01:24,  1.57it/s]Extractor Predicting: 137it [01:24,  1.53it/s]Extractor Predicting: 138it [01:25,  1.56it/s]Extractor Predicting: 139it [01:25,  1.58it/s]Extractor Predicting: 140it [01:26,  1.59it/s]Extractor Predicting: 141it [01:27,  1.58it/s]Extractor Predicting: 142it [01:27,  1.61it/s]Extractor Predicting: 143it [01:28,  1.52it/s]Extractor Predicting: 144it [01:29,  1.59it/s]Extractor Predicting: 145it [01:29,  1.56it/s]Extractor Predicting: 146it [01:30,  1.59it/s]Extractor Predicting: 147it [01:30,  1.64it/s]Extractor Predicting: 148it [01:31,  1.61it/s]Extractor Predicting: 149it [01:32,  1.65it/s]Extractor Predicting: 150it [01:32,  1.65it/s]Extractor Predicting: 151it [01:33,  1.67it/s]Extractor Predicting: 152it [01:34,  1.63it/s]Extractor Predicting: 153it [01:34,  1.63it/s]Extractor Predicting: 154it [01:35,  1.59it/s]Extractor Predicting: 155it [01:35,  1.59it/s]Extractor Predicting: 156it [01:36,  1.64it/s]Extractor Predicting: 157it [01:37,  1.60it/s]Extractor Predicting: 158it [01:37,  1.58it/s]Extractor Predicting: 159it [01:38,  1.60it/s]Extractor Predicting: 160it [01:39,  1.61it/s]Extractor Predicting: 161it [01:39,  1.64it/s]Extractor Predicting: 162it [01:40,  1.62it/s]Extractor Predicting: 163it [01:40,  1.61it/s]Extractor Predicting: 164it [01:41,  1.60it/s]Extractor Predicting: 165it [01:42,  1.56it/s]Extractor Predicting: 166it [01:42,  1.54it/s]Extractor Predicting: 167it [01:43,  1.54it/s]Extractor Predicting: 168it [01:44,  1.56it/s]Extractor Predicting: 169it [01:44,  1.59it/s]Extractor Predicting: 170it [01:45,  1.61it/s]Extractor Predicting: 171it [01:45,  1.62it/s]Extractor Predicting: 172it [01:46,  1.65it/s]Extractor Predicting: 173it [01:47,  1.64it/s]Extractor Predicting: 174it [01:47,  1.63it/s]Extractor Predicting: 175it [01:48,  1.63it/s]Extractor Predicting: 176it [01:48,  1.63it/s]Extractor Predicting: 177it [01:49,  1.61it/s]Extractor Predicting: 178it [01:50,  1.60it/s]Extractor Predicting: 179it [01:50,  1.57it/s]Extractor Predicting: 180it [01:51,  1.61it/s]Extractor Predicting: 181it [01:52,  1.61it/s]Extractor Predicting: 182it [01:52,  1.63it/s]Extractor Predicting: 183it [01:53,  1.66it/s]Extractor Predicting: 184it [01:53,  1.64it/s]Extractor Predicting: 185it [01:54,  1.64it/s]Extractor Predicting: 186it [01:55,  1.59it/s]Extractor Predicting: 187it [01:55,  1.62it/s]Extractor Predicting: 188it [01:56,  1.62it/s]Extractor Predicting: 189it [01:57,  1.61it/s]Extractor Predicting: 190it [01:57,  1.62it/s]Extractor Predicting: 191it [01:58,  1.62it/s]Extractor Predicting: 192it [01:58,  1.67it/s]Extractor Predicting: 193it [01:59,  1.65it/s]Extractor Predicting: 194it [02:00,  1.65it/s]Extractor Predicting: 195it [02:00,  1.63it/s]Extractor Predicting: 196it [02:01,  1.63it/s]Extractor Predicting: 197it [02:01,  1.62it/s]Extractor Predicting: 198it [02:02,  1.61it/s]Extractor Predicting: 199it [02:03,  1.60it/s]Extractor Predicting: 200it [02:03,  1.61it/s]Extractor Predicting: 201it [02:04,  1.63it/s]Extractor Predicting: 202it [02:05,  1.63it/s]Extractor Predicting: 203it [02:05,  1.65it/s]Extractor Predicting: 204it [02:06,  1.64it/s]Extractor Predicting: 205it [02:06,  1.65it/s]Extractor Predicting: 206it [02:07,  1.63it/s]Extractor Predicting: 207it [02:08,  1.65it/s]Extractor Predicting: 208it [02:08,  1.62it/s]Extractor Predicting: 209it [02:09,  1.59it/s]Extractor Predicting: 210it [02:09,  1.65it/s]Extractor Predicting: 211it [02:10,  1.63it/s]Extractor Predicting: 212it [02:11,  1.64it/s]Extractor Predicting: 213it [02:11,  1.65it/s]Extractor Predicting: 214it [02:12,  1.68it/s]Extractor Predicting: 215it [02:12,  1.66it/s]Extractor Predicting: 216it [02:13,  1.63it/s]Extractor Predicting: 217it [02:14,  1.64it/s]Extractor Predicting: 218it [02:14,  1.62it/s]Extractor Predicting: 219it [02:15,  1.62it/s]Extractor Predicting: 220it [02:15,  1.63it/s]Extractor Predicting: 221it [02:16,  1.64it/s]Extractor Predicting: 222it [02:17,  1.59it/s]Extractor Predicting: 223it [02:17,  1.54it/s]Extractor Predicting: 224it [02:18,  1.56it/s]Extractor Predicting: 225it [02:19,  1.59it/s]Extractor Predicting: 226it [02:19,  1.62it/s]Extractor Predicting: 227it [02:20,  1.62it/s]Extractor Predicting: 228it [02:20,  1.65it/s]Extractor Predicting: 229it [02:21,  1.67it/s]Extractor Predicting: 230it [02:22,  1.69it/s]Extractor Predicting: 231it [02:22,  1.69it/s]Extractor Predicting: 232it [02:23,  1.66it/s]Extractor Predicting: 233it [02:23,  1.66it/s]Extractor Predicting: 234it [02:24,  1.66it/s]Extractor Predicting: 235it [02:25,  1.63it/s]Extractor Predicting: 236it [02:25,  1.63it/s]Extractor Predicting: 237it [02:26,  1.64it/s]Extractor Predicting: 238it [02:27,  1.60it/s]Extractor Predicting: 239it [02:27,  1.63it/s]Extractor Predicting: 240it [02:28,  1.62it/s]Extractor Predicting: 241it [02:28,  1.63it/s]Extractor Predicting: 242it [02:29,  1.59it/s]Extractor Predicting: 243it [02:30,  1.50it/s]Extractor Predicting: 244it [02:30,  1.53it/s]Extractor Predicting: 245it [02:31,  1.41it/s]Extractor Predicting: 246it [02:32,  1.46it/s]Extractor Predicting: 247it [02:32,  1.53it/s]Extractor Predicting: 248it [02:33,  1.51it/s]Extractor Predicting: 249it [02:34,  1.53it/s]Extractor Predicting: 250it [02:34,  1.56it/s]Extractor Predicting: 251it [02:35,  1.57it/s]Extractor Predicting: 252it [02:36,  1.57it/s]Extractor Predicting: 253it [02:36,  1.54it/s]Extractor Predicting: 254it [02:37,  1.53it/s]Extractor Predicting: 255it [02:38,  1.55it/s]Extractor Predicting: 256it [02:38,  1.56it/s]Extractor Predicting: 257it [02:39,  1.58it/s]Extractor Predicting: 258it [02:40,  1.58it/s]Extractor Predicting: 259it [02:40,  1.57it/s]Extractor Predicting: 260it [02:41,  1.55it/s]Extractor Predicting: 261it [02:41,  1.57it/s]Extractor Predicting: 262it [02:42,  1.56it/s]Extractor Predicting: 263it [02:43,  1.57it/s]Extractor Predicting: 264it [02:43,  1.57it/s]Extractor Predicting: 265it [02:44,  1.59it/s]Extractor Predicting: 266it [02:45,  1.54it/s]Extractor Predicting: 267it [02:45,  1.54it/s]Extractor Predicting: 268it [02:46,  1.53it/s]Extractor Predicting: 269it [02:47,  1.55it/s]Extractor Predicting: 270it [02:47,  1.53it/s]Extractor Predicting: 271it [02:48,  1.53it/s]Extractor Predicting: 272it [02:49,  1.53it/s]Extractor Predicting: 273it [02:49,  1.55it/s]Extractor Predicting: 274it [02:50,  1.51it/s]Extractor Predicting: 275it [02:51,  1.53it/s]Extractor Predicting: 276it [02:51,  1.54it/s]Extractor Predicting: 277it [02:52,  1.55it/s]Extractor Predicting: 278it [02:52,  1.57it/s]Extractor Predicting: 279it [02:53,  1.55it/s]Extractor Predicting: 280it [02:54,  1.56it/s]Extractor Predicting: 281it [02:54,  1.52it/s]Extractor Predicting: 282it [02:55,  1.52it/s]Extractor Predicting: 283it [02:56,  1.54it/s]Extractor Predicting: 284it [02:56,  1.56it/s]Extractor Predicting: 285it [02:57,  1.56it/s]Extractor Predicting: 286it [02:58,  1.58it/s]Extractor Predicting: 287it [02:58,  1.53it/s]Extractor Predicting: 288it [02:59,  1.57it/s]Extractor Predicting: 289it [02:59,  1.59it/s]Extractor Predicting: 290it [03:00,  1.54it/s]Extractor Predicting: 291it [03:01,  1.52it/s]Extractor Predicting: 292it [03:01,  1.55it/s]Extractor Predicting: 293it [03:02,  1.55it/s]Extractor Predicting: 294it [03:03,  1.53it/s]Extractor Predicting: 295it [03:03,  1.53it/s]Extractor Predicting: 296it [03:04,  1.57it/s]Extractor Predicting: 297it [03:05,  1.58it/s]Extractor Predicting: 298it [03:05,  1.56it/s]Extractor Predicting: 299it [03:06,  1.56it/s]Extractor Predicting: 300it [03:07,  1.55it/s]Extractor Predicting: 301it [03:07,  1.57it/s]Extractor Predicting: 302it [03:08,  1.54it/s]Extractor Predicting: 303it [03:09,  1.57it/s]Extractor Predicting: 304it [03:09,  1.58it/s]Extractor Predicting: 305it [03:10,  1.61it/s]Extractor Predicting: 306it [03:10,  1.64it/s]Extractor Predicting: 307it [03:11,  1.61it/s]Extractor Predicting: 308it [03:12,  1.62it/s]Extractor Predicting: 309it [03:12,  1.59it/s]Extractor Predicting: 310it [03:13,  1.59it/s]Extractor Predicting: 311it [03:14,  1.55it/s]Extractor Predicting: 312it [03:14,  1.57it/s]Extractor Predicting: 313it [03:15,  1.51it/s]Extractor Predicting: 314it [03:16,  1.52it/s]Extractor Predicting: 315it [03:16,  1.55it/s]Extractor Predicting: 316it [03:17,  1.58it/s]Extractor Predicting: 317it [03:17,  1.60it/s]Extractor Predicting: 318it [03:18,  1.63it/s]Extractor Predicting: 319it [03:19,  1.60it/s]Extractor Predicting: 320it [03:19,  1.57it/s]Extractor Predicting: 321it [03:20,  1.57it/s]Extractor Predicting: 322it [03:21,  1.59it/s]Extractor Predicting: 323it [03:21,  1.54it/s]Extractor Predicting: 324it [03:22,  1.55it/s]Extractor Predicting: 325it [03:22,  1.58it/s]Extractor Predicting: 326it [03:23,  1.58it/s]Extractor Predicting: 327it [03:24,  1.58it/s]Extractor Predicting: 328it [03:24,  1.55it/s]Extractor Predicting: 329it [03:25,  1.56it/s]Extractor Predicting: 330it [03:26,  1.55it/s]Extractor Predicting: 331it [03:26,  1.56it/s]Extractor Predicting: 332it [03:27,  1.56it/s]Extractor Predicting: 333it [03:28,  1.54it/s]Extractor Predicting: 334it [03:28,  1.56it/s]Extractor Predicting: 335it [03:29,  1.55it/s]Extractor Predicting: 336it [03:30,  1.47it/s]Extractor Predicting: 337it [03:30,  1.48it/s]Extractor Predicting: 338it [03:31,  1.49it/s]Extractor Predicting: 339it [03:32,  1.51it/s]Extractor Predicting: 340it [03:32,  1.52it/s]Extractor Predicting: 341it [03:33,  1.51it/s]Extractor Predicting: 342it [03:34,  1.51it/s]Extractor Predicting: 343it [03:34,  1.54it/s]Extractor Predicting: 344it [03:35,  1.54it/s]Extractor Predicting: 345it [03:36,  1.49it/s]Extractor Predicting: 346it [03:36,  1.50it/s]Extractor Predicting: 347it [03:37,  1.52it/s]Extractor Predicting: 348it [03:38,  1.37it/s]Extractor Predicting: 349it [03:38,  1.42it/s]Extractor Predicting: 350it [03:39,  1.45it/s]Extractor Predicting: 351it [03:40,  1.50it/s]Extractor Predicting: 352it [03:40,  1.48it/s]Extractor Predicting: 353it [03:41,  1.47it/s]Extractor Predicting: 354it [03:42,  1.52it/s]Extractor Predicting: 355it [03:42,  1.55it/s]Extractor Predicting: 356it [03:43,  1.55it/s]Extractor Predicting: 357it [03:44,  1.55it/s]Extractor Predicting: 358it [03:44,  1.56it/s]Extractor Predicting: 359it [03:45,  1.56it/s]Extractor Predicting: 360it [03:46,  1.53it/s]Extractor Predicting: 361it [03:46,  1.52it/s]Extractor Predicting: 362it [03:47,  1.54it/s]Extractor Predicting: 363it [03:47,  1.58it/s]Extractor Predicting: 364it [03:48,  1.60it/s]Extractor Predicting: 365it [03:49,  1.57it/s]Extractor Predicting: 366it [03:49,  1.52it/s]Extractor Predicting: 367it [03:50,  1.52it/s]Extractor Predicting: 368it [03:51,  1.51it/s]Extractor Predicting: 369it [03:51,  1.46it/s]Extractor Predicting: 370it [03:52,  1.47it/s]Extractor Predicting: 371it [03:53,  1.49it/s]Extractor Predicting: 372it [03:53,  1.78it/s]Extractor Predicting: 372it [03:53,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:43,497 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:43,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:43,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:43,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:43,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:46:44,113 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:46:44,114 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:46:44,702 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:46:45,757 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:46:45,758 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:48,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:48,567 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:48,567 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:48,567 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:46:48,568 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:46:49,208 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:46:49,209 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:46:49,766 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:46:49,944 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:46:49,944 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.28451882845188287,
  "recall": 0.02289048473967684,
  "score": 0.04237200124623532,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.52it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:16,  1.51it/s]Extractor Predicting: 28it [00:17,  1.50it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:20,  1.58it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.50it/s]Extractor Predicting: 39it [00:24,  1.47it/s]Extractor Predicting: 40it [00:25,  1.47it/s]Extractor Predicting: 41it [00:26,  1.47it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.49it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:28,  1.48it/s]Extractor Predicting: 46it [00:29,  1.42it/s]Extractor Predicting: 47it [00:30,  1.43it/s]Extractor Predicting: 48it [00:30,  1.44it/s]Extractor Predicting: 49it [00:31,  1.46it/s]Extractor Predicting: 50it [00:32,  1.46it/s]Extractor Predicting: 51it [00:32,  1.44it/s]Extractor Predicting: 52it [00:33,  1.45it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:35,  1.45it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:36,  1.49it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:38,  1.47it/s]Extractor Predicting: 60it [00:39,  1.47it/s]Extractor Predicting: 61it [00:39,  1.44it/s]Extractor Predicting: 62it [00:40,  1.44it/s]Extractor Predicting: 63it [00:40,  1.59it/s]Extractor Predicting: 63it [00:40,  1.54it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:47:31,891 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:47:31,892 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:47:31,895 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:47:31,896 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:47:31,899 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:47:34,933 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:47:34,933 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:47:34,946 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:47:34,947 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:47:34,951 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:47:34,955 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:47:34,955 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:47:34,956 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:47:34,956 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:47:34,956 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:47:34,956 >> loading file outputs/wrapper/wiki/unseen_10_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5882352941176471,
  "recall": 0.014983518130056937,
  "score": 0.02922267679719462,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:47:35,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:35,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:36,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:36,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:37,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:38,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:38,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:39,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:39,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:40,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:41,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:41,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:42,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:43,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:43,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:44,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:44,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:45,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:46,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:46,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:47,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:48,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:48,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:49,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:50,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:50,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:51,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:53, 16.65s/it][WARNING|generation_utils.py:914] 2023-08-28 14:47:51,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:52,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:52,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:53,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:54,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:54,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:55,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:56,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:56,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:57,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:57,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:58,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:58,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:59,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:59,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:00,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:00,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:01,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:01,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:02,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:03,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:03,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:04,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:06, 14.38s/it][WARNING|generation_utils.py:914] 2023-08-28 14:48:04,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:05,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:05,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:06,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:06,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:07,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:07,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:08,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:08,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:09,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:10,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:10,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:11,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:11,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:12,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:12,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:13,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:13,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:14,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:14,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:31, 12.62s/it][WARNING|generation_utils.py:914] 2023-08-28 14:48:15,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:15,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:16,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:16,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:17,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:18,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:18,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:19,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:19,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:20,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:21,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:21,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:22,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:23,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:24,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:24,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:25,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:25,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:26,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:27,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:27,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:28,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:28,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:54<02:25, 13.22s/it][WARNING|generation_utils.py:914] 2023-08-28 14:48:29,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:29,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:30,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:31,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:31,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:32,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:32,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:33,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:33,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:34,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:35,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:35,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:36,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:37,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:37,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:38,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:39,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:39,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:40,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:40,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:41,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:42,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:42,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:43,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:08<02:17, 13.73s/it][WARNING|generation_utils.py:914] 2023-08-28 14:48:43,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:44,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:45,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:45,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:46,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:47,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:47,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:48,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:48,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:49,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:50,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:50,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:51,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:52,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:53,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:53,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:54,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:54,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:55,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:55,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:56,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:57,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:57,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:23<02:05, 13.95s/it][WARNING|generation_utils.py:914] 2023-08-28 14:48:58,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:58,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:48:59,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:00,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:00,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:01,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:01,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:02,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:03,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:03,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:04,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:04,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:05,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:06,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:06,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:07,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:07,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:08,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:08,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:09,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:10,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:35<01:47, 13.44s/it][WARNING|generation_utils.py:914] 2023-08-28 14:49:10,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:11,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:11,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:12,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:13,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:13,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:14,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:14,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:15,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:16,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:16,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:17,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:17,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:18,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:18,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:19,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:20,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:20,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:21,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:21,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:22,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:23,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:48<01:32, 13.28s/it][WARNING|generation_utils.py:914] 2023-08-28 14:49:23,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:24,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:24,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:25,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:26,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:26,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:27,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:27,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:28,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:29,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:29,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:30,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:30,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:31,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:32,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:32,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:33,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:34,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:34,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:35,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:36,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:36,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:37,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:38,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:38,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:04<01:24, 14.04s/it][WARNING|generation_utils.py:914] 2023-08-28 14:49:39,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:39,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:40,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:41,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:41,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:42,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:42,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:43,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:44,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:44,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:45,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:45,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:46,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:46,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:47,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:48,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:48,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:49,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:49,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:50,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:51,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:51,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:17<01:08, 13.72s/it][WARNING|generation_utils.py:914] 2023-08-28 14:49:52,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:52,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:53,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:53,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:54,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:54,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:55,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:56,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:56,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:57,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:57,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:57,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:58,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:59,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:49:59,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:00,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:00,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:01,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:01,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:02,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:02,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:27<00:51, 12.83s/it][WARNING|generation_utils.py:914] 2023-08-28 14:50:03,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:03,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:04,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:05,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:05,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:06,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:06,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:07,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:07,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:08,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:09,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:09,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:10,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:10,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:11,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:12,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:12,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:13,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:14,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:14,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:15,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:15,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:16,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:16,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:17,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:18,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:43<00:41, 13.68s/it][WARNING|generation_utils.py:914] 2023-08-28 14:50:18,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:19,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:20,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:20,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:21,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:22,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:22,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:23,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:24,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:24,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:25,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:26,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:26,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:27,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:28,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:28,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:29,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:30,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:30,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:31,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:31,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:32,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:33,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:33,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:59<00:28, 14.26s/it][WARNING|generation_utils.py:914] 2023-08-28 14:50:34,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:34,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:35,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:35,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:36,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:37,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:37,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:38,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:38,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:39,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:39,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:40,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:40,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:41,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:42,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:42,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:43,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:43,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:44,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:44,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:09<00:13, 13.19s/it][WARNING|generation_utils.py:914] 2023-08-28 14:50:45,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:45,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:46,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:46,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:47,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:48,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:48,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:49,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:50,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:50,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:51,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:52,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:52,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:53,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:53,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:54,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:55,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:55,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:56,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:57,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:57,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:58,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:50:59,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:24<00:00, 13.70s/it]Generating: 100%|██████████| 15/15 [03:24<00:00, 13.65s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:06,523 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:06,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:06,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:06,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:06,527 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:51:07,125 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:51:07,126 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:51:07,694 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:51:08,772 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:51:08,772 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:11,604 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:11,608 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:11,608 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:11,608 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:51:11,608 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:51:12,237 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:51:12,238 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:51:12,800 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:51:12,971 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:51:12,971 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 227, 'raw': 320}
{'target': 600, 'success': 248, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 294, 'raw': 416}
{'target': 600, 'success': 319, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 366, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 414, 'raw': 576}
{'target': 600, 'success': 432, 'raw': 608}
{'target': 600, 'success': 452, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 518, 'raw': 736}
{'target': 600, 'success': 541, 'raw': 768}
{'target': 600, 'success': 561, 'raw': 800}
{'target': 600, 'success': 582, 'raw': 832}
{'target': 600, 'success': 608, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7037037037037037, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : developer .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9765625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : composer .', 'success_rate': 0.8519021739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : country of citizenship . Context : Peter Cottler ( born 8 September 1974 , Glamorgan , County Down , England ) is a former Welsh cricket er . Head Entity : Peter Cottler , Tail Entity : Wales .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8536931818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best New Artist at the 2004 MTV Video Music Awards , alongside artists such as Rick Rubin , Rick Rubin III , and Tim McGraw . Head Entity : Rick Rubin , Tail Entity : music .\n']
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best New Artist at the 2004 MTV Video Music Awards , alongside artists such as Rick Rubin , Rick Rubin III , and Tim McGraw . Head Entity : Rick Rubin , Tail Entity : music .\n', 'Relation : field of work . Context : Eileen McClelland ( born June 24 , 1946 ) is a retired Pennsylvania State University professor specializing in the field of health care . Head Entity : McClelland , Tail Entity : biomedical .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 393, 'raw': 512}
{'target': 600, 'success': 419, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 520, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 592, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7675, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 445, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 531, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 575, 'raw': 768}
{'target': 600, 'success': 596, 'raw': 800}
{'target': 600, 'success': 618, 'raw': 832}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7427884615384616, 'errors': {'', "('The New York Museum of Art', 'occupation', '', 'His paintings and works are in the collections of The New York Museum of Art and the Metropolitan Museum of Art , New York .')", 'too many values to unpack (expected 2)', "('John M. Stegner', 'occupation', '', 'In 1943 he was nominated for the Medal of Honor by Colonel John M. Stegner , and had a brief part in the Battle of Midway .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.80859375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.959375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('bishop', 'twinned administrative body', '', 'Cesar de Jesus de Avilaire ( ; died 959 ) was an ecclesiastical bishop ( c.')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 11997
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12097, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.61it/s]Extractor Estimating: 2it [00:01,  1.57it/s]Extractor Estimating: 3it [00:01,  1.68it/s]Extractor Estimating: 4it [00:02,  1.71it/s]Extractor Estimating: 5it [00:02,  1.72it/s]Extractor Estimating: 6it [00:03,  1.69it/s]Extractor Estimating: 7it [00:04,  1.69it/s]Extractor Estimating: 8it [00:04,  1.75it/s]Extractor Estimating: 9it [00:05,  1.71it/s]Extractor Estimating: 10it [00:05,  1.69it/s]Extractor Estimating: 11it [00:06,  1.69it/s]Extractor Estimating: 12it [00:07,  1.72it/s]Extractor Estimating: 13it [00:07,  1.71it/s]Extractor Estimating: 14it [00:08,  1.67it/s]Extractor Estimating: 15it [00:08,  1.67it/s]Extractor Estimating: 16it [00:09,  1.61it/s]Extractor Estimating: 17it [00:10,  1.64it/s]Extractor Estimating: 18it [00:10,  1.64it/s]Extractor Estimating: 19it [00:11,  1.60it/s]Extractor Estimating: 20it [00:11,  1.64it/s]Extractor Estimating: 21it [00:12,  1.67it/s]Extractor Estimating: 22it [00:13,  1.68it/s]Extractor Estimating: 23it [00:13,  1.66it/s]Extractor Estimating: 24it [00:14,  1.58it/s]Extractor Estimating: 25it [00:15,  1.60it/s]Extractor Estimating: 26it [00:15,  1.62it/s]Extractor Estimating: 27it [00:16,  1.65it/s]Extractor Estimating: 28it [00:16,  1.67it/s]Extractor Estimating: 29it [00:17,  1.68it/s]Extractor Estimating: 30it [00:17,  1.76it/s]Extractor Estimating: 31it [00:18,  1.83it/s]Extractor Estimating: 32it [00:18,  1.80it/s]Extractor Estimating: 33it [00:19,  1.78it/s]Extractor Estimating: 34it [00:20,  1.74it/s]Extractor Estimating: 35it [00:20,  1.79it/s]Extractor Estimating: 36it [00:21,  1.87it/s]Extractor Estimating: 37it [00:21,  1.89it/s]Extractor Estimating: 38it [00:22,  1.82it/s]Extractor Estimating: 39it [00:22,  1.76it/s]Extractor Estimating: 40it [00:23,  1.79it/s]Extractor Estimating: 41it [00:23,  1.84it/s]Extractor Estimating: 42it [00:24,  1.80it/s]Extractor Estimating: 43it [00:25,  1.78it/s]Extractor Estimating: 44it [00:25,  1.75it/s]Extractor Estimating: 45it [00:26,  1.80it/s]Extractor Estimating: 46it [00:26,  1.78it/s]Extractor Estimating: 47it [00:27,  1.77it/s]Extractor Estimating: 48it [00:27,  1.75it/s]Extractor Estimating: 49it [00:28,  1.74it/s]Extractor Estimating: 50it [00:29,  1.78it/s]Extractor Estimating: 51it [00:29,  1.83it/s]Extractor Estimating: 52it [00:30,  1.93it/s]Extractor Estimating: 53it [00:30,  2.00it/s]Extractor Estimating: 54it [00:30,  2.10it/s]Extractor Estimating: 55it [00:31,  2.07it/s]Extractor Estimating: 56it [00:31,  2.05it/s]Extractor Estimating: 57it [00:32,  2.12it/s]Extractor Estimating: 58it [00:32,  2.04it/s]Extractor Estimating: 59it [00:33,  2.11it/s]Extractor Estimating: 60it [00:33,  2.09it/s]Extractor Estimating: 61it [00:34,  2.09it/s]Extractor Estimating: 62it [00:34,  2.07it/s]Extractor Estimating: 63it [00:35,  2.09it/s]Extractor Estimating: 64it [00:35,  2.10it/s]Extractor Estimating: 65it [00:36,  2.03it/s]Extractor Estimating: 66it [00:36,  2.11it/s]Extractor Estimating: 67it [00:37,  2.12it/s]Extractor Estimating: 68it [00:37,  2.12it/s]Extractor Estimating: 69it [00:38,  2.14it/s]Extractor Estimating: 70it [00:38,  2.19it/s]Extractor Estimating: 71it [00:38,  2.18it/s]Extractor Estimating: 72it [00:39,  2.25it/s]Extractor Estimating: 73it [00:39,  2.16it/s]Extractor Estimating: 74it [00:40,  2.19it/s]Extractor Estimating: 75it [00:40,  2.18it/s]Extractor Estimating: 76it [00:41,  2.00it/s]Extractor Estimating: 77it [00:41,  1.91it/s]Extractor Estimating: 78it [00:42,  1.84it/s]Extractor Estimating: 79it [00:43,  1.76it/s]Extractor Estimating: 80it [00:43,  1.78it/s]Extractor Estimating: 81it [00:44,  1.76it/s]Extractor Estimating: 82it [00:44,  1.72it/s]Extractor Estimating: 83it [00:45,  1.73it/s]Extractor Estimating: 84it [00:46,  1.71it/s]Extractor Estimating: 85it [00:46,  1.75it/s]Extractor Estimating: 86it [00:47,  1.70it/s]Extractor Estimating: 87it [00:47,  1.70it/s]Extractor Estimating: 88it [00:48,  1.70it/s]Extractor Estimating: 89it [00:49,  1.68it/s]Extractor Estimating: 90it [00:49,  1.56it/s]Extractor Estimating: 91it [00:50,  1.60it/s]Extractor Estimating: 92it [00:50,  1.64it/s]Extractor Estimating: 93it [00:51,  1.64it/s]Extractor Estimating: 94it [00:52,  1.61it/s]Extractor Estimating: 95it [00:52,  1.63it/s]Extractor Estimating: 96it [00:53,  1.60it/s]Extractor Estimating: 97it [00:54,  1.65it/s]Extractor Estimating: 98it [00:54,  1.66it/s]Extractor Estimating: 99it [00:55,  1.65it/s]Extractor Estimating: 100it [00:55,  1.69it/s]Extractor Estimating: 101it [00:56,  1.72it/s]Extractor Estimating: 102it [00:56,  1.68it/s]Extractor Estimating: 103it [00:57,  1.51it/s]Extractor Estimating: 104it [00:58,  1.58it/s]Extractor Estimating: 105it [00:58,  1.63it/s]Extractor Estimating: 106it [00:59,  1.65it/s]Extractor Estimating: 107it [01:00,  1.70it/s]Extractor Estimating: 108it [01:00,  1.68it/s]Extractor Estimating: 109it [01:01,  1.69it/s]Extractor Estimating: 110it [01:01,  1.64it/s]Extractor Estimating: 111it [01:02,  1.64it/s]Extractor Estimating: 112it [01:03,  1.60it/s]Extractor Estimating: 113it [01:03,  1.61it/s]Extractor Estimating: 114it [01:04,  1.61it/s]Extractor Estimating: 115it [01:05,  1.62it/s]Extractor Estimating: 116it [01:05,  1.65it/s]Extractor Estimating: 117it [01:06,  1.70it/s]Extractor Estimating: 118it [01:06,  1.68it/s]Extractor Estimating: 119it [01:07,  1.62it/s]Extractor Estimating: 120it [01:08,  1.63it/s]Extractor Estimating: 121it [01:08,  1.55it/s]Extractor Estimating: 122it [01:09,  1.62it/s]Extractor Estimating: 123it [01:09,  1.62it/s]Extractor Estimating: 124it [01:10,  1.65it/s]Extractor Estimating: 125it [01:11,  1.61it/s]Extractor Estimating: 126it [01:11,  1.62it/s]Extractor Estimating: 127it [01:12,  1.62it/s]Extractor Estimating: 128it [01:12,  1.67it/s]Extractor Estimating: 129it [01:13,  1.67it/s]Extractor Estimating: 130it [01:14,  1.65it/s]Extractor Estimating: 131it [01:14,  1.67it/s]Extractor Estimating: 132it [01:15,  1.70it/s]Extractor Estimating: 133it [01:15,  1.71it/s]Extractor Estimating: 134it [01:16,  1.66it/s]Extractor Estimating: 135it [01:17,  1.66it/s]Extractor Estimating: 136it [01:17,  1.69it/s]Extractor Estimating: 137it [01:18,  1.71it/s]Extractor Estimating: 138it [01:18,  1.67it/s]Extractor Estimating: 139it [01:19,  1.66it/s]Extractor Estimating: 140it [01:20,  1.68it/s]Extractor Estimating: 141it [01:20,  1.66it/s]Extractor Estimating: 142it [01:21,  1.71it/s]Extractor Estimating: 143it [01:21,  1.65it/s]Extractor Estimating: 144it [01:22,  1.67it/s]Extractor Estimating: 145it [01:23,  1.72it/s]Extractor Estimating: 146it [01:23,  1.73it/s]Extractor Estimating: 147it [01:24,  1.68it/s]Extractor Estimating: 148it [01:24,  1.75it/s]Extractor Estimating: 149it [01:25,  1.66it/s]Extractor Estimating: 150it [01:26,  1.66it/s]Extractor Estimating: 151it [01:26,  1.70it/s]Extractor Estimating: 152it [01:27,  1.70it/s]Extractor Estimating: 153it [01:27,  1.71it/s]Extractor Estimating: 154it [01:28,  1.73it/s]Extractor Estimating: 155it [01:28,  1.70it/s]Extractor Estimating: 156it [01:29,  1.68it/s]Extractor Estimating: 157it [01:30,  1.70it/s]Extractor Estimating: 158it [01:30,  1.73it/s]Extractor Estimating: 159it [01:31,  1.73it/s]Extractor Estimating: 160it [01:31,  1.67it/s]Extractor Estimating: 161it [01:32,  1.66it/s]Extractor Estimating: 162it [01:33,  1.68it/s]Extractor Estimating: 163it [01:33,  1.68it/s]Extractor Estimating: 164it [01:34,  1.69it/s]Extractor Estimating: 165it [01:34,  1.70it/s]Extractor Estimating: 166it [01:35,  1.71it/s]Extractor Estimating: 167it [01:36,  1.69it/s]Extractor Estimating: 168it [01:36,  1.65it/s]Extractor Estimating: 169it [01:37,  1.63it/s]Extractor Estimating: 170it [01:38,  1.52it/s]Extractor Estimating: 171it [01:38,  1.60it/s]Extractor Estimating: 172it [01:39,  1.65it/s]Extractor Estimating: 173it [01:39,  1.65it/s]Extractor Estimating: 174it [01:40,  1.67it/s]Extractor Estimating: 175it [01:40,  1.67it/s]Extractor Estimating: 176it [01:41,  1.68it/s]Extractor Estimating: 177it [01:42,  1.67it/s]Extractor Estimating: 178it [01:42,  1.67it/s]Extractor Estimating: 179it [01:43,  1.66it/s]Extractor Estimating: 180it [01:43,  1.62it/s]Extractor Estimating: 181it [01:44,  1.64it/s]Extractor Estimating: 182it [01:45,  1.70it/s]Extractor Estimating: 183it [01:45,  1.70it/s]Extractor Estimating: 184it [01:46,  1.69it/s]Extractor Estimating: 185it [01:46,  1.68it/s]Extractor Estimating: 186it [01:47,  1.69it/s]Extractor Estimating: 187it [01:48,  1.67it/s]Extractor Estimating: 188it [01:48,  1.67it/s]Extractor Estimating: 189it [01:49,  1.72it/s]Extractor Estimating: 190it [01:49,  1.71it/s]Extractor Estimating: 191it [01:50,  1.72it/s]Extractor Estimating: 192it [01:51,  1.72it/s]Extractor Estimating: 193it [01:51,  1.74it/s]Extractor Estimating: 194it [01:52,  1.74it/s]Extractor Estimating: 195it [01:52,  1.74it/s]Extractor Estimating: 196it [01:53,  1.72it/s]Extractor Estimating: 197it [01:53,  1.72it/s]Extractor Estimating: 198it [01:54,  1.74it/s]Extractor Estimating: 199it [01:55,  1.72it/s]Extractor Estimating: 200it [01:55,  1.67it/s]Extractor Estimating: 201it [01:56,  1.71it/s]Extractor Estimating: 202it [01:56,  1.75it/s]Extractor Estimating: 203it [01:57,  1.76it/s]Extractor Estimating: 204it [01:58,  1.62it/s]Extractor Estimating: 205it [01:58,  1.64it/s]Extractor Estimating: 206it [01:59,  1.63it/s]Extractor Estimating: 207it [01:59,  1.68it/s]Extractor Estimating: 208it [02:00,  1.66it/s]Extractor Estimating: 209it [02:00,  1.73it/s]Extractor Estimating: 210it [02:01,  1.68it/s]Extractor Estimating: 211it [02:02,  1.70it/s]Extractor Estimating: 212it [02:02,  1.67it/s]Extractor Estimating: 213it [02:03,  1.68it/s]Extractor Estimating: 214it [02:04,  1.67it/s]Extractor Estimating: 215it [02:04,  1.67it/s]Extractor Estimating: 216it [02:05,  1.64it/s]Extractor Estimating: 217it [02:05,  1.68it/s]Extractor Estimating: 218it [02:06,  1.71it/s]Extractor Estimating: 219it [02:06,  1.68it/s]Extractor Estimating: 220it [02:07,  1.70it/s]Extractor Estimating: 221it [02:08,  1.73it/s]Extractor Estimating: 222it [02:08,  1.69it/s]Extractor Estimating: 223it [02:09,  1.72it/s]Extractor Estimating: 224it [02:09,  1.69it/s]Extractor Estimating: 225it [02:10,  1.66it/s]Extractor Estimating: 226it [02:11,  1.60it/s]Extractor Estimating: 227it [02:11,  1.60it/s]Extractor Estimating: 228it [02:12,  1.60it/s]Extractor Estimating: 229it [02:13,  1.63it/s]Extractor Estimating: 230it [02:13,  1.63it/s]Extractor Estimating: 231it [02:14,  1.64it/s]Extractor Estimating: 232it [02:14,  1.65it/s]Extractor Estimating: 233it [02:15,  1.61it/s]Extractor Estimating: 234it [02:16,  1.64it/s]Extractor Estimating: 235it [02:16,  1.62it/s]Extractor Estimating: 236it [02:17,  1.63it/s]Extractor Estimating: 237it [02:17,  1.67it/s]Extractor Estimating: 238it [02:18,  1.71it/s]Extractor Estimating: 239it [02:19,  1.71it/s]Extractor Estimating: 240it [02:19,  1.62it/s]Extractor Estimating: 241it [02:20,  1.59it/s]Extractor Estimating: 242it [02:21,  1.60it/s]Extractor Estimating: 243it [02:21,  1.61it/s]Extractor Estimating: 244it [02:22,  1.48it/s]Extractor Estimating: 245it [02:23,  1.54it/s]Extractor Estimating: 246it [02:23,  1.58it/s]Extractor Estimating: 247it [02:24,  1.58it/s]Extractor Estimating: 248it [02:24,  1.60it/s]Extractor Estimating: 249it [02:25,  1.61it/s]Extractor Estimating: 250it [02:26,  1.62it/s]Extractor Estimating: 251it [02:26,  1.61it/s]Extractor Estimating: 252it [02:27,  1.64it/s]Extractor Estimating: 253it [02:27,  1.61it/s]Extractor Estimating: 254it [02:28,  1.63it/s]Extractor Estimating: 255it [02:29,  1.67it/s]Extractor Estimating: 256it [02:29,  1.69it/s]Extractor Estimating: 257it [02:30,  1.70it/s]Extractor Estimating: 258it [02:30,  1.68it/s]Extractor Estimating: 259it [02:31,  1.66it/s]Extractor Estimating: 260it [02:32,  1.65it/s]Extractor Estimating: 261it [02:32,  1.63it/s]Extractor Estimating: 262it [02:33,  1.62it/s]Extractor Estimating: 263it [02:33,  1.63it/s]Extractor Estimating: 264it [02:34,  1.66it/s]Extractor Estimating: 265it [02:35,  1.64it/s]Extractor Estimating: 266it [02:35,  1.64it/s]Extractor Estimating: 267it [02:36,  1.64it/s]Extractor Estimating: 268it [02:37,  1.61it/s]Extractor Estimating: 269it [02:37,  1.62it/s]Extractor Estimating: 270it [02:38,  1.59it/s]Extractor Estimating: 271it [02:38,  1.62it/s]Extractor Estimating: 272it [02:39,  1.60it/s]Extractor Estimating: 273it [02:40,  1.63it/s]Extractor Estimating: 274it [02:40,  1.64it/s]Extractor Estimating: 275it [02:41,  1.61it/s]Extractor Estimating: 276it [02:41,  1.61it/s]Extractor Estimating: 277it [02:42,  1.61it/s]Extractor Estimating: 278it [02:43,  1.60it/s]Extractor Estimating: 279it [02:43,  1.62it/s]Extractor Estimating: 280it [02:44,  1.66it/s]Extractor Estimating: 281it [02:44,  1.66it/s]Extractor Estimating: 282it [02:45,  1.71it/s]Extractor Estimating: 283it [02:46,  1.71it/s]Extractor Estimating: 284it [02:46,  1.68it/s]Extractor Estimating: 285it [02:47,  1.68it/s]Extractor Estimating: 286it [02:47,  1.69it/s]Extractor Estimating: 287it [02:48,  1.67it/s]Extractor Estimating: 288it [02:49,  1.66it/s]Extractor Estimating: 289it [02:49,  1.61it/s]Extractor Estimating: 290it [02:50,  1.60it/s]Extractor Estimating: 291it [02:51,  1.63it/s]Extractor Estimating: 292it [02:51,  1.66it/s]Extractor Estimating: 293it [02:52,  1.69it/s]Extractor Estimating: 294it [02:52,  1.69it/s]Extractor Estimating: 295it [02:53,  1.69it/s]Extractor Estimating: 296it [02:53,  1.73it/s]Extractor Estimating: 297it [02:54,  1.69it/s]Extractor Estimating: 298it [02:55,  1.68it/s]Extractor Estimating: 299it [02:55,  1.66it/s]Extractor Estimating: 300it [02:56,  1.67it/s]Extractor Estimating: 301it [02:56,  1.68it/s]Extractor Estimating: 302it [02:57,  1.68it/s]Extractor Estimating: 303it [02:58,  1.67it/s]Extractor Estimating: 304it [02:58,  1.66it/s]Extractor Estimating: 305it [02:59,  1.64it/s]Extractor Estimating: 306it [02:59,  1.64it/s]Extractor Estimating: 307it [03:00,  1.70it/s]Extractor Estimating: 308it [03:01,  1.69it/s]Extractor Estimating: 309it [03:01,  1.58it/s]Extractor Estimating: 310it [03:02,  1.59it/s]Extractor Estimating: 311it [03:03,  1.65it/s]Extractor Estimating: 312it [03:03,  1.63it/s]Extractor Estimating: 313it [03:04,  1.61it/s]Extractor Estimating: 314it [03:04,  1.64it/s]Extractor Estimating: 315it [03:05,  1.63it/s]Extractor Estimating: 316it [03:06,  1.68it/s]Extractor Estimating: 317it [03:06,  1.62it/s]Extractor Estimating: 318it [03:07,  1.59it/s]Extractor Estimating: 319it [03:08,  1.59it/s]Extractor Estimating: 320it [03:08,  1.64it/s]Extractor Estimating: 321it [03:09,  1.61it/s]Extractor Estimating: 322it [03:09,  1.59it/s]Extractor Estimating: 323it [03:10,  1.60it/s]Extractor Estimating: 324it [03:11,  1.64it/s]Extractor Estimating: 325it [03:11,  1.61it/s]Extractor Estimating: 326it [03:12,  1.62it/s]Extractor Estimating: 327it [03:12,  1.63it/s]Extractor Estimating: 328it [03:13,  1.60it/s]Extractor Estimating: 329it [03:14,  1.46it/s]Extractor Estimating: 330it [03:15,  1.48it/s]Extractor Estimating: 331it [03:15,  1.51it/s]Extractor Estimating: 332it [03:16,  1.55it/s]Extractor Estimating: 333it [03:16,  1.55it/s]Extractor Estimating: 334it [03:17,  1.59it/s]Extractor Estimating: 335it [03:18,  1.60it/s]Extractor Estimating: 336it [03:18,  1.60it/s]Extractor Estimating: 337it [03:19,  1.59it/s]Extractor Estimating: 338it [03:20,  1.61it/s]Extractor Estimating: 339it [03:20,  1.58it/s]Extractor Estimating: 340it [03:21,  1.59it/s]Extractor Estimating: 341it [03:21,  1.59it/s]Extractor Estimating: 342it [03:22,  1.60it/s]Extractor Estimating: 343it [03:23,  1.58it/s]Extractor Estimating: 344it [03:23,  1.58it/s]Extractor Estimating: 345it [03:24,  1.59it/s]Extractor Estimating: 346it [03:25,  1.59it/s]Extractor Estimating: 347it [03:25,  1.58it/s]Extractor Estimating: 348it [03:26,  1.58it/s]Extractor Estimating: 349it [03:26,  1.59it/s]Extractor Estimating: 350it [03:27,  1.65it/s]Extractor Estimating: 351it [03:28,  1.70it/s]Extractor Estimating: 352it [03:28,  1.76it/s]Extractor Estimating: 353it [03:29,  1.70it/s]Extractor Estimating: 354it [03:29,  1.70it/s]Extractor Estimating: 355it [03:30,  1.70it/s]Extractor Estimating: 356it [03:30,  1.73it/s]Extractor Estimating: 357it [03:31,  1.73it/s]Extractor Estimating: 358it [03:32,  1.74it/s]Extractor Estimating: 359it [03:32,  1.78it/s]Extractor Estimating: 360it [03:33,  1.80it/s]Extractor Estimating: 361it [03:33,  1.76it/s]Extractor Estimating: 362it [03:34,  1.75it/s]Extractor Estimating: 363it [03:34,  1.73it/s]Extractor Estimating: 364it [03:35,  1.80it/s]Extractor Estimating: 365it [03:35,  1.82it/s]Extractor Estimating: 366it [03:36,  1.83it/s]Extractor Estimating: 367it [03:37,  1.78it/s]Extractor Estimating: 368it [03:37,  1.73it/s]Extractor Estimating: 369it [03:38,  1.75it/s]Extractor Estimating: 370it [03:38,  1.74it/s]Extractor Estimating: 371it [03:40,  1.07it/s]Extractor Estimating: 372it [03:41,  1.20it/s]Extractor Estimating: 373it [03:41,  1.29it/s]Extractor Estimating: 374it [03:42,  1.40it/s]Extractor Estimating: 375it [03:42,  1.80it/s]Extractor Estimating: 375it [03:42,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:15,815 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:15,817 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:15,817 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:15,817 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:15,817 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:55:16,446 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:55:16,448 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:55:17,010 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:55:18,090 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:55:18,090 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:20,941 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:20,947 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:20,947 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:20,947 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:55:20,947 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:55:21,600 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:55:21,601 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:55:22,187 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:55:22,360 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:55:22,360 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:07:36,480 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:07:36,573 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7521 mean pseudo reward: 0.9601068605624138
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 25094
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25194, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25194, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.948, loss:750.6432
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.959, loss:688.2376
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.978, loss:703.4942
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 86, avg_time 0.958, loss:671.7144
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 186, avg_time 0.967, loss:691.8185
>> valid entity prec:0.5695, rec:0.4853, f1:0.5240
>> valid relation prec:0.1438, rec:0.0180, f1:0.0321
>> valid relation with NER prec:0.1438, rec:0.0180, f1:0.0321
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 286, avg_time 2.572, loss:719.3506
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 72, avg_time 0.964, loss:688.8953
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 172, avg_time 0.947, loss:657.1336
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 272, avg_time 0.968, loss:731.5773
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 58, avg_time 0.960, loss:706.3871
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5641, rec:0.3988, f1:0.4672
>> valid relation prec:0.2068, rec:0.0125, f1:0.0236
>> valid relation with NER prec:0.2068, rec:0.0125, f1:0.0236
g_step 1100, step 158, avg_time 2.562, loss:703.6177
g_step 1200, step 258, avg_time 0.962, loss:711.9008
g_step 1300, step 44, avg_time 0.964, loss:705.8184
g_step 1400, step 144, avg_time 0.973, loss:669.5535
g_step 1500, step 244, avg_time 0.966, loss:684.0877
>> valid entity prec:0.5819, rec:0.5777, f1:0.5798
>> valid relation prec:0.1388, rec:0.0281, f1:0.0467
>> valid relation with NER prec:0.1388, rec:0.0281, f1:0.0467
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 30, avg_time 2.550, loss:692.0954
g_step 1700, step 130, avg_time 0.963, loss:652.9042
g_step 1800, step 230, avg_time 0.962, loss:652.4297
g_step 1900, step 16, avg_time 0.955, loss:669.5360
g_step 2000, step 116, avg_time 0.958, loss:623.2305
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5802, rec:0.3925, f1:0.4682
>> valid relation prec:0.1459, rec:0.0197, f1:0.0347
>> valid relation with NER prec:0.1459, rec:0.0197, f1:0.0347
g_step 2100, step 216, avg_time 2.541, loss:645.2364
g_step 2200, step 2, avg_time 0.957, loss:636.6668
g_step 2300, step 102, avg_time 0.959, loss:617.2270
g_step 2400, step 202, avg_time 0.954, loss:622.7517
g_step 2500, step 302, avg_time 0.965, loss:646.4670
>> valid entity prec:0.5624, rec:0.5318, f1:0.5467
>> valid relation prec:0.1042, rec:0.0158, f1:0.0274
>> valid relation with NER prec:0.1042, rec:0.0158, f1:0.0274
g_step 2600, step 88, avg_time 2.558, loss:600.3860
g_step 2700, step 188, avg_time 0.953, loss:584.9054
g_step 2800, step 288, avg_time 0.957, loss:615.9545
g_step 2900, step 74, avg_time 0.950, loss:553.5064
g_step 3000, step 174, avg_time 0.952, loss:591.8539
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5569, rec:0.4849, f1:0.5184
>> valid relation prec:0.1508, rec:0.0283, f1:0.0476
>> valid relation with NER prec:0.1508, rec:0.0283, f1:0.0476
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 274, avg_time 2.545, loss:601.5971
g_step 3200, step 60, avg_time 0.957, loss:588.6095
g_step 3300, step 160, avg_time 0.950, loss:546.7341
g_step 3400, step 260, avg_time 0.969, loss:588.9600
g_step 3500, step 46, avg_time 0.941, loss:554.9857
>> valid entity prec:0.5379, rec:0.4212, f1:0.4724
>> valid relation prec:0.0971, rec:0.0162, f1:0.0278
>> valid relation with NER prec:0.0971, rec:0.0162, f1:0.0278
g_step 3600, step 146, avg_time 2.545, loss:547.6324
g_step 3700, step 246, avg_time 0.957, loss:563.6640
g_step 3800, step 32, avg_time 0.952, loss:525.4483
g_step 3900, step 132, avg_time 0.957, loss:528.9675
g_step 4000, step 232, avg_time 0.950, loss:563.2340
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5524, rec:0.4908, f1:0.5198
>> valid relation prec:0.0909, rec:0.0152, f1:0.0260
>> valid relation with NER prec:0.0909, rec:0.0152, f1:0.0260
g_step 4100, step 18, avg_time 2.546, loss:536.5779
g_step 4200, step 118, avg_time 0.962, loss:516.7775
g_step 4300, step 218, avg_time 0.955, loss:522.2769
g_step 4400, step 4, avg_time 0.956, loss:523.7200
g_step 4500, step 104, avg_time 0.960, loss:496.8045
>> valid entity prec:0.5045, rec:0.4854, f1:0.4948
>> valid relation prec:0.1180, rec:0.0291, f1:0.0467
>> valid relation with NER prec:0.1180, rec:0.0291, f1:0.0467
g_step 4600, step 204, avg_time 2.550, loss:505.5045
g_step 4700, step 304, avg_time 0.957, loss:528.4986
g_step 4800, step 90, avg_time 0.958, loss:505.8067
g_step 4900, step 190, avg_time 0.961, loss:477.6544
g_step 5000, step 290, avg_time 0.962, loss:502.2178
learning rate was adjusted to 0.0008
>> valid entity prec:0.5173, rec:0.5136, f1:0.5154
>> valid relation prec:0.1429, rec:0.0365, f1:0.0581
>> valid relation with NER prec:0.1429, rec:0.0365, f1:0.0581
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 76, avg_time 2.545, loss:456.6088
g_step 5200, step 176, avg_time 0.968, loss:486.2790
g_step 5300, step 276, avg_time 0.964, loss:499.5272
g_step 5400, step 62, avg_time 0.947, loss:448.6226
g_step 5500, step 162, avg_time 0.963, loss:474.3045
>> valid entity prec:0.4876, rec:0.4536, f1:0.4700
>> valid relation prec:0.1101, rec:0.0225, f1:0.0374
>> valid relation with NER prec:0.1101, rec:0.0225, f1:0.0374
g_step 5600, step 262, avg_time 2.552, loss:478.0837
g_step 5700, step 48, avg_time 0.958, loss:470.1317
g_step 5800, step 148, avg_time 0.965, loss:445.9491
g_step 5900, step 248, avg_time 0.970, loss:469.6118
g_step 6000, step 34, avg_time 0.940, loss:446.4805
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5230, rec:0.4866, f1:0.5041
>> valid relation prec:0.1087, rec:0.0254, f1:0.0412
>> valid relation with NER prec:0.1087, rec:0.0254, f1:0.0412
g_step 6100, step 134, avg_time 2.542, loss:433.4396
g_step 6200, step 234, avg_time 0.957, loss:456.6287
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:07:36 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:07:36 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-07-36_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:07:37 - WARNING - datasets.builder -   Using custom data configuration default-3aa3a3182d29e510
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-3aa3a3182d29e510/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:07:38,232 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:07:38,233 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:07:38,234 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:07:38,234 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:07:38,247 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:07:38,251 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:07:38,251 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:07:38,251 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:07:38,251 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:07:38,251 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:07:38,251 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:07:38,399 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:07:41,480 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:07:41,483 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-3aa3a3182d29e510/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.23ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.01ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.31ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.45ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.52ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.56ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.60ba/s]100%|██████████| 8/8 [00:01<00:00,  5.25ba/s]100%|██████████| 8/8 [00:01<00:00,  4.65ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.60ba/s] 40%|████      | 2/5 [00:00<00:00,  4.11ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.43ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.80ba/s]100%|██████████| 5/5 [00:01<00:00,  4.13ba/s]100%|██████████| 5/5 [00:01<00:00,  3.93ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.12ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.89ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.43ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.68ba/s]100%|██████████| 8/8 [00:00<00:00, 11.00ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.96ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.33ba/s]100%|██████████| 5/5 [00:00<00:00, 11.11ba/s]100%|██████████| 5/5 [00:00<00:00, 10.82ba/s]
[INFO|trainer.py:414] 2023-08-28 17:07:46,623 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:07:46,639 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:07:46,639 >>   Num examples = 7605
[INFO|trainer.py:1149] 2023-08-28 17:07:46,639 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:07:46,639 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:07:46,639 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:07:46,639 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:07:46,639 >>   Total optimization steps = 595
  0%|          | 0/595 [00:00<?, ?it/s]  0%|          | 1/595 [00:00<02:59,  3.30it/s]  0%|          | 2/595 [00:00<02:54,  3.39it/s]  1%|          | 3/595 [00:00<02:52,  3.42it/s]  1%|          | 4/595 [00:01<02:52,  3.44it/s]  1%|          | 5/595 [00:01<02:54,  3.39it/s]  1%|          | 6/595 [00:01<02:53,  3.39it/s]  1%|          | 7/595 [00:02<02:53,  3.40it/s]  1%|▏         | 8/595 [00:02<02:52,  3.40it/s]  2%|▏         | 9/595 [00:02<02:52,  3.40it/s]  2%|▏         | 10/595 [00:02<02:52,  3.40it/s]  2%|▏         | 11/595 [00:03<02:51,  3.40it/s]  2%|▏         | 12/595 [00:03<02:51,  3.40it/s]  2%|▏         | 13/595 [00:03<02:51,  3.40it/s]  2%|▏         | 14/595 [00:04<02:50,  3.40it/s]  3%|▎         | 15/595 [00:04<02:50,  3.41it/s]  3%|▎         | 16/595 [00:04<02:50,  3.39it/s]  3%|▎         | 17/595 [00:05<02:50,  3.39it/s]  3%|▎         | 18/595 [00:05<02:49,  3.40it/s]  3%|▎         | 19/595 [00:05<02:49,  3.40it/s]  3%|▎         | 20/595 [00:05<02:49,  3.40it/s]  4%|▎         | 21/595 [00:06<02:48,  3.40it/s]  4%|▎         | 22/595 [00:06<02:48,  3.40it/s]  4%|▍         | 23/595 [00:06<02:48,  3.40it/s]  4%|▍         | 24/595 [00:07<02:47,  3.40it/s]  4%|▍         | 25/595 [00:07<02:47,  3.40it/s]  4%|▍         | 26/595 [00:07<02:47,  3.40it/s]  5%|▍         | 27/595 [00:07<02:48,  3.37it/s]  5%|▍         | 28/595 [00:08<02:47,  3.38it/s]  5%|▍         | 29/595 [00:08<02:47,  3.38it/s]  5%|▌         | 30/595 [00:08<02:46,  3.39it/s]  5%|▌         | 31/595 [00:09<02:46,  3.39it/s]  5%|▌         | 32/595 [00:09<02:45,  3.39it/s]  6%|▌         | 33/595 [00:09<02:45,  3.40it/s]  6%|▌         | 34/595 [00:10<02:45,  3.40it/s]  6%|▌         | 35/595 [00:10<02:44,  3.40it/s]  6%|▌         | 36/595 [00:10<02:44,  3.40it/s]  6%|▌         | 37/595 [00:10<02:44,  3.40it/s]  6%|▋         | 38/595 [00:11<02:44,  3.38it/s]  7%|▋         | 39/595 [00:11<02:44,  3.38it/s]  7%|▋         | 40/595 [00:11<02:43,  3.39it/s]  7%|▋         | 41/595 [00:12<02:43,  3.39it/s]  7%|▋         | 42/595 [00:12<02:42,  3.39it/s]  7%|▋         | 43/595 [00:12<02:42,  3.39it/s]  7%|▋         | 44/595 [00:12<02:42,  3.40it/s]  8%|▊         | 45/595 [00:13<02:41,  3.40it/s]  8%|▊         | 46/595 [00:13<02:41,  3.40it/s]  8%|▊         | 47/595 [00:13<02:41,  3.40it/s]  8%|▊         | 48/595 [00:14<02:41,  3.40it/s]  8%|▊         | 49/595 [00:14<02:43,  3.35it/s]  8%|▊         | 50/595 [00:14<02:41,  3.37it/s]  9%|▊         | 51/595 [00:15<02:41,  3.38it/s]  9%|▊         | 52/595 [00:15<02:40,  3.38it/s]  9%|▉         | 53/595 [00:15<02:40,  3.39it/s]  9%|▉         | 54/595 [00:15<02:39,  3.39it/s]  9%|▉         | 55/595 [00:16<02:39,  3.39it/s]  9%|▉         | 56/595 [00:16<02:38,  3.39it/s] 10%|▉         | 57/595 [00:16<02:38,  3.40it/s] 10%|▉         | 58/595 [00:17<02:38,  3.40it/s] 10%|▉         | 59/595 [00:17<02:38,  3.39it/s] 10%|█         | 60/595 [00:17<02:39,  3.35it/s] 10%|█         | 61/595 [00:17<02:38,  3.37it/s] 10%|█         | 62/595 [00:18<02:38,  3.37it/s] 11%|█         | 63/595 [00:18<02:37,  3.38it/s] 11%|█         | 64/595 [00:18<02:36,  3.38it/s] 11%|█         | 65/595 [00:19<02:36,  3.39it/s] 11%|█         | 66/595 [00:19<02:36,  3.39it/s] 11%|█▏        | 67/595 [00:19<02:35,  3.39it/s] 11%|█▏        | 68/595 [00:20<02:35,  3.39it/s] 12%|█▏        | 69/595 [00:20<02:35,  3.39it/s] 12%|█▏        | 70/595 [00:20<02:34,  3.39it/s] 12%|█▏        | 71/595 [00:20<02:41,  3.25it/s] 12%|█▏        | 72/595 [00:21<02:39,  3.29it/s] 12%|█▏        | 73/595 [00:21<02:37,  3.32it/s] 12%|█▏        | 74/595 [00:21<02:36,  3.34it/s] 13%|█▎        | 75/595 [00:22<02:35,  3.35it/s] 13%|█▎        | 76/595 [00:22<02:34,  3.36it/s] 13%|█▎        | 77/595 [00:22<02:33,  3.37it/s] 13%|█▎        | 78/595 [00:23<02:33,  3.38it/s] 13%|█▎        | 79/595 [00:23<02:32,  3.38it/s] 13%|█▎        | 80/595 [00:23<02:32,  3.38it/s] 14%|█▎        | 81/595 [00:23<02:31,  3.38it/s] 14%|█▍        | 82/595 [00:24<02:31,  3.38it/s] 14%|█▍        | 83/595 [00:24<02:31,  3.38it/s] 14%|█▍        | 84/595 [00:24<02:31,  3.38it/s] 14%|█▍        | 85/595 [00:25<02:30,  3.38it/s] 14%|█▍        | 86/595 [00:25<02:30,  3.38it/s] 15%|█▍        | 87/595 [00:25<02:30,  3.38it/s] 15%|█▍        | 88/595 [00:26<02:29,  3.38it/s] 15%|█▍        | 89/595 [00:26<02:28,  3.40it/s] 15%|█▌        | 90/595 [00:26<02:28,  3.41it/s] 15%|█▌        | 91/595 [00:26<02:27,  3.42it/s] 15%|█▌        | 92/595 [00:27<02:26,  3.43it/s] 16%|█▌        | 93/595 [00:27<02:27,  3.40it/s] 16%|█▌        | 94/595 [00:27<02:26,  3.41it/s] 16%|█▌        | 95/595 [00:28<02:26,  3.42it/s] 16%|█▌        | 96/595 [00:28<02:25,  3.42it/s] 16%|█▋        | 97/595 [00:28<02:24,  3.43it/s] 16%|█▋        | 98/595 [00:28<02:25,  3.43it/s] 17%|█▋        | 99/595 [00:29<02:24,  3.43it/s] 17%|█▋        | 100/595 [00:29<02:24,  3.43it/s] 17%|█▋        | 101/595 [00:29<02:23,  3.44it/s] 17%|█▋        | 102/595 [00:30<02:23,  3.44it/s] 17%|█▋        | 103/595 [00:30<02:23,  3.44it/s] 17%|█▋        | 104/595 [00:30<02:26,  3.35it/s] 18%|█▊        | 105/595 [00:30<02:25,  3.38it/s] 18%|█▊        | 106/595 [00:31<02:24,  3.39it/s] 18%|█▊        | 107/595 [00:31<02:23,  3.41it/s] 18%|█▊        | 108/595 [00:31<02:22,  3.42it/s] 18%|█▊        | 109/595 [00:32<02:21,  3.43it/s] 18%|█▊        | 110/595 [00:32<02:21,  3.43it/s] 19%|█▊        | 111/595 [00:32<02:20,  3.43it/s] 19%|█▉        | 112/595 [00:33<02:20,  3.43it/s] 19%|█▉        | 113/595 [00:33<02:20,  3.44it/s] 19%|█▉        | 114/595 [00:33<02:19,  3.44it/s] 19%|█▉        | 115/595 [00:33<02:21,  3.40it/s] 19%|█▉        | 116/595 [00:34<02:20,  3.41it/s] 20%|█▉        | 117/595 [00:34<02:19,  3.42it/s] 20%|█▉        | 118/595 [00:34<02:19,  3.43it/s] 20%|██        | 119/595 [00:35<02:12,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 17:08:21,660 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:08:21,660 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 17:08:21,660 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.19it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.59it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.10it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.38it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.06it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.67it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.40it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.39it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.35it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.61it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.41it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.24it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.08it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.21it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.13it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.96it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.08it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.33it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.37it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.25it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.17it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.11it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.13it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.02it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.12it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.27it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.32it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.35it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.33it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.25it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.22it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.19it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.08it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.16it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.16it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.28it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.38it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.35it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.23it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.23it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.09it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.16it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.25it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.20it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.21it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.26it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.27it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.24it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.26it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.16it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.14it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.21it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.23it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.18it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.29it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.27it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.24it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.13it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.16it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.21it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.30it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.17it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.21it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.25it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.27it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.28it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.24it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.16it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.23it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.22it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.20it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.24it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.19it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.22it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.12it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.20it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.18it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.27it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.25it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.23it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.27it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.24it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.09it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.21it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.09it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.22it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.26it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.18it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.15it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.27it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.10it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.14it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.08it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.18it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.25it/s][A
 79%|███████▉  | 482/611 [00:10<00:03, 42.84it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 43.42it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.70it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 43.73it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 43.95it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.01it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.02it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.03it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.86it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.01it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.21it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.24it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.06it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.14it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.06it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.06it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.12it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.09it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.15it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.21it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.07it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.02it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.17it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.09it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.11it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.12it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.12it/s][A 20%|██        | 119/595 [00:48<02:12,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:08:35,518 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-119
[INFO|configuration_utils.py:351] 2023-08-28 17:08:35,542 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-119/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:08:39,565 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-119/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:08:39,606 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-119/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:08:39,619 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-119/special_tokens_map.json
 20%|██        | 120/595 [01:01<1:05:14,  8.24s/it] 20%|██        | 121/595 [01:02<46:16,  5.86s/it]   21%|██        | 122/595 [01:02<33:01,  4.19s/it] 21%|██        | 123/595 [01:02<23:45,  3.02s/it] 21%|██        | 124/595 [01:03<17:17,  2.20s/it] 21%|██        | 125/595 [01:03<12:45,  1.63s/it] 21%|██        | 126/595 [01:03<09:36,  1.23s/it] 21%|██▏       | 127/595 [01:03<07:23,  1.05it/s] 22%|██▏       | 128/595 [01:04<05:51,  1.33it/s] 22%|██▏       | 129/595 [01:04<04:46,  1.63it/s] 22%|██▏       | 130/595 [01:04<04:01,  1.93it/s] 22%|██▏       | 131/595 [01:05<03:29,  2.21it/s] 22%|██▏       | 132/595 [01:05<03:07,  2.46it/s] 22%|██▏       | 133/595 [01:05<02:52,  2.68it/s] 23%|██▎       | 134/595 [01:05<02:40,  2.86it/s] 23%|██▎       | 135/595 [01:06<02:33,  3.01it/s] 23%|██▎       | 136/595 [01:06<02:27,  3.11it/s] 23%|██▎       | 137/595 [01:06<02:23,  3.19it/s] 23%|██▎       | 138/595 [01:07<02:20,  3.25it/s] 23%|██▎       | 139/595 [01:07<02:18,  3.29it/s] 24%|██▎       | 140/595 [01:07<02:16,  3.33it/s] 24%|██▎       | 141/595 [01:08<02:15,  3.34it/s] 24%|██▍       | 142/595 [01:08<02:14,  3.36it/s] 24%|██▍       | 143/595 [01:08<02:14,  3.37it/s] 24%|██▍       | 144/595 [01:08<02:13,  3.38it/s] 24%|██▍       | 145/595 [01:09<02:13,  3.38it/s] 25%|██▍       | 146/595 [01:09<02:12,  3.38it/s] 25%|██▍       | 147/595 [01:09<02:12,  3.39it/s] 25%|██▍       | 148/595 [01:10<02:11,  3.39it/s] 25%|██▌       | 149/595 [01:10<02:11,  3.39it/s] 25%|██▌       | 150/595 [01:10<02:11,  3.39it/s] 25%|██▌       | 151/595 [01:10<02:10,  3.40it/s] 26%|██▌       | 152/595 [01:11<02:10,  3.40it/s] 26%|██▌       | 153/595 [01:11<02:10,  3.40it/s] 26%|██▌       | 154/595 [01:11<02:16,  3.24it/s] 26%|██▌       | 155/595 [01:12<02:13,  3.28it/s] 26%|██▌       | 156/595 [01:12<02:12,  3.32it/s] 26%|██▋       | 157/595 [01:12<02:11,  3.34it/s] 27%|██▋       | 158/595 [01:13<02:10,  3.35it/s] 27%|██▋       | 159/595 [01:13<02:09,  3.37it/s] 27%|██▋       | 160/595 [01:13<02:08,  3.37it/s] 27%|██▋       | 161/595 [01:13<02:08,  3.38it/s] 27%|██▋       | 162/595 [01:14<02:08,  3.38it/s] 27%|██▋       | 163/595 [01:14<02:07,  3.38it/s] 28%|██▊       | 164/595 [01:14<02:07,  3.39it/s] 28%|██▊       | 165/595 [01:15<02:08,  3.34it/s] 28%|██▊       | 166/595 [01:15<02:07,  3.35it/s] 28%|██▊       | 167/595 [01:15<02:07,  3.37it/s] 28%|██▊       | 168/595 [01:16<02:06,  3.37it/s] 28%|██▊       | 169/595 [01:16<02:06,  3.38it/s] 29%|██▊       | 170/595 [01:16<02:05,  3.38it/s] 29%|██▊       | 171/595 [01:16<02:05,  3.39it/s] 29%|██▉       | 172/595 [01:17<02:04,  3.39it/s] 29%|██▉       | 173/595 [01:17<02:04,  3.39it/s] 29%|██▉       | 174/595 [01:17<02:04,  3.39it/s] 29%|██▉       | 175/595 [01:18<02:03,  3.39it/s] 30%|██▉       | 176/595 [01:18<02:04,  3.37it/s] 30%|██▉       | 177/595 [01:18<02:03,  3.38it/s] 30%|██▉       | 178/595 [01:18<02:03,  3.38it/s] 30%|███       | 179/595 [01:19<02:02,  3.38it/s] 30%|███       | 180/595 [01:19<02:02,  3.39it/s] 30%|███       | 181/595 [01:19<02:02,  3.39it/s] 31%|███       | 182/595 [01:20<02:02,  3.38it/s] 31%|███       | 183/595 [01:20<02:01,  3.39it/s] 31%|███       | 184/595 [01:20<02:01,  3.39it/s] 31%|███       | 185/595 [01:21<02:01,  3.39it/s] 31%|███▏      | 186/595 [01:21<02:00,  3.39it/s] 31%|███▏      | 187/595 [01:21<02:01,  3.36it/s] 32%|███▏      | 188/595 [01:21<02:00,  3.37it/s] 32%|███▏      | 189/595 [01:22<02:00,  3.37it/s] 32%|███▏      | 190/595 [01:22<01:59,  3.38it/s] 32%|███▏      | 191/595 [01:22<01:58,  3.40it/s] 32%|███▏      | 192/595 [01:23<01:58,  3.41it/s] 32%|███▏      | 193/595 [01:23<01:57,  3.42it/s] 33%|███▎      | 194/595 [01:23<01:57,  3.42it/s] 33%|███▎      | 195/595 [01:24<01:56,  3.43it/s] 33%|███▎      | 196/595 [01:24<01:56,  3.43it/s] 33%|███▎      | 197/595 [01:24<01:55,  3.43it/s] 33%|███▎      | 198/595 [01:24<01:55,  3.43it/s] 33%|███▎      | 199/595 [01:25<01:55,  3.44it/s] 34%|███▎      | 200/595 [01:25<01:55,  3.43it/s] 34%|███▍      | 201/595 [01:25<01:57,  3.35it/s] 34%|███▍      | 202/595 [01:26<01:56,  3.38it/s] 34%|███▍      | 203/595 [01:26<01:55,  3.40it/s] 34%|███▍      | 204/595 [01:26<01:54,  3.41it/s] 34%|███▍      | 205/595 [01:26<01:54,  3.42it/s] 35%|███▍      | 206/595 [01:27<01:53,  3.43it/s] 35%|███▍      | 207/595 [01:27<01:53,  3.42it/s] 35%|███▍      | 208/595 [01:27<01:52,  3.43it/s] 35%|███▌      | 209/595 [01:28<01:52,  3.43it/s] 35%|███▌      | 210/595 [01:28<01:51,  3.44it/s] 35%|███▌      | 211/595 [01:28<01:51,  3.44it/s] 36%|███▌      | 212/595 [01:28<01:53,  3.39it/s] 36%|███▌      | 213/595 [01:29<01:52,  3.40it/s] 36%|███▌      | 214/595 [01:29<01:51,  3.42it/s] 36%|███▌      | 215/595 [01:29<01:50,  3.42it/s] 36%|███▋      | 216/595 [01:30<01:50,  3.43it/s] 36%|███▋      | 217/595 [01:30<01:50,  3.43it/s] 37%|███▋      | 218/595 [01:30<01:49,  3.44it/s] 37%|███▋      | 219/595 [01:31<01:49,  3.44it/s] 37%|███▋      | 220/595 [01:31<01:49,  3.44it/s] 37%|███▋      | 221/595 [01:31<01:48,  3.44it/s] 37%|███▋      | 222/595 [01:31<01:48,  3.44it/s] 37%|███▋      | 223/595 [01:32<01:48,  3.43it/s] 38%|███▊      | 224/595 [01:32<01:48,  3.43it/s] 38%|███▊      | 225/595 [01:32<01:47,  3.44it/s] 38%|███▊      | 226/595 [01:33<01:47,  3.44it/s] 38%|███▊      | 227/595 [01:33<01:47,  3.44it/s] 38%|███▊      | 228/595 [01:33<01:46,  3.44it/s] 38%|███▊      | 229/595 [01:33<01:46,  3.44it/s] 39%|███▊      | 230/595 [01:34<01:46,  3.44it/s] 39%|███▉      | 231/595 [01:34<01:45,  3.44it/s] 39%|███▉      | 232/595 [01:34<01:45,  3.44it/s] 39%|███▉      | 233/595 [01:35<01:45,  3.43it/s] 39%|███▉      | 234/595 [01:35<01:47,  3.35it/s] 39%|███▉      | 235/595 [01:35<01:46,  3.38it/s] 40%|███▉      | 236/595 [01:35<01:45,  3.39it/s] 40%|███▉      | 237/595 [01:36<01:45,  3.41it/s] 40%|████      | 238/595 [01:36<01:39,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 17:09:23,174 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:09:23,174 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 17:09:23,174 >>   Batch size = 8
{'eval_loss': 0.9011534452438354, 'eval_runtime': 13.8276, 'eval_samples_per_second': 353.062, 'eval_steps_per_second': 44.187, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.36it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.77it/s][A
  3%|▎         | 17/611 [00:00<00:12, 45.93it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.20it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.79it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.44it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.40it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.39it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.40it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.52it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.38it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.15it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.16it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.08it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.14it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.24it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.16it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.22it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.39it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.33it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.23it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.13it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.03it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.13it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.19it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.07it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.18it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.22it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.18it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.19it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.12it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.09it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.01it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.16it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.29it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.32it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.24it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.24it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.08it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.19it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.03it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.80it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.06it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.08it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.24it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.26it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.17it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.07it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.16it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.08it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.09it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.12it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.29it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.32it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.19it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.12it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.13it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.20it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.01it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.92it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.04it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.00it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.19it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.10it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.09it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.02it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.08it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.10it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 40.27it/s][A
 58%|█████▊    | 352/611 [00:07<00:06, 41.49it/s][A
 58%|█████▊    | 357/611 [00:08<00:06, 42.23it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 43.04it/s][A
 60%|██████    | 367/611 [00:08<00:05, 43.56it/s][A
 61%|██████    | 372/611 [00:08<00:05, 43.87it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 43.98it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 43.87it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.55it/s][A
 64%|██████▍   | 392/611 [00:08<00:05, 43.61it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 43.91it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.13it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.25it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.33it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.31it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.27it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.07it/s][A
 71%|███████   | 432/611 [00:09<00:04, 43.87it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.82it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 43.91it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.14it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.25it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.34it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.27it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.25it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.04it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.90it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.01it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.11it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.07it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.24it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.34it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.32it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.23it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.93it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.82it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.07it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.17it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.28it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.27it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.40it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.35it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.14it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.92it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.79it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.00it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.03it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.19it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.19it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.28it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.17it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.17it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 43.98it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 43.98it/s][A 40%|████      | 238/595 [01:50<01:39,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:09:37,094 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-238
[INFO|configuration_utils.py:351] 2023-08-28 17:09:37,131 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-238/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:09:40,695 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-238/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:09:40,720 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-238/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:09:40,729 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-238/special_tokens_map.json
 40%|████      | 239/595 [02:05<53:22,  9.00s/it] 40%|████      | 240/595 [02:06<37:46,  6.39s/it] 41%|████      | 241/595 [02:06<26:53,  4.56s/it] 41%|████      | 242/595 [02:06<19:17,  3.28s/it] 41%|████      | 243/595 [02:07<13:58,  2.38s/it] 41%|████      | 244/595 [02:07<10:16,  1.76s/it] 41%|████      | 245/595 [02:07<07:41,  1.32s/it] 41%|████▏     | 246/595 [02:07<05:52,  1.01s/it] 42%|████▏     | 247/595 [02:08<04:36,  1.26it/s] 42%|████▏     | 248/595 [02:08<03:43,  1.55it/s] 42%|████▏     | 249/595 [02:08<03:06,  1.85it/s] 42%|████▏     | 250/595 [02:09<02:40,  2.15it/s] 42%|████▏     | 251/595 [02:09<02:24,  2.38it/s] 42%|████▏     | 252/595 [02:09<02:10,  2.62it/s] 43%|████▎     | 253/595 [02:09<02:01,  2.82it/s] 43%|████▎     | 254/595 [02:10<01:54,  2.99it/s] 43%|████▎     | 255/595 [02:10<01:49,  3.11it/s] 43%|████▎     | 256/595 [02:10<01:45,  3.20it/s] 43%|████▎     | 257/595 [02:11<01:43,  3.28it/s] 43%|████▎     | 258/595 [02:11<01:41,  3.32it/s] 44%|████▎     | 259/595 [02:11<01:40,  3.36it/s] 44%|████▎     | 260/595 [02:12<01:38,  3.38it/s] 44%|████▍     | 261/595 [02:12<01:38,  3.41it/s] 44%|████▍     | 262/595 [02:12<01:39,  3.35it/s] 44%|████▍     | 263/595 [02:12<01:38,  3.38it/s] 44%|████▍     | 264/595 [02:13<01:37,  3.40it/s] 45%|████▍     | 265/595 [02:13<01:36,  3.42it/s] 45%|████▍     | 266/595 [02:13<01:36,  3.42it/s] 45%|████▍     | 267/595 [02:14<01:35,  3.43it/s] 45%|████▌     | 268/595 [02:14<01:35,  3.43it/s] 45%|████▌     | 269/595 [02:14<01:34,  3.44it/s] 45%|████▌     | 270/595 [02:14<01:34,  3.44it/s] 46%|████▌     | 271/595 [02:15<01:34,  3.44it/s] 46%|████▌     | 272/595 [02:15<01:33,  3.44it/s] 46%|████▌     | 273/595 [02:15<01:38,  3.27it/s] 46%|████▌     | 274/595 [02:16<01:36,  3.31it/s] 46%|████▌     | 275/595 [02:16<01:35,  3.35it/s] 46%|████▋     | 276/595 [02:16<01:34,  3.37it/s] 47%|████▋     | 277/595 [02:17<01:33,  3.39it/s] 47%|████▋     | 278/595 [02:17<01:33,  3.41it/s] 47%|████▋     | 279/595 [02:17<01:32,  3.41it/s] 47%|████▋     | 280/595 [02:17<01:32,  3.42it/s] 47%|████▋     | 281/595 [02:18<01:31,  3.43it/s] 47%|████▋     | 282/595 [02:18<01:31,  3.44it/s] 48%|████▊     | 283/595 [02:18<01:30,  3.44it/s] 48%|████▊     | 284/595 [02:19<01:31,  3.41it/s] 48%|████▊     | 285/595 [02:19<01:30,  3.42it/s] 48%|████▊     | 286/595 [02:19<01:30,  3.43it/s] 48%|████▊     | 287/595 [02:19<01:29,  3.43it/s] 48%|████▊     | 288/595 [02:20<01:29,  3.43it/s] 49%|████▊     | 289/595 [02:20<01:28,  3.44it/s] 49%|████▊     | 290/595 [02:20<01:28,  3.44it/s] 49%|████▉     | 291/595 [02:21<01:28,  3.44it/s] 49%|████▉     | 292/595 [02:21<01:28,  3.44it/s] 49%|████▉     | 293/595 [02:21<01:27,  3.44it/s] 49%|████▉     | 294/595 [02:21<01:27,  3.44it/s] 50%|████▉     | 295/595 [02:22<01:28,  3.41it/s] 50%|████▉     | 296/595 [02:22<01:27,  3.42it/s] 50%|████▉     | 297/595 [02:22<01:26,  3.43it/s] 50%|█████     | 298/595 [02:23<01:26,  3.43it/s] 50%|█████     | 299/595 [02:23<01:26,  3.43it/s] 50%|█████     | 300/595 [02:23<01:25,  3.44it/s] 51%|█████     | 301/595 [02:24<01:28,  3.32it/s] 51%|█████     | 302/595 [02:24<01:27,  3.35it/s] 51%|█████     | 303/595 [02:24<01:26,  3.37it/s] 51%|█████     | 304/595 [02:24<01:25,  3.40it/s] 51%|█████▏    | 305/595 [02:25<01:25,  3.41it/s] 51%|█████▏    | 306/595 [02:25<01:24,  3.42it/s] 52%|█████▏    | 307/595 [02:25<01:24,  3.42it/s] 52%|█████▏    | 308/595 [02:26<01:23,  3.43it/s] 52%|█████▏    | 309/595 [02:26<01:23,  3.43it/s] 52%|█████▏    | 310/595 [02:26<01:22,  3.44it/s] 52%|█████▏    | 311/595 [02:26<01:22,  3.44it/s] 52%|█████▏    | 312/595 [02:27<01:22,  3.42it/s] 53%|█████▎    | 313/595 [02:27<01:22,  3.41it/s] 53%|█████▎    | 314/595 [02:27<01:22,  3.41it/s] 53%|█████▎    | 315/595 [02:28<01:22,  3.40it/s] 53%|█████▎    | 316/595 [02:28<01:22,  3.40it/s] 53%|█████▎    | 317/595 [02:28<01:21,  3.40it/s] 53%|█████▎    | 318/595 [02:29<01:21,  3.39it/s] 54%|█████▎    | 319/595 [02:29<01:21,  3.39it/s] 54%|█████▍    | 320/595 [02:29<01:21,  3.39it/s] 54%|█████▍    | 321/595 [02:29<01:20,  3.39it/s] 54%|█████▍    | 322/595 [02:30<01:20,  3.40it/s] 54%|█████▍    | 323/595 [02:30<01:20,  3.39it/s] 54%|█████▍    | 324/595 [02:30<01:19,  3.39it/s] 55%|█████▍    | 325/595 [02:31<01:19,  3.41it/s] 55%|█████▍    | 326/595 [02:31<01:18,  3.42it/s] 55%|█████▍    | 327/595 [02:31<01:18,  3.42it/s] 55%|█████▌    | 328/595 [02:31<01:17,  3.43it/s] 55%|█████▌    | 329/595 [02:32<01:17,  3.43it/s] 55%|█████▌    | 330/595 [02:32<01:17,  3.43it/s] 56%|█████▌    | 331/595 [02:32<01:16,  3.43it/s] 56%|█████▌    | 332/595 [02:33<01:16,  3.44it/s] 56%|█████▌    | 333/595 [02:33<01:16,  3.44it/s] 56%|█████▌    | 334/595 [02:33<01:17,  3.39it/s] 56%|█████▋    | 335/595 [02:34<01:16,  3.40it/s] 56%|█████▋    | 336/595 [02:34<01:15,  3.42it/s] 57%|█████▋    | 337/595 [02:34<01:15,  3.42it/s] 57%|█████▋    | 338/595 [02:34<01:14,  3.43it/s] 57%|█████▋    | 339/595 [02:35<01:14,  3.43it/s] 57%|█████▋    | 340/595 [02:35<01:14,  3.43it/s] 57%|█████▋    | 341/595 [02:35<01:14,  3.43it/s] 57%|█████▋    | 342/595 [02:36<01:13,  3.44it/s] 58%|█████▊    | 343/595 [02:36<01:13,  3.43it/s] 58%|█████▊    | 344/595 [02:36<01:13,  3.44it/s] 58%|█████▊    | 345/595 [02:36<01:13,  3.40it/s] 58%|█████▊    | 346/595 [02:37<01:12,  3.41it/s] 58%|█████▊    | 347/595 [02:37<01:12,  3.42it/s] 58%|█████▊    | 348/595 [02:37<01:12,  3.43it/s] 59%|█████▊    | 349/595 [02:38<01:11,  3.43it/s] 59%|█████▉    | 350/595 [02:38<01:11,  3.44it/s] 59%|█████▉    | 351/595 [02:38<01:11,  3.43it/s] 59%|█████▉    | 352/595 [02:38<01:10,  3.44it/s] 59%|█████▉    | 353/595 [02:39<01:10,  3.44it/s] 59%|█████▉    | 354/595 [02:39<01:10,  3.44it/s] 60%|█████▉    | 355/595 [02:39<01:09,  3.44it/s] 60%|█████▉    | 356/595 [02:40<01:10,  3.40it/s] 60%|██████    | 357/595 [02:40<01:06,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 17:10:27,029 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:10:27,029 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 17:10:27,029 >>   Batch size = 8
{'eval_loss': 0.9058954119682312, 'eval_runtime': 13.8667, 'eval_samples_per_second': 352.066, 'eval_steps_per_second': 44.062, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.54it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.18it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.17it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.12it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.74it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.32it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.23it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.18it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.22it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.33it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.41it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.26it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.22it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.01it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.90it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.94it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.08it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.12it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.28it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.34it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.24it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.10it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.03it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.93it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.06it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.18it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.22it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.28it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.38it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.17it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.07it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.01it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.95it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.05it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.98it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.22it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.38it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.26it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.28it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.17it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.97it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.04it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.91it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.06it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.28it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.38it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.31it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.29it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.15it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.00it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.96it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.99it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.98it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.23it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.32it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.35it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.31it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.12it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.17it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.00it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.04it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.03it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.08it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.26it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.28it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.23it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.24it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.17it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.07it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.11it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.14it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.21it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.23it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.32it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.25it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.16it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.10it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.05it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.11it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.03it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.23it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.18it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.12it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.14it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.03it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.96it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.04it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.14it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.18it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.26it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.22it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.38it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.35it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.07it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.99it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 43.93it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.91it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.19it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.28it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.28it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.30it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.29it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.61it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.72it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.75it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 43.85it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.03it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.21it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.27it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.24it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.06it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.07it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.08it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.97it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.05it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.01it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.24it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.33it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.24it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.18it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.18it/s][A 60%|██████    | 357/595 [02:54<01:06,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:10:40,970 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-357
[INFO|configuration_utils.py:351] 2023-08-28 17:10:41,016 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-357/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:10:44,119 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-357/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:10:44,320 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-357/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:10:44,398 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-357/special_tokens_map.json
 60%|██████    | 358/595 [03:08<34:03,  8.62s/it] 60%|██████    | 359/595 [03:08<24:06,  6.13s/it] 61%|██████    | 360/595 [03:09<17:09,  4.38s/it] 61%|██████    | 361/595 [03:09<12:17,  3.15s/it] 61%|██████    | 362/595 [03:09<08:54,  2.29s/it] 61%|██████    | 363/595 [03:09<06:32,  1.69s/it] 61%|██████    | 364/595 [03:10<04:53,  1.27s/it] 61%|██████▏   | 365/595 [03:10<03:44,  1.02it/s] 62%|██████▏   | 366/595 [03:10<02:56,  1.30it/s] 62%|██████▏   | 367/595 [03:11<02:22,  1.60it/s] 62%|██████▏   | 368/595 [03:11<01:59,  1.90it/s] 62%|██████▏   | 369/595 [03:11<01:43,  2.19it/s] 62%|██████▏   | 370/595 [03:12<01:32,  2.42it/s] 62%|██████▏   | 371/595 [03:12<01:24,  2.65it/s] 63%|██████▎   | 372/595 [03:12<01:18,  2.84it/s] 63%|██████▎   | 373/595 [03:12<01:14,  2.99it/s] 63%|██████▎   | 374/595 [03:13<01:11,  3.10it/s] 63%|██████▎   | 375/595 [03:13<01:09,  3.18it/s] 63%|██████▎   | 376/595 [03:13<01:07,  3.25it/s] 63%|██████▎   | 377/595 [03:14<01:06,  3.29it/s] 64%|██████▎   | 378/595 [03:14<01:05,  3.32it/s] 64%|██████▎   | 379/595 [03:14<01:04,  3.35it/s] 64%|██████▍   | 380/595 [03:14<01:03,  3.36it/s] 64%|██████▍   | 381/595 [03:15<01:03,  3.34it/s] 64%|██████▍   | 382/595 [03:15<01:03,  3.36it/s] 64%|██████▍   | 383/595 [03:15<01:02,  3.37it/s] 65%|██████▍   | 384/595 [03:16<01:02,  3.38it/s] 65%|██████▍   | 385/595 [03:16<01:02,  3.39it/s] 65%|██████▍   | 386/595 [03:16<01:01,  3.39it/s] 65%|██████▌   | 387/595 [03:17<01:01,  3.40it/s] 65%|██████▌   | 388/595 [03:17<01:00,  3.40it/s] 65%|██████▌   | 389/595 [03:17<01:00,  3.40it/s] 66%|██████▌   | 390/595 [03:17<01:00,  3.40it/s] 66%|██████▌   | 391/595 [03:18<00:59,  3.40it/s] 66%|██████▌   | 392/595 [03:18<00:59,  3.39it/s] 66%|██████▌   | 393/595 [03:18<00:59,  3.39it/s] 66%|██████▌   | 394/595 [03:19<00:59,  3.39it/s] 66%|██████▋   | 395/595 [03:19<00:58,  3.40it/s] 67%|██████▋   | 396/595 [03:19<00:58,  3.39it/s] 67%|██████▋   | 397/595 [03:19<00:58,  3.39it/s] 67%|██████▋   | 398/595 [03:20<00:58,  3.39it/s] 67%|██████▋   | 399/595 [03:20<00:57,  3.39it/s] 67%|██████▋   | 400/595 [03:20<00:57,  3.40it/s] 67%|██████▋   | 401/595 [03:21<00:57,  3.39it/s] 68%|██████▊   | 402/595 [03:21<00:56,  3.39it/s] 68%|██████▊   | 403/595 [03:21<00:56,  3.39it/s] 68%|██████▊   | 404/595 [03:22<00:56,  3.39it/s] 68%|██████▊   | 405/595 [03:22<00:56,  3.39it/s] 68%|██████▊   | 406/595 [03:22<00:55,  3.39it/s] 68%|██████▊   | 407/595 [03:22<00:55,  3.39it/s] 69%|██████▊   | 408/595 [03:23<00:55,  3.39it/s] 69%|██████▊   | 409/595 [03:23<00:54,  3.39it/s] 69%|██████▉   | 410/595 [03:23<00:54,  3.39it/s] 69%|██████▉   | 411/595 [03:24<00:54,  3.39it/s] 69%|██████▉   | 412/595 [03:24<00:53,  3.39it/s] 69%|██████▉   | 413/595 [03:24<00:53,  3.39it/s] 70%|██████▉   | 414/595 [03:24<00:53,  3.38it/s] 70%|██████▉   | 415/595 [03:25<00:53,  3.38it/s] 70%|██████▉   | 416/595 [03:25<00:52,  3.39it/s] 70%|███████   | 417/595 [03:25<00:52,  3.40it/s] 70%|███████   | 418/595 [03:26<00:51,  3.41it/s] 70%|███████   | 419/595 [03:26<00:51,  3.42it/s] 71%|███████   | 420/595 [03:26<00:51,  3.43it/s] 71%|███████   | 421/595 [03:27<00:50,  3.43it/s] 71%|███████   | 422/595 [03:27<00:50,  3.44it/s] 71%|███████   | 423/595 [03:27<00:50,  3.44it/s] 71%|███████▏  | 424/595 [03:27<00:49,  3.44it/s] 71%|███████▏  | 425/595 [03:28<00:49,  3.44it/s] 72%|███████▏  | 426/595 [03:28<00:49,  3.43it/s] 72%|███████▏  | 427/595 [03:28<00:48,  3.43it/s] 72%|███████▏  | 428/595 [03:29<00:48,  3.44it/s] 72%|███████▏  | 429/595 [03:29<00:48,  3.44it/s] 72%|███████▏  | 430/595 [03:29<00:47,  3.44it/s] 72%|███████▏  | 431/595 [03:29<00:47,  3.44it/s] 73%|███████▎  | 432/595 [03:30<00:47,  3.44it/s] 73%|███████▎  | 433/595 [03:30<00:47,  3.44it/s] 73%|███████▎  | 434/595 [03:30<00:46,  3.44it/s] 73%|███████▎  | 435/595 [03:31<00:46,  3.44it/s] 73%|███████▎  | 436/595 [03:31<00:46,  3.44it/s] 73%|███████▎  | 437/595 [03:31<00:46,  3.42it/s] 74%|███████▎  | 438/595 [03:31<00:45,  3.42it/s] 74%|███████▍  | 439/595 [03:32<00:45,  3.43it/s] 74%|███████▍  | 440/595 [03:32<00:45,  3.43it/s] 74%|███████▍  | 441/595 [03:32<00:44,  3.43it/s] 74%|███████▍  | 442/595 [03:33<00:44,  3.44it/s] 74%|███████▍  | 443/595 [03:33<00:44,  3.44it/s] 75%|███████▍  | 444/595 [03:33<00:43,  3.44it/s] 75%|███████▍  | 445/595 [03:34<00:43,  3.44it/s] 75%|███████▍  | 446/595 [03:34<00:43,  3.44it/s] 75%|███████▌  | 447/595 [03:34<00:42,  3.44it/s] 75%|███████▌  | 448/595 [03:34<00:42,  3.43it/s] 75%|███████▌  | 449/595 [03:35<00:42,  3.43it/s] 76%|███████▌  | 450/595 [03:35<00:42,  3.44it/s] 76%|███████▌  | 451/595 [03:35<00:41,  3.43it/s] 76%|███████▌  | 452/595 [03:36<00:41,  3.44it/s] 76%|███████▌  | 453/595 [03:36<00:41,  3.44it/s] 76%|███████▋  | 454/595 [03:36<00:40,  3.44it/s] 76%|███████▋  | 455/595 [03:36<00:40,  3.44it/s] 77%|███████▋  | 456/595 [03:37<00:40,  3.44it/s] 77%|███████▋  | 457/595 [03:37<00:40,  3.44it/s] 77%|███████▋  | 458/595 [03:37<00:39,  3.44it/s] 77%|███████▋  | 459/595 [03:38<00:40,  3.36it/s] 77%|███████▋  | 460/595 [03:38<00:39,  3.38it/s] 77%|███████▋  | 461/595 [03:38<00:39,  3.40it/s] 78%|███████▊  | 462/595 [03:38<00:38,  3.41it/s] 78%|███████▊  | 463/595 [03:39<00:38,  3.42it/s] 78%|███████▊  | 464/595 [03:39<00:38,  3.42it/s] 78%|███████▊  | 465/595 [03:39<00:37,  3.43it/s] 78%|███████▊  | 466/595 [03:40<00:37,  3.43it/s] 78%|███████▊  | 467/595 [03:40<00:37,  3.43it/s] 79%|███████▊  | 468/595 [03:40<00:36,  3.44it/s] 79%|███████▉  | 469/595 [03:41<00:36,  3.44it/s] 79%|███████▉  | 470/595 [03:41<00:36,  3.38it/s] 79%|███████▉  | 471/595 [03:41<00:36,  3.40it/s] 79%|███████▉  | 472/595 [03:41<00:36,  3.41it/s] 79%|███████▉  | 473/595 [03:42<00:35,  3.42it/s] 80%|███████▉  | 474/595 [03:42<00:35,  3.43it/s] 80%|███████▉  | 475/595 [03:42<00:34,  3.43it/s] 80%|████████  | 476/595 [03:43<00:33,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 17:11:29,661 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:11:29,661 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 17:11:29,661 >>   Batch size = 8
{'eval_loss': 0.9134008884429932, 'eval_runtime': 13.837, 'eval_samples_per_second': 352.823, 'eval_steps_per_second': 44.157, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.29it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.58it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.05it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.21it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.94it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.53it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.26it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.25it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.42it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.45it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.40it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.20it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.19it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.15it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.05it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.01it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.04it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.18it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.30it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.32it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.22it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.19it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.06it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.90it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.74it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.01it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.25it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.29it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.40it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.25it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.19it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.14it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.00it/s][A
 28%|██▊       | 172/611 [00:03<00:10, 43.85it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.96it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.11it/s][A
 31%|███       | 187/611 [00:04<00:09, 43.21it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 43.61it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 43.75it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.88it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.83it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.70it/s][A
 36%|███▌      | 217/611 [00:04<00:09, 43.73it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.89it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.01it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 43.60it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.50it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.43it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.20it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.22it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.08it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.03it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.04it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.12it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.26it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.32it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.35it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.27it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.04it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.99it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.92it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.93it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.00it/s][A
 53%|█████▎    | 322/611 [00:07<00:07, 39.48it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 40.85it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 41.98it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 42.83it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.36it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.77it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.88it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.75it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 43.50it/s][A
 60%|██████    | 367/611 [00:08<00:05, 43.52it/s][A
 61%|██████    | 372/611 [00:08<00:05, 43.75it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 43.91it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.08it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.29it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.40it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 44.34it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.08it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 43.88it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 43.74it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 43.86it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.03it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.28it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.39it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.30it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.23it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.01it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 43.96it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 43.22it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 43.54it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.80it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.99it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.23it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.25it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.10it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.86it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 43.87it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 43.77it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 40.12it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 41.92it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 42.72it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.41it/s][A
 86%|████████▋ | 527/611 [00:12<00:01, 43.74it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 43.90it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 43.94it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 43.88it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 43.76it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 43.57it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.66it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.02it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.21it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 44.34it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.43it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.32it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.09it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 43.96it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 43.77it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.93it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.16it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.16it/s][A 80%|████████  | 476/595 [03:56<00:33,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:11:44,004 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-476
[INFO|configuration_utils.py:351] 2023-08-28 17:11:44,130 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-476/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:11:47,168 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-476/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:11:47,188 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-476/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:11:47,215 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-476/special_tokens_map.json
 80%|████████  | 477/595 [04:08<15:24,  7.84s/it] 80%|████████  | 478/595 [04:08<10:52,  5.58s/it] 81%|████████  | 479/595 [04:09<07:43,  3.99s/it] 81%|████████  | 480/595 [04:09<05:31,  2.88s/it] 81%|████████  | 481/595 [04:09<04:00,  2.11s/it] 81%|████████  | 482/595 [04:09<02:56,  1.56s/it] 81%|████████  | 483/595 [04:10<02:12,  1.18s/it] 81%|████████▏ | 484/595 [04:10<01:41,  1.09it/s] 82%|████████▏ | 485/595 [04:10<01:20,  1.37it/s] 82%|████████▏ | 486/595 [04:11<01:05,  1.67it/s] 82%|████████▏ | 487/595 [04:11<00:54,  1.97it/s] 82%|████████▏ | 488/595 [04:11<00:47,  2.26it/s] 82%|████████▏ | 489/595 [04:12<00:42,  2.50it/s] 82%|████████▏ | 490/595 [04:12<00:38,  2.71it/s] 83%|████████▎ | 491/595 [04:12<00:36,  2.89it/s] 83%|████████▎ | 492/595 [04:12<00:34,  3.03it/s] 83%|████████▎ | 493/595 [04:13<00:32,  3.13it/s] 83%|████████▎ | 494/595 [04:13<00:31,  3.20it/s] 83%|████████▎ | 495/595 [04:13<00:30,  3.26it/s] 83%|████████▎ | 496/595 [04:14<00:29,  3.30it/s] 84%|████████▎ | 497/595 [04:14<00:29,  3.33it/s] 84%|████████▎ | 498/595 [04:14<00:28,  3.35it/s] 84%|████████▍ | 499/595 [04:14<00:28,  3.36it/s] 84%|████████▍ | 500/595 [04:15<00:28,  3.36it/s]                                                  84%|████████▍ | 500/595 [04:15<00:28,  3.36it/s] 84%|████████▍ | 501/595 [04:15<00:27,  3.37it/s] 84%|████████▍ | 502/595 [04:15<00:27,  3.37it/s] 85%|████████▍ | 503/595 [04:16<00:27,  3.38it/s] 85%|████████▍ | 504/595 [04:16<00:26,  3.39it/s] 85%|████████▍ | 505/595 [04:16<00:26,  3.39it/s] 85%|████████▌ | 506/595 [04:17<00:26,  3.39it/s] 85%|████████▌ | 507/595 [04:17<00:25,  3.39it/s] 85%|████████▌ | 508/595 [04:17<00:25,  3.40it/s] 86%|████████▌ | 509/595 [04:17<00:25,  3.39it/s] 86%|████████▌ | 510/595 [04:18<00:25,  3.39it/s] 86%|████████▌ | 511/595 [04:18<00:24,  3.38it/s] 86%|████████▌ | 512/595 [04:18<00:24,  3.39it/s] 86%|████████▌ | 513/595 [04:19<00:24,  3.39it/s] 86%|████████▋ | 514/595 [04:19<00:23,  3.39it/s] 87%|████████▋ | 515/595 [04:19<00:23,  3.39it/s] 87%|████████▋ | 516/595 [04:19<00:23,  3.39it/s] 87%|████████▋ | 517/595 [04:20<00:23,  3.39it/s] 87%|████████▋ | 518/595 [04:20<00:22,  3.39it/s] 87%|████████▋ | 519/595 [04:20<00:22,  3.39it/s] 87%|████████▋ | 520/595 [04:21<00:22,  3.39it/s] 88%|████████▊ | 521/595 [04:21<00:21,  3.39it/s] 88%|████████▊ | 522/595 [04:21<00:21,  3.38it/s] 88%|████████▊ | 523/595 [04:22<00:21,  3.38it/s] 88%|████████▊ | 524/595 [04:22<00:20,  3.39it/s] 88%|████████▊ | 525/595 [04:22<00:20,  3.40it/s] 88%|████████▊ | 526/595 [04:22<00:20,  3.42it/s] 89%|████████▊ | 527/595 [04:23<00:19,  3.42it/s] 89%|████████▊ | 528/595 [04:23<00:19,  3.43it/s] 89%|████████▉ | 529/595 [04:23<00:19,  3.43it/s] 89%|████████▉ | 530/595 [04:24<00:18,  3.43it/s] 89%|████████▉ | 531/595 [04:24<00:18,  3.44it/s] 89%|████████▉ | 532/595 [04:24<00:18,  3.44it/s] 90%|████████▉ | 533/595 [04:24<00:18,  3.39it/s] 90%|████████▉ | 534/595 [04:25<00:17,  3.41it/s] 90%|████████▉ | 535/595 [04:25<00:17,  3.42it/s] 90%|█████████ | 536/595 [04:25<00:17,  3.43it/s] 90%|█████████ | 537/595 [04:26<00:16,  3.44it/s] 90%|█████████ | 538/595 [04:26<00:16,  3.44it/s] 91%|█████████ | 539/595 [04:26<00:16,  3.44it/s] 91%|█████████ | 540/595 [04:27<00:15,  3.45it/s] 91%|█████████ | 541/595 [04:27<00:15,  3.45it/s] 91%|█████████ | 542/595 [04:27<00:15,  3.44it/s] 91%|█████████▏| 543/595 [04:27<00:15,  3.45it/s] 91%|█████████▏| 544/595 [04:28<00:14,  3.45it/s] 92%|█████████▏| 545/595 [04:28<00:14,  3.44it/s] 92%|█████████▏| 546/595 [04:28<00:14,  3.44it/s] 92%|█████████▏| 547/595 [04:29<00:13,  3.44it/s] 92%|█████████▏| 548/595 [04:29<00:13,  3.45it/s] 92%|█████████▏| 549/595 [04:29<00:13,  3.45it/s] 92%|█████████▏| 550/595 [04:29<00:13,  3.43it/s] 93%|█████████▎| 551/595 [04:30<00:12,  3.44it/s] 93%|█████████▎| 552/595 [04:30<00:12,  3.44it/s] 93%|█████████▎| 553/595 [04:30<00:12,  3.45it/s] 93%|█████████▎| 554/595 [04:31<00:11,  3.44it/s] 93%|█████████▎| 555/595 [04:31<00:11,  3.44it/s] 93%|█████████▎| 556/595 [04:31<00:11,  3.44it/s] 94%|█████████▎| 557/595 [04:31<00:11,  3.44it/s] 94%|█████████▍| 558/595 [04:32<00:10,  3.44it/s] 94%|█████████▍| 559/595 [04:32<00:10,  3.44it/s] 94%|█████████▍| 560/595 [04:32<00:10,  3.44it/s] 94%|█████████▍| 561/595 [04:33<00:09,  3.40it/s] 94%|█████████▍| 562/595 [04:33<00:09,  3.41it/s] 95%|█████████▍| 563/595 [04:33<00:09,  3.42it/s] 95%|█████████▍| 564/595 [04:33<00:09,  3.42it/s] 95%|█████████▍| 565/595 [04:34<00:08,  3.43it/s] 95%|█████████▌| 566/595 [04:34<00:08,  3.43it/s] 95%|█████████▌| 567/595 [04:34<00:08,  3.44it/s] 95%|█████████▌| 568/595 [04:35<00:07,  3.44it/s] 96%|█████████▌| 569/595 [04:35<00:07,  3.44it/s] 96%|█████████▌| 570/595 [04:35<00:07,  3.44it/s] 96%|█████████▌| 571/595 [04:36<00:06,  3.44it/s] 96%|█████████▌| 572/595 [04:36<00:06,  3.41it/s] 96%|█████████▋| 573/595 [04:36<00:06,  3.42it/s] 96%|█████████▋| 574/595 [04:36<00:06,  3.42it/s] 97%|█████████▋| 575/595 [04:37<00:05,  3.43it/s] 97%|█████████▋| 576/595 [04:37<00:05,  3.43it/s] 97%|█████████▋| 577/595 [04:37<00:05,  3.43it/s] 97%|█████████▋| 578/595 [04:38<00:04,  3.43it/s] 97%|█████████▋| 579/595 [04:38<00:04,  3.43it/s] 97%|█████████▋| 580/595 [04:38<00:04,  3.43it/s] 98%|█████████▊| 581/595 [04:38<00:04,  3.43it/s] 98%|█████████▊| 582/595 [04:39<00:03,  3.43it/s] 98%|█████████▊| 583/595 [04:39<00:03,  3.41it/s] 98%|█████████▊| 584/595 [04:39<00:03,  3.41it/s] 98%|█████████▊| 585/595 [04:40<00:02,  3.41it/s] 98%|█████████▊| 586/595 [04:40<00:02,  3.42it/s] 99%|█████████▊| 587/595 [04:40<00:02,  3.42it/s] 99%|█████████▉| 588/595 [04:41<00:02,  3.43it/s] 99%|█████████▉| 589/595 [04:41<00:01,  3.43it/s] 99%|█████████▉| 590/595 [04:41<00:01,  3.43it/s] 99%|█████████▉| 591/595 [04:41<00:01,  3.44it/s] 99%|█████████▉| 592/595 [04:42<00:00,  3.44it/s]100%|█████████▉| 593/595 [04:42<00:00,  3.44it/s]100%|█████████▉| 594/595 [04:42<00:00,  3.25it/s]100%|██████████| 595/595 [04:43<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 17:12:29,696 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:12:29,697 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 17:12:29,697 >>   Batch size = 8
{'eval_loss': 0.9176284670829773, 'eval_runtime': 13.922, 'eval_samples_per_second': 350.667, 'eval_steps_per_second': 43.887, 'epoch': 4.0}
{'loss': 0.6327, 'learning_rate': 5.987394957983194e-06, 'epoch': 4.2}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.83it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.48it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.50it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.22it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.75it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.45it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.18it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.10it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.25it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.37it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.43it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.47it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.41it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.29it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.05it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.94it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.98it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.11it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.16it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.34it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.38it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 42.65it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.08it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.38it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.47it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.71it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.94it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 43.91it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.02it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 43.95it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.12it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.14it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.11it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.00it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.03it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.05it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.25it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.18it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.18it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.25it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.19it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.15it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.15it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.07it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.11it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 43.04it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 43.86it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 43.96it/s][A
 40%|████      | 247/611 [00:05<00:08, 43.29it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.65it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.76it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.89it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.82it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 43.93it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.09it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.09it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.18it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.28it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.29it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.07it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.06it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.11it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.16it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.22it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.21it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.23it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.24it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.19it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.30it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.22it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.19it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.17it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.25it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.23it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.27it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.15it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.26it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.17it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.17it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.19it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.15it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.23it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.25it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.28it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.24it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.20it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.18it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.22it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.14it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.13it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.25it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.25it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.20it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.27it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.11it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.27it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.12it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.12it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.22it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.25it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 43.78it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.13it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.71it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.94it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.94it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.00it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.11it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.11it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.15it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.21it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.01it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.06it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.10it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.15it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.24it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.20it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.15it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.15it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.14it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.17it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.18it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.18it/s][A100%|██████████| 595/595 [04:56<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:12:43,562 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-595
[INFO|configuration_utils.py:351] 2023-08-28 17:12:43,586 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-595/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:12:48,039 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-595/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:12:48,070 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-595/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:12:48,081 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-595/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:12:55,285 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:12:55,288 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-119 (score: 0.9011534452438354).
                                                 100%|██████████| 595/595 [05:13<00:00,  3.45it/s]100%|██████████| 595/595 [05:13<00:00,  1.90it/s]
[INFO|trainer.py:1894] 2023-08-28 17:12:59,762 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 17:12:59,796 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:13:05,479 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:13:05,537 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:13:05,585 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:13:05,879 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:05,879 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:05,879 >>   train_loss               =     0.6279
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:05,879 >>   train_runtime            = 0:05:13.07
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:05,879 >>   train_samples            =       7605
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:05,879 >>   train_samples_per_second =    121.458
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:05,880 >>   train_steps_per_second   =      1.901
{'eval_loss': 0.9215148687362671, 'eval_runtime': 13.851, 'eval_samples_per_second': 352.466, 'eval_steps_per_second': 44.112, 'epoch': 5.0}
{'train_runtime': 313.0702, 'train_samples_per_second': 121.458, 'train_steps_per_second': 1.901, 'train_loss': 0.6278855075355337, 'epoch': 5.0}
08/28/2023 17:13:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:13:05,918 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:13:05,919 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 17:13:05,919 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 56.43it/s]  2%|▏         | 12/611 [00:00<00:12, 49.09it/s]  3%|▎         | 17/611 [00:00<00:12, 47.44it/s]  4%|▎         | 22/611 [00:00<00:12, 46.46it/s]  4%|▍         | 27/611 [00:00<00:12, 45.87it/s]  5%|▌         | 32/611 [00:00<00:12, 45.41it/s]  6%|▌         | 37/611 [00:00<00:12, 45.43it/s]  7%|▋         | 42/611 [00:00<00:12, 44.94it/s]  8%|▊         | 47/611 [00:01<00:12, 44.44it/s]  9%|▊         | 52/611 [00:01<00:12, 44.05it/s]  9%|▉         | 57/611 [00:01<00:12, 44.07it/s] 10%|█         | 62/611 [00:01<00:12, 44.27it/s] 11%|█         | 67/611 [00:01<00:12, 44.37it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.56it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.62it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.76it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.53it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.32it/s] 16%|█▌        | 97/611 [00:02<00:11, 44.12it/s] 17%|█▋        | 102/611 [00:02<00:11, 44.10it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.14it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.36it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.52it/s] 20%|█▉        | 122/611 [00:02<00:11, 41.52it/s] 21%|██        | 127/611 [00:02<00:11, 42.47it/s] 22%|██▏       | 132/611 [00:02<00:11, 43.17it/s] 22%|██▏       | 137/611 [00:03<00:10, 43.47it/s] 23%|██▎       | 142/611 [00:03<00:10, 43.55it/s] 24%|██▍       | 147/611 [00:03<00:10, 43.71it/s] 25%|██▍       | 152/611 [00:03<00:10, 43.92it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.15it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.01it/s] 27%|██▋       | 167/611 [00:03<00:10, 44.08it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.26it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.39it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.41it/s] 31%|███       | 187/611 [00:04<00:09, 44.33it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.23it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.32it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.32it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.28it/s] 35%|███▍      | 212/611 [00:04<00:09, 44.24it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.39it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.49it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.40it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.32it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.37it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.34it/s] 40%|████      | 247/611 [00:05<00:08, 44.25it/s] 41%|████      | 252/611 [00:05<00:08, 44.18it/s] 42%|████▏     | 257/611 [00:05<00:08, 40.98it/s] 43%|████▎     | 262/611 [00:05<00:08, 42.15it/s] 44%|████▎     | 267/611 [00:06<00:08, 42.95it/s] 45%|████▍     | 272/611 [00:06<00:07, 43.47it/s] 45%|████▌     | 277/611 [00:06<00:07, 43.82it/s] 46%|████▌     | 282/611 [00:06<00:07, 43.94it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.05it/s] 48%|████▊     | 292/611 [00:06<00:07, 43.96it/s] 49%|████▊     | 297/611 [00:06<00:07, 43.73it/s] 49%|████▉     | 302/611 [00:06<00:07, 43.87it/s] 50%|█████     | 307/611 [00:06<00:06, 44.09it/s] 51%|█████     | 312/611 [00:07<00:06, 44.30it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.45it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.41it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.36it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.23it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.07it/s] 56%|█████▌    | 342/611 [00:07<00:06, 43.91it/s] 57%|█████▋    | 347/611 [00:07<00:06, 44.00it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.17it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.31it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.45it/s] 60%|██████    | 367/611 [00:08<00:05, 44.52it/s] 61%|██████    | 372/611 [00:08<00:05, 44.50it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.34it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.17it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.04it/s] 64%|██████▍   | 392/611 [00:09<00:07, 30.18it/s] 65%|██████▍   | 397/611 [00:09<00:06, 33.43it/s] 66%|██████▌   | 402/611 [00:09<00:05, 36.16it/s] 67%|██████▋   | 407/611 [00:09<00:05, 38.46it/s] 67%|██████▋   | 412/611 [00:09<00:04, 40.19it/s] 68%|██████▊   | 417/611 [00:09<00:04, 41.49it/s] 69%|██████▉   | 422/611 [00:09<00:04, 42.56it/s] 70%|██████▉   | 427/611 [00:09<00:04, 43.00it/s] 71%|███████   | 432/611 [00:09<00:04, 43.03it/s] 72%|███████▏  | 437/611 [00:10<00:04, 43.04it/s] 72%|███████▏  | 442/611 [00:10<00:03, 43.09it/s] 73%|███████▎  | 447/611 [00:10<00:03, 43.40it/s] 74%|███████▍  | 452/611 [00:10<00:03, 43.73it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.03it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.23it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.47it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.44it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.31it/s] 79%|███████▉  | 482/611 [00:11<00:02, 44.05it/s] 80%|███████▉  | 487/611 [00:11<00:02, 43.93it/s] 81%|████████  | 492/611 [00:11<00:02, 43.95it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.16it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.24it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.34it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.56it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.53it/s] 85%|████████▌ | 522/611 [00:11<00:02, 44.36it/s] 86%|████████▋ | 527/611 [00:12<00:01, 44.15it/s] 87%|████████▋ | 532/611 [00:12<00:01, 43.93it/s] 88%|████████▊ | 537/611 [00:12<00:01, 43.92it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.12it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.35it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.49it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.55it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.48it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.34it/s] 94%|█████████▎| 572/611 [00:13<00:00, 44.11it/s] 94%|█████████▍| 577/611 [00:13<00:00, 43.87it/s] 95%|█████████▌| 582/611 [00:13<00:00, 43.96it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.05it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.15it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.33it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.49it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.51it/s]100%|██████████| 611/611 [00:13<00:00, 43.69it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:13:19,922 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:19,922 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:19,922 >>   eval_loss               =     0.9012
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:19,922 >>   eval_runtime            = 0:00:14.00
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:19,922 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:19,922 >>   eval_samples_per_second =    348.644
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:19,922 >>   eval_steps_per_second   =     43.634
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:13:19,922 >>   perplexity              =     2.4624
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:28,016 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:28,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:28,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:28,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:28,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:13:28,718 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:13:28,719 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:13:29,315 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:13:30,360 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:13:30,360 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:34,303 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:34,312 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:34,313 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:34,313 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:34,313 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:13:35,153 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:13:35,154 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:13:35,710 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:13:35,877 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:13:35,877 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-119
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-238
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-476
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-357
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-595
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.46it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.69it/s]Extractor Predicting: 19it [00:11,  1.73it/s]Extractor Predicting: 20it [00:12,  1.73it/s]Extractor Predicting: 21it [00:12,  1.74it/s]Extractor Predicting: 22it [00:13,  1.79it/s]Extractor Predicting: 23it [00:13,  1.78it/s]Extractor Predicting: 24it [00:14,  1.78it/s]Extractor Predicting: 25it [00:14,  1.75it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.72it/s]Extractor Predicting: 28it [00:16,  1.72it/s]Extractor Predicting: 29it [00:17,  1.72it/s]Extractor Predicting: 30it [00:17,  1.73it/s]Extractor Predicting: 31it [00:18,  1.74it/s]Extractor Predicting: 32it [00:18,  1.78it/s]Extractor Predicting: 33it [00:19,  1.75it/s]Extractor Predicting: 34it [00:20,  1.72it/s]Extractor Predicting: 35it [00:20,  1.73it/s]Extractor Predicting: 36it [00:21,  1.70it/s]Extractor Predicting: 37it [00:21,  1.73it/s]Extractor Predicting: 38it [00:22,  1.73it/s]Extractor Predicting: 39it [00:23,  1.70it/s]Extractor Predicting: 40it [00:23,  1.73it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:24,  1.70it/s]Extractor Predicting: 43it [00:25,  1.70it/s]Extractor Predicting: 44it [00:25,  1.69it/s]Extractor Predicting: 45it [00:26,  1.72it/s]Extractor Predicting: 46it [00:27,  1.71it/s]Extractor Predicting: 47it [00:27,  1.71it/s]Extractor Predicting: 48it [00:28,  1.70it/s]Extractor Predicting: 49it [00:28,  1.70it/s]Extractor Predicting: 50it [00:29,  1.64it/s]Extractor Predicting: 51it [00:30,  1.64it/s]Extractor Predicting: 52it [00:30,  1.69it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:31,  1.66it/s]Extractor Predicting: 55it [00:32,  1.52it/s]Extractor Predicting: 56it [00:33,  1.51it/s]Extractor Predicting: 57it [00:33,  1.55it/s]Extractor Predicting: 58it [00:34,  1.56it/s]Extractor Predicting: 59it [00:35,  1.60it/s]Extractor Predicting: 60it [00:36,  1.45it/s]Extractor Predicting: 61it [00:36,  1.49it/s]Extractor Predicting: 62it [00:37,  1.50it/s]Extractor Predicting: 63it [00:37,  1.52it/s]Extractor Predicting: 64it [00:38,  1.55it/s]Extractor Predicting: 65it [00:39,  1.54it/s]Extractor Predicting: 66it [00:39,  1.52it/s]Extractor Predicting: 67it [00:40,  1.56it/s]Extractor Predicting: 68it [00:41,  1.60it/s]Extractor Predicting: 69it [00:41,  1.60it/s]Extractor Predicting: 70it [00:42,  1.57it/s]Extractor Predicting: 71it [00:43,  1.56it/s]Extractor Predicting: 72it [00:43,  1.54it/s]Extractor Predicting: 73it [00:44,  1.57it/s]Extractor Predicting: 74it [00:44,  1.58it/s]Extractor Predicting: 75it [00:45,  1.59it/s]Extractor Predicting: 76it [00:46,  1.59it/s]Extractor Predicting: 77it [00:46,  1.58it/s]Extractor Predicting: 78it [00:47,  1.61it/s]Extractor Predicting: 79it [00:48,  1.63it/s]Extractor Predicting: 80it [00:48,  1.62it/s]Extractor Predicting: 81it [00:49,  1.60it/s]Extractor Predicting: 82it [00:49,  1.58it/s]Extractor Predicting: 83it [00:50,  1.60it/s]Extractor Predicting: 84it [00:51,  1.59it/s]Extractor Predicting: 85it [00:51,  1.58it/s]Extractor Predicting: 86it [00:52,  1.48it/s]Extractor Predicting: 87it [00:53,  1.49it/s]Extractor Predicting: 88it [00:53,  1.49it/s]Extractor Predicting: 89it [00:54,  1.51it/s]Extractor Predicting: 90it [00:55,  1.52it/s]Extractor Predicting: 91it [00:55,  1.46it/s]Extractor Predicting: 92it [00:56,  1.54it/s]Extractor Predicting: 93it [00:57,  1.62it/s]Extractor Predicting: 94it [00:57,  1.63it/s]Extractor Predicting: 95it [00:58,  1.63it/s]Extractor Predicting: 96it [00:58,  1.63it/s]Extractor Predicting: 97it [00:59,  1.65it/s]Extractor Predicting: 98it [01:00,  1.48it/s]Extractor Predicting: 99it [01:01,  1.47it/s]Extractor Predicting: 100it [01:01,  1.53it/s]Extractor Predicting: 101it [01:02,  1.45it/s]Extractor Predicting: 102it [01:03,  1.47it/s]Extractor Predicting: 103it [01:03,  1.50it/s]Extractor Predicting: 104it [01:04,  1.54it/s]Extractor Predicting: 105it [01:04,  1.56it/s]Extractor Predicting: 106it [01:05,  1.62it/s]Extractor Predicting: 107it [01:06,  1.62it/s]Extractor Predicting: 108it [01:06,  1.65it/s]Extractor Predicting: 109it [01:07,  1.64it/s]Extractor Predicting: 110it [01:07,  1.65it/s]Extractor Predicting: 111it [01:08,  1.68it/s]Extractor Predicting: 112it [01:09,  1.67it/s]Extractor Predicting: 113it [01:09,  1.62it/s]Extractor Predicting: 114it [01:10,  1.61it/s]Extractor Predicting: 115it [01:10,  1.63it/s]Extractor Predicting: 116it [01:11,  1.59it/s]Extractor Predicting: 117it [01:12,  1.57it/s]Extractor Predicting: 118it [01:12,  1.58it/s]Extractor Predicting: 119it [01:13,  1.55it/s]Extractor Predicting: 120it [01:14,  1.53it/s]Extractor Predicting: 121it [01:14,  1.54it/s]Extractor Predicting: 122it [01:15,  1.55it/s]Extractor Predicting: 123it [01:16,  1.57it/s]Extractor Predicting: 124it [01:16,  1.58it/s]Extractor Predicting: 125it [01:17,  1.59it/s]Extractor Predicting: 126it [01:18,  1.59it/s]Extractor Predicting: 127it [01:18,  1.58it/s]Extractor Predicting: 128it [01:19,  1.59it/s]Extractor Predicting: 129it [01:19,  1.57it/s]Extractor Predicting: 130it [01:20,  1.54it/s]Extractor Predicting: 131it [01:21,  1.55it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.57it/s]Extractor Predicting: 134it [01:23,  1.57it/s]Extractor Predicting: 135it [01:23,  1.56it/s]Extractor Predicting: 136it [01:24,  1.58it/s]Extractor Predicting: 137it [01:25,  1.55it/s]Extractor Predicting: 138it [01:25,  1.57it/s]Extractor Predicting: 139it [01:26,  1.54it/s]Extractor Predicting: 140it [01:27,  1.54it/s]Extractor Predicting: 141it [01:27,  1.56it/s]Extractor Predicting: 142it [01:28,  1.57it/s]Extractor Predicting: 143it [01:28,  1.55it/s]Extractor Predicting: 144it [01:29,  1.57it/s]Extractor Predicting: 145it [01:30,  1.62it/s]Extractor Predicting: 146it [01:30,  1.60it/s]Extractor Predicting: 147it [01:31,  1.57it/s]Extractor Predicting: 148it [01:32,  1.58it/s]Extractor Predicting: 149it [01:32,  1.57it/s]Extractor Predicting: 150it [01:33,  1.55it/s]Extractor Predicting: 151it [01:34,  1.55it/s]Extractor Predicting: 152it [01:34,  1.55it/s]Extractor Predicting: 153it [01:35,  1.54it/s]Extractor Predicting: 154it [01:35,  1.54it/s]Extractor Predicting: 155it [01:36,  1.54it/s]Extractor Predicting: 156it [01:37,  1.49it/s]Extractor Predicting: 157it [01:38,  1.46it/s]Extractor Predicting: 158it [01:38,  1.43it/s]Extractor Predicting: 159it [01:39,  1.45it/s]Extractor Predicting: 160it [01:40,  1.49it/s]Extractor Predicting: 161it [01:40,  1.51it/s]Extractor Predicting: 162it [01:41,  1.52it/s]Extractor Predicting: 163it [01:42,  1.52it/s]Extractor Predicting: 164it [01:42,  1.56it/s]Extractor Predicting: 165it [01:43,  1.55it/s]Extractor Predicting: 166it [01:43,  1.58it/s]Extractor Predicting: 167it [01:44,  1.56it/s]Extractor Predicting: 168it [01:45,  1.57it/s]Extractor Predicting: 169it [01:45,  1.58it/s]Extractor Predicting: 170it [01:46,  1.57it/s]Extractor Predicting: 171it [01:47,  1.59it/s]Extractor Predicting: 172it [01:47,  1.61it/s]Extractor Predicting: 173it [01:48,  1.57it/s]Extractor Predicting: 174it [01:48,  1.58it/s]Extractor Predicting: 175it [01:49,  1.56it/s]Extractor Predicting: 176it [01:50,  1.58it/s]Extractor Predicting: 177it [01:50,  1.50it/s]Extractor Predicting: 178it [01:51,  1.52it/s]Extractor Predicting: 179it [01:52,  1.53it/s]Extractor Predicting: 180it [01:52,  1.54it/s]Extractor Predicting: 181it [01:53,  1.55it/s]Extractor Predicting: 182it [01:54,  1.41it/s]Extractor Predicting: 183it [01:55,  1.41it/s]Extractor Predicting: 184it [01:55,  1.46it/s]Extractor Predicting: 185it [01:56,  1.59it/s]Extractor Predicting: 185it [01:56,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:41,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:41,654 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:41,654 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:41,654 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:41,654 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:15:41,943 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:15:41,944 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:15:42,516 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:15:43,538 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:15:43,538 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:46,452 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:46,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:46,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:46,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:15:46,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:15:47,160 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:15:47,161 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:15:47,728 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:15:47,898 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:15:47,898 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3903846153846154,
  "recall": 0.04158131913150348,
  "score": 0.07515734912995187,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:02,  1.67it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.67it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:10,  1.61it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.68it/s]Extractor Predicting: 31it [00:18,  1.74it/s]Extractor Predicting: 32it [00:19,  1.74it/s]Extractor Predicting: 33it [00:20,  1.73it/s]Extractor Predicting: 34it [00:20,  1.73it/s]Extractor Predicting: 35it [00:21,  1.73it/s]Extractor Predicting: 36it [00:21,  1.72it/s]Extractor Predicting: 37it [00:22,  1.74it/s]Extractor Predicting: 38it [00:22,  1.73it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:24,  1.66it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.70it/s]Extractor Predicting: 43it [00:25,  1.71it/s]Extractor Predicting: 44it [00:26,  1.58it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:27,  1.69it/s]Extractor Predicting: 47it [00:28,  1.72it/s]Extractor Predicting: 48it [00:28,  1.71it/s]Extractor Predicting: 49it [00:29,  1.71it/s]Extractor Predicting: 50it [00:30,  1.71it/s]Extractor Predicting: 51it [00:30,  1.73it/s]Extractor Predicting: 52it [00:31,  1.73it/s]Extractor Predicting: 53it [00:31,  1.71it/s]Extractor Predicting: 54it [00:32,  1.68it/s]Extractor Predicting: 55it [00:33,  1.68it/s]Extractor Predicting: 56it [00:33,  1.68it/s]Extractor Predicting: 57it [00:34,  1.71it/s]Extractor Predicting: 58it [00:34,  1.77it/s]Extractor Predicting: 59it [00:35,  1.74it/s]Extractor Predicting: 60it [00:35,  1.71it/s]Extractor Predicting: 61it [00:36,  1.66it/s]Extractor Predicting: 62it [00:37,  1.62it/s]Extractor Predicting: 63it [00:37,  1.58it/s]Extractor Predicting: 64it [00:38,  1.55it/s]Extractor Predicting: 65it [00:39,  1.54it/s]Extractor Predicting: 66it [00:39,  1.53it/s]Extractor Predicting: 67it [00:40,  1.52it/s]Extractor Predicting: 68it [00:41,  1.53it/s]Extractor Predicting: 69it [00:41,  1.53it/s]Extractor Predicting: 70it [00:42,  1.55it/s]Extractor Predicting: 71it [00:43,  1.57it/s]Extractor Predicting: 72it [00:43,  1.59it/s]Extractor Predicting: 73it [00:44,  1.63it/s]Extractor Predicting: 74it [00:45,  1.59it/s]Extractor Predicting: 75it [00:45,  1.61it/s]Extractor Predicting: 76it [00:46,  1.63it/s]Extractor Predicting: 77it [00:46,  1.66it/s]Extractor Predicting: 78it [00:47,  1.65it/s]Extractor Predicting: 79it [00:47,  1.69it/s]Extractor Predicting: 80it [00:48,  1.73it/s]Extractor Predicting: 81it [00:49,  1.69it/s]Extractor Predicting: 82it [00:49,  1.71it/s]Extractor Predicting: 83it [00:50,  1.67it/s]Extractor Predicting: 84it [00:50,  1.65it/s]Extractor Predicting: 85it [00:51,  1.59it/s]Extractor Predicting: 86it [00:52,  1.58it/s]Extractor Predicting: 87it [00:52,  1.58it/s]Extractor Predicting: 88it [00:53,  1.61it/s]Extractor Predicting: 89it [00:54,  1.60it/s]Extractor Predicting: 90it [00:54,  1.61it/s]Extractor Predicting: 91it [00:55,  1.60it/s]Extractor Predicting: 92it [00:55,  1.59it/s]Extractor Predicting: 93it [00:56,  1.62it/s]Extractor Predicting: 94it [00:57,  1.63it/s]Extractor Predicting: 95it [00:57,  1.62it/s]Extractor Predicting: 96it [00:58,  1.61it/s]Extractor Predicting: 97it [00:59,  1.62it/s]Extractor Predicting: 98it [00:59,  1.59it/s]Extractor Predicting: 99it [01:00,  1.60it/s]Extractor Predicting: 100it [01:00,  1.59it/s]Extractor Predicting: 101it [01:01,  1.61it/s]Extractor Predicting: 102it [01:02,  1.62it/s]Extractor Predicting: 103it [01:02,  1.58it/s]Extractor Predicting: 104it [01:03,  1.62it/s]Extractor Predicting: 105it [01:04,  1.63it/s]Extractor Predicting: 106it [01:04,  1.66it/s]Extractor Predicting: 107it [01:05,  1.63it/s]Extractor Predicting: 108it [01:05,  1.65it/s]Extractor Predicting: 109it [01:06,  1.64it/s]Extractor Predicting: 110it [01:07,  1.66it/s]Extractor Predicting: 111it [01:07,  1.66it/s]Extractor Predicting: 112it [01:08,  1.63it/s]Extractor Predicting: 113it [01:08,  1.61it/s]Extractor Predicting: 114it [01:09,  1.61it/s]Extractor Predicting: 115it [01:10,  1.61it/s]Extractor Predicting: 116it [01:10,  1.60it/s]Extractor Predicting: 117it [01:11,  1.64it/s]Extractor Predicting: 118it [01:12,  1.60it/s]Extractor Predicting: 119it [01:12,  1.60it/s]Extractor Predicting: 120it [01:13,  1.63it/s]Extractor Predicting: 121it [01:13,  1.67it/s]Extractor Predicting: 122it [01:14,  1.65it/s]Extractor Predicting: 123it [01:15,  1.62it/s]Extractor Predicting: 124it [01:15,  1.60it/s]Extractor Predicting: 125it [01:16,  1.61it/s]Extractor Predicting: 126it [01:16,  1.60it/s]Extractor Predicting: 127it [01:17,  1.63it/s]Extractor Predicting: 128it [01:18,  1.58it/s]Extractor Predicting: 129it [01:18,  1.60it/s]Extractor Predicting: 130it [01:19,  1.64it/s]Extractor Predicting: 131it [01:20,  1.61it/s]Extractor Predicting: 132it [01:20,  1.61it/s]Extractor Predicting: 133it [01:21,  1.58it/s]Extractor Predicting: 134it [01:22,  1.41it/s]Extractor Predicting: 135it [01:22,  1.47it/s]Extractor Predicting: 136it [01:23,  1.55it/s]Extractor Predicting: 137it [01:24,  1.52it/s]Extractor Predicting: 138it [01:24,  1.56it/s]Extractor Predicting: 139it [01:25,  1.59it/s]Extractor Predicting: 140it [01:25,  1.61it/s]Extractor Predicting: 141it [01:26,  1.60it/s]Extractor Predicting: 142it [01:27,  1.63it/s]Extractor Predicting: 143it [01:27,  1.52it/s]Extractor Predicting: 144it [01:28,  1.60it/s]Extractor Predicting: 145it [01:29,  1.57it/s]Extractor Predicting: 146it [01:29,  1.61it/s]Extractor Predicting: 147it [01:30,  1.65it/s]Extractor Predicting: 148it [01:30,  1.63it/s]Extractor Predicting: 149it [01:31,  1.67it/s]Extractor Predicting: 150it [01:32,  1.67it/s]Extractor Predicting: 151it [01:32,  1.69it/s]Extractor Predicting: 152it [01:33,  1.65it/s]Extractor Predicting: 153it [01:33,  1.65it/s]Extractor Predicting: 154it [01:34,  1.61it/s]Extractor Predicting: 155it [01:35,  1.62it/s]Extractor Predicting: 156it [01:35,  1.67it/s]Extractor Predicting: 157it [01:36,  1.63it/s]Extractor Predicting: 158it [01:36,  1.62it/s]Extractor Predicting: 159it [01:37,  1.64it/s]Extractor Predicting: 160it [01:38,  1.64it/s]Extractor Predicting: 161it [01:38,  1.67it/s]Extractor Predicting: 162it [01:39,  1.65it/s]Extractor Predicting: 163it [01:39,  1.65it/s]Extractor Predicting: 164it [01:40,  1.63it/s]Extractor Predicting: 165it [01:41,  1.58it/s]Extractor Predicting: 166it [01:41,  1.56it/s]Extractor Predicting: 167it [01:42,  1.56it/s]Extractor Predicting: 168it [01:43,  1.57it/s]Extractor Predicting: 169it [01:43,  1.61it/s]Extractor Predicting: 170it [01:44,  1.64it/s]Extractor Predicting: 171it [01:44,  1.65it/s]Extractor Predicting: 172it [01:45,  1.68it/s]Extractor Predicting: 173it [01:46,  1.66it/s]Extractor Predicting: 174it [01:46,  1.63it/s]Extractor Predicting: 175it [01:47,  1.63it/s]Extractor Predicting: 176it [01:47,  1.65it/s]Extractor Predicting: 177it [01:48,  1.63it/s]Extractor Predicting: 178it [01:49,  1.62it/s]Extractor Predicting: 179it [01:49,  1.57it/s]Extractor Predicting: 180it [01:50,  1.61it/s]Extractor Predicting: 181it [01:51,  1.62it/s]Extractor Predicting: 182it [01:51,  1.64it/s]Extractor Predicting: 183it [01:52,  1.67it/s]Extractor Predicting: 184it [01:52,  1.64it/s]Extractor Predicting: 185it [01:53,  1.65it/s]Extractor Predicting: 186it [01:54,  1.61it/s]Extractor Predicting: 187it [01:54,  1.63it/s]Extractor Predicting: 188it [01:55,  1.63it/s]Extractor Predicting: 189it [01:56,  1.62it/s]Extractor Predicting: 190it [01:56,  1.64it/s]Extractor Predicting: 191it [01:57,  1.65it/s]Extractor Predicting: 192it [01:57,  1.69it/s]Extractor Predicting: 193it [01:58,  1.67it/s]Extractor Predicting: 194it [01:58,  1.67it/s]Extractor Predicting: 195it [01:59,  1.65it/s]Extractor Predicting: 196it [02:00,  1.65it/s]Extractor Predicting: 197it [02:00,  1.64it/s]Extractor Predicting: 198it [02:01,  1.62it/s]Extractor Predicting: 199it [02:02,  1.62it/s]Extractor Predicting: 200it [02:02,  1.62it/s]Extractor Predicting: 201it [02:03,  1.64it/s]Extractor Predicting: 202it [02:03,  1.64it/s]Extractor Predicting: 203it [02:04,  1.66it/s]Extractor Predicting: 204it [02:05,  1.66it/s]Extractor Predicting: 205it [02:05,  1.66it/s]Extractor Predicting: 206it [02:06,  1.65it/s]Extractor Predicting: 207it [02:06,  1.67it/s]Extractor Predicting: 208it [02:07,  1.65it/s]Extractor Predicting: 209it [02:08,  1.60it/s]Extractor Predicting: 210it [02:08,  1.67it/s]Extractor Predicting: 211it [02:09,  1.65it/s]Extractor Predicting: 212it [02:09,  1.67it/s]Extractor Predicting: 213it [02:10,  1.67it/s]Extractor Predicting: 214it [02:11,  1.69it/s]Extractor Predicting: 215it [02:11,  1.68it/s]Extractor Predicting: 216it [02:12,  1.65it/s]Extractor Predicting: 217it [02:12,  1.67it/s]Extractor Predicting: 218it [02:13,  1.64it/s]Extractor Predicting: 219it [02:14,  1.62it/s]Extractor Predicting: 220it [02:14,  1.64it/s]Extractor Predicting: 221it [02:15,  1.65it/s]Extractor Predicting: 222it [02:16,  1.60it/s]Extractor Predicting: 223it [02:16,  1.55it/s]Extractor Predicting: 224it [02:17,  1.57it/s]Extractor Predicting: 225it [02:17,  1.60it/s]Extractor Predicting: 226it [02:18,  1.64it/s]Extractor Predicting: 227it [02:19,  1.64it/s]Extractor Predicting: 228it [02:19,  1.67it/s]Extractor Predicting: 229it [02:20,  1.70it/s]Extractor Predicting: 230it [02:20,  1.69it/s]Extractor Predicting: 231it [02:21,  1.70it/s]Extractor Predicting: 232it [02:22,  1.69it/s]Extractor Predicting: 233it [02:22,  1.69it/s]Extractor Predicting: 234it [02:23,  1.69it/s]Extractor Predicting: 235it [02:23,  1.66it/s]Extractor Predicting: 236it [02:24,  1.66it/s]Extractor Predicting: 237it [02:25,  1.67it/s]Extractor Predicting: 238it [02:25,  1.64it/s]Extractor Predicting: 239it [02:26,  1.66it/s]Extractor Predicting: 240it [02:26,  1.65it/s]Extractor Predicting: 241it [02:27,  1.66it/s]Extractor Predicting: 242it [02:28,  1.64it/s]Extractor Predicting: 243it [02:28,  1.59it/s]Extractor Predicting: 244it [02:29,  1.60it/s]Extractor Predicting: 245it [02:29,  1.64it/s]Extractor Predicting: 246it [02:30,  1.63it/s]Extractor Predicting: 247it [02:31,  1.66it/s]Extractor Predicting: 248it [02:32,  1.39it/s]Extractor Predicting: 249it [02:32,  1.44it/s]Extractor Predicting: 250it [02:33,  1.50it/s]Extractor Predicting: 251it [02:34,  1.54it/s]Extractor Predicting: 252it [02:34,  1.55it/s]Extractor Predicting: 253it [02:35,  1.53it/s]Extractor Predicting: 254it [02:35,  1.52it/s]Extractor Predicting: 255it [02:36,  1.55it/s]Extractor Predicting: 256it [02:37,  1.57it/s]Extractor Predicting: 257it [02:37,  1.59it/s]Extractor Predicting: 258it [02:38,  1.59it/s]Extractor Predicting: 259it [02:39,  1.59it/s]Extractor Predicting: 260it [02:39,  1.57it/s]Extractor Predicting: 261it [02:40,  1.59it/s]Extractor Predicting: 262it [02:40,  1.58it/s]Extractor Predicting: 263it [02:41,  1.58it/s]Extractor Predicting: 264it [02:42,  1.59it/s]Extractor Predicting: 265it [02:42,  1.61it/s]Extractor Predicting: 266it [02:43,  1.56it/s]Extractor Predicting: 267it [02:44,  1.56it/s]Extractor Predicting: 268it [02:44,  1.54it/s]Extractor Predicting: 269it [02:45,  1.56it/s]Extractor Predicting: 270it [02:46,  1.54it/s]Extractor Predicting: 271it [02:46,  1.54it/s]Extractor Predicting: 272it [02:47,  1.55it/s]Extractor Predicting: 273it [02:48,  1.55it/s]Extractor Predicting: 274it [02:48,  1.53it/s]Extractor Predicting: 275it [02:49,  1.54it/s]Extractor Predicting: 276it [02:49,  1.56it/s]Extractor Predicting: 277it [02:50,  1.57it/s]Extractor Predicting: 278it [02:51,  1.53it/s]Extractor Predicting: 279it [02:51,  1.53it/s]Extractor Predicting: 280it [02:52,  1.55it/s]Extractor Predicting: 281it [02:53,  1.51it/s]Extractor Predicting: 282it [02:53,  1.52it/s]Extractor Predicting: 283it [02:54,  1.54it/s]Extractor Predicting: 284it [02:55,  1.57it/s]Extractor Predicting: 285it [02:55,  1.56it/s]Extractor Predicting: 286it [02:56,  1.59it/s]Extractor Predicting: 287it [02:57,  1.54it/s]Extractor Predicting: 288it [02:57,  1.56it/s]Extractor Predicting: 289it [02:58,  1.58it/s]Extractor Predicting: 290it [02:59,  1.55it/s]Extractor Predicting: 291it [02:59,  1.52it/s]Extractor Predicting: 292it [03:00,  1.56it/s]Extractor Predicting: 293it [03:00,  1.55it/s]Extractor Predicting: 294it [03:01,  1.53it/s]Extractor Predicting: 295it [03:02,  1.53it/s]Extractor Predicting: 296it [03:02,  1.57it/s]Extractor Predicting: 297it [03:03,  1.58it/s]Extractor Predicting: 298it [03:04,  1.56it/s]Extractor Predicting: 299it [03:04,  1.57it/s]Extractor Predicting: 300it [03:05,  1.56it/s]Extractor Predicting: 301it [03:06,  1.49it/s]Extractor Predicting: 302it [03:06,  1.48it/s]Extractor Predicting: 303it [03:07,  1.54it/s]Extractor Predicting: 304it [03:08,  1.57it/s]Extractor Predicting: 305it [03:08,  1.61it/s]Extractor Predicting: 306it [03:09,  1.63it/s]Extractor Predicting: 307it [03:09,  1.61it/s]Extractor Predicting: 308it [03:10,  1.62it/s]Extractor Predicting: 309it [03:11,  1.60it/s]Extractor Predicting: 310it [03:11,  1.60it/s]Extractor Predicting: 311it [03:12,  1.56it/s]Extractor Predicting: 312it [03:13,  1.58it/s]Extractor Predicting: 313it [03:13,  1.52it/s]Extractor Predicting: 314it [03:14,  1.53it/s]Extractor Predicting: 315it [03:15,  1.56it/s]Extractor Predicting: 316it [03:15,  1.59it/s]Extractor Predicting: 317it [03:16,  1.60it/s]Extractor Predicting: 318it [03:16,  1.63it/s]Extractor Predicting: 319it [03:17,  1.60it/s]Extractor Predicting: 320it [03:18,  1.57it/s]Extractor Predicting: 321it [03:18,  1.55it/s]Extractor Predicting: 322it [03:19,  1.58it/s]Extractor Predicting: 323it [03:20,  1.53it/s]Extractor Predicting: 324it [03:20,  1.55it/s]Extractor Predicting: 325it [03:21,  1.59it/s]Extractor Predicting: 326it [03:22,  1.58it/s]Extractor Predicting: 327it [03:22,  1.59it/s]Extractor Predicting: 328it [03:23,  1.56it/s]Extractor Predicting: 329it [03:23,  1.58it/s]Extractor Predicting: 330it [03:24,  1.56it/s]Extractor Predicting: 331it [03:25,  1.57it/s]Extractor Predicting: 332it [03:25,  1.57it/s]Extractor Predicting: 333it [03:26,  1.55it/s]Extractor Predicting: 334it [03:27,  1.57it/s]Extractor Predicting: 335it [03:27,  1.56it/s]Extractor Predicting: 336it [03:28,  1.46it/s]Extractor Predicting: 337it [03:29,  1.48it/s]Extractor Predicting: 338it [03:29,  1.49it/s]Extractor Predicting: 339it [03:30,  1.52it/s]Extractor Predicting: 340it [03:31,  1.52it/s]Extractor Predicting: 341it [03:31,  1.50it/s]Extractor Predicting: 342it [03:32,  1.51it/s]Extractor Predicting: 343it [03:33,  1.55it/s]Extractor Predicting: 344it [03:33,  1.55it/s]Extractor Predicting: 345it [03:34,  1.51it/s]Extractor Predicting: 346it [03:35,  1.52it/s]Extractor Predicting: 347it [03:35,  1.53it/s]Extractor Predicting: 348it [03:36,  1.36it/s]Extractor Predicting: 349it [03:37,  1.42it/s]Extractor Predicting: 350it [03:37,  1.45it/s]Extractor Predicting: 351it [03:38,  1.51it/s]Extractor Predicting: 352it [03:39,  1.49it/s]Extractor Predicting: 353it [03:39,  1.47it/s]Extractor Predicting: 354it [03:40,  1.52it/s]Extractor Predicting: 355it [03:41,  1.55it/s]Extractor Predicting: 356it [03:41,  1.55it/s]Extractor Predicting: 357it [03:42,  1.56it/s]Extractor Predicting: 358it [03:43,  1.57it/s]Extractor Predicting: 359it [03:43,  1.56it/s]Extractor Predicting: 360it [03:44,  1.53it/s]Extractor Predicting: 361it [03:45,  1.53it/s]Extractor Predicting: 362it [03:45,  1.55it/s]Extractor Predicting: 363it [03:46,  1.58it/s]Extractor Predicting: 364it [03:46,  1.59it/s]Extractor Predicting: 365it [03:47,  1.57it/s]Extractor Predicting: 366it [03:48,  1.53it/s]Extractor Predicting: 367it [03:48,  1.53it/s]Extractor Predicting: 368it [03:49,  1.49it/s]Extractor Predicting: 369it [03:50,  1.45it/s]Extractor Predicting: 370it [03:51,  1.46it/s]Extractor Predicting: 371it [03:51,  1.50it/s]Extractor Predicting: 372it [03:51,  1.79it/s]Extractor Predicting: 372it [03:51,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:51,410 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:51,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:51,418 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:51,418 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:51,418 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:19:52,238 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:19:52,239 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:19:52,827 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:19:53,884 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:19:53,885 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:57,132 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:57,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:57,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:57,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:19:57,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:19:57,781 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:19:57,782 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:19:58,361 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:19:58,531 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:19:58,531 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.23165760869565216,
  "recall": 0.03826301615798923,
  "score": 0.06567796610169492,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.70it/s]Extractor Predicting: 13it [00:07,  1.71it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.53it/s]Extractor Predicting: 17it [00:10,  1.51it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:14,  1.54it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:16,  1.53it/s]Extractor Predicting: 28it [00:17,  1.53it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:21,  1.62it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:23,  1.53it/s]Extractor Predicting: 39it [00:24,  1.49it/s]Extractor Predicting: 40it [00:25,  1.48it/s]Extractor Predicting: 41it [00:25,  1.49it/s]Extractor Predicting: 42it [00:26,  1.51it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:27,  1.53it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:29,  1.50it/s]Extractor Predicting: 48it [00:30,  1.50it/s]Extractor Predicting: 49it [00:31,  1.50it/s]Extractor Predicting: 50it [00:31,  1.50it/s]Extractor Predicting: 51it [00:32,  1.46it/s]Extractor Predicting: 52it [00:33,  1.46it/s]Extractor Predicting: 53it [00:33,  1.48it/s]Extractor Predicting: 54it [00:34,  1.47it/s]Extractor Predicting: 55it [00:35,  1.51it/s]Extractor Predicting: 56it [00:35,  1.51it/s]Extractor Predicting: 57it [00:36,  1.50it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:37,  1.48it/s]Extractor Predicting: 60it [00:38,  1.48it/s]Extractor Predicting: 61it [00:39,  1.37it/s]Extractor Predicting: 62it [00:40,  1.39it/s]Extractor Predicting: 63it [00:40,  1.53it/s]Extractor Predicting: 63it [00:40,  1.55it/s]
[INFO|configuration_utils.py:515] 2023-08-28 17:20:41,366 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:20:41,375 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:20:41,387 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:20:41,388 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 17:20:41,393 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:20:48,988 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 17:20:48,993 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 17:20:49,033 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:20:49,034 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:20:49,082 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:20:49,167 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:20:49,168 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:20:49,168 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:20:49,168 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:20:49,168 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:20:49,168 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4772727272727273,
  "recall": 0.018879232843871743,
  "score": 0.03632170654367253,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 17:20:49,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:50,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:50,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:51,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:52,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:52,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:53,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:53,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:54,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:54,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:55,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:56,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:56,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:57,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:58,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:58,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:59,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:20:59,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:00,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:01,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:01,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:02,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:02,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:03,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:04,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:31, 15.14s/it][WARNING|generation_utils.py:914] 2023-08-28 17:21:04,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:05,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:05,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:06,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:07,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:07,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:08,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:08,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:09,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:09,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:10,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:10,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:11,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:11,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:12,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:13,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:13,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:14,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:14,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:15,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:15,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:16,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:17,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:17,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:28<03:03, 14.08s/it][WARNING|generation_utils.py:914] 2023-08-28 17:21:18,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:18,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:19,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:19,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:20,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:20,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:21,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:21,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:22,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:22,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:23,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:23,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:24,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:24,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:25,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:25,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:26,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:26,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:27,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:27,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:38<02:27, 12.33s/it][WARNING|generation_utils.py:914] 2023-08-28 17:21:28,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:29,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:29,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:30,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:31,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:31,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:32,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:33,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:33,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:34,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:34,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:35,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:35,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:36,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:37,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:37,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:38,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:39,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:39,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:40,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:40,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:41,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:52<02:22, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-28 17:21:42,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:42,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:43,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:44,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:44,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:45,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:45,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:46,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:47,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:48,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:48,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:49,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:49,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:50,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:50,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:51,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:52,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:52,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:53,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:53,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:54,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:54,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:55,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:56,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:07<02:15, 13.51s/it][WARNING|generation_utils.py:914] 2023-08-28 17:21:56,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:57,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:57,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:58,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:59,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:21:59,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:00,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:00,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:01,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:02,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:02,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:03,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:04,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:04,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:05,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:05,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:06,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:06,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:07,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:08,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:08,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:09,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:20<02:00, 13.40s/it][WARNING|generation_utils.py:914] 2023-08-28 17:22:09,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:10,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:11,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:11,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:12,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:12,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:13,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:13,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:14,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:14,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:15,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:16,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:16,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:17,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:18,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:18,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:19,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:19,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:20,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:20,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:31<01:42, 12.78s/it][WARNING|generation_utils.py:914] 2023-08-28 17:22:21,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:22,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:22,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:23,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:24,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:24,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:25,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:26,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:26,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:27,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:27,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:28,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:28,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:29,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:30,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:30,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:31,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:31,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:32,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:32,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:33,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:34,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:45<01:30, 12.98s/it][WARNING|generation_utils.py:914] 2023-08-28 17:22:34,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:35,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:36,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:36,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:37,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:37,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:38,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:39,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:39,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:40,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:41,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:41,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:42,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:42,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:43,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:43,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:44,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:44,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:45,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:46,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:46,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:47,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:48,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:48,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:49,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:50,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:01<01:23, 13.91s/it][WARNING|generation_utils.py:914] 2023-08-28 17:22:50,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:51,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:52,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:52,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:53,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:53,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:55,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:55,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:56,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:56,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:57,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:58,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:58,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:22:59,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:00,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:00,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:01,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:01,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:02,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:03,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:03,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:14<01:08, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-28 17:23:04,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:04,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:05,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:05,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:06,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:06,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:07,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:07,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:08,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:08,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:09,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:09,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:10,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:10,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:11,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:11,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:12,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:12,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:13,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:13,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:24<00:50, 12.60s/it][WARNING|generation_utils.py:914] 2023-08-28 17:23:14,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:14,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:15,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:16,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:16,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:17,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:17,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:18,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:18,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:19,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:20,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:20,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:21,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:21,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:22,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:22,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:23,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:24,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:24,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:25,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:26,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:26,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:27,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:27,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:28,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:39<00:39, 13.23s/it][WARNING|generation_utils.py:914] 2023-08-28 17:23:28,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:29,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:30,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:30,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:31,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:31,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:32,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:33,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:33,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:34,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:35,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:35,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:36,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:37,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:37,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:38,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:38,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:39,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:40,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:40,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:41,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:41,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:42,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:53<00:26, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-28 17:23:43,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:43,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:44,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:44,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:44,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:45,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:46,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:46,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:47,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:47,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:48,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:48,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:49,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:49,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:50,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:50,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:50,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:51,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:52,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:02<00:12, 12.28s/it][WARNING|generation_utils.py:914] 2023-08-28 17:23:52,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:53,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:53,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:54,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:55,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:55,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:56,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:56,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:57,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:58,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:58,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:23:59,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:00,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:00,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:01,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:02,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:02,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:03,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:04,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:04,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:05,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:06,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:24:06,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:17<00:00, 13.03s/it]Generating: 100%|██████████| 15/15 [03:17<00:00, 13.17s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:19,208 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:19,271 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:19,272 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:19,272 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:19,272 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:24:20,252 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:24:20,253 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:24:20,834 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:24:21,929 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:24:21,929 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:26,365 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:26,374 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:26,374 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:26,374 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:24:26,374 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:24:27,308 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:24:27,309 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:24:28,074 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:24:28,311 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:24:28,311 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 435, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 487, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 559, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : conflict .', 'success_rate': 0.755, 'errors': {'', "('Imperial Japanese Navy', 'conflict', '', 'The Imperial Navy is a military body of the Imperial Japanese Navy headquartered in the Northern Yangon , China .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 409, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 471, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 565, 'raw': 576}
{'target': 600, 'success': 597, 'raw': 608}
{'target': 600, 'success': 629, 'raw': 640}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9828125, 'errors': {''}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : country of citizenship . Context : Steve Cottler ( born 8 September 1987 in Glamorgan , Ontario ) is an international football er who plays for Edmonton Eskimos of the Hockey East Division . Head Entity : Steve Cottler , Tail Entity : Canadian .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : creator .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : The song was nominated for the Grammy Award for Best New Artist at the 2004 MTV Video Music Awards , alongside artists such as Rick Ross , Rick Warren , and Nicki Minaj . Head Entity : Nicki Minaj , Tail Entity : alternative rock .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 271, 'raw': 384}
{'target': 600, 'success': 289, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 338, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 385, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 439, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 486, 'raw': 672}
{'target': 600, 'success': 506, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 559, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 612, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7355769230769231, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 591, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 444, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 607, 'raw': 800}
{'prompt': 'Relation : occupation .', 'success_rate': 0.75875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Chicago Fire', 'occupation', '', 'He is currently working for the Chicago Fire s new affiliate club , the Chicago RiverRiders , and is now coaching at a nearby school .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 288, 'raw': 288}
{'target': 600, 'success': 318, 'raw': 320}
{'target': 600, 'success': 350, 'raw': 352}
{'target': 600, 'success': 381, 'raw': 384}
{'target': 600, 'success': 413, 'raw': 416}
{'target': 600, 'success': 445, 'raw': 448}
{'target': 600, 'success': 477, 'raw': 480}
{'target': 600, 'success': 508, 'raw': 512}
{'target': 600, 'success': 540, 'raw': 544}
{'target': 600, 'success': 572, 'raw': 576}
{'target': 600, 'success': 603, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9917763157894737, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 10729
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10829, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.66it/s]Extractor Estimating: 2it [00:01,  1.54it/s]Extractor Estimating: 3it [00:01,  1.61it/s]Extractor Estimating: 4it [00:02,  1.69it/s]Extractor Estimating: 5it [00:03,  1.70it/s]Extractor Estimating: 6it [00:03,  1.74it/s]Extractor Estimating: 7it [00:04,  1.69it/s]Extractor Estimating: 8it [00:04,  1.71it/s]Extractor Estimating: 9it [00:05,  1.76it/s]Extractor Estimating: 10it [00:05,  1.73it/s]Extractor Estimating: 11it [00:06,  1.72it/s]Extractor Estimating: 12it [00:07,  1.68it/s]Extractor Estimating: 13it [00:07,  1.67it/s]Extractor Estimating: 14it [00:08,  1.71it/s]Extractor Estimating: 15it [00:08,  1.65it/s]Extractor Estimating: 16it [00:09,  1.59it/s]Extractor Estimating: 17it [00:10,  1.65it/s]Extractor Estimating: 18it [00:10,  1.64it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:11,  1.65it/s]Extractor Estimating: 21it [00:12,  1.65it/s]Extractor Estimating: 22it [00:13,  1.68it/s]Extractor Estimating: 23it [00:13,  1.69it/s]Extractor Estimating: 24it [00:14,  1.71it/s]Extractor Estimating: 25it [00:14,  1.75it/s]Extractor Estimating: 26it [00:15,  1.71it/s]Extractor Estimating: 27it [00:16,  1.72it/s]Extractor Estimating: 28it [00:16,  1.79it/s]Extractor Estimating: 29it [00:17,  1.80it/s]Extractor Estimating: 30it [00:17,  1.75it/s]Extractor Estimating: 31it [00:18,  1.79it/s]Extractor Estimating: 32it [00:18,  1.80it/s]Extractor Estimating: 33it [00:19,  1.76it/s]Extractor Estimating: 34it [00:19,  1.75it/s]Extractor Estimating: 35it [00:20,  1.80it/s]Extractor Estimating: 36it [00:21,  1.78it/s]Extractor Estimating: 37it [00:21,  1.77it/s]Extractor Estimating: 38it [00:22,  1.65it/s]Extractor Estimating: 39it [00:22,  1.67it/s]Extractor Estimating: 40it [00:23,  1.74it/s]Extractor Estimating: 41it [00:24,  1.72it/s]Extractor Estimating: 42it [00:24,  1.74it/s]Extractor Estimating: 43it [00:25,  1.75it/s]Extractor Estimating: 44it [00:25,  1.76it/s]Extractor Estimating: 45it [00:26,  1.72it/s]Extractor Estimating: 46it [00:26,  1.74it/s]Extractor Estimating: 47it [00:27,  1.78it/s]Extractor Estimating: 48it [00:28,  1.73it/s]Extractor Estimating: 49it [00:28,  1.68it/s]Extractor Estimating: 50it [00:29,  1.70it/s]Extractor Estimating: 51it [00:29,  1.82it/s]Extractor Estimating: 52it [00:30,  1.89it/s]Extractor Estimating: 53it [00:30,  1.96it/s]Extractor Estimating: 54it [00:31,  2.03it/s]Extractor Estimating: 55it [00:31,  2.13it/s]Extractor Estimating: 56it [00:31,  2.09it/s]Extractor Estimating: 57it [00:32,  2.16it/s]Extractor Estimating: 58it [00:32,  2.17it/s]Extractor Estimating: 59it [00:33,  2.14it/s]Extractor Estimating: 60it [00:33,  2.07it/s]Extractor Estimating: 61it [00:34,  2.02it/s]Extractor Estimating: 62it [00:34,  2.05it/s]Extractor Estimating: 63it [00:35,  2.06it/s]Extractor Estimating: 64it [00:35,  2.11it/s]Extractor Estimating: 65it [00:36,  2.15it/s]Extractor Estimating: 66it [00:36,  2.12it/s]Extractor Estimating: 67it [00:37,  2.07it/s]Extractor Estimating: 68it [00:37,  2.16it/s]Extractor Estimating: 69it [00:38,  2.12it/s]Extractor Estimating: 70it [00:38,  2.08it/s]Extractor Estimating: 71it [00:39,  2.08it/s]Extractor Estimating: 72it [00:39,  2.14it/s]Extractor Estimating: 73it [00:39,  2.19it/s]Extractor Estimating: 74it [00:40,  2.21it/s]Extractor Estimating: 75it [00:40,  2.17it/s]Extractor Estimating: 76it [00:41,  2.03it/s]Extractor Estimating: 77it [00:42,  1.99it/s]Extractor Estimating: 78it [00:42,  1.88it/s]Extractor Estimating: 79it [00:43,  1.84it/s]Extractor Estimating: 80it [00:43,  1.76it/s]Extractor Estimating: 81it [00:44,  1.71it/s]Extractor Estimating: 82it [00:45,  1.70it/s]Extractor Estimating: 83it [00:45,  1.70it/s]Extractor Estimating: 84it [00:46,  1.69it/s]Extractor Estimating: 85it [00:46,  1.74it/s]Extractor Estimating: 86it [00:47,  1.71it/s]Extractor Estimating: 87it [00:47,  1.73it/s]Extractor Estimating: 88it [00:48,  1.76it/s]Extractor Estimating: 89it [00:49,  1.70it/s]Extractor Estimating: 90it [00:49,  1.75it/s]Extractor Estimating: 91it [00:50,  1.73it/s]Extractor Estimating: 92it [00:50,  1.68it/s]Extractor Estimating: 93it [00:51,  1.66it/s]Extractor Estimating: 94it [00:52,  1.62it/s]Extractor Estimating: 95it [00:52,  1.68it/s]Extractor Estimating: 96it [00:53,  1.66it/s]Extractor Estimating: 97it [00:53,  1.62it/s]Extractor Estimating: 98it [00:54,  1.62it/s]Extractor Estimating: 99it [00:55,  1.60it/s]Extractor Estimating: 100it [00:55,  1.68it/s]Extractor Estimating: 101it [00:56,  1.70it/s]Extractor Estimating: 102it [00:56,  1.62it/s]Extractor Estimating: 103it [00:57,  1.56it/s]Extractor Estimating: 104it [00:58,  1.61it/s]Extractor Estimating: 105it [00:58,  1.62it/s]Extractor Estimating: 106it [00:59,  1.63it/s]Extractor Estimating: 107it [01:00,  1.64it/s]Extractor Estimating: 108it [01:00,  1.59it/s]Extractor Estimating: 109it [01:01,  1.63it/s]Extractor Estimating: 110it [01:01,  1.63it/s]Extractor Estimating: 111it [01:02,  1.68it/s]Extractor Estimating: 112it [01:03,  1.71it/s]Extractor Estimating: 113it [01:03,  1.68it/s]Extractor Estimating: 114it [01:04,  1.69it/s]Extractor Estimating: 115it [01:04,  1.74it/s]Extractor Estimating: 116it [01:05,  1.70it/s]Extractor Estimating: 117it [01:06,  1.68it/s]Extractor Estimating: 118it [01:06,  1.68it/s]Extractor Estimating: 119it [01:07,  1.70it/s]Extractor Estimating: 120it [01:07,  1.65it/s]Extractor Estimating: 121it [01:08,  1.61it/s]Extractor Estimating: 122it [01:09,  1.63it/s]Extractor Estimating: 123it [01:09,  1.66it/s]Extractor Estimating: 124it [01:10,  1.70it/s]Extractor Estimating: 125it [01:10,  1.67it/s]Extractor Estimating: 126it [01:11,  1.68it/s]Extractor Estimating: 127it [01:12,  1.66it/s]Extractor Estimating: 128it [01:12,  1.52it/s]Extractor Estimating: 129it [01:13,  1.58it/s]Extractor Estimating: 130it [01:14,  1.59it/s]Extractor Estimating: 131it [01:14,  1.57it/s]Extractor Estimating: 132it [01:15,  1.62it/s]Extractor Estimating: 133it [01:15,  1.66it/s]Extractor Estimating: 134it [01:16,  1.65it/s]Extractor Estimating: 135it [01:17,  1.65it/s]Extractor Estimating: 136it [01:17,  1.62it/s]Extractor Estimating: 137it [01:18,  1.63it/s]Extractor Estimating: 138it [01:18,  1.64it/s]Extractor Estimating: 139it [01:19,  1.67it/s]Extractor Estimating: 140it [01:20,  1.65it/s]Extractor Estimating: 141it [01:20,  1.59it/s]Extractor Estimating: 142it [01:21,  1.61it/s]Extractor Estimating: 143it [01:21,  1.64it/s]Extractor Estimating: 144it [01:22,  1.61it/s]Extractor Estimating: 145it [01:23,  1.63it/s]Extractor Estimating: 146it [01:23,  1.64it/s]Extractor Estimating: 147it [01:24,  1.62it/s]Extractor Estimating: 148it [01:25,  1.63it/s]Extractor Estimating: 149it [01:25,  1.63it/s]Extractor Estimating: 150it [01:26,  1.65it/s]Extractor Estimating: 151it [01:26,  1.65it/s]Extractor Estimating: 152it [01:27,  1.67it/s]Extractor Estimating: 153it [01:28,  1.66it/s]Extractor Estimating: 154it [01:28,  1.68it/s]Extractor Estimating: 155it [01:29,  1.70it/s]Extractor Estimating: 156it [01:29,  1.71it/s]Extractor Estimating: 157it [01:30,  1.67it/s]Extractor Estimating: 158it [01:30,  1.73it/s]Extractor Estimating: 159it [01:31,  1.68it/s]Extractor Estimating: 160it [01:32,  1.68it/s]Extractor Estimating: 161it [01:32,  1.66it/s]Extractor Estimating: 162it [01:33,  1.66it/s]Extractor Estimating: 163it [01:34,  1.62it/s]Extractor Estimating: 164it [01:34,  1.59it/s]Extractor Estimating: 165it [01:35,  1.57it/s]Extractor Estimating: 166it [01:36,  1.46it/s]Extractor Estimating: 167it [01:36,  1.49it/s]Extractor Estimating: 168it [01:37,  1.53it/s]Extractor Estimating: 169it [01:37,  1.59it/s]Extractor Estimating: 170it [01:38,  1.59it/s]Extractor Estimating: 171it [01:39,  1.59it/s]Extractor Estimating: 172it [01:39,  1.61it/s]Extractor Estimating: 173it [01:40,  1.62it/s]Extractor Estimating: 174it [01:41,  1.63it/s]Extractor Estimating: 175it [01:41,  1.66it/s]Extractor Estimating: 176it [01:42,  1.58it/s]Extractor Estimating: 177it [01:43,  1.53it/s]Extractor Estimating: 178it [01:43,  1.59it/s]Extractor Estimating: 179it [01:44,  1.62it/s]Extractor Estimating: 180it [01:44,  1.66it/s]Extractor Estimating: 181it [01:45,  1.64it/s]Extractor Estimating: 182it [01:46,  1.63it/s]Extractor Estimating: 183it [01:46,  1.58it/s]Extractor Estimating: 184it [01:47,  1.67it/s]Extractor Estimating: 185it [01:47,  1.60it/s]Extractor Estimating: 186it [01:48,  1.63it/s]Extractor Estimating: 187it [01:49,  1.64it/s]Extractor Estimating: 188it [01:49,  1.66it/s]Extractor Estimating: 189it [01:50,  1.63it/s]Extractor Estimating: 190it [01:50,  1.60it/s]Extractor Estimating: 191it [01:51,  1.55it/s]Extractor Estimating: 192it [01:52,  1.57it/s]Extractor Estimating: 193it [01:52,  1.60it/s]Extractor Estimating: 194it [01:53,  1.61it/s]Extractor Estimating: 195it [01:54,  1.63it/s]Extractor Estimating: 196it [01:54,  1.62it/s]Extractor Estimating: 197it [01:55,  1.64it/s]Extractor Estimating: 198it [01:55,  1.59it/s]Extractor Estimating: 199it [01:56,  1.62it/s]Extractor Estimating: 200it [01:57,  1.46it/s]Extractor Estimating: 201it [01:58,  1.49it/s]Extractor Estimating: 202it [01:58,  1.55it/s]Extractor Estimating: 203it [01:59,  1.62it/s]Extractor Estimating: 204it [01:59,  1.68it/s]Extractor Estimating: 205it [02:00,  1.64it/s]Extractor Estimating: 206it [02:00,  1.68it/s]Extractor Estimating: 207it [02:01,  1.66it/s]Extractor Estimating: 208it [02:02,  1.68it/s]Extractor Estimating: 209it [02:02,  1.65it/s]Extractor Estimating: 210it [02:03,  1.64it/s]Extractor Estimating: 211it [02:03,  1.64it/s]Extractor Estimating: 212it [02:04,  1.64it/s]Extractor Estimating: 213it [02:05,  1.63it/s]Extractor Estimating: 214it [02:05,  1.65it/s]Extractor Estimating: 215it [02:06,  1.66it/s]Extractor Estimating: 216it [02:06,  1.66it/s]Extractor Estimating: 217it [02:07,  1.66it/s]Extractor Estimating: 218it [02:08,  1.67it/s]Extractor Estimating: 219it [02:08,  1.69it/s]Extractor Estimating: 220it [02:09,  1.68it/s]Extractor Estimating: 221it [02:09,  1.72it/s]Extractor Estimating: 222it [02:10,  1.73it/s]Extractor Estimating: 223it [02:11,  1.76it/s]Extractor Estimating: 224it [02:11,  1.72it/s]Extractor Estimating: 225it [02:12,  1.69it/s]Extractor Estimating: 226it [02:12,  1.69it/s]Extractor Estimating: 227it [02:13,  1.69it/s]Extractor Estimating: 228it [02:14,  1.61it/s]Extractor Estimating: 229it [02:14,  1.63it/s]Extractor Estimating: 230it [02:15,  1.62it/s]Extractor Estimating: 231it [02:15,  1.65it/s]Extractor Estimating: 232it [02:16,  1.67it/s]Extractor Estimating: 233it [02:17,  1.63it/s]Extractor Estimating: 234it [02:17,  1.63it/s]Extractor Estimating: 235it [02:18,  1.62it/s]Extractor Estimating: 236it [02:19,  1.61it/s]Extractor Estimating: 237it [02:19,  1.62it/s]Extractor Estimating: 238it [02:20,  1.62it/s]Extractor Estimating: 239it [02:20,  1.61it/s]Extractor Estimating: 240it [02:21,  1.62it/s]Extractor Estimating: 241it [02:22,  1.61it/s]Extractor Estimating: 242it [02:22,  1.59it/s]Extractor Estimating: 243it [02:23,  1.66it/s]Extractor Estimating: 244it [02:23,  1.66it/s]Extractor Estimating: 245it [02:24,  1.69it/s]Extractor Estimating: 246it [02:25,  1.64it/s]Extractor Estimating: 247it [02:25,  1.60it/s]Extractor Estimating: 248it [02:26,  1.60it/s]Extractor Estimating: 249it [02:27,  1.62it/s]Extractor Estimating: 250it [02:27,  1.58it/s]Extractor Estimating: 251it [02:28,  1.56it/s]Extractor Estimating: 252it [02:28,  1.57it/s]Extractor Estimating: 253it [02:29,  1.60it/s]Extractor Estimating: 254it [02:30,  1.58it/s]Extractor Estimating: 255it [02:30,  1.57it/s]Extractor Estimating: 256it [02:31,  1.58it/s]Extractor Estimating: 257it [02:32,  1.57it/s]Extractor Estimating: 258it [02:32,  1.55it/s]Extractor Estimating: 259it [02:33,  1.55it/s]Extractor Estimating: 260it [02:34,  1.55it/s]Extractor Estimating: 261it [02:34,  1.56it/s]Extractor Estimating: 262it [02:35,  1.54it/s]Extractor Estimating: 263it [02:36,  1.55it/s]Extractor Estimating: 264it [02:36,  1.57it/s]Extractor Estimating: 265it [02:37,  1.57it/s]Extractor Estimating: 266it [02:37,  1.57it/s]Extractor Estimating: 267it [02:38,  1.58it/s]Extractor Estimating: 268it [02:39,  1.59it/s]Extractor Estimating: 269it [02:39,  1.46it/s]Extractor Estimating: 270it [02:40,  1.50it/s]Extractor Estimating: 271it [02:41,  1.52it/s]Extractor Estimating: 272it [02:41,  1.55it/s]Extractor Estimating: 273it [02:42,  1.56it/s]Extractor Estimating: 274it [02:43,  1.57it/s]Extractor Estimating: 275it [02:43,  1.51it/s]Extractor Estimating: 276it [02:44,  1.56it/s]Extractor Estimating: 277it [02:45,  1.59it/s]Extractor Estimating: 278it [02:45,  1.62it/s]Extractor Estimating: 279it [02:46,  1.68it/s]Extractor Estimating: 280it [02:46,  1.62it/s]Extractor Estimating: 281it [02:47,  1.65it/s]Extractor Estimating: 282it [02:47,  1.67it/s]Extractor Estimating: 283it [02:48,  1.60it/s]Extractor Estimating: 284it [02:49,  1.60it/s]Extractor Estimating: 285it [02:49,  1.60it/s]Extractor Estimating: 286it [02:50,  1.63it/s]Extractor Estimating: 287it [02:51,  1.65it/s]Extractor Estimating: 288it [02:51,  1.60it/s]Extractor Estimating: 289it [02:52,  1.61it/s]Extractor Estimating: 290it [02:52,  1.66it/s]Extractor Estimating: 291it [02:53,  1.63it/s]Extractor Estimating: 292it [02:54,  1.69it/s]Extractor Estimating: 293it [02:54,  1.67it/s]Extractor Estimating: 294it [02:55,  1.64it/s]Extractor Estimating: 295it [02:56,  1.58it/s]Extractor Estimating: 296it [02:56,  1.58it/s]Extractor Estimating: 297it [02:57,  1.61it/s]Extractor Estimating: 298it [02:57,  1.57it/s]Extractor Estimating: 299it [02:58,  1.62it/s]Extractor Estimating: 300it [02:59,  1.67it/s]Extractor Estimating: 301it [02:59,  1.70it/s]Extractor Estimating: 302it [03:00,  1.70it/s]Extractor Estimating: 303it [03:00,  1.65it/s]Extractor Estimating: 304it [03:01,  1.65it/s]Extractor Estimating: 305it [03:02,  1.67it/s]Extractor Estimating: 306it [03:02,  1.66it/s]Extractor Estimating: 307it [03:03,  1.61it/s]Extractor Estimating: 308it [03:03,  1.60it/s]Extractor Estimating: 309it [03:04,  1.61it/s]Extractor Estimating: 310it [03:05,  1.61it/s]Extractor Estimating: 311it [03:05,  1.62it/s]Extractor Estimating: 312it [03:06,  1.60it/s]Extractor Estimating: 313it [03:07,  1.60it/s]Extractor Estimating: 314it [03:07,  1.61it/s]Extractor Estimating: 315it [03:08,  1.58it/s]Extractor Estimating: 316it [03:08,  1.62it/s]Extractor Estimating: 317it [03:09,  1.66it/s]Extractor Estimating: 318it [03:10,  1.65it/s]Extractor Estimating: 319it [03:10,  1.65it/s]Extractor Estimating: 320it [03:11,  1.68it/s]Extractor Estimating: 321it [03:11,  1.68it/s]Extractor Estimating: 322it [03:12,  1.71it/s]Extractor Estimating: 323it [03:13,  1.73it/s]Extractor Estimating: 324it [03:13,  1.72it/s]Extractor Estimating: 325it [03:14,  1.69it/s]Extractor Estimating: 326it [03:14,  1.63it/s]Extractor Estimating: 327it [03:15,  1.61it/s]Extractor Estimating: 328it [03:16,  1.60it/s]Extractor Estimating: 329it [03:16,  1.60it/s]Extractor Estimating: 330it [03:17,  1.60it/s]Extractor Estimating: 331it [03:18,  1.62it/s]Extractor Estimating: 332it [03:18,  1.60it/s]Extractor Estimating: 333it [03:19,  1.61it/s]Extractor Estimating: 334it [03:19,  1.59it/s]Extractor Estimating: 335it [03:20,  1.58it/s]Extractor Estimating: 336it [03:21,  1.58it/s]Extractor Estimating: 337it [03:21,  1.57it/s]Extractor Estimating: 338it [03:22,  1.58it/s]Extractor Estimating: 339it [03:23,  1.58it/s]Extractor Estimating: 340it [03:23,  1.57it/s]Extractor Estimating: 341it [03:24,  1.44it/s]Extractor Estimating: 342it [03:25,  1.45it/s]Extractor Estimating: 343it [03:25,  1.50it/s]Extractor Estimating: 344it [03:26,  1.53it/s]Extractor Estimating: 345it [03:27,  1.53it/s]Extractor Estimating: 346it [03:27,  1.55it/s]Extractor Estimating: 347it [03:28,  1.56it/s]Extractor Estimating: 348it [03:29,  1.57it/s]Extractor Estimating: 349it [03:29,  1.57it/s]Extractor Estimating: 350it [03:30,  1.64it/s]Extractor Estimating: 351it [03:30,  1.63it/s]Extractor Estimating: 352it [03:31,  1.70it/s]Extractor Estimating: 353it [03:31,  1.70it/s]Extractor Estimating: 354it [03:32,  1.73it/s]Extractor Estimating: 355it [03:33,  1.75it/s]Extractor Estimating: 356it [03:33,  1.70it/s]Extractor Estimating: 357it [03:34,  1.72it/s]Extractor Estimating: 358it [03:34,  1.74it/s]Extractor Estimating: 359it [03:35,  1.77it/s]Extractor Estimating: 360it [03:35,  1.74it/s]Extractor Estimating: 361it [03:36,  1.79it/s]Extractor Estimating: 362it [03:37,  1.78it/s]Extractor Estimating: 363it [03:37,  1.76it/s]Extractor Estimating: 364it [03:38,  1.76it/s]Extractor Estimating: 365it [03:38,  1.72it/s]Extractor Estimating: 366it [03:39,  1.76it/s]Extractor Estimating: 367it [03:39,  1.76it/s]Extractor Estimating: 368it [03:40,  1.71it/s]Extractor Estimating: 369it [03:41,  1.72it/s]Extractor Estimating: 370it [03:41,  1.75it/s]Extractor Estimating: 371it [03:42,  1.74it/s]Extractor Estimating: 372it [03:42,  1.74it/s]Extractor Estimating: 373it [03:43,  1.74it/s]Extractor Estimating: 374it [03:43,  1.76it/s]Extractor Estimating: 374it [03:43,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:34,631 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:34,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:34,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:34,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:34,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:28:35,980 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:28:35,981 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:28:37,191 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:28:38,300 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:28:38,314 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:41,452 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:41,467 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:41,468 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:41,468 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:28:41,468 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:28:42,390 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:28:42,391 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:28:44,138 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:28:44,311 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:28:44,311 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:41:43,118 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:41:43,160 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7439 mean pseudo reward: 0.9538711542467653
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 22769
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22869, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22869, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.973, loss:675.4384
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.965, loss:623.8174
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.985, loss:603.0765
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 90, avg_time 0.985, loss:590.6210
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 190, avg_time 0.986, loss:605.7137
>> valid entity prec:0.5705, rec:0.4968, f1:0.5311
>> valid relation prec:0.1289, rec:0.0205, f1:0.0354
>> valid relation with NER prec:0.1289, rec:0.0205, f1:0.0354
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 290, avg_time 2.571, loss:616.3870
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 80, avg_time 0.983, loss:575.7912
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 180, avg_time 0.972, loss:588.6555
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 280, avg_time 0.974, loss:628.7400
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 70, avg_time 0.977, loss:614.9214
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5850, rec:0.4756, f1:0.5246
>> valid relation prec:0.1848, rec:0.0195, f1:0.0352
>> valid relation with NER prec:0.1848, rec:0.0195, f1:0.0352
g_step 1100, step 170, avg_time 2.569, loss:614.3008
g_step 1200, step 270, avg_time 0.973, loss:610.1822
g_step 1300, step 60, avg_time 0.991, loss:584.9572
g_step 1400, step 160, avg_time 0.977, loss:584.8413
g_step 1500, step 260, avg_time 0.979, loss:589.5303
>> valid entity prec:0.5879, rec:0.5524, f1:0.5696
>> valid relation prec:0.1861, rec:0.0514, f1:0.0806
>> valid relation with NER prec:0.1861, rec:0.0514, f1:0.0806
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 50, avg_time 2.590, loss:586.0098
g_step 1700, step 150, avg_time 0.971, loss:550.8796
g_step 1800, step 250, avg_time 0.981, loss:571.9372
g_step 1900, step 40, avg_time 0.980, loss:551.3161
g_step 2000, step 140, avg_time 0.979, loss:538.2010
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5154, rec:0.5955, f1:0.5526
>> valid relation prec:0.2012, rec:0.0467, f1:0.0758
>> valid relation with NER prec:0.2012, rec:0.0467, f1:0.0758
g_step 2100, step 240, avg_time 2.583, loss:557.3345
g_step 2200, step 30, avg_time 0.976, loss:541.2802
g_step 2300, step 130, avg_time 0.985, loss:520.5275
g_step 2400, step 230, avg_time 0.967, loss:518.7875
g_step 2500, step 20, avg_time 0.982, loss:535.8146
>> valid entity prec:0.5733, rec:0.5319, f1:0.5519
>> valid relation prec:0.2312, rec:0.0662, f1:0.1029
>> valid relation with NER prec:0.2312, rec:0.0662, f1:0.1029
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 120, avg_time 2.578, loss:485.6438
g_step 2700, step 220, avg_time 0.974, loss:520.4384
g_step 2800, step 10, avg_time 0.979, loss:532.3118
g_step 2900, step 110, avg_time 0.985, loss:475.2832
g_step 3000, step 210, avg_time 0.964, loss:497.3257
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4647, rec:0.5321, f1:0.4961
>> valid relation prec:0.1500, rec:0.0381, f1:0.0608
>> valid relation with NER prec:0.1500, rec:0.0381, f1:0.0608
g_step 3100, step 310, avg_time 2.576, loss:513.9836
g_step 3200, step 100, avg_time 0.967, loss:477.8697
g_step 3300, step 200, avg_time 0.982, loss:487.2938
g_step 3400, step 300, avg_time 0.981, loss:481.9243
g_step 3500, step 90, avg_time 0.979, loss:441.1732
>> valid entity prec:0.5722, rec:0.5047, f1:0.5363
>> valid relation prec:0.1583, rec:0.0445, f1:0.0694
>> valid relation with NER prec:0.1583, rec:0.0445, f1:0.0694
g_step 3600, step 190, avg_time 2.565, loss:474.5578
g_step 3700, step 290, avg_time 0.969, loss:473.4191
g_step 3800, step 80, avg_time 0.976, loss:433.4376
g_step 3900, step 180, avg_time 0.981, loss:452.1331
g_step 4000, step 280, avg_time 0.969, loss:460.2036
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5455, rec:0.5221, f1:0.5335
>> valid relation prec:0.1575, rec:0.0451, f1:0.0701
>> valid relation with NER prec:0.1575, rec:0.0451, f1:0.0701
g_step 4100, step 70, avg_time 2.573, loss:423.4238
g_step 4200, step 170, avg_time 0.974, loss:432.2397
g_step 4300, step 270, avg_time 0.977, loss:450.1708
g_step 4400, step 60, avg_time 0.976, loss:430.5376
g_step 4500, step 160, avg_time 0.971, loss:428.2842
>> valid entity prec:0.5418, rec:0.5760, f1:0.5584
>> valid relation prec:0.1567, rec:0.0598, f1:0.0866
>> valid relation with NER prec:0.1567, rec:0.0598, f1:0.0866
g_step 4600, step 260, avg_time 2.581, loss:434.4290
g_step 4700, step 50, avg_time 0.984, loss:425.8686
g_step 4800, step 150, avg_time 0.981, loss:414.7677
g_step 4900, step 250, avg_time 0.972, loss:418.9788
g_step 5000, step 40, avg_time 0.974, loss:404.0554
learning rate was adjusted to 0.0008
>> valid entity prec:0.5726, rec:0.4987, f1:0.5331
>> valid relation prec:0.1745, rec:0.0644, f1:0.0940
>> valid relation with NER prec:0.1745, rec:0.0644, f1:0.0940
g_step 5100, step 140, avg_time 2.562, loss:400.7488
g_step 5200, step 240, avg_time 0.975, loss:396.3622
g_step 5300, step 30, avg_time 0.982, loss:421.4356
g_step 5400, step 130, avg_time 0.974, loss:390.9329
g_step 5500, step 230, avg_time 0.992, loss:396.4972
>> valid entity prec:0.5267, rec:0.5255, f1:0.5261
>> valid relation prec:0.1449, rec:0.0594, f1:0.0843
>> valid relation with NER prec:0.1449, rec:0.0594, f1:0.0843
g_step 5600, step 20, avg_time 2.557, loss:381.2757
g_step 5700, step 120, avg_time 0.985, loss:359.8799
g_step 5800, step 220, avg_time 0.979, loss:402.2948
g_step 5900, step 10, avg_time 0.975, loss:404.1268
g_step 6000, step 110, avg_time 0.976, loss:362.4906
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5740, rec:0.4335, f1:0.4940
>> valid relation prec:0.1386, rec:0.0330, f1:0.0533
>> valid relation with NER prec:0.1386, rec:0.0330, f1:0.0533
g_step 6100, step 210, avg_time 2.571, loss:386.1184
g_step 6200, step 310, avg_time 0.971, loss:373.2704
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:41:43 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:41:43 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-41-43_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:41:44 - WARNING - datasets.builder -   Using custom data configuration default-b574d17260556795
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b574d17260556795/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:41:45,061 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:41:45,062 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:41:45,062 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:41:45,063 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:41:45,122 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:41:45,131 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:41:45,131 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:41:45,131 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:41:45,131 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:41:45,131 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:41:45,131 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:41:45,313 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:41:48,390 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:41:48,395 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b574d17260556795/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.64ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.61ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.09ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.33ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.46ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.55ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.61ba/s]100%|██████████| 8/8 [00:01<00:00,  5.46ba/s]100%|██████████| 8/8 [00:01<00:00,  4.58ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.12ba/s] 40%|████      | 2/5 [00:00<00:00,  4.39ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.46ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.51ba/s]100%|██████████| 5/5 [00:01<00:00,  4.70ba/s]100%|██████████| 5/5 [00:01<00:00,  4.45ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.15ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.50ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.43ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.02ba/s]100%|██████████| 8/8 [00:00<00:00,  8.64ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.18ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.30ba/s]100%|██████████| 5/5 [00:00<00:00, 10.97ba/s]100%|██████████| 5/5 [00:00<00:00, 10.72ba/s]
[INFO|trainer.py:414] 2023-08-28 19:41:53,733 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:41:53,811 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:41:53,811 >>   Num examples = 7505
[INFO|trainer.py:1149] 2023-08-28 19:41:53,811 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:41:53,811 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:41:53,811 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:41:53,811 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:41:53,811 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.32it/s]  0%|          | 2/585 [00:00<02:51,  3.39it/s]  1%|          | 3/585 [00:00<02:54,  3.33it/s]  1%|          | 4/585 [00:01<02:51,  3.38it/s]  1%|          | 5/585 [00:01<02:50,  3.40it/s]  1%|          | 6/585 [00:01<02:49,  3.42it/s]  1%|          | 7/585 [00:02<02:48,  3.43it/s]  1%|▏         | 8/585 [00:02<02:48,  3.43it/s]  2%|▏         | 9/585 [00:02<02:47,  3.43it/s]  2%|▏         | 10/585 [00:02<02:47,  3.44it/s]  2%|▏         | 11/585 [00:03<02:46,  3.44it/s]  2%|▏         | 12/585 [00:03<02:46,  3.44it/s]  2%|▏         | 13/585 [00:03<02:46,  3.44it/s]  2%|▏         | 14/585 [00:04<02:52,  3.31it/s]  3%|▎         | 15/585 [00:04<02:50,  3.35it/s]  3%|▎         | 16/585 [00:04<02:48,  3.38it/s]  3%|▎         | 17/585 [00:05<02:47,  3.40it/s]  3%|▎         | 18/585 [00:05<02:46,  3.41it/s]  3%|▎         | 19/585 [00:05<02:45,  3.42it/s]  3%|▎         | 20/585 [00:05<02:44,  3.43it/s]  4%|▎         | 21/585 [00:06<02:44,  3.43it/s]  4%|▍         | 22/585 [00:06<02:43,  3.43it/s]  4%|▍         | 23/585 [00:06<02:43,  3.44it/s]  4%|▍         | 24/585 [00:07<02:43,  3.44it/s]  4%|▍         | 25/585 [00:07<02:43,  3.43it/s]  4%|▍         | 26/585 [00:07<02:42,  3.43it/s]  5%|▍         | 27/585 [00:07<02:42,  3.44it/s]  5%|▍         | 28/585 [00:08<02:42,  3.44it/s]  5%|▍         | 29/585 [00:08<02:41,  3.44it/s]  5%|▌         | 30/585 [00:08<02:41,  3.44it/s]  5%|▌         | 31/585 [00:09<02:40,  3.44it/s]  5%|▌         | 32/585 [00:09<02:40,  3.44it/s]  6%|▌         | 33/585 [00:09<02:40,  3.44it/s]  6%|▌         | 34/585 [00:09<02:40,  3.44it/s]  6%|▌         | 35/585 [00:10<02:39,  3.44it/s]  6%|▌         | 36/585 [00:10<02:44,  3.34it/s]  6%|▋         | 37/585 [00:10<02:42,  3.37it/s]  6%|▋         | 38/585 [00:11<02:41,  3.39it/s]  7%|▋         | 39/585 [00:11<02:40,  3.40it/s]  7%|▋         | 40/585 [00:11<02:39,  3.42it/s]  7%|▋         | 41/585 [00:12<02:38,  3.42it/s]  7%|▋         | 42/585 [00:12<02:38,  3.43it/s]  7%|▋         | 43/585 [00:12<02:37,  3.43it/s]  8%|▊         | 44/585 [00:12<02:37,  3.43it/s]  8%|▊         | 45/585 [00:13<02:37,  3.43it/s]  8%|▊         | 46/585 [00:13<02:37,  3.43it/s]  8%|▊         | 47/585 [00:13<02:43,  3.28it/s]  8%|▊         | 48/585 [00:14<02:41,  3.33it/s]  8%|▊         | 49/585 [00:14<02:39,  3.36it/s]  9%|▊         | 50/585 [00:14<02:38,  3.38it/s]  9%|▊         | 51/585 [00:14<02:37,  3.40it/s]  9%|▉         | 52/585 [00:15<02:36,  3.41it/s]  9%|▉         | 53/585 [00:15<02:35,  3.42it/s]  9%|▉         | 54/585 [00:15<02:35,  3.42it/s]  9%|▉         | 55/585 [00:16<02:34,  3.43it/s] 10%|▉         | 56/585 [00:16<02:34,  3.43it/s] 10%|▉         | 57/585 [00:16<02:33,  3.43it/s] 10%|▉         | 58/585 [00:17<02:34,  3.40it/s] 10%|█         | 59/585 [00:17<02:34,  3.41it/s] 10%|█         | 60/585 [00:17<02:33,  3.42it/s] 10%|█         | 61/585 [00:17<02:33,  3.42it/s] 11%|█         | 62/585 [00:18<02:32,  3.42it/s] 11%|█         | 63/585 [00:18<02:32,  3.43it/s] 11%|█         | 64/585 [00:18<02:31,  3.43it/s] 11%|█         | 65/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 69/585 [00:20<02:32,  3.39it/s] 12%|█▏        | 70/585 [00:20<02:31,  3.41it/s] 12%|█▏        | 71/585 [00:20<02:30,  3.41it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.42it/s] 12%|█▏        | 73/585 [00:21<02:29,  3.42it/s] 13%|█▎        | 74/585 [00:21<02:29,  3.43it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.43it/s] 13%|█▎        | 76/585 [00:22<02:28,  3.43it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.43it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.43it/s] 14%|█▎        | 79/585 [00:23<02:27,  3.43it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.40it/s] 14%|█▍        | 81/585 [00:23<02:27,  3.41it/s] 14%|█▍        | 82/585 [00:24<02:27,  3.42it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.42it/s] 14%|█▍        | 84/585 [00:24<02:26,  3.43it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:25<02:25,  3.43it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.43it/s] 15%|█▌        | 89/585 [00:26<02:24,  3.43it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.43it/s] 16%|█▌        | 91/585 [00:26<02:24,  3.41it/s] 16%|█▌        | 92/585 [00:26<02:24,  3.42it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 94/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.43it/s] 16%|█▋        | 96/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 97/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.43it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.43it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.44it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.44it/s] 17%|█▋        | 102/585 [00:29<02:22,  3.39it/s] 18%|█▊        | 103/585 [00:30<02:21,  3.40it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.41it/s] 18%|█▊        | 105/585 [00:30<02:20,  3.42it/s] 18%|█▊        | 106/585 [00:31<02:19,  3.43it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.43it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.43it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.43it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 113/585 [00:33<02:21,  3.34it/s] 19%|█▉        | 114/585 [00:33<02:19,  3.37it/s] 20%|█▉        | 115/585 [00:33<02:18,  3.38it/s] 20%|█▉        | 116/585 [00:33<02:17,  3.40it/s] 20%|██        | 117/585 [00:34<02:17,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 19:42:28,105 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:42:28,105 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:42:28,105 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.14it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.73it/s][A
  3%|▎         | 17/611 [00:00<00:12, 45.98it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.29it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.82it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.43it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.25it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.03it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.18it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.28it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.30it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.23it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.10it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.04it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.05it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.99it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.94it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.99it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.16it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.14it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.07it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.15it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.04it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.00it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.95it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 43.99it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.04it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.10it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.06it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.10it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.12it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.99it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.99it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.97it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 43.93it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.10it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.07it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.11it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.97it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.05it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.03it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.95it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.01it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.02it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.05it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.10it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.00it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.01it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.99it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.08it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.95it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.00it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.11it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.01it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.14it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.02it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 43.99it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.95it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.96it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.96it/s][A
 51%|█████     | 312/611 [00:07<00:06, 43.95it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.10it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.05it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.11it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.07it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.97it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 39.80it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 41.17it/s][A
 58%|█████▊    | 352/611 [00:08<00:06, 42.13it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 42.88it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 43.28it/s][A
 60%|██████    | 367/611 [00:08<00:05, 43.63it/s][A
 61%|██████    | 372/611 [00:08<00:05, 43.88it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 43.76it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 43.49it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.47it/s][A
 64%|██████▍   | 392/611 [00:08<00:05, 43.74it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 43.99it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.16it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.23it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.22it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.21it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.00it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.83it/s][A
 71%|███████   | 432/611 [00:09<00:04, 43.73it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.89it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.14it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.11it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.21it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.19it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.07it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.93it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.87it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.81it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.92it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.20it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.20it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.29it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.24it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.08it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.98it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.75it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.88it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.99it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.20it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.27it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.14it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.17it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.00it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.90it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.86it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.86it/s][A
 94%|█████████▎| 572/611 [00:13<00:00, 44.07it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.25it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.22it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.27it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.08it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.12it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.92it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 43.90it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 43.90it/s][A 20%|██        | 117/585 [00:48<02:17,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:42:42,082 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 19:42:42,111 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:42:47,149 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:42:47,197 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:42:47,216 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:02<1:07:07,  8.62s/it] 20%|██        | 119/585 [01:02<47:36,  6.13s/it]   21%|██        | 120/585 [01:02<33:56,  4.38s/it] 21%|██        | 121/585 [01:03<24:23,  3.15s/it] 21%|██        | 122/585 [01:03<17:43,  2.30s/it] 21%|██        | 123/585 [01:03<13:03,  1.70s/it] 21%|██        | 124/585 [01:04<09:48,  1.28s/it] 21%|██▏       | 125/585 [01:04<07:31,  1.02it/s] 22%|██▏       | 126/585 [01:04<05:55,  1.29it/s] 22%|██▏       | 127/585 [01:04<04:49,  1.58it/s] 22%|██▏       | 128/585 [01:05<04:02,  1.88it/s] 22%|██▏       | 129/585 [01:05<03:29,  2.18it/s] 22%|██▏       | 130/585 [01:05<03:07,  2.43it/s] 22%|██▏       | 131/585 [01:06<02:50,  2.66it/s] 23%|██▎       | 132/585 [01:06<02:39,  2.84it/s] 23%|██▎       | 133/585 [01:06<02:31,  2.99it/s] 23%|██▎       | 134/585 [01:07<02:25,  3.10it/s] 23%|██▎       | 135/585 [01:07<02:21,  3.18it/s] 23%|██▎       | 136/585 [01:07<02:18,  3.24it/s] 23%|██▎       | 137/585 [01:07<02:16,  3.28it/s] 24%|██▎       | 138/585 [01:08<02:14,  3.32it/s] 24%|██▍       | 139/585 [01:08<02:13,  3.33it/s] 24%|██▍       | 140/585 [01:08<02:12,  3.35it/s] 24%|██▍       | 141/585 [01:09<02:13,  3.34it/s] 24%|██▍       | 142/585 [01:09<02:12,  3.36it/s] 24%|██▍       | 143/585 [01:09<02:11,  3.37it/s] 25%|██▍       | 144/585 [01:10<02:10,  3.37it/s] 25%|██▍       | 145/585 [01:10<02:10,  3.38it/s] 25%|██▍       | 146/585 [01:10<02:09,  3.38it/s] 25%|██▌       | 147/585 [01:10<02:09,  3.38it/s] 25%|██▌       | 148/585 [01:11<02:08,  3.39it/s] 25%|██▌       | 149/585 [01:11<02:08,  3.39it/s] 26%|██▌       | 150/585 [01:11<02:08,  3.39it/s] 26%|██▌       | 151/585 [01:12<02:07,  3.39it/s] 26%|██▌       | 152/585 [01:12<02:16,  3.18it/s] 26%|██▌       | 153/585 [01:12<02:13,  3.23it/s] 26%|██▋       | 154/585 [01:13<02:11,  3.28it/s] 26%|██▋       | 155/585 [01:13<02:09,  3.31it/s] 27%|██▋       | 156/585 [01:13<02:08,  3.33it/s] 27%|██▋       | 157/585 [01:13<02:07,  3.35it/s] 27%|██▋       | 158/585 [01:14<02:06,  3.36it/s] 27%|██▋       | 159/585 [01:14<02:06,  3.37it/s] 27%|██▋       | 160/585 [01:14<02:06,  3.37it/s] 28%|██▊       | 161/585 [01:15<02:05,  3.38it/s] 28%|██▊       | 162/585 [01:15<02:10,  3.24it/s] 28%|██▊       | 163/585 [01:15<02:08,  3.28it/s] 28%|██▊       | 164/585 [01:16<02:07,  3.31it/s] 28%|██▊       | 165/585 [01:16<02:06,  3.33it/s] 28%|██▊       | 166/585 [01:16<02:05,  3.35it/s] 29%|██▊       | 167/585 [01:16<02:04,  3.36it/s] 29%|██▊       | 168/585 [01:17<02:03,  3.37it/s] 29%|██▉       | 169/585 [01:17<02:03,  3.37it/s] 29%|██▉       | 170/585 [01:17<02:02,  3.38it/s] 29%|██▉       | 171/585 [01:18<02:02,  3.38it/s] 29%|██▉       | 172/585 [01:18<02:09,  3.18it/s] 30%|██▉       | 173/585 [01:18<02:07,  3.24it/s] 30%|██▉       | 174/585 [01:19<02:05,  3.28it/s] 30%|██▉       | 175/585 [01:19<02:03,  3.31it/s] 30%|███       | 176/585 [01:19<02:02,  3.34it/s] 30%|███       | 177/585 [01:19<02:01,  3.35it/s] 30%|███       | 178/585 [01:20<02:01,  3.36it/s] 31%|███       | 179/585 [01:20<02:00,  3.37it/s] 31%|███       | 180/585 [01:20<01:59,  3.38it/s] 31%|███       | 181/585 [01:21<01:59,  3.38it/s] 31%|███       | 182/585 [01:21<02:09,  3.12it/s] 31%|███▏      | 183/585 [01:21<02:05,  3.20it/s] 31%|███▏      | 184/585 [01:22<02:03,  3.25it/s] 32%|███▏      | 185/585 [01:22<02:01,  3.29it/s] 32%|███▏      | 186/585 [01:22<02:00,  3.32it/s] 32%|███▏      | 187/585 [01:22<01:59,  3.33it/s] 32%|███▏      | 188/585 [01:23<01:58,  3.35it/s] 32%|███▏      | 189/585 [01:23<01:57,  3.36it/s] 32%|███▏      | 190/585 [01:23<01:57,  3.37it/s] 33%|███▎      | 191/585 [01:24<01:56,  3.37it/s] 33%|███▎      | 192/585 [01:24<01:56,  3.38it/s] 33%|███▎      | 193/585 [01:24<01:56,  3.38it/s] 33%|███▎      | 194/585 [01:25<02:04,  3.14it/s] 33%|███▎      | 195/585 [01:25<02:01,  3.21it/s] 34%|███▎      | 196/585 [01:25<01:59,  3.26it/s] 34%|███▎      | 197/585 [01:25<01:57,  3.30it/s] 34%|███▍      | 198/585 [01:26<01:56,  3.32it/s] 34%|███▍      | 199/585 [01:26<01:55,  3.34it/s] 34%|███▍      | 200/585 [01:26<01:54,  3.35it/s] 34%|███▍      | 201/585 [01:27<01:54,  3.36it/s] 35%|███▍      | 202/585 [01:27<01:53,  3.37it/s] 35%|███▍      | 203/585 [01:27<01:53,  3.37it/s] 35%|███▍      | 204/585 [01:28<01:53,  3.36it/s] 35%|███▌      | 205/585 [01:28<01:52,  3.36it/s] 35%|███▌      | 206/585 [01:28<01:52,  3.37it/s] 35%|███▌      | 207/585 [01:28<01:51,  3.38it/s] 36%|███▌      | 208/585 [01:29<01:51,  3.38it/s] 36%|███▌      | 209/585 [01:29<01:51,  3.38it/s] 36%|███▌      | 210/585 [01:29<01:51,  3.38it/s] 36%|███▌      | 211/585 [01:30<01:50,  3.38it/s] 36%|███▌      | 212/585 [01:30<01:50,  3.38it/s] 36%|███▋      | 213/585 [01:30<01:49,  3.38it/s] 37%|███▋      | 214/585 [01:31<01:49,  3.38it/s] 37%|███▋      | 215/585 [01:31<01:50,  3.35it/s] 37%|███▋      | 216/585 [01:31<01:49,  3.36it/s] 37%|███▋      | 217/585 [01:31<01:49,  3.37it/s] 37%|███▋      | 218/585 [01:32<01:48,  3.37it/s] 37%|███▋      | 219/585 [01:32<01:48,  3.38it/s] 38%|███▊      | 220/585 [01:32<01:48,  3.38it/s] 38%|███▊      | 221/585 [01:33<01:47,  3.38it/s] 38%|███▊      | 222/585 [01:33<01:47,  3.37it/s] 38%|███▊      | 223/585 [01:33<01:47,  3.38it/s] 38%|███▊      | 224/585 [01:33<01:46,  3.38it/s] 38%|███▊      | 225/585 [01:34<01:46,  3.38it/s] 39%|███▊      | 226/585 [01:34<01:47,  3.34it/s] 39%|███▉      | 227/585 [01:34<01:46,  3.35it/s] 39%|███▉      | 228/585 [01:35<01:46,  3.36it/s] 39%|███▉      | 229/585 [01:35<01:45,  3.36it/s] 39%|███▉      | 230/585 [01:35<01:45,  3.37it/s] 39%|███▉      | 231/585 [01:36<01:44,  3.37it/s] 40%|███▉      | 232/585 [01:36<01:44,  3.38it/s] 40%|███▉      | 233/585 [01:36<01:44,  3.38it/s] 40%|████      | 234/585 [01:36<01:43,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 19:43:30,816 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:43:30,816 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:43:30,816 >>   Batch size = 8
{'eval_loss': 0.9035747647285461, 'eval_runtime': 13.9361, 'eval_samples_per_second': 350.312, 'eval_steps_per_second': 43.843, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.59it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.24it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.31it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.61it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.83it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.48it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.30it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.01it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.16it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.44it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.52it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.48it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.28it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.10it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.97it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.70it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.98it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.95it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.23it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.47it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.31it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.37it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.24it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.00it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.98it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.97it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.08it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.35it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.33it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.43it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.33it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.15it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.04it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.90it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 43.89it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.15it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.29it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.42it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.33it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.37it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.23it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.95it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.98it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.07it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.12it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.38it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.37it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.38it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.26it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.03it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.98it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.81it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.16it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.18it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.29it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.31it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 43.55it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 43.77it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.72it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.85it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.83it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.04it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.15it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.33it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.24it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.25it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.13it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.07it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.05it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.12it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.22it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.24it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.27it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.20it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.18it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.12it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.00it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.17it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.21it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.22it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.30it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.27it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 42.58it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.02it/s][A
 71%|███████   | 432/611 [00:09<00:04, 43.27it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 43.52it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 43.69it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 43.93it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.05it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 42.12it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.04it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.12it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.25it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.17it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.08it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.08it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.22it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.24it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.10it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.05it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.16it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.18it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.13it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.97it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.17it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.22it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.14it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.03it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.06it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.52it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.79it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.91it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.00it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.94it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.04it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.06it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.02it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.06it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.19it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.08it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.08it/s][A 40%|████      | 234/585 [01:50<01:43,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:43:44,731 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 19:43:44,754 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:43:48,431 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:43:48,461 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:43:48,481 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:03<47:29,  8.14s/it] 40%|████      | 236/585 [02:03<33:42,  5.80s/it] 41%|████      | 237/585 [02:04<24:02,  4.15s/it] 41%|████      | 238/585 [02:04<17:17,  2.99s/it] 41%|████      | 239/585 [02:04<12:34,  2.18s/it] 41%|████      | 240/585 [02:04<09:17,  1.62s/it] 41%|████      | 241/585 [02:05<06:59,  1.22s/it] 41%|████▏     | 242/585 [02:05<05:23,  1.06it/s] 42%|████▏     | 243/585 [02:05<04:15,  1.34it/s] 42%|████▏     | 244/585 [02:06<03:28,  1.63it/s] 42%|████▏     | 245/585 [02:06<02:55,  1.93it/s] 42%|████▏     | 246/585 [02:06<02:32,  2.22it/s] 42%|████▏     | 247/585 [02:07<02:19,  2.42it/s] 42%|████▏     | 248/585 [02:07<02:07,  2.65it/s] 43%|████▎     | 249/585 [02:07<01:58,  2.84it/s] 43%|████▎     | 250/585 [02:07<01:52,  2.98it/s] 43%|████▎     | 251/585 [02:08<01:48,  3.09it/s] 43%|████▎     | 252/585 [02:08<01:44,  3.17it/s] 43%|████▎     | 253/585 [02:08<01:42,  3.24it/s] 43%|████▎     | 254/585 [02:09<01:40,  3.29it/s] 44%|████▎     | 255/585 [02:09<01:38,  3.33it/s] 44%|████▍     | 256/585 [02:09<01:37,  3.37it/s] 44%|████▍     | 257/585 [02:09<01:36,  3.39it/s] 44%|████▍     | 258/585 [02:10<01:37,  3.35it/s] 44%|████▍     | 259/585 [02:10<01:36,  3.38it/s] 44%|████▍     | 260/585 [02:10<01:35,  3.39it/s] 45%|████▍     | 261/585 [02:11<01:35,  3.41it/s] 45%|████▍     | 262/585 [02:11<01:34,  3.42it/s] 45%|████▍     | 263/585 [02:11<01:34,  3.42it/s] 45%|████▌     | 264/585 [02:11<01:33,  3.42it/s] 45%|████▌     | 265/585 [02:12<01:33,  3.42it/s] 45%|████▌     | 266/585 [02:12<01:33,  3.43it/s] 46%|████▌     | 267/585 [02:12<01:32,  3.43it/s] 46%|████▌     | 268/585 [02:13<01:32,  3.43it/s] 46%|████▌     | 269/585 [02:13<01:36,  3.27it/s] 46%|████▌     | 270/585 [02:13<01:34,  3.32it/s] 46%|████▋     | 271/585 [02:14<01:33,  3.35it/s] 46%|████▋     | 272/585 [02:14<01:32,  3.38it/s] 47%|████▋     | 273/585 [02:14<01:31,  3.40it/s] 47%|████▋     | 274/585 [02:14<01:31,  3.41it/s] 47%|████▋     | 275/585 [02:15<01:30,  3.42it/s] 47%|████▋     | 276/585 [02:15<01:30,  3.42it/s] 47%|████▋     | 277/585 [02:15<01:29,  3.43it/s] 48%|████▊     | 278/585 [02:16<01:29,  3.43it/s] 48%|████▊     | 279/585 [02:16<01:29,  3.43it/s] 48%|████▊     | 280/585 [02:16<01:29,  3.39it/s] 48%|████▊     | 281/585 [02:17<01:29,  3.40it/s] 48%|████▊     | 282/585 [02:17<01:28,  3.41it/s] 48%|████▊     | 283/585 [02:17<01:28,  3.42it/s] 49%|████▊     | 284/585 [02:17<01:27,  3.43it/s] 49%|████▊     | 285/585 [02:18<01:27,  3.43it/s] 49%|████▉     | 286/585 [02:18<01:27,  3.43it/s] 49%|████▉     | 287/585 [02:18<01:26,  3.43it/s] 49%|████▉     | 288/585 [02:19<01:26,  3.43it/s] 49%|████▉     | 289/585 [02:19<01:26,  3.44it/s] 50%|████▉     | 290/585 [02:19<01:25,  3.43it/s] 50%|████▉     | 291/585 [02:19<01:27,  3.38it/s] 50%|████▉     | 292/585 [02:20<01:26,  3.39it/s] 50%|█████     | 293/585 [02:20<01:25,  3.41it/s] 50%|█████     | 294/585 [02:20<01:25,  3.41it/s] 50%|█████     | 295/585 [02:21<01:24,  3.41it/s] 51%|█████     | 296/585 [02:21<01:24,  3.42it/s] 51%|█████     | 297/585 [02:21<01:24,  3.42it/s] 51%|█████     | 298/585 [02:21<01:23,  3.43it/s] 51%|█████     | 299/585 [02:22<01:23,  3.43it/s] 51%|█████▏    | 300/585 [02:22<01:23,  3.43it/s] 51%|█████▏    | 301/585 [02:22<01:22,  3.43it/s] 52%|█████▏    | 302/585 [02:23<01:25,  3.31it/s] 52%|█████▏    | 303/585 [02:23<01:24,  3.34it/s] 52%|█████▏    | 304/585 [02:23<01:23,  3.37it/s] 52%|█████▏    | 305/585 [02:24<01:22,  3.39it/s] 52%|█████▏    | 306/585 [02:24<01:22,  3.40it/s] 52%|█████▏    | 307/585 [02:24<01:21,  3.41it/s] 53%|█████▎    | 308/585 [02:24<01:21,  3.42it/s] 53%|█████▎    | 309/585 [02:25<01:20,  3.42it/s] 53%|█████▎    | 310/585 [02:25<01:20,  3.43it/s] 53%|█████▎    | 311/585 [02:25<01:19,  3.43it/s] 53%|█████▎    | 312/585 [02:26<01:19,  3.43it/s] 54%|█████▎    | 313/585 [02:26<01:20,  3.38it/s] 54%|█████▎    | 314/585 [02:26<01:19,  3.40it/s] 54%|█████▍    | 315/585 [02:26<01:19,  3.41it/s] 54%|█████▍    | 316/585 [02:27<01:18,  3.42it/s] 54%|█████▍    | 317/585 [02:27<01:18,  3.41it/s] 54%|█████▍    | 318/585 [02:27<01:18,  3.42it/s] 55%|█████▍    | 319/585 [02:28<01:17,  3.42it/s] 55%|█████▍    | 320/585 [02:28<01:17,  3.43it/s] 55%|█████▍    | 321/585 [02:28<01:16,  3.43it/s] 55%|█████▌    | 322/585 [02:29<01:16,  3.43it/s] 55%|█████▌    | 323/585 [02:29<01:16,  3.43it/s] 55%|█████▌    | 324/585 [02:29<01:16,  3.40it/s] 56%|█████▌    | 325/585 [02:29<01:16,  3.41it/s] 56%|█████▌    | 326/585 [02:30<01:15,  3.42it/s] 56%|█████▌    | 327/585 [02:30<01:15,  3.42it/s] 56%|█████▌    | 328/585 [02:30<01:14,  3.43it/s] 56%|█████▌    | 329/585 [02:31<01:14,  3.43it/s] 56%|█████▋    | 330/585 [02:31<01:14,  3.43it/s] 57%|█████▋    | 331/585 [02:31<01:14,  3.43it/s] 57%|█████▋    | 332/585 [02:31<01:13,  3.43it/s] 57%|█████▋    | 333/585 [02:32<01:13,  3.43it/s] 57%|█████▋    | 334/585 [02:32<01:13,  3.43it/s] 57%|█████▋    | 335/585 [02:32<01:14,  3.38it/s] 57%|█████▋    | 336/585 [02:33<01:13,  3.39it/s] 58%|█████▊    | 337/585 [02:33<01:12,  3.40it/s] 58%|█████▊    | 338/585 [02:33<01:12,  3.41it/s] 58%|█████▊    | 339/585 [02:33<01:11,  3.42it/s] 58%|█████▊    | 340/585 [02:34<01:11,  3.42it/s] 58%|█████▊    | 341/585 [02:34<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:34<01:10,  3.43it/s] 59%|█████▊    | 343/585 [02:35<01:10,  3.43it/s] 59%|█████▉    | 344/585 [02:35<01:10,  3.43it/s] 59%|█████▉    | 345/585 [02:35<01:10,  3.43it/s] 59%|█████▉    | 346/585 [02:36<01:11,  3.35it/s] 59%|█████▉    | 347/585 [02:36<01:10,  3.38it/s] 59%|█████▉    | 348/585 [02:36<01:09,  3.39it/s] 60%|█████▉    | 349/585 [02:36<01:09,  3.41it/s] 60%|█████▉    | 350/585 [02:37<01:08,  3.41it/s] 60%|██████    | 351/585 [02:37<01:08,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 19:44:31,363 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:44:31,363 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:44:31,363 >>   Batch size = 8
{'eval_loss': 0.9197184443473816, 'eval_runtime': 13.8893, 'eval_samples_per_second': 351.495, 'eval_steps_per_second': 43.991, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.42it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.75it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.09it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.22it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.47it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.24it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.19it/s][A
  7%|▋         | 42/611 [00:00<00:12, 43.98it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.28it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.37it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.35it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.27it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.11it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 43.94it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.84it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.86it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.85it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.00it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.08it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.18it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.18it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.11it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.84it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.75it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.73it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.97it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.11it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.14it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.13it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.10it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.05it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 43.90it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.71it/s][A
 28%|██▊       | 172/611 [00:03<00:10, 43.77it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 43.89it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.11it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.25it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 43.36it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 43.51it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.66it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.57it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.66it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 43.80it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.82it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 43.97it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.01it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.12it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.12it/s][A
 40%|████      | 247/611 [00:05<00:08, 43.95it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.85it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.80it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.91it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.04it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.04it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.07it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.11it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 43.99it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 43.94it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.89it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.84it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.88it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.03it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 43.98it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.19it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 43.58it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 43.66it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.68it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.79it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.78it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.87it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.99it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.02it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.00it/s][A
 61%|██████    | 372/611 [00:08<00:05, 43.98it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 43.94it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 43.89it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.80it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.97it/s][A
 65%|██████▍   | 397/611 [00:09<00:04, 44.02it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.09it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.08it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.09it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 43.97it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 43.96it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.88it/s][A
 71%|███████   | 432/611 [00:09<00:04, 43.73it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.03it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.12it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.12it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.16it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.11it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 43.91it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.87it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.89it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.87it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.00it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.10it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.20it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.19it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.02it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 43.91it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.89it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.87it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.87it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.97it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.16it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.09it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.06it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 43.96it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 43.91it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.91it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.77it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.94it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.08it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.14it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.05it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.03it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 43.94it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 43.95it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.90it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 43.82it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 43.82it/s][A 60%|██████    | 351/585 [02:51<01:08,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:44:45,425 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 19:44:45,474 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:44:51,164 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:44:51,285 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:44:51,304 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:07<35:26,  9.13s/it] 60%|██████    | 353/585 [03:07<25:03,  6.48s/it] 61%|██████    | 354/585 [03:07<17:48,  4.63s/it] 61%|██████    | 355/585 [03:08<12:45,  3.33s/it] 61%|██████    | 356/585 [03:08<09:13,  2.42s/it] 61%|██████    | 357/585 [03:08<06:45,  1.78s/it] 61%|██████    | 358/585 [03:09<05:02,  1.33s/it] 61%|██████▏   | 359/585 [03:09<03:51,  1.02s/it] 62%|██████▏   | 360/585 [03:09<03:01,  1.24it/s] 62%|██████▏   | 361/585 [03:09<02:25,  1.53it/s] 62%|██████▏   | 362/585 [03:10<02:01,  1.84it/s] 62%|██████▏   | 363/585 [03:10<01:44,  2.13it/s] 62%|██████▏   | 364/585 [03:10<01:33,  2.36it/s] 62%|██████▏   | 365/585 [03:11<01:24,  2.60it/s] 63%|██████▎   | 366/585 [03:11<01:18,  2.79it/s] 63%|██████▎   | 367/585 [03:11<01:14,  2.94it/s] 63%|██████▎   | 368/585 [03:12<01:10,  3.06it/s] 63%|██████▎   | 369/585 [03:12<01:08,  3.16it/s] 63%|██████▎   | 370/585 [03:12<01:06,  3.22it/s] 63%|██████▎   | 371/585 [03:12<01:05,  3.27it/s] 64%|██████▎   | 372/585 [03:13<01:04,  3.31it/s] 64%|██████▍   | 373/585 [03:13<01:03,  3.33it/s] 64%|██████▍   | 374/585 [03:13<01:02,  3.35it/s] 64%|██████▍   | 375/585 [03:14<01:04,  3.28it/s] 64%|██████▍   | 376/585 [03:14<01:03,  3.31it/s] 64%|██████▍   | 377/585 [03:14<01:02,  3.33it/s] 65%|██████▍   | 378/585 [03:14<01:01,  3.35it/s] 65%|██████▍   | 379/585 [03:15<01:01,  3.36it/s] 65%|██████▍   | 380/585 [03:15<01:00,  3.37it/s] 65%|██████▌   | 381/585 [03:15<01:00,  3.37it/s] 65%|██████▌   | 382/585 [03:16<01:00,  3.38it/s] 65%|██████▌   | 383/585 [03:16<00:59,  3.38it/s] 66%|██████▌   | 384/585 [03:16<00:59,  3.39it/s] 66%|██████▌   | 385/585 [03:17<00:59,  3.39it/s] 66%|██████▌   | 386/585 [03:17<00:59,  3.34it/s] 66%|██████▌   | 387/585 [03:17<00:58,  3.36it/s] 66%|██████▋   | 388/585 [03:17<00:58,  3.37it/s] 66%|██████▋   | 389/585 [03:18<00:58,  3.37it/s] 67%|██████▋   | 390/585 [03:18<00:57,  3.38it/s] 67%|██████▋   | 391/585 [03:18<00:57,  3.38it/s] 67%|██████▋   | 392/585 [03:19<00:57,  3.38it/s] 67%|██████▋   | 393/585 [03:19<00:56,  3.38it/s] 67%|██████▋   | 394/585 [03:19<00:56,  3.38it/s] 68%|██████▊   | 395/585 [03:20<00:56,  3.38it/s] 68%|██████▊   | 396/585 [03:20<00:55,  3.38it/s] 68%|██████▊   | 397/585 [03:20<00:55,  3.36it/s] 68%|██████▊   | 398/585 [03:20<00:55,  3.37it/s] 68%|██████▊   | 399/585 [03:21<00:55,  3.37it/s] 68%|██████▊   | 400/585 [03:21<00:54,  3.38it/s] 69%|██████▊   | 401/585 [03:21<00:54,  3.38it/s] 69%|██████▊   | 402/585 [03:22<00:54,  3.38it/s] 69%|██████▉   | 403/585 [03:22<00:53,  3.38it/s] 69%|██████▉   | 404/585 [03:22<00:53,  3.39it/s] 69%|██████▉   | 405/585 [03:22<00:53,  3.39it/s] 69%|██████▉   | 406/585 [03:23<00:52,  3.39it/s] 70%|██████▉   | 407/585 [03:23<00:52,  3.39it/s] 70%|██████▉   | 408/585 [03:23<00:54,  3.23it/s] 70%|██████▉   | 409/585 [03:24<00:53,  3.27it/s] 70%|███████   | 410/585 [03:24<00:52,  3.31it/s] 70%|███████   | 411/585 [03:24<00:52,  3.33it/s] 70%|███████   | 412/585 [03:25<00:51,  3.35it/s] 71%|███████   | 413/585 [03:25<00:51,  3.36it/s] 71%|███████   | 414/585 [03:25<00:50,  3.37it/s] 71%|███████   | 415/585 [03:25<00:50,  3.37it/s] 71%|███████   | 416/585 [03:26<00:50,  3.38it/s] 71%|███████▏  | 417/585 [03:26<00:49,  3.38it/s] 71%|███████▏  | 418/585 [03:26<00:49,  3.38it/s] 72%|███████▏  | 419/585 [03:27<00:49,  3.38it/s] 72%|███████▏  | 420/585 [03:27<00:48,  3.39it/s] 72%|███████▏  | 421/585 [03:27<00:49,  3.33it/s] 72%|███████▏  | 422/585 [03:28<00:48,  3.34it/s] 72%|███████▏  | 423/585 [03:28<00:48,  3.36it/s] 72%|███████▏  | 424/585 [03:28<00:47,  3.37it/s] 73%|███████▎  | 425/585 [03:28<00:47,  3.37it/s] 73%|███████▎  | 426/585 [03:29<00:47,  3.38it/s] 73%|███████▎  | 427/585 [03:29<00:46,  3.38it/s] 73%|███████▎  | 428/585 [03:29<00:46,  3.38it/s] 73%|███████▎  | 429/585 [03:30<00:46,  3.38it/s] 74%|███████▎  | 430/585 [03:30<00:45,  3.38it/s] 74%|███████▎  | 431/585 [03:30<00:45,  3.38it/s] 74%|███████▍  | 432/585 [03:31<00:45,  3.35it/s] 74%|███████▍  | 433/585 [03:31<00:45,  3.36it/s] 74%|███████▍  | 434/585 [03:31<00:44,  3.37it/s] 74%|███████▍  | 435/585 [03:31<00:44,  3.37it/s] 75%|███████▍  | 436/585 [03:32<00:44,  3.38it/s] 75%|███████▍  | 437/585 [03:32<00:43,  3.38it/s] 75%|███████▍  | 438/585 [03:32<00:43,  3.38it/s] 75%|███████▌  | 439/585 [03:33<00:43,  3.38it/s] 75%|███████▌  | 440/585 [03:33<00:42,  3.38it/s] 75%|███████▌  | 441/585 [03:33<00:42,  3.38it/s] 76%|███████▌  | 442/585 [03:33<00:42,  3.38it/s] 76%|███████▌  | 443/585 [03:34<00:42,  3.36it/s] 76%|███████▌  | 444/585 [03:34<00:41,  3.36it/s] 76%|███████▌  | 445/585 [03:34<00:41,  3.37it/s] 76%|███████▌  | 446/585 [03:35<00:41,  3.38it/s] 76%|███████▋  | 447/585 [03:35<00:40,  3.38it/s] 77%|███████▋  | 448/585 [03:35<00:40,  3.38it/s] 77%|███████▋  | 449/585 [03:36<00:40,  3.38it/s] 77%|███████▋  | 450/585 [03:36<00:39,  3.38it/s] 77%|███████▋  | 451/585 [03:36<00:39,  3.38it/s] 77%|███████▋  | 452/585 [03:36<00:39,  3.38it/s] 77%|███████▋  | 453/585 [03:37<00:38,  3.39it/s] 78%|███████▊  | 454/585 [03:37<00:38,  3.37it/s] 78%|███████▊  | 455/585 [03:37<00:38,  3.37it/s] 78%|███████▊  | 456/585 [03:38<00:38,  3.38it/s] 78%|███████▊  | 457/585 [03:38<00:37,  3.38it/s] 78%|███████▊  | 458/585 [03:38<00:37,  3.38it/s] 78%|███████▊  | 459/585 [03:39<00:37,  3.38it/s] 79%|███████▊  | 460/585 [03:39<00:36,  3.38it/s] 79%|███████▉  | 461/585 [03:39<00:36,  3.38it/s] 79%|███████▉  | 462/585 [03:39<00:36,  3.38it/s] 79%|███████▉  | 463/585 [03:40<00:36,  3.39it/s] 79%|███████▉  | 464/585 [03:40<00:35,  3.39it/s] 79%|███████▉  | 465/585 [03:40<00:35,  3.36it/s] 80%|███████▉  | 466/585 [03:41<00:35,  3.37it/s] 80%|███████▉  | 467/585 [03:41<00:34,  3.37it/s] 80%|████████  | 468/585 [03:41<00:34,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 19:45:35,535 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:45:35,536 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:45:35,536 >>   Batch size = 8
{'eval_loss': 0.9281661510467529, 'eval_runtime': 13.927, 'eval_samples_per_second': 350.543, 'eval_steps_per_second': 43.872, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.16it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.10it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.10it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.36it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.80it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.42it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.39it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.18it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.30it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.33it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.52it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.26it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.31it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.20it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.11it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.90it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.08it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.15it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.40it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.37it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.24it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.27it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.08it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.03it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.88it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.09it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.13it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.40it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.38it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.36it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.25it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.13it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.91it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.99it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.14it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.27it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.41it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.28it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.15it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.19it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.16it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.10it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.04it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.15it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.30it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.29it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.31it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.19it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.11it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.02it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.92it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.07it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.24it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.26it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.26it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.18it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.20it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.12it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.98it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 43.99it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.22it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.17it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.32it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.21it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.14it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.08it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.13it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.02it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.00it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.18it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.15it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.23it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.17it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.10it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.11it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.15it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.08it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.17it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.25it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.15it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.23it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.11it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.16it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.17it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.09it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.14it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.16it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.30it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.30it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.22it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.17it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.13it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.15it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.84it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.21it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.08it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.18it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.20it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.23it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.17it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.23it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.08it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.11it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.20it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.24it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.29it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.21it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.20it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.11it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.03it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.08it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.11it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.21it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.27it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.14it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.21it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.18it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.13it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.14it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.01it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.01it/s][A 80%|████████  | 468/585 [03:55<00:34,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:45:49,426 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 19:45:49,491 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:45:54,833 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:45:54,850 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:45:54,859 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:12<18:18,  9.47s/it] 80%|████████  | 470/585 [04:12<12:52,  6.72s/it] 81%|████████  | 471/585 [04:13<09:06,  4.79s/it] 81%|████████  | 472/585 [04:13<06:29,  3.44s/it] 81%|████████  | 473/585 [04:13<04:39,  2.50s/it] 81%|████████  | 474/585 [04:14<03:23,  1.84s/it] 81%|████████  | 475/585 [04:14<02:31,  1.37s/it] 81%|████████▏ | 476/585 [04:14<01:54,  1.05s/it] 82%|████████▏ | 477/585 [04:14<01:29,  1.21it/s] 82%|████████▏ | 478/585 [04:15<01:11,  1.50it/s] 82%|████████▏ | 479/585 [04:15<00:58,  1.80it/s] 82%|████████▏ | 480/585 [04:15<00:50,  2.10it/s] 82%|████████▏ | 481/585 [04:16<00:46,  2.23it/s] 82%|████████▏ | 482/585 [04:16<00:41,  2.49it/s] 83%|████████▎ | 483/585 [04:16<00:37,  2.70it/s] 83%|████████▎ | 484/585 [04:17<00:35,  2.88it/s] 83%|████████▎ | 485/585 [04:17<00:33,  3.02it/s] 83%|████████▎ | 486/585 [04:17<00:31,  3.12it/s] 83%|████████▎ | 487/585 [04:17<00:30,  3.20it/s] 83%|████████▎ | 488/585 [04:18<00:29,  3.25it/s] 84%|████████▎ | 489/585 [04:18<00:29,  3.29it/s] 84%|████████▍ | 490/585 [04:18<00:28,  3.32it/s] 84%|████████▍ | 491/585 [04:19<00:28,  3.33it/s] 84%|████████▍ | 492/585 [04:19<00:27,  3.35it/s] 84%|████████▍ | 493/585 [04:19<00:27,  3.36it/s] 84%|████████▍ | 494/585 [04:20<00:27,  3.37it/s] 85%|████████▍ | 495/585 [04:20<00:26,  3.37it/s] 85%|████████▍ | 496/585 [04:20<00:26,  3.38it/s] 85%|████████▍ | 497/585 [04:20<00:25,  3.39it/s] 85%|████████▌ | 498/585 [04:21<00:25,  3.39it/s] 85%|████████▌ | 499/585 [04:21<00:25,  3.39it/s] 85%|████████▌ | 500/585 [04:21<00:25,  3.39it/s]                                                  85%|████████▌ | 500/585 [04:21<00:25,  3.39it/s] 86%|████████▌ | 501/585 [04:22<00:24,  3.39it/s] 86%|████████▌ | 502/585 [04:22<00:24,  3.37it/s] 86%|████████▌ | 503/585 [04:22<00:24,  3.39it/s] 86%|████████▌ | 504/585 [04:22<00:23,  3.41it/s] 86%|████████▋ | 505/585 [04:23<00:23,  3.42it/s] 86%|████████▋ | 506/585 [04:23<00:23,  3.42it/s] 87%|████████▋ | 507/585 [04:23<00:22,  3.43it/s] 87%|████████▋ | 508/585 [04:24<00:22,  3.43it/s] 87%|████████▋ | 509/585 [04:24<00:22,  3.43it/s] 87%|████████▋ | 510/585 [04:24<00:21,  3.44it/s] 87%|████████▋ | 511/585 [04:25<00:21,  3.44it/s] 88%|████████▊ | 512/585 [04:25<00:21,  3.44it/s] 88%|████████▊ | 513/585 [04:25<00:21,  3.41it/s] 88%|████████▊ | 514/585 [04:25<00:20,  3.42it/s] 88%|████████▊ | 515/585 [04:26<00:20,  3.36it/s] 88%|████████▊ | 516/585 [04:26<00:20,  3.38it/s] 88%|████████▊ | 517/585 [04:26<00:20,  3.40it/s] 89%|████████▊ | 518/585 [04:27<00:19,  3.41it/s] 89%|████████▊ | 519/585 [04:27<00:19,  3.42it/s] 89%|████████▉ | 520/585 [04:27<00:18,  3.42it/s] 89%|████████▉ | 521/585 [04:27<00:18,  3.43it/s] 89%|████████▉ | 522/585 [04:28<00:18,  3.43it/s] 89%|████████▉ | 523/585 [04:28<00:18,  3.43it/s] 90%|████████▉ | 524/585 [04:28<00:17,  3.43it/s] 90%|████████▉ | 525/585 [04:29<00:17,  3.43it/s] 90%|████████▉ | 526/585 [04:29<00:18,  3.26it/s] 90%|█████████ | 527/585 [04:29<00:17,  3.31it/s] 90%|█████████ | 528/585 [04:30<00:17,  3.35it/s] 90%|█████████ | 529/585 [04:30<00:16,  3.37it/s] 91%|█████████ | 530/585 [04:30<00:16,  3.40it/s] 91%|█████████ | 531/585 [04:30<00:15,  3.41it/s] 91%|█████████ | 532/585 [04:31<00:15,  3.41it/s] 91%|█████████ | 533/585 [04:31<00:15,  3.42it/s] 91%|█████████▏| 534/585 [04:31<00:14,  3.43it/s] 91%|█████████▏| 535/585 [04:32<00:14,  3.43it/s] 92%|█████████▏| 536/585 [04:32<00:14,  3.43it/s] 92%|█████████▏| 537/585 [04:32<00:14,  3.33it/s] 92%|█████████▏| 538/585 [04:32<00:13,  3.36it/s] 92%|█████████▏| 539/585 [04:33<00:13,  3.38it/s] 92%|█████████▏| 540/585 [04:33<00:13,  3.39it/s] 92%|█████████▏| 541/585 [04:33<00:12,  3.41it/s] 93%|█████████▎| 542/585 [04:34<00:12,  3.41it/s] 93%|█████████▎| 543/585 [04:34<00:12,  3.42it/s] 93%|█████████▎| 544/585 [04:34<00:11,  3.43it/s] 93%|█████████▎| 545/585 [04:35<00:11,  3.43it/s] 93%|█████████▎| 546/585 [04:35<00:11,  3.43it/s] 94%|█████████▎| 547/585 [04:35<00:11,  3.43it/s] 94%|█████████▎| 548/585 [04:35<00:11,  3.34it/s] 94%|█████████▍| 549/585 [04:36<00:10,  3.36it/s] 94%|█████████▍| 550/585 [04:36<00:10,  3.38it/s] 94%|█████████▍| 551/585 [04:36<00:09,  3.40it/s] 94%|█████████▍| 552/585 [04:37<00:09,  3.41it/s] 95%|█████████▍| 553/585 [04:37<00:09,  3.42it/s] 95%|█████████▍| 554/585 [04:37<00:09,  3.42it/s] 95%|█████████▍| 555/585 [04:37<00:08,  3.43it/s] 95%|█████████▌| 556/585 [04:38<00:08,  3.43it/s] 95%|█████████▌| 557/585 [04:38<00:08,  3.43it/s] 95%|█████████▌| 558/585 [04:38<00:07,  3.43it/s] 96%|█████████▌| 559/585 [04:39<00:07,  3.39it/s] 96%|█████████▌| 560/585 [04:39<00:07,  3.40it/s] 96%|█████████▌| 561/585 [04:39<00:07,  3.41it/s] 96%|█████████▌| 562/585 [04:40<00:06,  3.42it/s] 96%|█████████▌| 563/585 [04:40<00:06,  3.42it/s] 96%|█████████▋| 564/585 [04:40<00:06,  3.43it/s] 97%|█████████▋| 565/585 [04:40<00:05,  3.43it/s] 97%|█████████▋| 566/585 [04:41<00:05,  3.43it/s] 97%|█████████▋| 567/585 [04:41<00:05,  3.43it/s] 97%|█████████▋| 568/585 [04:41<00:04,  3.43it/s] 97%|█████████▋| 569/585 [04:42<00:04,  3.43it/s] 97%|█████████▋| 570/585 [04:42<00:04,  3.42it/s] 98%|█████████▊| 571/585 [04:42<00:04,  3.43it/s] 98%|█████████▊| 572/585 [04:42<00:03,  3.43it/s] 98%|█████████▊| 573/585 [04:43<00:03,  3.43it/s] 98%|█████████▊| 574/585 [04:43<00:03,  3.43it/s] 98%|█████████▊| 575/585 [04:43<00:02,  3.43it/s] 98%|█████████▊| 576/585 [04:44<00:02,  3.43it/s] 99%|█████████▊| 577/585 [04:44<00:02,  3.43it/s] 99%|█████████▉| 578/585 [04:44<00:02,  3.44it/s] 99%|█████████▉| 579/585 [04:44<00:01,  3.43it/s] 99%|█████████▉| 580/585 [04:45<00:01,  3.44it/s] 99%|█████████▉| 581/585 [04:45<00:01,  3.41it/s] 99%|█████████▉| 582/585 [04:45<00:00,  3.42it/s]100%|█████████▉| 583/585 [04:46<00:00,  3.42it/s]100%|█████████▉| 584/585 [04:46<00:00,  3.43it/s]100%|██████████| 585/585 [04:46<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 19:46:40,523 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:46:40,524 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:46:40,524 >>   Batch size = 8
{'eval_loss': 0.9380990862846375, 'eval_runtime': 13.8598, 'eval_samples_per_second': 352.242, 'eval_steps_per_second': 44.084, 'epoch': 4.0}
{'loss': 0.5604, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.30it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.01it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.16it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.10it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.83it/s][A
  5%|▌         | 32/611 [00:00<00:13, 43.93it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.70it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.35it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.43it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.39it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.40it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.35it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.16it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.08it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.86it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.85it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.03it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.18it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.20it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.22it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.00it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.14it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.99it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.01it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.22it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.28it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.15it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.24it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.16it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.09it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.05it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.04it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.08it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.10it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.18it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.24it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.16it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.15it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 43.99it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.04it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.13it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.06it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.17it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.20it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.15it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.28it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.11it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 43.97it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.16it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.13it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.14it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.15it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.17it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.27it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.23it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.09it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.02it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.03it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.15it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.11it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.21it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.27it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.31it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.23it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.19it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.00it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.96it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.03it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.31it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.61it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 43.94it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.02it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.00it/s][A
 61%|██████    | 372/611 [00:08<00:05, 43.89it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 43.86it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 43.92it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.99it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.07it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.89it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.33it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.31it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.24it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.01it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.06it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 43.89it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.12it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.23it/s][A
 72%|███████▏  | 442/611 [00:10<00:03, 44.25it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.19it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.10it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.05it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.01it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 43.98it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 43.96it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.05it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.19it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.10it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.24it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.26it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.13it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 43.98it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.89it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.98it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.12it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.25it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.19it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.29it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.28it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.21it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.09it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.98it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.97it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.98it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.20it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.82it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 43.90it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.04it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 43.99it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 43.93it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.95it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 43.93it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 43.93it/s][A100%|██████████| 585/585 [05:00<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:46:54,490 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 19:46:54,576 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:46:58,689 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:46:58,762 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:46:58,788 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:47:08,791 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:47:08,798 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117 (score: 0.9035747647285461).
                                                 100%|██████████| 585/585 [05:19<00:00,  3.43it/s]100%|██████████| 585/585 [05:19<00:00,  1.83it/s]
[INFO|trainer.py:1894] 2023-08-28 19:47:13,799 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 19:47:13,948 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:47:18,003 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:47:18,130 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:47:18,170 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:47:18,693 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:18,694 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:18,694 >>   train_loss               =     0.5566
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:18,694 >>   train_runtime            = 0:05:19.81
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:18,694 >>   train_samples            =       7505
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:18,694 >>   train_samples_per_second =    117.334
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:18,694 >>   train_steps_per_second   =      1.829
{'eval_loss': 0.9401085376739502, 'eval_runtime': 13.8492, 'eval_samples_per_second': 352.51, 'eval_steps_per_second': 44.118, 'epoch': 5.0}
{'train_runtime': 319.8146, 'train_samples_per_second': 117.334, 'train_steps_per_second': 1.829, 'train_loss': 0.5565750056861812, 'epoch': 5.0}
08/28/2023 19:47:18 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:47:18,747 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:47:18,748 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 19:47:18,748 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.21it/s]  2%|▏         | 12/611 [00:00<00:12, 48.60it/s]  3%|▎         | 17/611 [00:00<00:12, 46.83it/s]  4%|▎         | 22/611 [00:00<00:12, 46.30it/s]  4%|▍         | 27/611 [00:00<00:12, 45.86it/s]  5%|▌         | 32/611 [00:00<00:12, 45.55it/s]  6%|▌         | 37/611 [00:00<00:12, 45.35it/s]  7%|▋         | 42/611 [00:00<00:12, 44.86it/s]  8%|▊         | 47/611 [00:01<00:12, 44.26it/s]  9%|▊         | 52/611 [00:01<00:12, 43.91it/s]  9%|▉         | 57/611 [00:01<00:12, 44.03it/s] 10%|█         | 62/611 [00:01<00:12, 44.21it/s] 11%|█         | 67/611 [00:01<00:12, 44.48it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.64it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.66it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.64it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.26it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.06it/s] 16%|█▌        | 97/611 [00:02<00:11, 43.84it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.82it/s] 18%|█▊        | 107/611 [00:02<00:11, 43.96it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.33it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.50it/s] 20%|█▉        | 122/611 [00:02<00:10, 44.61it/s] 21%|██        | 127/611 [00:02<00:10, 44.53it/s] 22%|██▏       | 132/611 [00:02<00:10, 44.16it/s] 22%|██▏       | 137/611 [00:03<00:10, 43.94it/s] 23%|██▎       | 142/611 [00:03<00:10, 43.84it/s] 24%|██▍       | 147/611 [00:03<00:10, 43.86it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.04it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.36it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.40it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.57it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.60it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.32it/s] 30%|██▉       | 182/611 [00:04<00:09, 44.13it/s] 31%|███       | 187/611 [00:04<00:09, 43.91it/s] 31%|███▏      | 192/611 [00:04<00:09, 43.85it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.06it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.32it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.48it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.64it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.65it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.53it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.32it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.15it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.06it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.28it/s] 40%|████      | 247/611 [00:05<00:08, 44.41it/s] 41%|████      | 252/611 [00:05<00:08, 44.54it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.70it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.68it/s] 44%|████▎     | 267/611 [00:05<00:07, 44.54it/s] 45%|████▍     | 272/611 [00:06<00:07, 44.34it/s] 45%|████▌     | 277/611 [00:06<00:07, 44.17it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.19it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.29it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.39it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.50it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.59it/s] 50%|█████     | 307/611 [00:06<00:06, 44.65it/s] 51%|█████     | 312/611 [00:07<00:06, 44.65it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.39it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.33it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.28it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.36it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.33it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.51it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.69it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.68it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.51it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.49it/s] 60%|██████    | 367/611 [00:08<00:05, 44.35it/s] 61%|██████    | 372/611 [00:08<00:05, 44.26it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.16it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.30it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.51it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.52it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.65it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.55it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.37it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.25it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.23it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.30it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.29it/s] 71%|███████   | 432/611 [00:09<00:04, 44.50it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.63it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.71it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.63it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.52it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.32it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.30it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.09it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.27it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.45it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.55it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.59it/s] 81%|████████  | 492/611 [00:11<00:02, 44.49it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.37it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.33it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.20it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.17it/s] 85%|████████▍ | 517/611 [00:11<00:02, 42.03it/s] 85%|████████▌ | 522/611 [00:11<00:02, 42.90it/s] 86%|████████▋ | 527/611 [00:11<00:01, 43.45it/s] 87%|████████▋ | 532/611 [00:11<00:01, 43.88it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.11it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.29it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.27it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.14it/s] 91%|█████████ | 557/611 [00:12<00:01, 43.89it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.07it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.36it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.46it/s] 94%|█████████▍| 577/611 [00:12<00:00, 44.42it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.63it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.38it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.52it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.29it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.25it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.36it/s]100%|██████████| 611/611 [00:13<00:00, 44.40it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:47:32,525 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:32,525 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:32,525 >>   eval_loss               =     0.9036
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:32,525 >>   eval_runtime            = 0:00:13.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:32,525 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:32,525 >>   eval_samples_per_second =    354.351
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:32,525 >>   eval_steps_per_second   =     44.348
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:47:32,525 >>   perplexity              =     2.4684
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:38,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:38,735 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:38,735 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:38,735 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:38,735 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:47:39,028 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:47:39,029 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:47:39,284 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:47:40,341 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:47:40,341 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:41,798 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:41,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:41,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:41,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:41,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:47:42,157 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:47:42,158 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:47:42,836 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:47:43,006 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:47:43,006 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:12,  1.69it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.75it/s]Extractor Predicting: 23it [00:14,  1.75it/s]Extractor Predicting: 24it [00:14,  1.75it/s]Extractor Predicting: 25it [00:15,  1.72it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.68it/s]Extractor Predicting: 31it [00:18,  1.69it/s]Extractor Predicting: 32it [00:19,  1.74it/s]Extractor Predicting: 33it [00:19,  1.72it/s]Extractor Predicting: 34it [00:20,  1.70it/s]Extractor Predicting: 35it [00:21,  1.70it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:22,  1.71it/s]Extractor Predicting: 38it [00:22,  1.71it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.71it/s]Extractor Predicting: 41it [00:24,  1.66it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:25,  1.68it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:27,  1.71it/s]Extractor Predicting: 46it [00:27,  1.69it/s]Extractor Predicting: 47it [00:28,  1.69it/s]Extractor Predicting: 48it [00:28,  1.68it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:30,  1.65it/s]Extractor Predicting: 51it [00:30,  1.65it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:32,  1.66it/s]Extractor Predicting: 55it [00:33,  1.60it/s]Extractor Predicting: 56it [00:33,  1.57it/s]Extractor Predicting: 57it [00:34,  1.59it/s]Extractor Predicting: 58it [00:35,  1.59it/s]Extractor Predicting: 59it [00:35,  1.62it/s]Extractor Predicting: 60it [00:36,  1.60it/s]Extractor Predicting: 61it [00:36,  1.60it/s]Extractor Predicting: 62it [00:37,  1.57it/s]Extractor Predicting: 63it [00:38,  1.58it/s]Extractor Predicting: 64it [00:38,  1.59it/s]Extractor Predicting: 65it [00:39,  1.58it/s]Extractor Predicting: 66it [00:40,  1.55it/s]Extractor Predicting: 67it [00:40,  1.58it/s]Extractor Predicting: 68it [00:41,  1.60it/s]Extractor Predicting: 69it [00:41,  1.59it/s]Extractor Predicting: 70it [00:42,  1.57it/s]Extractor Predicting: 71it [00:43,  1.58it/s]Extractor Predicting: 72it [00:43,  1.55it/s]Extractor Predicting: 73it [00:44,  1.57it/s]Extractor Predicting: 74it [00:45,  1.58it/s]Extractor Predicting: 75it [00:45,  1.59it/s]Extractor Predicting: 76it [00:46,  1.60it/s]Extractor Predicting: 77it [00:47,  1.59it/s]Extractor Predicting: 78it [00:47,  1.61it/s]Extractor Predicting: 79it [00:48,  1.63it/s]Extractor Predicting: 80it [00:48,  1.62it/s]Extractor Predicting: 81it [00:49,  1.62it/s]Extractor Predicting: 82it [00:50,  1.60it/s]Extractor Predicting: 83it [00:50,  1.60it/s]Extractor Predicting: 84it [00:51,  1.59it/s]Extractor Predicting: 85it [00:51,  1.58it/s]Extractor Predicting: 86it [00:52,  1.57it/s]Extractor Predicting: 87it [00:53,  1.57it/s]Extractor Predicting: 88it [00:53,  1.53it/s]Extractor Predicting: 89it [00:54,  1.54it/s]Extractor Predicting: 90it [00:55,  1.54it/s]Extractor Predicting: 91it [00:55,  1.55it/s]Extractor Predicting: 92it [00:56,  1.61it/s]Extractor Predicting: 93it [00:57,  1.66it/s]Extractor Predicting: 94it [00:57,  1.66it/s]Extractor Predicting: 95it [00:58,  1.65it/s]Extractor Predicting: 96it [00:58,  1.66it/s]Extractor Predicting: 97it [00:59,  1.67it/s]Extractor Predicting: 98it [01:00,  1.64it/s]Extractor Predicting: 99it [01:00,  1.57it/s]Extractor Predicting: 100it [01:01,  1.60it/s]Extractor Predicting: 101it [01:02,  1.53it/s]Extractor Predicting: 102it [01:02,  1.52it/s]Extractor Predicting: 103it [01:03,  1.54it/s]Extractor Predicting: 104it [01:03,  1.57it/s]Extractor Predicting: 105it [01:04,  1.58it/s]Extractor Predicting: 106it [01:05,  1.63it/s]Extractor Predicting: 107it [01:05,  1.63it/s]Extractor Predicting: 108it [01:06,  1.65it/s]Extractor Predicting: 109it [01:06,  1.64it/s]Extractor Predicting: 110it [01:07,  1.64it/s]Extractor Predicting: 111it [01:08,  1.67it/s]Extractor Predicting: 112it [01:08,  1.67it/s]Extractor Predicting: 113it [01:09,  1.62it/s]Extractor Predicting: 114it [01:10,  1.61it/s]Extractor Predicting: 115it [01:10,  1.62it/s]Extractor Predicting: 116it [01:11,  1.58it/s]Extractor Predicting: 117it [01:12,  1.56it/s]Extractor Predicting: 118it [01:12,  1.57it/s]Extractor Predicting: 119it [01:13,  1.41it/s]Extractor Predicting: 120it [01:14,  1.42it/s]Extractor Predicting: 121it [01:14,  1.46it/s]Extractor Predicting: 122it [01:15,  1.48it/s]Extractor Predicting: 123it [01:16,  1.51it/s]Extractor Predicting: 124it [01:16,  1.53it/s]Extractor Predicting: 125it [01:17,  1.55it/s]Extractor Predicting: 126it [01:18,  1.56it/s]Extractor Predicting: 127it [01:18,  1.54it/s]Extractor Predicting: 128it [01:19,  1.55it/s]Extractor Predicting: 129it [01:19,  1.58it/s]Extractor Predicting: 130it [01:20,  1.54it/s]Extractor Predicting: 131it [01:21,  1.54it/s]Extractor Predicting: 132it [01:21,  1.57it/s]Extractor Predicting: 133it [01:22,  1.55it/s]Extractor Predicting: 134it [01:23,  1.56it/s]Extractor Predicting: 135it [01:23,  1.55it/s]Extractor Predicting: 136it [01:24,  1.57it/s]Extractor Predicting: 137it [01:25,  1.54it/s]Extractor Predicting: 138it [01:25,  1.56it/s]Extractor Predicting: 139it [01:26,  1.54it/s]Extractor Predicting: 140it [01:27,  1.53it/s]Extractor Predicting: 141it [01:27,  1.55it/s]Extractor Predicting: 142it [01:28,  1.56it/s]Extractor Predicting: 143it [01:29,  1.54it/s]Extractor Predicting: 144it [01:29,  1.58it/s]Extractor Predicting: 145it [01:30,  1.62it/s]Extractor Predicting: 146it [01:30,  1.60it/s]Extractor Predicting: 147it [01:31,  1.57it/s]Extractor Predicting: 148it [01:32,  1.58it/s]Extractor Predicting: 149it [01:32,  1.57it/s]Extractor Predicting: 150it [01:33,  1.55it/s]Extractor Predicting: 151it [01:34,  1.54it/s]Extractor Predicting: 152it [01:34,  1.54it/s]Extractor Predicting: 153it [01:35,  1.54it/s]Extractor Predicting: 154it [01:36,  1.55it/s]Extractor Predicting: 155it [01:36,  1.55it/s]Extractor Predicting: 156it [01:37,  1.50it/s]Extractor Predicting: 157it [01:38,  1.45it/s]Extractor Predicting: 158it [01:38,  1.43it/s]Extractor Predicting: 159it [01:39,  1.46it/s]Extractor Predicting: 160it [01:40,  1.49it/s]Extractor Predicting: 161it [01:40,  1.51it/s]Extractor Predicting: 162it [01:41,  1.52it/s]Extractor Predicting: 163it [01:42,  1.52it/s]Extractor Predicting: 164it [01:42,  1.55it/s]Extractor Predicting: 165it [01:43,  1.55it/s]Extractor Predicting: 166it [01:43,  1.58it/s]Extractor Predicting: 167it [01:44,  1.57it/s]Extractor Predicting: 168it [01:45,  1.57it/s]Extractor Predicting: 169it [01:45,  1.57it/s]Extractor Predicting: 170it [01:46,  1.57it/s]Extractor Predicting: 171it [01:47,  1.59it/s]Extractor Predicting: 172it [01:47,  1.62it/s]Extractor Predicting: 173it [01:48,  1.57it/s]Extractor Predicting: 174it [01:49,  1.57it/s]Extractor Predicting: 175it [01:49,  1.55it/s]Extractor Predicting: 176it [01:50,  1.57it/s]Extractor Predicting: 177it [01:50,  1.55it/s]Extractor Predicting: 178it [01:51,  1.55it/s]Extractor Predicting: 179it [01:52,  1.55it/s]Extractor Predicting: 180it [01:52,  1.56it/s]Extractor Predicting: 181it [01:53,  1.56it/s]Extractor Predicting: 182it [01:54,  1.55it/s]Extractor Predicting: 183it [01:54,  1.51it/s]Extractor Predicting: 184it [01:55,  1.53it/s]Extractor Predicting: 185it [01:56,  1.63it/s]Extractor Predicting: 185it [01:56,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:50,142 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:50,159 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:50,159 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:50,159 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:50,159 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:49:50,920 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:49:50,921 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:49:51,652 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:49:52,729 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:49:52,729 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:55,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:55,675 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:55,676 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:55,676 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:49:55,676 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:49:56,459 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:49:56,460 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:49:57,049 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:49:57,228 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:49:57,228 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4300847457627119,
  "recall": 0.08316263826300696,
  "score": 0.13937521455544113,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.62it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.69it/s]Extractor Predicting: 31it [00:19,  1.74it/s]Extractor Predicting: 32it [00:19,  1.74it/s]Extractor Predicting: 33it [00:20,  1.73it/s]Extractor Predicting: 34it [00:20,  1.74it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.66it/s]Extractor Predicting: 42it [00:25,  1.67it/s]Extractor Predicting: 43it [00:26,  1.68it/s]Extractor Predicting: 44it [00:26,  1.70it/s]Extractor Predicting: 45it [00:27,  1.72it/s]Extractor Predicting: 46it [00:28,  1.75it/s]Extractor Predicting: 47it [00:28,  1.76it/s]Extractor Predicting: 48it [00:29,  1.76it/s]Extractor Predicting: 49it [00:29,  1.73it/s]Extractor Predicting: 50it [00:30,  1.72it/s]Extractor Predicting: 51it [00:30,  1.73it/s]Extractor Predicting: 52it [00:31,  1.73it/s]Extractor Predicting: 53it [00:32,  1.71it/s]Extractor Predicting: 54it [00:32,  1.68it/s]Extractor Predicting: 55it [00:33,  1.66it/s]Extractor Predicting: 56it [00:33,  1.66it/s]Extractor Predicting: 57it [00:34,  1.69it/s]Extractor Predicting: 58it [00:35,  1.76it/s]Extractor Predicting: 59it [00:35,  1.73it/s]Extractor Predicting: 60it [00:36,  1.70it/s]Extractor Predicting: 61it [00:36,  1.65it/s]Extractor Predicting: 62it [00:37,  1.62it/s]Extractor Predicting: 63it [00:38,  1.57it/s]Extractor Predicting: 64it [00:38,  1.55it/s]Extractor Predicting: 65it [00:39,  1.54it/s]Extractor Predicting: 66it [00:40,  1.52it/s]Extractor Predicting: 67it [00:40,  1.51it/s]Extractor Predicting: 68it [00:41,  1.53it/s]Extractor Predicting: 69it [00:42,  1.54it/s]Extractor Predicting: 70it [00:42,  1.56it/s]Extractor Predicting: 71it [00:43,  1.56it/s]Extractor Predicting: 72it [00:44,  1.58it/s]Extractor Predicting: 73it [00:44,  1.63it/s]Extractor Predicting: 74it [00:45,  1.63it/s]Extractor Predicting: 75it [00:45,  1.63it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:47,  1.67it/s]Extractor Predicting: 78it [00:47,  1.65it/s]Extractor Predicting: 79it [00:48,  1.70it/s]Extractor Predicting: 80it [00:48,  1.74it/s]Extractor Predicting: 81it [00:49,  1.70it/s]Extractor Predicting: 82it [00:49,  1.71it/s]Extractor Predicting: 83it [00:50,  1.66it/s]Extractor Predicting: 84it [00:51,  1.64it/s]Extractor Predicting: 85it [00:51,  1.60it/s]Extractor Predicting: 86it [00:52,  1.58it/s]Extractor Predicting: 87it [00:53,  1.56it/s]Extractor Predicting: 88it [00:53,  1.59it/s]Extractor Predicting: 89it [00:54,  1.58it/s]Extractor Predicting: 90it [00:55,  1.60it/s]Extractor Predicting: 91it [00:55,  1.59it/s]Extractor Predicting: 92it [00:56,  1.54it/s]Extractor Predicting: 93it [00:56,  1.58it/s]Extractor Predicting: 94it [00:57,  1.60it/s]Extractor Predicting: 95it [00:58,  1.60it/s]Extractor Predicting: 96it [00:58,  1.60it/s]Extractor Predicting: 97it [00:59,  1.61it/s]Extractor Predicting: 98it [01:00,  1.61it/s]Extractor Predicting: 99it [01:00,  1.61it/s]Extractor Predicting: 100it [01:01,  1.60it/s]Extractor Predicting: 101it [01:01,  1.61it/s]Extractor Predicting: 102it [01:02,  1.62it/s]Extractor Predicting: 103it [01:03,  1.54it/s]Extractor Predicting: 104it [01:03,  1.58it/s]Extractor Predicting: 105it [01:04,  1.61it/s]Extractor Predicting: 106it [01:05,  1.64it/s]Extractor Predicting: 107it [01:05,  1.62it/s]Extractor Predicting: 108it [01:06,  1.64it/s]Extractor Predicting: 109it [01:06,  1.63it/s]Extractor Predicting: 110it [01:07,  1.65it/s]Extractor Predicting: 111it [01:08,  1.65it/s]Extractor Predicting: 112it [01:08,  1.63it/s]Extractor Predicting: 113it [01:09,  1.62it/s]Extractor Predicting: 114it [01:09,  1.61it/s]Extractor Predicting: 115it [01:10,  1.61it/s]Extractor Predicting: 116it [01:11,  1.61it/s]Extractor Predicting: 117it [01:11,  1.65it/s]Extractor Predicting: 118it [01:12,  1.62it/s]Extractor Predicting: 119it [01:13,  1.61it/s]Extractor Predicting: 120it [01:13,  1.63it/s]Extractor Predicting: 121it [01:14,  1.67it/s]Extractor Predicting: 122it [01:14,  1.66it/s]Extractor Predicting: 123it [01:15,  1.62it/s]Extractor Predicting: 124it [01:16,  1.60it/s]Extractor Predicting: 125it [01:16,  1.61it/s]Extractor Predicting: 126it [01:17,  1.60it/s]Extractor Predicting: 127it [01:17,  1.64it/s]Extractor Predicting: 128it [01:18,  1.58it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:19,  1.64it/s]Extractor Predicting: 131it [01:20,  1.61it/s]Extractor Predicting: 132it [01:21,  1.60it/s]Extractor Predicting: 133it [01:21,  1.60it/s]Extractor Predicting: 134it [01:22,  1.60it/s]Extractor Predicting: 135it [01:22,  1.61it/s]Extractor Predicting: 136it [01:23,  1.65it/s]Extractor Predicting: 137it [01:24,  1.59it/s]Extractor Predicting: 138it [01:24,  1.60it/s]Extractor Predicting: 139it [01:25,  1.44it/s]Extractor Predicting: 140it [01:26,  1.50it/s]Extractor Predicting: 141it [01:26,  1.52it/s]Extractor Predicting: 142it [01:27,  1.56it/s]Extractor Predicting: 143it [01:28,  1.49it/s]Extractor Predicting: 144it [01:28,  1.56it/s]Extractor Predicting: 145it [01:29,  1.55it/s]Extractor Predicting: 146it [01:30,  1.58it/s]Extractor Predicting: 147it [01:30,  1.63it/s]Extractor Predicting: 148it [01:31,  1.61it/s]Extractor Predicting: 149it [01:31,  1.65it/s]Extractor Predicting: 150it [01:32,  1.66it/s]Extractor Predicting: 151it [01:33,  1.69it/s]Extractor Predicting: 152it [01:33,  1.65it/s]Extractor Predicting: 153it [01:34,  1.65it/s]Extractor Predicting: 154it [01:34,  1.61it/s]Extractor Predicting: 155it [01:35,  1.61it/s]Extractor Predicting: 156it [01:36,  1.66it/s]Extractor Predicting: 157it [01:36,  1.62it/s]Extractor Predicting: 158it [01:37,  1.60it/s]Extractor Predicting: 159it [01:37,  1.62it/s]Extractor Predicting: 160it [01:38,  1.63it/s]Extractor Predicting: 161it [01:39,  1.66it/s]Extractor Predicting: 162it [01:39,  1.65it/s]Extractor Predicting: 163it [01:40,  1.60it/s]Extractor Predicting: 164it [01:41,  1.60it/s]Extractor Predicting: 165it [01:41,  1.56it/s]Extractor Predicting: 166it [01:42,  1.54it/s]Extractor Predicting: 167it [01:43,  1.55it/s]Extractor Predicting: 168it [01:43,  1.57it/s]Extractor Predicting: 169it [01:44,  1.61it/s]Extractor Predicting: 170it [01:44,  1.63it/s]Extractor Predicting: 171it [01:45,  1.63it/s]Extractor Predicting: 172it [01:46,  1.67it/s]Extractor Predicting: 173it [01:46,  1.66it/s]Extractor Predicting: 174it [01:47,  1.65it/s]Extractor Predicting: 175it [01:47,  1.64it/s]Extractor Predicting: 176it [01:48,  1.65it/s]Extractor Predicting: 177it [01:49,  1.63it/s]Extractor Predicting: 178it [01:49,  1.62it/s]Extractor Predicting: 179it [01:50,  1.58it/s]Extractor Predicting: 180it [01:50,  1.62it/s]Extractor Predicting: 181it [01:51,  1.62it/s]Extractor Predicting: 182it [01:52,  1.64it/s]Extractor Predicting: 183it [01:52,  1.67it/s]Extractor Predicting: 184it [01:53,  1.64it/s]Extractor Predicting: 185it [01:53,  1.65it/s]Extractor Predicting: 186it [01:54,  1.61it/s]Extractor Predicting: 187it [01:55,  1.63it/s]Extractor Predicting: 188it [01:55,  1.63it/s]Extractor Predicting: 189it [01:56,  1.62it/s]Extractor Predicting: 190it [01:57,  1.64it/s]Extractor Predicting: 191it [01:57,  1.65it/s]Extractor Predicting: 192it [01:58,  1.69it/s]Extractor Predicting: 193it [01:58,  1.68it/s]Extractor Predicting: 194it [01:59,  1.67it/s]Extractor Predicting: 195it [02:00,  1.65it/s]Extractor Predicting: 196it [02:00,  1.65it/s]Extractor Predicting: 197it [02:01,  1.64it/s]Extractor Predicting: 198it [02:01,  1.61it/s]Extractor Predicting: 199it [02:02,  1.61it/s]Extractor Predicting: 200it [02:03,  1.62it/s]Extractor Predicting: 201it [02:03,  1.64it/s]Extractor Predicting: 202it [02:04,  1.65it/s]Extractor Predicting: 203it [02:04,  1.65it/s]Extractor Predicting: 204it [02:05,  1.64it/s]Extractor Predicting: 205it [02:06,  1.65it/s]Extractor Predicting: 206it [02:06,  1.64it/s]Extractor Predicting: 207it [02:07,  1.66it/s]Extractor Predicting: 208it [02:08,  1.61it/s]Extractor Predicting: 209it [02:08,  1.59it/s]Extractor Predicting: 210it [02:09,  1.66it/s]Extractor Predicting: 211it [02:09,  1.65it/s]Extractor Predicting: 212it [02:10,  1.66it/s]Extractor Predicting: 213it [02:11,  1.59it/s]Extractor Predicting: 214it [02:11,  1.64it/s]Extractor Predicting: 215it [02:12,  1.64it/s]Extractor Predicting: 216it [02:12,  1.62it/s]Extractor Predicting: 217it [02:13,  1.65it/s]Extractor Predicting: 218it [02:14,  1.62it/s]Extractor Predicting: 219it [02:14,  1.63it/s]Extractor Predicting: 220it [02:15,  1.64it/s]Extractor Predicting: 221it [02:15,  1.66it/s]Extractor Predicting: 222it [02:16,  1.60it/s]Extractor Predicting: 223it [02:17,  1.54it/s]Extractor Predicting: 224it [02:17,  1.57it/s]Extractor Predicting: 225it [02:18,  1.60it/s]Extractor Predicting: 226it [02:19,  1.64it/s]Extractor Predicting: 227it [02:19,  1.64it/s]Extractor Predicting: 228it [02:20,  1.66it/s]Extractor Predicting: 229it [02:20,  1.69it/s]Extractor Predicting: 230it [02:21,  1.71it/s]Extractor Predicting: 231it [02:22,  1.70it/s]Extractor Predicting: 232it [02:22,  1.69it/s]Extractor Predicting: 233it [02:23,  1.68it/s]Extractor Predicting: 234it [02:23,  1.68it/s]Extractor Predicting: 235it [02:24,  1.65it/s]Extractor Predicting: 236it [02:25,  1.65it/s]Extractor Predicting: 237it [02:25,  1.66it/s]Extractor Predicting: 238it [02:26,  1.63it/s]Extractor Predicting: 239it [02:26,  1.61it/s]Extractor Predicting: 240it [02:27,  1.61it/s]Extractor Predicting: 241it [02:28,  1.64it/s]Extractor Predicting: 242it [02:28,  1.61it/s]Extractor Predicting: 243it [02:29,  1.58it/s]Extractor Predicting: 244it [02:30,  1.59it/s]Extractor Predicting: 245it [02:30,  1.62it/s]Extractor Predicting: 246it [02:31,  1.62it/s]Extractor Predicting: 247it [02:31,  1.66it/s]Extractor Predicting: 248it [02:32,  1.58it/s]Extractor Predicting: 249it [02:33,  1.58it/s]Extractor Predicting: 250it [02:33,  1.61it/s]Extractor Predicting: 251it [02:34,  1.61it/s]Extractor Predicting: 252it [02:35,  1.42it/s]Extractor Predicting: 253it [02:35,  1.43it/s]Extractor Predicting: 254it [02:36,  1.44it/s]Extractor Predicting: 255it [02:37,  1.49it/s]Extractor Predicting: 256it [02:37,  1.52it/s]Extractor Predicting: 257it [02:38,  1.55it/s]Extractor Predicting: 258it [02:39,  1.56it/s]Extractor Predicting: 259it [02:39,  1.57it/s]Extractor Predicting: 260it [02:40,  1.55it/s]Extractor Predicting: 261it [02:41,  1.56it/s]Extractor Predicting: 262it [02:41,  1.56it/s]Extractor Predicting: 263it [02:42,  1.55it/s]Extractor Predicting: 264it [02:43,  1.57it/s]Extractor Predicting: 265it [02:43,  1.59it/s]Extractor Predicting: 266it [02:44,  1.55it/s]Extractor Predicting: 267it [02:44,  1.55it/s]Extractor Predicting: 268it [02:45,  1.51it/s]Extractor Predicting: 269it [02:46,  1.54it/s]Extractor Predicting: 270it [02:46,  1.53it/s]Extractor Predicting: 271it [02:47,  1.53it/s]Extractor Predicting: 272it [02:48,  1.54it/s]Extractor Predicting: 273it [02:48,  1.54it/s]Extractor Predicting: 274it [02:49,  1.52it/s]Extractor Predicting: 275it [02:50,  1.53it/s]Extractor Predicting: 276it [02:50,  1.54it/s]Extractor Predicting: 277it [02:51,  1.55it/s]Extractor Predicting: 278it [02:52,  1.57it/s]Extractor Predicting: 279it [02:52,  1.55it/s]Extractor Predicting: 280it [02:53,  1.56it/s]Extractor Predicting: 281it [02:54,  1.52it/s]Extractor Predicting: 282it [02:54,  1.52it/s]Extractor Predicting: 283it [02:55,  1.53it/s]Extractor Predicting: 284it [02:55,  1.56it/s]Extractor Predicting: 285it [02:56,  1.56it/s]Extractor Predicting: 286it [02:57,  1.58it/s]Extractor Predicting: 287it [02:57,  1.53it/s]Extractor Predicting: 288it [02:58,  1.47it/s]Extractor Predicting: 289it [02:59,  1.52it/s]Extractor Predicting: 290it [02:59,  1.50it/s]Extractor Predicting: 291it [03:00,  1.49it/s]Extractor Predicting: 292it [03:01,  1.54it/s]Extractor Predicting: 293it [03:01,  1.54it/s]Extractor Predicting: 294it [03:02,  1.53it/s]Extractor Predicting: 295it [03:03,  1.47it/s]Extractor Predicting: 296it [03:03,  1.52it/s]Extractor Predicting: 297it [03:04,  1.55it/s]Extractor Predicting: 298it [03:05,  1.54it/s]Extractor Predicting: 299it [03:05,  1.55it/s]Extractor Predicting: 300it [03:06,  1.53it/s]Extractor Predicting: 301it [03:07,  1.55it/s]Extractor Predicting: 302it [03:07,  1.53it/s]Extractor Predicting: 303it [03:08,  1.57it/s]Extractor Predicting: 304it [03:09,  1.59it/s]Extractor Predicting: 305it [03:09,  1.60it/s]Extractor Predicting: 306it [03:10,  1.64it/s]Extractor Predicting: 307it [03:10,  1.61it/s]Extractor Predicting: 308it [03:11,  1.62it/s]Extractor Predicting: 309it [03:12,  1.59it/s]Extractor Predicting: 310it [03:12,  1.59it/s]Extractor Predicting: 311it [03:13,  1.55it/s]Extractor Predicting: 312it [03:14,  1.57it/s]Extractor Predicting: 313it [03:14,  1.51it/s]Extractor Predicting: 314it [03:15,  1.52it/s]Extractor Predicting: 315it [03:16,  1.55it/s]Extractor Predicting: 316it [03:16,  1.59it/s]Extractor Predicting: 317it [03:17,  1.60it/s]Extractor Predicting: 318it [03:17,  1.64it/s]Extractor Predicting: 319it [03:18,  1.60it/s]Extractor Predicting: 320it [03:19,  1.57it/s]Extractor Predicting: 321it [03:19,  1.57it/s]Extractor Predicting: 322it [03:20,  1.59it/s]Extractor Predicting: 323it [03:21,  1.54it/s]Extractor Predicting: 324it [03:21,  1.56it/s]Extractor Predicting: 325it [03:22,  1.58it/s]Extractor Predicting: 326it [03:22,  1.59it/s]Extractor Predicting: 327it [03:23,  1.59it/s]Extractor Predicting: 328it [03:24,  1.56it/s]Extractor Predicting: 329it [03:24,  1.58it/s]Extractor Predicting: 330it [03:25,  1.55it/s]Extractor Predicting: 331it [03:26,  1.57it/s]Extractor Predicting: 332it [03:26,  1.57it/s]Extractor Predicting: 333it [03:27,  1.56it/s]Extractor Predicting: 334it [03:28,  1.57it/s]Extractor Predicting: 335it [03:28,  1.56it/s]Extractor Predicting: 336it [03:29,  1.49it/s]Extractor Predicting: 337it [03:30,  1.50it/s]Extractor Predicting: 338it [03:30,  1.51it/s]Extractor Predicting: 339it [03:31,  1.53it/s]Extractor Predicting: 340it [03:32,  1.53it/s]Extractor Predicting: 341it [03:32,  1.52it/s]Extractor Predicting: 342it [03:33,  1.53it/s]Extractor Predicting: 343it [03:33,  1.55it/s]Extractor Predicting: 344it [03:34,  1.55it/s]Extractor Predicting: 345it [03:35,  1.51it/s]Extractor Predicting: 346it [03:35,  1.52it/s]Extractor Predicting: 347it [03:36,  1.53it/s]Extractor Predicting: 348it [03:37,  1.54it/s]Extractor Predicting: 349it [03:37,  1.55it/s]Extractor Predicting: 350it [03:38,  1.54it/s]Extractor Predicting: 351it [03:39,  1.57it/s]Extractor Predicting: 352it [03:40,  1.36it/s]Extractor Predicting: 353it [03:40,  1.38it/s]Extractor Predicting: 354it [03:41,  1.44it/s]Extractor Predicting: 355it [03:42,  1.49it/s]Extractor Predicting: 356it [03:42,  1.51it/s]Extractor Predicting: 357it [03:43,  1.53it/s]Extractor Predicting: 358it [03:43,  1.53it/s]Extractor Predicting: 359it [03:44,  1.54it/s]Extractor Predicting: 360it [03:45,  1.51it/s]Extractor Predicting: 361it [03:45,  1.52it/s]Extractor Predicting: 362it [03:46,  1.54it/s]Extractor Predicting: 363it [03:47,  1.57it/s]Extractor Predicting: 364it [03:47,  1.59it/s]Extractor Predicting: 365it [03:48,  1.57it/s]Extractor Predicting: 366it [03:49,  1.53it/s]Extractor Predicting: 367it [03:49,  1.52it/s]Extractor Predicting: 368it [03:50,  1.47it/s]Extractor Predicting: 369it [03:51,  1.44it/s]Extractor Predicting: 370it [03:51,  1.46it/s]Extractor Predicting: 371it [03:52,  1.49it/s]Extractor Predicting: 372it [03:52,  1.78it/s]Extractor Predicting: 372it [03:52,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:59,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:59,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:59,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:59,445 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:53:59,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:53:59,774 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:53:59,775 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:54:00,036 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:54:01,123 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:54:01,123 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:54:02,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:54:02,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:54:02,845 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:54:02,845 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:54:02,845 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:54:03,248 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:54:03,249 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:54:04,009 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:54:04,222 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:54:04,222 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3253119429590018,
  "recall": 0.04095601436265709,
  "score": 0.0727526410205302,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:14,  1.55it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:16,  1.55it/s]Extractor Predicting: 28it [00:17,  1.54it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:21,  1.57it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:23,  1.52it/s]Extractor Predicting: 39it [00:24,  1.49it/s]Extractor Predicting: 40it [00:25,  1.47it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:27,  1.51it/s]Extractor Predicting: 45it [00:28,  1.49it/s]Extractor Predicting: 46it [00:29,  1.52it/s]Extractor Predicting: 47it [00:29,  1.49it/s]Extractor Predicting: 48it [00:30,  1.50it/s]Extractor Predicting: 49it [00:31,  1.50it/s]Extractor Predicting: 50it [00:31,  1.49it/s]Extractor Predicting: 51it [00:32,  1.47it/s]Extractor Predicting: 52it [00:33,  1.46it/s]Extractor Predicting: 53it [00:34,  1.48it/s]Extractor Predicting: 54it [00:34,  1.47it/s]Extractor Predicting: 55it [00:35,  1.51it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:38,  1.48it/s]Extractor Predicting: 61it [00:39,  1.45it/s]Extractor Predicting: 62it [00:40,  1.34it/s]Extractor Predicting: 63it [00:40,  1.49it/s]Extractor Predicting: 63it [00:40,  1.54it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:54:46,471 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:54:46,473 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:54:46,481 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:54:46,482 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:54:46,487 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:54:53,470 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:54:53,482 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:54:53,530 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:54:53,531 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:54:53,543 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:54:53,549 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:54:53,549 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:54:53,549 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:54:53,549 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:54:53,549 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:54:53,549 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5853658536585366,
  "recall": 0.02876835480970932,
  "score": 0.054841473864610114,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:54:53,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:54,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:55,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:55,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:56,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:56,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:57,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:58,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:58,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:59,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:54:59,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:00,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:01,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:01,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:02,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:02,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:03,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:04,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:04,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:05,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:05,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:06,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:06,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:07,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:08,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:27, 14.83s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:08,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:09,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:09,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:10,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:10,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:11,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:11,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:12,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:13,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:13,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:14,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:14,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:15,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:15,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:16,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:17,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:17,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:18,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:18,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:19,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:19,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:20,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:53, 13.32s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:20,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:21,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:21,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:22,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:22,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:23,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:23,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:24,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:24,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:25,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:25,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:26,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:26,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:27,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:27,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:28,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:28,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:29,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:29,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:36<02:18, 11.52s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:30,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:30,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:31,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:31,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:32,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:33,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:33,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:34,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:34,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:35,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:36,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:36,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:37,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:37,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:38,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:39,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:39,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:40,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:40,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:41,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:42,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:48<02:10, 11.91s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:42,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:43,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:43,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:44,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:45,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:45,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:46,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:46,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:47,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:47,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:48,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:49,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:49,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:50,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:50,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:51,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:52,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:52,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:53,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:54,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:54,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:55,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:02<02:03, 12.32s/it][WARNING|generation_utils.py:914] 2023-08-28 19:55:55,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:56,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:57,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:57,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:58,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:58,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:55:59,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:00,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:00,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:01,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:01,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:02,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:03,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:03,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:04,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:04,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:05,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:05,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:06,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:06,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:07,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:14<01:50, 12.31s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:08,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:08,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:09,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:09,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:10,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:11,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:11,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:12,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:12,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:13,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:13,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:14,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:15,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:15,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:16,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:16,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:17,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:18,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:18,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:19,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:25<01:36, 12.06s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:19,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:20,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:20,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:21,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:22,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:22,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:23,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:23,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:24,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:24,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:25,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:26,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:26,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:27,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:27,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:28,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:28,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:29,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:29,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:30,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:30,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:37<01:23, 12.00s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:31,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:32,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:32,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:33,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:33,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:34,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:34,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:35,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:36,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:36,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:37,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:37,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:38,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:39,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:39,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:40,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:41,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:41,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:42,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:42,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:43,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:44,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:44,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:45,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:52<01:16, 12.72s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:45,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:46,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:46,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:47,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:48,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:48,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:49,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:49,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:50,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:51,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:51,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:52,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:53,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:53,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:54,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:54,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:55,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:55,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:56,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:57,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:03<01:02, 12.44s/it][WARNING|generation_utils.py:914] 2023-08-28 19:56:57,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:58,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:58,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:59,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:56:59,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:00,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:00,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:01,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:01,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:02,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:02,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:03,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:03,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:04,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:04,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:05,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:05,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:06,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:06,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:13<00:46, 11.55s/it][WARNING|generation_utils.py:914] 2023-08-28 19:57:07,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:07,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:08,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:08,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:09,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:10,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:10,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:11,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:11,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:12,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:13,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:13,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:14,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:14,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:15,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:16,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:16,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:17,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:17,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:18,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:19,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:25<00:35, 11.84s/it][WARNING|generation_utils.py:914] 2023-08-28 19:57:19,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:20,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:20,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:21,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:22,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:22,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:23,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:24,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:24,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:25,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:26,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:26,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:27,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:27,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:28,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:29,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:29,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:30,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:31,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:31,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:32,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:33,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:39<00:24, 12.48s/it][WARNING|generation_utils.py:914] 2023-08-28 19:57:33,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:34,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:34,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:35,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:35,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:36,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:36,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:37,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:37,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:38,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:38,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:39,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:39,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:40,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:40,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:41,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:41,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:42,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:42,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:49<00:11, 11.60s/it][WARNING|generation_utils.py:914] 2023-08-28 19:57:43,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:43,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:44,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:45,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:45,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:46,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:47,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:47,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:48,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:49,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:49,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:50,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:51,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:51,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:52,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:52,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:53,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:54,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:54,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:55,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:56,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:57:57,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:03<00:00, 12.48s/it]Generating: 100%|██████████| 15/15 [03:03<00:00, 12.26s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:05,038 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:05,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:05,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:05,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:05,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:58:05,784 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:58:05,785 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:58:06,376 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:58:07,500 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:58:07,500 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,735 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:58:09,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:58:10,175 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:58:10,176 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:58:10,444 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:58:10,637 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:58:10,637 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 419, 'raw': 544}
{'target': 600, 'success': 443, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 492, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : conflict .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 410, 'raw': 416}
{'target': 600, 'success': 441, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 505, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 569, 'raw': 576}
{'target': 600, 'success': 601, 'raw': 608}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.9884868421052632, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : work location .', 'success_rate': 0.8551136363636364, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 562, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9765625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8111979166666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 318, 'raw': 320}
{'target': 600, 'success': 349, 'raw': 352}
{'target': 600, 'success': 380, 'raw': 384}
{'target': 600, 'success': 412, 'raw': 416}
{'target': 600, 'success': 444, 'raw': 448}
{'target': 600, 'success': 476, 'raw': 480}
{'target': 600, 'success': 508, 'raw': 512}
{'target': 600, 'success': 540, 'raw': 544}
{'target': 600, 'success': 571, 'raw': 576}
{'target': 600, 'success': 603, 'raw': 608}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9917763157894737, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8664772727272727, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 254, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 318, 'raw': 320}
{'target': 600, 'success': 350, 'raw': 352}
{'target': 600, 'success': 382, 'raw': 384}
{'target': 600, 'success': 414, 'raw': 416}
{'target': 600, 'success': 446, 'raw': 448}
{'target': 600, 'success': 478, 'raw': 480}
{'target': 600, 'success': 510, 'raw': 512}
{'target': 600, 'success': 541, 'raw': 544}
{'target': 600, 'success': 572, 'raw': 576}
{'target': 600, 'success': 604, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.993421052631579, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 9343
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9443, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.62it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:01,  1.61it/s]Extractor Estimating: 4it [00:02,  1.65it/s]Extractor Estimating: 5it [00:03,  1.64it/s]Extractor Estimating: 6it [00:03,  1.55it/s]Extractor Estimating: 7it [00:04,  1.64it/s]Extractor Estimating: 8it [00:04,  1.68it/s]Extractor Estimating: 9it [00:05,  1.68it/s]Extractor Estimating: 10it [00:06,  1.66it/s]Extractor Estimating: 11it [00:06,  1.63it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.65it/s]Extractor Estimating: 16it [00:09,  1.57it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.60it/s]Extractor Estimating: 19it [00:11,  1.56it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:12,  1.64it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:14,  1.65it/s]Extractor Estimating: 25it [00:15,  1.66it/s]Extractor Estimating: 26it [00:15,  1.63it/s]Extractor Estimating: 27it [00:16,  1.65it/s]Extractor Estimating: 28it [00:17,  1.66it/s]Extractor Estimating: 29it [00:17,  1.70it/s]Extractor Estimating: 30it [00:18,  1.67it/s]Extractor Estimating: 31it [00:18,  1.71it/s]Extractor Estimating: 32it [00:19,  1.75it/s]Extractor Estimating: 33it [00:20,  1.72it/s]Extractor Estimating: 34it [00:20,  1.74it/s]Extractor Estimating: 35it [00:21,  1.75it/s]Extractor Estimating: 36it [00:21,  1.73it/s]Extractor Estimating: 37it [00:22,  1.75it/s]Extractor Estimating: 38it [00:22,  1.69it/s]Extractor Estimating: 39it [00:23,  1.69it/s]Extractor Estimating: 40it [00:24,  1.69it/s]Extractor Estimating: 41it [00:24,  1.70it/s]Extractor Estimating: 42it [00:25,  1.56it/s]Extractor Estimating: 43it [00:26,  1.62it/s]Extractor Estimating: 44it [00:26,  1.67it/s]Extractor Estimating: 45it [00:27,  1.70it/s]Extractor Estimating: 46it [00:27,  1.72it/s]Extractor Estimating: 47it [00:28,  1.74it/s]Extractor Estimating: 48it [00:28,  1.70it/s]Extractor Estimating: 49it [00:29,  1.75it/s]Extractor Estimating: 50it [00:30,  1.65it/s]Extractor Estimating: 51it [00:30,  1.80it/s]Extractor Estimating: 52it [00:31,  1.86it/s]Extractor Estimating: 53it [00:31,  1.91it/s]Extractor Estimating: 54it [00:32,  1.98it/s]Extractor Estimating: 55it [00:32,  2.08it/s]Extractor Estimating: 56it [00:32,  2.12it/s]Extractor Estimating: 57it [00:33,  2.08it/s]Extractor Estimating: 58it [00:33,  2.11it/s]Extractor Estimating: 59it [00:34,  2.07it/s]Extractor Estimating: 60it [00:34,  2.14it/s]Extractor Estimating: 61it [00:35,  2.22it/s]Extractor Estimating: 62it [00:35,  2.18it/s]Extractor Estimating: 63it [00:36,  2.16it/s]Extractor Estimating: 64it [00:36,  2.17it/s]Extractor Estimating: 65it [00:37,  2.16it/s]Extractor Estimating: 66it [00:37,  2.19it/s]Extractor Estimating: 67it [00:37,  2.19it/s]Extractor Estimating: 68it [00:38,  2.23it/s]Extractor Estimating: 69it [00:38,  2.28it/s]Extractor Estimating: 70it [00:39,  2.27it/s]Extractor Estimating: 71it [00:39,  2.23it/s]Extractor Estimating: 72it [00:40,  2.24it/s]Extractor Estimating: 73it [00:40,  2.17it/s]Extractor Estimating: 74it [00:41,  2.17it/s]Extractor Estimating: 75it [00:41,  2.03it/s]Extractor Estimating: 76it [00:42,  1.95it/s]Extractor Estimating: 77it [00:42,  1.76it/s]Extractor Estimating: 78it [00:43,  1.74it/s]Extractor Estimating: 79it [00:44,  1.72it/s]Extractor Estimating: 80it [00:44,  1.67it/s]Extractor Estimating: 81it [00:45,  1.68it/s]Extractor Estimating: 82it [00:45,  1.68it/s]Extractor Estimating: 83it [00:46,  1.66it/s]Extractor Estimating: 84it [00:47,  1.61it/s]Extractor Estimating: 85it [00:47,  1.68it/s]Extractor Estimating: 86it [00:48,  1.65it/s]Extractor Estimating: 87it [00:48,  1.67it/s]Extractor Estimating: 88it [00:49,  1.69it/s]Extractor Estimating: 89it [00:50,  1.62it/s]Extractor Estimating: 90it [00:51,  1.51it/s]Extractor Estimating: 91it [00:51,  1.56it/s]Extractor Estimating: 92it [00:52,  1.46it/s]Extractor Estimating: 93it [00:53,  1.47it/s]Extractor Estimating: 94it [00:53,  1.54it/s]Extractor Estimating: 95it [00:54,  1.60it/s]Extractor Estimating: 96it [00:54,  1.63it/s]Extractor Estimating: 97it [00:55,  1.65it/s]Extractor Estimating: 98it [00:55,  1.67it/s]Extractor Estimating: 99it [00:56,  1.65it/s]Extractor Estimating: 100it [00:57,  1.58it/s]Extractor Estimating: 101it [00:57,  1.61it/s]Extractor Estimating: 102it [00:58,  1.53it/s]Extractor Estimating: 103it [00:59,  1.60it/s]Extractor Estimating: 104it [00:59,  1.67it/s]Extractor Estimating: 105it [01:00,  1.59it/s]Extractor Estimating: 106it [01:00,  1.65it/s]Extractor Estimating: 107it [01:01,  1.60it/s]Extractor Estimating: 108it [01:02,  1.48it/s]Extractor Estimating: 109it [01:02,  1.55it/s]Extractor Estimating: 110it [01:03,  1.62it/s]Extractor Estimating: 111it [01:04,  1.64it/s]Extractor Estimating: 112it [01:05,  1.38it/s]Extractor Estimating: 113it [01:05,  1.50it/s]Extractor Estimating: 114it [01:06,  1.49it/s]Extractor Estimating: 115it [01:06,  1.57it/s]Extractor Estimating: 116it [01:07,  1.64it/s]Extractor Estimating: 117it [01:08,  1.52it/s]Extractor Estimating: 118it [01:08,  1.58it/s]Extractor Estimating: 119it [01:09,  1.65it/s]Extractor Estimating: 120it [01:09,  1.69it/s]Extractor Estimating: 121it [01:10,  1.68it/s]Extractor Estimating: 122it [01:11,  1.64it/s]Extractor Estimating: 123it [01:11,  1.74it/s]Extractor Estimating: 124it [01:12,  1.71it/s]Extractor Estimating: 125it [01:12,  1.69it/s]Extractor Estimating: 126it [01:13,  1.69it/s]Extractor Estimating: 127it [01:14,  1.64it/s]Extractor Estimating: 128it [01:14,  1.62it/s]Extractor Estimating: 129it [01:15,  1.64it/s]Extractor Estimating: 130it [01:15,  1.66it/s]Extractor Estimating: 131it [01:16,  1.62it/s]Extractor Estimating: 132it [01:17,  1.62it/s]Extractor Estimating: 133it [01:17,  1.66it/s]Extractor Estimating: 134it [01:18,  1.61it/s]Extractor Estimating: 135it [01:19,  1.63it/s]Extractor Estimating: 136it [01:19,  1.63it/s]Extractor Estimating: 137it [01:20,  1.59it/s]Extractor Estimating: 138it [01:20,  1.59it/s]Extractor Estimating: 139it [01:21,  1.61it/s]Extractor Estimating: 140it [01:22,  1.63it/s]Extractor Estimating: 141it [01:22,  1.68it/s]Extractor Estimating: 142it [01:23,  1.72it/s]Extractor Estimating: 143it [01:23,  1.68it/s]Extractor Estimating: 144it [01:24,  1.70it/s]Extractor Estimating: 145it [01:24,  1.72it/s]Extractor Estimating: 146it [01:25,  1.67it/s]Extractor Estimating: 147it [01:26,  1.60it/s]Extractor Estimating: 148it [01:26,  1.62it/s]Extractor Estimating: 149it [01:27,  1.66it/s]Extractor Estimating: 150it [01:28,  1.68it/s]Extractor Estimating: 151it [01:28,  1.66it/s]Extractor Estimating: 152it [01:29,  1.64it/s]Extractor Estimating: 153it [01:29,  1.68it/s]Extractor Estimating: 154it [01:30,  1.64it/s]Extractor Estimating: 155it [01:31,  1.66it/s]Extractor Estimating: 156it [01:31,  1.62it/s]Extractor Estimating: 157it [01:32,  1.61it/s]Extractor Estimating: 158it [01:32,  1.63it/s]Extractor Estimating: 159it [01:33,  1.65it/s]Extractor Estimating: 160it [01:34,  1.64it/s]Extractor Estimating: 161it [01:34,  1.64it/s]Extractor Estimating: 162it [01:35,  1.64it/s]Extractor Estimating: 163it [01:35,  1.67it/s]Extractor Estimating: 164it [01:36,  1.62it/s]Extractor Estimating: 165it [01:37,  1.68it/s]Extractor Estimating: 166it [01:37,  1.62it/s]Extractor Estimating: 167it [01:38,  1.64it/s]Extractor Estimating: 168it [01:39,  1.65it/s]Extractor Estimating: 169it [01:39,  1.65it/s]Extractor Estimating: 170it [01:40,  1.67it/s]Extractor Estimating: 171it [01:40,  1.68it/s]Extractor Estimating: 172it [01:41,  1.64it/s]Extractor Estimating: 173it [01:42,  1.63it/s]Extractor Estimating: 174it [01:42,  1.62it/s]Extractor Estimating: 175it [01:43,  1.64it/s]Extractor Estimating: 176it [01:44,  1.52it/s]Extractor Estimating: 177it [01:44,  1.53it/s]Extractor Estimating: 178it [01:45,  1.55it/s]Extractor Estimating: 179it [01:45,  1.59it/s]Extractor Estimating: 180it [01:46,  1.61it/s]Extractor Estimating: 181it [01:47,  1.62it/s]Extractor Estimating: 182it [01:47,  1.62it/s]Extractor Estimating: 183it [01:48,  1.65it/s]Extractor Estimating: 184it [01:48,  1.68it/s]Extractor Estimating: 185it [01:49,  1.65it/s]Extractor Estimating: 186it [01:50,  1.65it/s]Extractor Estimating: 187it [01:50,  1.66it/s]Extractor Estimating: 188it [01:51,  1.65it/s]Extractor Estimating: 189it [01:52,  1.47it/s]Extractor Estimating: 190it [01:52,  1.49it/s]Extractor Estimating: 191it [01:53,  1.52it/s]Extractor Estimating: 192it [01:54,  1.53it/s]Extractor Estimating: 193it [01:54,  1.60it/s]Extractor Estimating: 194it [01:55,  1.61it/s]Extractor Estimating: 195it [01:55,  1.64it/s]Extractor Estimating: 196it [01:56,  1.68it/s]Extractor Estimating: 197it [01:57,  1.59it/s]Extractor Estimating: 198it [01:57,  1.59it/s]Extractor Estimating: 199it [01:58,  1.61it/s]Extractor Estimating: 200it [01:58,  1.63it/s]Extractor Estimating: 201it [01:59,  1.64it/s]Extractor Estimating: 202it [02:00,  1.62it/s]Extractor Estimating: 203it [02:00,  1.65it/s]Extractor Estimating: 204it [02:01,  1.67it/s]Extractor Estimating: 205it [02:01,  1.64it/s]Extractor Estimating: 206it [02:02,  1.68it/s]Extractor Estimating: 207it [02:03,  1.67it/s]Extractor Estimating: 208it [02:03,  1.56it/s]Extractor Estimating: 209it [02:04,  1.58it/s]Extractor Estimating: 210it [02:05,  1.62it/s]Extractor Estimating: 211it [02:05,  1.67it/s]Extractor Estimating: 212it [02:06,  1.69it/s]Extractor Estimating: 213it [02:06,  1.70it/s]Extractor Estimating: 214it [02:07,  1.64it/s]Extractor Estimating: 215it [02:08,  1.61it/s]Extractor Estimating: 216it [02:08,  1.63it/s]Extractor Estimating: 217it [02:09,  1.65it/s]Extractor Estimating: 218it [02:09,  1.64it/s]Extractor Estimating: 219it [02:10,  1.65it/s]Extractor Estimating: 220it [02:11,  1.63it/s]Extractor Estimating: 221it [02:11,  1.65it/s]Extractor Estimating: 222it [02:12,  1.53it/s]Extractor Estimating: 223it [02:13,  1.57it/s]Extractor Estimating: 224it [02:13,  1.61it/s]Extractor Estimating: 225it [02:14,  1.62it/s]Extractor Estimating: 226it [02:14,  1.59it/s]Extractor Estimating: 227it [02:15,  1.62it/s]Extractor Estimating: 228it [02:16,  1.54it/s]Extractor Estimating: 229it [02:16,  1.59it/s]Extractor Estimating: 230it [02:17,  1.57it/s]Extractor Estimating: 231it [02:18,  1.56it/s]Extractor Estimating: 232it [02:18,  1.55it/s]Extractor Estimating: 233it [02:19,  1.54it/s]Extractor Estimating: 234it [02:20,  1.57it/s]Extractor Estimating: 235it [02:20,  1.60it/s]Extractor Estimating: 236it [02:21,  1.54it/s]Extractor Estimating: 237it [02:22,  1.51it/s]Extractor Estimating: 238it [02:22,  1.55it/s]Extractor Estimating: 239it [02:23,  1.56it/s]Extractor Estimating: 240it [02:23,  1.58it/s]Extractor Estimating: 241it [02:24,  1.57it/s]Extractor Estimating: 242it [02:25,  1.45it/s]Extractor Estimating: 243it [02:26,  1.46it/s]Extractor Estimating: 244it [02:26,  1.51it/s]Extractor Estimating: 245it [02:27,  1.54it/s]Extractor Estimating: 246it [02:27,  1.54it/s]Extractor Estimating: 247it [02:28,  1.57it/s]Extractor Estimating: 248it [02:29,  1.56it/s]Extractor Estimating: 249it [02:29,  1.55it/s]Extractor Estimating: 250it [02:30,  1.58it/s]Extractor Estimating: 251it [02:31,  1.55it/s]Extractor Estimating: 252it [02:31,  1.56it/s]Extractor Estimating: 253it [02:32,  1.58it/s]Extractor Estimating: 254it [02:33,  1.56it/s]Extractor Estimating: 255it [02:33,  1.57it/s]Extractor Estimating: 256it [02:34,  1.58it/s]Extractor Estimating: 257it [02:34,  1.58it/s]Extractor Estimating: 258it [02:35,  1.59it/s]Extractor Estimating: 259it [02:36,  1.58it/s]Extractor Estimating: 260it [02:36,  1.57it/s]Extractor Estimating: 261it [02:37,  1.58it/s]Extractor Estimating: 262it [02:38,  1.59it/s]Extractor Estimating: 263it [02:38,  1.58it/s]Extractor Estimating: 264it [02:39,  1.58it/s]Extractor Estimating: 265it [02:39,  1.60it/s]Extractor Estimating: 266it [02:40,  1.57it/s]Extractor Estimating: 267it [02:41,  1.57it/s]Extractor Estimating: 268it [02:41,  1.56it/s]Extractor Estimating: 269it [02:42,  1.53it/s]Extractor Estimating: 270it [02:43,  1.54it/s]Extractor Estimating: 271it [02:44,  1.14it/s]Extractor Estimating: 272it [02:45,  1.25it/s]Extractor Estimating: 273it [02:45,  1.34it/s]Extractor Estimating: 274it [02:46,  1.40it/s]Extractor Estimating: 275it [02:47,  1.29it/s]Extractor Estimating: 276it [02:48,  1.36it/s]Extractor Estimating: 277it [02:48,  1.41it/s]Extractor Estimating: 278it [02:49,  1.48it/s]Extractor Estimating: 279it [02:49,  1.49it/s]Extractor Estimating: 280it [02:50,  1.51it/s]Extractor Estimating: 281it [02:51,  1.56it/s]Extractor Estimating: 282it [02:51,  1.55it/s]Extractor Estimating: 283it [02:52,  1.57it/s]Extractor Estimating: 284it [02:53,  1.56it/s]Extractor Estimating: 285it [02:54,  1.38it/s]Extractor Estimating: 286it [02:54,  1.44it/s]Extractor Estimating: 287it [02:55,  1.42it/s]Extractor Estimating: 288it [02:56,  1.48it/s]Extractor Estimating: 289it [02:56,  1.50it/s]Extractor Estimating: 290it [02:57,  1.52it/s]Extractor Estimating: 291it [02:57,  1.55it/s]Extractor Estimating: 292it [02:58,  1.46it/s]Extractor Estimating: 293it [02:59,  1.52it/s]Extractor Estimating: 294it [02:59,  1.57it/s]Extractor Estimating: 295it [03:00,  1.59it/s]Extractor Estimating: 296it [03:01,  1.59it/s]Extractor Estimating: 297it [03:01,  1.49it/s]Extractor Estimating: 298it [03:02,  1.47it/s]Extractor Estimating: 299it [03:03,  1.53it/s]Extractor Estimating: 300it [03:03,  1.59it/s]Extractor Estimating: 301it [03:04,  1.65it/s]Extractor Estimating: 302it [03:04,  1.66it/s]Extractor Estimating: 303it [03:05,  1.69it/s]Extractor Estimating: 304it [03:06,  1.68it/s]Extractor Estimating: 305it [03:06,  1.66it/s]Extractor Estimating: 306it [03:07,  1.63it/s]Extractor Estimating: 307it [03:07,  1.60it/s]Extractor Estimating: 308it [03:08,  1.61it/s]Extractor Estimating: 309it [03:09,  1.64it/s]Extractor Estimating: 310it [03:09,  1.52it/s]Extractor Estimating: 311it [03:10,  1.52it/s]Extractor Estimating: 312it [03:11,  1.55it/s]Extractor Estimating: 313it [03:11,  1.57it/s]Extractor Estimating: 314it [03:12,  1.59it/s]Extractor Estimating: 315it [03:13,  1.58it/s]Extractor Estimating: 316it [03:13,  1.62it/s]Extractor Estimating: 317it [03:14,  1.56it/s]Extractor Estimating: 318it [03:14,  1.59it/s]Extractor Estimating: 319it [03:15,  1.61it/s]Extractor Estimating: 320it [03:16,  1.63it/s]Extractor Estimating: 321it [03:16,  1.58it/s]Extractor Estimating: 322it [03:17,  1.57it/s]Extractor Estimating: 323it [03:18,  1.59it/s]Extractor Estimating: 324it [03:18,  1.61it/s]Extractor Estimating: 325it [03:19,  1.58it/s]Extractor Estimating: 326it [03:19,  1.60it/s]Extractor Estimating: 327it [03:20,  1.58it/s]Extractor Estimating: 328it [03:21,  1.59it/s]Extractor Estimating: 329it [03:21,  1.57it/s]Extractor Estimating: 330it [03:22,  1.55it/s]Extractor Estimating: 331it [03:23,  1.55it/s]Extractor Estimating: 332it [03:23,  1.49it/s]Extractor Estimating: 333it [03:24,  1.52it/s]Extractor Estimating: 334it [03:25,  1.53it/s]Extractor Estimating: 335it [03:25,  1.54it/s]Extractor Estimating: 336it [03:26,  1.55it/s]Extractor Estimating: 337it [03:27,  1.55it/s]Extractor Estimating: 338it [03:27,  1.57it/s]Extractor Estimating: 339it [03:28,  1.56it/s]Extractor Estimating: 340it [03:29,  1.54it/s]Extractor Estimating: 341it [03:29,  1.55it/s]Extractor Estimating: 342it [03:30,  1.57it/s]Extractor Estimating: 343it [03:30,  1.59it/s]Extractor Estimating: 344it [03:31,  1.58it/s]Extractor Estimating: 345it [03:32,  1.46it/s]Extractor Estimating: 346it [03:33,  1.48it/s]Extractor Estimating: 347it [03:33,  1.50it/s]Extractor Estimating: 348it [03:34,  1.51it/s]Extractor Estimating: 349it [03:35,  1.47it/s]Extractor Estimating: 350it [03:35,  1.57it/s]Extractor Estimating: 351it [03:36,  1.62it/s]Extractor Estimating: 352it [03:36,  1.65it/s]Extractor Estimating: 353it [03:37,  1.73it/s]Extractor Estimating: 354it [03:37,  1.71it/s]Extractor Estimating: 355it [03:38,  1.73it/s]Extractor Estimating: 356it [03:39,  1.73it/s]Extractor Estimating: 357it [03:39,  1.74it/s]Extractor Estimating: 358it [03:40,  1.67it/s]Extractor Estimating: 359it [03:40,  1.72it/s]Extractor Estimating: 360it [03:41,  1.80it/s]Extractor Estimating: 361it [03:41,  1.79it/s]Extractor Estimating: 362it [03:42,  1.74it/s]Extractor Estimating: 363it [03:43,  1.75it/s]Extractor Estimating: 364it [03:43,  1.77it/s]Extractor Estimating: 365it [03:44,  1.79it/s]Extractor Estimating: 366it [03:44,  1.78it/s]Extractor Estimating: 367it [03:45,  1.79it/s]Extractor Estimating: 368it [03:45,  1.77it/s]Extractor Estimating: 369it [03:46,  1.77it/s]Extractor Estimating: 370it [03:46,  1.81it/s]Extractor Estimating: 371it [03:47,  1.77it/s]Extractor Estimating: 372it [03:48,  1.79it/s]Extractor Estimating: 373it [03:48,  1.78it/s]Extractor Estimating: 374it [03:49,  1.85it/s]Extractor Estimating: 374it [03:49,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:22,442 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:22,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:22,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:22,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:22,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:02:23,035 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:02:23,036 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:02:23,619 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:02:24,677 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:02:24,678 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:27,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:27,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:27,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:27,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:02:27,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:02:28,255 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:02:28,256 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:02:28,816 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:02:28,985 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:02:28,985 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 22:17:05,577 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 22:17:05,589 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7458 mean pseudo reward: 0.9554144007923473
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 19836
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19936, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19936, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.973, loss:535.4301
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.036, loss:481.1907
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.031, loss:501.0571
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 89, avg_time 0.993, loss:466.7838
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 189, avg_time 1.003, loss:483.3551
>> valid entity prec:0.5606, rec:0.5640, f1:0.5623
>> valid relation prec:0.2110, rec:0.0566, f1:0.0892
>> valid relation with NER prec:0.2110, rec:0.0566, f1:0.0892
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 289, avg_time 2.597, loss:492.8952
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 78, avg_time 0.991, loss:442.6501
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 178, avg_time 0.987, loss:484.2423
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 278, avg_time 0.993, loss:504.3829
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 67, avg_time 1.000, loss:473.5322
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5515, rec:0.5196, f1:0.5351
>> valid relation prec:0.1536, rec:0.0377, f1:0.0606
>> valid relation with NER prec:0.1536, rec:0.0377, f1:0.0606
g_step 1100, step 167, avg_time 2.580, loss:484.6772
g_step 1200, step 267, avg_time 0.991, loss:504.4440
g_step 1300, step 56, avg_time 0.990, loss:467.5098
g_step 1400, step 156, avg_time 0.998, loss:451.9858
g_step 1500, step 256, avg_time 0.980, loss:481.9762
>> valid entity prec:0.5439, rec:0.5623, f1:0.5529
>> valid relation prec:0.1410, rec:0.0293, f1:0.0485
>> valid relation with NER prec:0.1410, rec:0.0293, f1:0.0485
g_step 1600, step 45, avg_time 2.582, loss:462.7098
g_step 1700, step 145, avg_time 0.988, loss:442.2512
g_step 1800, step 245, avg_time 1.003, loss:481.0267
g_step 1900, step 34, avg_time 0.993, loss:427.4346
g_step 2000, step 134, avg_time 0.997, loss:432.1572
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5619, rec:0.5029, f1:0.5307
>> valid relation prec:0.1420, rec:0.0414, f1:0.0641
>> valid relation with NER prec:0.1420, rec:0.0414, f1:0.0641
g_step 2100, step 234, avg_time 2.583, loss:446.1204
g_step 2200, step 23, avg_time 0.995, loss:429.6036
g_step 2300, step 123, avg_time 0.987, loss:402.8800
g_step 2400, step 223, avg_time 0.993, loss:413.8264
g_step 2500, step 12, avg_time 0.989, loss:427.1187
>> valid entity prec:0.5866, rec:0.4357, f1:0.5000
>> valid relation prec:0.1288, rec:0.0285, f1:0.0467
>> valid relation with NER prec:0.1288, rec:0.0285, f1:0.0467
g_step 2600, step 112, avg_time 2.573, loss:387.3093
g_step 2700, step 212, avg_time 1.002, loss:410.5725
g_step 2800, step 1, avg_time 0.993, loss:410.9211
g_step 2900, step 101, avg_time 0.992, loss:362.3117
g_step 3000, step 201, avg_time 0.995, loss:390.9640
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5715, rec:0.4721, f1:0.5171
>> valid relation prec:0.1557, rec:0.0430, f1:0.0674
>> valid relation with NER prec:0.1557, rec:0.0430, f1:0.0674
g_step 3100, step 301, avg_time 2.568, loss:418.0073
g_step 3200, step 90, avg_time 0.992, loss:356.7056
g_step 3300, step 190, avg_time 0.992, loss:370.8161
g_step 3400, step 290, avg_time 0.989, loss:390.9702
g_step 3500, step 79, avg_time 0.992, loss:352.4289
>> valid entity prec:0.5796, rec:0.4825, f1:0.5266
>> valid relation prec:0.1795, rec:0.0500, f1:0.0782
>> valid relation with NER prec:0.1795, rec:0.0500, f1:0.0782
g_step 3600, step 179, avg_time 2.594, loss:380.1735
g_step 3700, step 279, avg_time 0.980, loss:371.2961
g_step 3800, step 68, avg_time 0.979, loss:348.2483
g_step 3900, step 168, avg_time 0.993, loss:352.4975
g_step 4000, step 268, avg_time 0.995, loss:368.2713
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5487, rec:0.4681, f1:0.5052
>> valid relation prec:0.1041, rec:0.0213, f1:0.0354
>> valid relation with NER prec:0.1041, rec:0.0213, f1:0.0354
g_step 4100, step 57, avg_time 2.573, loss:339.5452
g_step 4200, step 157, avg_time 0.999, loss:341.9102
g_step 4300, step 257, avg_time 0.993, loss:345.7905
g_step 4400, step 46, avg_time 0.983, loss:338.8977
g_step 4500, step 146, avg_time 0.988, loss:315.5575
>> valid entity prec:0.5418, rec:0.4557, f1:0.4951
>> valid relation prec:0.1688, rec:0.0529, f1:0.0805
>> valid relation with NER prec:0.1688, rec:0.0529, f1:0.0805
g_step 4600, step 246, avg_time 2.608, loss:356.3523
g_step 4700, step 35, avg_time 0.978, loss:325.5040
g_step 4800, step 135, avg_time 0.983, loss:311.4307
g_step 4900, step 235, avg_time 0.998, loss:334.7981
g_step 5000, step 24, avg_time 0.975, loss:331.3602
learning rate was adjusted to 0.0008
>> valid entity prec:0.5183, rec:0.4579, f1:0.4862
>> valid relation prec:0.1075, rec:0.0281, f1:0.0445
>> valid relation with NER prec:0.1075, rec:0.0281, f1:0.0445
g_step 5100, step 124, avg_time 2.595, loss:314.2106
g_step 5200, step 224, avg_time 0.981, loss:309.8463
g_step 5300, step 13, avg_time 0.968, loss:314.9420
g_step 5400, step 113, avg_time 0.994, loss:287.3771
g_step 5500, step 213, avg_time 0.982, loss:312.6144
>> valid entity prec:0.5721, rec:0.4640, f1:0.5124
>> valid relation prec:0.1472, rec:0.0359, f1:0.0577
>> valid relation with NER prec:0.1472, rec:0.0359, f1:0.0577
g_step 5600, step 2, avg_time 2.562, loss:319.2957
g_step 5700, step 102, avg_time 0.988, loss:288.9942
g_step 5800, step 202, avg_time 0.972, loss:293.9472
g_step 5900, step 302, avg_time 0.994, loss:295.0214
g_step 6000, step 91, avg_time 0.975, loss:272.0963
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5404, rec:0.5269, f1:0.5336
>> valid relation prec:0.1610, rec:0.0502, f1:0.0766
>> valid relation with NER prec:0.1610, rec:0.0502, f1:0.0766
g_step 6100, step 191, avg_time 2.574, loss:292.9117
g_step 6200, step 291, avg_time 0.985, loss:292.3773
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 22:17:05 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 22:17:05 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_22-17-05_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 22:17:06 - WARNING - datasets.builder -   Using custom data configuration default-7fd1789686a0c40a
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7fd1789686a0c40a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 22:17:07,515 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:17:07,516 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:17:07,516 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:17:07,517 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:17:07,601 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:17:07,623 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:17:07,624 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:17:07,624 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:17:07,624 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:17:07,624 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:17:07,624 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 22:17:08,065 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:17:11,127 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 22:17:11,139 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7fd1789686a0c40a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.20ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.24ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.83ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.20ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.39ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.52ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.61ba/s]100%|██████████| 8/8 [00:01<00:00,  5.50ba/s]100%|██████████| 8/8 [00:01<00:00,  4.44ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.66ba/s] 40%|████      | 2/5 [00:00<00:00,  4.14ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.31ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.41ba/s]100%|██████████| 5/5 [00:01<00:00,  4.64ba/s]100%|██████████| 5/5 [00:01<00:00,  4.37ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.81ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.97ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.42ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.67ba/s]100%|██████████| 8/8 [00:00<00:00, 11.09ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  5.34ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.63ba/s]100%|██████████| 5/5 [00:00<00:00, 10.05ba/s]100%|██████████| 5/5 [00:00<00:00,  9.31ba/s]
[INFO|trainer.py:414] 2023-08-28 22:17:16,028 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 22:17:16,048 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 22:17:16,048 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 22:17:16,048 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 22:17:16,048 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 22:17:16,049 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 22:17:16,049 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 22:17:16,049 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.33it/s]  0%|          | 2/585 [00:00<02:51,  3.39it/s]  1%|          | 3/585 [00:00<02:50,  3.42it/s]  1%|          | 4/585 [00:01<02:49,  3.43it/s]  1%|          | 5/585 [00:01<02:54,  3.32it/s]  1%|          | 6/585 [00:01<02:52,  3.36it/s]  1%|          | 7/585 [00:02<02:50,  3.39it/s]  1%|▏         | 8/585 [00:02<02:49,  3.41it/s]  2%|▏         | 9/585 [00:02<02:48,  3.42it/s]  2%|▏         | 10/585 [00:02<02:47,  3.42it/s]  2%|▏         | 11/585 [00:03<02:47,  3.43it/s]  2%|▏         | 12/585 [00:03<02:46,  3.43it/s]  2%|▏         | 13/585 [00:03<02:46,  3.43it/s]  2%|▏         | 14/585 [00:04<02:46,  3.44it/s]  3%|▎         | 15/585 [00:04<02:45,  3.44it/s]  3%|▎         | 16/585 [00:04<02:46,  3.43it/s]  3%|▎         | 17/585 [00:04<02:45,  3.43it/s]  3%|▎         | 18/585 [00:05<02:45,  3.44it/s]  3%|▎         | 19/585 [00:05<02:44,  3.44it/s]  3%|▎         | 20/585 [00:05<02:44,  3.44it/s]  4%|▎         | 21/585 [00:06<02:43,  3.44it/s]  4%|▍         | 22/585 [00:06<02:43,  3.44it/s]  4%|▍         | 23/585 [00:06<02:43,  3.44it/s]  4%|▍         | 24/585 [00:07<02:42,  3.44it/s]  4%|▍         | 25/585 [00:07<02:42,  3.44it/s]  4%|▍         | 26/585 [00:07<02:42,  3.44it/s]  5%|▍         | 27/585 [00:07<02:54,  3.19it/s]  5%|▍         | 28/585 [00:08<02:50,  3.26it/s]  5%|▍         | 29/585 [00:08<02:47,  3.31it/s]  5%|▌         | 30/585 [00:08<02:45,  3.35it/s]  5%|▌         | 31/585 [00:09<02:43,  3.38it/s]  5%|▌         | 32/585 [00:09<02:42,  3.40it/s]  6%|▌         | 33/585 [00:09<02:41,  3.41it/s]  6%|▌         | 34/585 [00:09<02:41,  3.42it/s]  6%|▌         | 35/585 [00:10<02:40,  3.43it/s]  6%|▌         | 36/585 [00:10<02:40,  3.43it/s]  6%|▋         | 37/585 [00:10<02:39,  3.43it/s]  6%|▋         | 38/585 [00:11<02:39,  3.42it/s]  7%|▋         | 39/585 [00:11<02:39,  3.42it/s]  7%|▋         | 40/585 [00:11<02:38,  3.43it/s]  7%|▋         | 41/585 [00:12<02:38,  3.43it/s]  7%|▋         | 42/585 [00:12<02:38,  3.43it/s]  7%|▋         | 43/585 [00:12<02:37,  3.44it/s]  8%|▊         | 44/585 [00:12<02:37,  3.44it/s]  8%|▊         | 45/585 [00:13<02:37,  3.44it/s]  8%|▊         | 46/585 [00:13<02:36,  3.44it/s]  8%|▊         | 47/585 [00:13<02:36,  3.44it/s]  8%|▊         | 48/585 [00:14<02:36,  3.44it/s]  8%|▊         | 49/585 [00:14<02:39,  3.37it/s]  9%|▊         | 50/585 [00:14<02:37,  3.39it/s]  9%|▊         | 51/585 [00:14<02:36,  3.40it/s]  9%|▉         | 52/585 [00:15<02:36,  3.41it/s]  9%|▉         | 53/585 [00:15<02:35,  3.42it/s]  9%|▉         | 54/585 [00:15<02:35,  3.42it/s]  9%|▉         | 55/585 [00:16<02:34,  3.43it/s] 10%|▉         | 56/585 [00:16<02:34,  3.43it/s] 10%|▉         | 57/585 [00:16<02:33,  3.43it/s] 10%|▉         | 58/585 [00:16<02:33,  3.43it/s] 10%|█         | 59/585 [00:17<02:33,  3.44it/s] 10%|█         | 60/585 [00:17<02:34,  3.40it/s] 10%|█         | 61/585 [00:17<02:33,  3.41it/s] 11%|█         | 62/585 [00:18<02:33,  3.41it/s] 11%|█         | 63/585 [00:18<02:32,  3.42it/s] 11%|█         | 64/585 [00:18<02:32,  3.43it/s] 11%|█         | 65/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 70/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 71/585 [00:20<02:30,  3.41it/s] 12%|█▏        | 72/585 [00:21<02:30,  3.42it/s] 12%|█▏        | 73/585 [00:21<02:29,  3.42it/s] 13%|█▎        | 74/585 [00:21<02:29,  3.43it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.43it/s] 13%|█▎        | 76/585 [00:22<02:28,  3.43it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.43it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.44it/s] 14%|█▎        | 79/585 [00:23<02:28,  3.42it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.41it/s] 14%|█▍        | 81/585 [00:23<02:27,  3.42it/s] 14%|█▍        | 82/585 [00:24<02:27,  3.41it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.42it/s] 14%|█▍        | 84/585 [00:24<02:26,  3.42it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:25<02:25,  3.43it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.43it/s] 15%|█▌        | 89/585 [00:26<02:24,  3.43it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.43it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 92/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.44it/s] 16%|█▌        | 94/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 95/585 [00:27<02:25,  3.36it/s] 16%|█▋        | 96/585 [00:28<02:24,  3.38it/s] 17%|█▋        | 97/585 [00:28<02:23,  3.40it/s] 17%|█▋        | 98/585 [00:28<02:22,  3.41it/s] 17%|█▋        | 99/585 [00:28<02:22,  3.41it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.42it/s] 17%|█▋        | 101/585 [00:29<02:21,  3.42it/s] 17%|█▋        | 102/585 [00:29<02:20,  3.43it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 106/585 [00:31<02:19,  3.42it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.43it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.43it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.43it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 113/585 [00:33<02:17,  3.43it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.43it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.43it/s] 20%|█▉        | 116/585 [00:33<02:16,  3.43it/s] 20%|██        | 117/585 [00:34<02:17,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 22:17:50,327 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:17:50,327 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:17:50,327 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.77it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.13it/s][A
  3%|▎         | 17/611 [00:00<00:12, 45.93it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.53it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.88it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.53it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.42it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.23it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.42it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.59it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.44it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.37it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.25it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.16it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.96it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.10it/s][A
 14%|█▍        | 87/611 [00:01<00:12, 43.10it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 43.73it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.03it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.14it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.24it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.14it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.14it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.98it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.80it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.95it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.15it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.39it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.44it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.40it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.24it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.23it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.05it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.01it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.08it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.27it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.34it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.47it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.31it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.29it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.24it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.10it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.05it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.16it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.22it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.40it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.40it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.36it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.27it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.18it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.09it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.03it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.13it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.28it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.43it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.43it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.24it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.28it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.22it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.08it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.09it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.11it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.30it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.44it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.28it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.26it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.26it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.20it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.01it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.04it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.13it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.28it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.41it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.32it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.30it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.32it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.10it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.02it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.07it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.21it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.31it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.40it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.39it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.39it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.34it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.15it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.03it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.02it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.14it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.35it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.37it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.40it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.31it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.32it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.22it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.10it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.13it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.09it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.29it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.37it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.31it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.22it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.22it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.05it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.15it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.10it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.26it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.29it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.41it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.35it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.31it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.27it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.20it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.10it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.20it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.23it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.33it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.38it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.32it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.28it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.16it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:48<02:17,  3.41it/s]
100%|██████████| 611/611 [00:13<00:00, 44.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:18:04,256 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 22:18:04,374 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:18:09,653 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:18:09,680 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:18:09,692 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:03<1:09:09,  8.89s/it] 20%|██        | 119/585 [01:03<49:00,  6.31s/it]   21%|██        | 120/585 [01:03<34:55,  4.51s/it] 21%|██        | 121/585 [01:04<25:04,  3.24s/it] 21%|██        | 122/585 [01:04<18:11,  2.36s/it] 21%|██        | 123/585 [01:04<13:23,  1.74s/it] 21%|██        | 124/585 [01:04<10:02,  1.31s/it] 21%|██▏       | 125/585 [01:05<07:41,  1.00s/it] 22%|██▏       | 126/585 [01:05<06:02,  1.27it/s] 22%|██▏       | 127/585 [01:05<04:54,  1.56it/s] 22%|██▏       | 128/585 [01:06<04:05,  1.86it/s] 22%|██▏       | 129/585 [01:06<03:31,  2.15it/s] 22%|██▏       | 130/585 [01:06<03:11,  2.37it/s] 22%|██▏       | 131/585 [01:07<02:54,  2.60it/s] 23%|██▎       | 132/585 [01:07<02:41,  2.80it/s] 23%|██▎       | 133/585 [01:07<02:32,  2.95it/s] 23%|██▎       | 134/585 [01:07<02:26,  3.07it/s] 23%|██▎       | 135/585 [01:08<02:22,  3.16it/s] 23%|██▎       | 136/585 [01:08<02:19,  3.22it/s] 23%|██▎       | 137/585 [01:08<02:16,  3.27it/s] 24%|██▎       | 138/585 [01:09<02:15,  3.31it/s] 24%|██▍       | 139/585 [01:09<02:13,  3.33it/s] 24%|██▍       | 140/585 [01:09<02:12,  3.35it/s] 24%|██▍       | 141/585 [01:09<02:12,  3.34it/s] 24%|██▍       | 142/585 [01:10<02:12,  3.35it/s] 24%|██▍       | 143/585 [01:10<02:11,  3.36it/s] 25%|██▍       | 144/585 [01:10<02:10,  3.37it/s] 25%|██▍       | 145/585 [01:11<02:10,  3.38it/s] 25%|██▍       | 146/585 [01:11<02:09,  3.38it/s] 25%|██▌       | 147/585 [01:11<02:09,  3.39it/s] 25%|██▌       | 148/585 [01:12<02:08,  3.39it/s] 25%|██▌       | 149/585 [01:12<02:08,  3.39it/s] 26%|██▌       | 150/585 [01:12<02:08,  3.39it/s] 26%|██▌       | 151/585 [01:12<02:08,  3.38it/s] 26%|██▌       | 152/585 [01:13<02:10,  3.33it/s] 26%|██▌       | 153/585 [01:13<02:09,  3.35it/s] 26%|██▋       | 154/585 [01:13<02:08,  3.36it/s] 26%|██▋       | 155/585 [01:14<02:07,  3.37it/s] 27%|██▋       | 156/585 [01:14<02:07,  3.38it/s] 27%|██▋       | 157/585 [01:14<02:06,  3.38it/s] 27%|██▋       | 158/585 [01:15<02:06,  3.38it/s] 27%|██▋       | 159/585 [01:15<02:05,  3.39it/s] 27%|██▋       | 160/585 [01:15<02:05,  3.39it/s] 28%|██▊       | 161/585 [01:15<02:05,  3.38it/s] 28%|██▊       | 162/585 [01:16<02:04,  3.39it/s] 28%|██▊       | 163/585 [01:16<02:04,  3.38it/s] 28%|██▊       | 164/585 [01:16<02:04,  3.37it/s] 28%|██▊       | 165/585 [01:17<02:04,  3.38it/s] 28%|██▊       | 166/585 [01:17<02:03,  3.38it/s] 29%|██▊       | 167/585 [01:17<02:03,  3.38it/s] 29%|██▊       | 168/585 [01:17<02:03,  3.38it/s] 29%|██▉       | 169/585 [01:18<02:02,  3.39it/s] 29%|██▉       | 170/585 [01:18<02:02,  3.39it/s] 29%|██▉       | 171/585 [01:18<02:02,  3.38it/s] 29%|██▉       | 172/585 [01:19<02:02,  3.38it/s] 30%|██▉       | 173/585 [01:19<02:01,  3.39it/s] 30%|██▉       | 174/585 [01:19<02:02,  3.37it/s] 30%|██▉       | 175/585 [01:20<02:01,  3.38it/s] 30%|███       | 176/585 [01:20<02:01,  3.38it/s] 30%|███       | 177/585 [01:20<02:00,  3.38it/s] 30%|███       | 178/585 [01:20<02:00,  3.38it/s] 31%|███       | 179/585 [01:21<01:59,  3.39it/s] 31%|███       | 180/585 [01:21<01:59,  3.39it/s] 31%|███       | 181/585 [01:21<01:59,  3.39it/s] 31%|███       | 182/585 [01:22<01:59,  3.39it/s] 31%|███▏      | 183/585 [01:22<01:58,  3.39it/s] 31%|███▏      | 184/585 [01:22<01:58,  3.39it/s] 32%|███▏      | 185/585 [01:23<02:00,  3.33it/s] 32%|███▏      | 186/585 [01:23<01:59,  3.35it/s] 32%|███▏      | 187/585 [01:23<01:58,  3.36it/s] 32%|███▏      | 188/585 [01:23<01:57,  3.37it/s] 32%|███▏      | 189/585 [01:24<01:57,  3.37it/s] 32%|███▏      | 190/585 [01:24<01:56,  3.38it/s] 33%|███▎      | 191/585 [01:24<01:56,  3.38it/s] 33%|███▎      | 192/585 [01:25<01:56,  3.38it/s] 33%|███▎      | 193/585 [01:25<01:55,  3.38it/s] 33%|███▎      | 194/585 [01:25<01:55,  3.38it/s] 33%|███▎      | 195/585 [01:25<01:55,  3.38it/s] 34%|███▎      | 196/585 [01:26<01:54,  3.38it/s] 34%|███▎      | 197/585 [01:26<01:54,  3.38it/s] 34%|███▍      | 198/585 [01:26<01:54,  3.38it/s] 34%|███▍      | 199/585 [01:27<01:54,  3.38it/s] 34%|███▍      | 200/585 [01:27<01:53,  3.38it/s] 34%|███▍      | 201/585 [01:27<01:53,  3.38it/s] 35%|███▍      | 202/585 [01:28<01:53,  3.37it/s] 35%|███▍      | 203/585 [01:28<01:53,  3.37it/s] 35%|███▍      | 204/585 [01:28<01:52,  3.38it/s] 35%|███▌      | 205/585 [01:28<01:52,  3.38it/s] 35%|███▌      | 206/585 [01:29<01:53,  3.35it/s] 35%|███▌      | 207/585 [01:29<01:52,  3.36it/s] 36%|███▌      | 208/585 [01:29<01:51,  3.37it/s] 36%|███▌      | 209/585 [01:30<01:51,  3.37it/s] 36%|███▌      | 210/585 [01:30<01:51,  3.37it/s] 36%|███▌      | 211/585 [01:30<01:50,  3.38it/s] 36%|███▌      | 212/585 [01:31<01:50,  3.38it/s] 36%|███▋      | 213/585 [01:31<01:49,  3.38it/s] 37%|███▋      | 214/585 [01:31<01:49,  3.38it/s] 37%|███▋      | 215/585 [01:31<01:49,  3.38it/s] 37%|███▋      | 216/585 [01:32<01:49,  3.38it/s] 37%|███▋      | 217/585 [01:32<01:49,  3.36it/s] 37%|███▋      | 218/585 [01:32<01:48,  3.37it/s] 37%|███▋      | 219/585 [01:33<01:48,  3.37it/s] 38%|███▊      | 220/585 [01:33<01:48,  3.38it/s] 38%|███▊      | 221/585 [01:33<01:47,  3.38it/s] 38%|███▊      | 222/585 [01:33<01:46,  3.40it/s] 38%|███▊      | 223/585 [01:34<01:46,  3.41it/s] 38%|███▊      | 224/585 [01:34<01:45,  3.41it/s] 38%|███▊      | 225/585 [01:34<01:45,  3.42it/s] 39%|███▊      | 226/585 [01:35<01:44,  3.42it/s] 39%|███▉      | 227/585 [01:35<01:44,  3.43it/s] 39%|███▉      | 228/585 [01:35<01:45,  3.40it/s] 39%|███▉      | 229/585 [01:36<01:44,  3.40it/s] 39%|███▉      | 230/585 [01:36<01:48,  3.26it/s] 39%|███▉      | 231/585 [01:36<01:46,  3.31it/s] 40%|███▉      | 232/585 [01:36<01:45,  3.35it/s] 40%|███▉      | 233/585 [01:37<01:44,  3.37it/s] 40%|████      | 234/585 [01:37<01:43,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 22:18:53,615 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:18:53,616 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:18:53,616 >>   Batch size = 8
{'eval_loss': 0.9238537549972534, 'eval_runtime': 13.8239, 'eval_samples_per_second': 353.155, 'eval_steps_per_second': 44.199, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 56.01it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.60it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.55it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.61it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.90it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.69it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.56it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.27it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.11it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.30it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.46it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.48it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.32it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.22it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.19it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.17it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.17it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.24it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.30it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.49it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.55it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.28it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.33it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.20it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.22it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.23it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.14it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.40it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.54it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.45it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.35it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.37it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.28it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.34it/s][A
 29%|██▉       | 177/611 [00:04<00:09, 44.28it/s][A
 30%|██▉       | 182/611 [00:04<00:10, 41.87it/s][A
 31%|███       | 187/611 [00:04<00:09, 42.81it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 43.40it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 43.73it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.93it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.97it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.12it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.06it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.84it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 43.97it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.15it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.30it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.41it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.40it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.25it/s][A
 42%|████▏     | 257/611 [00:05<00:07, 44.26it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.08it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 43.96it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.11it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.30it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.37it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.50it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.39it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.30it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.23it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.07it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.01it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.09it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.26it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.34it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.45it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.48it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.43it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.16it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.10it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.07it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.21it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.27it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.35it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.46it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.41it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.38it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.18it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.09it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.18it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.13it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.24it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.34it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.47it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.49it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.38it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.22it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.19it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.12it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.14it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.19it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.36it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.44it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.40it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.34it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.34it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.24it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.17it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.11it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.18it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.28it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.53it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.35it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.28it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.21it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.14it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.11it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.24it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.25it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.34it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.36it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.39it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.29it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.22it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.16it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.21it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.18it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.22it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.24it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.40it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.47it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.47it/s][A 40%|████      | 234/585 [01:51<01:43,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:19:07,459 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 22:19:07,478 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:19:11,357 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:19:11,377 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:19:11,402 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [02:05<50:55,  8.73s/it] 40%|████      | 236/585 [02:06<36:03,  6.20s/it] 41%|████      | 237/585 [02:06<25:40,  4.43s/it] 41%|████      | 238/585 [02:06<18:25,  3.19s/it] 41%|████      | 239/585 [02:07<13:21,  2.32s/it] 41%|████      | 240/585 [02:07<09:49,  1.71s/it] 41%|████      | 241/585 [02:07<07:21,  1.28s/it] 41%|████▏     | 242/585 [02:07<05:38,  1.01it/s] 42%|████▏     | 243/585 [02:08<04:25,  1.29it/s] 42%|████▏     | 244/585 [02:08<03:35,  1.58it/s] 42%|████▏     | 245/585 [02:08<02:59,  1.89it/s] 42%|████▏     | 246/585 [02:09<02:35,  2.19it/s] 42%|████▏     | 247/585 [02:09<02:18,  2.44it/s] 42%|████▏     | 248/585 [02:09<02:06,  2.67it/s] 43%|████▎     | 249/585 [02:10<01:57,  2.86it/s] 43%|████▎     | 250/585 [02:10<01:51,  3.02it/s] 43%|████▎     | 251/585 [02:10<01:46,  3.13it/s] 43%|████▎     | 252/585 [02:10<01:43,  3.22it/s] 43%|████▎     | 253/585 [02:11<01:41,  3.28it/s] 43%|████▎     | 254/585 [02:11<01:39,  3.33it/s] 44%|████▎     | 255/585 [02:11<01:38,  3.36it/s] 44%|████▍     | 256/585 [02:12<01:37,  3.38it/s] 44%|████▍     | 257/585 [02:12<01:36,  3.40it/s] 44%|████▍     | 258/585 [02:12<01:37,  3.36it/s] 44%|████▍     | 259/585 [02:12<01:36,  3.38it/s] 44%|████▍     | 260/585 [02:13<01:35,  3.40it/s] 45%|████▍     | 261/585 [02:13<01:34,  3.41it/s] 45%|████▍     | 262/585 [02:13<01:34,  3.42it/s] 45%|████▍     | 263/585 [02:14<01:33,  3.43it/s] 45%|████▌     | 264/585 [02:14<01:33,  3.43it/s] 45%|████▌     | 265/585 [02:14<01:33,  3.44it/s] 45%|████▌     | 266/585 [02:14<01:32,  3.43it/s] 46%|████▌     | 267/585 [02:15<01:32,  3.42it/s] 46%|████▌     | 268/585 [02:15<01:32,  3.41it/s] 46%|████▌     | 269/585 [02:15<01:35,  3.30it/s] 46%|████▌     | 270/585 [02:16<01:34,  3.32it/s] 46%|████▋     | 271/585 [02:16<01:33,  3.34it/s] 46%|████▋     | 272/585 [02:16<01:33,  3.35it/s] 47%|████▋     | 273/585 [02:17<01:32,  3.36it/s] 47%|████▋     | 274/585 [02:17<01:32,  3.37it/s] 47%|████▋     | 275/585 [02:17<01:31,  3.38it/s] 47%|████▋     | 276/585 [02:17<01:31,  3.38it/s] 47%|████▋     | 277/585 [02:18<01:31,  3.38it/s] 48%|████▊     | 278/585 [02:18<01:30,  3.38it/s] 48%|████▊     | 279/585 [02:18<01:30,  3.39it/s] 48%|████▊     | 280/585 [02:19<01:31,  3.35it/s] 48%|████▊     | 281/585 [02:19<01:30,  3.36it/s] 48%|████▊     | 282/585 [02:19<01:29,  3.37it/s] 48%|████▊     | 283/585 [02:20<01:29,  3.37it/s] 49%|████▊     | 284/585 [02:20<01:29,  3.38it/s] 49%|████▊     | 285/585 [02:20<01:28,  3.38it/s] 49%|████▉     | 286/585 [02:20<01:27,  3.40it/s] 49%|████▉     | 287/585 [02:21<01:27,  3.41it/s] 49%|████▉     | 288/585 [02:21<01:26,  3.42it/s] 49%|████▉     | 289/585 [02:21<01:26,  3.42it/s] 50%|████▉     | 290/585 [02:22<01:26,  3.43it/s] 50%|████▉     | 291/585 [02:22<01:26,  3.39it/s] 50%|████▉     | 292/585 [02:22<01:26,  3.40it/s] 50%|█████     | 293/585 [02:22<01:25,  3.41it/s] 50%|█████     | 294/585 [02:23<01:25,  3.42it/s] 50%|█████     | 295/585 [02:23<01:24,  3.43it/s] 51%|█████     | 296/585 [02:23<01:24,  3.43it/s] 51%|█████     | 297/585 [02:24<01:23,  3.43it/s] 51%|█████     | 298/585 [02:24<01:23,  3.43it/s] 51%|█████     | 299/585 [02:24<01:23,  3.43it/s] 51%|█████▏    | 300/585 [02:25<01:23,  3.43it/s] 51%|█████▏    | 301/585 [02:25<01:22,  3.43it/s] 52%|█████▏    | 302/585 [02:25<01:23,  3.38it/s] 52%|█████▏    | 303/585 [02:25<01:23,  3.40it/s] 52%|█████▏    | 304/585 [02:26<01:22,  3.41it/s] 52%|█████▏    | 305/585 [02:26<01:21,  3.42it/s] 52%|█████▏    | 306/585 [02:26<01:21,  3.42it/s] 52%|█████▏    | 307/585 [02:27<01:21,  3.43it/s] 53%|█████▎    | 308/585 [02:27<01:20,  3.43it/s] 53%|█████▎    | 309/585 [02:27<01:20,  3.44it/s] 53%|█████▎    | 310/585 [02:27<01:20,  3.43it/s] 53%|█████▎    | 311/585 [02:28<01:21,  3.37it/s] 53%|█████▎    | 312/585 [02:28<01:20,  3.39it/s] 54%|█████▎    | 313/585 [02:28<01:19,  3.40it/s] 54%|█████▎    | 314/585 [02:29<01:19,  3.41it/s] 54%|█████▍    | 315/585 [02:29<01:18,  3.42it/s] 54%|█████▍    | 316/585 [02:29<01:18,  3.42it/s] 54%|█████▍    | 317/585 [02:29<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:30<01:17,  3.43it/s] 55%|█████▍    | 319/585 [02:30<01:17,  3.43it/s] 55%|█████▍    | 320/585 [02:30<01:17,  3.40it/s] 55%|█████▍    | 321/585 [02:31<01:17,  3.41it/s] 55%|█████▌    | 322/585 [02:31<01:16,  3.42it/s] 55%|█████▌    | 323/585 [02:31<01:16,  3.43it/s] 55%|█████▌    | 324/585 [02:32<01:16,  3.43it/s] 56%|█████▌    | 325/585 [02:32<01:15,  3.43it/s] 56%|█████▌    | 326/585 [02:32<01:15,  3.44it/s] 56%|█████▌    | 327/585 [02:32<01:15,  3.44it/s] 56%|█████▌    | 328/585 [02:33<01:14,  3.44it/s] 56%|█████▌    | 329/585 [02:33<01:14,  3.44it/s] 56%|█████▋    | 330/585 [02:33<01:14,  3.44it/s] 57%|█████▋    | 331/585 [02:34<01:15,  3.36it/s] 57%|█████▋    | 332/585 [02:34<01:14,  3.39it/s] 57%|█████▋    | 333/585 [02:34<01:14,  3.40it/s] 57%|█████▋    | 334/585 [02:34<01:13,  3.41it/s] 57%|█████▋    | 335/585 [02:35<01:13,  3.42it/s] 57%|█████▋    | 336/585 [02:35<01:12,  3.42it/s] 58%|█████▊    | 337/585 [02:35<01:12,  3.43it/s] 58%|█████▊    | 338/585 [02:36<01:12,  3.43it/s] 58%|█████▊    | 339/585 [02:36<01:14,  3.30it/s] 58%|█████▊    | 340/585 [02:36<01:13,  3.34it/s] 58%|█████▊    | 341/585 [02:37<01:12,  3.37it/s] 58%|█████▊    | 342/585 [02:37<01:12,  3.36it/s] 59%|█████▊    | 343/585 [02:37<01:11,  3.38it/s] 59%|█████▉    | 344/585 [02:37<01:11,  3.39it/s] 59%|█████▉    | 345/585 [02:38<01:10,  3.40it/s] 59%|█████▉    | 346/585 [02:38<01:10,  3.41it/s] 59%|█████▉    | 347/585 [02:38<01:09,  3.42it/s] 59%|█████▉    | 348/585 [02:39<01:09,  3.42it/s] 60%|█████▉    | 349/585 [02:39<01:08,  3.43it/s] 60%|█████▉    | 350/585 [02:39<01:08,  3.43it/s] 60%|██████    | 351/585 [02:39<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 22:19:56,048 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:19:56,048 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:19:56,048 >>   Batch size = 8
{'eval_loss': 0.9339793920516968, 'eval_runtime': 13.8214, 'eval_samples_per_second': 353.219, 'eval_steps_per_second': 44.207, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.41it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.05it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.26it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.40it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.94it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.64it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.59it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.34it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.36it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.52it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.51it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.52it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.37it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.31it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.20it/s][A
 13%|█▎        | 82/611 [00:01<00:11, 44.26it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.08it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.15it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.38it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.48it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.52it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.36it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.40it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.28it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.17it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.19it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.20it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.27it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.39it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.44it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.39it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.29it/s][A
 27%|██▋       | 167/611 [00:03<00:09, 44.41it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.21it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.13it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.26it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.30it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.47it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.41it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.45it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.39it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.30it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.24it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.21it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.29it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.32it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.48it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.42it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.37it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.29it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.19it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.26it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.24it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.33it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.30it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.40it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.41it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.43it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.29it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.28it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.16it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.20it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.30it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.38it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.42it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.43it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.44it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.32it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.23it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.19it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.28it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.39it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.45it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.41it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.43it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.30it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.25it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.17it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.22it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.31it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.39it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.42it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.42it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.41it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.38it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.29it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.16it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.20it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.35it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.42it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.34it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.35it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.44it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.41it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.12it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.02it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.15it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.33it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.37it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.34it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.36it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.38it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.41it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.11it/s][A
 87%|████████▋ | 532/611 [00:11<00:01, 44.12it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.27it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.28it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.35it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 43.18it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 43.59it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.88it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.93it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 43.99it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.01it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.20it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.28it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.18it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.23it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.39it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.31it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.31it/s][A 60%|██████    | 351/585 [02:53<01:08,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:20:09,888 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 22:20:09,923 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:20:14,010 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:20:14,040 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:20:14,138 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [03:05<30:46,  7.92s/it] 60%|██████    | 353/585 [03:05<21:47,  5.64s/it] 61%|██████    | 354/585 [03:06<15:31,  4.03s/it] 61%|██████    | 355/585 [03:06<11:09,  2.91s/it] 61%|██████    | 356/585 [03:06<08:07,  2.13s/it] 61%|██████    | 357/585 [03:07<05:59,  1.58s/it] 61%|██████    | 358/585 [03:07<04:30,  1.19s/it] 61%|██████▏   | 359/585 [03:07<03:28,  1.08it/s] 62%|██████▏   | 360/585 [03:08<02:45,  1.36it/s] 62%|██████▏   | 361/585 [03:08<02:15,  1.66it/s] 62%|██████▏   | 362/585 [03:08<01:53,  1.96it/s] 62%|██████▏   | 363/585 [03:08<01:38,  2.24it/s] 62%|██████▏   | 364/585 [03:09<01:29,  2.46it/s] 62%|██████▏   | 365/585 [03:09<01:22,  2.68it/s] 63%|██████▎   | 366/585 [03:09<01:16,  2.86it/s] 63%|██████▎   | 367/585 [03:10<01:12,  3.00it/s] 63%|██████▎   | 368/585 [03:10<01:09,  3.11it/s] 63%|██████▎   | 369/585 [03:10<01:07,  3.19it/s] 63%|██████▎   | 370/585 [03:11<01:06,  3.25it/s] 63%|██████▎   | 371/585 [03:11<01:05,  3.29it/s] 64%|██████▎   | 372/585 [03:11<01:04,  3.32it/s] 64%|██████▍   | 373/585 [03:11<01:03,  3.34it/s] 64%|██████▍   | 374/585 [03:12<01:02,  3.35it/s] 64%|██████▍   | 375/585 [03:12<01:02,  3.35it/s] 64%|██████▍   | 376/585 [03:12<01:02,  3.37it/s] 64%|██████▍   | 377/585 [03:13<01:01,  3.37it/s] 65%|██████▍   | 378/585 [03:13<01:01,  3.38it/s] 65%|██████▍   | 379/585 [03:13<01:00,  3.38it/s] 65%|██████▍   | 380/585 [03:13<01:00,  3.38it/s] 65%|██████▌   | 381/585 [03:14<01:00,  3.38it/s] 65%|██████▌   | 382/585 [03:14<00:59,  3.38it/s] 65%|██████▌   | 383/585 [03:14<00:59,  3.39it/s] 66%|██████▌   | 384/585 [03:15<00:59,  3.39it/s] 66%|██████▌   | 385/585 [03:15<00:59,  3.39it/s] 66%|██████▌   | 386/585 [03:15<01:00,  3.31it/s] 66%|██████▌   | 387/585 [03:16<00:59,  3.34it/s] 66%|██████▋   | 388/585 [03:16<00:58,  3.35it/s] 66%|██████▋   | 389/585 [03:16<00:58,  3.36it/s] 67%|██████▋   | 390/585 [03:16<00:57,  3.37it/s] 67%|██████▋   | 391/585 [03:17<00:57,  3.38it/s] 67%|██████▋   | 392/585 [03:17<00:57,  3.38it/s] 67%|██████▋   | 393/585 [03:17<00:56,  3.38it/s] 67%|██████▋   | 394/585 [03:18<00:56,  3.38it/s] 68%|██████▊   | 395/585 [03:18<00:56,  3.39it/s] 68%|██████▊   | 396/585 [03:18<00:55,  3.39it/s] 68%|██████▊   | 397/585 [03:19<00:55,  3.36it/s] 68%|██████▊   | 398/585 [03:19<00:55,  3.37it/s] 68%|██████▊   | 399/585 [03:19<00:55,  3.37it/s] 68%|██████▊   | 400/585 [03:19<00:54,  3.38it/s] 69%|██████▊   | 401/585 [03:20<00:54,  3.38it/s] 69%|██████▊   | 402/585 [03:20<00:54,  3.39it/s] 69%|██████▉   | 403/585 [03:20<00:53,  3.38it/s] 69%|██████▉   | 404/585 [03:21<00:53,  3.38it/s] 69%|██████▉   | 405/585 [03:21<00:53,  3.39it/s] 69%|██████▉   | 406/585 [03:21<00:52,  3.39it/s] 70%|██████▉   | 407/585 [03:21<00:52,  3.39it/s] 70%|██████▉   | 408/585 [03:22<00:52,  3.36it/s] 70%|██████▉   | 409/585 [03:22<00:52,  3.37it/s] 70%|███████   | 410/585 [03:22<00:51,  3.38it/s] 70%|███████   | 411/585 [03:23<00:51,  3.38it/s] 70%|███████   | 412/585 [03:23<00:51,  3.38it/s] 71%|███████   | 413/585 [03:23<00:50,  3.38it/s] 71%|███████   | 414/585 [03:24<00:50,  3.38it/s] 71%|███████   | 415/585 [03:24<00:50,  3.39it/s] 71%|███████   | 416/585 [03:24<00:49,  3.39it/s] 71%|███████▏  | 417/585 [03:24<00:49,  3.39it/s] 71%|███████▏  | 418/585 [03:25<00:49,  3.39it/s] 72%|███████▏  | 419/585 [03:25<00:50,  3.32it/s] 72%|███████▏  | 420/585 [03:25<00:49,  3.34it/s] 72%|███████▏  | 421/585 [03:26<00:48,  3.36it/s] 72%|███████▏  | 422/585 [03:26<00:48,  3.36it/s] 72%|███████▏  | 423/585 [03:26<00:48,  3.37it/s] 72%|███████▏  | 424/585 [03:27<00:47,  3.38it/s] 73%|███████▎  | 425/585 [03:27<00:47,  3.38it/s] 73%|███████▎  | 426/585 [03:27<00:47,  3.38it/s] 73%|███████▎  | 427/585 [03:27<00:46,  3.38it/s] 73%|███████▎  | 428/585 [03:28<00:46,  3.38it/s] 73%|███████▎  | 429/585 [03:28<00:46,  3.38it/s] 74%|███████▎  | 430/585 [03:28<00:46,  3.32it/s] 74%|███████▎  | 431/585 [03:29<00:47,  3.23it/s] 74%|███████▍  | 432/585 [03:29<00:46,  3.28it/s] 74%|███████▍  | 433/585 [03:29<00:45,  3.31it/s] 74%|███████▍  | 434/585 [03:30<00:45,  3.33it/s] 74%|███████▍  | 435/585 [03:30<00:44,  3.35it/s] 75%|███████▍  | 436/585 [03:30<00:44,  3.36it/s] 75%|███████▍  | 437/585 [03:30<00:43,  3.37it/s] 75%|███████▍  | 438/585 [03:31<00:43,  3.37it/s] 75%|███████▌  | 439/585 [03:31<00:43,  3.38it/s] 75%|███████▌  | 440/585 [03:31<00:42,  3.38it/s] 75%|███████▌  | 441/585 [03:32<00:42,  3.38it/s] 76%|███████▌  | 442/585 [03:32<00:42,  3.37it/s] 76%|███████▌  | 443/585 [03:32<00:42,  3.37it/s] 76%|███████▌  | 444/585 [03:32<00:41,  3.38it/s] 76%|███████▌  | 445/585 [03:33<00:41,  3.38it/s] 76%|███████▌  | 446/585 [03:33<00:41,  3.38it/s] 76%|███████▋  | 447/585 [03:33<00:40,  3.38it/s] 77%|███████▋  | 448/585 [03:34<00:40,  3.38it/s] 77%|███████▋  | 449/585 [03:34<00:40,  3.38it/s] 77%|███████▋  | 450/585 [03:34<00:39,  3.39it/s] 77%|███████▋  | 451/585 [03:35<00:39,  3.38it/s] 77%|███████▋  | 452/585 [03:35<00:39,  3.38it/s] 77%|███████▋  | 453/585 [03:35<00:40,  3.26it/s] 78%|███████▊  | 454/585 [03:35<00:39,  3.30it/s] 78%|███████▊  | 455/585 [03:36<00:39,  3.32it/s] 78%|███████▊  | 456/585 [03:36<00:40,  3.19it/s] 78%|███████▊  | 457/585 [03:36<00:39,  3.25it/s] 78%|███████▊  | 458/585 [03:37<00:38,  3.30it/s] 78%|███████▊  | 459/585 [03:37<00:37,  3.34it/s] 79%|███████▊  | 460/585 [03:37<00:37,  3.37it/s] 79%|███████▉  | 461/585 [03:38<00:36,  3.39it/s] 79%|███████▉  | 462/585 [03:38<00:36,  3.40it/s] 79%|███████▉  | 463/585 [03:38<00:36,  3.36it/s] 79%|███████▉  | 464/585 [03:38<00:35,  3.38it/s] 79%|███████▉  | 465/585 [03:39<00:35,  3.40it/s] 80%|███████▉  | 466/585 [03:39<00:34,  3.41it/s] 80%|███████▉  | 467/585 [03:39<00:34,  3.42it/s] 80%|████████  | 468/585 [03:40<00:34,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 22:20:56,218 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:20:56,218 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:20:56,218 >>   Batch size = 8
{'eval_loss': 0.9488993883132935, 'eval_runtime': 13.8018, 'eval_samples_per_second': 353.723, 'eval_steps_per_second': 44.27, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.85it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.39it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.46it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.51it/s][A
  4%|▍         | 27/611 [00:00<00:12, 44.96it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.70it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.52it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.21it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.48it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.48it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.60it/s][A
 10%|█         | 62/611 [00:01<00:12, 43.57it/s][A
 11%|█         | 67/611 [00:01<00:12, 43.80it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 43.83it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.97it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.99it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.11it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.28it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.38it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.30it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.41it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.41it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.28it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.27it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.16it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.25it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.28it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.38it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.39it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.43it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.39it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.32it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.23it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.21it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.24it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.34it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.44it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.49it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.36it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.43it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.26it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.20it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.11it/s][A
 36%|███▋      | 222/611 [00:04<00:08, 44.19it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.31it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.43it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.43it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.39it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.23it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.29it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.17it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.19it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.16it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.34it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.51it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.45it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.31it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.29it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.29it/s][A
 49%|████▉     | 302/611 [00:06<00:06, 44.21it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.16it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.14it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.31it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.49it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.48it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 43.62it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 43.94it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.01it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.99it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.05it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.11it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.36it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.41it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.29it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.26it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.34it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.27it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.13it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.07it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.24it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.31it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.42it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.40it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.34it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.30it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.30it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.20it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.17it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.16it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.35it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.33it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.29it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 41.42it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 42.45it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 43.00it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 43.39it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 43.58it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.80it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.10it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.11it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 43.86it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 43.99it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.18it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.33it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.26it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.28it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.29it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.30it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.19it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.14it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.21it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.33it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.43it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.38it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.40it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.29it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.27it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.23it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.11it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 43.16it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 43.69it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:54<00:34,  3.42it/s]
100%|██████████| 611/611 [00:13<00:00, 43.69it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:21:10,130 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 22:21:10,159 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:21:14,773 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:21:14,802 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:21:14,812 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [04:08<16:53,  8.74s/it] 80%|████████  | 470/585 [04:08<11:54,  6.21s/it] 81%|████████  | 471/585 [04:09<08:25,  4.44s/it] 81%|████████  | 472/585 [04:09<06:01,  3.19s/it] 81%|████████  | 473/585 [04:09<04:20,  2.33s/it] 81%|████████  | 474/585 [04:10<03:10,  1.72s/it] 81%|████████  | 475/585 [04:10<02:21,  1.29s/it] 81%|████████▏ | 476/585 [04:10<01:48,  1.01it/s] 82%|████████▏ | 477/585 [04:10<01:24,  1.28it/s] 82%|████████▏ | 478/585 [04:11<01:08,  1.57it/s] 82%|████████▏ | 479/585 [04:11<00:56,  1.87it/s] 82%|████████▏ | 480/585 [04:11<00:48,  2.17it/s] 82%|████████▏ | 481/585 [04:12<00:43,  2.42it/s] 82%|████████▏ | 482/585 [04:12<00:38,  2.64it/s] 83%|████████▎ | 483/585 [04:12<00:36,  2.83it/s] 83%|████████▎ | 484/585 [04:13<00:33,  2.98it/s] 83%|████████▎ | 485/585 [04:13<00:32,  3.08it/s] 83%|████████▎ | 486/585 [04:13<00:31,  3.17it/s] 83%|████████▎ | 487/585 [04:13<00:30,  3.23it/s] 83%|████████▎ | 488/585 [04:14<00:29,  3.28it/s] 84%|████████▎ | 489/585 [04:14<00:28,  3.31it/s] 84%|████████▍ | 490/585 [04:14<00:28,  3.34it/s] 84%|████████▍ | 491/585 [04:15<00:28,  3.35it/s] 84%|████████▍ | 492/585 [04:15<00:28,  3.25it/s] 84%|████████▍ | 493/585 [04:15<00:27,  3.29it/s] 84%|████████▍ | 494/585 [04:16<00:27,  3.32it/s] 85%|████████▍ | 495/585 [04:16<00:26,  3.34it/s] 85%|████████▍ | 496/585 [04:16<00:26,  3.35it/s] 85%|████████▍ | 497/585 [04:16<00:26,  3.37it/s] 85%|████████▌ | 498/585 [04:17<00:25,  3.37it/s] 85%|████████▌ | 499/585 [04:17<00:25,  3.38it/s] 85%|████████▌ | 500/585 [04:17<00:25,  3.38it/s]                                                  85%|████████▌ | 500/585 [04:17<00:25,  3.38it/s] 86%|████████▌ | 501/585 [04:18<00:24,  3.38it/s] 86%|████████▌ | 502/585 [04:18<00:24,  3.39it/s] 86%|████████▌ | 503/585 [04:18<00:26,  3.15it/s] 86%|████████▌ | 504/585 [04:19<00:25,  3.22it/s] 86%|████████▋ | 505/585 [04:19<00:24,  3.27it/s] 86%|████████▋ | 506/585 [04:19<00:23,  3.30it/s] 87%|████████▋ | 507/585 [04:19<00:23,  3.33it/s] 87%|████████▋ | 508/585 [04:20<00:22,  3.35it/s] 87%|████████▋ | 509/585 [04:20<00:22,  3.36it/s] 87%|████████▋ | 510/585 [04:20<00:22,  3.37it/s] 87%|████████▋ | 511/585 [04:21<00:21,  3.38it/s] 88%|████████▊ | 512/585 [04:21<00:21,  3.40it/s] 88%|████████▊ | 513/585 [04:21<00:21,  3.40it/s] 88%|████████▊ | 514/585 [04:21<00:20,  3.41it/s] 88%|████████▊ | 515/585 [04:22<00:20,  3.42it/s] 88%|████████▊ | 516/585 [04:22<00:20,  3.43it/s] 88%|████████▊ | 517/585 [04:22<00:19,  3.43it/s] 89%|████████▊ | 518/585 [04:23<00:19,  3.43it/s] 89%|████████▊ | 519/585 [04:23<00:19,  3.43it/s] 89%|████████▉ | 520/585 [04:23<00:18,  3.44it/s] 89%|████████▉ | 521/585 [04:24<00:18,  3.44it/s] 89%|████████▉ | 522/585 [04:24<00:18,  3.44it/s] 89%|████████▉ | 523/585 [04:24<00:18,  3.44it/s] 90%|████████▉ | 524/585 [04:24<00:18,  3.39it/s] 90%|████████▉ | 525/585 [04:25<00:17,  3.40it/s] 90%|████████▉ | 526/585 [04:25<00:17,  3.41it/s] 90%|█████████ | 527/585 [04:25<00:16,  3.42it/s] 90%|█████████ | 528/585 [04:26<00:16,  3.42it/s] 90%|█████████ | 529/585 [04:26<00:16,  3.43it/s] 91%|█████████ | 530/585 [04:26<00:16,  3.43it/s] 91%|█████████ | 531/585 [04:26<00:15,  3.43it/s] 91%|█████████ | 532/585 [04:27<00:15,  3.43it/s] 91%|█████████ | 533/585 [04:27<00:15,  3.44it/s] 91%|█████████▏| 534/585 [04:27<00:14,  3.43it/s] 91%|█████████▏| 535/585 [04:28<00:14,  3.44it/s] 92%|█████████▏| 536/585 [04:28<00:14,  3.44it/s] 92%|█████████▏| 537/585 [04:28<00:14,  3.38it/s] 92%|█████████▏| 538/585 [04:28<00:13,  3.40it/s] 92%|█████████▏| 539/585 [04:29<00:13,  3.41it/s] 92%|█████████▏| 540/585 [04:29<00:13,  3.42it/s] 92%|█████████▏| 541/585 [04:29<00:12,  3.42it/s] 93%|█████████▎| 542/585 [04:30<00:12,  3.43it/s] 93%|█████████▎| 543/585 [04:30<00:12,  3.43it/s] 93%|█████████▎| 544/585 [04:30<00:11,  3.43it/s] 93%|█████████▎| 545/585 [04:31<00:11,  3.43it/s] 93%|█████████▎| 546/585 [04:31<00:11,  3.43it/s] 94%|█████████▎| 547/585 [04:31<00:11,  3.44it/s] 94%|█████████▎| 548/585 [04:31<00:10,  3.42it/s] 94%|█████████▍| 549/585 [04:32<00:10,  3.43it/s] 94%|█████████▍| 550/585 [04:32<00:10,  3.43it/s] 94%|█████████▍| 551/585 [04:32<00:09,  3.43it/s] 94%|█████████▍| 552/585 [04:33<00:09,  3.43it/s] 95%|█████████▍| 553/585 [04:33<00:09,  3.43it/s] 95%|█████████▍| 554/585 [04:33<00:09,  3.44it/s] 95%|█████████▍| 555/585 [04:33<00:08,  3.43it/s] 95%|█████████▌| 556/585 [04:34<00:08,  3.44it/s] 95%|█████████▌| 557/585 [04:34<00:08,  3.44it/s] 95%|█████████▌| 558/585 [04:34<00:07,  3.44it/s] 96%|█████████▌| 559/585 [04:35<00:07,  3.39it/s] 96%|█████████▌| 560/585 [04:35<00:07,  3.40it/s] 96%|█████████▌| 561/585 [04:35<00:07,  3.41it/s] 96%|█████████▌| 562/585 [04:36<00:06,  3.41it/s] 96%|█████████▌| 563/585 [04:36<00:06,  3.42it/s] 96%|█████████▋| 564/585 [04:36<00:06,  3.39it/s] 97%|█████████▋| 565/585 [04:36<00:05,  3.39it/s] 97%|█████████▋| 566/585 [04:37<00:05,  3.41it/s] 97%|█████████▋| 567/585 [04:37<00:05,  3.41it/s] 97%|█████████▋| 568/585 [04:37<00:04,  3.42it/s] 97%|█████████▋| 569/585 [04:38<00:04,  3.42it/s] 97%|█████████▋| 570/585 [04:38<00:04,  3.38it/s] 98%|█████████▊| 571/585 [04:38<00:04,  3.39it/s] 98%|█████████▊| 572/585 [04:38<00:03,  3.41it/s] 98%|█████████▊| 573/585 [04:39<00:03,  3.41it/s] 98%|█████████▊| 574/585 [04:39<00:03,  3.42it/s] 98%|█████████▊| 575/585 [04:39<00:02,  3.42it/s] 98%|█████████▊| 576/585 [04:40<00:02,  3.43it/s] 99%|█████████▊| 577/585 [04:40<00:02,  3.43it/s] 99%|█████████▉| 578/585 [04:40<00:02,  3.43it/s] 99%|█████████▉| 579/585 [04:40<00:01,  3.43it/s] 99%|█████████▉| 580/585 [04:41<00:01,  3.43it/s] 99%|█████████▉| 581/585 [04:41<00:01,  3.37it/s] 99%|█████████▉| 582/585 [04:41<00:00,  3.39it/s]100%|█████████▉| 583/585 [04:42<00:00,  3.40it/s]100%|█████████▉| 584/585 [04:42<00:00,  3.41it/s]100%|██████████| 585/585 [04:42<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 22:21:58,797 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:21:58,797 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:21:58,797 >>   Batch size = 8
{'eval_loss': 0.960566520690918, 'eval_runtime': 13.8392, 'eval_samples_per_second': 352.765, 'eval_steps_per_second': 44.15, 'epoch': 4.0}
{'loss': 0.4757, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.96it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.31it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.48it/s][A
  4%|▎         | 22/611 [00:00<00:12, 45.58it/s][A
  4%|▍         | 27/611 [00:00<00:12, 45.08it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.58it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.45it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.33it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.47it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.64it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.63it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.63it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.32it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.27it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.38it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.75it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.91it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.09it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.34it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.31it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.40it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.34it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.13it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.06it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.16it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.33it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.45it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.49it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.49it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.44it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.25it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.08it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.09it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.09it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.30it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.44it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.50it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.41it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.39it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.28it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.13it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 42.75it/s][A
 36%|███▌      | 217/611 [00:04<00:09, 43.40it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 43.76it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.03it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.16it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.04it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.29it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.19it/s][A
 41%|████      | 252/611 [00:05<00:08, 43.86it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.01it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.28it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.39it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.48it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.35it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.36it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.23it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.15it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.01it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.12it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.27it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.50it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.59it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.50it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.41it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.23it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.08it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 43.91it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.08it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.23it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.40it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.47it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.35it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.39it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.27it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.15it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 43.99it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.21it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.32it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.40it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.50it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.46it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.25it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.12it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.08it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.11it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.25it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.39it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.44it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.55it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.48it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.37it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.08it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.11it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.11it/s][A
 79%|███████▉  | 482/611 [00:10<00:03, 41.55it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 42.49it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.22it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 43.66it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.01it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.02it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.02it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 43.96it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 43.72it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 43.86it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.09it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.29it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.48it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.46it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.21it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.24it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.03it/s][A
 93%|█████████▎| 567/611 [00:12<00:01, 43.84it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 43.89it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.13it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.31it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.47it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.55it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.38it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.24it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.02it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.02it/s][A100%|██████████| 585/585 [04:56<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:22:12,646 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 22:22:12,667 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:22:16,428 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:22:16,486 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:22:16,510 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 22:22:25,198 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 22:22:25,202 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117 (score: 0.9238537549972534).
                                                 100%|██████████| 585/585 [05:13<00:00,  3.42it/s]100%|██████████| 585/585 [05:13<00:00,  1.86it/s]
[INFO|trainer.py:1894] 2023-08-28 22:22:29,958 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 22:22:30,007 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:22:33,497 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:22:33,527 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:22:33,558 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:22:33,831 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:33,831 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:33,831 >>   train_loss               =     0.4722
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:33,831 >>   train_runtime            = 0:05:13.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:33,831 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:33,831 >>   train_samples_per_second =    119.453
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:33,831 >>   train_steps_per_second   =      1.864
{'eval_loss': 0.9646832346916199, 'eval_runtime': 13.8277, 'eval_samples_per_second': 353.059, 'eval_steps_per_second': 44.187, 'epoch': 5.0}
{'train_runtime': 313.89, 'train_samples_per_second': 119.453, 'train_steps_per_second': 1.864, 'train_loss': 0.47220771333091277, 'epoch': 5.0}
08/28/2023 22:22:33 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 22:22:33,930 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:22:33,931 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-28 22:22:33,931 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.04it/s]  2%|▏         | 12/611 [00:00<00:12, 48.80it/s]  3%|▎         | 17/611 [00:00<00:12, 47.27it/s]  4%|▎         | 22/611 [00:00<00:12, 46.39it/s]  4%|▍         | 27/611 [00:00<00:12, 45.75it/s]  5%|▌         | 32/611 [00:00<00:12, 45.50it/s]  6%|▌         | 37/611 [00:00<00:12, 45.33it/s]  7%|▋         | 42/611 [00:00<00:12, 44.87it/s]  8%|▊         | 47/611 [00:01<00:12, 44.31it/s]  9%|▊         | 52/611 [00:01<00:12, 44.17it/s]  9%|▉         | 57/611 [00:01<00:12, 44.40it/s] 10%|█         | 62/611 [00:01<00:12, 44.54it/s] 11%|█         | 67/611 [00:01<00:12, 44.62it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.63it/s] 13%|█▎        | 77/611 [00:01<00:11, 44.67it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.60it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.28it/s] 15%|█▌        | 92/611 [00:02<00:11, 44.05it/s] 16%|█▌        | 97/611 [00:02<00:11, 43.92it/s] 17%|█▋        | 102/611 [00:02<00:11, 44.12it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.41it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.59it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.63it/s] 20%|█▉        | 122/611 [00:02<00:11, 44.02it/s] 21%|██        | 127/611 [00:02<00:10, 44.10it/s] 22%|██▏       | 132/611 [00:02<00:10, 43.89it/s] 22%|██▏       | 137/611 [00:03<00:10, 43.85it/s] 23%|██▎       | 142/611 [00:03<00:10, 43.83it/s] 24%|██▍       | 147/611 [00:03<00:10, 44.05it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.30it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.56it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.66it/s] 27%|██▋       | 167/611 [00:03<00:09, 44.71it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.42it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.12it/s] 30%|██▉       | 182/611 [00:04<00:09, 43.98it/s] 31%|███       | 187/611 [00:04<00:09, 43.98it/s] 31%|███▏      | 192/611 [00:04<00:09, 44.14it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.17it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.45it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.63it/s] 35%|███▍      | 212/611 [00:04<00:08, 44.70it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.53it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.29it/s] 37%|███▋      | 227/611 [00:05<00:08, 44.14it/s] 38%|███▊      | 232/611 [00:05<00:08, 44.18it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.14it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.27it/s] 40%|████      | 247/611 [00:05<00:08, 44.42it/s] 41%|████      | 252/611 [00:05<00:08, 44.60it/s] 42%|████▏     | 257/611 [00:05<00:08, 40.84it/s] 43%|████▎     | 262/611 [00:05<00:08, 41.89it/s] 44%|████▎     | 267/611 [00:06<00:08, 42.63it/s] 45%|████▍     | 272/611 [00:06<00:07, 43.18it/s] 45%|████▌     | 277/611 [00:06<00:07, 43.45it/s] 46%|████▌     | 282/611 [00:06<00:07, 43.74it/s] 47%|████▋     | 287/611 [00:06<00:07, 43.92it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.11it/s] 49%|████▊     | 297/611 [00:06<00:07, 43.96it/s] 49%|████▉     | 302/611 [00:06<00:07, 44.14it/s] 50%|█████     | 307/611 [00:06<00:06, 44.26it/s] 51%|█████     | 312/611 [00:07<00:06, 44.39it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.29it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.36it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.36it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.30it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.31it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.16it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.21it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.29it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.33it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.43it/s] 60%|██████    | 367/611 [00:08<00:05, 44.36it/s] 61%|██████    | 372/611 [00:08<00:05, 44.36it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.24it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.36it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.34it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.20it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.26it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.33it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.41it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.35it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.37it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.35it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.35it/s] 71%|███████   | 432/611 [00:09<00:04, 44.30it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.16it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.25it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.24it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.35it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.33it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.39it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.36it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.34it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.29it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.23it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.29it/s] 81%|████████  | 492/611 [00:11<00:02, 44.29it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.24it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.37it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.41it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.37it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.28it/s] 85%|████████▌ | 522/611 [00:11<00:02, 44.27it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.20it/s] 87%|████████▋ | 532/611 [00:12<00:01, 44.27it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.24it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.34it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.33it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.33it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.37it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.36it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.30it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.14it/s] 94%|█████████▍| 577/611 [00:13<00:00, 44.23it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.24it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.30it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.34it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.32it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.31it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.41it/s]100%|██████████| 611/611 [00:13<00:00, 44.32it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:22:47,737 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:47,737 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:47,737 >>   eval_loss               =     0.9239
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:47,737 >>   eval_runtime            = 0:00:13.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:47,737 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:47,737 >>   eval_samples_per_second =    353.614
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:47,737 >>   eval_steps_per_second   =     44.256
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:22:47,737 >>   perplexity              =      2.519
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:22:56,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:22:56,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:22:56,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:22:56,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:22:56,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:22:57,218 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:22:57,219 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:22:57,893 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:22:58,967 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:22:58,967 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:23:04,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:23:04,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:23:04,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:23:04,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:23:04,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:23:04,851 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:23:04,852 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:23:05,882 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:23:06,066 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:23:06,066 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.65it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.59it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.66it/s]Extractor Predicting: 22it [00:13,  1.72it/s]Extractor Predicting: 23it [00:14,  1.73it/s]Extractor Predicting: 24it [00:14,  1.74it/s]Extractor Predicting: 25it [00:15,  1.71it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:17,  1.69it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.71it/s]Extractor Predicting: 31it [00:18,  1.72it/s]Extractor Predicting: 32it [00:19,  1.75it/s]Extractor Predicting: 33it [00:19,  1.72it/s]Extractor Predicting: 34it [00:20,  1.70it/s]Extractor Predicting: 35it [00:21,  1.71it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:22,  1.71it/s]Extractor Predicting: 38it [00:22,  1.71it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.71it/s]Extractor Predicting: 41it [00:24,  1.68it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:25,  1.68it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:27,  1.72it/s]Extractor Predicting: 46it [00:27,  1.71it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.65it/s]Extractor Predicting: 51it [00:30,  1.65it/s]Extractor Predicting: 52it [00:31,  1.70it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.61it/s]Extractor Predicting: 56it [00:33,  1.58it/s]Extractor Predicting: 57it [00:34,  1.60it/s]Extractor Predicting: 58it [00:34,  1.60it/s]Extractor Predicting: 59it [00:35,  1.62it/s]Extractor Predicting: 60it [00:36,  1.60it/s]Extractor Predicting: 61it [00:36,  1.60it/s]Extractor Predicting: 62it [00:37,  1.57it/s]Extractor Predicting: 63it [00:38,  1.58it/s]Extractor Predicting: 64it [00:38,  1.59it/s]Extractor Predicting: 65it [00:39,  1.58it/s]Extractor Predicting: 66it [00:40,  1.54it/s]Extractor Predicting: 67it [00:40,  1.57it/s]Extractor Predicting: 68it [00:41,  1.61it/s]Extractor Predicting: 69it [00:41,  1.60it/s]Extractor Predicting: 70it [00:42,  1.58it/s]Extractor Predicting: 71it [00:43,  1.57it/s]Extractor Predicting: 72it [00:43,  1.55it/s]Extractor Predicting: 73it [00:44,  1.57it/s]Extractor Predicting: 74it [00:45,  1.58it/s]Extractor Predicting: 75it [00:45,  1.59it/s]Extractor Predicting: 76it [00:46,  1.59it/s]Extractor Predicting: 77it [00:46,  1.58it/s]Extractor Predicting: 78it [00:47,  1.61it/s]Extractor Predicting: 79it [00:48,  1.63it/s]Extractor Predicting: 80it [00:48,  1.63it/s]Extractor Predicting: 81it [00:49,  1.63it/s]Extractor Predicting: 82it [00:50,  1.60it/s]Extractor Predicting: 83it [00:50,  1.61it/s]Extractor Predicting: 84it [00:51,  1.60it/s]Extractor Predicting: 85it [00:51,  1.59it/s]Extractor Predicting: 86it [00:52,  1.58it/s]Extractor Predicting: 87it [00:53,  1.57it/s]Extractor Predicting: 88it [00:53,  1.54it/s]Extractor Predicting: 89it [00:54,  1.55it/s]Extractor Predicting: 90it [00:55,  1.55it/s]Extractor Predicting: 91it [00:55,  1.55it/s]Extractor Predicting: 92it [00:56,  1.61it/s]Extractor Predicting: 93it [00:56,  1.68it/s]Extractor Predicting: 94it [00:57,  1.67it/s]Extractor Predicting: 95it [00:58,  1.66it/s]Extractor Predicting: 96it [00:58,  1.66it/s]Extractor Predicting: 97it [00:59,  1.67it/s]Extractor Predicting: 98it [00:59,  1.64it/s]Extractor Predicting: 99it [01:00,  1.56it/s]Extractor Predicting: 100it [01:01,  1.60it/s]Extractor Predicting: 101it [01:01,  1.52it/s]Extractor Predicting: 102it [01:02,  1.52it/s]Extractor Predicting: 103it [01:03,  1.54it/s]Extractor Predicting: 104it [01:03,  1.57it/s]Extractor Predicting: 105it [01:04,  1.58it/s]Extractor Predicting: 106it [01:05,  1.64it/s]Extractor Predicting: 107it [01:05,  1.48it/s]Extractor Predicting: 108it [01:06,  1.54it/s]Extractor Predicting: 109it [01:07,  1.55it/s]Extractor Predicting: 110it [01:07,  1.58it/s]Extractor Predicting: 111it [01:08,  1.62it/s]Extractor Predicting: 112it [01:08,  1.64it/s]Extractor Predicting: 113it [01:09,  1.59it/s]Extractor Predicting: 114it [01:10,  1.58it/s]Extractor Predicting: 115it [01:10,  1.60it/s]Extractor Predicting: 116it [01:11,  1.57it/s]Extractor Predicting: 117it [01:12,  1.55it/s]Extractor Predicting: 118it [01:12,  1.55it/s]Extractor Predicting: 119it [01:13,  1.53it/s]Extractor Predicting: 120it [01:14,  1.51it/s]Extractor Predicting: 121it [01:14,  1.52it/s]Extractor Predicting: 122it [01:15,  1.53it/s]Extractor Predicting: 123it [01:16,  1.56it/s]Extractor Predicting: 124it [01:16,  1.57it/s]Extractor Predicting: 125it [01:17,  1.58it/s]Extractor Predicting: 126it [01:17,  1.58it/s]Extractor Predicting: 127it [01:18,  1.57it/s]Extractor Predicting: 128it [01:19,  1.58it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:20,  1.55it/s]Extractor Predicting: 131it [01:21,  1.55it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.57it/s]Extractor Predicting: 134it [01:23,  1.58it/s]Extractor Predicting: 135it [01:23,  1.56it/s]Extractor Predicting: 136it [01:24,  1.57it/s]Extractor Predicting: 137it [01:24,  1.55it/s]Extractor Predicting: 138it [01:25,  1.57it/s]Extractor Predicting: 139it [01:26,  1.55it/s]Extractor Predicting: 140it [01:26,  1.54it/s]Extractor Predicting: 141it [01:27,  1.56it/s]Extractor Predicting: 142it [01:28,  1.57it/s]Extractor Predicting: 143it [01:28,  1.55it/s]Extractor Predicting: 144it [01:29,  1.59it/s]Extractor Predicting: 145it [01:30,  1.62it/s]Extractor Predicting: 146it [01:30,  1.60it/s]Extractor Predicting: 147it [01:31,  1.57it/s]Extractor Predicting: 148it [01:31,  1.58it/s]Extractor Predicting: 149it [01:32,  1.57it/s]Extractor Predicting: 150it [01:33,  1.55it/s]Extractor Predicting: 151it [01:33,  1.54it/s]Extractor Predicting: 152it [01:34,  1.55it/s]Extractor Predicting: 153it [01:35,  1.55it/s]Extractor Predicting: 154it [01:35,  1.56it/s]Extractor Predicting: 155it [01:36,  1.55it/s]Extractor Predicting: 156it [01:37,  1.50it/s]Extractor Predicting: 157it [01:37,  1.46it/s]Extractor Predicting: 158it [01:38,  1.44it/s]Extractor Predicting: 159it [01:39,  1.47it/s]Extractor Predicting: 160it [01:39,  1.50it/s]Extractor Predicting: 161it [01:40,  1.52it/s]Extractor Predicting: 162it [01:41,  1.52it/s]Extractor Predicting: 163it [01:41,  1.52it/s]Extractor Predicting: 164it [01:42,  1.56it/s]Extractor Predicting: 165it [01:43,  1.56it/s]Extractor Predicting: 166it [01:43,  1.58it/s]Extractor Predicting: 167it [01:44,  1.57it/s]Extractor Predicting: 168it [01:45,  1.57it/s]Extractor Predicting: 169it [01:45,  1.58it/s]Extractor Predicting: 170it [01:46,  1.58it/s]Extractor Predicting: 171it [01:46,  1.60it/s]Extractor Predicting: 172it [01:47,  1.62it/s]Extractor Predicting: 173it [01:48,  1.57it/s]Extractor Predicting: 174it [01:48,  1.59it/s]Extractor Predicting: 175it [01:49,  1.56it/s]Extractor Predicting: 176it [01:50,  1.58it/s]Extractor Predicting: 177it [01:50,  1.48it/s]Extractor Predicting: 178it [01:51,  1.51it/s]Extractor Predicting: 179it [01:52,  1.52it/s]Extractor Predicting: 180it [01:52,  1.54it/s]Extractor Predicting: 181it [01:53,  1.54it/s]Extractor Predicting: 182it [01:54,  1.53it/s]Extractor Predicting: 183it [01:54,  1.50it/s]Extractor Predicting: 184it [01:55,  1.38it/s]Extractor Predicting: 185it [01:56,  1.51it/s]Extractor Predicting: 185it [01:56,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:13,840 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:13,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:13,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:13,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:13,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:25:14,575 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:25:14,576 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:25:15,718 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:25:16,768 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:25:16,768 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:20,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:20,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:20,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:20,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:25:20,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:25:21,949 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:25:21,950 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:25:23,085 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:25:23,251 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:25:23,251 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3906420021762786,
  "recall": 0.07353543629659975,
  "score": 0.12377176348905361,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.45it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.67it/s]Extractor Predicting: 31it [00:19,  1.72it/s]Extractor Predicting: 32it [00:19,  1.72it/s]Extractor Predicting: 33it [00:20,  1.71it/s]Extractor Predicting: 34it [00:20,  1.72it/s]Extractor Predicting: 35it [00:21,  1.72it/s]Extractor Predicting: 36it [00:22,  1.70it/s]Extractor Predicting: 37it [00:22,  1.72it/s]Extractor Predicting: 38it [00:23,  1.71it/s]Extractor Predicting: 39it [00:23,  1.65it/s]Extractor Predicting: 40it [00:24,  1.67it/s]Extractor Predicting: 41it [00:25,  1.68it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:26,  1.69it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:27,  1.73it/s]Extractor Predicting: 46it [00:27,  1.76it/s]Extractor Predicting: 47it [00:28,  1.74it/s]Extractor Predicting: 48it [00:29,  1.74it/s]Extractor Predicting: 49it [00:29,  1.73it/s]Extractor Predicting: 50it [00:30,  1.72it/s]Extractor Predicting: 51it [00:30,  1.73it/s]Extractor Predicting: 52it [00:31,  1.73it/s]Extractor Predicting: 53it [00:32,  1.71it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.66it/s]Extractor Predicting: 56it [00:33,  1.66it/s]Extractor Predicting: 57it [00:34,  1.69it/s]Extractor Predicting: 58it [00:34,  1.76it/s]Extractor Predicting: 59it [00:35,  1.72it/s]Extractor Predicting: 60it [00:36,  1.69it/s]Extractor Predicting: 61it [00:36,  1.65it/s]Extractor Predicting: 62it [00:37,  1.61it/s]Extractor Predicting: 63it [00:38,  1.56it/s]Extractor Predicting: 64it [00:38,  1.54it/s]Extractor Predicting: 65it [00:39,  1.53it/s]Extractor Predicting: 66it [00:40,  1.53it/s]Extractor Predicting: 67it [00:40,  1.51it/s]Extractor Predicting: 68it [00:41,  1.52it/s]Extractor Predicting: 69it [00:42,  1.54it/s]Extractor Predicting: 70it [00:42,  1.56it/s]Extractor Predicting: 71it [00:43,  1.57it/s]Extractor Predicting: 72it [00:43,  1.59it/s]Extractor Predicting: 73it [00:44,  1.63it/s]Extractor Predicting: 74it [00:45,  1.63it/s]Extractor Predicting: 75it [00:45,  1.64it/s]Extractor Predicting: 76it [00:46,  1.65it/s]Extractor Predicting: 77it [00:46,  1.67it/s]Extractor Predicting: 78it [00:47,  1.66it/s]Extractor Predicting: 79it [00:48,  1.70it/s]Extractor Predicting: 80it [00:48,  1.73it/s]Extractor Predicting: 81it [00:49,  1.69it/s]Extractor Predicting: 82it [00:49,  1.71it/s]Extractor Predicting: 83it [00:50,  1.66it/s]Extractor Predicting: 84it [00:51,  1.64it/s]Extractor Predicting: 85it [00:51,  1.59it/s]Extractor Predicting: 86it [00:52,  1.58it/s]Extractor Predicting: 87it [00:53,  1.58it/s]Extractor Predicting: 88it [00:53,  1.61it/s]Extractor Predicting: 89it [00:54,  1.59it/s]Extractor Predicting: 90it [00:54,  1.60it/s]Extractor Predicting: 91it [00:55,  1.59it/s]Extractor Predicting: 92it [00:56,  1.58it/s]Extractor Predicting: 93it [00:56,  1.61it/s]Extractor Predicting: 94it [00:57,  1.62it/s]Extractor Predicting: 95it [00:58,  1.61it/s]Extractor Predicting: 96it [00:58,  1.60it/s]Extractor Predicting: 97it [00:59,  1.60it/s]Extractor Predicting: 98it [00:59,  1.60it/s]Extractor Predicting: 99it [01:00,  1.60it/s]Extractor Predicting: 100it [01:01,  1.59it/s]Extractor Predicting: 101it [01:01,  1.61it/s]Extractor Predicting: 102it [01:02,  1.60it/s]Extractor Predicting: 103it [01:03,  1.57it/s]Extractor Predicting: 104it [01:03,  1.61it/s]Extractor Predicting: 105it [01:04,  1.63it/s]Extractor Predicting: 106it [01:04,  1.65it/s]Extractor Predicting: 107it [01:05,  1.62it/s]Extractor Predicting: 108it [01:06,  1.65it/s]Extractor Predicting: 109it [01:06,  1.63it/s]Extractor Predicting: 110it [01:07,  1.65it/s]Extractor Predicting: 111it [01:07,  1.65it/s]Extractor Predicting: 112it [01:08,  1.61it/s]Extractor Predicting: 113it [01:09,  1.61it/s]Extractor Predicting: 114it [01:09,  1.61it/s]Extractor Predicting: 115it [01:10,  1.61it/s]Extractor Predicting: 116it [01:11,  1.61it/s]Extractor Predicting: 117it [01:11,  1.64it/s]Extractor Predicting: 118it [01:12,  1.62it/s]Extractor Predicting: 119it [01:12,  1.61it/s]Extractor Predicting: 120it [01:13,  1.64it/s]Extractor Predicting: 121it [01:14,  1.67it/s]Extractor Predicting: 122it [01:14,  1.66it/s]Extractor Predicting: 123it [01:15,  1.63it/s]Extractor Predicting: 124it [01:15,  1.60it/s]Extractor Predicting: 125it [01:16,  1.61it/s]Extractor Predicting: 126it [01:17,  1.60it/s]Extractor Predicting: 127it [01:17,  1.63it/s]Extractor Predicting: 128it [01:18,  1.59it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:19,  1.63it/s]Extractor Predicting: 131it [01:20,  1.61it/s]Extractor Predicting: 132it [01:21,  1.40it/s]Extractor Predicting: 133it [01:21,  1.46it/s]Extractor Predicting: 134it [01:22,  1.49it/s]Extractor Predicting: 135it [01:23,  1.53it/s]Extractor Predicting: 136it [01:23,  1.59it/s]Extractor Predicting: 137it [01:24,  1.55it/s]Extractor Predicting: 138it [01:24,  1.58it/s]Extractor Predicting: 139it [01:25,  1.60it/s]Extractor Predicting: 140it [01:26,  1.61it/s]Extractor Predicting: 141it [01:26,  1.59it/s]Extractor Predicting: 142it [01:27,  1.62it/s]Extractor Predicting: 143it [01:28,  1.52it/s]Extractor Predicting: 144it [01:28,  1.59it/s]Extractor Predicting: 145it [01:29,  1.56it/s]Extractor Predicting: 146it [01:30,  1.59it/s]Extractor Predicting: 147it [01:30,  1.63it/s]Extractor Predicting: 148it [01:31,  1.62it/s]Extractor Predicting: 149it [01:31,  1.66it/s]Extractor Predicting: 150it [01:32,  1.67it/s]Extractor Predicting: 151it [01:32,  1.69it/s]Extractor Predicting: 152it [01:33,  1.65it/s]Extractor Predicting: 153it [01:34,  1.66it/s]Extractor Predicting: 154it [01:34,  1.61it/s]Extractor Predicting: 155it [01:35,  1.62it/s]Extractor Predicting: 156it [01:36,  1.66it/s]Extractor Predicting: 157it [01:36,  1.62it/s]Extractor Predicting: 158it [01:37,  1.61it/s]Extractor Predicting: 159it [01:37,  1.62it/s]Extractor Predicting: 160it [01:38,  1.63it/s]Extractor Predicting: 161it [01:39,  1.66it/s]Extractor Predicting: 162it [01:39,  1.63it/s]Extractor Predicting: 163it [01:40,  1.63it/s]Extractor Predicting: 164it [01:40,  1.62it/s]Extractor Predicting: 165it [01:41,  1.58it/s]Extractor Predicting: 166it [01:42,  1.55it/s]Extractor Predicting: 167it [01:42,  1.55it/s]Extractor Predicting: 168it [01:43,  1.57it/s]Extractor Predicting: 169it [01:44,  1.61it/s]Extractor Predicting: 170it [01:44,  1.63it/s]Extractor Predicting: 171it [01:45,  1.64it/s]Extractor Predicting: 172it [01:45,  1.66it/s]Extractor Predicting: 173it [01:46,  1.65it/s]Extractor Predicting: 174it [01:47,  1.64it/s]Extractor Predicting: 175it [01:47,  1.64it/s]Extractor Predicting: 176it [01:48,  1.65it/s]Extractor Predicting: 177it [01:49,  1.62it/s]Extractor Predicting: 178it [01:49,  1.62it/s]Extractor Predicting: 179it [01:50,  1.59it/s]Extractor Predicting: 180it [01:50,  1.63it/s]Extractor Predicting: 181it [01:51,  1.63it/s]Extractor Predicting: 182it [01:52,  1.64it/s]Extractor Predicting: 183it [01:52,  1.67it/s]Extractor Predicting: 184it [01:53,  1.65it/s]Extractor Predicting: 185it [01:53,  1.66it/s]Extractor Predicting: 186it [01:54,  1.61it/s]Extractor Predicting: 187it [01:55,  1.63it/s]Extractor Predicting: 188it [01:55,  1.63it/s]Extractor Predicting: 189it [01:56,  1.63it/s]Extractor Predicting: 190it [01:56,  1.65it/s]Extractor Predicting: 191it [01:57,  1.64it/s]Extractor Predicting: 192it [01:58,  1.69it/s]Extractor Predicting: 193it [01:58,  1.67it/s]Extractor Predicting: 194it [01:59,  1.67it/s]Extractor Predicting: 195it [01:59,  1.65it/s]Extractor Predicting: 196it [02:00,  1.64it/s]Extractor Predicting: 197it [02:01,  1.64it/s]Extractor Predicting: 198it [02:01,  1.62it/s]Extractor Predicting: 199it [02:02,  1.62it/s]Extractor Predicting: 200it [02:03,  1.62it/s]Extractor Predicting: 201it [02:03,  1.63it/s]Extractor Predicting: 202it [02:04,  1.64it/s]Extractor Predicting: 203it [02:04,  1.66it/s]Extractor Predicting: 204it [02:05,  1.66it/s]Extractor Predicting: 205it [02:06,  1.66it/s]Extractor Predicting: 206it [02:06,  1.65it/s]Extractor Predicting: 207it [02:07,  1.66it/s]Extractor Predicting: 208it [02:07,  1.64it/s]Extractor Predicting: 209it [02:08,  1.61it/s]Extractor Predicting: 210it [02:09,  1.68it/s]Extractor Predicting: 211it [02:09,  1.66it/s]Extractor Predicting: 212it [02:10,  1.64it/s]Extractor Predicting: 213it [02:10,  1.65it/s]Extractor Predicting: 214it [02:11,  1.68it/s]Extractor Predicting: 215it [02:12,  1.67it/s]Extractor Predicting: 216it [02:12,  1.64it/s]Extractor Predicting: 217it [02:13,  1.65it/s]Extractor Predicting: 218it [02:13,  1.63it/s]Extractor Predicting: 219it [02:14,  1.64it/s]Extractor Predicting: 220it [02:15,  1.65it/s]Extractor Predicting: 221it [02:15,  1.66it/s]Extractor Predicting: 222it [02:16,  1.60it/s]Extractor Predicting: 223it [02:17,  1.55it/s]Extractor Predicting: 224it [02:17,  1.57it/s]Extractor Predicting: 225it [02:18,  1.60it/s]Extractor Predicting: 226it [02:18,  1.64it/s]Extractor Predicting: 227it [02:19,  1.63it/s]Extractor Predicting: 228it [02:20,  1.66it/s]Extractor Predicting: 229it [02:20,  1.69it/s]Extractor Predicting: 230it [02:21,  1.71it/s]Extractor Predicting: 231it [02:21,  1.70it/s]Extractor Predicting: 232it [02:22,  1.69it/s]Extractor Predicting: 233it [02:23,  1.68it/s]Extractor Predicting: 234it [02:23,  1.68it/s]Extractor Predicting: 235it [02:24,  1.65it/s]Extractor Predicting: 236it [02:24,  1.66it/s]Extractor Predicting: 237it [02:25,  1.66it/s]Extractor Predicting: 238it [02:26,  1.62it/s]Extractor Predicting: 239it [02:26,  1.65it/s]Extractor Predicting: 240it [02:27,  1.64it/s]Extractor Predicting: 241it [02:27,  1.65it/s]Extractor Predicting: 242it [02:28,  1.63it/s]Extractor Predicting: 243it [02:29,  1.59it/s]Extractor Predicting: 244it [02:29,  1.60it/s]Extractor Predicting: 245it [02:30,  1.63it/s]Extractor Predicting: 246it [02:31,  1.63it/s]Extractor Predicting: 247it [02:31,  1.66it/s]Extractor Predicting: 248it [02:32,  1.41it/s]Extractor Predicting: 249it [02:33,  1.45it/s]Extractor Predicting: 250it [02:33,  1.51it/s]Extractor Predicting: 251it [02:34,  1.51it/s]Extractor Predicting: 252it [02:35,  1.53it/s]Extractor Predicting: 253it [02:35,  1.51it/s]Extractor Predicting: 254it [02:36,  1.50it/s]Extractor Predicting: 255it [02:37,  1.53it/s]Extractor Predicting: 256it [02:37,  1.55it/s]Extractor Predicting: 257it [02:38,  1.57it/s]Extractor Predicting: 258it [02:38,  1.57it/s]Extractor Predicting: 259it [02:39,  1.57it/s]Extractor Predicting: 260it [02:40,  1.55it/s]Extractor Predicting: 261it [02:40,  1.56it/s]Extractor Predicting: 262it [02:41,  1.55it/s]Extractor Predicting: 263it [02:42,  1.56it/s]Extractor Predicting: 264it [02:42,  1.57it/s]Extractor Predicting: 265it [02:43,  1.59it/s]Extractor Predicting: 266it [02:44,  1.54it/s]Extractor Predicting: 267it [02:44,  1.55it/s]Extractor Predicting: 268it [02:45,  1.54it/s]Extractor Predicting: 269it [02:46,  1.56it/s]Extractor Predicting: 270it [02:46,  1.54it/s]Extractor Predicting: 271it [02:47,  1.54it/s]Extractor Predicting: 272it [02:48,  1.55it/s]Extractor Predicting: 273it [02:48,  1.56it/s]Extractor Predicting: 274it [02:49,  1.52it/s]Extractor Predicting: 275it [02:49,  1.53it/s]Extractor Predicting: 276it [02:50,  1.54it/s]Extractor Predicting: 277it [02:51,  1.56it/s]Extractor Predicting: 278it [02:51,  1.58it/s]Extractor Predicting: 279it [02:52,  1.56it/s]Extractor Predicting: 280it [02:53,  1.57it/s]Extractor Predicting: 281it [02:53,  1.53it/s]Extractor Predicting: 282it [02:54,  1.53it/s]Extractor Predicting: 283it [02:55,  1.55it/s]Extractor Predicting: 284it [02:55,  1.57it/s]Extractor Predicting: 285it [02:56,  1.56it/s]Extractor Predicting: 286it [02:56,  1.59it/s]Extractor Predicting: 287it [02:57,  1.54it/s]Extractor Predicting: 288it [02:58,  1.57it/s]Extractor Predicting: 289it [02:58,  1.58it/s]Extractor Predicting: 290it [02:59,  1.55it/s]Extractor Predicting: 291it [03:00,  1.52it/s]Extractor Predicting: 292it [03:00,  1.55it/s]Extractor Predicting: 293it [03:01,  1.56it/s]Extractor Predicting: 294it [03:02,  1.53it/s]Extractor Predicting: 295it [03:02,  1.54it/s]Extractor Predicting: 296it [03:03,  1.58it/s]Extractor Predicting: 297it [03:04,  1.58it/s]Extractor Predicting: 298it [03:04,  1.56it/s]Extractor Predicting: 299it [03:05,  1.56it/s]Extractor Predicting: 300it [03:06,  1.55it/s]Extractor Predicting: 301it [03:06,  1.57it/s]Extractor Predicting: 302it [03:07,  1.54it/s]Extractor Predicting: 303it [03:07,  1.58it/s]Extractor Predicting: 304it [03:08,  1.59it/s]Extractor Predicting: 305it [03:09,  1.62it/s]Extractor Predicting: 306it [03:09,  1.65it/s]Extractor Predicting: 307it [03:10,  1.62it/s]Extractor Predicting: 308it [03:10,  1.63it/s]Extractor Predicting: 309it [03:11,  1.59it/s]Extractor Predicting: 310it [03:12,  1.59it/s]Extractor Predicting: 311it [03:12,  1.56it/s]Extractor Predicting: 312it [03:13,  1.58it/s]Extractor Predicting: 313it [03:14,  1.52it/s]Extractor Predicting: 314it [03:14,  1.52it/s]Extractor Predicting: 315it [03:15,  1.56it/s]Extractor Predicting: 316it [03:16,  1.59it/s]Extractor Predicting: 317it [03:16,  1.61it/s]Extractor Predicting: 318it [03:17,  1.64it/s]Extractor Predicting: 319it [03:17,  1.60it/s]Extractor Predicting: 320it [03:18,  1.57it/s]Extractor Predicting: 321it [03:19,  1.57it/s]Extractor Predicting: 322it [03:19,  1.59it/s]Extractor Predicting: 323it [03:20,  1.54it/s]Extractor Predicting: 324it [03:21,  1.55it/s]Extractor Predicting: 325it [03:21,  1.58it/s]Extractor Predicting: 326it [03:22,  1.58it/s]Extractor Predicting: 327it [03:23,  1.59it/s]Extractor Predicting: 328it [03:23,  1.56it/s]Extractor Predicting: 329it [03:24,  1.57it/s]Extractor Predicting: 330it [03:25,  1.55it/s]Extractor Predicting: 331it [03:25,  1.57it/s]Extractor Predicting: 332it [03:26,  1.56it/s]Extractor Predicting: 333it [03:26,  1.55it/s]Extractor Predicting: 334it [03:27,  1.57it/s]Extractor Predicting: 335it [03:28,  1.56it/s]Extractor Predicting: 336it [03:28,  1.49it/s]Extractor Predicting: 337it [03:29,  1.48it/s]Extractor Predicting: 338it [03:30,  1.50it/s]Extractor Predicting: 339it [03:30,  1.52it/s]Extractor Predicting: 340it [03:31,  1.52it/s]Extractor Predicting: 341it [03:32,  1.51it/s]Extractor Predicting: 342it [03:32,  1.52it/s]Extractor Predicting: 343it [03:33,  1.55it/s]Extractor Predicting: 344it [03:34,  1.55it/s]Extractor Predicting: 345it [03:34,  1.51it/s]Extractor Predicting: 346it [03:35,  1.52it/s]Extractor Predicting: 347it [03:36,  1.50it/s]Extractor Predicting: 348it [03:36,  1.53it/s]Extractor Predicting: 349it [03:37,  1.54it/s]Extractor Predicting: 350it [03:38,  1.54it/s]Extractor Predicting: 351it [03:38,  1.57it/s]Extractor Predicting: 352it [03:39,  1.35it/s]Extractor Predicting: 353it [03:40,  1.38it/s]Extractor Predicting: 354it [03:41,  1.44it/s]Extractor Predicting: 355it [03:41,  1.49it/s]Extractor Predicting: 356it [03:42,  1.51it/s]Extractor Predicting: 357it [03:42,  1.52it/s]Extractor Predicting: 358it [03:43,  1.54it/s]Extractor Predicting: 359it [03:44,  1.53it/s]Extractor Predicting: 360it [03:44,  1.51it/s]Extractor Predicting: 361it [03:45,  1.51it/s]Extractor Predicting: 362it [03:46,  1.53it/s]Extractor Predicting: 363it [03:46,  1.57it/s]Extractor Predicting: 364it [03:47,  1.58it/s]Extractor Predicting: 365it [03:48,  1.56it/s]Extractor Predicting: 366it [03:48,  1.52it/s]Extractor Predicting: 367it [03:49,  1.51it/s]Extractor Predicting: 368it [03:50,  1.50it/s]Extractor Predicting: 369it [03:50,  1.46it/s]Extractor Predicting: 370it [03:51,  1.47it/s]Extractor Predicting: 371it [03:52,  1.50it/s]Extractor Predicting: 372it [03:52,  1.76it/s]Extractor Predicting: 372it [03:52,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:24,710 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:24,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:24,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:24,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:24,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:29:25,055 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:29:25,084 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:29:25,355 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:29:26,423 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:29:26,423 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:28,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:28,295 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:28,295 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:28,295 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:29:28,295 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:29:28,644 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:29:28,645 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:29:29,090 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:29:29,291 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:29:29,291 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3111559139784946,
  "recall": 0.0519524236983842,
  "score": 0.08903846153846154,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:14,  1.55it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:16,  1.56it/s]Extractor Predicting: 28it [00:17,  1.55it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:18,  1.59it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:21,  1.57it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:23,  1.52it/s]Extractor Predicting: 39it [00:24,  1.49it/s]Extractor Predicting: 40it [00:25,  1.38it/s]Extractor Predicting: 41it [00:26,  1.41it/s]Extractor Predicting: 42it [00:26,  1.44it/s]Extractor Predicting: 43it [00:27,  1.46it/s]Extractor Predicting: 44it [00:27,  1.48it/s]Extractor Predicting: 45it [00:28,  1.46it/s]Extractor Predicting: 46it [00:29,  1.49it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:30,  1.48it/s]Extractor Predicting: 49it [00:31,  1.49it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:32,  1.45it/s]Extractor Predicting: 52it [00:33,  1.45it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:34,  1.45it/s]Extractor Predicting: 55it [00:35,  1.50it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:36,  1.49it/s]Extractor Predicting: 58it [00:37,  1.47it/s]Extractor Predicting: 59it [00:38,  1.47it/s]Extractor Predicting: 60it [00:38,  1.48it/s]Extractor Predicting: 61it [00:39,  1.43it/s]Extractor Predicting: 62it [00:40,  1.43it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 63it [00:40,  1.54it/s]
[INFO|configuration_utils.py:515] 2023-08-28 22:30:11,478 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:30:11,479 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:30:11,489 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:30:11,490 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 22:30:11,495 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:30:18,616 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 22:30:18,622 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 22:30:18,648 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:30:18,648 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:30:18,657 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:30:18,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:30:18,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:30:18,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:30:18,663 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:30:18,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:30:18,663 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.49065420560747663,
  "recall": 0.03146538807311957,
  "score": 0.05913827090960293,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 22:30:18,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:19,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:19,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:20,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:21,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:21,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:22,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:22,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:23,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:23,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:24,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:24,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:25,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:26,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:26,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:27,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:27,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:28,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:29,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:29,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:30,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:30,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<02:57, 12.65s/it][WARNING|generation_utils.py:914] 2023-08-28 22:30:31,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:32,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:32,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:33,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:34,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:35,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:36,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:36,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:37,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:37,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:38,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:38,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:39,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:40,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:40,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:41,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:41,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:42,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:42,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:43,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:44,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:44,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:52, 13.24s/it][WARNING|generation_utils.py:914] 2023-08-28 22:30:45,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:45,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:46,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:46,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:47,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:47,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:48,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:48,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:49,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:49,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:50,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:50,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:51,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:51,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:52,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:52,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:53,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:53,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:54,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:35<02:17, 11.50s/it][WARNING|generation_utils.py:914] 2023-08-28 22:30:54,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:55,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:55,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:56,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:56,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:57,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:58,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:58,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:59,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:30:59,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:00,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:01,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:01,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:02,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:03,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:03,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:04,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:04,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:05,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:06,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:06,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:48<02:11, 11.96s/it][WARNING|generation_utils.py:914] 2023-08-28 22:31:07,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:07,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:08,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:09,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:09,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:10,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:11,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:11,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:12,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:12,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:13,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:13,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:14,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:15,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:15,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:16,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:16,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:17,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:17,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:18,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:18,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:19,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:01<02:02, 12.25s/it][WARNING|generation_utils.py:914] 2023-08-28 22:31:20,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:21,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:22,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:22,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:23,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:24,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:24,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:25,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:25,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:26,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:27,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:27,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:28,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:28,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:29,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:29,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:30,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:31,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:31,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:32,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:13<01:51, 12.41s/it][WARNING|generation_utils.py:914] 2023-08-28 22:31:32,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:33,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:33,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:34,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:35,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:35,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:36,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:36,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:37,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:37,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:38,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:39,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:39,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:40,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:40,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:41,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:41,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:42,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:43,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:43,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:25<01:36, 12.08s/it][WARNING|generation_utils.py:914] 2023-08-28 22:31:44,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:44,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:45,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:46,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:46,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:47,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:47,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:48,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:48,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:49,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:49,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:50,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:51,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:51,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:52,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:52,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:53,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:53,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:54,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:55,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:55,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:37<01:24, 12.06s/it][WARNING|generation_utils.py:914] 2023-08-28 22:31:56,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:56,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:57,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:57,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:58,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:59,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:31:59,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:00,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:00,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:01,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:02,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:02,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:03,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:03,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:04,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:05,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:05,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:06,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:06,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:07,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:08,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:08,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:50<01:14, 12.35s/it][WARNING|generation_utils.py:914] 2023-08-28 22:32:09,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:09,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:10,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:11,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:11,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:12,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:12,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:13,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:13,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:14,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:15,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:15,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:16,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:16,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:17,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:18,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:18,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:19,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:19,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:20,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:01<01:00, 12.14s/it][WARNING|generation_utils.py:914] 2023-08-28 22:32:20,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:21,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:21,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:22,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:22,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:23,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:23,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:24,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:24,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:25,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:25,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:26,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:26,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:27,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:27,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:28,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:28,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:29,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:29,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:11<00:45, 11.25s/it][WARNING|generation_utils.py:914] 2023-08-28 22:32:30,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:30,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:31,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:31,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:32,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:32,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:33,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:34,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:34,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:35,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:35,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:36,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:36,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:37,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:38,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:38,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:39,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:39,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:40,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:40,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:22<00:33, 11.31s/it][WARNING|generation_utils.py:914] 2023-08-28 22:32:41,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:42,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:42,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:43,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:44,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:44,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:45,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:45,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:46,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:47,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:47,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:48,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:48,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:49,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:49,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:50,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:51,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:51,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:52,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:52,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:53,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:35<00:23, 11.71s/it][WARNING|generation_utils.py:914] 2023-08-28 22:32:54,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:54,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:55,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:55,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:56,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:56,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:57,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:57,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:58,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:58,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:59,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:32:59,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:00,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:00,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:00,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:01,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:01,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:02,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:02,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:44<00:10, 10.95s/it][WARNING|generation_utils.py:914] 2023-08-28 22:33:03,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:04,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:04,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:05,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:05,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:06,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:07,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:07,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:08,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:09,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:09,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:10,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:11,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:11,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:12,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:13,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:13,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:14,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:15,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:15,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:33:16,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [02:58<00:00, 11.78s/it]Generating: 100%|██████████| 15/15 [02:58<00:00, 11.88s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:23,293 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:23,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:23,300 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:23,300 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:23,300 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:33:23,626 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:33:23,626 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:33:23,892 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:33:24,985 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:33:24,985 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:27,291 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:27,298 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:27,299 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:27,299 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:33:27,299 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:33:27,649 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:33:27,649 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:33:27,939 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:33:28,124 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:33:28,125 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : conflict .', 'success_rate': 0.8678977272727273, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 318, 'raw': 320}
{'target': 600, 'success': 350, 'raw': 352}
{'target': 600, 'success': 382, 'raw': 384}
{'target': 600, 'success': 414, 'raw': 416}
{'target': 600, 'success': 446, 'raw': 448}
{'target': 600, 'success': 478, 'raw': 480}
{'target': 600, 'success': 510, 'raw': 512}
{'target': 600, 'success': 541, 'raw': 544}
{'target': 600, 'success': 572, 'raw': 576}
{'target': 600, 'success': 604, 'raw': 608}
{'prompt': 'Relation : parent astronomical body .', 'success_rate': 0.993421052631579, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9360119047619048, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : work location . Context : Later in 2008 , he studied at The New School , Oxford and at Harvard College at the suggestion of John Heilemann . Head Entity : John Heilemann , Tail Entity : Cambridge .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : work location .', 'success_rate': 0.8806818181818182, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 439, 'raw': 448}
{'target': 600, 'success': 471, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 535, 'raw': 544}
{'target': 600, 'success': 567, 'raw': 576}
{'target': 600, 'success': 599, 'raw': 608}
{'target': 600, 'success': 630, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.984375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : creator .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8678977272727273, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 287, 'raw': 288}
{'target': 600, 'success': 319, 'raw': 320}
{'target': 600, 'success': 351, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 415, 'raw': 416}
{'target': 600, 'success': 447, 'raw': 448}
{'target': 600, 'success': 479, 'raw': 480}
{'target': 600, 'success': 511, 'raw': 512}
{'target': 600, 'success': 543, 'raw': 544}
{'target': 600, 'success': 575, 'raw': 576}
{'target': 600, 'success': 607, 'raw': 608}
{'prompt': 'Relation : member of sports team .', 'success_rate': 0.9983552631578947, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : occupation .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 160, 'raw': 160}
{'target': 600, 'success': 192, 'raw': 192}
{'target': 600, 'success': 224, 'raw': 224}
{'target': 600, 'success': 256, 'raw': 256}
{'target': 600, 'success': 288, 'raw': 288}
{'target': 600, 'success': 320, 'raw': 320}
{'target': 600, 'success': 351, 'raw': 352}
{'target': 600, 'success': 383, 'raw': 384}
{'target': 600, 'success': 415, 'raw': 416}
{'target': 600, 'success': 447, 'raw': 448}
{'target': 600, 'success': 479, 'raw': 480}
{'target': 600, 'success': 511, 'raw': 512}
{'target': 600, 'success': 543, 'raw': 544}
{'target': 600, 'success': 575, 'raw': 576}
{'target': 600, 'success': 607, 'raw': 608}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.9983552631578947, 'errors': {''}}
['Relation : twinned administrative body . Context : Later in the year ( 1143 ) , he served as a chancellor in the Legislative Assembly of the Kingdom of Great Britain from 1164 to 1170 , serving from 1176 , to 1185 , and finally 1241 . Head Entity : State Assembly , Tail Entity : Kingdom of Great Britain .\n']
['Relation : twinned administrative body . Context : Later in the year ( 1143 ) , he served as a chancellor in the Legislative Assembly of the Kingdom of Great Britain from 1164 to 1170 , serving from 1176 , to 1185 , and finally 1241 . Head Entity : State Assembly , Tail Entity : Kingdom of Great Britain .\n', 'Relation : twinned administrative body . Context : Eta Giorgi is the administrative district ( c. Head Entity : Eta Giorgi , Tail Entity : Cenkale .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 7919
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8019, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.69it/s]Extractor Estimating: 2it [00:01,  1.63it/s]Extractor Estimating: 3it [00:01,  1.63it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.58it/s]Extractor Estimating: 6it [00:03,  1.65it/s]Extractor Estimating: 7it [00:04,  1.72it/s]Extractor Estimating: 8it [00:04,  1.76it/s]Extractor Estimating: 9it [00:05,  1.71it/s]Extractor Estimating: 10it [00:05,  1.74it/s]Extractor Estimating: 11it [00:06,  1.71it/s]Extractor Estimating: 12it [00:07,  1.69it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.70it/s]Extractor Estimating: 15it [00:08,  1.71it/s]Extractor Estimating: 16it [00:09,  1.67it/s]Extractor Estimating: 17it [00:10,  1.64it/s]Extractor Estimating: 18it [00:10,  1.66it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:11,  1.66it/s]Extractor Estimating: 21it [00:12,  1.65it/s]Extractor Estimating: 22it [00:13,  1.64it/s]Extractor Estimating: 23it [00:13,  1.71it/s]Extractor Estimating: 24it [00:14,  1.68it/s]Extractor Estimating: 25it [00:14,  1.68it/s]Extractor Estimating: 26it [00:15,  1.72it/s]Extractor Estimating: 27it [00:16,  1.72it/s]Extractor Estimating: 28it [00:16,  1.74it/s]Extractor Estimating: 29it [00:17,  1.65it/s]Extractor Estimating: 30it [00:17,  1.62it/s]Extractor Estimating: 31it [00:18,  1.57it/s]Extractor Estimating: 32it [00:19,  1.62it/s]Extractor Estimating: 33it [00:19,  1.70it/s]Extractor Estimating: 34it [00:20,  1.69it/s]Extractor Estimating: 35it [00:20,  1.70it/s]Extractor Estimating: 36it [00:21,  1.76it/s]Extractor Estimating: 37it [00:22,  1.70it/s]Extractor Estimating: 38it [00:22,  1.72it/s]Extractor Estimating: 39it [00:23,  1.72it/s]Extractor Estimating: 40it [00:23,  1.70it/s]Extractor Estimating: 41it [00:24,  1.75it/s]Extractor Estimating: 42it [00:24,  1.71it/s]Extractor Estimating: 43it [00:25,  1.70it/s]Extractor Estimating: 44it [00:26,  1.68it/s]Extractor Estimating: 45it [00:26,  1.70it/s]Extractor Estimating: 46it [00:27,  1.68it/s]Extractor Estimating: 47it [00:27,  1.74it/s]Extractor Estimating: 48it [00:28,  1.75it/s]Extractor Estimating: 49it [00:29,  1.70it/s]Extractor Estimating: 50it [00:29,  1.69it/s]Extractor Estimating: 51it [00:30,  1.85it/s]Extractor Estimating: 52it [00:30,  1.94it/s]Extractor Estimating: 53it [00:31,  2.00it/s]Extractor Estimating: 54it [00:31,  2.05it/s]Extractor Estimating: 55it [00:31,  2.08it/s]Extractor Estimating: 56it [00:32,  2.07it/s]Extractor Estimating: 57it [00:32,  2.08it/s]Extractor Estimating: 58it [00:33,  2.10it/s]Extractor Estimating: 59it [00:33,  2.18it/s]Extractor Estimating: 60it [00:34,  2.22it/s]Extractor Estimating: 61it [00:34,  2.23it/s]Extractor Estimating: 62it [00:35,  2.17it/s]Extractor Estimating: 63it [00:35,  2.24it/s]Extractor Estimating: 64it [00:36,  2.10it/s]Extractor Estimating: 65it [00:36,  2.14it/s]Extractor Estimating: 66it [00:37,  2.10it/s]Extractor Estimating: 67it [00:37,  2.15it/s]Extractor Estimating: 68it [00:37,  2.18it/s]Extractor Estimating: 69it [00:38,  2.14it/s]Extractor Estimating: 70it [00:38,  2.14it/s]Extractor Estimating: 71it [00:39,  2.16it/s]Extractor Estimating: 72it [00:39,  2.21it/s]Extractor Estimating: 73it [00:40,  2.26it/s]Extractor Estimating: 74it [00:40,  2.07it/s]Extractor Estimating: 75it [00:41,  1.96it/s]Extractor Estimating: 76it [00:41,  1.88it/s]Extractor Estimating: 77it [00:42,  1.88it/s]Extractor Estimating: 78it [00:42,  1.87it/s]Extractor Estimating: 79it [00:43,  1.75it/s]Extractor Estimating: 80it [00:44,  1.71it/s]Extractor Estimating: 81it [00:44,  1.70it/s]Extractor Estimating: 82it [00:45,  1.71it/s]Extractor Estimating: 83it [00:46,  1.68it/s]Extractor Estimating: 84it [00:46,  1.68it/s]Extractor Estimating: 85it [00:47,  1.58it/s]Extractor Estimating: 86it [00:48,  1.58it/s]Extractor Estimating: 87it [00:48,  1.64it/s]Extractor Estimating: 88it [00:49,  1.59it/s]Extractor Estimating: 89it [00:49,  1.64it/s]Extractor Estimating: 90it [00:50,  1.62it/s]Extractor Estimating: 91it [00:50,  1.66it/s]Extractor Estimating: 92it [00:51,  1.64it/s]Extractor Estimating: 93it [00:52,  1.59it/s]Extractor Estimating: 94it [00:52,  1.59it/s]Extractor Estimating: 95it [00:53,  1.60it/s]Extractor Estimating: 96it [00:54,  1.67it/s]Extractor Estimating: 97it [00:54,  1.51it/s]Extractor Estimating: 98it [00:55,  1.52it/s]Extractor Estimating: 99it [00:56,  1.58it/s]Extractor Estimating: 100it [00:56,  1.58it/s]Extractor Estimating: 101it [00:57,  1.61it/s]Extractor Estimating: 102it [00:57,  1.58it/s]Extractor Estimating: 103it [00:58,  1.65it/s]Extractor Estimating: 104it [00:59,  1.67it/s]Extractor Estimating: 105it [00:59,  1.66it/s]Extractor Estimating: 106it [01:00,  1.67it/s]Extractor Estimating: 107it [01:00,  1.66it/s]Extractor Estimating: 108it [01:01,  1.67it/s]Extractor Estimating: 109it [01:02,  1.68it/s]Extractor Estimating: 110it [01:02,  1.69it/s]Extractor Estimating: 111it [01:03,  1.72it/s]Extractor Estimating: 112it [01:03,  1.72it/s]Extractor Estimating: 113it [01:04,  1.71it/s]Extractor Estimating: 114it [01:05,  1.70it/s]Extractor Estimating: 115it [01:05,  1.66it/s]Extractor Estimating: 116it [01:06,  1.68it/s]Extractor Estimating: 117it [01:06,  1.72it/s]Extractor Estimating: 118it [01:07,  1.73it/s]Extractor Estimating: 119it [01:07,  1.78it/s]Extractor Estimating: 120it [01:08,  1.81it/s]Extractor Estimating: 121it [01:08,  1.78it/s]Extractor Estimating: 122it [01:09,  1.77it/s]Extractor Estimating: 123it [01:10,  1.76it/s]Extractor Estimating: 124it [01:10,  1.73it/s]Extractor Estimating: 125it [01:11,  1.68it/s]Extractor Estimating: 126it [01:11,  1.71it/s]Extractor Estimating: 127it [01:12,  1.69it/s]Extractor Estimating: 128it [01:13,  1.59it/s]Extractor Estimating: 129it [01:13,  1.64it/s]Extractor Estimating: 130it [01:14,  1.68it/s]Extractor Estimating: 131it [01:14,  1.76it/s]Extractor Estimating: 132it [01:15,  1.72it/s]Extractor Estimating: 133it [01:16,  1.73it/s]Extractor Estimating: 134it [01:16,  1.68it/s]Extractor Estimating: 135it [01:17,  1.65it/s]Extractor Estimating: 136it [01:17,  1.68it/s]Extractor Estimating: 137it [01:18,  1.68it/s]Extractor Estimating: 138it [01:19,  1.68it/s]Extractor Estimating: 139it [01:19,  1.69it/s]Extractor Estimating: 140it [01:20,  1.70it/s]Extractor Estimating: 141it [01:20,  1.65it/s]Extractor Estimating: 142it [01:21,  1.66it/s]Extractor Estimating: 143it [01:22,  1.65it/s]Extractor Estimating: 144it [01:22,  1.65it/s]Extractor Estimating: 145it [01:23,  1.65it/s]Extractor Estimating: 146it [01:24,  1.60it/s]Extractor Estimating: 147it [01:24,  1.65it/s]Extractor Estimating: 148it [01:25,  1.64it/s]Extractor Estimating: 149it [01:25,  1.67it/s]Extractor Estimating: 150it [01:26,  1.68it/s]Extractor Estimating: 151it [01:26,  1.66it/s]Extractor Estimating: 152it [01:27,  1.68it/s]Extractor Estimating: 153it [01:28,  1.66it/s]Extractor Estimating: 154it [01:28,  1.64it/s]Extractor Estimating: 155it [01:29,  1.65it/s]Extractor Estimating: 156it [01:30,  1.61it/s]Extractor Estimating: 157it [01:30,  1.64it/s]Extractor Estimating: 158it [01:31,  1.66it/s]Extractor Estimating: 159it [01:31,  1.62it/s]Extractor Estimating: 160it [01:32,  1.60it/s]Extractor Estimating: 161it [01:33,  1.60it/s]Extractor Estimating: 162it [01:33,  1.59it/s]Extractor Estimating: 163it [01:34,  1.52it/s]Extractor Estimating: 164it [01:35,  1.53it/s]Extractor Estimating: 165it [01:35,  1.58it/s]Extractor Estimating: 166it [01:36,  1.62it/s]Extractor Estimating: 167it [01:36,  1.63it/s]Extractor Estimating: 168it [01:37,  1.61it/s]Extractor Estimating: 169it [01:38,  1.61it/s]Extractor Estimating: 170it [01:38,  1.62it/s]Extractor Estimating: 171it [01:39,  1.63it/s]Extractor Estimating: 172it [01:40,  1.45it/s]Extractor Estimating: 173it [01:40,  1.50it/s]Extractor Estimating: 174it [01:41,  1.54it/s]Extractor Estimating: 175it [01:42,  1.60it/s]Extractor Estimating: 176it [01:42,  1.60it/s]Extractor Estimating: 177it [01:43,  1.59it/s]Extractor Estimating: 178it [01:43,  1.56it/s]Extractor Estimating: 179it [01:44,  1.58it/s]Extractor Estimating: 180it [01:45,  1.61it/s]Extractor Estimating: 181it [01:45,  1.61it/s]Extractor Estimating: 182it [01:46,  1.63it/s]Extractor Estimating: 183it [01:46,  1.66it/s]Extractor Estimating: 184it [01:47,  1.68it/s]Extractor Estimating: 185it [01:48,  1.69it/s]Extractor Estimating: 186it [01:48,  1.65it/s]Extractor Estimating: 187it [01:49,  1.63it/s]Extractor Estimating: 188it [01:50,  1.57it/s]Extractor Estimating: 189it [01:50,  1.57it/s]Extractor Estimating: 190it [01:51,  1.61it/s]Extractor Estimating: 191it [01:51,  1.64it/s]Extractor Estimating: 192it [01:52,  1.64it/s]Extractor Estimating: 193it [01:53,  1.58it/s]Extractor Estimating: 194it [01:53,  1.63it/s]Extractor Estimating: 195it [01:54,  1.61it/s]Extractor Estimating: 196it [01:54,  1.66it/s]Extractor Estimating: 197it [01:55,  1.67it/s]Extractor Estimating: 198it [01:56,  1.65it/s]Extractor Estimating: 199it [01:56,  1.64it/s]Extractor Estimating: 200it [01:57,  1.63it/s]Extractor Estimating: 201it [01:58,  1.55it/s]Extractor Estimating: 202it [01:58,  1.59it/s]Extractor Estimating: 203it [01:59,  1.60it/s]Extractor Estimating: 204it [02:00,  1.58it/s]Extractor Estimating: 205it [02:00,  1.58it/s]Extractor Estimating: 206it [02:01,  1.59it/s]Extractor Estimating: 207it [02:01,  1.61it/s]Extractor Estimating: 208it [02:02,  1.57it/s]Extractor Estimating: 209it [02:03,  1.56it/s]Extractor Estimating: 210it [02:03,  1.56it/s]Extractor Estimating: 211it [02:04,  1.57it/s]Extractor Estimating: 212it [02:05,  1.61it/s]Extractor Estimating: 213it [02:05,  1.59it/s]Extractor Estimating: 214it [02:06,  1.57it/s]Extractor Estimating: 215it [02:06,  1.59it/s]Extractor Estimating: 216it [02:07,  1.57it/s]Extractor Estimating: 217it [02:08,  1.58it/s]Extractor Estimating: 218it [02:08,  1.55it/s]Extractor Estimating: 219it [02:09,  1.61it/s]Extractor Estimating: 220it [02:10,  1.59it/s]Extractor Estimating: 221it [02:10,  1.63it/s]Extractor Estimating: 222it [02:11,  1.61it/s]Extractor Estimating: 223it [02:11,  1.60it/s]Extractor Estimating: 224it [02:12,  1.66it/s]Extractor Estimating: 225it [02:13,  1.56it/s]Extractor Estimating: 226it [02:13,  1.56it/s]Extractor Estimating: 227it [02:14,  1.55it/s]Extractor Estimating: 228it [02:15,  1.58it/s]Extractor Estimating: 229it [02:15,  1.57it/s]Extractor Estimating: 230it [02:16,  1.60it/s]Extractor Estimating: 231it [02:16,  1.62it/s]Extractor Estimating: 232it [02:17,  1.59it/s]Extractor Estimating: 233it [02:18,  1.60it/s]Extractor Estimating: 234it [02:18,  1.58it/s]Extractor Estimating: 235it [02:19,  1.53it/s]Extractor Estimating: 236it [02:20,  1.53it/s]Extractor Estimating: 237it [02:20,  1.55it/s]Extractor Estimating: 238it [02:21,  1.56it/s]Extractor Estimating: 239it [02:22,  1.59it/s]Extractor Estimating: 240it [02:22,  1.63it/s]Extractor Estimating: 241it [02:23,  1.63it/s]Extractor Estimating: 242it [02:23,  1.59it/s]Extractor Estimating: 243it [02:24,  1.58it/s]Extractor Estimating: 244it [02:25,  1.59it/s]Extractor Estimating: 245it [02:25,  1.62it/s]Extractor Estimating: 246it [02:26,  1.63it/s]Extractor Estimating: 247it [02:27,  1.61it/s]Extractor Estimating: 248it [02:27,  1.60it/s]Extractor Estimating: 249it [02:28,  1.46it/s]Extractor Estimating: 250it [02:29,  1.47it/s]Extractor Estimating: 251it [02:29,  1.50it/s]Extractor Estimating: 252it [02:30,  1.53it/s]Extractor Estimating: 253it [02:31,  1.56it/s]Extractor Estimating: 254it [02:31,  1.56it/s]Extractor Estimating: 255it [02:32,  1.53it/s]Extractor Estimating: 256it [02:33,  1.55it/s]Extractor Estimating: 257it [02:33,  1.57it/s]Extractor Estimating: 258it [02:34,  1.60it/s]Extractor Estimating: 259it [02:34,  1.60it/s]Extractor Estimating: 260it [02:35,  1.60it/s]Extractor Estimating: 261it [02:36,  1.59it/s]Extractor Estimating: 262it [02:36,  1.60it/s]Extractor Estimating: 263it [02:37,  1.57it/s]Extractor Estimating: 264it [02:38,  1.57it/s]Extractor Estimating: 265it [02:38,  1.58it/s]Extractor Estimating: 266it [02:39,  1.57it/s]Extractor Estimating: 267it [02:39,  1.57it/s]Extractor Estimating: 268it [02:40,  1.57it/s]Extractor Estimating: 269it [02:41,  1.57it/s]Extractor Estimating: 270it [02:41,  1.53it/s]Extractor Estimating: 271it [02:42,  1.54it/s]Extractor Estimating: 272it [02:43,  1.56it/s]Extractor Estimating: 273it [02:43,  1.57it/s]Extractor Estimating: 274it [02:44,  1.58it/s]Extractor Estimating: 275it [02:45,  1.59it/s]Extractor Estimating: 276it [02:45,  1.57it/s]Extractor Estimating: 277it [02:46,  1.58it/s]Extractor Estimating: 278it [02:46,  1.58it/s]Extractor Estimating: 279it [02:47,  1.62it/s]Extractor Estimating: 280it [02:48,  1.60it/s]Extractor Estimating: 281it [02:49,  1.46it/s]Extractor Estimating: 282it [02:49,  1.48it/s]Extractor Estimating: 283it [02:50,  1.49it/s]Extractor Estimating: 284it [02:50,  1.55it/s]Extractor Estimating: 285it [02:51,  1.57it/s]Extractor Estimating: 286it [02:52,  1.54it/s]Extractor Estimating: 287it [02:52,  1.54it/s]Extractor Estimating: 288it [02:53,  1.53it/s]Extractor Estimating: 289it [02:54,  1.57it/s]Extractor Estimating: 290it [02:54,  1.58it/s]Extractor Estimating: 291it [02:55,  1.55it/s]Extractor Estimating: 292it [02:56,  1.59it/s]Extractor Estimating: 293it [02:56,  1.60it/s]Extractor Estimating: 294it [02:57,  1.61it/s]Extractor Estimating: 295it [02:57,  1.61it/s]Extractor Estimating: 296it [02:58,  1.54it/s]Extractor Estimating: 297it [02:59,  1.54it/s]Extractor Estimating: 298it [02:59,  1.55it/s]Extractor Estimating: 299it [03:00,  1.63it/s]Extractor Estimating: 300it [03:01,  1.63it/s]Extractor Estimating: 301it [03:01,  1.64it/s]Extractor Estimating: 302it [03:02,  1.65it/s]Extractor Estimating: 303it [03:02,  1.68it/s]Extractor Estimating: 304it [03:03,  1.70it/s]Extractor Estimating: 305it [03:03,  1.69it/s]Extractor Estimating: 306it [03:04,  1.72it/s]Extractor Estimating: 307it [03:05,  1.72it/s]Extractor Estimating: 308it [03:05,  1.73it/s]Extractor Estimating: 309it [03:06,  1.72it/s]Extractor Estimating: 310it [03:06,  1.70it/s]Extractor Estimating: 311it [03:07,  1.68it/s]Extractor Estimating: 312it [03:08,  1.71it/s]Extractor Estimating: 313it [03:08,  1.71it/s]Extractor Estimating: 314it [03:09,  1.71it/s]Extractor Estimating: 315it [03:09,  1.71it/s]Extractor Estimating: 316it [03:10,  1.72it/s]Extractor Estimating: 317it [03:10,  1.70it/s]Extractor Estimating: 318it [03:11,  1.69it/s]Extractor Estimating: 319it [03:12,  1.76it/s]Extractor Estimating: 320it [03:12,  1.77it/s]Extractor Estimating: 321it [03:13,  1.80it/s]Extractor Estimating: 322it [03:13,  1.80it/s]Extractor Estimating: 323it [03:14,  1.77it/s]Extractor Estimating: 324it [03:14,  1.73it/s]Extractor Estimating: 325it [03:15,  1.68it/s]Extractor Estimating: 326it [03:16,  1.64it/s]Extractor Estimating: 327it [03:16,  1.61it/s]Extractor Estimating: 328it [03:17,  1.60it/s]Extractor Estimating: 329it [03:18,  1.60it/s]Extractor Estimating: 330it [03:18,  1.60it/s]Extractor Estimating: 331it [03:19,  1.58it/s]Extractor Estimating: 332it [03:20,  1.43it/s]Extractor Estimating: 333it [03:20,  1.46it/s]Extractor Estimating: 334it [03:21,  1.49it/s]Extractor Estimating: 335it [03:22,  1.52it/s]Extractor Estimating: 336it [03:22,  1.53it/s]Extractor Estimating: 337it [03:23,  1.53it/s]Extractor Estimating: 338it [03:24,  1.53it/s]Extractor Estimating: 339it [03:24,  1.54it/s]Extractor Estimating: 340it [03:25,  1.56it/s]Extractor Estimating: 341it [03:25,  1.56it/s]Extractor Estimating: 342it [03:26,  1.55it/s]Extractor Estimating: 343it [03:27,  1.55it/s]Extractor Estimating: 344it [03:27,  1.56it/s]Extractor Estimating: 345it [03:28,  1.58it/s]Extractor Estimating: 346it [03:29,  1.57it/s]Extractor Estimating: 347it [03:29,  1.60it/s]Extractor Estimating: 348it [03:30,  1.64it/s]Extractor Estimating: 349it [03:30,  1.66it/s]Extractor Estimating: 350it [03:31,  1.69it/s]Extractor Estimating: 351it [03:32,  1.74it/s]Extractor Estimating: 352it [03:32,  1.77it/s]Extractor Estimating: 353it [03:33,  1.78it/s]Extractor Estimating: 354it [03:33,  1.75it/s]Extractor Estimating: 355it [03:34,  1.73it/s]Extractor Estimating: 356it [03:34,  1.76it/s]Extractor Estimating: 357it [03:35,  1.75it/s]Extractor Estimating: 358it [03:35,  1.78it/s]Extractor Estimating: 359it [03:36,  1.75it/s]Extractor Estimating: 360it [03:37,  1.75it/s]Extractor Estimating: 361it [03:37,  1.74it/s]Extractor Estimating: 362it [03:38,  1.77it/s]Extractor Estimating: 363it [03:38,  1.78it/s]Extractor Estimating: 364it [03:39,  1.73it/s]Extractor Estimating: 365it [03:40,  1.70it/s]Extractor Estimating: 366it [03:40,  1.70it/s]Extractor Estimating: 367it [03:41,  1.74it/s]Extractor Estimating: 368it [03:41,  1.73it/s]Extractor Estimating: 369it [03:42,  1.70it/s]Extractor Estimating: 370it [03:42,  1.74it/s]Extractor Estimating: 371it [03:43,  1.80it/s]Extractor Estimating: 372it [03:43,  1.84it/s]Extractor Estimating: 372it [03:43,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:28,919 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:28,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:28,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:28,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:28,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:37:29,252 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:37:29,252 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:37:29,514 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:37:30,587 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:37:30,587 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:33,397 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:33,401 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:33,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:33,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:37:33,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:37:33,744 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:37:33,745 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:37:34,417 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:37:34,600 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:37:34,600 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:53:17,098 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:53:17,123 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7415 mean pseudo reward: 0.946016856211222
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 16382
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16482, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16482, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.005, loss:468.9650
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.994, loss:455.2185
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.998, loss:451.8438
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 91, avg_time 1.002, loss:432.4902
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 191, avg_time 0.997, loss:432.8890
>> valid entity prec:0.5936, rec:0.5522, f1:0.5722
>> valid relation prec:0.2140, rec:0.0703, f1:0.1058
>> valid relation with NER prec:0.2140, rec:0.0703, f1:0.1058
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 291, avg_time 2.591, loss:423.8857
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 82, avg_time 1.000, loss:387.6472
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 182, avg_time 0.999, loss:432.4413
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 282, avg_time 0.995, loss:416.6605
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 73, avg_time 0.996, loss:394.5220
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5817, rec:0.5466, f1:0.5636
>> valid relation prec:0.2257, rec:0.0762, f1:0.1140
>> valid relation with NER prec:0.2257, rec:0.0762, f1:0.1140
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 173, avg_time 2.584, loss:412.1027
g_step 1200, step 273, avg_time 0.996, loss:399.8513
g_step 1300, step 64, avg_time 1.001, loss:384.1044
g_step 1400, step 164, avg_time 1.002, loss:394.5559
g_step 1500, step 264, avg_time 0.994, loss:396.7785
>> valid entity prec:0.5398, rec:0.5188, f1:0.5291
>> valid relation prec:0.2027, rec:0.0635, f1:0.0968
>> valid relation with NER prec:0.2027, rec:0.0635, f1:0.0968
g_step 1600, step 55, avg_time 2.597, loss:378.1196
g_step 1700, step 155, avg_time 1.002, loss:358.5030
g_step 1800, step 255, avg_time 0.999, loss:385.5218
g_step 1900, step 46, avg_time 1.001, loss:370.7859
g_step 2000, step 146, avg_time 1.005, loss:365.6053
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5452, rec:0.5756, f1:0.5600
>> valid relation prec:0.2048, rec:0.0986, f1:0.1331
>> valid relation with NER prec:0.2048, rec:0.0986, f1:0.1331
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 246, avg_time 2.626, loss:359.8478
g_step 2200, step 37, avg_time 1.004, loss:360.9605
g_step 2300, step 137, avg_time 1.008, loss:344.3406
g_step 2400, step 237, avg_time 1.003, loss:347.0259
g_step 2500, step 28, avg_time 1.023, loss:348.2599
>> valid entity prec:0.5738, rec:0.5471, f1:0.5601
>> valid relation prec:0.1817, rec:0.1019, f1:0.1305
>> valid relation with NER prec:0.1817, rec:0.1019, f1:0.1305
g_step 2600, step 128, avg_time 2.613, loss:321.5998
g_step 2700, step 228, avg_time 1.011, loss:350.3184
g_step 2800, step 19, avg_time 1.008, loss:328.9671
g_step 2900, step 119, avg_time 1.003, loss:307.2628
g_step 3000, step 219, avg_time 1.021, loss:340.0289
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5687, rec:0.4780, f1:0.5194
>> valid relation prec:0.2121, rec:0.0828, f1:0.1191
>> valid relation with NER prec:0.2121, rec:0.0828, f1:0.1191
g_step 3100, step 10, avg_time 2.593, loss:336.8822
g_step 3200, step 110, avg_time 1.010, loss:305.4628
g_step 3300, step 210, avg_time 1.005, loss:302.3687
g_step 3400, step 1, avg_time 1.016, loss:320.4482
g_step 3500, step 101, avg_time 1.022, loss:299.0322
>> valid entity prec:0.5724, rec:0.5181, f1:0.5439
>> valid relation prec:0.1884, rec:0.0672, f1:0.0991
>> valid relation with NER prec:0.1884, rec:0.0672, f1:0.0991
g_step 3600, step 201, avg_time 2.601, loss:283.1129
g_step 3700, step 301, avg_time 1.005, loss:317.3190
g_step 3800, step 92, avg_time 0.999, loss:288.1941
g_step 3900, step 192, avg_time 1.020, loss:301.9843
g_step 4000, step 292, avg_time 1.014, loss:302.0928
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5535, rec:0.5816, f1:0.5672
>> valid relation prec:0.1781, rec:0.0937, f1:0.1228
>> valid relation with NER prec:0.1781, rec:0.0937, f1:0.1228
g_step 4100, step 83, avg_time 2.605, loss:264.5433
g_step 4200, step 183, avg_time 1.008, loss:301.0130
g_step 4300, step 283, avg_time 1.014, loss:291.4673
g_step 4400, step 74, avg_time 1.025, loss:278.9277
g_step 4500, step 174, avg_time 1.002, loss:274.9095
>> valid entity prec:0.5465, rec:0.5125, f1:0.5290
>> valid relation prec:0.2009, rec:0.1041, f1:0.1372
>> valid relation with NER prec:0.2009, rec:0.1041, f1:0.1372
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 274, avg_time 2.610, loss:301.4218
g_step 4700, step 65, avg_time 0.998, loss:274.1832
g_step 4800, step 165, avg_time 1.021, loss:269.6723
g_step 4900, step 265, avg_time 1.018, loss:273.3121
g_step 5000, step 56, avg_time 1.014, loss:271.9005
learning rate was adjusted to 0.0008
>> valid entity prec:0.5610, rec:0.5301, f1:0.5451
>> valid relation prec:0.2324, rec:0.0814, f1:0.1205
>> valid relation with NER prec:0.2324, rec:0.0814, f1:0.1205
g_step 5100, step 156, avg_time 2.623, loss:273.9441
g_step 5200, step 256, avg_time 1.014, loss:260.4197
g_step 5300, step 47, avg_time 1.005, loss:252.2393
g_step 5400, step 147, avg_time 1.017, loss:251.3598
g_step 5500, step 247, avg_time 1.015, loss:259.7015
>> valid entity prec:0.5569, rec:0.5150, f1:0.5351
>> valid relation prec:0.2139, rec:0.0756, f1:0.1118
>> valid relation with NER prec:0.2139, rec:0.0756, f1:0.1118
g_step 5600, step 38, avg_time 2.617, loss:245.9483
g_step 5700, step 138, avg_time 0.997, loss:250.6912
g_step 5800, step 238, avg_time 1.002, loss:247.4943
g_step 5900, step 29, avg_time 1.017, loss:247.1851
g_step 6000, step 129, avg_time 1.003, loss:237.9244
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5797, rec:0.5218, f1:0.5492
>> valid relation prec:0.2000, rec:0.0953, f1:0.1291
>> valid relation with NER prec:0.2000, rec:0.0953, f1:0.1291
g_step 6100, step 229, avg_time 2.595, loss:248.6381
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:53:17 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:53:17 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-53-17_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:53:18 - WARNING - datasets.builder -   Using custom data configuration default-8b286ae9533c2042
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8b286ae9533c2042/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  5.84 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:53:18,640 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:53:18,641 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:53:18,642 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:53:18,643 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:53:18,654 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:53:18,658 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:53:18,658 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:53:18,658 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:53:18,658 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:53:18,658 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:53:18,658 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:53:18,842 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:53:21,924 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:53:21,927 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8b286ae9533c2042/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.07ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.93ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.33ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.55ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.68ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.77ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.81ba/s]100%|██████████| 8/8 [00:01<00:00,  5.72ba/s]100%|██████████| 8/8 [00:01<00:00,  4.85ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.27ba/s] 40%|████      | 2/5 [00:00<00:00,  4.49ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.59ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.63ba/s]100%|██████████| 5/5 [00:01<00:00,  4.85ba/s]100%|██████████| 5/5 [00:01<00:00,  4.70ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.36ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.40ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.54ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.76ba/s]100%|██████████| 8/8 [00:00<00:00, 11.32ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.58ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.65ba/s]100%|██████████| 5/5 [00:00<00:00, 10.92ba/s]100%|██████████| 5/5 [00:00<00:00, 10.78ba/s]
[INFO|trainer.py:414] 2023-08-29 00:53:26,200 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:53:26,215 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:53:26,215 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 00:53:26,216 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:53:26,216 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:53:26,216 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:53:26,216 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:53:26,216 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<03:05,  3.15it/s]  0%|          | 2/585 [00:00<02:55,  3.32it/s]  1%|          | 3/585 [00:00<02:52,  3.37it/s]  1%|          | 4/585 [00:01<02:51,  3.40it/s]  1%|          | 5/585 [00:01<02:49,  3.41it/s]  1%|          | 6/585 [00:01<02:49,  3.41it/s]  1%|          | 7/585 [00:02<02:49,  3.40it/s]  1%|▏         | 8/585 [00:02<02:50,  3.38it/s]  2%|▏         | 9/585 [00:02<02:50,  3.38it/s]  2%|▏         | 10/585 [00:02<02:49,  3.39it/s]  2%|▏         | 11/585 [00:03<02:49,  3.39it/s]  2%|▏         | 12/585 [00:03<02:49,  3.39it/s]  2%|▏         | 13/585 [00:03<02:48,  3.39it/s]  2%|▏         | 14/585 [00:04<02:48,  3.39it/s]  3%|▎         | 15/585 [00:04<02:47,  3.39it/s]  3%|▎         | 16/585 [00:04<02:47,  3.39it/s]  3%|▎         | 17/585 [00:05<02:47,  3.39it/s]  3%|▎         | 18/585 [00:05<02:47,  3.39it/s]  3%|▎         | 19/585 [00:05<02:47,  3.39it/s]  3%|▎         | 20/585 [00:05<02:46,  3.38it/s]  4%|▎         | 21/585 [00:06<02:46,  3.39it/s]  4%|▍         | 22/585 [00:06<02:46,  3.39it/s]  4%|▍         | 23/585 [00:06<02:45,  3.39it/s]  4%|▍         | 24/585 [00:07<02:45,  3.39it/s]  4%|▍         | 25/585 [00:07<02:45,  3.39it/s]  4%|▍         | 26/585 [00:07<02:44,  3.39it/s]  5%|▍         | 27/585 [00:07<02:44,  3.40it/s]  5%|▍         | 28/585 [00:08<02:44,  3.39it/s]  5%|▍         | 29/585 [00:08<02:43,  3.39it/s]  5%|▌         | 30/585 [00:08<02:43,  3.39it/s]  5%|▌         | 31/585 [00:09<02:43,  3.39it/s]  5%|▌         | 32/585 [00:09<02:43,  3.38it/s]  6%|▌         | 33/585 [00:09<02:43,  3.38it/s]  6%|▌         | 34/585 [00:10<02:42,  3.39it/s]  6%|▌         | 35/585 [00:10<02:42,  3.39it/s]  6%|▌         | 36/585 [00:10<02:42,  3.38it/s]  6%|▋         | 37/585 [00:10<02:41,  3.38it/s]  6%|▋         | 38/585 [00:11<02:41,  3.39it/s]  7%|▋         | 39/585 [00:11<02:41,  3.39it/s]  7%|▋         | 40/585 [00:11<02:40,  3.39it/s]  7%|▋         | 41/585 [00:12<02:40,  3.39it/s]  7%|▋         | 42/585 [00:12<02:40,  3.39it/s]  7%|▋         | 43/585 [00:12<02:39,  3.39it/s]  8%|▊         | 44/585 [00:12<02:39,  3.39it/s]  8%|▊         | 45/585 [00:13<02:39,  3.39it/s]  8%|▊         | 46/585 [00:13<02:39,  3.39it/s]  8%|▊         | 47/585 [00:13<02:38,  3.39it/s]  8%|▊         | 48/585 [00:14<02:38,  3.39it/s]  8%|▊         | 49/585 [00:14<02:38,  3.38it/s]  9%|▊         | 50/585 [00:14<02:38,  3.38it/s]  9%|▊         | 51/585 [00:15<02:38,  3.38it/s]  9%|▉         | 52/585 [00:15<02:37,  3.38it/s]  9%|▉         | 53/585 [00:15<02:37,  3.39it/s]  9%|▉         | 54/585 [00:15<02:36,  3.38it/s]  9%|▉         | 55/585 [00:16<02:36,  3.38it/s] 10%|▉         | 56/585 [00:16<02:37,  3.37it/s] 10%|▉         | 57/585 [00:16<02:36,  3.38it/s] 10%|▉         | 58/585 [00:17<02:35,  3.38it/s] 10%|█         | 59/585 [00:17<02:35,  3.38it/s] 10%|█         | 60/585 [00:17<02:35,  3.38it/s] 10%|█         | 61/585 [00:18<02:34,  3.38it/s] 11%|█         | 62/585 [00:18<02:34,  3.39it/s] 11%|█         | 63/585 [00:18<02:34,  3.38it/s] 11%|█         | 64/585 [00:18<02:34,  3.38it/s] 11%|█         | 65/585 [00:19<02:33,  3.38it/s] 11%|█▏        | 66/585 [00:19<02:33,  3.39it/s] 11%|█▏        | 67/585 [00:19<02:33,  3.37it/s] 12%|█▏        | 68/585 [00:20<02:33,  3.38it/s] 12%|█▏        | 69/585 [00:20<02:32,  3.38it/s] 12%|█▏        | 70/585 [00:20<02:32,  3.38it/s] 12%|█▏        | 71/585 [00:20<02:31,  3.38it/s] 12%|█▏        | 72/585 [00:21<02:31,  3.39it/s] 12%|█▏        | 73/585 [00:21<02:31,  3.39it/s] 13%|█▎        | 74/585 [00:21<02:31,  3.38it/s] 13%|█▎        | 75/585 [00:22<02:30,  3.38it/s] 13%|█▎        | 76/585 [00:22<02:30,  3.39it/s] 13%|█▎        | 77/585 [00:22<02:30,  3.39it/s] 13%|█▎        | 78/585 [00:23<02:29,  3.38it/s] 14%|█▎        | 79/585 [00:23<02:29,  3.38it/s] 14%|█▎        | 80/585 [00:23<02:29,  3.39it/s] 14%|█▍        | 81/585 [00:23<02:28,  3.39it/s] 14%|█▍        | 82/585 [00:24<02:28,  3.38it/s] 14%|█▍        | 83/585 [00:24<02:28,  3.38it/s] 14%|█▍        | 84/585 [00:24<02:29,  3.36it/s] 15%|█▍        | 85/585 [00:25<02:28,  3.37it/s] 15%|█▍        | 86/585 [00:25<02:28,  3.37it/s] 15%|█▍        | 87/585 [00:25<02:27,  3.37it/s] 15%|█▌        | 88/585 [00:26<02:27,  3.38it/s] 15%|█▌        | 89/585 [00:26<02:26,  3.38it/s] 15%|█▌        | 90/585 [00:26<02:26,  3.38it/s] 16%|█▌        | 91/585 [00:26<02:26,  3.38it/s] 16%|█▌        | 92/585 [00:27<02:25,  3.39it/s] 16%|█▌        | 93/585 [00:27<02:25,  3.38it/s] 16%|█▌        | 94/585 [00:27<02:25,  3.38it/s] 16%|█▌        | 95/585 [00:28<02:25,  3.36it/s] 16%|█▋        | 96/585 [00:28<02:25,  3.37it/s] 17%|█▋        | 97/585 [00:28<02:24,  3.37it/s] 17%|█▋        | 98/585 [00:28<02:24,  3.37it/s] 17%|█▋        | 99/585 [00:29<02:23,  3.38it/s] 17%|█▋        | 100/585 [00:29<02:23,  3.38it/s] 17%|█▋        | 101/585 [00:29<02:23,  3.38it/s] 17%|█▋        | 102/585 [00:30<02:22,  3.38it/s] 18%|█▊        | 103/585 [00:30<02:22,  3.38it/s] 18%|█▊        | 104/585 [00:30<02:22,  3.38it/s] 18%|█▊        | 105/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 106/585 [00:31<02:22,  3.37it/s] 18%|█▊        | 107/585 [00:31<02:21,  3.37it/s] 18%|█▊        | 108/585 [00:31<02:21,  3.38it/s] 19%|█▊        | 109/585 [00:32<02:20,  3.38it/s] 19%|█▉        | 110/585 [00:32<02:20,  3.38it/s] 19%|█▉        | 111/585 [00:32<02:20,  3.38it/s] 19%|█▉        | 112/585 [00:33<02:19,  3.38it/s] 19%|█▉        | 113/585 [00:33<02:19,  3.38it/s] 19%|█▉        | 114/585 [00:33<02:19,  3.38it/s] 20%|█▉        | 115/585 [00:33<02:19,  3.38it/s] 20%|█▉        | 116/585 [00:34<02:18,  3.38it/s] 20%|██        | 117/585 [00:34<02:19,  3.37it/s][INFO|trainer.py:2140] 2023-08-29 00:54:00,851 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:54:00,852 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:54:00,852 >>   Batch size = 8

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:11, 54.84it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.73it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.00it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.23it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.71it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.39it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.39it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.23it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.31it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.58it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.42it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.31it/s][A
 11%|█         | 67/611 [00:01<00:12, 43.97it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.02it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 43.97it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.99it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.06it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.24it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.33it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.37it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.32it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.20it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.15it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.09it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.98it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.07it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.27it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.33it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.21it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.31it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.24it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.12it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.16it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.06it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.03it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.24it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.36it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.23it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.23it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.26it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.13it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.11it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.03it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.13it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.28it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.31it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.31it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.31it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.18it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.11it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.05it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.04it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.23it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.30it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.28it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.35it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.17it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.12it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.14it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.14it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.08it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.16it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.29it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.21it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.25it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.22it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.15it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.19it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.24it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.28it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.35it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.30it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.14it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.20it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.21it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.20it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.15it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.07it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.19it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.27it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.23it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.20it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.12it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.11it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.25it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.11it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.25it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.31it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.30it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.24it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.19it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.18it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.15it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.14it/s][A
 80%|███████▉  | 487/611 [00:10<00:02, 44.19it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.14it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.20it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.23it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.15it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.19it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.28it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.20it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.13it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.15it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.21it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.11it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.20it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.20it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.12it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.19it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.23it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 43.85it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.06it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.08it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.12it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.22it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.17it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.15it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.17it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.17it/s][A 20%|██        | 117/585 [00:48<02:19,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:54:14,706 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 00:54:14,760 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:54:17,121 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:54:17,173 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:54:17,185 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:55<50:04,  6.43s/it] 20%|██        | 119/585 [00:55<35:40,  4.59s/it] 21%|██        | 120/585 [00:55<25:36,  3.30s/it] 21%|██        | 121/585 [00:56<18:34,  2.40s/it] 21%|██        | 122/585 [00:56<13:39,  1.77s/it] 21%|██        | 123/585 [00:56<10:13,  1.33s/it] 21%|██        | 124/585 [00:57<07:48,  1.02s/it] 21%|██▏       | 125/585 [00:57<06:08,  1.25it/s] 22%|██▏       | 126/585 [00:57<04:57,  1.54it/s] 22%|██▏       | 127/585 [00:57<04:07,  1.85it/s] 22%|██▏       | 128/585 [00:58<03:32,  2.15it/s] 22%|██▏       | 129/585 [00:58<03:08,  2.42it/s] 22%|██▏       | 130/585 [00:58<02:52,  2.65it/s] 22%|██▏       | 131/585 [00:59<02:39,  2.84it/s] 23%|██▎       | 132/585 [00:59<02:31,  3.00it/s] 23%|██▎       | 133/585 [00:59<02:24,  3.12it/s] 23%|██▎       | 134/585 [01:00<02:20,  3.21it/s] 23%|██▎       | 135/585 [01:00<02:17,  3.27it/s] 23%|██▎       | 136/585 [01:00<02:15,  3.32it/s] 23%|██▎       | 137/585 [01:00<02:13,  3.36it/s] 24%|██▎       | 138/585 [01:01<02:12,  3.38it/s] 24%|██▍       | 139/585 [01:01<02:11,  3.39it/s] 24%|██▍       | 140/585 [01:01<02:10,  3.41it/s] 24%|██▍       | 141/585 [01:02<02:10,  3.41it/s] 24%|██▍       | 142/585 [01:02<02:09,  3.41it/s] 24%|██▍       | 143/585 [01:02<02:09,  3.42it/s] 25%|██▍       | 144/585 [01:02<02:08,  3.42it/s] 25%|██▍       | 145/585 [01:03<02:08,  3.43it/s] 25%|██▍       | 146/585 [01:03<02:08,  3.43it/s] 25%|██▌       | 147/585 [01:03<02:07,  3.43it/s] 25%|██▌       | 148/585 [01:04<02:07,  3.43it/s] 25%|██▌       | 149/585 [01:04<02:06,  3.43it/s] 26%|██▌       | 150/585 [01:04<02:06,  3.44it/s] 26%|██▌       | 151/585 [01:04<02:06,  3.43it/s] 26%|██▌       | 152/585 [01:05<02:06,  3.42it/s] 26%|██▌       | 153/585 [01:05<02:06,  3.42it/s] 26%|██▋       | 154/585 [01:05<02:05,  3.42it/s] 26%|██▋       | 155/585 [01:06<02:05,  3.43it/s] 27%|██▋       | 156/585 [01:06<02:05,  3.43it/s] 27%|██▋       | 157/585 [01:06<02:04,  3.43it/s] 27%|██▋       | 158/585 [01:07<02:04,  3.43it/s] 27%|██▋       | 159/585 [01:07<02:04,  3.43it/s] 27%|██▋       | 160/585 [01:07<02:03,  3.43it/s] 28%|██▊       | 161/585 [01:07<02:03,  3.43it/s] 28%|██▊       | 162/585 [01:08<02:03,  3.43it/s] 28%|██▊       | 163/585 [01:08<02:03,  3.42it/s] 28%|██▊       | 164/585 [01:08<02:02,  3.43it/s] 28%|██▊       | 165/585 [01:09<02:02,  3.43it/s] 28%|██▊       | 166/585 [01:09<02:02,  3.43it/s] 29%|██▊       | 167/585 [01:09<02:01,  3.43it/s] 29%|██▊       | 168/585 [01:09<02:01,  3.43it/s] 29%|██▉       | 169/585 [01:10<02:01,  3.43it/s] 29%|██▉       | 170/585 [01:10<02:01,  3.43it/s] 29%|██▉       | 171/585 [01:10<02:00,  3.43it/s] 29%|██▉       | 172/585 [01:11<02:00,  3.43it/s] 30%|██▉       | 173/585 [01:11<01:59,  3.43it/s] 30%|██▉       | 174/585 [01:11<02:00,  3.42it/s] 30%|██▉       | 175/585 [01:11<01:59,  3.42it/s] 30%|███       | 176/585 [01:12<01:59,  3.42it/s] 30%|███       | 177/585 [01:12<01:59,  3.42it/s] 30%|███       | 178/585 [01:12<01:58,  3.42it/s] 31%|███       | 179/585 [01:13<01:58,  3.43it/s] 31%|███       | 180/585 [01:13<01:58,  3.43it/s] 31%|███       | 181/585 [01:13<01:57,  3.43it/s] 31%|███       | 182/585 [01:14<01:57,  3.43it/s] 31%|███▏      | 183/585 [01:14<01:57,  3.43it/s] 31%|███▏      | 184/585 [01:14<01:56,  3.43it/s] 32%|███▏      | 185/585 [01:14<01:57,  3.42it/s] 32%|███▏      | 186/585 [01:15<01:56,  3.42it/s] 32%|███▏      | 187/585 [01:15<01:56,  3.42it/s] 32%|███▏      | 188/585 [01:15<01:55,  3.43it/s] 32%|███▏      | 189/585 [01:16<01:55,  3.42it/s] 32%|███▏      | 190/585 [01:16<01:55,  3.42it/s] 33%|███▎      | 191/585 [01:16<01:54,  3.43it/s] 33%|███▎      | 192/585 [01:16<01:54,  3.43it/s] 33%|███▎      | 193/585 [01:17<01:54,  3.43it/s] 33%|███▎      | 194/585 [01:17<01:53,  3.43it/s] 33%|███▎      | 195/585 [01:17<01:53,  3.43it/s] 34%|███▎      | 196/585 [01:18<01:53,  3.42it/s] 34%|███▎      | 197/585 [01:18<01:53,  3.42it/s] 34%|███▍      | 198/585 [01:18<01:52,  3.43it/s] 34%|███▍      | 199/585 [01:18<01:52,  3.43it/s] 34%|███▍      | 200/585 [01:19<01:52,  3.43it/s] 34%|███▍      | 201/585 [01:19<01:51,  3.43it/s] 35%|███▍      | 202/585 [01:19<01:51,  3.43it/s] 35%|███▍      | 203/585 [01:20<01:51,  3.43it/s] 35%|███▍      | 204/585 [01:20<01:51,  3.43it/s] 35%|███▌      | 205/585 [01:20<01:50,  3.43it/s] 35%|███▌      | 206/585 [01:21<01:50,  3.43it/s] 35%|███▌      | 207/585 [01:21<01:50,  3.42it/s] 36%|███▌      | 208/585 [01:21<01:50,  3.42it/s] 36%|███▌      | 209/585 [01:21<01:49,  3.42it/s] 36%|███▌      | 210/585 [01:22<01:49,  3.42it/s] 36%|███▌      | 211/585 [01:22<01:49,  3.43it/s] 36%|███▌      | 212/585 [01:22<01:48,  3.43it/s] 36%|███▋      | 213/585 [01:23<01:48,  3.43it/s] 37%|███▋      | 214/585 [01:23<01:48,  3.43it/s] 37%|███▋      | 215/585 [01:23<01:47,  3.43it/s] 37%|███▋      | 216/585 [01:23<01:47,  3.43it/s] 37%|███▋      | 217/585 [01:24<01:47,  3.43it/s] 37%|███▋      | 218/585 [01:24<01:47,  3.43it/s] 37%|███▋      | 219/585 [01:24<01:46,  3.43it/s] 38%|███▊      | 220/585 [01:25<01:46,  3.42it/s] 38%|███▊      | 221/585 [01:25<01:46,  3.43it/s] 38%|███▊      | 222/585 [01:25<01:45,  3.43it/s] 38%|███▊      | 223/585 [01:25<01:45,  3.43it/s] 38%|███▊      | 224/585 [01:26<01:45,  3.42it/s] 38%|███▊      | 225/585 [01:26<01:45,  3.42it/s] 39%|███▊      | 226/585 [01:26<01:44,  3.42it/s] 39%|███▉      | 227/585 [01:27<01:44,  3.42it/s] 39%|███▉      | 228/585 [01:27<01:44,  3.42it/s] 39%|███▉      | 229/585 [01:27<01:43,  3.43it/s] 39%|███▉      | 230/585 [01:28<01:43,  3.43it/s] 39%|███▉      | 231/585 [01:28<01:43,  3.43it/s] 40%|███▉      | 232/585 [01:28<01:42,  3.43it/s] 40%|███▉      | 233/585 [01:28<01:42,  3.43it/s] 40%|████      | 234/585 [01:29<01:42,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 00:54:55,464 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:54:55,464 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:54:55,464 >>   Batch size = 8
{'eval_loss': 0.9694299101829529, 'eval_runtime': 13.8357, 'eval_samples_per_second': 352.855, 'eval_steps_per_second': 44.161, 'epoch': 1.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:11, 54.75it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.80it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.05it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.22it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.74it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.46it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.37it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.30it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.25it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.33it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.35it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.21it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.15it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.11it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.10it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.04it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.01it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.20it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.32it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.31it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.27it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.14it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.12it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.11it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.97it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.99it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.13it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.27it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.25it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.23it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.11it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.17it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.00it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.03it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.05it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.19it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.25it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.18it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.26it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.19it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.13it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.94it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.11it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.15it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.26it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.28it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.27it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.20it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.15it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.10it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.99it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 43.99it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.17it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.12it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.14it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.19it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.15it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.11it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.10it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.00it/s][A
 50%|█████     | 307/611 [00:06<00:06, 43.89it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.14it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.23it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.07it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.26it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.17it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.10it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.11it/s][A
 57%|█████▋    | 347/611 [00:07<00:06, 43.99it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 43.91it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.16it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.24it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.10it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.32it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.23it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.12it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.08it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.09it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 43.94it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.08it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.22it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.16it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.29it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.22it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.01it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.08it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.09it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.10it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.10it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.21it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.27it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.26it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.14it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.12it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.09it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.06it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.02it/s][A
 81%|████████  | 492/611 [00:11<00:02, 43.96it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.19it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.24it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.18it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.16it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.15it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.19it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.02it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.08it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.20it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.22it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.17it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.16it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.16it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.10it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.12it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.10it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 43.98it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.17it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.20it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.28it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.26it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.18it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.10it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.10it/s][A 40%|████      | 234/585 [01:43<01:42,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:55:09,338 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 00:55:09,382 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:55:12,030 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:55:12,051 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:55:12,083 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:51<39:32,  6.78s/it] 40%|████      | 236/585 [01:51<28:09,  4.84s/it] 41%|████      | 237/585 [01:51<20:10,  3.48s/it] 41%|████      | 238/585 [01:52<14:35,  2.52s/it] 41%|████      | 239/585 [01:52<10:41,  1.85s/it] 41%|████      | 240/585 [01:52<07:58,  1.39s/it] 41%|████      | 241/585 [01:52<06:04,  1.06s/it] 41%|████▏     | 242/585 [01:53<04:44,  1.20it/s] 42%|████▏     | 243/585 [01:53<03:49,  1.49it/s] 42%|████▏     | 244/585 [01:53<03:10,  1.79it/s] 42%|████▏     | 245/585 [01:54<02:42,  2.09it/s] 42%|████▏     | 246/585 [01:54<02:23,  2.36it/s] 42%|████▏     | 247/585 [01:54<02:10,  2.59it/s] 42%|████▏     | 248/585 [01:54<02:00,  2.79it/s] 43%|████▎     | 249/585 [01:55<01:53,  2.95it/s] 43%|████▎     | 250/585 [01:55<01:49,  3.07it/s] 43%|████▎     | 251/585 [01:55<01:45,  3.15it/s] 43%|████▎     | 252/585 [01:56<01:43,  3.22it/s] 43%|████▎     | 253/585 [01:56<01:41,  3.27it/s] 43%|████▎     | 254/585 [01:56<01:40,  3.31it/s] 44%|████▎     | 255/585 [01:57<01:38,  3.33it/s] 44%|████▍     | 256/585 [01:57<01:37,  3.36it/s] 44%|████▍     | 257/585 [01:57<01:36,  3.38it/s] 44%|████▍     | 258/585 [01:57<01:36,  3.40it/s] 44%|████▍     | 259/585 [01:58<01:35,  3.41it/s] 44%|████▍     | 260/585 [01:58<01:35,  3.42it/s] 45%|████▍     | 261/585 [01:58<01:34,  3.42it/s] 45%|████▍     | 262/585 [01:59<01:34,  3.42it/s] 45%|████▍     | 263/585 [01:59<01:33,  3.43it/s] 45%|████▌     | 264/585 [01:59<01:33,  3.43it/s] 45%|████▌     | 265/585 [01:59<01:33,  3.43it/s] 45%|████▌     | 266/585 [02:00<01:33,  3.42it/s] 46%|████▌     | 267/585 [02:00<01:32,  3.42it/s] 46%|████▌     | 268/585 [02:00<01:32,  3.43it/s] 46%|████▌     | 269/585 [02:01<01:32,  3.43it/s] 46%|████▌     | 270/585 [02:01<01:31,  3.43it/s] 46%|████▋     | 271/585 [02:01<01:31,  3.43it/s] 46%|████▋     | 272/585 [02:02<01:31,  3.43it/s] 47%|████▋     | 273/585 [02:02<01:30,  3.43it/s] 47%|████▋     | 274/585 [02:02<01:30,  3.43it/s] 47%|████▋     | 275/585 [02:02<01:30,  3.43it/s] 47%|████▋     | 276/585 [02:03<01:30,  3.43it/s] 47%|████▋     | 277/585 [02:03<01:30,  3.42it/s] 48%|████▊     | 278/585 [02:03<01:29,  3.42it/s] 48%|████▊     | 279/585 [02:04<01:29,  3.42it/s] 48%|████▊     | 280/585 [02:04<01:28,  3.43it/s] 48%|████▊     | 281/585 [02:04<01:28,  3.43it/s] 48%|████▊     | 282/585 [02:04<01:28,  3.43it/s] 48%|████▊     | 283/585 [02:05<01:28,  3.43it/s] 49%|████▊     | 284/585 [02:05<01:27,  3.43it/s] 49%|████▊     | 285/585 [02:05<01:27,  3.43it/s] 49%|████▉     | 286/585 [02:06<01:27,  3.43it/s] 49%|████▉     | 287/585 [02:06<01:26,  3.43it/s] 49%|████▉     | 288/585 [02:06<01:26,  3.42it/s] 49%|████▉     | 289/585 [02:06<01:26,  3.43it/s] 50%|████▉     | 290/585 [02:07<01:26,  3.43it/s] 50%|████▉     | 291/585 [02:07<01:25,  3.43it/s] 50%|████▉     | 292/585 [02:07<01:25,  3.43it/s] 50%|█████     | 293/585 [02:08<01:25,  3.43it/s] 50%|█████     | 294/585 [02:08<01:24,  3.43it/s] 50%|█████     | 295/585 [02:08<01:24,  3.43it/s] 51%|█████     | 296/585 [02:09<01:24,  3.43it/s] 51%|█████     | 297/585 [02:09<01:23,  3.43it/s] 51%|█████     | 298/585 [02:09<01:23,  3.43it/s] 51%|█████     | 299/585 [02:09<01:23,  3.42it/s] 51%|█████▏    | 300/585 [02:10<01:23,  3.42it/s] 51%|█████▏    | 301/585 [02:10<01:23,  3.42it/s] 52%|█████▏    | 302/585 [02:10<01:22,  3.43it/s] 52%|█████▏    | 303/585 [02:11<01:22,  3.43it/s] 52%|█████▏    | 304/585 [02:11<01:21,  3.43it/s] 52%|█████▏    | 305/585 [02:11<01:21,  3.43it/s] 52%|█████▏    | 306/585 [02:11<01:21,  3.43it/s] 52%|█████▏    | 307/585 [02:12<01:21,  3.43it/s] 53%|█████▎    | 308/585 [02:12<01:20,  3.43it/s] 53%|█████▎    | 309/585 [02:12<01:20,  3.43it/s] 53%|█████▎    | 310/585 [02:13<01:20,  3.42it/s] 53%|█████▎    | 311/585 [02:13<01:20,  3.42it/s] 53%|█████▎    | 312/585 [02:13<01:19,  3.42it/s] 54%|█████▎    | 313/585 [02:13<01:19,  3.43it/s] 54%|█████▎    | 314/585 [02:14<01:18,  3.43it/s] 54%|█████▍    | 315/585 [02:14<01:18,  3.43it/s] 54%|█████▍    | 316/585 [02:14<01:18,  3.43it/s] 54%|█████▍    | 317/585 [02:15<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:15<01:17,  3.43it/s] 55%|█████▍    | 319/585 [02:15<01:17,  3.43it/s] 55%|█████▍    | 320/585 [02:16<01:17,  3.43it/s] 55%|█████▍    | 321/585 [02:16<01:17,  3.41it/s] 55%|█████▌    | 322/585 [02:16<01:16,  3.42it/s] 55%|█████▌    | 323/585 [02:16<01:16,  3.42it/s] 55%|█████▌    | 324/585 [02:17<01:16,  3.42it/s] 56%|█████▌    | 325/585 [02:17<01:15,  3.43it/s] 56%|█████▌    | 326/585 [02:17<01:15,  3.43it/s] 56%|█████▌    | 327/585 [02:18<01:15,  3.43it/s] 56%|█████▌    | 328/585 [02:18<01:14,  3.43it/s] 56%|█████▌    | 329/585 [02:18<01:14,  3.43it/s] 56%|█████▋    | 330/585 [02:18<01:14,  3.43it/s] 57%|█████▋    | 331/585 [02:19<01:14,  3.43it/s] 57%|█████▋    | 332/585 [02:19<01:13,  3.42it/s] 57%|█████▋    | 333/585 [02:19<01:13,  3.42it/s] 57%|█████▋    | 334/585 [02:20<01:13,  3.42it/s] 57%|█████▋    | 335/585 [02:20<01:12,  3.43it/s] 57%|█████▋    | 336/585 [02:20<01:12,  3.43it/s] 58%|█████▊    | 337/585 [02:20<01:12,  3.43it/s] 58%|█████▊    | 338/585 [02:21<01:12,  3.43it/s] 58%|█████▊    | 339/585 [02:21<01:11,  3.43it/s] 58%|█████▊    | 340/585 [02:21<01:11,  3.43it/s] 58%|█████▊    | 341/585 [02:22<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:22<01:10,  3.43it/s] 59%|█████▊    | 343/585 [02:22<01:10,  3.42it/s] 59%|█████▉    | 344/585 [02:23<01:10,  3.42it/s] 59%|█████▉    | 345/585 [02:23<01:10,  3.43it/s] 59%|█████▉    | 346/585 [02:23<01:09,  3.42it/s] 59%|█████▉    | 347/585 [02:23<01:09,  3.43it/s] 59%|█████▉    | 348/585 [02:24<01:09,  3.42it/s] 60%|█████▉    | 349/585 [02:24<01:08,  3.43it/s] 60%|█████▉    | 350/585 [02:24<01:08,  3.43it/s] 60%|██████    | 351/585 [02:25<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 00:55:51,313 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:55:51,314 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:55:51,314 >>   Batch size = 8
{'eval_loss': 0.9851479530334473, 'eval_runtime': 13.8494, 'eval_samples_per_second': 352.506, 'eval_steps_per_second': 44.117, 'epoch': 2.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:11, 54.77it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.55it/s][A
  3%|▎         | 17/611 [00:00<00:12, 45.89it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.04it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.64it/s][A
  5%|▌         | 32/611 [00:00<00:12, 44.55it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.40it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.29it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.33it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.45it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.30it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.14it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.01it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 43.95it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.06it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.01it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.04it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.21it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.32it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.30it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.23it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.12it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 44.05it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 44.04it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.06it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 43.97it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.08it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.20it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.19it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.16it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.19it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.03it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.96it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.06it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.11it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.17it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.28it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.25it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.17it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.13it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.12it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.00it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.03it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.10it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.18it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.22it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.24it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.28it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.13it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.08it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.04it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.16it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.23it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.26it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.23it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.16it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.17it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.04it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.08it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.14it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.07it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.22it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.26it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.22it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.20it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.16it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.07it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.07it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.14it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.05it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.23it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.27it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.20it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.08it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.02it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 43.85it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.01it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.18it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.18it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.33it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.30it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.22it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.15it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.12it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.10it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.11it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.18it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.20it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.22it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.22it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.29it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.08it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.10it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.01it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.07it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.17it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.23it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.24it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.16it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.18it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.12it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.11it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.12it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.08it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.04it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.20it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.31it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.15it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.18it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.09it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.03it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.16it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.10it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.13it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.22it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.29it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.12it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.21it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.12it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.12it/s][A 60%|██████    | 351/585 [02:38<01:08,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:56:05,181 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 00:56:05,199 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:56:07,088 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:56:07,103 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:56:07,117 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:45<24:19,  6.26s/it] 60%|██████    | 353/585 [02:45<17:18,  4.48s/it] 61%|██████    | 354/585 [02:45<12:24,  3.22s/it] 61%|██████    | 355/585 [02:46<08:59,  2.34s/it] 61%|██████    | 356/585 [02:46<06:35,  1.73s/it] 61%|██████    | 357/585 [02:46<04:56,  1.30s/it] 61%|██████    | 358/585 [02:47<03:46,  1.00it/s] 61%|██████▏   | 359/585 [02:47<02:57,  1.27it/s] 62%|██████▏   | 360/585 [02:47<02:23,  1.56it/s] 62%|██████▏   | 361/585 [02:47<02:00,  1.86it/s] 62%|██████▏   | 362/585 [02:48<01:43,  2.15it/s] 62%|██████▏   | 363/585 [02:48<01:31,  2.42it/s] 62%|██████▏   | 364/585 [02:48<01:23,  2.64it/s] 62%|██████▏   | 365/585 [02:49<01:17,  2.82it/s] 63%|██████▎   | 366/585 [02:49<01:13,  2.97it/s] 63%|██████▎   | 367/585 [02:49<01:10,  3.08it/s] 63%|██████▎   | 368/585 [02:49<01:08,  3.17it/s] 63%|██████▎   | 369/585 [02:50<01:06,  3.23it/s] 63%|██████▎   | 370/585 [02:50<01:05,  3.27it/s] 63%|██████▎   | 371/585 [02:50<01:04,  3.31it/s] 64%|██████▎   | 372/585 [02:51<01:04,  3.32it/s] 64%|██████▍   | 373/585 [02:51<01:05,  3.25it/s] 64%|██████▍   | 374/585 [02:51<01:03,  3.30it/s] 64%|██████▍   | 375/585 [02:52<01:03,  3.33it/s] 64%|██████▍   | 376/585 [02:52<01:02,  3.36it/s] 64%|██████▍   | 377/585 [02:52<01:01,  3.38it/s] 65%|██████▍   | 378/585 [02:52<01:00,  3.40it/s] 65%|██████▍   | 379/585 [02:53<01:00,  3.41it/s] 65%|██████▍   | 380/585 [02:53<01:00,  3.41it/s] 65%|██████▌   | 381/585 [02:53<00:59,  3.42it/s] 65%|██████▌   | 382/585 [02:54<00:59,  3.42it/s] 65%|██████▌   | 383/585 [02:54<00:58,  3.43it/s] 66%|██████▌   | 384/585 [02:54<00:58,  3.43it/s] 66%|██████▌   | 385/585 [02:55<00:58,  3.43it/s] 66%|██████▌   | 386/585 [02:55<00:58,  3.43it/s] 66%|██████▌   | 387/585 [02:55<00:57,  3.43it/s] 66%|██████▋   | 388/585 [02:55<00:57,  3.43it/s] 66%|██████▋   | 389/585 [02:56<00:57,  3.43it/s] 67%|██████▋   | 390/585 [02:56<00:56,  3.44it/s] 67%|██████▋   | 391/585 [02:56<00:56,  3.43it/s] 67%|██████▋   | 392/585 [02:57<00:56,  3.43it/s] 67%|██████▋   | 393/585 [02:57<00:56,  3.43it/s] 67%|██████▋   | 394/585 [02:57<00:55,  3.42it/s] 68%|██████▊   | 395/585 [02:57<00:55,  3.43it/s] 68%|██████▊   | 396/585 [02:58<00:55,  3.43it/s] 68%|██████▊   | 397/585 [02:58<00:55,  3.42it/s] 68%|██████▊   | 398/585 [02:58<00:54,  3.42it/s] 68%|██████▊   | 399/585 [02:59<00:54,  3.42it/s] 68%|██████▊   | 400/585 [02:59<00:53,  3.43it/s] 69%|██████▊   | 401/585 [02:59<00:53,  3.42it/s] 69%|██████▊   | 402/585 [02:59<00:53,  3.43it/s] 69%|██████▉   | 403/585 [03:00<00:53,  3.43it/s] 69%|██████▉   | 404/585 [03:00<00:52,  3.43it/s] 69%|██████▉   | 405/585 [03:00<00:52,  3.43it/s] 69%|██████▉   | 406/585 [03:01<00:52,  3.42it/s] 70%|██████▉   | 407/585 [03:01<00:51,  3.43it/s] 70%|██████▉   | 408/585 [03:01<00:51,  3.41it/s] 70%|██████▉   | 409/585 [03:02<00:51,  3.42it/s] 70%|███████   | 410/585 [03:02<00:51,  3.42it/s] 70%|███████   | 411/585 [03:02<00:50,  3.42it/s] 70%|███████   | 412/585 [03:02<00:50,  3.42it/s] 71%|███████   | 413/585 [03:03<00:50,  3.43it/s] 71%|███████   | 414/585 [03:03<00:49,  3.43it/s] 71%|███████   | 415/585 [03:03<00:49,  3.43it/s] 71%|███████   | 416/585 [03:04<00:49,  3.43it/s] 71%|███████▏  | 417/585 [03:04<00:48,  3.43it/s] 71%|███████▏  | 418/585 [03:04<00:48,  3.43it/s] 72%|███████▏  | 419/585 [03:04<00:48,  3.41it/s] 72%|███████▏  | 420/585 [03:05<00:48,  3.42it/s] 72%|███████▏  | 421/585 [03:05<00:47,  3.42it/s] 72%|███████▏  | 422/585 [03:05<00:47,  3.42it/s] 72%|███████▏  | 423/585 [03:06<00:47,  3.43it/s] 72%|███████▏  | 424/585 [03:06<00:47,  3.42it/s] 73%|███████▎  | 425/585 [03:06<00:46,  3.43it/s] 73%|███████▎  | 426/585 [03:06<00:46,  3.43it/s] 73%|███████▎  | 427/585 [03:07<00:46,  3.43it/s] 73%|███████▎  | 428/585 [03:07<00:45,  3.43it/s] 73%|███████▎  | 429/585 [03:07<00:45,  3.43it/s] 74%|███████▎  | 430/585 [03:08<00:45,  3.41it/s] 74%|███████▎  | 431/585 [03:08<00:45,  3.41it/s] 74%|███████▍  | 432/585 [03:08<00:44,  3.42it/s] 74%|███████▍  | 433/585 [03:09<00:44,  3.42it/s] 74%|███████▍  | 434/585 [03:09<00:44,  3.42it/s] 74%|███████▍  | 435/585 [03:09<00:43,  3.43it/s] 75%|███████▍  | 436/585 [03:09<00:43,  3.43it/s] 75%|███████▍  | 437/585 [03:10<00:43,  3.43it/s] 75%|███████▍  | 438/585 [03:10<00:42,  3.43it/s] 75%|███████▌  | 439/585 [03:10<00:42,  3.43it/s] 75%|███████▌  | 440/585 [03:11<00:42,  3.43it/s] 75%|███████▌  | 441/585 [03:11<00:42,  3.43it/s] 76%|███████▌  | 442/585 [03:11<00:41,  3.43it/s] 76%|███████▌  | 443/585 [03:11<00:41,  3.43it/s] 76%|███████▌  | 444/585 [03:12<00:41,  3.42it/s] 76%|███████▌  | 445/585 [03:12<00:40,  3.43it/s] 76%|███████▌  | 446/585 [03:12<00:40,  3.43it/s] 76%|███████▋  | 447/585 [03:13<00:40,  3.43it/s] 77%|███████▋  | 448/585 [03:13<00:39,  3.43it/s] 77%|███████▋  | 449/585 [03:13<00:39,  3.42it/s] 77%|███████▋  | 450/585 [03:13<00:39,  3.43it/s] 77%|███████▋  | 451/585 [03:14<00:39,  3.43it/s] 77%|███████▋  | 452/585 [03:14<00:38,  3.42it/s] 77%|███████▋  | 453/585 [03:14<00:38,  3.42it/s] 78%|███████▊  | 454/585 [03:15<00:38,  3.42it/s] 78%|███████▊  | 455/585 [03:15<00:37,  3.43it/s] 78%|███████▊  | 456/585 [03:15<00:37,  3.43it/s] 78%|███████▊  | 457/585 [03:16<00:37,  3.43it/s] 78%|███████▊  | 458/585 [03:16<00:37,  3.43it/s] 78%|███████▊  | 459/585 [03:16<00:36,  3.43it/s] 79%|███████▊  | 460/585 [03:16<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:17<00:36,  3.43it/s] 79%|███████▉  | 462/585 [03:17<00:35,  3.43it/s] 79%|███████▉  | 463/585 [03:17<00:35,  3.42it/s] 79%|███████▉  | 464/585 [03:18<00:35,  3.42it/s] 79%|███████▉  | 465/585 [03:18<00:35,  3.42it/s] 80%|███████▉  | 466/585 [03:18<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:18<00:34,  3.43it/s] 80%|████████  | 468/585 [03:19<00:34,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 00:56:45,486 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:56:45,486 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:56:45,486 >>   Batch size = 8
{'eval_loss': 1.0012855529785156, 'eval_runtime': 13.8508, 'eval_samples_per_second': 352.471, 'eval_steps_per_second': 44.113, 'epoch': 3.0}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.41it/s][A
  2%|▏         | 12/611 [00:00<00:12, 47.84it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.03it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.14it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.79it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.50it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.31it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.17it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.30it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.46it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.34it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.20it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.11it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 43.95it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.02it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 44.05it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 44.01it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.14it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.25it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.31it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.19it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.05it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.90it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.97it/s][A
 21%|██        | 127/611 [00:02<00:10, 44.03it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.11it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.18it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.23it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.21it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.16it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.13it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.08it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 43.96it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 44.09it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.21it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.16it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.24it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.31it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.13it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 43.98it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 44.04it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 44.08it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.18it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.13it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.27it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.17it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.27it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.18it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.04it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.04it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 44.10it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.17it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.09it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.11it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.16it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.24it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.16it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.08it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 43.94it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.03it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.03it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.03it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.25it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.28it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.17it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.18it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.08it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.10it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.10it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.13it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.17it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.20it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.28it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.13it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.07it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 44.14it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.22it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.25it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.20it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.18it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.24it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.20it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.13it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.12it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.00it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.12it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.20it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.14it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.26it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.28it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.24it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.08it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.11it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.17it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.09it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.19it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.17it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.23it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.28it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.19it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.16it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.08it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.09it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.15it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.10it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.17it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.16it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.15it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.18it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.09it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.09it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 43.99it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.21it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.25it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.26it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.22it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.21it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.07it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.08it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.08it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.03it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.03it/s][A 80%|████████  | 468/585 [03:33<00:34,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:56:59,349 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 00:56:59,368 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:57:01,163 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:57:01,178 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:57:01,187 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:39<11:57,  6.18s/it] 80%|████████  | 470/585 [03:39<08:27,  4.42s/it] 81%|████████  | 471/585 [03:39<06:02,  3.18s/it] 81%|████████  | 472/585 [03:40<04:21,  2.31s/it] 81%|████████  | 473/585 [03:40<03:11,  1.71s/it] 81%|████████  | 474/585 [03:40<02:22,  1.28s/it] 81%|████████  | 475/585 [03:40<01:48,  1.01it/s] 81%|████████▏ | 476/585 [03:41<01:24,  1.28it/s] 82%|████████▏ | 477/585 [03:41<01:08,  1.58it/s] 82%|████████▏ | 478/585 [03:41<00:56,  1.89it/s] 82%|████████▏ | 479/585 [03:42<00:48,  2.18it/s] 82%|████████▏ | 480/585 [03:42<00:42,  2.45it/s] 82%|████████▏ | 481/585 [03:42<00:38,  2.68it/s] 82%|████████▏ | 482/585 [03:42<00:35,  2.86it/s] 83%|████████▎ | 483/585 [03:43<00:33,  3.01it/s] 83%|████████▎ | 484/585 [03:43<00:32,  3.13it/s] 83%|████████▎ | 485/585 [03:43<00:31,  3.21it/s] 83%|████████▎ | 486/585 [03:44<00:30,  3.28it/s] 83%|████████▎ | 487/585 [03:44<00:29,  3.32it/s] 83%|████████▎ | 488/585 [03:44<00:28,  3.35it/s] 84%|████████▎ | 489/585 [03:45<00:28,  3.38it/s] 84%|████████▍ | 490/585 [03:45<00:28,  3.39it/s] 84%|████████▍ | 491/585 [03:45<00:27,  3.41it/s] 84%|████████▍ | 492/585 [03:45<00:27,  3.40it/s] 84%|████████▍ | 493/585 [03:46<00:26,  3.41it/s] 84%|████████▍ | 494/585 [03:46<00:26,  3.42it/s] 85%|████████▍ | 495/585 [03:46<00:26,  3.41it/s] 85%|████████▍ | 496/585 [03:47<00:26,  3.42it/s] 85%|████████▍ | 497/585 [03:47<00:25,  3.43it/s] 85%|████████▌ | 498/585 [03:47<00:25,  3.42it/s] 85%|████████▌ | 499/585 [03:47<00:25,  3.43it/s] 85%|████████▌ | 500/585 [03:48<00:24,  3.43it/s]                                                  85%|████████▌ | 500/585 [03:48<00:24,  3.43it/s] 86%|████████▌ | 501/585 [03:48<00:24,  3.43it/s] 86%|████████▌ | 502/585 [03:48<00:24,  3.43it/s] 86%|████████▌ | 503/585 [03:49<00:24,  3.41it/s] 86%|████████▌ | 504/585 [03:49<00:23,  3.42it/s] 86%|████████▋ | 505/585 [03:49<00:23,  3.42it/s] 86%|████████▋ | 506/585 [03:49<00:23,  3.43it/s] 87%|████████▋ | 507/585 [03:50<00:22,  3.43it/s] 87%|████████▋ | 508/585 [03:50<00:22,  3.43it/s] 87%|████████▋ | 509/585 [03:50<00:22,  3.43it/s] 87%|████████▋ | 510/585 [03:51<00:21,  3.43it/s] 87%|████████▋ | 511/585 [03:51<00:21,  3.42it/s] 88%|████████▊ | 512/585 [03:51<00:21,  3.33it/s] 88%|████████▊ | 513/585 [03:52<00:21,  3.37it/s] 88%|████████▊ | 514/585 [03:52<00:21,  3.37it/s] 88%|████████▊ | 515/585 [03:52<00:20,  3.39it/s] 88%|████████▊ | 516/585 [03:52<00:20,  3.40it/s] 88%|████████▊ | 517/585 [03:53<00:19,  3.41it/s] 89%|████████▊ | 518/585 [03:53<00:19,  3.42it/s] 89%|████████▊ | 519/585 [03:53<00:19,  3.42it/s] 89%|████████▉ | 520/585 [03:54<00:18,  3.43it/s] 89%|████████▉ | 521/585 [03:54<00:18,  3.42it/s] 89%|████████▉ | 522/585 [03:54<00:18,  3.43it/s] 89%|████████▉ | 523/585 [03:54<00:18,  3.43it/s] 90%|████████▉ | 524/585 [03:55<00:17,  3.43it/s] 90%|████████▉ | 525/585 [03:55<00:17,  3.41it/s] 90%|████████▉ | 526/585 [03:55<00:17,  3.42it/s] 90%|█████████ | 527/585 [03:56<00:16,  3.42it/s] 90%|█████████ | 528/585 [03:56<00:16,  3.42it/s] 90%|█████████ | 529/585 [03:56<00:16,  3.42it/s] 91%|█████████ | 530/585 [03:57<00:16,  3.43it/s] 91%|█████████ | 531/585 [03:57<00:15,  3.42it/s] 91%|█████████ | 532/585 [03:57<00:15,  3.43it/s] 91%|█████████ | 533/585 [03:57<00:15,  3.43it/s] 91%|█████████▏| 534/585 [03:58<00:14,  3.43it/s] 91%|█████████▏| 535/585 [03:58<00:14,  3.43it/s] 92%|█████████▏| 536/585 [03:58<00:14,  3.43it/s] 92%|█████████▏| 537/585 [03:59<00:13,  3.43it/s] 92%|█████████▏| 538/585 [03:59<00:13,  3.43it/s] 92%|█████████▏| 539/585 [03:59<00:13,  3.43it/s] 92%|█████████▏| 540/585 [03:59<00:13,  3.42it/s] 92%|█████████▏| 541/585 [04:00<00:12,  3.42it/s] 93%|█████████▎| 542/585 [04:00<00:12,  3.43it/s] 93%|█████████▎| 543/585 [04:00<00:12,  3.43it/s] 93%|█████████▎| 544/585 [04:01<00:11,  3.43it/s] 93%|█████████▎| 545/585 [04:01<00:11,  3.43it/s] 93%|█████████▎| 546/585 [04:01<00:11,  3.43it/s] 94%|█████████▎| 547/585 [04:01<00:11,  3.43it/s] 94%|█████████▎| 548/585 [04:02<00:10,  3.43it/s] 94%|█████████▍| 549/585 [04:02<00:10,  3.43it/s] 94%|█████████▍| 550/585 [04:02<00:10,  3.43it/s] 94%|█████████▍| 551/585 [04:03<00:09,  3.42it/s] 94%|█████████▍| 552/585 [04:03<00:09,  3.42it/s] 95%|█████████▍| 553/585 [04:03<00:09,  3.43it/s] 95%|█████████▍| 554/585 [04:04<00:09,  3.43it/s] 95%|█████████▍| 555/585 [04:04<00:08,  3.43it/s] 95%|█████████▌| 556/585 [04:04<00:08,  3.43it/s] 95%|█████████▌| 557/585 [04:04<00:08,  3.43it/s] 95%|█████████▌| 558/585 [04:05<00:07,  3.43it/s] 96%|█████████▌| 559/585 [04:05<00:07,  3.43it/s] 96%|█████████▌| 560/585 [04:05<00:07,  3.43it/s] 96%|█████████▌| 561/585 [04:06<00:06,  3.43it/s] 96%|█████████▌| 562/585 [04:06<00:06,  3.42it/s] 96%|█████████▌| 563/585 [04:06<00:06,  3.42it/s] 96%|█████████▋| 564/585 [04:06<00:06,  3.43it/s] 97%|█████████▋| 565/585 [04:07<00:05,  3.43it/s] 97%|█████████▋| 566/585 [04:07<00:05,  3.43it/s] 97%|█████████▋| 567/585 [04:07<00:05,  3.43it/s] 97%|█████████▋| 568/585 [04:08<00:04,  3.43it/s] 97%|█████████▋| 569/585 [04:08<00:04,  3.43it/s] 97%|█████████▋| 570/585 [04:08<00:04,  3.43it/s] 98%|█████████▊| 571/585 [04:08<00:04,  3.43it/s] 98%|█████████▊| 572/585 [04:09<00:03,  3.43it/s] 98%|█████████▊| 573/585 [04:09<00:03,  3.41it/s] 98%|█████████▊| 574/585 [04:09<00:03,  3.42it/s] 98%|█████████▊| 575/585 [04:10<00:02,  3.42it/s] 98%|█████████▊| 576/585 [04:10<00:02,  3.42it/s] 99%|█████████▊| 577/585 [04:10<00:02,  3.43it/s] 99%|█████████▉| 578/585 [04:11<00:02,  3.43it/s] 99%|█████████▉| 579/585 [04:11<00:01,  3.43it/s] 99%|█████████▉| 580/585 [04:11<00:01,  3.43it/s] 99%|█████████▉| 581/585 [04:11<00:01,  3.43it/s] 99%|█████████▉| 582/585 [04:12<00:00,  3.43it/s]100%|█████████▉| 583/585 [04:12<00:00,  3.43it/s]100%|█████████▉| 584/585 [04:12<00:00,  3.42it/s]100%|██████████| 585/585 [04:13<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 00:57:39,275 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:57:39,276 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:57:39,276 >>   Batch size = 8
{'eval_loss': 1.011523962020874, 'eval_runtime': 13.8497, 'eval_samples_per_second': 352.499, 'eval_steps_per_second': 44.117, 'epoch': 4.0}
{'loss': 0.3804, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/611 [00:00<?, ?it/s][A
  1%|          | 6/611 [00:00<00:10, 55.49it/s][A
  2%|▏         | 12/611 [00:00<00:12, 48.14it/s][A
  3%|▎         | 17/611 [00:00<00:12, 46.25it/s][A
  4%|▎         | 22/611 [00:00<00:13, 45.24it/s][A
  4%|▍         | 27/611 [00:00<00:13, 44.77it/s][A
  5%|▌         | 32/611 [00:00<00:13, 44.38it/s][A
  6%|▌         | 37/611 [00:00<00:12, 44.24it/s][A
  7%|▋         | 42/611 [00:00<00:12, 44.14it/s][A
  8%|▊         | 47/611 [00:01<00:12, 44.22it/s][A
  9%|▊         | 52/611 [00:01<00:12, 44.38it/s][A
  9%|▉         | 57/611 [00:01<00:12, 44.41it/s][A
 10%|█         | 62/611 [00:01<00:12, 44.26it/s][A
 11%|█         | 67/611 [00:01<00:12, 44.24it/s][A
 12%|█▏        | 72/611 [00:01<00:12, 44.03it/s][A
 13%|█▎        | 77/611 [00:01<00:12, 44.06it/s][A
 13%|█▎        | 82/611 [00:01<00:12, 43.99it/s][A
 14%|█▍        | 87/611 [00:01<00:11, 43.96it/s][A
 15%|█▌        | 92/611 [00:02<00:11, 44.17it/s][A
 16%|█▌        | 97/611 [00:02<00:11, 44.32it/s][A
 17%|█▋        | 102/611 [00:02<00:11, 44.24it/s][A
 18%|█▊        | 107/611 [00:02<00:11, 44.22it/s][A
 18%|█▊        | 112/611 [00:02<00:11, 44.14it/s][A
 19%|█▉        | 117/611 [00:02<00:11, 43.93it/s][A
 20%|█▉        | 122/611 [00:02<00:11, 43.93it/s][A
 21%|██        | 127/611 [00:02<00:11, 43.93it/s][A
 22%|██▏       | 132/611 [00:02<00:10, 44.00it/s][A
 22%|██▏       | 137/611 [00:03<00:10, 44.19it/s][A
 23%|██▎       | 142/611 [00:03<00:10, 44.28it/s][A
 24%|██▍       | 147/611 [00:03<00:10, 44.16it/s][A
 25%|██▍       | 152/611 [00:03<00:10, 44.21it/s][A
 26%|██▌       | 157/611 [00:03<00:10, 44.03it/s][A
 27%|██▋       | 162/611 [00:03<00:10, 44.11it/s][A
 27%|██▋       | 167/611 [00:03<00:10, 44.05it/s][A
 28%|██▊       | 172/611 [00:03<00:09, 43.99it/s][A
 29%|██▉       | 177/611 [00:03<00:09, 44.04it/s][A
 30%|██▉       | 182/611 [00:04<00:09, 44.26it/s][A
 31%|███       | 187/611 [00:04<00:09, 44.30it/s][A
 31%|███▏      | 192/611 [00:04<00:09, 44.28it/s][A
 32%|███▏      | 197/611 [00:04<00:09, 44.14it/s][A
 33%|███▎      | 202/611 [00:04<00:09, 44.15it/s][A
 34%|███▍      | 207/611 [00:04<00:09, 43.98it/s][A
 35%|███▍      | 212/611 [00:04<00:09, 43.98it/s][A
 36%|███▌      | 217/611 [00:04<00:08, 44.06it/s][A
 36%|███▋      | 222/611 [00:05<00:08, 44.12it/s][A
 37%|███▋      | 227/611 [00:05<00:08, 44.16it/s][A
 38%|███▊      | 232/611 [00:05<00:08, 44.28it/s][A
 39%|███▉      | 237/611 [00:05<00:08, 44.20it/s][A
 40%|███▉      | 242/611 [00:05<00:08, 44.25it/s][A
 40%|████      | 247/611 [00:05<00:08, 44.09it/s][A
 41%|████      | 252/611 [00:05<00:08, 44.00it/s][A
 42%|████▏     | 257/611 [00:05<00:08, 43.93it/s][A
 43%|████▎     | 262/611 [00:05<00:07, 44.09it/s][A
 44%|████▎     | 267/611 [00:06<00:07, 44.19it/s][A
 45%|████▍     | 272/611 [00:06<00:07, 44.23it/s][A
 45%|████▌     | 277/611 [00:06<00:07, 44.22it/s][A
 46%|████▌     | 282/611 [00:06<00:07, 44.27it/s][A
 47%|████▋     | 287/611 [00:06<00:07, 44.21it/s][A
 48%|████▊     | 292/611 [00:06<00:07, 44.15it/s][A
 49%|████▊     | 297/611 [00:06<00:07, 44.06it/s][A
 49%|████▉     | 302/611 [00:06<00:07, 44.01it/s][A
 50%|█████     | 307/611 [00:06<00:06, 44.09it/s][A
 51%|█████     | 312/611 [00:07<00:06, 44.21it/s][A
 52%|█████▏    | 317/611 [00:07<00:06, 44.20it/s][A
 53%|█████▎    | 322/611 [00:07<00:06, 44.12it/s][A
 54%|█████▎    | 327/611 [00:07<00:06, 44.22it/s][A
 54%|█████▍    | 332/611 [00:07<00:06, 44.18it/s][A
 55%|█████▌    | 337/611 [00:07<00:06, 44.05it/s][A
 56%|█████▌    | 342/611 [00:07<00:06, 44.04it/s][A
 57%|█████▋    | 347/611 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 352/611 [00:07<00:05, 44.20it/s][A
 58%|█████▊    | 357/611 [00:08<00:05, 44.19it/s][A
 59%|█████▉    | 362/611 [00:08<00:05, 44.14it/s][A
 60%|██████    | 367/611 [00:08<00:05, 44.23it/s][A
 61%|██████    | 372/611 [00:08<00:05, 44.17it/s][A
 62%|██████▏   | 377/611 [00:08<00:05, 44.12it/s][A
 63%|██████▎   | 382/611 [00:08<00:05, 43.93it/s][A
 63%|██████▎   | 387/611 [00:08<00:05, 44.03it/s][A
 64%|██████▍   | 392/611 [00:08<00:04, 44.15it/s][A
 65%|██████▍   | 397/611 [00:08<00:04, 44.22it/s][A
 66%|██████▌   | 402/611 [00:09<00:04, 44.23it/s][A
 67%|██████▋   | 407/611 [00:09<00:04, 44.27it/s][A
 67%|██████▋   | 412/611 [00:09<00:04, 44.22it/s][A
 68%|██████▊   | 417/611 [00:09<00:04, 44.16it/s][A
 69%|██████▉   | 422/611 [00:09<00:04, 44.12it/s][A
 70%|██████▉   | 427/611 [00:09<00:04, 44.07it/s][A
 71%|███████   | 432/611 [00:09<00:04, 44.09it/s][A
 72%|███████▏  | 437/611 [00:09<00:03, 44.15it/s][A
 72%|███████▏  | 442/611 [00:09<00:03, 44.20it/s][A
 73%|███████▎  | 447/611 [00:10<00:03, 44.25it/s][A
 74%|███████▍  | 452/611 [00:10<00:03, 44.28it/s][A
 75%|███████▍  | 457/611 [00:10<00:03, 44.18it/s][A
 76%|███████▌  | 462/611 [00:10<00:03, 44.06it/s][A
 76%|███████▋  | 467/611 [00:10<00:03, 44.11it/s][A
 77%|███████▋  | 472/611 [00:10<00:03, 44.13it/s][A
 78%|███████▊  | 477/611 [00:10<00:03, 44.11it/s][A
 79%|███████▉  | 482/611 [00:10<00:02, 44.17it/s][A
 80%|███████▉  | 487/611 [00:11<00:02, 44.14it/s][A
 81%|████████  | 492/611 [00:11<00:02, 44.27it/s][A
 81%|████████▏ | 497/611 [00:11<00:02, 44.22it/s][A
 82%|████████▏ | 502/611 [00:11<00:02, 44.10it/s][A
 83%|████████▎ | 507/611 [00:11<00:02, 44.11it/s][A
 84%|████████▍ | 512/611 [00:11<00:02, 44.21it/s][A
 85%|████████▍ | 517/611 [00:11<00:02, 44.07it/s][A
 85%|████████▌ | 522/611 [00:11<00:02, 44.09it/s][A
 86%|████████▋ | 527/611 [00:11<00:01, 44.12it/s][A
 87%|████████▋ | 532/611 [00:12<00:01, 44.23it/s][A
 88%|████████▊ | 537/611 [00:12<00:01, 44.25it/s][A
 89%|████████▊ | 542/611 [00:12<00:01, 44.16it/s][A
 90%|████████▉ | 547/611 [00:12<00:01, 44.07it/s][A
 90%|█████████ | 552/611 [00:12<00:01, 44.08it/s][A
 91%|█████████ | 557/611 [00:12<00:01, 44.23it/s][A
 92%|█████████▏| 562/611 [00:12<00:01, 44.16it/s][A
 93%|█████████▎| 567/611 [00:12<00:00, 44.10it/s][A
 94%|█████████▎| 572/611 [00:12<00:00, 44.17it/s][A
 94%|█████████▍| 577/611 [00:13<00:00, 44.24it/s][A
 95%|█████████▌| 582/611 [00:13<00:00, 44.20it/s][A
 96%|█████████▌| 587/611 [00:13<00:00, 44.23it/s][A
 97%|█████████▋| 592/611 [00:13<00:00, 44.12it/s][A
 98%|█████████▊| 597/611 [00:13<00:00, 44.17it/s][A
 99%|█████████▊| 602/611 [00:13<00:00, 44.07it/s][A
 99%|█████████▉| 607/611 [00:13<00:00, 44.11it/s][A
                                                 [A                                                 
100%|██████████| 611/611 [00:13<00:00, 44.11it/s][A100%|██████████| 585/585 [04:26<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:57:53,133 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 00:57:53,155 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:57:54,803 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:57:54,818 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:57:54,831 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:57:58,929 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:57:58,934 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117 (score: 0.9694299101829529).
                                                 100%|██████████| 585/585 [04:34<00:00,  3.42it/s]100%|██████████| 585/585 [04:34<00:00,  2.13it/s]
[INFO|trainer.py:1894] 2023-08-29 00:58:00,682 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 00:58:00,705 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:58:02,508 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:58:02,529 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:58:02,540 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:58:02,748 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:02,748 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:02,748 >>   train_loss               =     0.3779
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:02,748 >>   train_runtime            = 0:04:34.45
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:02,748 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:02,748 >>   train_samples_per_second =    136.633
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:02,748 >>   train_steps_per_second   =      2.131
{'eval_loss': 1.0176044702529907, 'eval_runtime': 13.8325, 'eval_samples_per_second': 352.937, 'eval_steps_per_second': 44.171, 'epoch': 5.0}
{'train_runtime': 274.4584, 'train_samples_per_second': 136.633, 'train_steps_per_second': 2.131, 'train_loss': 0.37794854905870223, 'epoch': 5.0}
08/29/2023 00:58:02 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:58:02,792 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:58:02,793 >>   Num examples = 4882
[INFO|trainer.py:2145] 2023-08-29 00:58:02,793 >>   Batch size = 8
  0%|          | 0/611 [00:00<?, ?it/s]  1%|          | 6/611 [00:00<00:10, 55.64it/s]  2%|▏         | 12/611 [00:00<00:12, 48.68it/s]  3%|▎         | 17/611 [00:00<00:12, 47.00it/s]  4%|▎         | 22/611 [00:00<00:12, 46.22it/s]  4%|▍         | 27/611 [00:00<00:12, 45.74it/s]  5%|▌         | 32/611 [00:00<00:12, 45.23it/s]  6%|▌         | 37/611 [00:00<00:12, 45.04it/s]  7%|▋         | 42/611 [00:00<00:12, 44.52it/s]  8%|▊         | 47/611 [00:01<00:12, 44.07it/s]  9%|▊         | 52/611 [00:01<00:12, 43.91it/s]  9%|▉         | 57/611 [00:01<00:12, 44.04it/s] 10%|█         | 62/611 [00:01<00:12, 44.20it/s] 11%|█         | 67/611 [00:01<00:12, 44.38it/s] 12%|█▏        | 72/611 [00:01<00:12, 44.49it/s] 13%|█▎        | 77/611 [00:01<00:12, 44.43it/s] 13%|█▎        | 82/611 [00:01<00:11, 44.33it/s] 14%|█▍        | 87/611 [00:01<00:11, 44.06it/s] 15%|█▌        | 92/611 [00:02<00:11, 43.90it/s] 16%|█▌        | 97/611 [00:02<00:11, 43.85it/s] 17%|█▋        | 102/611 [00:02<00:11, 43.92it/s] 18%|█▊        | 107/611 [00:02<00:11, 44.05it/s] 18%|█▊        | 112/611 [00:02<00:11, 44.36it/s] 19%|█▉        | 117/611 [00:02<00:11, 44.42it/s] 20%|█▉        | 122/611 [00:02<00:11, 44.35it/s] 21%|██        | 127/611 [00:02<00:10, 44.20it/s] 22%|██▏       | 132/611 [00:02<00:10, 43.92it/s] 22%|██▏       | 137/611 [00:03<00:10, 43.89it/s] 23%|██▎       | 142/611 [00:03<00:10, 43.80it/s] 24%|██▍       | 147/611 [00:03<00:10, 43.98it/s] 25%|██▍       | 152/611 [00:03<00:10, 44.19it/s] 26%|██▌       | 157/611 [00:03<00:10, 44.37it/s] 27%|██▋       | 162/611 [00:03<00:10, 44.38it/s] 27%|██▋       | 167/611 [00:03<00:10, 44.27it/s] 28%|██▊       | 172/611 [00:03<00:09, 44.14it/s] 29%|██▉       | 177/611 [00:03<00:09, 44.04it/s] 30%|██▉       | 182/611 [00:04<00:09, 43.92it/s] 31%|███       | 187/611 [00:04<00:09, 43.86it/s] 31%|███▏      | 192/611 [00:04<00:09, 43.86it/s] 32%|███▏      | 197/611 [00:04<00:09, 44.16it/s] 33%|███▎      | 202/611 [00:04<00:09, 44.37it/s] 34%|███▍      | 207/611 [00:04<00:09, 44.33it/s] 35%|███▍      | 212/611 [00:04<00:09, 44.26it/s] 36%|███▌      | 217/611 [00:04<00:08, 44.10it/s] 36%|███▋      | 222/611 [00:04<00:08, 44.02it/s] 37%|███▋      | 227/611 [00:05<00:08, 43.85it/s] 38%|███▊      | 232/611 [00:05<00:08, 43.91it/s] 39%|███▉      | 237/611 [00:05<00:08, 44.05it/s] 40%|███▉      | 242/611 [00:05<00:08, 44.09it/s] 40%|████      | 247/611 [00:05<00:08, 44.24it/s] 41%|████      | 252/611 [00:05<00:08, 44.28it/s] 42%|████▏     | 257/611 [00:05<00:07, 44.29it/s] 43%|████▎     | 262/611 [00:05<00:07, 44.24it/s] 44%|████▎     | 267/611 [00:06<00:07, 44.06it/s] 45%|████▍     | 272/611 [00:06<00:07, 43.98it/s] 45%|████▌     | 277/611 [00:06<00:07, 43.96it/s] 46%|████▌     | 282/611 [00:06<00:07, 44.05it/s] 47%|████▋     | 287/611 [00:06<00:07, 44.08it/s] 48%|████▊     | 292/611 [00:06<00:07, 44.21it/s] 49%|████▊     | 297/611 [00:06<00:07, 44.26it/s] 49%|████▉     | 302/611 [00:06<00:06, 44.35it/s] 50%|█████     | 307/611 [00:06<00:06, 44.31it/s] 51%|█████     | 312/611 [00:07<00:06, 44.22it/s] 52%|█████▏    | 317/611 [00:07<00:06, 44.18it/s] 53%|█████▎    | 322/611 [00:07<00:06, 44.15it/s] 54%|█████▎    | 327/611 [00:07<00:06, 44.14it/s] 54%|█████▍    | 332/611 [00:07<00:06, 44.34it/s] 55%|█████▌    | 337/611 [00:07<00:06, 44.48it/s] 56%|█████▌    | 342/611 [00:07<00:06, 44.43it/s] 57%|█████▋    | 347/611 [00:07<00:05, 44.42it/s] 58%|█████▊    | 352/611 [00:07<00:05, 44.24it/s] 58%|█████▊    | 357/611 [00:08<00:05, 44.23it/s] 59%|█████▉    | 362/611 [00:08<00:05, 44.22it/s] 60%|██████    | 367/611 [00:08<00:05, 44.17it/s] 61%|██████    | 372/611 [00:08<00:05, 44.17it/s] 62%|██████▏   | 377/611 [00:08<00:05, 44.21it/s] 63%|██████▎   | 382/611 [00:08<00:05, 44.38it/s] 63%|██████▎   | 387/611 [00:08<00:05, 44.37it/s] 64%|██████▍   | 392/611 [00:08<00:04, 44.35it/s] 65%|██████▍   | 397/611 [00:08<00:04, 44.32it/s] 66%|██████▌   | 402/611 [00:09<00:04, 44.36it/s] 67%|██████▋   | 407/611 [00:09<00:04, 44.29it/s] 67%|██████▋   | 412/611 [00:09<00:04, 44.12it/s] 68%|██████▊   | 417/611 [00:09<00:04, 44.25it/s] 69%|██████▉   | 422/611 [00:09<00:04, 44.38it/s] 70%|██████▉   | 427/611 [00:09<00:04, 44.31it/s] 71%|███████   | 432/611 [00:09<00:04, 44.45it/s] 72%|███████▏  | 437/611 [00:09<00:03, 44.44it/s] 72%|███████▏  | 442/611 [00:09<00:03, 44.38it/s] 73%|███████▎  | 447/611 [00:10<00:03, 44.35it/s] 74%|███████▍  | 452/611 [00:10<00:03, 44.24it/s] 75%|███████▍  | 457/611 [00:10<00:03, 44.10it/s] 76%|███████▌  | 462/611 [00:10<00:03, 44.13it/s] 76%|███████▋  | 467/611 [00:10<00:03, 44.36it/s] 77%|███████▋  | 472/611 [00:10<00:03, 44.38it/s] 78%|███████▊  | 477/611 [00:10<00:03, 44.30it/s] 79%|███████▉  | 482/611 [00:10<00:02, 44.36it/s] 80%|███████▉  | 487/611 [00:10<00:02, 44.41it/s] 81%|████████  | 492/611 [00:11<00:02, 44.30it/s] 81%|████████▏ | 497/611 [00:11<00:02, 44.18it/s] 82%|████████▏ | 502/611 [00:11<00:02, 44.27it/s] 83%|████████▎ | 507/611 [00:11<00:02, 44.29it/s] 84%|████████▍ | 512/611 [00:11<00:02, 44.41it/s] 85%|████████▍ | 517/611 [00:11<00:02, 44.39it/s] 85%|████████▌ | 522/611 [00:11<00:02, 44.38it/s] 86%|████████▋ | 527/611 [00:11<00:01, 44.31it/s] 87%|████████▋ | 532/611 [00:12<00:01, 44.30it/s] 88%|████████▊ | 537/611 [00:12<00:01, 44.18it/s] 89%|████████▊ | 542/611 [00:12<00:01, 44.29it/s] 90%|████████▉ | 547/611 [00:12<00:01, 44.22it/s] 90%|█████████ | 552/611 [00:12<00:01, 44.10it/s] 91%|█████████ | 557/611 [00:12<00:01, 44.34it/s] 92%|█████████▏| 562/611 [00:12<00:01, 44.32it/s] 93%|█████████▎| 567/611 [00:12<00:00, 44.35it/s] 94%|█████████▎| 572/611 [00:12<00:00, 44.37it/s] 94%|█████████▍| 577/611 [00:13<00:00, 44.30it/s] 95%|█████████▌| 582/611 [00:13<00:00, 44.12it/s] 96%|█████████▌| 587/611 [00:13<00:00, 44.23it/s] 97%|█████████▋| 592/611 [00:13<00:00, 44.27it/s] 98%|█████████▊| 597/611 [00:13<00:00, 44.12it/s] 99%|█████████▊| 602/611 [00:13<00:00, 44.31it/s] 99%|█████████▉| 607/611 [00:13<00:00, 44.34it/s]100%|██████████| 611/611 [00:13<00:00, 44.32it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:58:16,599 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:16,599 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:16,599 >>   eval_loss               =     0.9694
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:16,599 >>   eval_runtime            = 0:00:13.80
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:16,599 >>   eval_samples            =       4882
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:16,599 >>   eval_samples_per_second =     353.61
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:16,599 >>   eval_steps_per_second   =     44.256
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:58:16,599 >>   perplexity              =     2.6364
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:22,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:22,914 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:22,914 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:22,914 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:22,914 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:58:23,295 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:58:23,295 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:58:23,564 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:58:24,644 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:58:24,644 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:27,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:27,507 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:27,507 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:27,507 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:27,507 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:58:28,139 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:58:28,140 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:58:28,790 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:58:28,969 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:58:28,969 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.65it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:11,  1.67it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:12,  1.71it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:13,  1.76it/s]Extractor Predicting: 23it [00:13,  1.76it/s]Extractor Predicting: 24it [00:14,  1.76it/s]Extractor Predicting: 25it [00:15,  1.72it/s]Extractor Predicting: 26it [00:15,  1.70it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.60it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:18,  1.68it/s]Extractor Predicting: 32it [00:19,  1.72it/s]Extractor Predicting: 33it [00:19,  1.71it/s]Extractor Predicting: 34it [00:20,  1.69it/s]Extractor Predicting: 35it [00:21,  1.69it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.70it/s]Extractor Predicting: 38it [00:22,  1.70it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:23,  1.71it/s]Extractor Predicting: 41it [00:24,  1.67it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:25,  1.68it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:26,  1.71it/s]Extractor Predicting: 46it [00:27,  1.70it/s]Extractor Predicting: 47it [00:28,  1.69it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:29,  1.64it/s]Extractor Predicting: 51it [00:30,  1.65it/s]Extractor Predicting: 52it [00:31,  1.69it/s]Extractor Predicting: 53it [00:31,  1.67it/s]Extractor Predicting: 54it [00:32,  1.66it/s]Extractor Predicting: 55it [00:33,  1.60it/s]Extractor Predicting: 56it [00:33,  1.57it/s]Extractor Predicting: 57it [00:34,  1.60it/s]Extractor Predicting: 58it [00:34,  1.59it/s]Extractor Predicting: 59it [00:35,  1.62it/s]Extractor Predicting: 60it [00:36,  1.61it/s]Extractor Predicting: 61it [00:36,  1.59it/s]Extractor Predicting: 62it [00:37,  1.57it/s]Extractor Predicting: 63it [00:38,  1.58it/s]Extractor Predicting: 64it [00:38,  1.59it/s]Extractor Predicting: 65it [00:39,  1.58it/s]Extractor Predicting: 66it [00:40,  1.55it/s]Extractor Predicting: 67it [00:40,  1.57it/s]Extractor Predicting: 68it [00:41,  1.61it/s]Extractor Predicting: 69it [00:41,  1.60it/s]Extractor Predicting: 70it [00:42,  1.57it/s]Extractor Predicting: 71it [00:43,  1.58it/s]Extractor Predicting: 72it [00:43,  1.55it/s]Extractor Predicting: 73it [00:44,  1.58it/s]Extractor Predicting: 74it [00:45,  1.58it/s]Extractor Predicting: 75it [00:45,  1.59it/s]Extractor Predicting: 76it [00:46,  1.60it/s]Extractor Predicting: 77it [00:46,  1.58it/s]Extractor Predicting: 78it [00:47,  1.62it/s]Extractor Predicting: 79it [00:48,  1.63it/s]Extractor Predicting: 80it [00:48,  1.62it/s]Extractor Predicting: 81it [00:49,  1.62it/s]Extractor Predicting: 82it [00:50,  1.59it/s]Extractor Predicting: 83it [00:50,  1.60it/s]Extractor Predicting: 84it [00:51,  1.60it/s]Extractor Predicting: 85it [00:51,  1.58it/s]Extractor Predicting: 86it [00:52,  1.57it/s]Extractor Predicting: 87it [00:53,  1.57it/s]Extractor Predicting: 88it [00:53,  1.54it/s]Extractor Predicting: 89it [00:54,  1.54it/s]Extractor Predicting: 90it [00:55,  1.54it/s]Extractor Predicting: 91it [00:55,  1.54it/s]Extractor Predicting: 92it [00:56,  1.60it/s]Extractor Predicting: 93it [00:56,  1.67it/s]Extractor Predicting: 94it [00:57,  1.66it/s]Extractor Predicting: 95it [00:58,  1.66it/s]Extractor Predicting: 96it [00:58,  1.66it/s]Extractor Predicting: 97it [00:59,  1.68it/s]Extractor Predicting: 98it [00:59,  1.64it/s]Extractor Predicting: 99it [01:00,  1.57it/s]Extractor Predicting: 100it [01:01,  1.61it/s]Extractor Predicting: 101it [01:01,  1.53it/s]Extractor Predicting: 102it [01:02,  1.52it/s]Extractor Predicting: 103it [01:03,  1.54it/s]Extractor Predicting: 104it [01:03,  1.57it/s]Extractor Predicting: 105it [01:04,  1.58it/s]Extractor Predicting: 106it [01:05,  1.64it/s]Extractor Predicting: 107it [01:05,  1.63it/s]Extractor Predicting: 108it [01:06,  1.66it/s]Extractor Predicting: 109it [01:06,  1.64it/s]Extractor Predicting: 110it [01:07,  1.64it/s]Extractor Predicting: 111it [01:08,  1.67it/s]Extractor Predicting: 112it [01:08,  1.67it/s]Extractor Predicting: 113it [01:09,  1.62it/s]Extractor Predicting: 114it [01:09,  1.61it/s]Extractor Predicting: 115it [01:10,  1.62it/s]Extractor Predicting: 116it [01:11,  1.58it/s]Extractor Predicting: 117it [01:11,  1.56it/s]Extractor Predicting: 118it [01:12,  1.57it/s]Extractor Predicting: 119it [01:13,  1.40it/s]Extractor Predicting: 120it [01:14,  1.42it/s]Extractor Predicting: 121it [01:14,  1.45it/s]Extractor Predicting: 122it [01:15,  1.48it/s]Extractor Predicting: 123it [01:16,  1.52it/s]Extractor Predicting: 124it [01:16,  1.54it/s]Extractor Predicting: 125it [01:17,  1.55it/s]Extractor Predicting: 126it [01:17,  1.56it/s]Extractor Predicting: 127it [01:18,  1.55it/s]Extractor Predicting: 128it [01:19,  1.56it/s]Extractor Predicting: 129it [01:19,  1.58it/s]Extractor Predicting: 130it [01:20,  1.54it/s]Extractor Predicting: 131it [01:21,  1.55it/s]Extractor Predicting: 132it [01:21,  1.57it/s]Extractor Predicting: 133it [01:22,  1.55it/s]Extractor Predicting: 134it [01:23,  1.56it/s]Extractor Predicting: 135it [01:23,  1.54it/s]Extractor Predicting: 136it [01:24,  1.56it/s]Extractor Predicting: 137it [01:25,  1.53it/s]Extractor Predicting: 138it [01:25,  1.55it/s]Extractor Predicting: 139it [01:26,  1.53it/s]Extractor Predicting: 140it [01:26,  1.52it/s]Extractor Predicting: 141it [01:27,  1.55it/s]Extractor Predicting: 142it [01:28,  1.56it/s]Extractor Predicting: 143it [01:28,  1.54it/s]Extractor Predicting: 144it [01:29,  1.58it/s]Extractor Predicting: 145it [01:30,  1.62it/s]Extractor Predicting: 146it [01:30,  1.60it/s]Extractor Predicting: 147it [01:31,  1.56it/s]Extractor Predicting: 148it [01:32,  1.58it/s]Extractor Predicting: 149it [01:32,  1.57it/s]Extractor Predicting: 150it [01:33,  1.55it/s]Extractor Predicting: 151it [01:33,  1.54it/s]Extractor Predicting: 152it [01:34,  1.54it/s]Extractor Predicting: 153it [01:35,  1.54it/s]Extractor Predicting: 154it [01:35,  1.55it/s]Extractor Predicting: 155it [01:36,  1.55it/s]Extractor Predicting: 156it [01:37,  1.49it/s]Extractor Predicting: 157it [01:38,  1.46it/s]Extractor Predicting: 158it [01:38,  1.43it/s]Extractor Predicting: 159it [01:39,  1.46it/s]Extractor Predicting: 160it [01:40,  1.49it/s]Extractor Predicting: 161it [01:40,  1.51it/s]Extractor Predicting: 162it [01:41,  1.52it/s]Extractor Predicting: 163it [01:41,  1.52it/s]Extractor Predicting: 164it [01:42,  1.55it/s]Extractor Predicting: 165it [01:43,  1.53it/s]Extractor Predicting: 166it [01:43,  1.57it/s]Extractor Predicting: 167it [01:44,  1.56it/s]Extractor Predicting: 168it [01:45,  1.56it/s]Extractor Predicting: 169it [01:45,  1.57it/s]Extractor Predicting: 170it [01:46,  1.57it/s]Extractor Predicting: 171it [01:47,  1.59it/s]Extractor Predicting: 172it [01:47,  1.62it/s]Extractor Predicting: 173it [01:48,  1.56it/s]Extractor Predicting: 174it [01:48,  1.58it/s]Extractor Predicting: 175it [01:49,  1.55it/s]Extractor Predicting: 176it [01:50,  1.57it/s]Extractor Predicting: 177it [01:50,  1.55it/s]Extractor Predicting: 178it [01:51,  1.55it/s]Extractor Predicting: 179it [01:52,  1.55it/s]Extractor Predicting: 180it [01:52,  1.53it/s]Extractor Predicting: 181it [01:53,  1.54it/s]Extractor Predicting: 182it [01:54,  1.54it/s]Extractor Predicting: 183it [01:54,  1.50it/s]Extractor Predicting: 184it [01:55,  1.53it/s]Extractor Predicting: 185it [01:56,  1.61it/s]Extractor Predicting: 185it [01:56,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:34,030 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:34,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:34,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:34,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:34,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:00:34,652 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:00:34,653 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:00:35,215 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:00:36,277 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:00:36,277 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:39,147 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:39,150 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:39,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:39,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:00:39,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:00:39,796 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:00:39,797 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:00:40,366 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:00:40,523 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:00:40,523 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.26211671612265086,
  "recall": 0.10856206472757067,
  "score": 0.15353418308227115,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.60it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.69it/s]Extractor Predicting: 31it [00:18,  1.74it/s]Extractor Predicting: 32it [00:19,  1.74it/s]Extractor Predicting: 33it [00:20,  1.73it/s]Extractor Predicting: 34it [00:20,  1.73it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:23,  1.61it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.66it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.68it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:27,  1.73it/s]Extractor Predicting: 46it [00:27,  1.76it/s]Extractor Predicting: 47it [00:28,  1.76it/s]Extractor Predicting: 48it [00:29,  1.76it/s]Extractor Predicting: 49it [00:29,  1.73it/s]Extractor Predicting: 50it [00:30,  1.72it/s]Extractor Predicting: 51it [00:30,  1.72it/s]Extractor Predicting: 52it [00:31,  1.72it/s]Extractor Predicting: 53it [00:31,  1.71it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.66it/s]Extractor Predicting: 56it [00:33,  1.66it/s]Extractor Predicting: 57it [00:34,  1.68it/s]Extractor Predicting: 58it [00:34,  1.75it/s]Extractor Predicting: 59it [00:35,  1.72it/s]Extractor Predicting: 60it [00:36,  1.69it/s]Extractor Predicting: 61it [00:36,  1.64it/s]Extractor Predicting: 62it [00:37,  1.61it/s]Extractor Predicting: 63it [00:38,  1.56it/s]Extractor Predicting: 64it [00:38,  1.54it/s]Extractor Predicting: 65it [00:39,  1.52it/s]Extractor Predicting: 66it [00:40,  1.51it/s]Extractor Predicting: 67it [00:40,  1.50it/s]Extractor Predicting: 68it [00:41,  1.52it/s]Extractor Predicting: 69it [00:42,  1.52it/s]Extractor Predicting: 70it [00:42,  1.55it/s]Extractor Predicting: 71it [00:43,  1.56it/s]Extractor Predicting: 72it [00:43,  1.58it/s]Extractor Predicting: 73it [00:44,  1.63it/s]Extractor Predicting: 74it [00:45,  1.63it/s]Extractor Predicting: 75it [00:45,  1.64it/s]Extractor Predicting: 76it [00:46,  1.65it/s]Extractor Predicting: 77it [00:46,  1.67it/s]Extractor Predicting: 78it [00:47,  1.66it/s]Extractor Predicting: 79it [00:48,  1.71it/s]Extractor Predicting: 80it [00:48,  1.75it/s]Extractor Predicting: 81it [00:49,  1.70it/s]Extractor Predicting: 82it [00:49,  1.71it/s]Extractor Predicting: 83it [00:50,  1.67it/s]Extractor Predicting: 84it [00:51,  1.65it/s]Extractor Predicting: 85it [00:51,  1.61it/s]Extractor Predicting: 86it [00:52,  1.59it/s]Extractor Predicting: 87it [00:53,  1.58it/s]Extractor Predicting: 88it [00:53,  1.61it/s]Extractor Predicting: 89it [00:54,  1.60it/s]Extractor Predicting: 90it [00:54,  1.61it/s]Extractor Predicting: 91it [00:55,  1.59it/s]Extractor Predicting: 92it [00:56,  1.58it/s]Extractor Predicting: 93it [00:56,  1.61it/s]Extractor Predicting: 94it [00:57,  1.62it/s]Extractor Predicting: 95it [00:57,  1.61it/s]Extractor Predicting: 96it [00:58,  1.60it/s]Extractor Predicting: 97it [00:59,  1.61it/s]Extractor Predicting: 98it [00:59,  1.61it/s]Extractor Predicting: 99it [01:00,  1.61it/s]Extractor Predicting: 100it [01:01,  1.59it/s]Extractor Predicting: 101it [01:01,  1.62it/s]Extractor Predicting: 102it [01:02,  1.62it/s]Extractor Predicting: 103it [01:02,  1.59it/s]Extractor Predicting: 104it [01:03,  1.62it/s]Extractor Predicting: 105it [01:04,  1.64it/s]Extractor Predicting: 106it [01:04,  1.65it/s]Extractor Predicting: 107it [01:05,  1.63it/s]Extractor Predicting: 108it [01:05,  1.66it/s]Extractor Predicting: 109it [01:06,  1.64it/s]Extractor Predicting: 110it [01:07,  1.66it/s]Extractor Predicting: 111it [01:07,  1.65it/s]Extractor Predicting: 112it [01:08,  1.63it/s]Extractor Predicting: 113it [01:09,  1.61it/s]Extractor Predicting: 114it [01:09,  1.61it/s]Extractor Predicting: 115it [01:10,  1.60it/s]Extractor Predicting: 116it [01:10,  1.61it/s]Extractor Predicting: 117it [01:11,  1.64it/s]Extractor Predicting: 118it [01:12,  1.62it/s]Extractor Predicting: 119it [01:12,  1.61it/s]Extractor Predicting: 120it [01:13,  1.64it/s]Extractor Predicting: 121it [01:13,  1.67it/s]Extractor Predicting: 122it [01:14,  1.66it/s]Extractor Predicting: 123it [01:15,  1.62it/s]Extractor Predicting: 124it [01:15,  1.60it/s]Extractor Predicting: 125it [01:16,  1.61it/s]Extractor Predicting: 126it [01:17,  1.60it/s]Extractor Predicting: 127it [01:17,  1.63it/s]Extractor Predicting: 128it [01:18,  1.58it/s]Extractor Predicting: 129it [01:18,  1.60it/s]Extractor Predicting: 130it [01:19,  1.63it/s]Extractor Predicting: 131it [01:20,  1.61it/s]Extractor Predicting: 132it [01:20,  1.60it/s]Extractor Predicting: 133it [01:21,  1.60it/s]Extractor Predicting: 134it [01:22,  1.59it/s]Extractor Predicting: 135it [01:22,  1.61it/s]Extractor Predicting: 136it [01:23,  1.64it/s]Extractor Predicting: 137it [01:23,  1.59it/s]Extractor Predicting: 138it [01:24,  1.60it/s]Extractor Predicting: 139it [01:25,  1.62it/s]Extractor Predicting: 140it [01:25,  1.63it/s]Extractor Predicting: 141it [01:26,  1.61it/s]Extractor Predicting: 142it [01:27,  1.63it/s]Extractor Predicting: 143it [01:27,  1.52it/s]Extractor Predicting: 144it [01:28,  1.42it/s]Extractor Predicting: 145it [01:29,  1.44it/s]Extractor Predicting: 146it [01:29,  1.50it/s]Extractor Predicting: 147it [01:30,  1.57it/s]Extractor Predicting: 148it [01:31,  1.57it/s]Extractor Predicting: 149it [01:31,  1.62it/s]Extractor Predicting: 150it [01:32,  1.64it/s]Extractor Predicting: 151it [01:32,  1.67it/s]Extractor Predicting: 152it [01:33,  1.63it/s]Extractor Predicting: 153it [01:34,  1.63it/s]Extractor Predicting: 154it [01:34,  1.59it/s]Extractor Predicting: 155it [01:35,  1.59it/s]Extractor Predicting: 156it [01:35,  1.64it/s]Extractor Predicting: 157it [01:36,  1.59it/s]Extractor Predicting: 158it [01:37,  1.58it/s]Extractor Predicting: 159it [01:37,  1.60it/s]Extractor Predicting: 160it [01:38,  1.61it/s]Extractor Predicting: 161it [01:39,  1.64it/s]Extractor Predicting: 162it [01:39,  1.62it/s]Extractor Predicting: 163it [01:40,  1.62it/s]Extractor Predicting: 164it [01:40,  1.60it/s]Extractor Predicting: 165it [01:41,  1.56it/s]Extractor Predicting: 166it [01:42,  1.54it/s]Extractor Predicting: 167it [01:42,  1.54it/s]Extractor Predicting: 168it [01:43,  1.56it/s]Extractor Predicting: 169it [01:44,  1.59it/s]Extractor Predicting: 170it [01:44,  1.61it/s]Extractor Predicting: 171it [01:45,  1.62it/s]Extractor Predicting: 172it [01:45,  1.65it/s]Extractor Predicting: 173it [01:46,  1.65it/s]Extractor Predicting: 174it [01:47,  1.64it/s]Extractor Predicting: 175it [01:47,  1.63it/s]Extractor Predicting: 176it [01:48,  1.64it/s]Extractor Predicting: 177it [01:49,  1.62it/s]Extractor Predicting: 178it [01:49,  1.61it/s]Extractor Predicting: 179it [01:50,  1.58it/s]Extractor Predicting: 180it [01:50,  1.62it/s]Extractor Predicting: 181it [01:51,  1.61it/s]Extractor Predicting: 182it [01:52,  1.64it/s]Extractor Predicting: 183it [01:52,  1.66it/s]Extractor Predicting: 184it [01:53,  1.65it/s]Extractor Predicting: 185it [01:53,  1.65it/s]Extractor Predicting: 186it [01:54,  1.60it/s]Extractor Predicting: 187it [01:55,  1.62it/s]Extractor Predicting: 188it [01:55,  1.62it/s]Extractor Predicting: 189it [01:56,  1.62it/s]Extractor Predicting: 190it [01:56,  1.64it/s]Extractor Predicting: 191it [01:57,  1.65it/s]Extractor Predicting: 192it [01:58,  1.69it/s]Extractor Predicting: 193it [01:58,  1.67it/s]Extractor Predicting: 194it [01:59,  1.67it/s]Extractor Predicting: 195it [01:59,  1.65it/s]Extractor Predicting: 196it [02:00,  1.65it/s]Extractor Predicting: 197it [02:01,  1.64it/s]Extractor Predicting: 198it [02:01,  1.62it/s]Extractor Predicting: 199it [02:02,  1.62it/s]Extractor Predicting: 200it [02:03,  1.62it/s]Extractor Predicting: 201it [02:03,  1.64it/s]Extractor Predicting: 202it [02:04,  1.64it/s]Extractor Predicting: 203it [02:04,  1.66it/s]Extractor Predicting: 204it [02:05,  1.65it/s]Extractor Predicting: 205it [02:06,  1.66it/s]Extractor Predicting: 206it [02:06,  1.64it/s]Extractor Predicting: 207it [02:07,  1.66it/s]Extractor Predicting: 208it [02:07,  1.63it/s]Extractor Predicting: 209it [02:08,  1.61it/s]Extractor Predicting: 210it [02:09,  1.67it/s]Extractor Predicting: 211it [02:09,  1.65it/s]Extractor Predicting: 212it [02:10,  1.66it/s]Extractor Predicting: 213it [02:10,  1.66it/s]Extractor Predicting: 214it [02:11,  1.69it/s]Extractor Predicting: 215it [02:12,  1.67it/s]Extractor Predicting: 216it [02:12,  1.64it/s]Extractor Predicting: 217it [02:13,  1.66it/s]Extractor Predicting: 218it [02:13,  1.64it/s]Extractor Predicting: 219it [02:14,  1.63it/s]Extractor Predicting: 220it [02:15,  1.65it/s]Extractor Predicting: 221it [02:15,  1.66it/s]Extractor Predicting: 222it [02:16,  1.60it/s]Extractor Predicting: 223it [02:17,  1.55it/s]Extractor Predicting: 224it [02:17,  1.58it/s]Extractor Predicting: 225it [02:18,  1.60it/s]Extractor Predicting: 226it [02:18,  1.64it/s]Extractor Predicting: 227it [02:19,  1.64it/s]Extractor Predicting: 228it [02:20,  1.66it/s]Extractor Predicting: 229it [02:20,  1.69it/s]Extractor Predicting: 230it [02:21,  1.70it/s]Extractor Predicting: 231it [02:21,  1.71it/s]Extractor Predicting: 232it [02:22,  1.69it/s]Extractor Predicting: 233it [02:23,  1.68it/s]Extractor Predicting: 234it [02:23,  1.68it/s]Extractor Predicting: 235it [02:24,  1.65it/s]Extractor Predicting: 236it [02:24,  1.65it/s]Extractor Predicting: 237it [02:25,  1.65it/s]Extractor Predicting: 238it [02:26,  1.62it/s]Extractor Predicting: 239it [02:26,  1.64it/s]Extractor Predicting: 240it [02:27,  1.63it/s]Extractor Predicting: 241it [02:27,  1.65it/s]Extractor Predicting: 242it [02:28,  1.62it/s]Extractor Predicting: 243it [02:29,  1.59it/s]Extractor Predicting: 244it [02:29,  1.60it/s]Extractor Predicting: 245it [02:30,  1.63it/s]Extractor Predicting: 246it [02:31,  1.62it/s]Extractor Predicting: 247it [02:31,  1.66it/s]Extractor Predicting: 248it [02:32,  1.59it/s]Extractor Predicting: 249it [02:32,  1.59it/s]Extractor Predicting: 250it [02:33,  1.61it/s]Extractor Predicting: 251it [02:34,  1.60it/s]Extractor Predicting: 252it [02:34,  1.59it/s]Extractor Predicting: 253it [02:35,  1.56it/s]Extractor Predicting: 254it [02:36,  1.54it/s]Extractor Predicting: 255it [02:36,  1.56it/s]Extractor Predicting: 256it [02:37,  1.40it/s]Extractor Predicting: 257it [02:38,  1.46it/s]Extractor Predicting: 258it [02:38,  1.49it/s]Extractor Predicting: 259it [02:39,  1.51it/s]Extractor Predicting: 260it [02:40,  1.50it/s]Extractor Predicting: 261it [02:40,  1.54it/s]Extractor Predicting: 262it [02:41,  1.54it/s]Extractor Predicting: 263it [02:42,  1.55it/s]Extractor Predicting: 264it [02:42,  1.56it/s]Extractor Predicting: 265it [02:43,  1.59it/s]Extractor Predicting: 266it [02:44,  1.53it/s]Extractor Predicting: 267it [02:44,  1.54it/s]Extractor Predicting: 268it [02:45,  1.52it/s]Extractor Predicting: 269it [02:46,  1.54it/s]Extractor Predicting: 270it [02:46,  1.53it/s]Extractor Predicting: 271it [02:47,  1.53it/s]Extractor Predicting: 272it [02:47,  1.53it/s]Extractor Predicting: 273it [02:48,  1.54it/s]Extractor Predicting: 274it [02:49,  1.51it/s]Extractor Predicting: 275it [02:49,  1.53it/s]Extractor Predicting: 276it [02:50,  1.54it/s]Extractor Predicting: 277it [02:51,  1.55it/s]Extractor Predicting: 278it [02:51,  1.57it/s]Extractor Predicting: 279it [02:52,  1.55it/s]Extractor Predicting: 280it [02:53,  1.56it/s]Extractor Predicting: 281it [02:53,  1.52it/s]Extractor Predicting: 282it [02:54,  1.52it/s]Extractor Predicting: 283it [02:55,  1.53it/s]Extractor Predicting: 284it [02:55,  1.57it/s]Extractor Predicting: 285it [02:56,  1.56it/s]Extractor Predicting: 286it [02:57,  1.58it/s]Extractor Predicting: 287it [02:57,  1.53it/s]Extractor Predicting: 288it [02:58,  1.57it/s]Extractor Predicting: 289it [02:58,  1.59it/s]Extractor Predicting: 290it [02:59,  1.55it/s]Extractor Predicting: 291it [03:00,  1.52it/s]Extractor Predicting: 292it [03:00,  1.56it/s]Extractor Predicting: 293it [03:01,  1.56it/s]Extractor Predicting: 294it [03:02,  1.54it/s]Extractor Predicting: 295it [03:02,  1.54it/s]Extractor Predicting: 296it [03:03,  1.58it/s]Extractor Predicting: 297it [03:04,  1.58it/s]Extractor Predicting: 298it [03:04,  1.56it/s]Extractor Predicting: 299it [03:05,  1.56it/s]Extractor Predicting: 300it [03:06,  1.55it/s]Extractor Predicting: 301it [03:06,  1.57it/s]Extractor Predicting: 302it [03:07,  1.54it/s]Extractor Predicting: 303it [03:07,  1.58it/s]Extractor Predicting: 304it [03:08,  1.59it/s]Extractor Predicting: 305it [03:09,  1.62it/s]Extractor Predicting: 306it [03:09,  1.65it/s]Extractor Predicting: 307it [03:10,  1.62it/s]Extractor Predicting: 308it [03:10,  1.63it/s]Extractor Predicting: 309it [03:11,  1.60it/s]Extractor Predicting: 310it [03:12,  1.60it/s]Extractor Predicting: 311it [03:12,  1.56it/s]Extractor Predicting: 312it [03:13,  1.58it/s]Extractor Predicting: 313it [03:14,  1.51it/s]Extractor Predicting: 314it [03:14,  1.52it/s]Extractor Predicting: 315it [03:15,  1.55it/s]Extractor Predicting: 316it [03:16,  1.59it/s]Extractor Predicting: 317it [03:16,  1.60it/s]Extractor Predicting: 318it [03:17,  1.64it/s]Extractor Predicting: 319it [03:17,  1.59it/s]Extractor Predicting: 320it [03:18,  1.56it/s]Extractor Predicting: 321it [03:19,  1.56it/s]Extractor Predicting: 322it [03:19,  1.58it/s]Extractor Predicting: 323it [03:20,  1.54it/s]Extractor Predicting: 324it [03:21,  1.55it/s]Extractor Predicting: 325it [03:21,  1.58it/s]Extractor Predicting: 326it [03:22,  1.58it/s]Extractor Predicting: 327it [03:23,  1.59it/s]Extractor Predicting: 328it [03:23,  1.56it/s]Extractor Predicting: 329it [03:24,  1.57it/s]Extractor Predicting: 330it [03:25,  1.55it/s]Extractor Predicting: 331it [03:25,  1.57it/s]Extractor Predicting: 332it [03:26,  1.56it/s]Extractor Predicting: 333it [03:26,  1.55it/s]Extractor Predicting: 334it [03:27,  1.57it/s]Extractor Predicting: 335it [03:28,  1.55it/s]Extractor Predicting: 336it [03:29,  1.48it/s]Extractor Predicting: 337it [03:29,  1.49it/s]Extractor Predicting: 338it [03:30,  1.50it/s]Extractor Predicting: 339it [03:30,  1.52it/s]Extractor Predicting: 340it [03:31,  1.51it/s]Extractor Predicting: 341it [03:32,  1.50it/s]Extractor Predicting: 342it [03:32,  1.51it/s]Extractor Predicting: 343it [03:33,  1.54it/s]Extractor Predicting: 344it [03:34,  1.54it/s]Extractor Predicting: 345it [03:34,  1.50it/s]Extractor Predicting: 346it [03:35,  1.51it/s]Extractor Predicting: 347it [03:36,  1.53it/s]Extractor Predicting: 348it [03:36,  1.54it/s]Extractor Predicting: 349it [03:37,  1.55it/s]Extractor Predicting: 350it [03:38,  1.55it/s]Extractor Predicting: 351it [03:38,  1.58it/s]Extractor Predicting: 352it [03:39,  1.53it/s]Extractor Predicting: 353it [03:40,  1.51it/s]Extractor Predicting: 354it [03:40,  1.54it/s]Extractor Predicting: 355it [03:41,  1.57it/s]Extractor Predicting: 356it [03:41,  1.57it/s]Extractor Predicting: 357it [03:42,  1.57it/s]Extractor Predicting: 358it [03:43,  1.57it/s]Extractor Predicting: 359it [03:43,  1.57it/s]Extractor Predicting: 360it [03:44,  1.36it/s]Extractor Predicting: 361it [03:45,  1.40it/s]Extractor Predicting: 362it [03:46,  1.45it/s]Extractor Predicting: 363it [03:46,  1.51it/s]Extractor Predicting: 364it [03:47,  1.54it/s]Extractor Predicting: 365it [03:48,  1.52it/s]Extractor Predicting: 366it [03:48,  1.49it/s]Extractor Predicting: 367it [03:49,  1.49it/s]Extractor Predicting: 368it [03:50,  1.48it/s]Extractor Predicting: 369it [03:50,  1.44it/s]Extractor Predicting: 370it [03:51,  1.45it/s]Extractor Predicting: 371it [03:52,  1.48it/s]Extractor Predicting: 372it [03:52,  1.75it/s]Extractor Predicting: 372it [03:52,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:42,182 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:42,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:42,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:42,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:42,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:04:42,797 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:04:42,798 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:04:43,390 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:04:44,447 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:04:44,447 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:47,264 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:47,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:47,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:47,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:04:47,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:04:47,914 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:04:47,915 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:04:48,479 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:04:48,653 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:04:48,653 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2563131313131313,
  "recall": 0.09111310592459605,
  "score": 0.13443708609271524,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:09,  1.52it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.58it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:17,  1.52it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:18,  1.56it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.49it/s]Extractor Predicting: 39it [00:24,  1.46it/s]Extractor Predicting: 40it [00:25,  1.46it/s]Extractor Predicting: 41it [00:26,  1.46it/s]Extractor Predicting: 42it [00:26,  1.48it/s]Extractor Predicting: 43it [00:27,  1.49it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:28,  1.48it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:30,  1.48it/s]Extractor Predicting: 49it [00:31,  1.49it/s]Extractor Predicting: 50it [00:32,  1.48it/s]Extractor Predicting: 51it [00:32,  1.46it/s]Extractor Predicting: 52it [00:33,  1.46it/s]Extractor Predicting: 53it [00:34,  1.48it/s]Extractor Predicting: 54it [00:34,  1.47it/s]Extractor Predicting: 55it [00:35,  1.51it/s]Extractor Predicting: 56it [00:36,  1.52it/s]Extractor Predicting: 57it [00:36,  1.50it/s]Extractor Predicting: 58it [00:37,  1.48it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:38,  1.48it/s]Extractor Predicting: 61it [00:39,  1.44it/s]Extractor Predicting: 62it [00:40,  1.44it/s]Extractor Predicting: 63it [00:40,  1.55it/s]Extractor Predicting: 63it [00:40,  1.54it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.46808510638297873,
  "recall": 0.05933473179502547,
  "score": 0.10531914893617021,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
