Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_15_seed_3', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:16<05:05, 16.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:30<04:33, 15.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:49<04:46, 16.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:03<04:14, 15.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:19<03:57, 15.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:35<03:43, 15.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:49<03:19, 15.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:04<03:02, 15.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:20<02:47, 15.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:34<02:30, 15.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:49<02:14, 14.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:04<01:59, 14.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:18<01:42, 14.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:32<01:26, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:45<01:09, 13.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:03<01:01, 15.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:16<00:43, 14.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:29<00:28, 14.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:44<00:14, 14.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:59<00:00, 14.64s/it]Generating: 100%|██████████| 20/20 [04:59<00:00, 14.99s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 358, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7118055555555556, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('συῆνος', 'said to be the same as', '', 'They are known as a form of Greek σάρτικρος ( συῆνος ) , τεῖς ( συθροτος ) , τίνρας ( συῆλώηνος ) ; and of the forms σατος τικρος in Latin .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : student .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 331, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 503, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7391826923076923, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 303, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 466, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 531, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 576, 'raw': 800}
{'target': 600, 'success': 598, 'raw': 832}
{'target': 600, 'success': 620, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7175925925925926, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8233695652173914, 'errors': {'', "('', 'continent', 'Aegean Sea', 'The island of Aranet , located in the Aegean Sea , is one of two islands that the Greek island of Argos has in common with the Arabian sea .')"}}
['Relation : field of this occupation . Context : The main characters in the series are The New Guy , The New York City Police Detective and the Los Angeles Police Department . Head Entity : Los Angeles Police Department , Tail Entity : police .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Walter L. Cusack', 'field of this occupation', '', 'Walter L. Cusack ( June 23 , 1924 February 21 , 2002 ) was an English cricket er who played six Tests in England .')"}}
['Relation : field of work . Context : Following his graduation in 1957 , he founded the PSA ( now in association with B. C. ) , an academic association for linguistics , linguistics , and other fields , and served as a member of the Princeton University Research Committee . Head Entity : linguistics , Tail Entity : graduate .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.76875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n']
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n', 'Relation : given name . Context : James M. Young ( February 14 , 1821 October 19 , 1924 ) was an American poet and writer specializing in the arts . Head Entity : James M. Young , Tail Entity : named name .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8206521739130435, 'errors': {'', "('Doctor Who', 'given name', '', 'He is best known for his play as a character on Doctor Who , in which he played the name of the villainess in the Doctor Who special , The One and Only Thing .')", "('Joseph L. M. Rennelly', 'given name', '', 'Joseph L. M. Rennelly is a former New York City mayor who served as the 62nd governor of New York City .')", 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Never Let Me Go', 'lyrics by', '', 'She followed this up with her sixth studio album , Never Let Me Go , which is now a cult classic on the Billboard R&B Top 40 .')", "('Never Say Goodbye', 'lyrics by', '', 'After graduating from Berklee College of Music , he started to write a single called Never Say Goodbye that debuted at No. 1 on the Billboard 200 single Hot Hot Song in July 1991 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Central railway', 'owned by', '', 'It was built between 1941 and 1943 under the direction of the Central railway company ( D.')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('GMAW', 'performer', '', 'He also performed with the band Pampers on the opening night of WMAW in Los Angeles and the opening night of the ABC drama GMAW in Atlanta .')"}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 128, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 185, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 223, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 288, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 371, 'raw': 576}
{'target': 600, 'success': 393, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 433, 'raw': 672}
{'target': 600, 'success': 451, 'raw': 704}
{'target': 600, 'success': 471, 'raw': 736}
{'target': 600, 'success': 495, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 583, 'raw': 896}
{'target': 600, 'success': 601, 'raw': 928}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.6476293103448276, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : replaces .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 17944
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18044, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:15, 15.34s/it]Extractor Estimating: 2it [00:17,  7.66s/it]Extractor Estimating: 3it [00:18,  4.43s/it]Extractor Estimating: 4it [00:18,  2.90s/it]Extractor Estimating: 5it [00:19,  2.07s/it]Extractor Estimating: 6it [00:19,  1.57s/it]Extractor Estimating: 7it [00:20,  1.23s/it]Extractor Estimating: 8it [00:21,  1.03s/it]Extractor Estimating: 9it [00:21,  1.11it/s]Extractor Estimating: 10it [00:22,  1.26it/s]Extractor Estimating: 11it [00:22,  1.38it/s]Extractor Estimating: 12it [00:23,  1.46it/s]Extractor Estimating: 13it [00:24,  1.52it/s]Extractor Estimating: 14it [00:24,  1.61it/s]Extractor Estimating: 15it [00:25,  1.61it/s]Extractor Estimating: 16it [00:25,  1.58it/s]Extractor Estimating: 17it [00:26,  1.65it/s]Extractor Estimating: 18it [00:27,  1.63it/s]Extractor Estimating: 19it [00:27,  1.62it/s]Extractor Estimating: 20it [00:28,  1.62it/s]Extractor Estimating: 21it [00:28,  1.66it/s]Extractor Estimating: 22it [00:29,  1.71it/s]Extractor Estimating: 23it [00:29,  1.67it/s]Extractor Estimating: 24it [00:30,  1.71it/s]Extractor Estimating: 25it [00:31,  1.66it/s]Extractor Estimating: 26it [00:34,  1.27s/it]Extractor Estimating: 27it [00:34,  1.07s/it]Extractor Estimating: 28it [00:35,  1.09it/s]Extractor Estimating: 29it [00:35,  1.24it/s]Extractor Estimating: 30it [00:36,  1.36it/s]Extractor Estimating: 31it [00:36,  1.46it/s]Extractor Estimating: 32it [00:37,  1.56it/s]Extractor Estimating: 33it [00:38,  1.58it/s]Extractor Estimating: 34it [00:38,  1.59it/s]Extractor Estimating: 35it [00:39,  1.63it/s]Extractor Estimating: 36it [00:39,  1.67it/s]Extractor Estimating: 37it [00:40,  1.72it/s]Extractor Estimating: 38it [00:40,  1.70it/s]Extractor Estimating: 39it [00:41,  1.66it/s]Extractor Estimating: 40it [00:42,  1.70it/s]Extractor Estimating: 41it [00:42,  1.76it/s]Extractor Estimating: 42it [00:43,  1.76it/s]Extractor Estimating: 43it [00:43,  1.74it/s]Extractor Estimating: 44it [00:44,  1.75it/s]Extractor Estimating: 45it [00:44,  1.76it/s]Extractor Estimating: 46it [00:45,  1.74it/s]Extractor Estimating: 47it [00:46,  1.70it/s]Extractor Estimating: 48it [00:46,  1.76it/s]Extractor Estimating: 49it [00:47,  1.76it/s]Extractor Estimating: 50it [00:47,  1.76it/s]Extractor Estimating: 51it [00:48,  1.75it/s]Extractor Estimating: 52it [00:48,  1.72it/s]Extractor Estimating: 53it [00:49,  1.69it/s]Extractor Estimating: 54it [00:50,  1.70it/s]Extractor Estimating: 55it [00:50,  1.71it/s]Extractor Estimating: 56it [00:51,  1.61it/s]Extractor Estimating: 57it [00:52,  1.59it/s]Extractor Estimating: 58it [00:52,  1.59it/s]Extractor Estimating: 59it [00:53,  1.59it/s]Extractor Estimating: 60it [00:53,  1.59it/s]Extractor Estimating: 61it [00:54,  1.62it/s]Extractor Estimating: 62it [00:55,  1.64it/s]Extractor Estimating: 63it [00:55,  1.66it/s]Extractor Estimating: 64it [00:56,  1.71it/s]Extractor Estimating: 65it [00:56,  1.68it/s]Extractor Estimating: 66it [00:57,  1.65it/s]Extractor Estimating: 67it [00:58,  1.63it/s]Extractor Estimating: 68it [00:58,  1.51it/s]Extractor Estimating: 69it [00:59,  1.53it/s]Extractor Estimating: 70it [01:00,  1.54it/s]Extractor Estimating: 71it [01:00,  1.50it/s]Extractor Estimating: 72it [01:01,  1.56it/s]Extractor Estimating: 73it [01:02,  1.58it/s]Extractor Estimating: 74it [01:02,  1.60it/s]Extractor Estimating: 75it [01:03,  1.59it/s]Extractor Estimating: 76it [01:03,  1.62it/s]Extractor Estimating: 77it [01:04,  1.63it/s]Extractor Estimating: 78it [01:05,  1.61it/s]Extractor Estimating: 79it [01:05,  1.65it/s]Extractor Estimating: 80it [01:06,  1.60it/s]Extractor Estimating: 81it [01:07,  1.62it/s]Extractor Estimating: 82it [01:07,  1.66it/s]Extractor Estimating: 83it [01:08,  1.68it/s]Extractor Estimating: 84it [01:08,  1.64it/s]Extractor Estimating: 85it [01:09,  1.62it/s]Extractor Estimating: 86it [01:10,  1.61it/s]Extractor Estimating: 87it [01:10,  1.62it/s]Extractor Estimating: 88it [01:11,  1.61it/s]Extractor Estimating: 89it [01:11,  1.63it/s]Extractor Estimating: 90it [01:12,  1.60it/s]Extractor Estimating: 91it [01:13,  1.65it/s]Extractor Estimating: 92it [01:13,  1.61it/s]Extractor Estimating: 93it [01:14,  1.65it/s]Extractor Estimating: 94it [01:15,  1.62it/s]Extractor Estimating: 95it [01:15,  1.62it/s]Extractor Estimating: 96it [01:16,  1.62it/s]Extractor Estimating: 97it [01:16,  1.63it/s]Extractor Estimating: 98it [01:17,  1.62it/s]Extractor Estimating: 99it [01:18,  1.62it/s]Extractor Estimating: 100it [01:18,  1.55it/s]Extractor Estimating: 101it [01:19,  1.59it/s]Extractor Estimating: 102it [01:20,  1.58it/s]Extractor Estimating: 103it [01:20,  1.59it/s]Extractor Estimating: 104it [01:21,  1.57it/s]Extractor Estimating: 105it [01:21,  1.56it/s]Extractor Estimating: 106it [01:22,  1.59it/s]Extractor Estimating: 107it [01:23,  1.59it/s]Extractor Estimating: 108it [01:23,  1.63it/s]Extractor Estimating: 109it [01:24,  1.61it/s]Extractor Estimating: 110it [01:25,  1.61it/s]Extractor Estimating: 111it [01:25,  1.63it/s]Extractor Estimating: 112it [01:26,  1.62it/s]Extractor Estimating: 113it [01:26,  1.65it/s]Extractor Estimating: 114it [01:27,  1.61it/s]Extractor Estimating: 115it [01:28,  1.63it/s]Extractor Estimating: 116it [01:28,  1.65it/s]Extractor Estimating: 117it [01:29,  1.60it/s]Extractor Estimating: 118it [01:29,  1.64it/s]Extractor Estimating: 119it [01:30,  1.62it/s]Extractor Estimating: 120it [01:31,  1.64it/s]Extractor Estimating: 121it [01:31,  1.66it/s]Extractor Estimating: 122it [01:32,  1.63it/s]Extractor Estimating: 123it [01:32,  1.63it/s]Extractor Estimating: 124it [01:33,  1.65it/s]Extractor Estimating: 125it [01:34,  1.64it/s]Extractor Estimating: 126it [01:34,  1.62it/s]Extractor Estimating: 127it [01:35,  1.61it/s]Extractor Estimating: 128it [01:36,  1.62it/s]Extractor Estimating: 129it [01:36,  1.63it/s]Extractor Estimating: 130it [01:37,  1.60it/s]Extractor Estimating: 131it [01:37,  1.59it/s]Extractor Estimating: 132it [01:38,  1.63it/s]Extractor Estimating: 133it [01:39,  1.68it/s]Extractor Estimating: 134it [01:39,  1.67it/s]Extractor Estimating: 135it [01:40,  1.70it/s]Extractor Estimating: 136it [01:40,  1.65it/s]Extractor Estimating: 137it [01:41,  1.67it/s]Extractor Estimating: 138it [01:42,  1.61it/s]Extractor Estimating: 139it [01:42,  1.63it/s]Extractor Estimating: 140it [01:43,  1.63it/s]Extractor Estimating: 141it [01:44,  1.63it/s]Extractor Estimating: 142it [01:44,  1.68it/s]Extractor Estimating: 143it [01:45,  1.63it/s]Extractor Estimating: 144it [01:45,  1.62it/s]Extractor Estimating: 145it [01:46,  1.64it/s]Extractor Estimating: 146it [01:47,  1.63it/s]Extractor Estimating: 147it [01:47,  1.57it/s]Extractor Estimating: 148it [01:48,  1.58it/s]Extractor Estimating: 149it [01:49,  1.43it/s]Extractor Estimating: 150it [01:49,  1.50it/s]Extractor Estimating: 151it [01:50,  1.56it/s]Extractor Estimating: 152it [01:51,  1.55it/s]Extractor Estimating: 153it [01:51,  1.64it/s]Extractor Estimating: 154it [01:52,  1.65it/s]Extractor Estimating: 155it [01:52,  1.67it/s]Extractor Estimating: 156it [01:53,  1.68it/s]Extractor Estimating: 157it [01:53,  1.72it/s]Extractor Estimating: 158it [01:55,  1.28it/s]Extractor Estimating: 159it [01:55,  1.38it/s]Extractor Estimating: 160it [01:56,  1.49it/s]Extractor Estimating: 161it [01:56,  1.56it/s]Extractor Estimating: 162it [01:57,  1.64it/s]Extractor Estimating: 163it [01:57,  1.71it/s]Extractor Estimating: 164it [01:58,  1.74it/s]Extractor Estimating: 165it [01:59,  1.75it/s]Extractor Estimating: 166it [01:59,  1.77it/s]Extractor Estimating: 167it [02:00,  1.78it/s]Extractor Estimating: 168it [02:00,  1.80it/s]Extractor Estimating: 169it [02:01,  1.84it/s]Extractor Estimating: 170it [02:01,  1.78it/s]Extractor Estimating: 171it [02:02,  1.81it/s]Extractor Estimating: 172it [02:02,  1.82it/s]Extractor Estimating: 173it [02:03,  1.82it/s]Extractor Estimating: 174it [02:03,  1.82it/s]Extractor Estimating: 175it [02:04,  1.75it/s]Extractor Estimating: 176it [02:05,  1.71it/s]Extractor Estimating: 177it [02:05,  1.68it/s]Extractor Estimating: 178it [02:06,  1.71it/s]Extractor Estimating: 179it [02:07,  1.63it/s]Extractor Estimating: 180it [02:07,  1.63it/s]Extractor Estimating: 181it [02:08,  1.58it/s]Extractor Estimating: 182it [02:08,  1.64it/s]Extractor Estimating: 183it [02:09,  1.65it/s]Extractor Estimating: 184it [02:10,  1.63it/s]Extractor Estimating: 185it [02:10,  1.62it/s]Extractor Estimating: 186it [02:11,  1.61it/s]Extractor Estimating: 187it [02:12,  1.62it/s]Extractor Estimating: 188it [02:12,  1.66it/s]Extractor Estimating: 189it [02:13,  1.69it/s]Extractor Estimating: 190it [02:13,  1.68it/s]Extractor Estimating: 191it [02:14,  1.69it/s]Extractor Estimating: 192it [02:14,  1.65it/s]Extractor Estimating: 193it [02:15,  1.69it/s]Extractor Estimating: 194it [02:16,  1.66it/s]Extractor Estimating: 195it [02:16,  1.65it/s]Extractor Estimating: 196it [02:17,  1.64it/s]Extractor Estimating: 197it [02:18,  1.64it/s]Extractor Estimating: 198it [02:18,  1.61it/s]Extractor Estimating: 199it [02:19,  1.59it/s]Extractor Estimating: 200it [02:19,  1.65it/s]Extractor Estimating: 201it [02:20,  1.62it/s]Extractor Estimating: 202it [02:21,  1.63it/s]Extractor Estimating: 203it [02:21,  1.60it/s]Extractor Estimating: 204it [02:22,  1.64it/s]Extractor Estimating: 205it [02:22,  1.59it/s]Extractor Estimating: 206it [02:23,  1.56it/s]Extractor Estimating: 207it [02:24,  1.56it/s]Extractor Estimating: 208it [02:24,  1.58it/s]Extractor Estimating: 209it [02:25,  1.54it/s]Extractor Estimating: 210it [02:26,  1.57it/s]Extractor Estimating: 211it [02:26,  1.58it/s]Extractor Estimating: 212it [02:27,  1.57it/s]Extractor Estimating: 213it [02:28,  1.59it/s]Extractor Estimating: 214it [02:28,  1.61it/s]Extractor Estimating: 215it [02:29,  1.55it/s]Extractor Estimating: 216it [02:30,  1.57it/s]Extractor Estimating: 217it [02:30,  1.57it/s]Extractor Estimating: 218it [02:31,  1.62it/s]Extractor Estimating: 219it [02:31,  1.65it/s]Extractor Estimating: 220it [02:32,  1.62it/s]Extractor Estimating: 221it [02:33,  1.65it/s]Extractor Estimating: 222it [02:33,  1.61it/s]Extractor Estimating: 223it [02:34,  1.63it/s]Extractor Estimating: 224it [02:35,  1.45it/s]Extractor Estimating: 225it [02:35,  1.51it/s]Extractor Estimating: 226it [02:36,  1.54it/s]Extractor Estimating: 227it [02:37,  1.56it/s]Extractor Estimating: 228it [02:37,  1.60it/s]Extractor Estimating: 229it [02:38,  1.62it/s]Extractor Estimating: 230it [02:38,  1.61it/s]Extractor Estimating: 231it [02:39,  1.64it/s]Extractor Estimating: 232it [02:40,  1.58it/s]Extractor Estimating: 233it [02:40,  1.60it/s]Extractor Estimating: 234it [02:41,  1.60it/s]Extractor Estimating: 235it [02:41,  1.63it/s]Extractor Estimating: 236it [02:42,  1.67it/s]Extractor Estimating: 237it [02:43,  1.64it/s]Extractor Estimating: 238it [02:43,  1.62it/s]Extractor Estimating: 239it [02:44,  1.64it/s]Extractor Estimating: 240it [02:44,  1.66it/s]Extractor Estimating: 241it [02:45,  1.66it/s]Extractor Estimating: 242it [02:46,  1.64it/s]Extractor Estimating: 243it [02:46,  1.64it/s]Extractor Estimating: 244it [02:47,  1.62it/s]Extractor Estimating: 245it [02:48,  1.63it/s]Extractor Estimating: 246it [02:48,  1.63it/s]Extractor Estimating: 247it [02:49,  1.54it/s]Extractor Estimating: 248it [02:49,  1.60it/s]Extractor Estimating: 249it [02:50,  1.61it/s]Extractor Estimating: 250it [02:51,  1.65it/s]Extractor Estimating: 251it [02:51,  1.67it/s]Extractor Estimating: 252it [02:52,  1.62it/s]Extractor Estimating: 253it [02:53,  1.56it/s]Extractor Estimating: 254it [02:53,  1.54it/s]Extractor Estimating: 255it [02:54,  1.57it/s]Extractor Estimating: 256it [02:54,  1.59it/s]Extractor Estimating: 257it [02:55,  1.61it/s]Extractor Estimating: 258it [02:56,  1.61it/s]Extractor Estimating: 259it [02:56,  1.59it/s]Extractor Estimating: 260it [02:57,  1.57it/s]Extractor Estimating: 261it [02:58,  1.57it/s]Extractor Estimating: 262it [02:58,  1.59it/s]Extractor Estimating: 263it [02:59,  1.60it/s]Extractor Estimating: 264it [02:59,  1.61it/s]Extractor Estimating: 265it [03:00,  1.60it/s]Extractor Estimating: 266it [03:01,  1.58it/s]Extractor Estimating: 267it [03:01,  1.60it/s]Extractor Estimating: 268it [03:02,  1.56it/s]Extractor Estimating: 269it [03:03,  1.57it/s]Extractor Estimating: 270it [03:03,  1.60it/s]Extractor Estimating: 271it [03:04,  1.57it/s]Extractor Estimating: 272it [03:05,  1.59it/s]Extractor Estimating: 273it [03:05,  1.55it/s]Extractor Estimating: 274it [03:06,  1.58it/s]Extractor Estimating: 275it [03:06,  1.53it/s]Extractor Estimating: 276it [03:07,  1.59it/s]Extractor Estimating: 277it [03:08,  1.58it/s]Extractor Estimating: 278it [03:08,  1.54it/s]Extractor Estimating: 279it [03:09,  1.56it/s]Extractor Estimating: 280it [03:10,  1.54it/s]Extractor Estimating: 281it [03:10,  1.53it/s]Extractor Estimating: 282it [03:11,  1.58it/s]Extractor Estimating: 283it [03:12,  1.59it/s]Extractor Estimating: 284it [03:12,  1.59it/s]Extractor Estimating: 285it [03:13,  1.58it/s]Extractor Estimating: 286it [03:13,  1.55it/s]Extractor Estimating: 287it [03:14,  1.59it/s]Extractor Estimating: 288it [03:15,  1.56it/s]Extractor Estimating: 289it [03:15,  1.59it/s]Extractor Estimating: 290it [03:16,  1.61it/s]Extractor Estimating: 291it [03:17,  1.58it/s]Extractor Estimating: 292it [03:17,  1.46it/s]Extractor Estimating: 293it [03:18,  1.45it/s]Extractor Estimating: 294it [03:19,  1.46it/s]Extractor Estimating: 295it [03:19,  1.46it/s]Extractor Estimating: 296it [03:20,  1.49it/s]Extractor Estimating: 297it [03:21,  1.53it/s]Extractor Estimating: 298it [03:21,  1.51it/s]Extractor Estimating: 299it [03:22,  1.51it/s]Extractor Estimating: 300it [03:23,  1.55it/s]Extractor Estimating: 301it [03:23,  1.54it/s]Extractor Estimating: 302it [03:24,  1.56it/s]Extractor Estimating: 303it [03:25,  1.58it/s]Extractor Estimating: 304it [03:25,  1.59it/s]Extractor Estimating: 305it [03:26,  1.59it/s]Extractor Estimating: 306it [03:26,  1.61it/s]Extractor Estimating: 307it [03:27,  1.66it/s]Extractor Estimating: 308it [03:28,  1.63it/s]Extractor Estimating: 309it [03:28,  1.65it/s]Extractor Estimating: 310it [03:29,  1.64it/s]Extractor Estimating: 311it [03:30,  1.58it/s]Extractor Estimating: 312it [03:30,  1.60it/s]Extractor Estimating: 313it [03:31,  1.62it/s]Extractor Estimating: 314it [03:31,  1.53it/s]Extractor Estimating: 315it [03:32,  1.53it/s]Extractor Estimating: 316it [03:33,  1.52it/s]Extractor Estimating: 317it [03:33,  1.56it/s]Extractor Estimating: 318it [03:34,  1.61it/s]Extractor Estimating: 319it [03:35,  1.56it/s]Extractor Estimating: 320it [03:35,  1.61it/s]Extractor Estimating: 321it [03:36,  1.64it/s]Extractor Estimating: 322it [03:36,  1.67it/s]Extractor Estimating: 323it [03:37,  1.65it/s]Extractor Estimating: 324it [03:38,  1.63it/s]Extractor Estimating: 325it [03:38,  1.64it/s]Extractor Estimating: 326it [03:39,  1.66it/s]Extractor Estimating: 327it [03:39,  1.67it/s]Extractor Estimating: 328it [03:40,  1.68it/s]Extractor Estimating: 329it [03:41,  1.67it/s]Extractor Estimating: 330it [03:41,  1.69it/s]Extractor Estimating: 331it [03:42,  1.66it/s]Extractor Estimating: 332it [03:42,  1.65it/s]Extractor Estimating: 333it [03:43,  1.59it/s]Extractor Estimating: 334it [03:44,  1.64it/s]Extractor Estimating: 335it [03:44,  1.66it/s]Extractor Estimating: 336it [03:45,  1.63it/s]Extractor Estimating: 337it [03:45,  1.64it/s]Extractor Estimating: 338it [03:46,  1.62it/s]Extractor Estimating: 339it [03:47,  1.67it/s]Extractor Estimating: 340it [03:47,  1.68it/s]Extractor Estimating: 341it [03:48,  1.67it/s]Extractor Estimating: 342it [03:48,  1.70it/s]Extractor Estimating: 343it [03:49,  1.63it/s]Extractor Estimating: 344it [03:50,  1.63it/s]Extractor Estimating: 345it [03:50,  1.64it/s]Extractor Estimating: 346it [03:51,  1.68it/s]Extractor Estimating: 347it [03:51,  1.67it/s]Extractor Estimating: 348it [03:52,  1.69it/s]Extractor Estimating: 349it [03:53,  1.68it/s]Extractor Estimating: 350it [03:53,  1.69it/s]Extractor Estimating: 351it [03:54,  1.68it/s]Extractor Estimating: 352it [03:54,  1.66it/s]Extractor Estimating: 353it [03:55,  1.62it/s]Extractor Estimating: 354it [03:56,  1.61it/s]Extractor Estimating: 355it [03:56,  1.63it/s]Extractor Estimating: 356it [03:57,  1.58it/s]Extractor Estimating: 357it [03:58,  1.61it/s]Extractor Estimating: 358it [03:58,  1.63it/s]Extractor Estimating: 359it [03:59,  1.58it/s]Extractor Estimating: 360it [03:59,  1.62it/s]Extractor Estimating: 361it [04:00,  1.63it/s]Extractor Estimating: 362it [04:01,  1.65it/s]Extractor Estimating: 363it [04:01,  1.60it/s]Extractor Estimating: 364it [04:02,  1.62it/s]Extractor Estimating: 365it [04:03,  1.63it/s]Extractor Estimating: 366it [04:03,  1.60it/s]Extractor Estimating: 367it [04:04,  1.47it/s]Extractor Estimating: 368it [04:05,  1.56it/s]Extractor Estimating: 369it [04:05,  1.54it/s]Extractor Estimating: 370it [04:06,  1.52it/s]Extractor Estimating: 371it [04:07,  1.54it/s]Extractor Estimating: 372it [04:07,  1.52it/s]Extractor Estimating: 373it [04:08,  1.53it/s]Extractor Estimating: 374it [04:08,  1.56it/s]Extractor Estimating: 375it [04:09,  1.61it/s]Extractor Estimating: 376it [04:10,  1.55it/s]Extractor Estimating: 377it [04:10,  1.55it/s]Extractor Estimating: 378it [04:11,  1.55it/s]Extractor Estimating: 379it [04:12,  1.57it/s]Extractor Estimating: 380it [04:12,  1.56it/s]Extractor Estimating: 381it [04:13,  1.55it/s]Extractor Estimating: 382it [04:14,  1.55it/s]Extractor Estimating: 383it [04:14,  1.52it/s]Extractor Estimating: 384it [04:15,  1.47it/s]Extractor Estimating: 385it [04:16,  1.50it/s]Extractor Estimating: 386it [04:16,  1.54it/s]Extractor Estimating: 387it [04:17,  1.54it/s]Extractor Estimating: 388it [04:18,  1.56it/s]Extractor Estimating: 389it [04:18,  1.54it/s]Extractor Estimating: 390it [04:19,  1.50it/s]Extractor Estimating: 391it [04:20,  1.55it/s]Extractor Estimating: 392it [04:20,  1.56it/s]Extractor Estimating: 393it [04:21,  1.49it/s]Extractor Estimating: 394it [04:22,  1.48it/s]Extractor Estimating: 395it [04:22,  1.51it/s]Extractor Estimating: 396it [04:23,  1.54it/s]Extractor Estimating: 397it [04:23,  1.53it/s]Extractor Estimating: 398it [04:24,  1.52it/s]Extractor Estimating: 399it [04:25,  1.50it/s]Extractor Estimating: 400it [04:26,  1.49it/s]Extractor Estimating: 401it [04:26,  1.49it/s]Extractor Estimating: 402it [04:27,  1.55it/s]Extractor Estimating: 403it [04:27,  1.57it/s]Extractor Estimating: 404it [04:28,  1.46it/s]Extractor Estimating: 405it [04:29,  1.52it/s]Extractor Estimating: 406it [04:29,  1.55it/s]Extractor Estimating: 407it [04:30,  1.55it/s]Extractor Estimating: 408it [04:31,  1.55it/s]Extractor Estimating: 409it [04:31,  1.54it/s]Extractor Estimating: 410it [04:32,  1.58it/s]Extractor Estimating: 411it [04:33,  1.60it/s]Extractor Estimating: 412it [04:33,  1.62it/s]Extractor Estimating: 413it [04:34,  1.61it/s]Extractor Estimating: 414it [04:34,  1.57it/s]Extractor Estimating: 415it [04:35,  1.55it/s]Extractor Estimating: 416it [04:36,  1.58it/s]Extractor Estimating: 417it [04:36,  1.59it/s]Extractor Estimating: 418it [04:37,  1.62it/s]Extractor Estimating: 419it [04:38,  1.62it/s]Extractor Estimating: 420it [04:38,  1.57it/s]Extractor Estimating: 421it [04:39,  1.62it/s]Extractor Estimating: 422it [04:39,  1.59it/s]Extractor Estimating: 423it [04:40,  1.63it/s]Extractor Estimating: 424it [04:41,  1.66it/s]Extractor Estimating: 425it [04:41,  1.67it/s]Extractor Estimating: 426it [04:42,  1.61it/s]Extractor Estimating: 427it [04:42,  1.64it/s]Extractor Estimating: 428it [04:43,  1.63it/s]Extractor Estimating: 429it [04:44,  1.65it/s]Extractor Estimating: 430it [04:44,  1.62it/s]Extractor Estimating: 431it [04:45,  1.66it/s]Extractor Estimating: 432it [04:45,  1.66it/s]Extractor Estimating: 433it [04:46,  1.66it/s]Extractor Estimating: 434it [04:47,  1.61it/s]Extractor Estimating: 435it [04:47,  1.60it/s]Extractor Estimating: 436it [04:48,  1.58it/s]Extractor Estimating: 437it [04:49,  1.60it/s]Extractor Estimating: 438it [04:49,  1.57it/s]Extractor Estimating: 439it [04:50,  1.52it/s]Extractor Estimating: 440it [04:51,  1.57it/s]Extractor Estimating: 441it [04:51,  1.41it/s]Extractor Estimating: 442it [04:52,  1.49it/s]Extractor Estimating: 443it [04:53,  1.54it/s]Extractor Estimating: 444it [04:53,  1.57it/s]Extractor Estimating: 445it [04:54,  1.58it/s]Extractor Estimating: 446it [04:55,  1.54it/s]Extractor Estimating: 447it [04:55,  1.56it/s]Extractor Estimating: 448it [04:56,  1.56it/s]Extractor Estimating: 449it [04:57,  1.41it/s]Extractor Estimating: 450it [04:57,  1.49it/s]Extractor Estimating: 451it [04:58,  1.50it/s]Extractor Estimating: 452it [04:59,  1.52it/s]Extractor Estimating: 453it [04:59,  1.55it/s]Extractor Estimating: 454it [05:00,  1.51it/s]Extractor Estimating: 455it [05:01,  1.54it/s]Extractor Estimating: 456it [05:01,  1.54it/s]Extractor Estimating: 457it [05:02,  1.53it/s]Extractor Estimating: 458it [05:03,  1.53it/s]Extractor Estimating: 459it [05:03,  1.50it/s]Extractor Estimating: 460it [05:04,  1.52it/s]Extractor Estimating: 461it [05:04,  1.57it/s]Extractor Estimating: 462it [05:05,  1.53it/s]Extractor Estimating: 463it [05:06,  1.49it/s]Extractor Estimating: 464it [05:06,  1.51it/s]Extractor Estimating: 465it [05:07,  1.52it/s]Extractor Estimating: 466it [05:08,  1.53it/s]Extractor Estimating: 467it [05:08,  1.58it/s]Extractor Estimating: 468it [05:09,  1.66it/s]Extractor Estimating: 469it [05:10,  1.62it/s]Extractor Estimating: 470it [05:10,  1.65it/s]Extractor Estimating: 471it [05:11,  1.55it/s]Extractor Estimating: 472it [05:11,  1.56it/s]Extractor Estimating: 473it [05:12,  1.50it/s]Extractor Estimating: 474it [05:13,  1.51it/s]Extractor Estimating: 475it [05:13,  1.53it/s]Extractor Estimating: 476it [05:14,  1.54it/s]Extractor Estimating: 477it [05:15,  1.52it/s]Extractor Estimating: 478it [05:15,  1.53it/s]Extractor Estimating: 479it [05:16,  1.58it/s]Extractor Estimating: 480it [05:17,  1.59it/s]Extractor Estimating: 481it [05:17,  1.58it/s]Extractor Estimating: 482it [05:18,  1.56it/s]Extractor Estimating: 483it [05:19,  1.56it/s]Extractor Estimating: 484it [05:19,  1.57it/s]Extractor Estimating: 485it [05:20,  1.57it/s]Extractor Estimating: 486it [05:21,  1.57it/s]Extractor Estimating: 487it [05:21,  1.57it/s]Extractor Estimating: 488it [05:22,  1.60it/s]Extractor Estimating: 489it [05:22,  1.56it/s]Extractor Estimating: 490it [05:23,  1.50it/s]Extractor Estimating: 491it [05:24,  1.55it/s]Extractor Estimating: 492it [05:24,  1.55it/s]Extractor Estimating: 493it [05:25,  1.61it/s]Extractor Estimating: 494it [05:26,  1.61it/s]Extractor Estimating: 495it [05:26,  1.57it/s]Extractor Estimating: 496it [05:27,  1.55it/s]Extractor Estimating: 497it [05:28,  1.56it/s]Extractor Estimating: 498it [05:28,  1.54it/s]Extractor Estimating: 499it [05:29,  1.52it/s]Extractor Estimating: 500it [05:31,  1.08s/it]Extractor Estimating: 500it [05:31,  1.51it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9814 mean pseudo reward: 0.9492589064532903
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 31826
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31926, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_15_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31926, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.205, loss:2831.8239
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.954, loss:2101.8514
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.952, loss:1758.2725
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.949, loss:1622.4042
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 91, avg_time 0.946, loss:1567.7668
>> valid entity prec:0.5155, rec:0.5054, f1:0.5104
>> valid relation prec:0.6765, rec:0.0284, f1:0.0546
>> valid relation with NER prec:0.6765, rec:0.0284, f1:0.0546
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 191, avg_time 2.527, loss:1504.2500
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 291, avg_time 0.944, loss:1423.2171
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 391, avg_time 0.949, loss:1399.2227
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 82, avg_time 0.942, loss:1283.7356
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 182, avg_time 0.930, loss:1266.3551
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4379, rec:0.5529, f1:0.4887
>> valid relation prec:0.5950, rec:0.0393, f1:0.0738
>> valid relation with NER prec:0.5950, rec:0.0393, f1:0.0738
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 282, avg_time 2.518, loss:1250.7497
g_step 1200, step 382, avg_time 0.952, loss:1158.2571
g_step 1300, step 73, avg_time 0.947, loss:1109.1077
g_step 1400, step 173, avg_time 0.945, loss:1109.4857
g_step 1500, step 273, avg_time 0.954, loss:1108.0014
>> valid entity prec:0.4780, rec:0.5107, f1:0.4938
>> valid relation prec:0.4499, rec:0.0379, f1:0.0699
>> valid relation with NER prec:0.4499, rec:0.0379, f1:0.0699
g_step 1600, step 373, avg_time 2.507, loss:1053.1832
g_step 1700, step 64, avg_time 0.955, loss:1082.5036
g_step 1800, step 164, avg_time 0.952, loss:983.9281
g_step 1900, step 264, avg_time 0.950, loss:1021.6515
g_step 2000, step 364, avg_time 0.942, loss:1038.6790
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4593, rec:0.4298, f1:0.4441
>> valid relation prec:0.0412, rec:0.0043, f1:0.0078
>> valid relation with NER prec:0.0412, rec:0.0043, f1:0.0078
g_step 2100, step 55, avg_time 2.516, loss:964.2889
g_step 2200, step 155, avg_time 0.950, loss:974.6283
g_step 2300, step 255, avg_time 0.937, loss:960.3296
g_step 2400, step 355, avg_time 0.955, loss:978.1906
g_step 2500, step 46, avg_time 0.941, loss:950.0593
>> valid entity prec:0.5428, rec:0.4073, f1:0.4654
>> valid relation prec:0.2084, rec:0.0183, f1:0.0337
>> valid relation with NER prec:0.2084, rec:0.0183, f1:0.0337
g_step 2600, step 146, avg_time 2.515, loss:907.5592
g_step 2700, step 246, avg_time 0.952, loss:949.2674
g_step 2800, step 346, avg_time 0.941, loss:917.0386
g_step 2900, step 37, avg_time 0.943, loss:920.0301
g_step 3000, step 137, avg_time 0.948, loss:855.6553
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4671, rec:0.4731, f1:0.4701
>> valid relation prec:0.1224, rec:0.0222, f1:0.0377
>> valid relation with NER prec:0.1224, rec:0.0222, f1:0.0377
g_step 3100, step 237, avg_time 2.512, loss:882.2393
g_step 3200, step 337, avg_time 0.944, loss:899.5380
g_step 3300, step 28, avg_time 0.955, loss:900.3538
g_step 3400, step 128, avg_time 0.946, loss:838.2005
g_step 3500, step 228, avg_time 0.951, loss:858.2835
>> valid entity prec:0.5071, rec:0.4311, f1:0.4660
>> valid relation prec:0.1013, rec:0.0132, f1:0.0233
>> valid relation with NER prec:0.1013, rec:0.0132, f1:0.0233
g_step 3600, step 328, avg_time 2.519, loss:855.3289
g_step 3700, step 19, avg_time 0.948, loss:833.3432
g_step 3800, step 119, avg_time 0.939, loss:782.3284
g_step 3900, step 219, avg_time 0.947, loss:807.2888
g_step 4000, step 319, avg_time 0.945, loss:882.9718
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5190, rec:0.4442, f1:0.4787
>> valid relation prec:0.2158, rec:0.0406, f1:0.0683
>> valid relation with NER prec:0.2158, rec:0.0406, f1:0.0683
g_step 4100, step 10, avg_time 2.521, loss:826.4081
g_step 4200, step 110, avg_time 0.950, loss:770.2550
g_step 4300, step 210, avg_time 0.946, loss:797.0466
g_step 4400, step 310, avg_time 0.940, loss:762.0890
g_step 4500, step 1, avg_time 0.951, loss:820.7512
>> valid entity prec:0.4134, rec:0.4615, f1:0.4361
>> valid relation prec:0.1554, rec:0.0218, f1:0.0383
>> valid relation with NER prec:0.1554, rec:0.0218, f1:0.0383
g_step 4600, step 101, avg_time 2.520, loss:765.7438
g_step 4700, step 201, avg_time 0.952, loss:755.0471
g_step 4800, step 301, avg_time 0.944, loss:767.7943
g_step 4900, step 401, avg_time 0.943, loss:776.9784
g_step 5000, step 92, avg_time 0.949, loss:737.5019
learning rate was adjusted to 0.0008
>> valid entity prec:0.4794, rec:0.4771, f1:0.4782
>> valid relation prec:0.0963, rec:0.0189, f1:0.0317
>> valid relation with NER prec:0.0963, rec:0.0189, f1:0.0317
g_step 5100, step 192, avg_time 2.512, loss:728.8408
g_step 5200, step 292, avg_time 0.951, loss:764.6183
g_step 5300, step 392, avg_time 0.952, loss:737.1977
g_step 5400, step 83, avg_time 0.949, loss:684.2546
g_step 5500, step 183, avg_time 0.942, loss:705.7855
>> valid entity prec:0.4818, rec:0.4293, f1:0.4541
>> valid relation prec:0.0619, rec:0.0126, f1:0.0209
>> valid relation with NER prec:0.0619, rec:0.0126, f1:0.0209
g_step 5600, step 283, avg_time 2.516, loss:727.1958
g_step 5700, step 383, avg_time 0.945, loss:747.6313
g_step 5800, step 74, avg_time 0.938, loss:714.0742
g_step 5900, step 174, avg_time 0.948, loss:669.0444
g_step 6000, step 274, avg_time 0.945, loss:705.5511
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4841, rec:0.5050, f1:0.4943
>> valid relation prec:0.2331, rec:0.0511, f1:0.0838
>> valid relation with NER prec:0.2331, rec:0.0511, f1:0.0838
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6100, step 374, avg_time 2.517, loss:691.3185
g_step 6200, step 65, avg_time 0.958, loss:673.5815
g_step 6300, step 165, avg_time 0.945, loss:683.1865
g_step 6400, step 265, avg_time 0.949, loss:689.8966
g_step 6500, step 365, avg_time 0.943, loss:683.8515
>> valid entity prec:0.5198, rec:0.4211, f1:0.4652
>> valid relation prec:0.1491, rec:0.0307, f1:0.0509
>> valid relation with NER prec:0.1491, rec:0.0307, f1:0.0509
g_step 6600, step 56, avg_time 2.505, loss:661.5866
g_step 6700, step 156, avg_time 0.953, loss:642.1848
g_step 6800, step 256, avg_time 0.953, loss:648.4740
g_step 6900, step 356, avg_time 0.943, loss:667.5779
g_step 7000, step 47, avg_time 0.951, loss:626.3908
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4696, rec:0.4902, f1:0.4797
>> valid relation prec:0.1181, rec:0.0266, f1:0.0434
>> valid relation with NER prec:0.1181, rec:0.0266, f1:0.0434
g_step 7100, step 147, avg_time 2.515, loss:615.2999
g_step 7200, step 247, avg_time 0.944, loss:633.2145
g_step 7300, step 347, avg_time 0.947, loss:635.8669
g_step 7400, step 38, avg_time 0.956, loss:626.4907
g_step 7500, step 138, avg_time 0.945, loss:598.3382
>> valid entity prec:0.4367, rec:0.4984, f1:0.4656
>> valid relation prec:0.1244, rec:0.0297, f1:0.0479
>> valid relation with NER prec:0.1244, rec:0.0297, f1:0.0479
g_step 7600, step 238, avg_time 2.519, loss:605.2744
g_step 7700, step 338, avg_time 0.942, loss:636.9856
g_step 7800, step 29, avg_time 0.940, loss:604.5520
g_step 7900, step 129, avg_time 0.958, loss:580.7893
g_step 8000, step 229, avg_time 0.942, loss:582.4918
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5002, rec:0.4020, f1:0.4457
>> valid relation prec:0.1063, rec:0.0183, f1:0.0313
>> valid relation with NER prec:0.1063, rec:0.0183, f1:0.0313
g_step 8100, step 329, avg_time 2.515, loss:605.6055
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 00:03:02 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 00:03:02 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_00-03-02_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 00:03:03 - WARNING - datasets.builder -   Using custom data configuration default-f3f3b52b3c789d2e
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f3f3b52b3c789d2e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 00:03:06,196 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:03:06,197 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:03:06,197 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:03:06,198 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:03:06,290 >> Didn't find file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:03:06,333 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:03:06,334 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:03:06,334 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:03:06,334 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:03:06,334 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:03:06,334 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 00:03:06,760 >> loading weights file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:03:09,860 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 00:03:09,860 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_15_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f3f3b52b3c789d2e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 00:03:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1488954063b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:05,  1.72ba/s] 18%|█▊        | 2/11 [00:00<00:03,  2.76ba/s] 27%|██▋       | 3/11 [00:01<00:02,  3.38ba/s] 36%|███▋      | 4/11 [00:01<00:01,  3.77ba/s] 45%|████▌     | 5/11 [00:01<00:01,  3.35ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  3.59ba/s] 64%|██████▎   | 7/11 [00:02<00:01,  3.86ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.08ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.24ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.33ba/s]100%|██████████| 11/11 [00:02<00:00,  4.06ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.35ba/s] 40%|████      | 2/5 [00:00<00:00,  3.95ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.20ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.32ba/s]100%|██████████| 5/5 [00:01<00:00,  4.60ba/s]100%|██████████| 5/5 [00:01<00:00,  4.33ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  5.62ba/s] 27%|██▋       | 3/11 [00:00<00:00,  8.85ba/s] 45%|████▌     | 5/11 [00:00<00:00,  9.96ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 10.36ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.59ba/s]100%|██████████| 11/11 [00:00<00:00, 11.07ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  5.61ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.92ba/s]100%|██████████| 5/5 [00:00<00:00, 10.23ba/s]100%|██████████| 5/5 [00:00<00:00,  9.53ba/s]
[INFO|trainer.py:414] 2023-08-28 00:03:16,423 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 00:03:16,530 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 00:03:16,530 >>   Num examples = 10044
[INFO|trainer.py:1149] 2023-08-28 00:03:16,530 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 00:03:16,530 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 00:03:16,530 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 00:03:16,531 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 00:03:16,531 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:55,  3.33it/s]  0%|          | 2/785 [00:00<03:48,  3.42it/s]  0%|          | 3/785 [00:00<03:46,  3.45it/s]  1%|          | 4/785 [00:01<03:45,  3.46it/s]  1%|          | 5/785 [00:01<03:44,  3.47it/s]  1%|          | 6/785 [00:01<03:45,  3.46it/s]  1%|          | 7/785 [00:02<03:45,  3.45it/s]  1%|          | 8/785 [00:02<03:45,  3.45it/s]  1%|          | 9/785 [00:02<03:45,  3.44it/s]  1%|▏         | 10/785 [00:02<03:45,  3.44it/s]  1%|▏         | 11/785 [00:03<03:44,  3.44it/s]  2%|▏         | 12/785 [00:03<03:44,  3.44it/s]  2%|▏         | 13/785 [00:03<03:44,  3.44it/s]  2%|▏         | 14/785 [00:04<03:44,  3.44it/s]  2%|▏         | 15/785 [00:04<03:43,  3.44it/s]  2%|▏         | 16/785 [00:04<03:43,  3.44it/s]  2%|▏         | 17/785 [00:04<03:43,  3.43it/s]  2%|▏         | 18/785 [00:05<03:43,  3.43it/s]  2%|▏         | 19/785 [00:05<03:42,  3.44it/s]  3%|▎         | 20/785 [00:05<03:42,  3.44it/s]  3%|▎         | 21/785 [00:06<03:42,  3.44it/s]  3%|▎         | 22/785 [00:06<03:42,  3.44it/s]  3%|▎         | 23/785 [00:06<03:41,  3.44it/s]  3%|▎         | 24/785 [00:06<03:41,  3.43it/s]  3%|▎         | 25/785 [00:07<03:47,  3.35it/s]  3%|▎         | 26/785 [00:07<03:45,  3.37it/s]  3%|▎         | 27/785 [00:07<03:43,  3.39it/s]  4%|▎         | 28/785 [00:08<03:42,  3.40it/s]  4%|▎         | 29/785 [00:08<03:41,  3.41it/s]  4%|▍         | 30/785 [00:08<03:40,  3.42it/s]  4%|▍         | 31/785 [00:09<03:40,  3.42it/s]  4%|▍         | 32/785 [00:09<03:39,  3.42it/s]  4%|▍         | 33/785 [00:09<03:39,  3.43it/s]  4%|▍         | 34/785 [00:09<03:38,  3.43it/s]  4%|▍         | 35/785 [00:10<03:38,  3.43it/s]  5%|▍         | 36/785 [00:10<03:38,  3.43it/s]  5%|▍         | 37/785 [00:10<03:37,  3.43it/s]  5%|▍         | 38/785 [00:11<03:37,  3.43it/s]  5%|▍         | 39/785 [00:11<03:37,  3.43it/s]  5%|▌         | 40/785 [00:11<03:37,  3.43it/s]  5%|▌         | 41/785 [00:11<03:36,  3.43it/s]  5%|▌         | 42/785 [00:12<03:36,  3.43it/s]  5%|▌         | 43/785 [00:12<03:36,  3.43it/s]  6%|▌         | 44/785 [00:12<03:35,  3.43it/s]  6%|▌         | 45/785 [00:13<03:35,  3.43it/s]  6%|▌         | 46/785 [00:13<03:35,  3.43it/s]  6%|▌         | 47/785 [00:13<03:34,  3.43it/s]  6%|▌         | 48/785 [00:13<03:34,  3.43it/s]  6%|▌         | 49/785 [00:14<03:34,  3.43it/s]  6%|▋         | 50/785 [00:14<03:34,  3.43it/s]  6%|▋         | 51/785 [00:14<03:34,  3.43it/s]  7%|▋         | 52/785 [00:15<03:33,  3.43it/s]  7%|▋         | 53/785 [00:15<03:33,  3.43it/s]  7%|▋         | 54/785 [00:15<03:33,  3.43it/s]  7%|▋         | 55/785 [00:16<03:32,  3.43it/s]  7%|▋         | 56/785 [00:16<03:32,  3.43it/s]  7%|▋         | 57/785 [00:16<03:32,  3.43it/s]  7%|▋         | 58/785 [00:16<03:31,  3.43it/s]  8%|▊         | 59/785 [00:17<03:31,  3.43it/s]  8%|▊         | 60/785 [00:17<03:31,  3.43it/s]  8%|▊         | 61/785 [00:17<03:30,  3.43it/s]  8%|▊         | 62/785 [00:18<03:30,  3.43it/s]  8%|▊         | 63/785 [00:18<03:30,  3.43it/s]  8%|▊         | 64/785 [00:18<03:29,  3.44it/s]  8%|▊         | 65/785 [00:18<03:28,  3.45it/s]  8%|▊         | 66/785 [00:19<03:28,  3.46it/s]  9%|▊         | 67/785 [00:19<03:27,  3.46it/s]  9%|▊         | 68/785 [00:19<03:26,  3.47it/s]  9%|▉         | 69/785 [00:20<03:26,  3.47it/s]  9%|▉         | 70/785 [00:20<03:25,  3.47it/s]  9%|▉         | 71/785 [00:20<03:25,  3.48it/s]  9%|▉         | 72/785 [00:20<03:25,  3.48it/s]  9%|▉         | 73/785 [00:21<03:24,  3.48it/s]  9%|▉         | 74/785 [00:21<03:24,  3.48it/s] 10%|▉         | 75/785 [00:21<03:23,  3.48it/s] 10%|▉         | 76/785 [00:22<03:23,  3.48it/s] 10%|▉         | 77/785 [00:22<03:23,  3.48it/s] 10%|▉         | 78/785 [00:22<03:23,  3.48it/s] 10%|█         | 79/785 [00:22<03:22,  3.48it/s] 10%|█         | 80/785 [00:23<03:22,  3.48it/s] 10%|█         | 81/785 [00:23<03:22,  3.48it/s] 10%|█         | 82/785 [00:23<03:22,  3.48it/s] 11%|█         | 83/785 [00:24<03:21,  3.48it/s] 11%|█         | 84/785 [00:24<03:21,  3.48it/s] 11%|█         | 85/785 [00:24<03:21,  3.48it/s] 11%|█         | 86/785 [00:24<03:21,  3.48it/s] 11%|█         | 87/785 [00:25<03:20,  3.48it/s] 11%|█         | 88/785 [00:25<03:20,  3.48it/s] 11%|█▏        | 89/785 [00:25<03:20,  3.48it/s] 11%|█▏        | 90/785 [00:26<03:19,  3.48it/s] 12%|█▏        | 91/785 [00:26<03:19,  3.48it/s] 12%|█▏        | 92/785 [00:26<03:19,  3.48it/s] 12%|█▏        | 93/785 [00:26<03:19,  3.48it/s] 12%|█▏        | 94/785 [00:27<03:18,  3.48it/s] 12%|█▏        | 95/785 [00:27<03:18,  3.48it/s] 12%|█▏        | 96/785 [00:27<03:18,  3.48it/s] 12%|█▏        | 97/785 [00:28<03:30,  3.27it/s] 12%|█▏        | 98/785 [00:28<03:26,  3.33it/s] 13%|█▎        | 99/785 [00:28<03:23,  3.37it/s] 13%|█▎        | 100/785 [00:29<03:21,  3.40it/s] 13%|█▎        | 101/785 [00:29<03:19,  3.42it/s] 13%|█▎        | 102/785 [00:29<03:18,  3.44it/s] 13%|█▎        | 103/785 [00:29<03:17,  3.45it/s] 13%|█▎        | 104/785 [00:30<03:16,  3.46it/s] 13%|█▎        | 105/785 [00:30<04:43,  2.40it/s] 14%|█▎        | 106/785 [00:31<04:16,  2.65it/s] 14%|█▎        | 107/785 [00:31<03:57,  2.85it/s] 14%|█▍        | 108/785 [00:31<03:44,  3.02it/s] 14%|█▍        | 109/785 [00:32<03:35,  3.14it/s] 14%|█▍        | 110/785 [00:32<03:28,  3.23it/s] 14%|█▍        | 111/785 [00:32<03:23,  3.30it/s] 14%|█▍        | 112/785 [00:32<03:20,  3.35it/s] 14%|█▍        | 113/785 [00:33<03:18,  3.39it/s] 15%|█▍        | 114/785 [00:33<03:16,  3.42it/s] 15%|█▍        | 115/785 [00:33<03:15,  3.43it/s] 15%|█▍        | 116/785 [00:34<03:14,  3.45it/s] 15%|█▍        | 117/785 [00:34<03:13,  3.46it/s] 15%|█▌        | 118/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 119/785 [00:34<03:12,  3.47it/s] 15%|█▌        | 120/785 [00:35<03:11,  3.47it/s] 15%|█▌        | 121/785 [00:35<03:11,  3.47it/s] 16%|█▌        | 122/785 [00:35<03:10,  3.48it/s] 16%|█▌        | 123/785 [00:36<03:10,  3.48it/s] 16%|█▌        | 124/785 [00:36<03:10,  3.48it/s] 16%|█▌        | 125/785 [00:36<03:09,  3.48it/s] 16%|█▌        | 126/785 [00:36<03:09,  3.48it/s] 16%|█▌        | 127/785 [00:37<03:09,  3.48it/s] 16%|█▋        | 128/785 [00:37<03:08,  3.48it/s] 16%|█▋        | 129/785 [00:37<03:08,  3.48it/s] 17%|█▋        | 130/785 [00:38<03:08,  3.48it/s] 17%|█▋        | 131/785 [00:38<03:08,  3.48it/s] 17%|█▋        | 132/785 [00:38<03:07,  3.47it/s] 17%|█▋        | 133/785 [00:38<03:07,  3.48it/s] 17%|█▋        | 134/785 [00:39<03:07,  3.47it/s] 17%|█▋        | 135/785 [00:39<03:07,  3.48it/s] 17%|█▋        | 136/785 [00:39<03:06,  3.48it/s] 17%|█▋        | 137/785 [00:40<03:06,  3.48it/s] 18%|█▊        | 138/785 [00:40<03:05,  3.48it/s] 18%|█▊        | 139/785 [00:40<03:05,  3.48it/s] 18%|█▊        | 140/785 [00:40<03:05,  3.48it/s] 18%|█▊        | 141/785 [00:41<03:05,  3.48it/s] 18%|█▊        | 142/785 [00:41<03:04,  3.48it/s] 18%|█▊        | 143/785 [00:41<03:04,  3.48it/s] 18%|█▊        | 144/785 [00:42<03:04,  3.48it/s] 18%|█▊        | 145/785 [00:42<03:04,  3.47it/s] 19%|█▊        | 146/785 [00:42<03:03,  3.48it/s] 19%|█▊        | 147/785 [00:43<03:03,  3.48it/s] 19%|█▉        | 148/785 [00:43<03:12,  3.31it/s] 19%|█▉        | 149/785 [00:43<03:09,  3.35it/s] 19%|█▉        | 150/785 [00:43<03:07,  3.39it/s] 19%|█▉        | 151/785 [00:44<03:05,  3.42it/s] 19%|█▉        | 152/785 [00:44<03:04,  3.43it/s] 19%|█▉        | 153/785 [00:44<03:03,  3.44it/s] 20%|█▉        | 154/785 [00:45<03:02,  3.45it/s] 20%|█▉        | 155/785 [00:45<03:02,  3.46it/s] 20%|█▉        | 156/785 [00:45<03:01,  3.46it/s] 20%|██        | 157/785 [00:45<02:58,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 00:04:02,454 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:04:02,454 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 00:04:02,454 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.82it/s][A
  2%|▏         | 12/608 [00:00<00:12, 48.92it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.36it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.46it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.95it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.64it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.57it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.39it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.39it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.46it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.65it/s][A
 10%|█         | 62/608 [00:01<00:11, 45.58it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.47it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.39it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.36it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.27it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.29it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 45.29it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.40it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.51it/s][A
 18%|█▊        | 107/608 [00:02<00:10, 45.59it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.54it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.46it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.36it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.29it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 45.28it/s][A
 23%|██▎       | 137/608 [00:02<00:10, 45.28it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.37it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.01it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.16it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.20it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.18it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.11it/s][A
 28%|██▊       | 172/608 [00:03<00:10, 41.34it/s][A
 29%|██▉       | 177/608 [00:03<00:10, 42.60it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 43.56it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.18it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 44.55it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 44.94it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.16it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.26it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 44.94it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.87it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.05it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.27it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.40it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.47it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.53it/s][A
 41%|████      | 247/608 [00:05<00:07, 45.60it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.48it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 45.14it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.16it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.11it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.34it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.52it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.56it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.59it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.61it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.46it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.17it/s][A
 50%|█████     | 307/608 [00:06<00:06, 43.85it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 44.36it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.79it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.09it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.27it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.30it/s][A
 55%|█████▌    | 337/608 [00:07<00:05, 45.40it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.23it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.07it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.01it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.20it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 45.33it/s][A
 60%|██████    | 367/608 [00:08<00:05, 45.50it/s][A
 61%|██████    | 372/608 [00:08<00:05, 45.55it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.55it/s][A
 63%|██████▎   | 382/608 [00:08<00:04, 45.52it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.37it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 45.15it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.15it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.97it/s][A
 67%|██████▋   | 407/608 [00:08<00:04, 45.29it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.49it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.54it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.56it/s][A
 70%|███████   | 427/608 [00:09<00:03, 45.44it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.38it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.13it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.13it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 44.03it/s][A
 74%|███████▍  | 452/608 [00:09<00:03, 44.61it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.97it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.16it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.24it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.30it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.23it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.02it/s][A
 80%|████████  | 487/608 [00:10<00:02, 44.91it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.09it/s][A
 82%|████████▏ | 497/608 [00:10<00:02, 45.34it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.48it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.48it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.58it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.37it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.27it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.10it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.03it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.15it/s][A
 89%|████████▉ | 542/608 [00:11<00:01, 45.23it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.32it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.54it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.65it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.52it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.40it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.20it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.17it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.20it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 37.75it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 39.89it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 41.55it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 42.76it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 43.62it/s][A                                                 
                                                 [A 20%|██        | 157/785 [00:59<02:58,  3.52it/s]
100%|██████████| 608/608 [00:13<00:00, 43.62it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:04:16,300 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-28 00:04:16,452 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:04:19,876 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:04:19,991 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:04:20,055 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:10<1:19:26,  7.60s/it] 20%|██        | 159/785 [01:10<56:32,  5.42s/it]   20%|██        | 160/785 [01:11<40:25,  3.88s/it] 21%|██        | 161/785 [01:11<29:09,  2.80s/it] 21%|██        | 162/785 [01:11<21:17,  2.05s/it] 21%|██        | 163/785 [01:12<15:47,  1.52s/it] 21%|██        | 164/785 [01:12<11:56,  1.15s/it] 21%|██        | 165/785 [01:12<09:15,  1.12it/s] 21%|██        | 166/785 [01:12<07:21,  1.40it/s] 21%|██▏       | 167/785 [01:13<06:03,  1.70it/s] 21%|██▏       | 168/785 [01:13<05:07,  2.01it/s] 22%|██▏       | 169/785 [01:13<04:28,  2.29it/s] 22%|██▏       | 170/785 [01:14<04:07,  2.49it/s] 22%|██▏       | 171/785 [01:14<03:45,  2.72it/s] 22%|██▏       | 172/785 [01:14<03:30,  2.91it/s] 22%|██▏       | 173/785 [01:15<03:19,  3.06it/s] 22%|██▏       | 174/785 [01:15<03:12,  3.18it/s] 22%|██▏       | 175/785 [01:15<03:07,  3.26it/s] 22%|██▏       | 176/785 [01:15<03:03,  3.32it/s] 23%|██▎       | 177/785 [01:16<03:00,  3.37it/s] 23%|██▎       | 178/785 [01:16<02:58,  3.40it/s] 23%|██▎       | 179/785 [01:16<02:56,  3.43it/s] 23%|██▎       | 180/785 [01:17<02:55,  3.44it/s] 23%|██▎       | 181/785 [01:17<03:02,  3.31it/s] 23%|██▎       | 182/785 [01:17<02:59,  3.35it/s] 23%|██▎       | 183/785 [01:17<02:57,  3.39it/s] 23%|██▎       | 184/785 [01:18<02:56,  3.41it/s] 24%|██▎       | 185/785 [01:18<02:54,  3.43it/s] 24%|██▎       | 186/785 [01:18<02:54,  3.44it/s] 24%|██▍       | 187/785 [01:19<02:53,  3.44it/s] 24%|██▍       | 188/785 [01:19<02:52,  3.45it/s] 24%|██▍       | 189/785 [01:19<02:52,  3.46it/s] 24%|██▍       | 190/785 [01:19<02:52,  3.46it/s] 24%|██▍       | 191/785 [01:20<02:51,  3.46it/s] 24%|██▍       | 192/785 [01:20<02:54,  3.40it/s] 25%|██▍       | 193/785 [01:20<02:52,  3.42it/s] 25%|██▍       | 194/785 [01:21<02:52,  3.44it/s] 25%|██▍       | 195/785 [01:21<02:51,  3.44it/s] 25%|██▍       | 196/785 [01:21<02:50,  3.45it/s] 25%|██▌       | 197/785 [01:21<02:50,  3.45it/s] 25%|██▌       | 198/785 [01:22<02:49,  3.46it/s] 25%|██▌       | 199/785 [01:22<02:49,  3.46it/s] 25%|██▌       | 200/785 [01:22<02:48,  3.46it/s] 26%|██▌       | 201/785 [01:23<02:48,  3.47it/s] 26%|██▌       | 202/785 [01:23<02:48,  3.47it/s] 26%|██▌       | 203/785 [01:23<02:58,  3.26it/s] 26%|██▌       | 204/785 [01:24<02:55,  3.32it/s] 26%|██▌       | 205/785 [01:24<02:52,  3.36it/s] 26%|██▌       | 206/785 [01:24<02:50,  3.39it/s] 26%|██▋       | 207/785 [01:24<02:49,  3.42it/s] 26%|██▋       | 208/785 [01:25<02:48,  3.43it/s] 27%|██▋       | 209/785 [01:25<02:47,  3.44it/s] 27%|██▋       | 210/785 [01:25<02:46,  3.45it/s] 27%|██▋       | 211/785 [01:26<02:46,  3.45it/s] 27%|██▋       | 212/785 [01:26<02:45,  3.46it/s] 27%|██▋       | 213/785 [01:26<02:45,  3.46it/s] 27%|██▋       | 214/785 [01:27<02:57,  3.22it/s] 27%|██▋       | 215/785 [01:27<02:52,  3.30it/s] 28%|██▊       | 216/785 [01:27<02:49,  3.35it/s] 28%|██▊       | 217/785 [01:27<02:47,  3.38it/s] 28%|██▊       | 218/785 [01:28<02:55,  3.24it/s] 28%|██▊       | 219/785 [01:28<02:51,  3.30it/s] 28%|██▊       | 220/785 [01:28<02:48,  3.35it/s] 28%|██▊       | 221/785 [01:29<02:46,  3.39it/s] 28%|██▊       | 222/785 [01:29<02:44,  3.41it/s] 28%|██▊       | 223/785 [01:29<02:43,  3.43it/s] 29%|██▊       | 224/785 [01:30<02:52,  3.25it/s] 29%|██▊       | 225/785 [01:30<02:48,  3.32it/s] 29%|██▉       | 226/785 [01:31<04:27,  2.09it/s] 29%|██▉       | 227/785 [01:31<03:55,  2.37it/s] 29%|██▉       | 228/785 [01:31<03:32,  2.62it/s] 29%|██▉       | 229/785 [01:32<03:16,  2.83it/s] 29%|██▉       | 230/785 [01:32<03:05,  3.00it/s] 29%|██▉       | 231/785 [01:32<02:57,  3.13it/s] 30%|██▉       | 232/785 [01:32<02:51,  3.23it/s] 30%|██▉       | 233/785 [01:33<03:00,  3.07it/s] 30%|██▉       | 234/785 [01:33<02:53,  3.18it/s] 30%|██▉       | 235/785 [01:33<02:48,  3.26it/s] 30%|███       | 236/785 [01:34<02:45,  3.32it/s] 30%|███       | 237/785 [01:34<02:42,  3.37it/s] 30%|███       | 238/785 [01:34<02:40,  3.40it/s] 30%|███       | 239/785 [01:35<02:39,  3.42it/s] 31%|███       | 240/785 [01:35<02:38,  3.44it/s] 31%|███       | 241/785 [01:35<02:37,  3.45it/s] 31%|███       | 242/785 [01:35<02:37,  3.46it/s] 31%|███       | 243/785 [01:36<02:36,  3.46it/s] 31%|███       | 244/785 [01:36<02:40,  3.37it/s] 31%|███       | 245/785 [01:36<02:38,  3.40it/s] 31%|███▏      | 246/785 [01:37<02:37,  3.42it/s] 31%|███▏      | 247/785 [01:37<02:36,  3.44it/s] 32%|███▏      | 248/785 [01:37<02:35,  3.45it/s] 32%|███▏      | 249/785 [01:37<02:35,  3.46it/s] 32%|███▏      | 250/785 [01:38<02:34,  3.46it/s] 32%|███▏      | 251/785 [01:38<02:34,  3.46it/s] 32%|███▏      | 252/785 [01:38<02:33,  3.46it/s] 32%|███▏      | 253/785 [01:39<02:33,  3.47it/s] 32%|███▏      | 254/785 [01:39<02:33,  3.47it/s] 32%|███▏      | 255/785 [01:39<02:39,  3.32it/s] 33%|███▎      | 256/785 [01:39<02:37,  3.36it/s] 33%|███▎      | 257/785 [01:40<02:35,  3.39it/s] 33%|███▎      | 258/785 [01:40<02:34,  3.42it/s] 33%|███▎      | 259/785 [01:40<02:33,  3.43it/s] 33%|███▎      | 260/785 [01:41<02:32,  3.45it/s] 33%|███▎      | 261/785 [01:41<02:31,  3.45it/s] 33%|███▎      | 262/785 [01:41<02:31,  3.46it/s] 34%|███▎      | 263/785 [01:41<02:30,  3.47it/s] 34%|███▎      | 264/785 [01:42<02:30,  3.46it/s] 34%|███▍      | 265/785 [01:42<02:29,  3.47it/s] 34%|███▍      | 266/785 [01:42<02:33,  3.37it/s] 34%|███▍      | 267/785 [01:43<02:32,  3.40it/s] 34%|███▍      | 268/785 [01:43<02:30,  3.42it/s] 34%|███▍      | 269/785 [01:43<02:30,  3.44it/s] 34%|███▍      | 270/785 [01:44<02:29,  3.45it/s] 35%|███▍      | 271/785 [01:44<02:28,  3.45it/s] 35%|███▍      | 272/785 [01:44<02:28,  3.46it/s] 35%|███▍      | 273/785 [01:44<02:27,  3.46it/s] 35%|███▍      | 274/785 [01:45<02:27,  3.47it/s] 35%|███▌      | 275/785 [01:45<02:26,  3.47it/s] 35%|███▌      | 276/785 [01:45<02:26,  3.47it/s] 35%|███▌      | 277/785 [01:46<02:29,  3.39it/s] 35%|███▌      | 278/785 [01:46<02:28,  3.41it/s] 36%|███▌      | 279/785 [01:46<02:27,  3.43it/s] 36%|███▌      | 280/785 [01:46<02:26,  3.44it/s] 36%|███▌      | 281/785 [01:47<02:26,  3.45it/s] 36%|███▌      | 282/785 [01:47<02:25,  3.46it/s] 36%|███▌      | 283/785 [01:47<02:25,  3.46it/s] 36%|███▌      | 284/785 [01:48<02:24,  3.46it/s] 36%|███▋      | 285/785 [01:48<02:24,  3.47it/s] 36%|███▋      | 286/785 [01:48<02:24,  3.46it/s] 37%|███▋      | 287/785 [01:48<02:23,  3.46it/s] 37%|███▋      | 288/785 [01:49<02:25,  3.42it/s] 37%|███▋      | 289/785 [01:49<02:24,  3.43it/s] 37%|███▋      | 290/785 [01:49<02:23,  3.45it/s] 37%|███▋      | 291/785 [01:50<02:23,  3.45it/s] 37%|███▋      | 292/785 [01:50<02:22,  3.46it/s] 37%|███▋      | 293/785 [01:50<02:22,  3.46it/s] 37%|███▋      | 294/785 [01:50<02:21,  3.46it/s] 38%|███▊      | 295/785 [01:51<02:28,  3.31it/s] 38%|███▊      | 296/785 [01:51<02:25,  3.36it/s] 38%|███▊      | 297/785 [01:51<02:24,  3.39it/s] 38%|███▊      | 298/785 [01:52<02:22,  3.41it/s] 38%|███▊      | 299/785 [01:52<02:21,  3.43it/s] 38%|███▊      | 300/785 [01:52<02:21,  3.44it/s] 38%|███▊      | 301/785 [01:53<02:20,  3.45it/s] 38%|███▊      | 302/785 [01:53<02:19,  3.46it/s] 39%|███▊      | 303/785 [01:53<02:19,  3.46it/s] 39%|███▊      | 304/785 [01:53<02:18,  3.47it/s] 39%|███▉      | 305/785 [01:54<02:18,  3.47it/s] 39%|███▉      | 306/785 [01:54<02:27,  3.24it/s] 39%|███▉      | 307/785 [01:54<02:24,  3.31it/s] 39%|███▉      | 308/785 [01:55<02:22,  3.36it/s] 39%|███▉      | 309/785 [01:55<02:20,  3.40it/s] 39%|███▉      | 310/785 [01:55<02:18,  3.42it/s] 40%|███▉      | 311/785 [01:56<02:18,  3.43it/s] 40%|███▉      | 312/785 [01:56<02:17,  3.45it/s] 40%|███▉      | 313/785 [01:56<02:16,  3.45it/s] 40%|████      | 314/785 [01:56<02:14,  3.50it/s][INFO|trainer.py:2140] 2023-08-28 00:05:13,388 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:05:13,388 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 00:05:13,388 >>   Batch size = 8
{'eval_loss': 0.9171123504638672, 'eval_runtime': 13.5302, 'eval_samples_per_second': 359.493, 'eval_steps_per_second': 44.937, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.79it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.54it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.35it/s][A
  4%|▍         | 23/608 [00:00<00:12, 45.29it/s][A
  5%|▍         | 28/608 [00:00<00:12, 45.27it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.07it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.07it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.00it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.15it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.34it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.44it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.46it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.44it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.30it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.19it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.10it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.99it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.08it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.19it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.44it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.47it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.46it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.42it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.32it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.12it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.04it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.07it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.13it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.30it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.45it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.49it/s][A
 27%|██▋       | 163/608 [00:03<00:10, 41.39it/s][A
 28%|██▊       | 168/608 [00:03<00:10, 42.61it/s][A
 28%|██▊       | 173/608 [00:03<00:10, 43.45it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 43.92it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.29it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.52it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.91it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.11it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 44.91it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.05it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.22it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.27it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.28it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.24it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.30it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.30it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.31it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.22it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.19it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.12it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.38it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.34it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.27it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.19it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.22it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.17it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.14it/s][A
 49%|████▉     | 298/608 [00:06<00:07, 43.76it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 44.40it/s][A
 51%|█████     | 308/608 [00:06<00:06, 44.78it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.06it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.15it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.26it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.19it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.21it/s][A
 56%|█████▌    | 338/608 [00:07<00:06, 44.89it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 44.99it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.22it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.34it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.40it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.38it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.42it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.34it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.29it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.03it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.07it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.20it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.37it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.43it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.44it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.31it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.42it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.30it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.10it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.11it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 44.43it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 44.88it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.09it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.23it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.26it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.34it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.17it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.07it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.05it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.13it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.30it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.44it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.48it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.54it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.46it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.22it/s][A
 85%|████████▌ | 518/608 [00:11<00:02, 44.96it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.08it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.09it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.14it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.43it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.58it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.50it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.50it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.36it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.24it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.12it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.07it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 44.47it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 44.90it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.19it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.39it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.34it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.27it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.07it/s][A                                                 
                                                 [A 40%|████      | 314/785 [02:10<02:14,  3.50it/s]
100%|██████████| 608/608 [00:13<00:00, 45.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:05:27,108 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 00:05:27,308 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:05:30,512 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:05:30,693 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:05:30,800 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:23<1:04:36,  8.25s/it] 40%|████      | 316/785 [02:23<45:48,  5.86s/it]   40%|████      | 317/785 [02:24<32:40,  4.19s/it] 41%|████      | 318/785 [02:24<23:30,  3.02s/it] 41%|████      | 319/785 [02:24<17:05,  2.20s/it] 41%|████      | 320/785 [02:25<12:43,  1.64s/it] 41%|████      | 321/785 [02:25<09:34,  1.24s/it] 41%|████      | 322/785 [02:25<07:21,  1.05it/s] 41%|████      | 323/785 [02:26<05:48,  1.32it/s] 41%|████▏     | 324/785 [02:26<04:43,  1.62it/s] 41%|████▏     | 325/785 [02:26<03:58,  1.93it/s] 42%|████▏     | 326/785 [02:26<03:26,  2.22it/s] 42%|████▏     | 327/785 [02:27<03:04,  2.48it/s] 42%|████▏     | 328/785 [02:27<02:48,  2.71it/s] 42%|████▏     | 329/785 [02:27<02:37,  2.89it/s] 42%|████▏     | 330/785 [02:28<02:30,  3.03it/s] 42%|████▏     | 331/785 [02:28<02:33,  2.95it/s] 42%|████▏     | 332/785 [02:28<02:26,  3.09it/s] 42%|████▏     | 333/785 [02:29<02:22,  3.18it/s] 43%|████▎     | 334/785 [02:29<02:18,  3.25it/s] 43%|████▎     | 335/785 [02:29<02:16,  3.30it/s] 43%|████▎     | 336/785 [02:29<02:14,  3.34it/s] 43%|████▎     | 337/785 [02:30<02:13,  3.37it/s] 43%|████▎     | 338/785 [02:30<02:12,  3.38it/s] 43%|████▎     | 339/785 [02:31<02:42,  2.74it/s] 43%|████▎     | 340/785 [02:31<02:32,  2.91it/s] 43%|████▎     | 341/785 [02:31<02:30,  2.94it/s] 44%|████▎     | 342/785 [02:31<02:24,  3.07it/s] 44%|████▎     | 343/785 [02:32<02:19,  3.17it/s] 44%|████▍     | 344/785 [02:32<02:16,  3.24it/s] 44%|████▍     | 345/785 [02:32<02:13,  3.30it/s] 44%|████▍     | 346/785 [02:33<02:11,  3.33it/s] 44%|████▍     | 347/785 [02:33<02:10,  3.36it/s] 44%|████▍     | 348/785 [02:33<02:09,  3.38it/s] 44%|████▍     | 349/785 [02:33<02:08,  3.40it/s] 45%|████▍     | 350/785 [02:34<02:07,  3.41it/s] 45%|████▍     | 351/785 [02:34<02:07,  3.41it/s] 45%|████▍     | 352/785 [02:34<02:14,  3.23it/s] 45%|████▍     | 353/785 [02:35<02:11,  3.29it/s] 45%|████▌     | 354/785 [02:35<02:09,  3.33it/s] 45%|████▌     | 355/785 [02:35<02:08,  3.36it/s] 45%|████▌     | 356/785 [02:36<02:06,  3.38it/s] 45%|████▌     | 357/785 [02:36<02:06,  3.39it/s] 46%|████▌     | 358/785 [02:36<02:05,  3.40it/s] 46%|████▌     | 359/785 [02:36<02:05,  3.41it/s] 46%|████▌     | 360/785 [02:37<02:04,  3.41it/s] 46%|████▌     | 361/785 [02:37<02:04,  3.42it/s] 46%|████▌     | 362/785 [02:37<02:03,  3.42it/s] 46%|████▌     | 363/785 [02:38<02:10,  3.24it/s] 46%|████▋     | 364/785 [02:38<02:07,  3.29it/s] 46%|████▋     | 365/785 [02:38<02:05,  3.33it/s] 47%|████▋     | 366/785 [02:39<02:04,  3.36it/s] 47%|████▋     | 367/785 [02:39<02:03,  3.38it/s] 47%|████▋     | 368/785 [02:39<02:02,  3.40it/s] 47%|████▋     | 369/785 [02:39<02:02,  3.40it/s] 47%|████▋     | 370/785 [02:40<02:01,  3.41it/s] 47%|████▋     | 371/785 [02:40<02:01,  3.42it/s] 47%|████▋     | 372/785 [02:40<02:00,  3.42it/s] 48%|████▊     | 373/785 [02:41<02:00,  3.42it/s] 48%|████▊     | 374/785 [02:41<02:05,  3.27it/s] 48%|████▊     | 375/785 [02:41<02:03,  3.32it/s] 48%|████▊     | 376/785 [02:42<02:02,  3.35it/s] 48%|████▊     | 377/785 [02:42<02:01,  3.37it/s] 48%|████▊     | 378/785 [02:42<02:00,  3.39it/s] 48%|████▊     | 379/785 [02:42<01:59,  3.40it/s] 48%|████▊     | 380/785 [02:43<01:59,  3.40it/s] 49%|████▊     | 381/785 [02:43<01:58,  3.41it/s] 49%|████▊     | 382/785 [02:43<01:58,  3.41it/s] 49%|████▉     | 383/785 [02:44<01:57,  3.42it/s] 49%|████▉     | 384/785 [02:44<01:57,  3.42it/s] 49%|████▉     | 385/785 [02:44<02:05,  3.18it/s] 49%|████▉     | 386/785 [02:45<02:02,  3.25it/s] 49%|████▉     | 387/785 [02:45<02:00,  3.31it/s] 49%|████▉     | 388/785 [02:45<01:58,  3.34it/s] 50%|████▉     | 389/785 [02:45<01:57,  3.37it/s] 50%|████▉     | 390/785 [02:46<01:56,  3.39it/s] 50%|████▉     | 391/785 [02:46<01:55,  3.40it/s] 50%|████▉     | 392/785 [02:46<01:55,  3.41it/s] 50%|█████     | 393/785 [02:47<01:54,  3.42it/s] 50%|█████     | 394/785 [02:47<01:54,  3.42it/s] 50%|█████     | 395/785 [02:47<01:53,  3.42it/s] 50%|█████     | 396/785 [02:47<01:59,  3.26it/s] 51%|█████     | 397/785 [02:48<01:57,  3.31it/s] 51%|█████     | 398/785 [02:48<01:55,  3.34it/s] 51%|█████     | 399/785 [02:48<01:54,  3.37it/s] 51%|█████     | 400/785 [02:49<01:53,  3.38it/s] 51%|█████     | 401/785 [02:49<01:53,  3.40it/s] 51%|█████     | 402/785 [02:49<01:52,  3.40it/s] 51%|█████▏    | 403/785 [02:50<01:52,  3.41it/s] 51%|█████▏    | 404/785 [02:50<01:51,  3.42it/s] 52%|█████▏    | 405/785 [02:50<01:51,  3.42it/s] 52%|█████▏    | 406/785 [02:50<01:50,  3.42it/s] 52%|█████▏    | 407/785 [02:51<01:53,  3.34it/s] 52%|█████▏    | 408/785 [02:51<01:52,  3.36it/s] 52%|█████▏    | 409/785 [02:51<01:51,  3.38it/s] 52%|█████▏    | 410/785 [02:52<01:50,  3.39it/s] 52%|█████▏    | 411/785 [02:52<01:49,  3.40it/s] 52%|█████▏    | 412/785 [02:52<01:49,  3.41it/s] 53%|█████▎    | 413/785 [02:52<01:48,  3.41it/s] 53%|█████▎    | 414/785 [02:53<01:48,  3.42it/s] 53%|█████▎    | 415/785 [02:53<01:48,  3.42it/s] 53%|█████▎    | 416/785 [02:53<01:47,  3.42it/s] 53%|█████▎    | 417/785 [02:54<01:47,  3.42it/s] 53%|█████▎    | 418/785 [02:54<01:47,  3.42it/s] 53%|█████▎    | 419/785 [02:54<01:46,  3.43it/s] 54%|█████▎    | 420/785 [02:55<01:46,  3.42it/s] 54%|█████▎    | 421/785 [02:55<01:46,  3.43it/s] 54%|█████▍    | 422/785 [02:55<01:46,  3.42it/s] 54%|█████▍    | 423/785 [02:55<01:49,  3.29it/s] 54%|█████▍    | 424/785 [02:56<01:48,  3.33it/s] 54%|█████▍    | 425/785 [02:56<01:47,  3.36it/s] 54%|█████▍    | 426/785 [02:56<01:46,  3.38it/s] 54%|█████▍    | 427/785 [02:57<01:45,  3.39it/s] 55%|█████▍    | 428/785 [02:57<01:45,  3.40it/s] 55%|█████▍    | 429/785 [02:57<01:44,  3.41it/s] 55%|█████▍    | 430/785 [02:57<01:44,  3.41it/s] 55%|█████▍    | 431/785 [02:58<01:43,  3.42it/s] 55%|█████▌    | 432/785 [02:58<01:43,  3.42it/s] 55%|█████▌    | 433/785 [02:58<01:42,  3.42it/s] 55%|█████▌    | 434/785 [02:59<01:46,  3.28it/s] 55%|█████▌    | 435/785 [02:59<01:45,  3.32it/s] 56%|█████▌    | 436/785 [02:59<01:44,  3.35it/s] 56%|█████▌    | 437/785 [03:00<01:43,  3.38it/s] 56%|█████▌    | 438/785 [03:00<01:42,  3.39it/s] 56%|█████▌    | 439/785 [03:00<01:41,  3.40it/s] 56%|█████▌    | 440/785 [03:00<01:41,  3.41it/s] 56%|█████▌    | 441/785 [03:01<01:40,  3.41it/s] 56%|█████▋    | 442/785 [03:01<01:40,  3.42it/s] 56%|█████▋    | 443/785 [03:01<01:40,  3.42it/s] 57%|█████▋    | 444/785 [03:02<01:39,  3.42it/s] 57%|█████▋    | 445/785 [03:02<01:42,  3.32it/s] 57%|█████▋    | 446/785 [03:02<01:41,  3.35it/s] 57%|█████▋    | 447/785 [03:03<01:40,  3.37it/s] 57%|█████▋    | 448/785 [03:03<01:39,  3.39it/s] 57%|█████▋    | 449/785 [03:03<01:38,  3.40it/s] 57%|█████▋    | 450/785 [03:03<01:38,  3.41it/s] 57%|█████▋    | 451/785 [03:04<01:37,  3.41it/s] 58%|█████▊    | 452/785 [03:04<01:37,  3.42it/s] 58%|█████▊    | 453/785 [03:04<01:37,  3.42it/s] 58%|█████▊    | 454/785 [03:05<01:36,  3.42it/s] 58%|█████▊    | 455/785 [03:05<01:36,  3.42it/s] 58%|█████▊    | 456/785 [03:05<01:38,  3.33it/s] 58%|█████▊    | 457/785 [03:05<01:37,  3.35it/s] 58%|█████▊    | 458/785 [03:06<01:37,  3.37it/s] 58%|█████▊    | 459/785 [03:06<01:36,  3.38it/s] 59%|█████▊    | 460/785 [03:06<01:35,  3.39it/s] 59%|█████▊    | 461/785 [03:07<01:35,  3.40it/s] 59%|█████▉    | 462/785 [03:07<01:34,  3.41it/s] 59%|█████▉    | 463/785 [03:07<01:34,  3.41it/s] 59%|█████▉    | 464/785 [03:07<01:33,  3.42it/s] 59%|█████▉    | 465/785 [03:08<01:33,  3.42it/s] 59%|█████▉    | 466/785 [03:08<01:33,  3.42it/s] 59%|█████▉    | 467/785 [03:08<01:36,  3.30it/s] 60%|█████▉    | 468/785 [03:09<01:34,  3.34it/s] 60%|█████▉    | 469/785 [03:09<01:34,  3.36it/s] 60%|█████▉    | 470/785 [03:09<01:33,  3.37it/s] 60%|██████    | 471/785 [03:10<01:31,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 00:06:26,602 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:06:26,603 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 00:06:26,603 >>   Batch size = 8
{'eval_loss': 0.9075345396995544, 'eval_runtime': 13.4899, 'eval_samples_per_second': 360.565, 'eval_steps_per_second': 45.071, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.84it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.30it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.59it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.74it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.18it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.71it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.49it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.45it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.39it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.48it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.51it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.44it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.29it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.18it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.08it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.05it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.06it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 45.08it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.41it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.53it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.52it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.48it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.39it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.30it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.19it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 45.16it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 45.18it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.34it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.52it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.44it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.42it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.34it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.29it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.12it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 45.15it/s][A
 30%|██▉       | 182/608 [00:03<00:09, 45.16it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.33it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.46it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.58it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.50it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.40it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.22it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 43.67it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.10it/s][A
 37%|███▋      | 227/608 [00:04<00:08, 44.47it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 44.76it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 42.27it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 43.81it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.43it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.79it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.79it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.92it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.05it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.20it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.26it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.12it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.23it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.41it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.44it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.36it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.26it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.35it/s][A
 52%|█████▏    | 317/608 [00:06<00:06, 45.45it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.32it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.29it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.30it/s][A
 55%|█████▌    | 337/608 [00:07<00:05, 45.50it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.48it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.39it/s][A
 58%|█████▊    | 352/608 [00:07<00:06, 41.30it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 42.69it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 43.54it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.20it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.52it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.93it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 45.08it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.22it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.93it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 44.76it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.97it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 45.23it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.43it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.48it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.53it/s][A
 70%|███████   | 427/608 [00:09<00:03, 45.51it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.44it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.11it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.00it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.14it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 45.22it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 45.43it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.57it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.65it/s][A
 78%|███████▊  | 472/608 [00:10<00:02, 45.56it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.35it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.20it/s][A
 80%|████████  | 487/608 [00:10<00:02, 42.88it/s][A
 81%|████████  | 492/608 [00:10<00:02, 43.60it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.25it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 44.60it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.98it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.16it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.39it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.26it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.06it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.08it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.15it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 45.27it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.38it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.52it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.57it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.51it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.34it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.18it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.07it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.25it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.31it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.40it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 45.29it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 45.43it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.38it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [03:23<01:31,  3.43it/s]
100%|██████████| 608/608 [00:13<00:00, 45.38it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:06:40,253 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 00:06:40,448 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:06:44,711 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:06:44,932 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:06:45,061 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:39<46:55,  9.00s/it] 60%|██████    | 473/785 [03:39<33:19,  6.41s/it] 60%|██████    | 474/785 [03:40<23:42,  4.57s/it] 61%|██████    | 475/785 [03:40<16:59,  3.29s/it] 61%|██████    | 476/785 [03:40<12:18,  2.39s/it] 61%|██████    | 477/785 [03:40<09:02,  1.76s/it] 61%|██████    | 478/785 [03:41<06:45,  1.32s/it] 61%|██████    | 479/785 [03:41<05:09,  1.01s/it] 61%|██████    | 480/785 [03:41<04:02,  1.26it/s] 61%|██████▏   | 481/785 [03:42<03:15,  1.55it/s] 61%|██████▏   | 482/785 [03:42<02:43,  1.86it/s] 62%|██████▏   | 483/785 [03:42<02:20,  2.16it/s] 62%|██████▏   | 484/785 [03:42<02:06,  2.38it/s] 62%|██████▏   | 485/785 [03:43<01:54,  2.62it/s] 62%|██████▏   | 486/785 [03:43<01:46,  2.82it/s] 62%|██████▏   | 487/785 [03:43<01:40,  2.98it/s] 62%|██████▏   | 488/785 [03:44<01:35,  3.10it/s] 62%|██████▏   | 489/785 [03:44<01:32,  3.20it/s] 62%|██████▏   | 490/785 [03:44<01:30,  3.26it/s] 63%|██████▎   | 491/785 [03:45<01:28,  3.31it/s] 63%|██████▎   | 492/785 [03:45<01:27,  3.35it/s] 63%|██████▎   | 493/785 [03:45<01:26,  3.37it/s] 63%|██████▎   | 494/785 [03:45<01:25,  3.39it/s] 63%|██████▎   | 495/785 [03:46<01:28,  3.29it/s] 63%|██████▎   | 496/785 [03:46<01:26,  3.33it/s] 63%|██████▎   | 497/785 [03:46<01:25,  3.36it/s] 63%|██████▎   | 498/785 [03:47<01:24,  3.38it/s] 64%|██████▎   | 499/785 [03:47<01:24,  3.39it/s] 64%|██████▎   | 500/785 [03:50<05:40,  1.19s/it]                                                  64%|██████▎   | 500/785 [03:50<05:40,  1.19s/it] 64%|██████▍   | 501/785 [03:51<04:30,  1.05it/s] 64%|██████▍   | 502/785 [03:51<03:33,  1.32it/s] 64%|██████▍   | 503/785 [03:51<02:53,  1.62it/s] 64%|██████▍   | 504/785 [03:51<02:25,  1.93it/s] 64%|██████▍   | 505/785 [03:52<02:06,  2.22it/s] 64%|██████▍   | 506/785 [03:52<01:52,  2.49it/s] 65%|██████▍   | 507/785 [03:52<01:42,  2.71it/s] 65%|██████▍   | 508/785 [03:53<01:35,  2.89it/s] 65%|██████▍   | 509/785 [03:53<01:30,  3.04it/s] 65%|██████▍   | 510/785 [03:53<01:27,  3.14it/s] 65%|██████▌   | 511/785 [03:54<01:28,  3.10it/s] 65%|██████▌   | 512/785 [03:54<01:25,  3.20it/s] 65%|██████▌   | 513/785 [03:54<01:23,  3.26it/s] 65%|██████▌   | 514/785 [03:54<01:21,  3.31it/s] 66%|██████▌   | 515/785 [03:55<01:20,  3.35it/s] 66%|██████▌   | 516/785 [03:55<01:19,  3.37it/s] 66%|██████▌   | 517/785 [03:55<01:19,  3.39it/s] 66%|██████▌   | 518/785 [03:56<01:18,  3.40it/s] 66%|██████▌   | 519/785 [03:56<01:18,  3.41it/s] 66%|██████▌   | 520/785 [03:56<01:17,  3.41it/s] 66%|██████▋   | 521/785 [03:56<01:17,  3.42it/s] 66%|██████▋   | 522/785 [03:57<01:20,  3.28it/s] 67%|██████▋   | 523/785 [03:57<01:18,  3.32it/s] 67%|██████▋   | 524/785 [03:57<01:17,  3.36it/s] 67%|██████▋   | 525/785 [03:58<01:16,  3.38it/s] 67%|██████▋   | 526/785 [03:58<01:16,  3.40it/s] 67%|██████▋   | 527/785 [03:58<01:15,  3.40it/s] 67%|██████▋   | 528/785 [03:59<01:15,  3.41it/s] 67%|██████▋   | 529/785 [03:59<01:14,  3.42it/s] 68%|██████▊   | 530/785 [03:59<01:14,  3.42it/s] 68%|██████▊   | 531/785 [03:59<01:14,  3.43it/s] 68%|██████▊   | 532/785 [04:00<01:13,  3.42it/s] 68%|██████▊   | 533/785 [04:00<01:15,  3.35it/s] 68%|██████▊   | 534/785 [04:00<01:14,  3.37it/s] 68%|██████▊   | 535/785 [04:01<01:13,  3.39it/s] 68%|██████▊   | 536/785 [04:01<01:13,  3.40it/s] 68%|██████▊   | 537/785 [04:01<01:12,  3.41it/s] 69%|██████▊   | 538/785 [04:01<01:12,  3.41it/s] 69%|██████▊   | 539/785 [04:02<01:11,  3.42it/s] 69%|██████▉   | 540/785 [04:02<01:11,  3.42it/s] 69%|██████▉   | 541/785 [04:02<01:11,  3.43it/s] 69%|██████▉   | 542/785 [04:03<01:10,  3.43it/s] 69%|██████▉   | 543/785 [04:03<01:10,  3.43it/s] 69%|██████▉   | 544/785 [04:03<01:15,  3.17it/s] 69%|██████▉   | 545/785 [04:04<01:13,  3.25it/s] 70%|██████▉   | 546/785 [04:04<01:12,  3.30it/s] 70%|██████▉   | 547/785 [04:04<01:11,  3.34it/s] 70%|██████▉   | 548/785 [04:04<01:10,  3.37it/s] 70%|██████▉   | 549/785 [04:05<01:09,  3.38it/s] 70%|███████   | 550/785 [04:05<01:09,  3.40it/s] 70%|███████   | 551/785 [04:05<01:08,  3.41it/s] 70%|███████   | 552/785 [04:06<01:08,  3.41it/s] 70%|███████   | 553/785 [04:06<01:07,  3.42it/s] 71%|███████   | 554/785 [04:06<01:07,  3.42it/s] 71%|███████   | 555/785 [04:07<01:10,  3.24it/s] 71%|███████   | 556/785 [04:07<01:09,  3.29it/s] 71%|███████   | 557/785 [04:07<01:08,  3.33it/s] 71%|███████   | 558/785 [04:07<01:07,  3.36it/s] 71%|███████   | 559/785 [04:08<01:06,  3.38it/s] 71%|███████▏  | 560/785 [04:08<01:06,  3.39it/s] 71%|███████▏  | 561/785 [04:08<01:05,  3.40it/s] 72%|███████▏  | 562/785 [04:09<01:05,  3.41it/s] 72%|███████▏  | 563/785 [04:09<01:05,  3.41it/s] 72%|███████▏  | 564/785 [04:09<01:04,  3.42it/s] 72%|███████▏  | 565/785 [04:09<01:04,  3.42it/s] 72%|███████▏  | 566/785 [04:10<01:04,  3.38it/s] 72%|███████▏  | 567/785 [04:10<01:04,  3.40it/s] 72%|███████▏  | 568/785 [04:10<01:03,  3.41it/s] 72%|███████▏  | 569/785 [04:11<01:03,  3.41it/s] 73%|███████▎  | 570/785 [04:11<01:02,  3.41it/s] 73%|███████▎  | 571/785 [04:11<01:02,  3.42it/s] 73%|███████▎  | 572/785 [04:12<01:02,  3.42it/s] 73%|███████▎  | 573/785 [04:12<01:01,  3.42it/s] 73%|███████▎  | 574/785 [04:12<01:01,  3.42it/s] 73%|███████▎  | 575/785 [04:12<01:01,  3.42it/s] 73%|███████▎  | 576/785 [04:13<01:00,  3.43it/s] 74%|███████▎  | 577/785 [04:13<01:02,  3.33it/s] 74%|███████▎  | 578/785 [04:13<01:01,  3.36it/s] 74%|███████▍  | 579/785 [04:14<01:00,  3.38it/s] 74%|███████▍  | 580/785 [04:14<01:00,  3.39it/s] 74%|███████▍  | 581/785 [04:14<00:59,  3.40it/s] 74%|███████▍  | 582/785 [04:14<00:59,  3.41it/s] 74%|███████▍  | 583/785 [04:15<00:59,  3.41it/s] 74%|███████▍  | 584/785 [04:15<00:58,  3.42it/s] 75%|███████▍  | 585/785 [04:15<00:58,  3.42it/s] 75%|███████▍  | 586/785 [04:16<00:58,  3.42it/s] 75%|███████▍  | 587/785 [04:16<00:57,  3.42it/s] 75%|███████▍  | 588/785 [04:16<00:58,  3.35it/s] 75%|███████▌  | 589/785 [04:17<00:58,  3.38it/s] 75%|███████▌  | 590/785 [04:17<00:57,  3.39it/s] 75%|███████▌  | 591/785 [04:17<00:57,  3.40it/s] 75%|███████▌  | 592/785 [04:17<00:56,  3.41it/s] 76%|███████▌  | 593/785 [04:18<00:56,  3.41it/s] 76%|███████▌  | 594/785 [04:18<00:56,  3.41it/s] 76%|███████▌  | 595/785 [04:18<00:55,  3.42it/s] 76%|███████▌  | 596/785 [04:19<00:55,  3.42it/s] 76%|███████▌  | 597/785 [04:19<00:54,  3.42it/s] 76%|███████▌  | 598/785 [04:19<00:54,  3.42it/s] 76%|███████▋  | 599/785 [04:19<00:55,  3.33it/s] 76%|███████▋  | 600/785 [04:20<00:55,  3.36it/s] 77%|███████▋  | 601/785 [04:20<00:54,  3.38it/s] 77%|███████▋  | 602/785 [04:20<00:53,  3.39it/s] 77%|███████▋  | 603/785 [04:21<00:53,  3.40it/s] 77%|███████▋  | 604/785 [04:21<00:53,  3.41it/s] 77%|███████▋  | 605/785 [04:21<00:52,  3.41it/s] 77%|███████▋  | 606/785 [04:22<00:52,  3.41it/s] 77%|███████▋  | 607/785 [04:22<00:52,  3.41it/s] 77%|███████▋  | 608/785 [04:22<00:51,  3.42it/s] 78%|███████▊  | 609/785 [04:22<00:51,  3.42it/s] 78%|███████▊  | 610/785 [04:23<00:51,  3.38it/s] 78%|███████▊  | 611/785 [04:23<00:51,  3.39it/s] 78%|███████▊  | 612/785 [04:23<00:50,  3.40it/s] 78%|███████▊  | 613/785 [04:24<00:50,  3.41it/s] 78%|███████▊  | 614/785 [04:24<00:50,  3.41it/s] 78%|███████▊  | 615/785 [04:24<00:49,  3.42it/s] 78%|███████▊  | 616/785 [04:24<00:49,  3.42it/s] 79%|███████▊  | 617/785 [04:25<00:49,  3.42it/s] 79%|███████▊  | 618/785 [04:25<00:48,  3.42it/s] 79%|███████▉  | 619/785 [04:25<00:48,  3.42it/s] 79%|███████▉  | 620/785 [04:26<00:48,  3.42it/s] 79%|███████▉  | 621/785 [04:26<00:47,  3.42it/s] 79%|███████▉  | 622/785 [04:26<00:47,  3.42it/s] 79%|███████▉  | 623/785 [04:27<00:47,  3.43it/s] 79%|███████▉  | 624/785 [04:27<00:47,  3.42it/s] 80%|███████▉  | 625/785 [04:27<00:46,  3.43it/s] 80%|███████▉  | 626/785 [04:27<00:49,  3.23it/s] 80%|███████▉  | 627/785 [04:28<00:50,  3.10it/s] 80%|████████  | 628/785 [04:28<00:53,  2.95it/s][INFO|trainer.py:2140] 2023-08-28 00:07:46,195 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:07:46,195 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 00:07:46,195 >>   Batch size = 8
{'eval_loss': 0.9134320616722107, 'eval_runtime': 13.4951, 'eval_samples_per_second': 360.428, 'eval_steps_per_second': 45.054, 'epoch': 3.0}
{'loss': 0.7517, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 7/608 [00:00<00:10, 58.93it/s][A
  2%|▏         | 13/608 [00:00<00:11, 51.12it/s][A
  3%|▎         | 19/608 [00:00<00:12, 48.86it/s][A
  4%|▍         | 24/608 [00:00<00:12, 47.78it/s][A
  5%|▍         | 29/608 [00:00<00:12, 47.17it/s][A
  6%|▌         | 34/608 [00:00<00:12, 46.79it/s][A
  6%|▋         | 39/608 [00:00<00:12, 46.40it/s][A
  7%|▋         | 44/608 [00:00<00:13, 40.81it/s][A
  8%|▊         | 49/608 [00:01<00:28, 19.50it/s][A
  9%|▉         | 54/608 [00:01<00:23, 23.71it/s][A
 10%|▉         | 59/608 [00:01<00:19, 27.77it/s][A
 11%|█         | 64/608 [00:01<00:17, 31.56it/s][A
 11%|█▏        | 69/608 [00:01<00:15, 34.83it/s][A
 12%|█▏        | 74/608 [00:02<00:14, 37.57it/s][A
 13%|█▎        | 79/608 [00:02<00:13, 39.70it/s][A
 14%|█▍        | 84/608 [00:02<00:12, 41.41it/s][A
 15%|█▍        | 89/608 [00:02<00:12, 42.31it/s][A
 15%|█▌        | 94/608 [00:02<00:11, 42.92it/s][A
 16%|█▋        | 99/608 [00:02<00:11, 43.52it/s][A
 17%|█▋        | 104/608 [00:02<00:11, 44.16it/s][A
 18%|█▊        | 109/608 [00:02<00:11, 44.70it/s][A
 19%|█▉        | 114/608 [00:02<00:10, 45.06it/s][A
 20%|█▉        | 119/608 [00:03<00:10, 45.28it/s][A
 20%|██        | 124/608 [00:03<00:10, 45.46it/s][A
 21%|██        | 129/608 [00:03<00:10, 45.37it/s][A
 22%|██▏       | 134/608 [00:03<00:10, 45.17it/s][A
 23%|██▎       | 139/608 [00:03<00:10, 45.09it/s][A
 24%|██▎       | 144/608 [00:03<00:10, 45.11it/s][A
 25%|██▍       | 149/608 [00:03<00:10, 45.16it/s][A
 25%|██▌       | 154/608 [00:03<00:10, 45.36it/s][A
 26%|██▌       | 159/608 [00:03<00:10, 41.68it/s][A
 27%|██▋       | 164/608 [00:04<00:10, 42.85it/s][A
 28%|██▊       | 169/608 [00:04<00:10, 43.69it/s][A
 29%|██▊       | 174/608 [00:04<00:09, 44.32it/s][A
 29%|██▉       | 179/608 [00:04<00:09, 44.79it/s][A
 30%|███       | 184/608 [00:04<00:09, 45.09it/s][A
 31%|███       | 189/608 [00:04<00:09, 45.13it/s][A
 32%|███▏      | 194/608 [00:04<00:09, 45.31it/s][A
 33%|███▎      | 199/608 [00:04<00:09, 45.14it/s][A
 34%|███▎      | 204/608 [00:04<00:08, 45.09it/s][A
 34%|███▍      | 209/608 [00:05<00:08, 45.30it/s][A
 35%|███▌      | 214/608 [00:05<00:08, 45.35it/s][A
 36%|███▌      | 219/608 [00:05<00:08, 45.50it/s][A
 37%|███▋      | 224/608 [00:05<00:08, 45.59it/s][A
 38%|███▊      | 229/608 [00:05<00:08, 45.57it/s][A
 38%|███▊      | 234/608 [00:05<00:08, 45.60it/s][A
 39%|███▉      | 239/608 [00:05<00:08, 45.46it/s][A
 40%|████      | 244/608 [00:05<00:08, 45.35it/s][A
 41%|████      | 249/608 [00:05<00:07, 45.33it/s][A
 42%|████▏     | 254/608 [00:06<00:07, 45.40it/s][A
 43%|████▎     | 259/608 [00:06<00:07, 45.46it/s][A
 43%|████▎     | 264/608 [00:06<00:07, 45.58it/s][A
 44%|████▍     | 269/608 [00:06<00:07, 45.56it/s][A
 45%|████▌     | 274/608 [00:06<00:07, 45.61it/s][A
 46%|████▌     | 279/608 [00:06<00:07, 45.55it/s][A
 47%|████▋     | 284/608 [00:06<00:07, 45.47it/s][A
 48%|████▊     | 289/608 [00:06<00:07, 45.37it/s][A
 48%|████▊     | 294/608 [00:06<00:07, 42.95it/s][A
 49%|████▉     | 299/608 [00:07<00:07, 43.78it/s][A
 50%|█████     | 304/608 [00:07<00:06, 44.37it/s][A
 51%|█████     | 309/608 [00:07<00:06, 44.83it/s][A
 52%|█████▏    | 314/608 [00:07<00:06, 45.13it/s][A
 52%|█████▏    | 319/608 [00:07<00:06, 45.29it/s][A
 53%|█████▎    | 324/608 [00:07<00:06, 45.37it/s][A
 54%|█████▍    | 329/608 [00:07<00:06, 45.30it/s][A
 55%|█████▍    | 334/608 [00:07<00:06, 45.06it/s][A
 56%|█████▌    | 339/608 [00:07<00:05, 45.09it/s][A
 57%|█████▋    | 344/608 [00:08<00:05, 45.16it/s][A
 57%|█████▋    | 349/608 [00:08<00:05, 45.33it/s][A
 58%|█████▊    | 354/608 [00:08<00:05, 45.52it/s][A
 59%|█████▉    | 359/608 [00:08<00:05, 45.50it/s][A
 60%|█████▉    | 364/608 [00:08<00:05, 45.65it/s][A
 61%|██████    | 369/608 [00:08<00:05, 45.58it/s][A
 62%|██████▏   | 374/608 [00:08<00:05, 45.65it/s][A
 62%|██████▏   | 379/608 [00:08<00:05, 45.43it/s][A
 63%|██████▎   | 384/608 [00:08<00:04, 45.32it/s][A
 64%|██████▍   | 389/608 [00:09<00:04, 45.34it/s][A
 65%|██████▍   | 394/608 [00:09<00:04, 45.48it/s][A
 66%|██████▌   | 399/608 [00:09<00:04, 45.54it/s][A
 66%|██████▋   | 404/608 [00:09<00:04, 45.62it/s][A
 67%|██████▋   | 409/608 [00:09<00:04, 45.55it/s][A
 68%|██████▊   | 414/608 [00:09<00:04, 45.63it/s][A
 69%|██████▉   | 419/608 [00:09<00:04, 45.56it/s][A
 70%|██████▉   | 424/608 [00:09<00:04, 45.42it/s][A
 71%|███████   | 429/608 [00:09<00:03, 45.32it/s][A
 71%|███████▏  | 434/608 [00:10<00:03, 44.37it/s][A
 72%|███████▏  | 439/608 [00:10<00:03, 44.87it/s][A
 73%|███████▎  | 444/608 [00:10<00:03, 45.10it/s][A
 74%|███████▍  | 449/608 [00:10<00:03, 45.19it/s][A
 75%|███████▍  | 454/608 [00:10<00:03, 45.36it/s][A
 75%|███████▌  | 459/608 [00:10<00:03, 45.48it/s][A
 76%|███████▋  | 464/608 [00:10<00:03, 45.47it/s][A
 77%|███████▋  | 469/608 [00:10<00:03, 45.30it/s][A
 78%|███████▊  | 474/608 [00:10<00:02, 45.14it/s][A
 79%|███████▉  | 479/608 [00:11<00:02, 45.24it/s][A
 80%|███████▉  | 484/608 [00:11<00:02, 45.36it/s][A
 80%|████████  | 489/608 [00:11<00:02, 45.50it/s][A
 81%|████████▏ | 494/608 [00:11<00:02, 45.47it/s][A
 82%|████████▏ | 499/608 [00:11<00:02, 45.61it/s][A
 83%|████████▎ | 504/608 [00:11<00:02, 45.53it/s][A
 84%|████████▎ | 509/608 [00:11<00:02, 45.57it/s][A
 85%|████████▍ | 514/608 [00:11<00:02, 45.38it/s][A
 85%|████████▌ | 519/608 [00:11<00:01, 45.32it/s][A
 86%|████████▌ | 524/608 [00:12<00:01, 45.26it/s][A
 87%|████████▋ | 529/608 [00:12<00:01, 45.30it/s][A
 88%|████████▊ | 534/608 [00:12<00:01, 45.35it/s][A
 89%|████████▊ | 539/608 [00:12<00:01, 45.51it/s][A
 89%|████████▉ | 544/608 [00:12<00:01, 45.59it/s][A
 90%|█████████ | 549/608 [00:12<00:01, 45.66it/s][A
 91%|█████████ | 554/608 [00:12<00:01, 45.61it/s][A
 92%|█████████▏| 559/608 [00:12<00:01, 45.63it/s][A
 93%|█████████▎| 564/608 [00:12<00:00, 45.49it/s][A
 94%|█████████▎| 569/608 [00:13<00:00, 45.47it/s][A
 94%|█████████▍| 574/608 [00:13<00:00, 43.04it/s][A
 95%|█████████▌| 579/608 [00:13<00:00, 43.98it/s][A
 96%|█████████▌| 584/608 [00:13<00:00, 44.46it/s][A
 97%|█████████▋| 589/608 [00:13<00:00, 44.84it/s][A
 98%|█████████▊| 594/608 [00:13<00:00, 45.08it/s][A
 99%|█████████▊| 599/608 [00:13<00:00, 45.22it/s][A
 99%|█████████▉| 604/608 [00:13<00:00, 45.32it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:43<00:53,  2.95it/s]
100%|██████████| 608/608 [00:13<00:00, 45.32it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:08:00,371 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 00:08:00,481 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:08:03,410 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:08:03,595 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:08:03,679 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:55<21:49,  8.39s/it] 80%|████████  | 630/785 [04:56<15:24,  5.96s/it] 80%|████████  | 631/785 [04:56<10:56,  4.26s/it] 81%|████████  | 632/785 [04:56<07:49,  3.07s/it] 81%|████████  | 633/785 [04:57<05:39,  2.24s/it] 81%|████████  | 634/785 [04:57<04:09,  1.65s/it] 81%|████████  | 635/785 [04:57<03:06,  1.24s/it] 81%|████████  | 636/785 [04:57<02:22,  1.04it/s] 81%|████████  | 637/785 [04:58<01:52,  1.32it/s] 81%|████████▏ | 638/785 [04:58<01:30,  1.62it/s] 81%|████████▏ | 639/785 [04:58<01:17,  1.88it/s] 82%|████████▏ | 640/785 [04:59<01:06,  2.17it/s] 82%|████████▏ | 641/785 [04:59<00:58,  2.44it/s] 82%|████████▏ | 642/785 [04:59<00:53,  2.67it/s] 82%|████████▏ | 643/785 [04:59<00:49,  2.87it/s] 82%|████████▏ | 644/785 [05:00<00:46,  3.02it/s] 82%|████████▏ | 645/785 [05:00<00:44,  3.13it/s] 82%|████████▏ | 646/785 [05:00<00:43,  3.22it/s] 82%|████████▏ | 647/785 [05:01<00:42,  3.28it/s] 83%|████████▎ | 648/785 [05:01<00:41,  3.32it/s] 83%|████████▎ | 649/785 [05:01<00:40,  3.35it/s] 83%|████████▎ | 650/785 [05:02<00:40,  3.30it/s] 83%|████████▎ | 651/785 [05:02<00:40,  3.34it/s] 83%|████████▎ | 652/785 [05:02<00:39,  3.37it/s] 83%|████████▎ | 653/785 [05:02<00:38,  3.39it/s] 83%|████████▎ | 654/785 [05:03<00:38,  3.40it/s] 83%|████████▎ | 655/785 [05:03<00:38,  3.41it/s] 84%|████████▎ | 656/785 [05:03<00:37,  3.42it/s] 84%|████████▎ | 657/785 [05:04<00:37,  3.42it/s] 84%|████████▍ | 658/785 [05:04<00:37,  3.43it/s] 84%|████████▍ | 659/785 [05:04<00:36,  3.43it/s] 84%|████████▍ | 660/785 [05:04<00:36,  3.43it/s] 84%|████████▍ | 661/785 [05:05<00:37,  3.33it/s] 84%|████████▍ | 662/785 [05:05<00:36,  3.36it/s] 84%|████████▍ | 663/785 [05:05<00:36,  3.38it/s] 85%|████████▍ | 664/785 [05:06<00:35,  3.40it/s] 85%|████████▍ | 665/785 [05:06<00:35,  3.41it/s] 85%|████████▍ | 666/785 [05:06<00:34,  3.42it/s] 85%|████████▍ | 667/785 [05:07<00:34,  3.42it/s] 85%|████████▌ | 668/785 [05:07<00:34,  3.43it/s] 85%|████████▌ | 669/785 [05:07<00:33,  3.43it/s] 85%|████████▌ | 670/785 [05:07<00:33,  3.43it/s] 85%|████████▌ | 671/785 [05:08<00:33,  3.43it/s] 86%|████████▌ | 672/785 [05:08<00:33,  3.33it/s] 86%|████████▌ | 673/785 [05:08<00:33,  3.36it/s] 86%|████████▌ | 674/785 [05:09<00:32,  3.38it/s] 86%|████████▌ | 675/785 [05:09<00:32,  3.40it/s] 86%|████████▌ | 676/785 [05:09<00:32,  3.40it/s] 86%|████████▌ | 677/785 [05:09<00:31,  3.41it/s] 86%|████████▋ | 678/785 [05:10<00:31,  3.42it/s] 86%|████████▋ | 679/785 [05:10<00:30,  3.43it/s] 87%|████████▋ | 680/785 [05:10<00:30,  3.43it/s] 87%|████████▋ | 681/785 [05:11<00:30,  3.42it/s] 87%|████████▋ | 682/785 [05:11<00:30,  3.42it/s] 87%|████████▋ | 683/785 [05:11<00:30,  3.30it/s] 87%|████████▋ | 684/785 [05:12<00:30,  3.34it/s] 87%|████████▋ | 685/785 [05:12<00:29,  3.37it/s] 87%|████████▋ | 686/785 [05:12<00:29,  3.39it/s] 88%|████████▊ | 687/785 [05:12<00:28,  3.40it/s] 88%|████████▊ | 688/785 [05:13<00:28,  3.41it/s] 88%|████████▊ | 689/785 [05:13<00:28,  3.42it/s] 88%|████████▊ | 690/785 [05:13<00:27,  3.43it/s] 88%|████████▊ | 691/785 [05:14<00:27,  3.43it/s] 88%|████████▊ | 692/785 [05:14<00:27,  3.43it/s] 88%|████████▊ | 693/785 [05:14<00:26,  3.43it/s] 88%|████████▊ | 694/785 [05:14<00:27,  3.33it/s] 89%|████████▊ | 695/785 [05:15<00:26,  3.36it/s] 89%|████████▊ | 696/785 [05:15<00:26,  3.37it/s] 89%|████████▉ | 697/785 [05:15<00:25,  3.39it/s] 89%|████████▉ | 698/785 [05:16<00:25,  3.39it/s] 89%|████████▉ | 699/785 [05:16<00:25,  3.40it/s] 89%|████████▉ | 700/785 [05:16<00:24,  3.41it/s] 89%|████████▉ | 701/785 [05:17<00:24,  3.42it/s] 89%|████████▉ | 702/785 [05:17<00:24,  3.42it/s] 90%|████████▉ | 703/785 [05:17<00:23,  3.43it/s] 90%|████████▉ | 704/785 [05:17<00:23,  3.43it/s] 90%|████████▉ | 705/785 [05:18<00:24,  3.29it/s] 90%|████████▉ | 706/785 [05:18<00:23,  3.33it/s] 90%|█████████ | 707/785 [05:18<00:23,  3.36it/s] 90%|█████████ | 708/785 [05:19<00:22,  3.38it/s] 90%|█████████ | 709/785 [05:19<00:22,  3.40it/s] 90%|█████████ | 710/785 [05:19<00:22,  3.41it/s] 91%|█████████ | 711/785 [05:19<00:21,  3.41it/s] 91%|█████████ | 712/785 [05:20<00:21,  3.42it/s] 91%|█████████ | 713/785 [05:20<00:21,  3.42it/s] 91%|█████████ | 714/785 [05:20<00:20,  3.43it/s] 91%|█████████ | 715/785 [05:21<00:20,  3.43it/s] 91%|█████████ | 716/785 [05:21<00:21,  3.26it/s] 91%|█████████▏| 717/785 [05:21<00:20,  3.31it/s] 91%|█████████▏| 718/785 [05:22<00:20,  3.34it/s] 92%|█████████▏| 719/785 [05:22<00:19,  3.37it/s] 92%|█████████▏| 720/785 [05:22<00:19,  3.39it/s] 92%|█████████▏| 721/785 [05:22<00:18,  3.40it/s] 92%|█████████▏| 722/785 [05:23<00:18,  3.41it/s] 92%|█████████▏| 723/785 [05:23<00:18,  3.41it/s] 92%|█████████▏| 724/785 [05:23<00:17,  3.42it/s] 92%|█████████▏| 725/785 [05:24<00:17,  3.42it/s] 92%|█████████▏| 726/785 [05:24<00:17,  3.42it/s] 93%|█████████▎| 727/785 [05:24<00:17,  3.31it/s] 93%|█████████▎| 728/785 [05:25<00:16,  3.36it/s] 93%|█████████▎| 729/785 [05:25<00:16,  3.39it/s] 93%|█████████▎| 730/785 [05:25<00:16,  3.42it/s] 93%|█████████▎| 731/785 [05:25<00:15,  3.44it/s] 93%|█████████▎| 732/785 [05:26<00:15,  3.45it/s] 93%|█████████▎| 733/785 [05:26<00:15,  3.46it/s] 94%|█████████▎| 734/785 [05:26<00:14,  3.47it/s] 94%|█████████▎| 735/785 [05:27<00:14,  3.46it/s] 94%|█████████▍| 736/785 [05:27<00:14,  3.47it/s] 94%|█████████▍| 737/785 [05:27<00:13,  3.47it/s] 94%|█████████▍| 738/785 [05:27<00:13,  3.48it/s] 94%|█████████▍| 739/785 [05:28<00:13,  3.47it/s] 94%|█████████▍| 740/785 [05:28<00:13,  3.25it/s] 94%|█████████▍| 741/785 [05:28<00:13,  3.32it/s] 95%|█████████▍| 742/785 [05:29<00:12,  3.36it/s] 95%|█████████▍| 743/785 [05:29<00:13,  3.18it/s] 95%|█████████▍| 744/785 [05:29<00:12,  3.27it/s] 95%|█████████▍| 745/785 [05:30<00:12,  3.32it/s] 95%|█████████▌| 746/785 [05:30<00:11,  3.37it/s] 95%|█████████▌| 747/785 [05:30<00:11,  3.40it/s] 95%|█████████▌| 748/785 [05:31<00:16,  2.26it/s] 95%|█████████▌| 749/785 [05:31<00:14,  2.53it/s] 96%|█████████▌| 750/785 [05:31<00:12,  2.75it/s] 96%|█████████▌| 751/785 [05:32<00:11,  2.94it/s] 96%|█████████▌| 752/785 [05:32<00:11,  2.93it/s] 96%|█████████▌| 753/785 [05:32<00:10,  3.07it/s] 96%|█████████▌| 754/785 [05:33<00:09,  3.17it/s] 96%|█████████▌| 755/785 [05:33<00:09,  3.24it/s] 96%|█████████▋| 756/785 [05:33<00:08,  3.30it/s] 96%|█████████▋| 757/785 [05:34<00:08,  3.34it/s] 97%|█████████▋| 758/785 [05:34<00:08,  3.37it/s] 97%|█████████▋| 759/785 [05:34<00:07,  3.39it/s] 97%|█████████▋| 760/785 [05:34<00:07,  3.40it/s] 97%|█████████▋| 761/785 [05:35<00:07,  3.41it/s] 97%|█████████▋| 762/785 [05:35<00:06,  3.42it/s] 97%|█████████▋| 763/785 [05:35<00:06,  3.21it/s] 97%|█████████▋| 764/785 [05:36<00:06,  3.29it/s] 97%|█████████▋| 765/785 [05:36<00:05,  3.35it/s] 98%|█████████▊| 766/785 [05:36<00:05,  3.39it/s] 98%|█████████▊| 767/785 [05:37<00:05,  3.42it/s] 98%|█████████▊| 768/785 [05:37<00:04,  3.44it/s] 98%|█████████▊| 769/785 [05:37<00:04,  3.45it/s] 98%|█████████▊| 770/785 [05:37<00:04,  3.46it/s] 98%|█████████▊| 771/785 [05:38<00:04,  3.47it/s] 98%|█████████▊| 772/785 [05:38<00:03,  3.47it/s] 98%|█████████▊| 773/785 [05:38<00:03,  3.47it/s] 99%|█████████▊| 774/785 [05:39<00:03,  3.28it/s] 99%|█████████▊| 775/785 [05:39<00:02,  3.34it/s] 99%|█████████▉| 776/785 [05:39<00:02,  3.38it/s] 99%|█████████▉| 777/785 [05:39<00:02,  3.41it/s] 99%|█████████▉| 778/785 [05:40<00:02,  3.43it/s] 99%|█████████▉| 779/785 [05:40<00:01,  3.45it/s] 99%|█████████▉| 780/785 [05:40<00:01,  3.46it/s] 99%|█████████▉| 781/785 [05:41<00:01,  3.47it/s]100%|█████████▉| 782/785 [05:41<00:00,  3.47it/s]100%|█████████▉| 783/785 [05:41<00:00,  3.48it/s]100%|█████████▉| 784/785 [05:41<00:00,  3.48it/s]100%|██████████| 785/785 [05:42<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 00:08:58,782 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:08:58,782 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 00:08:58,782 >>   Batch size = 8
{'eval_loss': 0.9173946380615234, 'eval_runtime': 14.0416, 'eval_samples_per_second': 346.399, 'eval_steps_per_second': 43.3, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.07it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.08it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.93it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.08it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.46it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.78it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.47it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.23it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.38it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.44it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.61it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.67it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.76it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.72it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.46it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.24it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.26it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.31it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.36it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.41it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.58it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.71it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.65it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.44it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.22it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 44.44it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.81it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 44.93it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.17it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.37it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.54it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.54it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.43it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.15it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.23it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.21it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.38it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.56it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 45.69it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.70it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.68it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.49it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.30it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.12it/s][A
 38%|███▊      | 228/608 [00:04<00:08, 45.24it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.34it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.43it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.54it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.68it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.67it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.55it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.32it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.20it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 42.09it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 43.18it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 44.00it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 44.52it/s][A
 48%|████▊     | 293/608 [00:06<00:07, 44.99it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.18it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.24it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.09it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 44.95it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 44.98it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.18it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.35it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.54it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.65it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.73it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.67it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.39it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.17it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 45.06it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.29it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.42it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.48it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.55it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.64it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.65it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.47it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.29it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.10it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 44.12it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 44.65it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.00it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.22it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.39it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.48it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.35it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.21it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 45.04it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.16it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.34it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.45it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.63it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.68it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.63it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.43it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.30it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 45.09it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.14it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.20it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.40it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.60it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.69it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.69it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.58it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.37it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 45.25it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.16it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 43.25it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 43.96it/s][A
 93%|█████████▎| 563/608 [00:12<00:01, 44.59it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 44.96it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.18it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.30it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.19it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 45.18it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 44.72it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.04it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.35it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.55it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:55<00:00,  3.44it/s]
100%|██████████| 608/608 [00:13<00:00, 45.55it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:09:12,398 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 00:09:12,560 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:09:16,325 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:09:16,690 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:09:16,781 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:09:24,339 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:09:24,366 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314 (score: 0.9075345396995544).
                                                 100%|██████████| 785/785 [06:16<00:00,  3.44it/s]100%|██████████| 785/785 [06:16<00:00,  2.08it/s]
[INFO|trainer.py:1894] 2023-08-28 00:09:33,405 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:09:33,517 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:09:37,320 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:09:37,404 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:09:37,462 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:09:37,925 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:37,926 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:37,926 >>   train_loss               =     0.7387
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:37,926 >>   train_runtime            = 0:06:16.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:37,926 >>   train_samples            =      10044
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:37,926 >>   train_samples_per_second =    133.277
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:37,926 >>   train_steps_per_second   =      2.083
{'eval_loss': 0.9210616946220398, 'eval_runtime': 13.4398, 'eval_samples_per_second': 361.91, 'eval_steps_per_second': 45.239, 'epoch': 5.0}
{'train_runtime': 376.8095, 'train_samples_per_second': 133.277, 'train_steps_per_second': 2.083, 'train_loss': 0.7387272537134255, 'epoch': 5.0}
08/28/2023 00:09:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:09:38,241 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:09:38,241 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 00:09:38,241 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 56.41it/s]  2%|▏         | 12/608 [00:00<00:11, 50.25it/s]  3%|▎         | 18/608 [00:00<00:12, 48.40it/s]  4%|▍         | 23/608 [00:00<00:12, 47.66it/s]  5%|▍         | 28/608 [00:00<00:12, 47.11it/s]  5%|▌         | 33/608 [00:00<00:12, 46.86it/s]  6%|▋         | 38/608 [00:00<00:12, 46.64it/s]  7%|▋         | 43/608 [00:00<00:12, 46.24it/s]  8%|▊         | 48/608 [00:01<00:12, 45.73it/s]  9%|▊         | 53/608 [00:01<00:12, 45.44it/s] 10%|▉         | 58/608 [00:01<00:12, 45.45it/s] 10%|█         | 63/608 [00:01<00:11, 45.59it/s] 11%|█         | 68/608 [00:01<00:11, 45.71it/s] 12%|█▏        | 73/608 [00:01<00:11, 45.84it/s] 13%|█▎        | 78/608 [00:01<00:11, 45.95it/s] 14%|█▎        | 83/608 [00:01<00:11, 46.04it/s] 14%|█▍        | 88/608 [00:01<00:11, 45.88it/s] 15%|█▌        | 93/608 [00:02<00:11, 45.55it/s] 16%|█▌        | 98/608 [00:02<00:11, 45.39it/s] 17%|█▋        | 103/608 [00:02<00:11, 43.45it/s] 18%|█▊        | 108/608 [00:02<00:11, 44.22it/s] 19%|█▊        | 113/608 [00:02<00:11, 44.72it/s] 19%|█▉        | 118/608 [00:02<00:10, 45.15it/s] 20%|██        | 123/608 [00:02<00:10, 45.41it/s] 21%|██        | 128/608 [00:02<00:10, 45.63it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.72it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.59it/s] 24%|██▎       | 143/608 [00:03<00:10, 45.35it/s] 24%|██▍       | 148/608 [00:03<00:10, 45.29it/s] 25%|██▌       | 153/608 [00:03<00:10, 45.46it/s] 26%|██▌       | 158/608 [00:03<00:09, 45.58it/s] 27%|██▋       | 163/608 [00:03<00:09, 45.72it/s] 28%|██▊       | 168/608 [00:03<00:09, 45.76it/s] 28%|██▊       | 173/608 [00:03<00:09, 45.86it/s] 29%|██▉       | 178/608 [00:03<00:09, 45.84it/s] 30%|███       | 183/608 [00:03<00:09, 45.62it/s] 31%|███       | 188/608 [00:04<00:09, 45.39it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.25it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.25it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.44it/s] 34%|███▍      | 208/608 [00:04<00:08, 45.57it/s] 35%|███▌      | 213/608 [00:04<00:08, 45.71it/s] 36%|███▌      | 218/608 [00:04<00:08, 45.79it/s] 37%|███▋      | 223/608 [00:04<00:08, 45.79it/s] 38%|███▊      | 228/608 [00:04<00:08, 45.71it/s] 38%|███▊      | 233/608 [00:05<00:08, 45.50it/s] 39%|███▉      | 238/608 [00:05<00:08, 45.51it/s] 40%|███▉      | 243/608 [00:05<00:08, 43.41it/s] 41%|████      | 248/608 [00:05<00:08, 44.26it/s] 42%|████▏     | 253/608 [00:05<00:07, 44.76it/s] 42%|████▏     | 258/608 [00:05<00:07, 45.12it/s] 43%|████▎     | 263/608 [00:05<00:07, 45.36it/s] 44%|████▍     | 268/608 [00:05<00:08, 41.23it/s] 45%|████▍     | 273/608 [00:06<00:07, 42.67it/s] 46%|████▌     | 278/608 [00:06<00:07, 43.53it/s] 47%|████▋     | 283/608 [00:06<00:07, 44.08it/s] 47%|████▋     | 288/608 [00:06<00:07, 44.50it/s] 48%|████▊     | 293/608 [00:06<00:07, 44.90it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.20it/s] 50%|████▉     | 303/608 [00:06<00:06, 45.49it/s] 51%|█████     | 308/608 [00:06<00:06, 44.63it/s] 51%|█████▏    | 313/608 [00:06<00:06, 45.07it/s] 52%|█████▏    | 318/608 [00:07<00:06, 45.24it/s] 53%|█████▎    | 323/608 [00:07<00:06, 45.51it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.59it/s] 55%|█████▍    | 333/608 [00:07<00:06, 45.71it/s] 56%|█████▌    | 338/608 [00:07<00:05, 45.59it/s] 56%|█████▋    | 343/608 [00:07<00:05, 45.63it/s] 57%|█████▋    | 348/608 [00:07<00:05, 45.55it/s] 58%|█████▊    | 353/608 [00:07<00:05, 45.17it/s] 59%|█████▉    | 358/608 [00:07<00:05, 44.96it/s] 60%|█████▉    | 363/608 [00:07<00:05, 45.03it/s] 61%|██████    | 368/608 [00:08<00:05, 45.13it/s] 61%|██████▏   | 373/608 [00:08<00:05, 45.34it/s] 62%|██████▏   | 378/608 [00:08<00:05, 45.44it/s] 63%|██████▎   | 383/608 [00:08<00:04, 45.62it/s] 64%|██████▍   | 388/608 [00:08<00:04, 45.78it/s] 65%|██████▍   | 393/608 [00:08<00:04, 45.71it/s] 65%|██████▌   | 398/608 [00:08<00:04, 45.40it/s] 66%|██████▋   | 403/608 [00:08<00:04, 41.41it/s] 67%|██████▋   | 408/608 [00:09<00:04, 42.78it/s] 68%|██████▊   | 413/608 [00:09<00:04, 43.75it/s] 69%|██████▉   | 418/608 [00:09<00:10, 18.68it/s] 70%|██████▉   | 424/608 [00:09<00:07, 23.69it/s] 71%|███████   | 429/608 [00:09<00:06, 27.53it/s] 71%|███████▏  | 434/608 [00:10<00:05, 31.18it/s] 72%|███████▏  | 439/608 [00:10<00:04, 34.44it/s] 73%|███████▎  | 444/608 [00:10<00:04, 37.20it/s] 74%|███████▍  | 449/608 [00:10<00:04, 39.51it/s] 75%|███████▍  | 454/608 [00:10<00:03, 41.23it/s] 75%|███████▌  | 459/608 [00:10<00:03, 42.49it/s] 76%|███████▋  | 464/608 [00:10<00:03, 42.99it/s] 77%|███████▋  | 469/608 [00:10<00:03, 43.51it/s] 78%|███████▊  | 474/608 [00:10<00:03, 44.03it/s] 79%|███████▉  | 479/608 [00:11<00:02, 44.56it/s] 80%|███████▉  | 484/608 [00:11<00:02, 44.93it/s] 80%|████████  | 489/608 [00:11<00:02, 45.21it/s] 81%|████████▏ | 494/608 [00:11<00:02, 45.34it/s] 82%|████████▏ | 499/608 [00:11<00:02, 45.65it/s] 83%|████████▎ | 504/608 [00:11<00:02, 45.64it/s] 84%|████████▎ | 509/608 [00:11<00:02, 45.46it/s] 85%|████████▍ | 514/608 [00:11<00:02, 45.19it/s] 85%|████████▌ | 519/608 [00:11<00:02, 41.50it/s] 86%|████████▌ | 524/608 [00:12<00:01, 42.83it/s] 87%|████████▋ | 529/608 [00:12<00:01, 43.72it/s] 88%|████████▊ | 534/608 [00:12<00:01, 44.40it/s] 89%|████████▊ | 539/608 [00:12<00:01, 44.94it/s] 89%|████████▉ | 544/608 [00:12<00:01, 45.32it/s] 90%|█████████ | 549/608 [00:12<00:01, 45.37it/s] 91%|█████████ | 554/608 [00:12<00:01, 45.29it/s] 92%|█████████▏| 559/608 [00:12<00:01, 44.94it/s] 93%|█████████▎| 564/608 [00:12<00:00, 44.92it/s] 94%|█████████▎| 569/608 [00:13<00:00, 45.07it/s] 94%|█████████▍| 574/608 [00:13<00:00, 45.44it/s] 95%|█████████▌| 579/608 [00:13<00:00, 45.65it/s] 96%|█████████▌| 584/608 [00:13<00:00, 45.85it/s] 97%|█████████▋| 589/608 [00:13<00:00, 45.91it/s] 98%|█████████▊| 594/608 [00:13<00:00, 45.82it/s] 99%|█████████▊| 599/608 [00:13<00:00, 45.52it/s] 99%|█████████▉| 604/608 [00:13<00:00, 45.29it/s]100%|██████████| 608/608 [00:13<00:00, 43.56it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:09:52,216 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:52,216 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:52,216 >>   eval_loss               =     0.9075
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:52,216 >>   eval_runtime            = 0:00:13.97
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:52,216 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:52,216 >>   eval_samples_per_second =    348.063
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:52,216 >>   eval_steps_per_second   =     43.508
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:09:52,216 >>   perplexity              =     2.4782
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_15_seed_3', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:21<06:54, 21.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:36<05:15, 17.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:55<05:07, 18.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:09<04:24, 16.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:24<04:01, 16.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:40<03:45, 16.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:54<03:19, 15.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:09<03:01, 15.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:23<02:45, 15.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:38<02:27, 14.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:52<02:11, 14.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:06<01:56, 14.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:20<01:40, 14.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:33<01:24, 14.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:46<01:07, 13.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:04<00:59, 14.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:16<00:42, 14.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:30<00:27, 13.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:44<00:13, 13.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:59<00:00, 14.33s/it]Generating: 100%|██████████| 20/20 [04:59<00:00, 14.97s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 358, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7118055555555556, 'errors': {'', "('συῆνος', 'said to be the same as', '', 'They are known as a form of Greek σάρτικρος ( συῆνος ) , τεῖς ( συθροτος ) , τίνρας ( συῆλώηνος ) ; and of the forms σατος τικρος in Latin .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : student .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 331, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 503, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7391826923076923, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 303, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 466, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 531, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 576, 'raw': 800}
{'target': 600, 'success': 598, 'raw': 832}
{'target': 600, 'success': 620, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7175925925925926, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8233695652173914, 'errors': {'', "('', 'continent', 'Aegean Sea', 'The island of Aranet , located in the Aegean Sea , is one of two islands that the Greek island of Argos has in common with the Arabian sea .')"}}
['Relation : field of this occupation . Context : The main characters in the series are The New Guy , The New York City Police Detective and the Los Angeles Police Department . Head Entity : Los Angeles Police Department , Tail Entity : police .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Walter L. Cusack', 'field of this occupation', '', 'Walter L. Cusack ( June 23 , 1924 February 21 , 2002 ) was an English cricket er who played six Tests in England .')"}}
['Relation : field of work . Context : Following his graduation in 1957 , he founded the PSA ( now in association with B. C. ) , an academic association for linguistics , linguistics , and other fields , and served as a member of the Princeton University Research Committee . Head Entity : linguistics , Tail Entity : graduate .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.76875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n']
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n', 'Relation : given name . Context : James M. Young ( February 14 , 1821 October 19 , 1924 ) was an American poet and writer specializing in the arts . Head Entity : James M. Young , Tail Entity : named name .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8206521739130435, 'errors': {'', "('Doctor Who', 'given name', '', 'He is best known for his play as a character on Doctor Who , in which he played the name of the villainess in the Doctor Who special , The One and Only Thing .')", 'too many values to unpack (expected 2)', "('Joseph L. M. Rennelly', 'given name', '', 'Joseph L. M. Rennelly is a former New York City mayor who served as the 62nd governor of New York City .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Never Say Goodbye', 'lyrics by', '', 'After graduating from Berklee College of Music , he started to write a single called Never Say Goodbye that debuted at No. 1 on the Billboard 200 single Hot Hot Song in July 1991 .')", "('Never Let Me Go', 'lyrics by', '', 'She followed this up with her sixth studio album , Never Let Me Go , which is now a cult classic on the Billboard R&B Top 40 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Central railway', 'owned by', '', 'It was built between 1941 and 1943 under the direction of the Central railway company ( D.')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9077380952380952, 'errors': {'', "('GMAW', 'performer', '', 'He also performed with the band Pampers on the opening night of WMAW in Los Angeles and the opening night of the ABC drama GMAW in Atlanta .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 128, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 185, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 223, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 288, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 371, 'raw': 576}
{'target': 600, 'success': 393, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 433, 'raw': 672}
{'target': 600, 'success': 451, 'raw': 704}
{'target': 600, 'success': 471, 'raw': 736}
{'target': 600, 'success': 495, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 583, 'raw': 896}
{'target': 600, 'success': 601, 'raw': 928}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.6476293103448276, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : replaces .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 17944
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18044, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_15_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:14, 14.97s/it]Extractor Estimating: 2it [00:17,  7.51s/it]Extractor Estimating: 3it [00:17,  4.34s/it]Extractor Estimating: 4it [00:18,  2.85s/it]Extractor Estimating: 5it [00:19,  2.04s/it]Extractor Estimating: 6it [00:19,  1.55s/it]Extractor Estimating: 7it [00:20,  1.22s/it]Extractor Estimating: 8it [00:20,  1.02s/it]Extractor Estimating: 9it [00:21,  1.13it/s]Extractor Estimating: 10it [00:21,  1.28it/s]Extractor Estimating: 11it [00:22,  1.40it/s]Extractor Estimating: 12it [00:23,  1.44it/s]Extractor Estimating: 13it [00:23,  1.52it/s]Extractor Estimating: 14it [00:24,  1.60it/s]Extractor Estimating: 15it [00:24,  1.61it/s]Extractor Estimating: 16it [00:25,  1.59it/s]Extractor Estimating: 17it [00:26,  1.62it/s]Extractor Estimating: 18it [00:26,  1.61it/s]Extractor Estimating: 19it [00:27,  1.61it/s]Extractor Estimating: 20it [00:27,  1.63it/s]Extractor Estimating: 21it [00:28,  1.68it/s]Extractor Estimating: 22it [00:28,  1.74it/s]Extractor Estimating: 23it [00:29,  1.67it/s]Extractor Estimating: 24it [00:30,  1.72it/s]Extractor Estimating: 25it [00:30,  1.67it/s]Extractor Estimating: 26it [00:33,  1.24s/it]Extractor Estimating: 27it [00:34,  1.06s/it]Extractor Estimating: 28it [00:34,  1.10it/s]Extractor Estimating: 29it [00:35,  1.26it/s]Extractor Estimating: 30it [00:35,  1.39it/s]Extractor Estimating: 31it [00:36,  1.48it/s]Extractor Estimating: 32it [00:36,  1.59it/s]Extractor Estimating: 33it [00:37,  1.60it/s]Extractor Estimating: 34it [00:38,  1.61it/s]Extractor Estimating: 35it [00:38,  1.65it/s]Extractor Estimating: 36it [00:39,  1.69it/s]Extractor Estimating: 37it [00:39,  1.74it/s]Extractor Estimating: 38it [00:40,  1.72it/s]Extractor Estimating: 39it [00:41,  1.65it/s]Extractor Estimating: 40it [00:41,  1.71it/s]Extractor Estimating: 41it [00:42,  1.77it/s]Extractor Estimating: 42it [00:42,  1.77it/s]Extractor Estimating: 43it [00:43,  1.76it/s]Extractor Estimating: 44it [00:43,  1.78it/s]Extractor Estimating: 45it [00:44,  1.77it/s]Extractor Estimating: 46it [00:44,  1.75it/s]Extractor Estimating: 47it [00:45,  1.71it/s]Extractor Estimating: 48it [00:46,  1.77it/s]Extractor Estimating: 49it [00:46,  1.77it/s]Extractor Estimating: 50it [00:47,  1.78it/s]Extractor Estimating: 51it [00:47,  1.76it/s]Extractor Estimating: 52it [00:48,  1.75it/s]Extractor Estimating: 53it [00:48,  1.72it/s]Extractor Estimating: 54it [00:49,  1.73it/s]Extractor Estimating: 55it [00:50,  1.75it/s]Extractor Estimating: 56it [00:50,  1.65it/s]Extractor Estimating: 57it [00:51,  1.63it/s]Extractor Estimating: 58it [00:52,  1.64it/s]Extractor Estimating: 59it [00:52,  1.63it/s]Extractor Estimating: 60it [00:53,  1.63it/s]Extractor Estimating: 61it [00:53,  1.63it/s]Extractor Estimating: 62it [00:54,  1.66it/s]Extractor Estimating: 63it [00:55,  1.69it/s]Extractor Estimating: 64it [00:55,  1.74it/s]Extractor Estimating: 65it [00:56,  1.71it/s]Extractor Estimating: 66it [00:56,  1.69it/s]Extractor Estimating: 67it [00:57,  1.64it/s]Extractor Estimating: 68it [00:58,  1.51it/s]Extractor Estimating: 69it [00:58,  1.54it/s]Extractor Estimating: 70it [00:59,  1.56it/s]Extractor Estimating: 71it [01:00,  1.53it/s]Extractor Estimating: 72it [01:00,  1.56it/s]Extractor Estimating: 73it [01:01,  1.59it/s]Extractor Estimating: 74it [01:01,  1.62it/s]Extractor Estimating: 75it [01:02,  1.62it/s]Extractor Estimating: 76it [01:03,  1.65it/s]Extractor Estimating: 77it [01:03,  1.67it/s]Extractor Estimating: 78it [01:04,  1.63it/s]Extractor Estimating: 79it [01:04,  1.68it/s]Extractor Estimating: 80it [01:05,  1.64it/s]Extractor Estimating: 81it [01:06,  1.66it/s]Extractor Estimating: 82it [01:06,  1.70it/s]Extractor Estimating: 83it [01:07,  1.72it/s]Extractor Estimating: 84it [01:07,  1.66it/s]Extractor Estimating: 85it [01:08,  1.65it/s]Extractor Estimating: 86it [01:09,  1.65it/s]Extractor Estimating: 87it [01:09,  1.66it/s]Extractor Estimating: 88it [01:10,  1.65it/s]Extractor Estimating: 89it [01:10,  1.65it/s]Extractor Estimating: 90it [01:11,  1.63it/s]Extractor Estimating: 91it [01:12,  1.68it/s]Extractor Estimating: 92it [01:12,  1.64it/s]Extractor Estimating: 93it [01:13,  1.68it/s]Extractor Estimating: 94it [01:13,  1.65it/s]Extractor Estimating: 95it [01:14,  1.62it/s]Extractor Estimating: 96it [01:15,  1.64it/s]Extractor Estimating: 97it [01:15,  1.65it/s]Extractor Estimating: 98it [01:16,  1.64it/s]Extractor Estimating: 99it [01:17,  1.66it/s]Extractor Estimating: 100it [01:17,  1.56it/s]Extractor Estimating: 101it [01:18,  1.60it/s]Extractor Estimating: 102it [01:18,  1.60it/s]Extractor Estimating: 103it [01:19,  1.62it/s]Extractor Estimating: 104it [01:20,  1.60it/s]Extractor Estimating: 105it [01:20,  1.58it/s]Extractor Estimating: 106it [01:21,  1.62it/s]Extractor Estimating: 107it [01:22,  1.62it/s]Extractor Estimating: 108it [01:22,  1.67it/s]Extractor Estimating: 109it [01:23,  1.64it/s]Extractor Estimating: 110it [01:23,  1.64it/s]Extractor Estimating: 111it [01:24,  1.61it/s]Extractor Estimating: 112it [01:25,  1.62it/s]Extractor Estimating: 113it [01:25,  1.67it/s]Extractor Estimating: 114it [01:26,  1.65it/s]Extractor Estimating: 115it [01:26,  1.67it/s]Extractor Estimating: 116it [01:27,  1.65it/s]Extractor Estimating: 117it [01:28,  1.62it/s]Extractor Estimating: 118it [01:28,  1.66it/s]Extractor Estimating: 119it [01:29,  1.65it/s]Extractor Estimating: 120it [01:29,  1.67it/s]Extractor Estimating: 121it [01:30,  1.66it/s]Extractor Estimating: 122it [01:31,  1.64it/s]Extractor Estimating: 123it [01:31,  1.66it/s]Extractor Estimating: 124it [01:32,  1.68it/s]Extractor Estimating: 125it [01:32,  1.68it/s]Extractor Estimating: 126it [01:33,  1.65it/s]Extractor Estimating: 127it [01:34,  1.61it/s]Extractor Estimating: 128it [01:34,  1.63it/s]Extractor Estimating: 129it [01:35,  1.65it/s]Extractor Estimating: 130it [01:36,  1.62it/s]Extractor Estimating: 131it [01:36,  1.62it/s]Extractor Estimating: 132it [01:37,  1.64it/s]Extractor Estimating: 133it [01:37,  1.69it/s]Extractor Estimating: 134it [01:38,  1.69it/s]Extractor Estimating: 135it [01:38,  1.73it/s]Extractor Estimating: 136it [01:39,  1.67it/s]Extractor Estimating: 137it [01:40,  1.70it/s]Extractor Estimating: 138it [01:40,  1.63it/s]Extractor Estimating: 139it [01:41,  1.66it/s]Extractor Estimating: 140it [01:41,  1.66it/s]Extractor Estimating: 141it [01:42,  1.66it/s]Extractor Estimating: 142it [01:43,  1.72it/s]Extractor Estimating: 143it [01:43,  1.67it/s]Extractor Estimating: 144it [01:44,  1.63it/s]Extractor Estimating: 145it [01:44,  1.66it/s]Extractor Estimating: 146it [01:45,  1.66it/s]Extractor Estimating: 147it [01:46,  1.60it/s]Extractor Estimating: 148it [01:46,  1.61it/s]Extractor Estimating: 149it [01:47,  1.45it/s]Extractor Estimating: 150it [01:48,  1.53it/s]Extractor Estimating: 151it [01:48,  1.59it/s]Extractor Estimating: 152it [01:49,  1.58it/s]Extractor Estimating: 153it [01:49,  1.68it/s]Extractor Estimating: 154it [01:50,  1.67it/s]Extractor Estimating: 155it [01:51,  1.70it/s]Extractor Estimating: 156it [01:51,  1.71it/s]Extractor Estimating: 157it [01:52,  1.76it/s]Extractor Estimating: 158it [01:53,  1.31it/s]Extractor Estimating: 159it [01:54,  1.41it/s]Extractor Estimating: 160it [01:54,  1.53it/s]Extractor Estimating: 161it [01:55,  1.57it/s]Extractor Estimating: 162it [01:55,  1.67it/s]Extractor Estimating: 163it [01:56,  1.74it/s]Extractor Estimating: 164it [01:56,  1.78it/s]Extractor Estimating: 165it [01:57,  1.79it/s]Extractor Estimating: 166it [01:57,  1.81it/s]Extractor Estimating: 167it [01:58,  1.79it/s]Extractor Estimating: 168it [01:58,  1.82it/s]Extractor Estimating: 169it [01:59,  1.87it/s]Extractor Estimating: 170it [02:00,  1.81it/s]Extractor Estimating: 171it [02:00,  1.86it/s]Extractor Estimating: 172it [02:01,  1.87it/s]Extractor Estimating: 173it [02:01,  1.85it/s]Extractor Estimating: 174it [02:02,  1.85it/s]Extractor Estimating: 175it [02:02,  1.80it/s]Extractor Estimating: 176it [02:03,  1.76it/s]Extractor Estimating: 177it [02:03,  1.73it/s]Extractor Estimating: 178it [02:04,  1.76it/s]Extractor Estimating: 179it [02:05,  1.65it/s]Extractor Estimating: 180it [02:05,  1.65it/s]Extractor Estimating: 181it [02:06,  1.60it/s]Extractor Estimating: 182it [02:07,  1.67it/s]Extractor Estimating: 183it [02:07,  1.68it/s]Extractor Estimating: 184it [02:08,  1.63it/s]Extractor Estimating: 185it [02:08,  1.63it/s]Extractor Estimating: 186it [02:09,  1.63it/s]Extractor Estimating: 187it [02:10,  1.64it/s]Extractor Estimating: 188it [02:10,  1.69it/s]Extractor Estimating: 189it [02:11,  1.68it/s]Extractor Estimating: 190it [02:11,  1.69it/s]Extractor Estimating: 191it [02:12,  1.71it/s]Extractor Estimating: 192it [02:13,  1.67it/s]Extractor Estimating: 193it [02:13,  1.71it/s]Extractor Estimating: 194it [02:14,  1.69it/s]Extractor Estimating: 195it [02:14,  1.65it/s]Extractor Estimating: 196it [02:15,  1.65it/s]Extractor Estimating: 197it [02:16,  1.66it/s]Extractor Estimating: 198it [02:16,  1.65it/s]Extractor Estimating: 199it [02:17,  1.62it/s]Extractor Estimating: 200it [02:17,  1.67it/s]Extractor Estimating: 201it [02:18,  1.64it/s]Extractor Estimating: 202it [02:19,  1.65it/s]Extractor Estimating: 203it [02:19,  1.63it/s]Extractor Estimating: 204it [02:20,  1.67it/s]Extractor Estimating: 205it [02:20,  1.64it/s]Extractor Estimating: 206it [02:21,  1.53it/s]Extractor Estimating: 207it [02:22,  1.56it/s]Extractor Estimating: 208it [02:22,  1.57it/s]Extractor Estimating: 209it [02:23,  1.55it/s]Extractor Estimating: 210it [02:24,  1.59it/s]Extractor Estimating: 211it [02:24,  1.61it/s]Extractor Estimating: 212it [02:25,  1.61it/s]Extractor Estimating: 213it [02:26,  1.58it/s]Extractor Estimating: 214it [02:26,  1.61it/s]Extractor Estimating: 215it [02:27,  1.57it/s]Extractor Estimating: 216it [02:27,  1.59it/s]Extractor Estimating: 217it [02:28,  1.61it/s]Extractor Estimating: 218it [02:29,  1.62it/s]Extractor Estimating: 219it [02:29,  1.66it/s]Extractor Estimating: 220it [02:30,  1.65it/s]Extractor Estimating: 221it [02:30,  1.69it/s]Extractor Estimating: 222it [02:31,  1.65it/s]Extractor Estimating: 223it [02:32,  1.67it/s]Extractor Estimating: 224it [02:32,  1.46it/s]Extractor Estimating: 225it [02:33,  1.53it/s]Extractor Estimating: 226it [02:34,  1.57it/s]Extractor Estimating: 227it [02:34,  1.60it/s]Extractor Estimating: 228it [02:35,  1.64it/s]Extractor Estimating: 229it [02:35,  1.62it/s]Extractor Estimating: 230it [02:36,  1.63it/s]Extractor Estimating: 231it [02:37,  1.67it/s]Extractor Estimating: 232it [02:37,  1.61it/s]Extractor Estimating: 233it [02:38,  1.63it/s]Extractor Estimating: 234it [02:39,  1.54it/s]Extractor Estimating: 235it [02:39,  1.60it/s]Extractor Estimating: 236it [02:40,  1.65it/s]Extractor Estimating: 237it [02:40,  1.64it/s]Extractor Estimating: 238it [02:41,  1.64it/s]Extractor Estimating: 239it [02:42,  1.63it/s]Extractor Estimating: 240it [02:42,  1.67it/s]Extractor Estimating: 241it [02:43,  1.68it/s]Extractor Estimating: 242it [02:43,  1.66it/s]Extractor Estimating: 243it [02:44,  1.66it/s]Extractor Estimating: 244it [02:45,  1.66it/s]Extractor Estimating: 245it [02:45,  1.63it/s]Extractor Estimating: 246it [02:46,  1.65it/s]Extractor Estimating: 247it [02:47,  1.56it/s]Extractor Estimating: 248it [02:47,  1.63it/s]Extractor Estimating: 249it [02:48,  1.64it/s]Extractor Estimating: 250it [02:48,  1.67it/s]Extractor Estimating: 251it [02:49,  1.70it/s]Extractor Estimating: 252it [02:49,  1.65it/s]Extractor Estimating: 253it [02:50,  1.59it/s]Extractor Estimating: 254it [02:51,  1.57it/s]Extractor Estimating: 255it [02:51,  1.58it/s]Extractor Estimating: 256it [02:52,  1.61it/s]Extractor Estimating: 257it [02:53,  1.63it/s]Extractor Estimating: 258it [02:53,  1.63it/s]Extractor Estimating: 259it [02:54,  1.61it/s]Extractor Estimating: 260it [02:54,  1.60it/s]Extractor Estimating: 261it [02:55,  1.60it/s]Extractor Estimating: 262it [02:56,  1.62it/s]Extractor Estimating: 263it [02:56,  1.59it/s]Extractor Estimating: 264it [02:57,  1.61it/s]Extractor Estimating: 265it [02:58,  1.62it/s]Extractor Estimating: 266it [02:58,  1.60it/s]Extractor Estimating: 267it [02:59,  1.62it/s]Extractor Estimating: 268it [03:00,  1.56it/s]Extractor Estimating: 269it [03:00,  1.59it/s]Extractor Estimating: 270it [03:01,  1.62it/s]Extractor Estimating: 271it [03:01,  1.60it/s]Extractor Estimating: 272it [03:02,  1.61it/s]Extractor Estimating: 273it [03:03,  1.57it/s]Extractor Estimating: 274it [03:03,  1.60it/s]Extractor Estimating: 275it [03:04,  1.56it/s]Extractor Estimating: 276it [03:04,  1.62it/s]Extractor Estimating: 277it [03:05,  1.62it/s]Extractor Estimating: 278it [03:06,  1.56it/s]Extractor Estimating: 279it [03:06,  1.59it/s]Extractor Estimating: 280it [03:07,  1.58it/s]Extractor Estimating: 281it [03:08,  1.56it/s]Extractor Estimating: 282it [03:08,  1.61it/s]Extractor Estimating: 283it [03:09,  1.59it/s]Extractor Estimating: 284it [03:10,  1.61it/s]Extractor Estimating: 285it [03:10,  1.61it/s]Extractor Estimating: 286it [03:11,  1.58it/s]Extractor Estimating: 287it [03:11,  1.62it/s]Extractor Estimating: 288it [03:12,  1.58it/s]Extractor Estimating: 289it [03:13,  1.61it/s]Extractor Estimating: 290it [03:13,  1.63it/s]Extractor Estimating: 291it [03:14,  1.62it/s]Extractor Estimating: 292it [03:15,  1.49it/s]Extractor Estimating: 293it [03:15,  1.47it/s]Extractor Estimating: 294it [03:16,  1.49it/s]Extractor Estimating: 295it [03:17,  1.49it/s]Extractor Estimating: 296it [03:17,  1.53it/s]Extractor Estimating: 297it [03:18,  1.57it/s]Extractor Estimating: 298it [03:19,  1.51it/s]Extractor Estimating: 299it [03:19,  1.53it/s]Extractor Estimating: 300it [03:20,  1.58it/s]Extractor Estimating: 301it [03:20,  1.57it/s]Extractor Estimating: 302it [03:21,  1.59it/s]Extractor Estimating: 303it [03:22,  1.59it/s]Extractor Estimating: 304it [03:22,  1.60it/s]Extractor Estimating: 305it [03:23,  1.61it/s]Extractor Estimating: 306it [03:24,  1.63it/s]Extractor Estimating: 307it [03:24,  1.70it/s]Extractor Estimating: 308it [03:25,  1.67it/s]Extractor Estimating: 309it [03:25,  1.68it/s]Extractor Estimating: 310it [03:26,  1.67it/s]Extractor Estimating: 311it [03:27,  1.62it/s]Extractor Estimating: 312it [03:27,  1.58it/s]Extractor Estimating: 313it [03:28,  1.62it/s]Extractor Estimating: 314it [03:29,  1.55it/s]Extractor Estimating: 315it [03:29,  1.55it/s]Extractor Estimating: 316it [03:30,  1.56it/s]Extractor Estimating: 317it [03:30,  1.55it/s]Extractor Estimating: 318it [03:31,  1.62it/s]Extractor Estimating: 319it [03:32,  1.58it/s]Extractor Estimating: 320it [03:32,  1.64it/s]Extractor Estimating: 321it [03:33,  1.68it/s]Extractor Estimating: 322it [03:33,  1.72it/s]Extractor Estimating: 323it [03:34,  1.66it/s]Extractor Estimating: 324it [03:35,  1.65it/s]Extractor Estimating: 325it [03:35,  1.67it/s]Extractor Estimating: 326it [03:36,  1.70it/s]Extractor Estimating: 327it [03:36,  1.71it/s]Extractor Estimating: 328it [03:37,  1.72it/s]Extractor Estimating: 329it [03:38,  1.67it/s]Extractor Estimating: 330it [03:38,  1.70it/s]Extractor Estimating: 331it [03:39,  1.68it/s]Extractor Estimating: 332it [03:39,  1.68it/s]Extractor Estimating: 333it [03:40,  1.63it/s]Extractor Estimating: 334it [03:41,  1.65it/s]Extractor Estimating: 335it [03:41,  1.68it/s]Extractor Estimating: 336it [03:42,  1.66it/s]Extractor Estimating: 337it [03:42,  1.67it/s]Extractor Estimating: 338it [03:43,  1.67it/s]Extractor Estimating: 339it [03:43,  1.73it/s]Extractor Estimating: 340it [03:44,  1.68it/s]Extractor Estimating: 341it [03:45,  1.69it/s]Extractor Estimating: 342it [03:45,  1.73it/s]Extractor Estimating: 343it [03:46,  1.66it/s]Extractor Estimating: 344it [03:46,  1.67it/s]Extractor Estimating: 345it [03:47,  1.66it/s]Extractor Estimating: 346it [03:48,  1.71it/s]Extractor Estimating: 347it [03:48,  1.71it/s]Extractor Estimating: 348it [03:49,  1.73it/s]Extractor Estimating: 349it [03:49,  1.71it/s]Extractor Estimating: 350it [03:50,  1.73it/s]Extractor Estimating: 351it [03:51,  1.69it/s]Extractor Estimating: 352it [03:51,  1.68it/s]Extractor Estimating: 353it [03:52,  1.66it/s]Extractor Estimating: 354it [03:52,  1.64it/s]Extractor Estimating: 355it [03:53,  1.66it/s]Extractor Estimating: 356it [03:54,  1.60it/s]Extractor Estimating: 357it [03:54,  1.64it/s]Extractor Estimating: 358it [03:55,  1.65it/s]Extractor Estimating: 359it [03:55,  1.61it/s]Extractor Estimating: 360it [03:56,  1.66it/s]Extractor Estimating: 361it [03:57,  1.67it/s]Extractor Estimating: 362it [03:57,  1.70it/s]Extractor Estimating: 363it [03:58,  1.63it/s]Extractor Estimating: 364it [03:58,  1.65it/s]Extractor Estimating: 365it [03:59,  1.67it/s]Extractor Estimating: 366it [04:00,  1.64it/s]Extractor Estimating: 367it [04:00,  1.51it/s]Extractor Estimating: 368it [04:01,  1.58it/s]Extractor Estimating: 369it [04:02,  1.57it/s]Extractor Estimating: 370it [04:02,  1.54it/s]Extractor Estimating: 371it [04:03,  1.57it/s]Extractor Estimating: 372it [04:04,  1.56it/s]Extractor Estimating: 373it [04:04,  1.54it/s]Extractor Estimating: 374it [04:05,  1.59it/s]Extractor Estimating: 375it [04:05,  1.64it/s]Extractor Estimating: 376it [04:06,  1.58it/s]Extractor Estimating: 377it [04:07,  1.59it/s]Extractor Estimating: 378it [04:07,  1.55it/s]Extractor Estimating: 379it [04:08,  1.58it/s]Extractor Estimating: 380it [04:09,  1.56it/s]Extractor Estimating: 381it [04:09,  1.56it/s]Extractor Estimating: 382it [04:10,  1.57it/s]Extractor Estimating: 383it [04:11,  1.51it/s]Extractor Estimating: 384it [04:11,  1.47it/s]Extractor Estimating: 385it [04:12,  1.52it/s]Extractor Estimating: 386it [04:13,  1.57it/s]Extractor Estimating: 387it [04:13,  1.57it/s]Extractor Estimating: 388it [04:14,  1.57it/s]Extractor Estimating: 389it [04:15,  1.55it/s]Extractor Estimating: 390it [04:15,  1.51it/s]Extractor Estimating: 391it [04:16,  1.55it/s]Extractor Estimating: 392it [04:16,  1.56it/s]Extractor Estimating: 393it [04:17,  1.48it/s]Extractor Estimating: 394it [04:18,  1.47it/s]Extractor Estimating: 395it [04:19,  1.51it/s]Extractor Estimating: 396it [04:19,  1.54it/s]Extractor Estimating: 397it [04:20,  1.54it/s]Extractor Estimating: 398it [04:21,  1.50it/s]Extractor Estimating: 399it [04:21,  1.50it/s]Extractor Estimating: 400it [04:22,  1.49it/s]Extractor Estimating: 401it [04:23,  1.49it/s]Extractor Estimating: 402it [04:23,  1.56it/s]Extractor Estimating: 403it [04:24,  1.56it/s]Extractor Estimating: 404it [04:24,  1.58it/s]Extractor Estimating: 405it [04:25,  1.62it/s]Extractor Estimating: 406it [04:26,  1.63it/s]Extractor Estimating: 407it [04:26,  1.61it/s]Extractor Estimating: 408it [04:27,  1.60it/s]Extractor Estimating: 409it [04:27,  1.58it/s]Extractor Estimating: 410it [04:28,  1.63it/s]Extractor Estimating: 411it [04:29,  1.59it/s]Extractor Estimating: 412it [04:29,  1.62it/s]Extractor Estimating: 413it [04:30,  1.61it/s]Extractor Estimating: 414it [04:31,  1.58it/s]Extractor Estimating: 415it [04:31,  1.56it/s]Extractor Estimating: 416it [04:32,  1.56it/s]Extractor Estimating: 417it [04:32,  1.58it/s]Extractor Estimating: 418it [04:33,  1.62it/s]Extractor Estimating: 419it [04:34,  1.63it/s]Extractor Estimating: 420it [04:34,  1.58it/s]Extractor Estimating: 421it [04:35,  1.59it/s]Extractor Estimating: 422it [04:36,  1.57it/s]Extractor Estimating: 423it [04:36,  1.62it/s]Extractor Estimating: 424it [04:37,  1.66it/s]Extractor Estimating: 425it [04:37,  1.67it/s]Extractor Estimating: 426it [04:38,  1.58it/s]Extractor Estimating: 427it [04:39,  1.62it/s]Extractor Estimating: 428it [04:39,  1.63it/s]Extractor Estimating: 429it [04:40,  1.65it/s]Extractor Estimating: 430it [04:40,  1.63it/s]Extractor Estimating: 431it [04:41,  1.62it/s]Extractor Estimating: 432it [04:42,  1.64it/s]Extractor Estimating: 433it [04:42,  1.65it/s]Extractor Estimating: 434it [04:43,  1.61it/s]Extractor Estimating: 435it [04:44,  1.60it/s]Extractor Estimating: 436it [04:44,  1.57it/s]Extractor Estimating: 437it [04:45,  1.59it/s]Extractor Estimating: 438it [04:46,  1.57it/s]Extractor Estimating: 439it [04:46,  1.53it/s]Extractor Estimating: 440it [04:47,  1.58it/s]Extractor Estimating: 441it [04:48,  1.41it/s]Extractor Estimating: 442it [04:48,  1.49it/s]Extractor Estimating: 443it [04:49,  1.55it/s]Extractor Estimating: 444it [04:49,  1.58it/s]Extractor Estimating: 445it [04:50,  1.58it/s]Extractor Estimating: 446it [04:51,  1.52it/s]Extractor Estimating: 447it [04:51,  1.56it/s]Extractor Estimating: 448it [04:52,  1.56it/s]Extractor Estimating: 449it [04:53,  1.60it/s]Extractor Estimating: 450it [04:53,  1.63it/s]Extractor Estimating: 451it [04:54,  1.59it/s]Extractor Estimating: 452it [04:55,  1.58it/s]Extractor Estimating: 453it [04:55,  1.60it/s]Extractor Estimating: 454it [04:56,  1.55it/s]Extractor Estimating: 455it [04:56,  1.58it/s]Extractor Estimating: 456it [04:57,  1.57it/s]Extractor Estimating: 457it [04:58,  1.56it/s]Extractor Estimating: 458it [04:58,  1.56it/s]Extractor Estimating: 459it [04:59,  1.51it/s]Extractor Estimating: 460it [05:00,  1.54it/s]Extractor Estimating: 461it [05:00,  1.58it/s]Extractor Estimating: 462it [05:01,  1.54it/s]Extractor Estimating: 463it [05:02,  1.50it/s]Extractor Estimating: 464it [05:02,  1.51it/s]Extractor Estimating: 465it [05:03,  1.52it/s]Extractor Estimating: 466it [05:04,  1.53it/s]Extractor Estimating: 467it [05:04,  1.59it/s]Extractor Estimating: 468it [05:05,  1.67it/s]Extractor Estimating: 469it [05:05,  1.60it/s]Extractor Estimating: 470it [05:06,  1.65it/s]Extractor Estimating: 471it [05:07,  1.56it/s]Extractor Estimating: 472it [05:07,  1.57it/s]Extractor Estimating: 473it [05:08,  1.51it/s]Extractor Estimating: 474it [05:09,  1.51it/s]Extractor Estimating: 475it [05:09,  1.54it/s]Extractor Estimating: 476it [05:10,  1.55it/s]Extractor Estimating: 477it [05:11,  1.54it/s]Extractor Estimating: 478it [05:11,  1.55it/s]Extractor Estimating: 479it [05:12,  1.58it/s]Extractor Estimating: 480it [05:12,  1.60it/s]Extractor Estimating: 481it [05:13,  1.59it/s]Extractor Estimating: 482it [05:14,  1.58it/s]Extractor Estimating: 483it [05:14,  1.59it/s]Extractor Estimating: 484it [05:15,  1.55it/s]Extractor Estimating: 485it [05:16,  1.56it/s]Extractor Estimating: 486it [05:16,  1.56it/s]Extractor Estimating: 487it [05:17,  1.59it/s]Extractor Estimating: 488it [05:18,  1.62it/s]Extractor Estimating: 489it [05:18,  1.57it/s]Extractor Estimating: 490it [05:19,  1.51it/s]Extractor Estimating: 491it [05:20,  1.57it/s]Extractor Estimating: 492it [05:20,  1.58it/s]Extractor Estimating: 493it [05:21,  1.64it/s]Extractor Estimating: 494it [05:21,  1.61it/s]Extractor Estimating: 495it [05:22,  1.58it/s]Extractor Estimating: 496it [05:23,  1.56it/s]Extractor Estimating: 497it [05:23,  1.59it/s]Extractor Estimating: 498it [05:24,  1.56it/s]Extractor Estimating: 499it [05:25,  1.52it/s]Extractor Estimating: 500it [05:25,  1.63it/s]Extractor Estimating: 500it [05:25,  1.54it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 10079 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 32321
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 32421, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_train_large/unseen_15_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=32421, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.236, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.939, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.921, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.942, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 80, avg_time 0.937, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 180, avg_time 2.231, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 280, avg_time 0.943, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 380, avg_time 0.934, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 60, avg_time 0.946, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 160, avg_time 0.936, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 260, avg_time 2.237, loss:nan
g_step 1200, step 360, avg_time 0.931, loss:nan
g_step 1300, step 40, avg_time 0.937, loss:nan
g_step 1400, step 140, avg_time 0.935, loss:nan
g_step 1500, step 240, avg_time 0.938, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 340, avg_time 2.224, loss:nan
g_step 1700, step 20, avg_time 0.956, loss:nan
g_step 1800, step 120, avg_time 0.930, loss:nan
g_step 1900, step 220, avg_time 0.936, loss:nan
g_step 2000, step 320, avg_time 0.941, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 420, avg_time 2.232, loss:nan
g_step 2200, step 100, avg_time 0.932, loss:nan
g_step 2300, step 200, avg_time 0.933, loss:nan
g_step 2400, step 300, avg_time 0.947, loss:nan
g_step 2500, step 400, avg_time 0.946, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 80, avg_time 2.226, loss:nan
g_step 2700, step 180, avg_time 0.950, loss:nan
g_step 2800, step 280, avg_time 0.945, loss:nan
g_step 2900, step 380, avg_time 0.928, loss:nan
g_step 3000, step 60, avg_time 0.939, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 160, avg_time 2.233, loss:nan
g_step 3200, step 260, avg_time 0.942, loss:nan
g_step 3300, step 360, avg_time 0.930, loss:nan
g_step 3400, step 40, avg_time 0.936, loss:nan
g_step 3500, step 140, avg_time 0.939, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 240, avg_time 2.218, loss:nan
g_step 3700, step 340, avg_time 0.943, loss:nan
g_step 3800, step 20, avg_time 0.946, loss:nan
g_step 3900, step 120, avg_time 0.943, loss:nan
g_step 4000, step 220, avg_time 0.936, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 320, avg_time 2.220, loss:nan
g_step 4200, step 420, avg_time 0.942, loss:nan
g_step 4300, step 100, avg_time 0.938, loss:nan
g_step 4400, step 200, avg_time 0.947, loss:nan
g_step 4500, step 300, avg_time 0.948, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 400, avg_time 2.217, loss:nan
g_step 4700, step 80, avg_time 0.942, loss:nan
g_step 4800, step 180, avg_time 0.941, loss:nan
g_step 4900, step 280, avg_time 0.937, loss:nan
g_step 5000, step 380, avg_time 0.937, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 60, avg_time 2.226, loss:nan
g_step 5200, step 160, avg_time 0.937, loss:nan
g_step 5300, step 260, avg_time 0.945, loss:nan
g_step 5400, step 360, avg_time 0.941, loss:nan
g_step 5500, step 40, avg_time 0.930, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 140, avg_time 2.223, loss:nan
g_step 5700, step 240, avg_time 0.942, loss:nan
g_step 5800, step 340, avg_time 0.942, loss:nan
g_step 5900, step 20, avg_time 0.936, loss:nan
g_step 6000, step 120, avg_time 0.943, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 220, avg_time 2.222, loss:nan
g_step 6200, step 320, avg_time 0.940, loss:nan
g_step 6300, step 420, avg_time 0.946, loss:nan
g_step 6400, step 100, avg_time 0.931, loss:nan
g_step 6500, step 200, avg_time 0.939, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 300, avg_time 2.235, loss:nan
g_step 6700, step 400, avg_time 0.938, loss:nan
g_step 6800, step 80, avg_time 0.933, loss:nan
g_step 6900, step 180, avg_time 0.948, loss:nan
g_step 7000, step 280, avg_time 0.931, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 380, avg_time 2.224, loss:nan
g_step 7200, step 60, avg_time 0.942, loss:nan
g_step 7300, step 160, avg_time 0.938, loss:nan
g_step 7400, step 260, avg_time 0.944, loss:nan
g_step 7500, step 360, avg_time 0.939, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 40, avg_time 2.213, loss:nan
g_step 7700, step 140, avg_time 0.945, loss:nan
g_step 7800, step 240, avg_time 0.948, loss:nan
g_step 7900, step 340, avg_time 0.933, loss:nan
g_step 8000, step 20, avg_time 0.931, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 120, avg_time 2.226, loss:nan
g_step 8200, step 220, avg_time 0.938, loss:nan
g_step 8300, step 320, avg_time 0.937, loss:nan
g_step 8400, step 420, avg_time 0.939, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 03:16:04 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 03:16:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_03-16-04_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 03:16:05 - WARNING - datasets.builder -   Using custom data configuration default-af12f93c342ff3ad
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-af12f93c342ff3ad/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 03:16:09,567 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:16:09,569 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:16:09,569 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:16:09,570 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:16:09,712 >> Didn't find file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:16:09,805 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:16:09,805 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:16:09,805 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:16:09,805 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:16:09,805 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:16:09,805 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 03:16:10,302 >> loading weights file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:16:13,429 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 03:16:13,470 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_15_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-af12f93c342ff3ad/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 03:16:13 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1458726bc200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:01<00:10,  1.09s/ba] 18%|█▊        | 2/11 [00:01<00:05,  1.75ba/s] 27%|██▋       | 3/11 [00:01<00:03,  2.44ba/s] 36%|███▋      | 4/11 [00:01<00:02,  2.56ba/s] 45%|████▌     | 5/11 [00:02<00:01,  3.05ba/s] 55%|█████▍    | 6/11 [00:02<00:01,  3.45ba/s] 64%|██████▎   | 7/11 [00:02<00:01,  3.76ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.01ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.18ba/s] 91%|█████████ | 10/11 [00:03<00:00,  4.05ba/s]100%|██████████| 11/11 [00:03<00:00,  3.34ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.70ba/s] 40%|████      | 2/5 [00:00<00:00,  3.54ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.96ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.19ba/s]100%|██████████| 5/5 [00:01<00:00,  4.53ba/s]100%|██████████| 5/5 [00:01<00:00,  4.12ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  6.10ba/s] 27%|██▋       | 3/11 [00:00<00:00,  9.36ba/s] 36%|███▋      | 4/11 [00:00<00:00,  9.38ba/s] 55%|█████▍    | 6/11 [00:00<00:00, 10.29ba/s] 73%|███████▎  | 8/11 [00:00<00:00, 10.77ba/s] 91%|█████████ | 10/11 [00:00<00:00, 11.06ba/s]100%|██████████| 11/11 [00:00<00:00, 11.06ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.34ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.33ba/s]100%|██████████| 5/5 [00:00<00:00, 10.72ba/s]100%|██████████| 5/5 [00:00<00:00, 10.06ba/s]
[INFO|trainer.py:414] 2023-08-28 03:16:21,359 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 03:16:21,412 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 03:16:21,412 >>   Num examples = 10320
[INFO|trainer.py:1149] 2023-08-28 03:16:21,412 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 03:16:21,412 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 03:16:21,412 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 03:16:21,412 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 03:16:21,412 >>   Total optimization steps = 805
  0%|          | 0/805 [00:00<?, ?it/s]  0%|          | 1/805 [00:00<03:51,  3.47it/s]  0%|          | 2/805 [00:00<03:43,  3.59it/s]  0%|          | 3/805 [00:00<03:40,  3.63it/s]  0%|          | 4/805 [00:01<03:39,  3.64it/s]  1%|          | 5/805 [00:01<03:39,  3.65it/s]  1%|          | 6/805 [00:01<03:42,  3.59it/s]  1%|          | 7/805 [00:01<03:40,  3.61it/s]  1%|          | 8/805 [00:02<03:39,  3.63it/s]  1%|          | 9/805 [00:02<03:38,  3.65it/s]  1%|          | 10/805 [00:02<03:37,  3.65it/s]  1%|▏         | 11/805 [00:03<03:37,  3.66it/s]  1%|▏         | 12/805 [00:03<03:36,  3.66it/s]  2%|▏         | 13/805 [00:03<03:36,  3.66it/s]  2%|▏         | 14/805 [00:03<03:36,  3.66it/s]  2%|▏         | 15/805 [00:04<03:35,  3.66it/s]  2%|▏         | 16/805 [00:04<03:35,  3.66it/s]  2%|▏         | 17/805 [00:04<03:39,  3.60it/s]  2%|▏         | 18/805 [00:04<03:37,  3.62it/s]  2%|▏         | 19/805 [00:05<03:36,  3.63it/s]  2%|▏         | 20/805 [00:05<03:35,  3.64it/s]  3%|▎         | 21/805 [00:05<03:34,  3.65it/s]  3%|▎         | 22/805 [00:06<03:34,  3.66it/s]  3%|▎         | 23/805 [00:06<03:33,  3.66it/s]  3%|▎         | 24/805 [00:06<03:33,  3.66it/s]  3%|▎         | 25/805 [00:06<03:32,  3.66it/s]  3%|▎         | 26/805 [00:07<03:32,  3.66it/s]  3%|▎         | 27/805 [00:07<03:32,  3.66it/s]  3%|▎         | 28/805 [00:07<03:37,  3.57it/s]  4%|▎         | 29/805 [00:07<03:35,  3.60it/s]  4%|▎         | 30/805 [00:08<03:34,  3.62it/s]  4%|▍         | 31/805 [00:08<03:33,  3.63it/s]  4%|▍         | 32/805 [00:08<03:32,  3.64it/s]  4%|▍         | 33/805 [00:09<03:31,  3.64it/s]  4%|▍         | 34/805 [00:09<03:31,  3.65it/s]  4%|▍         | 35/805 [00:09<03:30,  3.65it/s]  4%|▍         | 36/805 [00:09<03:30,  3.65it/s]  5%|▍         | 37/805 [00:10<03:30,  3.66it/s]  5%|▍         | 38/805 [00:10<03:29,  3.65it/s]  5%|▍         | 39/805 [00:10<03:34,  3.57it/s]  5%|▍         | 40/805 [00:11<03:32,  3.59it/s]  5%|▌         | 41/805 [00:11<03:31,  3.61it/s]  5%|▌         | 42/805 [00:11<03:30,  3.62it/s]  5%|▌         | 43/805 [00:11<03:29,  3.63it/s]  5%|▌         | 44/805 [00:12<03:29,  3.63it/s]  6%|▌         | 45/805 [00:12<03:29,  3.63it/s]  6%|▌         | 46/805 [00:12<03:28,  3.64it/s]  6%|▌         | 47/805 [00:12<03:28,  3.64it/s]  6%|▌         | 48/805 [00:13<03:27,  3.64it/s]  6%|▌         | 49/805 [00:13<03:28,  3.63it/s]  6%|▌         | 50/805 [00:13<03:27,  3.64it/s]  6%|▋         | 51/805 [00:14<03:27,  3.64it/s]  6%|▋         | 52/805 [00:14<03:26,  3.64it/s]  7%|▋         | 53/805 [00:14<03:26,  3.64it/s]  7%|▋         | 54/805 [00:14<03:26,  3.65it/s]  7%|▋         | 55/805 [00:15<03:25,  3.64it/s]  7%|▋         | 56/805 [00:15<03:25,  3.65it/s]  7%|▋         | 57/805 [00:15<03:25,  3.64it/s]  7%|▋         | 58/805 [00:15<03:24,  3.64it/s]  7%|▋         | 59/805 [00:16<03:24,  3.64it/s]  7%|▋         | 60/805 [00:16<03:24,  3.64it/s]  8%|▊         | 61/805 [00:16<03:26,  3.61it/s]  8%|▊         | 62/805 [00:17<03:25,  3.61it/s]  8%|▊         | 63/805 [00:17<03:24,  3.62it/s]  8%|▊         | 64/805 [00:17<03:24,  3.63it/s]  8%|▊         | 65/805 [00:17<03:23,  3.63it/s]  8%|▊         | 66/805 [00:18<03:23,  3.64it/s]  8%|▊         | 67/805 [00:18<03:22,  3.64it/s]  8%|▊         | 68/805 [00:18<03:22,  3.64it/s]  9%|▊         | 69/805 [00:18<03:22,  3.64it/s]  9%|▊         | 70/805 [00:19<03:21,  3.64it/s]  9%|▉         | 71/805 [00:19<03:21,  3.65it/s]  9%|▉         | 72/805 [00:19<03:25,  3.57it/s]  9%|▉         | 73/805 [00:20<03:24,  3.59it/s]  9%|▉         | 74/805 [00:20<03:22,  3.60it/s]  9%|▉         | 75/805 [00:20<03:21,  3.62it/s]  9%|▉         | 76/805 [00:20<03:21,  3.62it/s] 10%|▉         | 77/805 [00:21<03:20,  3.63it/s] 10%|▉         | 78/805 [00:21<03:19,  3.64it/s] 10%|▉         | 79/805 [00:21<03:19,  3.64it/s] 10%|▉         | 80/805 [00:22<03:19,  3.64it/s] 10%|█         | 81/805 [00:22<03:19,  3.64it/s] 10%|█         | 82/805 [00:22<03:18,  3.64it/s] 10%|█         | 83/805 [00:22<03:24,  3.52it/s] 10%|█         | 84/805 [00:23<03:22,  3.56it/s] 11%|█         | 85/805 [00:23<03:20,  3.58it/s] 11%|█         | 86/805 [00:23<03:19,  3.60it/s] 11%|█         | 87/805 [00:23<03:18,  3.61it/s] 11%|█         | 88/805 [00:24<03:17,  3.62it/s] 11%|█         | 89/805 [00:24<03:17,  3.63it/s] 11%|█         | 90/805 [00:24<03:16,  3.63it/s] 11%|█▏        | 91/805 [00:25<03:16,  3.63it/s] 11%|█▏        | 92/805 [00:25<03:15,  3.64it/s] 12%|█▏        | 93/805 [00:25<03:15,  3.64it/s] 12%|█▏        | 94/805 [00:25<03:21,  3.52it/s] 12%|█▏        | 95/805 [00:26<03:19,  3.56it/s] 12%|█▏        | 96/805 [00:26<03:18,  3.58it/s] 12%|█▏        | 97/805 [00:26<03:16,  3.60it/s] 12%|█▏        | 98/805 [00:27<03:15,  3.61it/s] 12%|█▏        | 99/805 [00:27<03:15,  3.62it/s] 12%|█▏        | 100/805 [00:27<03:14,  3.62it/s] 13%|█▎        | 101/805 [00:27<03:14,  3.62it/s] 13%|█▎        | 102/805 [00:28<03:13,  3.63it/s] 13%|█▎        | 103/805 [00:28<03:13,  3.63it/s] 13%|█▎        | 104/805 [00:28<03:13,  3.63it/s] 13%|█▎        | 105/805 [00:28<03:17,  3.55it/s] 13%|█▎        | 106/805 [00:29<03:15,  3.58it/s] 13%|█▎        | 107/805 [00:29<03:14,  3.60it/s] 13%|█▎        | 108/805 [00:29<03:13,  3.60it/s] 14%|█▎        | 109/805 [00:30<03:12,  3.61it/s] 14%|█▎        | 110/805 [00:30<03:11,  3.62it/s] 14%|█▍        | 111/805 [00:30<03:11,  3.62it/s] 14%|█▍        | 112/805 [00:30<03:11,  3.63it/s] 14%|█▍        | 113/805 [00:31<03:10,  3.63it/s] 14%|█▍        | 114/805 [00:31<03:10,  3.64it/s] 14%|█▍        | 115/805 [00:31<03:09,  3.64it/s] 14%|█▍        | 116/805 [00:32<03:13,  3.55it/s] 15%|█▍        | 117/805 [00:32<03:12,  3.58it/s] 15%|█▍        | 118/805 [00:32<03:11,  3.60it/s] 15%|█▍        | 119/805 [00:32<03:10,  3.61it/s] 15%|█▍        | 120/805 [00:33<03:09,  3.62it/s] 15%|█▌        | 121/805 [00:33<03:08,  3.62it/s] 15%|█▌        | 122/805 [00:33<03:08,  3.63it/s] 15%|█▌        | 123/805 [00:33<03:07,  3.63it/s] 15%|█▌        | 124/805 [00:34<03:07,  3.63it/s] 16%|█▌        | 125/805 [00:34<03:07,  3.63it/s] 16%|█▌        | 126/805 [00:34<03:06,  3.64it/s] 16%|█▌        | 127/805 [00:35<03:11,  3.54it/s] 16%|█▌        | 128/805 [00:35<03:09,  3.57it/s] 16%|█▌        | 129/805 [00:35<03:08,  3.59it/s] 16%|█▌        | 130/805 [00:35<03:07,  3.60it/s] 16%|█▋        | 131/805 [00:36<03:06,  3.61it/s] 16%|█▋        | 132/805 [00:36<03:05,  3.62it/s] 17%|█▋        | 133/805 [00:36<03:05,  3.63it/s] 17%|█▋        | 134/805 [00:36<03:04,  3.63it/s] 17%|█▋        | 135/805 [00:37<03:04,  3.64it/s] 17%|█▋        | 136/805 [00:37<03:03,  3.64it/s] 17%|█▋        | 137/805 [00:37<03:03,  3.64it/s] 17%|█▋        | 138/805 [00:38<03:10,  3.51it/s] 17%|█▋        | 139/805 [00:38<03:07,  3.55it/s] 17%|█▋        | 140/805 [00:38<03:06,  3.57it/s] 18%|█▊        | 141/805 [00:38<03:04,  3.59it/s] 18%|█▊        | 142/805 [00:39<03:03,  3.60it/s] 18%|█▊        | 143/805 [00:39<03:02,  3.62it/s] 18%|█▊        | 144/805 [00:39<03:02,  3.62it/s] 18%|█▊        | 145/805 [00:40<03:05,  3.56it/s] 18%|█▊        | 146/805 [00:40<03:04,  3.57it/s] 18%|█▊        | 147/805 [00:40<03:03,  3.59it/s] 18%|█▊        | 148/805 [00:40<03:02,  3.60it/s] 19%|█▊        | 149/805 [00:41<03:08,  3.48it/s] 19%|█▊        | 150/805 [00:41<03:05,  3.53it/s] 19%|█▉        | 151/805 [00:41<03:03,  3.56it/s] 19%|█▉        | 152/805 [00:42<03:02,  3.58it/s] 19%|█▉        | 153/805 [00:42<03:01,  3.59it/s] 19%|█▉        | 154/805 [00:42<03:00,  3.61it/s] 19%|█▉        | 155/805 [00:42<02:59,  3.62it/s] 19%|█▉        | 156/805 [00:43<03:32,  3.05it/s] 20%|█▉        | 157/805 [00:43<03:23,  3.19it/s] 20%|█▉        | 158/805 [00:43<03:15,  3.31it/s] 20%|█▉        | 159/805 [00:44<03:09,  3.40it/s] 20%|█▉        | 160/805 [00:44<03:08,  3.42it/s] 20%|██        | 161/805 [00:44<03:04,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 03:17:06,147 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:17:06,147 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 03:17:06,147 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.68it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.38it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.71it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.92it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.14it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.77it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.40it/s][A
  7%|▋         | 42/608 [00:00<00:12, 44.94it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.10it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.31it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.45it/s][A
 10%|█         | 62/608 [00:01<00:11, 45.57it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.60it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.46it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.31it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.12it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.94it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.96it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.02it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.35it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.51it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 42.05it/s][A
 19%|█▉        | 117/608 [00:02<00:11, 43.15it/s][A
 20%|██        | 122/608 [00:02<00:11, 43.61it/s][A
 21%|██        | 127/608 [00:02<00:10, 43.94it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.17it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.41it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.76it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.10it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 44.98it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 45.09it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.15it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.13it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.18it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 45.11it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.98it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.17it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.27it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.29it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.28it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.29it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.35it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.32it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.21it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.00it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.13it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.28it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.31it/s][A
 41%|████      | 247/608 [00:05<00:08, 42.23it/s][A
 41%|████▏     | 252/608 [00:05<00:08, 43.13it/s][A
 42%|████▏     | 257/608 [00:05<00:08, 43.81it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.38it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.61it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.78it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.80it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.05it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.75it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 44.81it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 44.94it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.19it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.23it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.34it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 45.32it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.32it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.20it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.00it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 44.98it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.05it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.14it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.28it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.43it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 45.40it/s][A
 60%|██████    | 367/608 [00:08<00:05, 45.38it/s][A
 61%|██████    | 372/608 [00:08<00:05, 45.11it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.94it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 39.30it/s][A
 64%|██████▎   | 387/608 [00:08<00:05, 41.10it/s][A
 64%|██████▍   | 392/608 [00:08<00:05, 42.27it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 43.31it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.06it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.59it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.77it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.77it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 44.48it/s][A
 70%|███████   | 427/608 [00:09<00:04, 44.33it/s][A
 71%|███████   | 432/608 [00:09<00:03, 44.44it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 44.75it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 44.97it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.18it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 45.26it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 45.46it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.41it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.15it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 44.81it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 44.79it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 44.95it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.10it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.24it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 45.33it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.44it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.47it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.30it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 41.38it/s][A
 86%|████████▌ | 522/608 [00:11<00:02, 42.65it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 43.56it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 44.20it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 44.61it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.95it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.26it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.34it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 44.93it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 44.95it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.09it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.28it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.43it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.48it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.59it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.66it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 45.46it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 45.25it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.21it/s][A                                                 
                                                 [A 20%|██        | 161/805 [00:58<03:04,  3.48it/s]
100%|██████████| 608/608 [00:13<00:00, 45.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:17:20,322 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161
[INFO|configuration_utils.py:351] 2023-08-28 03:17:20,637 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:17:23,381 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:17:23,532 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:17:23,575 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161/special_tokens_map.json
 20%|██        | 162/805 [01:03<1:01:53,  5.78s/it] 20%|██        | 163/805 [01:03<44:14,  4.13s/it]   20%|██        | 164/805 [01:03<31:48,  2.98s/it] 20%|██        | 165/805 [01:04<23:07,  2.17s/it] 21%|██        | 166/805 [01:04<17:03,  1.60s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 21%|██        | 167/805 [01:04<13:13,  1.24s/it] 21%|██        | 168/805 [01:05<10:08,  1.05it/s] 21%|██        | 169/805 [01:05<07:58,  1.33it/s] 21%|██        | 170/805 [01:05<06:27,  1.64it/s] 21%|██        | 171/805 [01:05<05:24,  1.96it/s] 21%|██▏       | 172/805 [01:06<04:39,  2.26it/s] 21%|██▏       | 173/805 [01:06<04:08,  2.55it/s] 22%|██▏       | 174/805 [01:06<03:53,  2.71it/s] 22%|██▏       | 175/805 [01:07<03:35,  2.92it/s] 22%|██▏       | 176/805 [01:07<03:23,  3.09it/s] 22%|██▏       | 177/805 [01:07<03:14,  3.22it/s] 22%|██▏       | 178/805 [01:07<03:08,  3.32it/s] 22%|██▏       | 179/805 [01:08<03:04,  3.39it/s] 22%|██▏       | 180/805 [01:08<03:01,  3.44it/s] 22%|██▏       | 181/805 [01:08<02:59,  3.48it/s] 23%|██▎       | 182/805 [01:09<02:57,  3.51it/s] 23%|██▎       | 183/805 [01:09<02:56,  3.53it/s] 23%|██▎       | 184/805 [01:09<02:55,  3.55it/s] 23%|██▎       | 185/805 [01:09<02:59,  3.46it/s] 23%|██▎       | 186/805 [01:10<02:57,  3.49it/s] 23%|██▎       | 187/805 [01:10<02:55,  3.52it/s] 23%|██▎       | 188/805 [01:10<02:54,  3.53it/s] 23%|██▎       | 189/805 [01:11<02:53,  3.54it/s] 24%|██▎       | 190/805 [01:11<02:53,  3.55it/s] 24%|██▎       | 191/805 [01:11<02:52,  3.56it/s] 24%|██▍       | 192/805 [01:11<02:52,  3.56it/s] 24%|██▍       | 193/805 [01:12<02:51,  3.57it/s] 24%|██▍       | 194/805 [01:12<02:51,  3.57it/s] 24%|██▍       | 195/805 [01:12<02:51,  3.57it/s] 24%|██▍       | 196/805 [01:13<02:55,  3.47it/s] 24%|██▍       | 197/805 [01:13<02:53,  3.50it/s] 25%|██▍       | 198/805 [01:13<02:52,  3.52it/s] 25%|██▍       | 199/805 [01:13<02:51,  3.54it/s] 25%|██▍       | 200/805 [01:14<02:50,  3.55it/s] 25%|██▍       | 201/805 [01:14<02:49,  3.56it/s] 25%|██▌       | 202/805 [01:14<02:49,  3.56it/s] 25%|██▌       | 203/805 [01:14<02:48,  3.57it/s] 25%|██▌       | 204/805 [01:15<02:48,  3.57it/s] 25%|██▌       | 205/805 [01:15<02:47,  3.57it/s] 26%|██▌       | 206/805 [01:15<02:47,  3.58it/s] 26%|██▌       | 207/805 [01:16<02:47,  3.57it/s] 26%|██▌       | 208/805 [01:16<02:47,  3.57it/s] 26%|██▌       | 209/805 [01:16<02:46,  3.57it/s] 26%|██▌       | 210/805 [01:16<02:46,  3.57it/s] 26%|██▌       | 211/805 [01:17<02:46,  3.57it/s] 26%|██▋       | 212/805 [01:17<02:45,  3.57it/s] 26%|██▋       | 213/805 [01:17<02:45,  3.57it/s] 27%|██▋       | 214/805 [01:18<02:53,  3.41it/s] 27%|██▋       | 215/805 [01:18<02:50,  3.46it/s] 27%|██▋       | 216/805 [01:18<02:48,  3.50it/s] 27%|██▋       | 217/805 [01:18<02:47,  3.52it/s] 27%|██▋       | 218/805 [01:19<02:46,  3.54it/s] 27%|██▋       | 219/805 [01:19<02:45,  3.54it/s] 27%|██▋       | 220/805 [01:19<02:44,  3.56it/s] 27%|██▋       | 221/805 [01:20<02:44,  3.56it/s] 28%|██▊       | 222/805 [01:20<02:43,  3.57it/s] 28%|██▊       | 223/805 [01:20<02:42,  3.59it/s] 28%|██▊       | 224/805 [01:20<02:41,  3.60it/s] 28%|██▊       | 225/805 [01:21<02:42,  3.56it/s] 28%|██▊       | 226/805 [01:21<02:41,  3.58it/s] 28%|██▊       | 227/805 [01:21<02:40,  3.60it/s] 28%|██▊       | 228/805 [01:22<02:39,  3.61it/s] 28%|██▊       | 229/805 [01:22<02:39,  3.61it/s] 29%|██▊       | 230/805 [01:22<02:38,  3.62it/s] 29%|██▊       | 231/805 [01:22<02:38,  3.62it/s] 29%|██▉       | 232/805 [01:23<02:37,  3.63it/s] 29%|██▉       | 233/805 [01:23<02:37,  3.63it/s] 29%|██▉       | 234/805 [01:23<02:37,  3.63it/s] 29%|██▉       | 235/805 [01:23<02:37,  3.63it/s] 29%|██▉       | 236/805 [01:24<02:39,  3.57it/s] 29%|██▉       | 237/805 [01:24<02:38,  3.59it/s] 30%|██▉       | 238/805 [01:24<02:37,  3.60it/s] 30%|██▉       | 239/805 [01:25<02:36,  3.61it/s] 30%|██▉       | 240/805 [01:25<02:36,  3.62it/s] 30%|██▉       | 241/805 [01:25<02:35,  3.63it/s] 30%|███       | 242/805 [01:25<02:35,  3.63it/s] 30%|███       | 243/805 [01:26<02:34,  3.63it/s] 30%|███       | 244/805 [01:26<02:34,  3.63it/s] 30%|███       | 245/805 [01:26<02:34,  3.63it/s] 31%|███       | 246/805 [01:26<02:33,  3.63it/s] 31%|███       | 247/805 [01:27<02:36,  3.56it/s] 31%|███       | 248/805 [01:27<02:35,  3.59it/s] 31%|███       | 249/805 [01:27<02:34,  3.60it/s] 31%|███       | 250/805 [01:28<02:33,  3.61it/s] 31%|███       | 251/805 [01:28<02:32,  3.62it/s] 31%|███▏      | 252/805 [01:28<02:32,  3.63it/s] 31%|███▏      | 253/805 [01:28<02:32,  3.63it/s] 32%|███▏      | 254/805 [01:29<02:31,  3.63it/s] 32%|███▏      | 255/805 [01:29<02:31,  3.63it/s] 32%|███▏      | 256/805 [01:29<02:31,  3.63it/s] 32%|███▏      | 257/805 [01:30<02:30,  3.63it/s] 32%|███▏      | 258/805 [01:30<02:35,  3.52it/s] 32%|███▏      | 259/805 [01:30<02:33,  3.55it/s] 32%|███▏      | 260/805 [01:30<02:32,  3.57it/s] 32%|███▏      | 261/805 [01:31<02:31,  3.59it/s] 33%|███▎      | 262/805 [01:31<02:30,  3.61it/s] 33%|███▎      | 263/805 [01:31<02:29,  3.62it/s] 33%|███▎      | 264/805 [01:31<02:29,  3.62it/s] 33%|███▎      | 265/805 [01:32<02:28,  3.63it/s] 33%|███▎      | 266/805 [01:32<02:28,  3.63it/s] 33%|███▎      | 267/805 [01:32<02:28,  3.63it/s] 33%|███▎      | 268/805 [01:33<02:27,  3.63it/s] 33%|███▎      | 269/805 [01:33<02:29,  3.59it/s] 34%|███▎      | 270/805 [01:33<02:28,  3.60it/s] 34%|███▎      | 271/805 [01:33<02:28,  3.60it/s] 34%|███▍      | 272/805 [01:34<02:27,  3.61it/s] 34%|███▍      | 273/805 [01:34<02:27,  3.62it/s] 34%|███▍      | 274/805 [01:34<02:26,  3.62it/s] 34%|███▍      | 275/805 [01:35<02:26,  3.62it/s] 34%|███▍      | 276/805 [01:35<02:25,  3.63it/s] 34%|███▍      | 277/805 [01:35<02:25,  3.63it/s] 35%|███▍      | 278/805 [01:35<02:25,  3.63it/s] 35%|███▍      | 279/805 [01:36<02:24,  3.63it/s] 35%|███▍      | 280/805 [01:36<02:27,  3.55it/s] 35%|███▍      | 281/805 [01:36<02:26,  3.57it/s] 35%|███▌      | 282/805 [01:36<02:25,  3.59it/s] 35%|███▌      | 283/805 [01:37<02:24,  3.60it/s] 35%|███▌      | 284/805 [01:37<02:24,  3.61it/s] 35%|███▌      | 285/805 [01:37<02:23,  3.61it/s] 36%|███▌      | 286/805 [01:38<02:23,  3.62it/s] 36%|███▌      | 287/805 [01:38<02:22,  3.62it/s] 36%|███▌      | 288/805 [01:38<02:22,  3.63it/s] 36%|███▌      | 289/805 [01:38<02:22,  3.63it/s] 36%|███▌      | 290/805 [01:39<02:21,  3.63it/s] 36%|███▌      | 291/805 [01:39<02:25,  3.54it/s] 36%|███▋      | 292/805 [01:39<02:23,  3.57it/s] 36%|███▋      | 293/805 [01:40<02:22,  3.59it/s] 37%|███▋      | 294/805 [01:40<02:24,  3.55it/s] 37%|███▋      | 295/805 [01:40<02:23,  3.56it/s] 37%|███▋      | 296/805 [01:40<02:21,  3.59it/s] 37%|███▋      | 297/805 [01:41<02:21,  3.60it/s] 37%|███▋      | 298/805 [01:41<02:20,  3.61it/s] 37%|███▋      | 299/805 [01:41<02:19,  3.62it/s] 37%|███▋      | 300/805 [01:41<02:19,  3.62it/s] 37%|███▋      | 301/805 [01:42<02:19,  3.62it/s] 38%|███▊      | 302/805 [01:42<02:25,  3.46it/s] 38%|███▊      | 303/805 [01:42<02:23,  3.51it/s] 38%|███▊      | 304/805 [01:43<02:21,  3.54it/s] 38%|███▊      | 305/805 [01:43<03:00,  2.77it/s] 38%|███▊      | 306/805 [01:43<02:47,  2.98it/s] 38%|███▊      | 307/805 [01:44<02:38,  3.15it/s] 38%|███▊      | 308/805 [01:44<02:31,  3.28it/s] 38%|███▊      | 309/805 [01:44<02:26,  3.38it/s] 39%|███▊      | 310/805 [01:45<02:23,  3.45it/s] 39%|███▊      | 311/805 [01:45<02:21,  3.50it/s] 39%|███▉      | 312/805 [01:45<02:25,  3.40it/s] 39%|███▉      | 313/805 [01:45<02:21,  3.47it/s] 39%|███▉      | 314/805 [01:46<02:19,  3.51it/s] 39%|███▉      | 315/805 [01:46<02:18,  3.55it/s] 39%|███▉      | 316/805 [01:46<02:16,  3.57it/s] 39%|███▉      | 317/805 [01:47<02:15,  3.59it/s] 40%|███▉      | 318/805 [01:47<02:15,  3.60it/s] 40%|███▉      | 319/805 [01:47<02:14,  3.61it/s] 40%|███▉      | 320/805 [01:47<02:14,  3.61it/s] 40%|███▉      | 321/805 [01:48<02:13,  3.62it/s] 40%|████      | 322/805 [01:48<02:13,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 03:18:09,834 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:18:09,834 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 03:18:09,834 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.6137, 'eval_samples_per_second': 357.286, 'eval_steps_per_second': 44.661, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.93it/s][A
  2%|▏         | 12/608 [00:00<00:15, 38.10it/s][A
  3%|▎         | 17/608 [00:00<00:14, 40.80it/s][A
  4%|▎         | 22/608 [00:00<00:13, 42.56it/s][A
  4%|▍         | 27/608 [00:00<00:13, 43.68it/s][A
  5%|▌         | 32/608 [00:00<00:12, 44.33it/s][A
  6%|▌         | 37/608 [00:00<00:12, 44.78it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.03it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.14it/s][A
  9%|▊         | 52/608 [00:01<00:12, 44.79it/s][A
  9%|▉         | 57/608 [00:01<00:12, 44.56it/s][A
 10%|█         | 62/608 [00:01<00:12, 44.73it/s][A
 11%|█         | 67/608 [00:01<00:12, 44.98it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.11it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.30it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.48it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.54it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 45.46it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.14it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.03it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.03it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 45.08it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.26it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.34it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.38it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 45.57it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 45.45it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.28it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 43.17it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 43.83it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 44.27it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 44.68it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 44.82it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.09it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 45.23it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 45.28it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.97it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 44.95it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.09it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.26it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.29it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.28it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.41it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.47it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.31it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.14it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.14it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.17it/s][A
 41%|████      | 247/608 [00:05<00:07, 45.21it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.25it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 45.32it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.35it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.41it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.24it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.20it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 41.11it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 42.35it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 43.34it/s][A
 49%|████▉     | 297/608 [00:06<00:07, 44.04it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 44.54it/s][A
 50%|█████     | 307/608 [00:06<00:06, 44.90it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.06it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 45.18it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.84it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.76it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.94it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 45.12it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.33it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.40it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.44it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.43it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 45.23it/s][A
 60%|██████    | 367/608 [00:08<00:05, 45.01it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.96it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.03it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 45.18it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.28it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 45.46it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.46it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 45.45it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 45.20it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.10it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 41.03it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 42.39it/s][A
 70%|███████   | 427/608 [00:09<00:04, 43.36it/s][A
 71%|███████   | 432/608 [00:09<00:03, 44.12it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 44.63it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 44.97it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.00it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 45.02it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.62it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.56it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 44.85it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.17it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.34it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.46it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.49it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.44it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 45.15it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 44.92it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.86it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.95it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.28it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.43it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.54it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.51it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.52it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 45.14it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.98it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 43.28it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 44.02it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 44.55it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 44.91it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.19it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.23it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.20it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.05it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.74it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.78it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.95it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.27it/s][A                                                 
                                                 [A 40%|████      | 322/805 [02:02<02:13,  3.62it/s]
100%|██████████| 608/608 [00:13<00:00, 45.27it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:18:23,618 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322
[INFO|configuration_utils.py:351] 2023-08-28 03:18:23,757 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:18:26,507 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:18:26,592 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:18:26,619 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322/special_tokens_map.json
 40%|████      | 323/805 [02:06<44:28,  5.54s/it] 40%|████      | 324/805 [02:06<31:44,  3.96s/it] 40%|████      | 325/805 [02:06<22:50,  2.85s/it] 40%|████      | 326/805 [02:07<16:37,  2.08s/it] 41%|████      | 327/805 [02:07<12:16,  1.54s/it] 41%|████      | 328/805 [02:07<09:14,  1.16s/it] 41%|████      | 329/805 [02:07<07:06,  1.12it/s] 41%|████      | 330/805 [02:08<05:37,  1.41it/s] 41%|████      | 331/805 [02:08<04:36,  1.71it/s] 41%|████      | 332/805 [02:08<03:52,  2.03it/s] 41%|████▏     | 333/805 [02:08<03:22,  2.33it/s] 41%|████▏     | 334/805 [02:09<03:00,  2.61it/s] 42%|████▏     | 335/805 [02:09<02:45,  2.84it/s] 42%|████▏     | 336/805 [02:09<02:34,  3.03it/s] 42%|████▏     | 337/805 [02:10<02:27,  3.18it/s] 42%|████▏     | 338/805 [02:10<02:21,  3.29it/s] 42%|████▏     | 339/805 [02:10<02:18,  3.38it/s] 42%|████▏     | 340/805 [02:10<02:15,  3.44it/s] 42%|████▏     | 341/805 [02:11<02:13,  3.48it/s] 42%|████▏     | 342/805 [02:11<02:15,  3.42it/s] 43%|████▎     | 343/805 [02:11<02:13,  3.47it/s] 43%|████▎     | 344/805 [02:12<02:11,  3.50it/s] 43%|████▎     | 345/805 [02:12<02:10,  3.52it/s] 43%|████▎     | 346/805 [02:12<02:09,  3.54it/s] 43%|████▎     | 347/805 [02:12<02:08,  3.56it/s] 43%|████▎     | 348/805 [02:13<02:08,  3.56it/s] 43%|████▎     | 349/805 [02:13<02:07,  3.57it/s] 43%|████▎     | 350/805 [02:13<02:07,  3.57it/s] 44%|████▎     | 351/805 [02:14<02:07,  3.57it/s] 44%|████▎     | 352/805 [02:14<02:06,  3.58it/s] 44%|████▍     | 353/805 [02:14<02:11,  3.44it/s] 44%|████▍     | 354/805 [02:14<02:09,  3.48it/s] 44%|████▍     | 355/805 [02:15<02:08,  3.51it/s] 44%|████▍     | 356/805 [02:15<02:07,  3.53it/s] 44%|████▍     | 357/805 [02:15<02:06,  3.54it/s] 44%|████▍     | 358/805 [02:16<02:05,  3.55it/s] 45%|████▍     | 359/805 [02:16<02:05,  3.56it/s] 45%|████▍     | 360/805 [02:16<02:05,  3.56it/s] 45%|████▍     | 361/805 [02:16<02:04,  3.56it/s] 45%|████▍     | 362/805 [02:17<02:03,  3.58it/s] 45%|████▌     | 363/805 [02:17<02:02,  3.59it/s] 45%|████▌     | 364/805 [02:17<02:02,  3.61it/s] 45%|████▌     | 365/805 [02:17<02:01,  3.62it/s] 45%|████▌     | 366/805 [02:18<02:01,  3.62it/s] 46%|████▌     | 367/805 [02:18<02:00,  3.62it/s] 46%|████▌     | 368/805 [02:18<02:00,  3.62it/s] 46%|████▌     | 369/805 [02:19<02:00,  3.63it/s] 46%|████▌     | 370/805 [02:19<01:59,  3.63it/s] 46%|████▌     | 371/805 [02:19<02:02,  3.53it/s] 46%|████▌     | 372/805 [02:19<02:01,  3.56it/s] 46%|████▋     | 373/805 [02:20<02:00,  3.58it/s] 46%|████▋     | 374/805 [02:20<01:59,  3.60it/s] 47%|████▋     | 375/805 [02:20<01:59,  3.61it/s] 47%|████▋     | 376/805 [02:21<01:58,  3.61it/s] 47%|████▋     | 377/805 [02:21<01:58,  3.62it/s] 47%|████▋     | 378/805 [02:21<01:57,  3.63it/s] 47%|████▋     | 379/805 [02:21<01:57,  3.63it/s] 47%|████▋     | 380/805 [02:22<01:57,  3.63it/s] 47%|████▋     | 381/805 [02:22<01:56,  3.63it/s] 47%|████▋     | 382/805 [02:22<01:59,  3.53it/s] 48%|████▊     | 383/805 [02:22<01:58,  3.57it/s] 48%|████▊     | 384/805 [02:23<01:57,  3.59it/s] 48%|████▊     | 385/805 [02:23<01:56,  3.60it/s] 48%|████▊     | 386/805 [02:23<01:56,  3.61it/s] 48%|████▊     | 387/805 [02:24<01:55,  3.62it/s] 48%|████▊     | 388/805 [02:24<01:55,  3.62it/s] 48%|████▊     | 389/805 [02:24<01:54,  3.62it/s] 48%|████▊     | 390/805 [02:24<01:54,  3.62it/s] 49%|████▊     | 391/805 [02:25<01:54,  3.63it/s] 49%|████▊     | 392/805 [02:25<01:53,  3.63it/s] 49%|████▉     | 393/805 [02:25<01:55,  3.56it/s] 49%|████▉     | 394/805 [02:26<01:54,  3.58it/s] 49%|████▉     | 395/805 [02:26<01:54,  3.59it/s] 49%|████▉     | 396/805 [02:26<01:53,  3.61it/s] 49%|████▉     | 397/805 [02:26<01:52,  3.61it/s] 49%|████▉     | 398/805 [02:27<01:52,  3.61it/s] 50%|████▉     | 399/805 [02:27<01:52,  3.62it/s] 50%|████▉     | 400/805 [02:27<01:51,  3.62it/s] 50%|████▉     | 401/805 [02:27<01:51,  3.63it/s] 50%|████▉     | 402/805 [02:28<01:51,  3.63it/s] 50%|█████     | 403/805 [02:28<01:50,  3.63it/s] 50%|█████     | 404/805 [02:28<01:53,  3.53it/s] 50%|█████     | 405/805 [02:29<01:52,  3.56it/s] 50%|█████     | 406/805 [02:29<01:51,  3.58it/s] 51%|█████     | 407/805 [02:29<01:50,  3.60it/s] 51%|█████     | 408/805 [02:29<01:50,  3.61it/s] 51%|█████     | 409/805 [02:30<01:49,  3.61it/s] 51%|█████     | 410/805 [02:30<01:49,  3.62it/s] 51%|█████     | 411/805 [02:30<01:48,  3.63it/s] 51%|█████     | 412/805 [02:31<01:48,  3.62it/s] 51%|█████▏    | 413/805 [02:31<01:48,  3.62it/s] 51%|█████▏    | 414/805 [02:31<01:47,  3.63it/s] 52%|█████▏    | 415/805 [02:31<01:53,  3.44it/s] 52%|█████▏    | 416/805 [02:32<01:51,  3.50it/s] 52%|█████▏    | 417/805 [02:32<01:49,  3.54it/s] 52%|█████▏    | 418/805 [02:32<01:48,  3.56it/s] 52%|█████▏    | 419/805 [02:32<01:47,  3.59it/s] 52%|█████▏    | 420/805 [02:33<01:47,  3.60it/s] 52%|█████▏    | 421/805 [02:33<01:46,  3.61it/s] 52%|█████▏    | 422/805 [02:33<01:45,  3.62it/s] 53%|█████▎    | 423/805 [02:34<01:45,  3.62it/s] 53%|█████▎    | 424/805 [02:34<01:45,  3.63it/s] 53%|█████▎    | 425/805 [02:34<01:44,  3.63it/s] 53%|█████▎    | 426/805 [02:34<01:48,  3.48it/s] 53%|█████▎    | 427/805 [02:35<01:47,  3.53it/s] 53%|█████▎    | 428/805 [02:35<01:46,  3.55it/s] 53%|█████▎    | 429/805 [02:35<01:45,  3.57it/s] 53%|█████▎    | 430/805 [02:36<01:44,  3.59it/s] 54%|█████▎    | 431/805 [02:36<01:43,  3.60it/s] 54%|█████▎    | 432/805 [02:36<01:43,  3.61it/s] 54%|█████▍    | 433/805 [02:36<01:42,  3.61it/s] 54%|█████▍    | 434/805 [02:37<01:42,  3.62it/s] 54%|█████▍    | 435/805 [02:37<01:42,  3.63it/s] 54%|█████▍    | 436/805 [02:37<01:41,  3.62it/s] 54%|█████▍    | 437/805 [02:37<01:43,  3.55it/s] 54%|█████▍    | 438/805 [02:38<01:42,  3.58it/s] 55%|█████▍    | 439/805 [02:38<01:41,  3.60it/s] 55%|█████▍    | 440/805 [02:38<01:41,  3.61it/s] 55%|█████▍    | 441/805 [02:39<01:40,  3.61it/s] 55%|█████▍    | 442/805 [02:39<01:40,  3.61it/s] 55%|█████▌    | 443/805 [02:39<01:40,  3.62it/s] 55%|█████▌    | 444/805 [02:39<01:39,  3.62it/s] 55%|█████▌    | 445/805 [02:40<01:39,  3.62it/s] 55%|█████▌    | 446/805 [02:40<01:41,  3.53it/s] 56%|█████▌    | 447/805 [02:40<01:41,  3.54it/s] 56%|█████▌    | 448/805 [02:41<01:44,  3.42it/s] 56%|█████▌    | 449/805 [02:41<01:42,  3.48it/s] 56%|█████▌    | 450/805 [02:41<01:40,  3.53it/s] 56%|█████▌    | 451/805 [02:41<01:39,  3.56it/s] 56%|█████▌    | 452/805 [02:42<01:38,  3.58it/s] 56%|█████▋    | 453/805 [02:42<01:37,  3.60it/s] 56%|█████▋    | 454/805 [02:42<01:37,  3.61it/s] 57%|█████▋    | 455/805 [02:43<01:36,  3.62it/s] 57%|█████▋    | 456/805 [02:43<01:36,  3.62it/s] 57%|█████▋    | 457/805 [02:44<02:23,  2.42it/s] 57%|█████▋    | 458/805 [02:44<02:12,  2.63it/s] 57%|█████▋    | 459/805 [02:44<02:00,  2.86it/s] 57%|█████▋    | 460/805 [02:44<01:52,  3.06it/s] 57%|█████▋    | 461/805 [02:45<01:47,  3.21it/s] 57%|█████▋    | 462/805 [02:45<01:43,  3.33it/s] 58%|█████▊    | 463/805 [02:45<01:40,  3.41it/s] 58%|█████▊    | 464/805 [02:45<01:38,  3.47it/s] 58%|█████▊    | 465/805 [02:46<01:36,  3.52it/s] 58%|█████▊    | 466/805 [02:46<01:35,  3.55it/s] 58%|█████▊    | 467/805 [02:46<01:34,  3.57it/s] 58%|█████▊    | 468/805 [02:47<01:34,  3.58it/s] 58%|█████▊    | 469/805 [02:47<01:33,  3.60it/s] 58%|█████▊    | 470/805 [02:47<01:32,  3.61it/s] 59%|█████▊    | 471/805 [02:47<01:32,  3.61it/s] 59%|█████▊    | 472/805 [02:48<01:32,  3.61it/s] 59%|█████▉    | 473/805 [02:48<01:31,  3.61it/s] 59%|█████▉    | 474/805 [02:48<01:31,  3.62it/s] 59%|█████▉    | 475/805 [02:49<01:31,  3.62it/s] 59%|█████▉    | 476/805 [02:49<01:30,  3.63it/s] 59%|█████▉    | 477/805 [02:49<01:30,  3.62it/s] 59%|█████▉    | 478/805 [02:49<01:30,  3.62it/s] 60%|█████▉    | 479/805 [02:50<01:29,  3.62it/s] 60%|█████▉    | 480/805 [02:50<01:40,  3.22it/s] 60%|█████▉    | 481/805 [02:50<01:37,  3.33it/s] 60%|█████▉    | 482/805 [02:51<01:34,  3.42it/s] 60%|██████    | 483/805 [02:51<01:32,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 03:19:12,794 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:19:12,794 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 03:19:12,794 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.6206, 'eval_samples_per_second': 357.107, 'eval_steps_per_second': 44.638, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.55it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.49it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.58it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.00it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.22it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.64it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.12it/s][A
  7%|▋         | 43/608 [00:00<00:12, 44.95it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.12it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.28it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.45it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.56it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.62it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.52it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.26it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 44.99it/s][A
 14%|█▍        | 88/608 [00:01<00:13, 37.57it/s][A
 15%|█▌        | 93/608 [00:02<00:12, 39.66it/s][A
 16%|█▌        | 98/608 [00:02<00:12, 41.31it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 42.58it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 43.51it/s][A
 19%|█▊        | 113/608 [00:02<00:11, 44.18it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 44.68it/s][A
 20%|██        | 123/608 [00:02<00:10, 44.90it/s][A
 21%|██        | 128/608 [00:02<00:10, 44.60it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 44.47it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.59it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 44.69it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.12it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.33it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.48it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.61it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.45it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.16it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.94it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.86it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.94it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.18it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.36it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.51it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.61it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.43it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.23it/s][A
 37%|███▋      | 223/608 [00:05<00:09, 41.95it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 42.93it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 43.66it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 44.21it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 44.71it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.02it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.13it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.16it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 44.86it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 44.81it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 44.83it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.08it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.24it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.40it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.54it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.48it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.32it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.05it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 44.92it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.02it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.16it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.28it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.38it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.38it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.36it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.15it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.05it/s][A
 59%|█████▉    | 358/608 [00:08<00:06, 39.60it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 41.27it/s][A
 61%|██████    | 368/608 [00:08<00:05, 42.50it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 43.45it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 44.09it/s][A
 63%|██████▎   | 383/608 [00:08<00:05, 44.47it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 44.87it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.12it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 44.76it/s][A
 66%|██████▋   | 403/608 [00:09<00:04, 44.66it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 44.76it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.00it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.25it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.29it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.42it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.51it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.49it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.16it/s][A
 74%|███████▎  | 448/608 [00:10<00:03, 45.10it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.14it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.23it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.26it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.27it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.30it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.37it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.23it/s][A
 80%|████████  | 488/608 [00:10<00:02, 44.94it/s][A
 81%|████████  | 493/608 [00:11<00:02, 39.22it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 41.04it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 42.39it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 43.34it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 44.03it/s][A
 85%|████████▌ | 518/608 [00:11<00:02, 44.64it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 44.96it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 44.91it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 44.58it/s][A
 88%|████████▊ | 538/608 [00:12<00:01, 44.43it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 44.66it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 44.90it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.20it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.37it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.55it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.65it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.44it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 44.85it/s][A
 96%|█████████▌| 583/608 [00:13<00:00, 44.80it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 44.79it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 44.96it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.08it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.35it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.49it/s][A                                                 
                                                 [A 60%|██████    | 483/805 [03:05<01:32,  3.48it/s]
100%|██████████| 608/608 [00:13<00:00, 45.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:19:26,914 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483
[INFO|configuration_utils.py:351] 2023-08-28 03:19:27,302 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:19:31,138 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:19:31,272 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:19:31,337 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483/special_tokens_map.json
 60%|██████    | 484/805 [03:11<33:20,  6.23s/it] 60%|██████    | 485/805 [03:11<23:42,  4.45s/it] 60%|██████    | 486/805 [03:12<16:59,  3.20s/it] 60%|██████    | 487/805 [03:12<12:18,  2.32s/it] 61%|██████    | 488/805 [03:12<09:01,  1.71s/it] 61%|██████    | 489/805 [03:12<06:44,  1.28s/it] 61%|██████    | 490/805 [03:13<05:11,  1.01it/s] 61%|██████    | 491/805 [03:13<04:03,  1.29it/s] 61%|██████    | 492/805 [03:13<03:16,  1.59it/s] 61%|██████    | 493/805 [03:13<02:43,  1.91it/s] 61%|██████▏   | 494/805 [03:14<02:20,  2.22it/s] 61%|██████▏   | 495/805 [03:14<02:03,  2.51it/s] 62%|██████▏   | 496/805 [03:14<01:52,  2.75it/s] 62%|██████▏   | 497/805 [03:15<01:44,  2.95it/s] 62%|██████▏   | 498/805 [03:15<01:38,  3.11it/s] 62%|██████▏   | 499/805 [03:15<01:34,  3.24it/s] 62%|██████▏   | 500/805 [03:16<01:37,  3.14it/s]                                                  62%|██████▏   | 500/805 [03:16<01:37,  3.14it/s] 62%|██████▏   | 501/805 [03:16<01:36,  3.14it/s] 62%|██████▏   | 502/805 [03:16<01:33,  3.26it/s] 62%|██████▏   | 503/805 [03:16<01:30,  3.34it/s] 63%|██████▎   | 504/805 [03:17<01:28,  3.41it/s] 63%|██████▎   | 505/805 [03:17<01:26,  3.46it/s] 63%|██████▎   | 506/805 [03:17<01:25,  3.49it/s] 63%|██████▎   | 507/805 [03:18<01:24,  3.52it/s] 63%|██████▎   | 508/805 [03:18<01:24,  3.53it/s] 63%|██████▎   | 509/805 [03:18<01:23,  3.55it/s] 63%|██████▎   | 510/805 [03:18<01:23,  3.55it/s] 63%|██████▎   | 511/805 [03:19<01:22,  3.56it/s] 64%|██████▎   | 512/805 [03:19<01:22,  3.57it/s] 64%|██████▎   | 513/805 [03:19<01:21,  3.57it/s] 64%|██████▍   | 514/805 [03:19<01:21,  3.57it/s] 64%|██████▍   | 515/805 [03:20<01:21,  3.57it/s] 64%|██████▍   | 516/805 [03:20<01:20,  3.57it/s] 64%|██████▍   | 517/805 [03:20<01:20,  3.57it/s] 64%|██████▍   | 518/805 [03:21<01:21,  3.50it/s] 64%|██████▍   | 519/805 [03:21<01:21,  3.52it/s] 65%|██████▍   | 520/805 [03:21<01:20,  3.54it/s] 65%|██████▍   | 521/805 [03:21<01:20,  3.55it/s] 65%|██████▍   | 522/805 [03:22<01:19,  3.55it/s] 65%|██████▍   | 523/805 [03:22<01:19,  3.56it/s] 65%|██████▌   | 524/805 [03:22<01:18,  3.56it/s] 65%|██████▌   | 525/805 [03:23<01:18,  3.57it/s] 65%|██████▌   | 526/805 [03:23<01:18,  3.57it/s] 65%|██████▌   | 527/805 [03:23<01:17,  3.57it/s] 66%|██████▌   | 528/805 [03:23<01:17,  3.57it/s] 66%|██████▌   | 529/805 [03:24<01:18,  3.49it/s] 66%|██████▌   | 530/805 [03:24<01:18,  3.52it/s] 66%|██████▌   | 531/805 [03:24<01:17,  3.53it/s] 66%|██████▌   | 532/805 [03:25<01:16,  3.56it/s] 66%|██████▌   | 533/805 [03:25<01:15,  3.58it/s] 66%|██████▋   | 534/805 [03:25<01:15,  3.59it/s] 66%|██████▋   | 535/805 [03:25<01:14,  3.61it/s] 67%|██████▋   | 536/805 [03:26<01:14,  3.62it/s] 67%|██████▋   | 537/805 [03:26<01:14,  3.62it/s] 67%|██████▋   | 538/805 [03:26<01:13,  3.62it/s] 67%|██████▋   | 539/805 [03:26<01:13,  3.63it/s] 67%|██████▋   | 540/805 [03:27<01:14,  3.55it/s] 67%|██████▋   | 541/805 [03:27<01:13,  3.57it/s] 67%|██████▋   | 542/805 [03:27<01:13,  3.60it/s] 67%|██████▋   | 543/805 [03:28<01:12,  3.60it/s] 68%|██████▊   | 544/805 [03:28<01:12,  3.61it/s] 68%|██████▊   | 545/805 [03:28<01:11,  3.62it/s] 68%|██████▊   | 546/805 [03:28<01:11,  3.62it/s] 68%|██████▊   | 547/805 [03:29<01:11,  3.62it/s] 68%|██████▊   | 548/805 [03:29<01:10,  3.63it/s] 68%|██████▊   | 549/805 [03:29<01:10,  3.63it/s] 68%|██████▊   | 550/805 [03:30<01:10,  3.63it/s] 68%|██████▊   | 551/805 [03:30<01:12,  3.49it/s] 69%|██████▊   | 552/805 [03:30<01:11,  3.53it/s] 69%|██████▊   | 553/805 [03:30<01:10,  3.56it/s] 69%|██████▉   | 554/805 [03:31<01:10,  3.58it/s] 69%|██████▉   | 555/805 [03:31<01:09,  3.59it/s] 69%|██████▉   | 556/805 [03:31<01:09,  3.61it/s] 69%|██████▉   | 557/805 [03:31<01:08,  3.61it/s] 69%|██████▉   | 558/805 [03:32<01:08,  3.62it/s] 69%|██████▉   | 559/805 [03:32<01:07,  3.62it/s] 70%|██████▉   | 560/805 [03:32<01:07,  3.62it/s] 70%|██████▉   | 561/805 [03:33<01:07,  3.63it/s] 70%|██████▉   | 562/805 [03:33<01:08,  3.53it/s] 70%|██████▉   | 563/805 [03:33<01:08,  3.56it/s] 70%|███████   | 564/805 [03:33<01:07,  3.58it/s] 70%|███████   | 565/805 [03:34<01:06,  3.59it/s] 70%|███████   | 566/805 [03:34<01:06,  3.61it/s] 70%|███████   | 567/805 [03:34<01:05,  3.62it/s] 71%|███████   | 568/805 [03:35<01:05,  3.62it/s] 71%|███████   | 569/805 [03:35<01:05,  3.62it/s] 71%|███████   | 570/805 [03:35<01:04,  3.63it/s] 71%|███████   | 571/805 [03:35<01:04,  3.63it/s] 71%|███████   | 572/805 [03:36<01:04,  3.63it/s] 71%|███████   | 573/805 [03:36<01:05,  3.52it/s] 71%|███████▏  | 574/805 [03:36<01:05,  3.55it/s] 71%|███████▏  | 575/805 [03:36<01:04,  3.57it/s] 72%|███████▏  | 576/805 [03:37<01:03,  3.59it/s] 72%|███████▏  | 577/805 [03:37<01:03,  3.60it/s] 72%|███████▏  | 578/805 [03:37<01:02,  3.61it/s] 72%|███████▏  | 579/805 [03:38<01:02,  3.62it/s] 72%|███████▏  | 580/805 [03:38<01:02,  3.62it/s] 72%|███████▏  | 581/805 [03:38<01:01,  3.62it/s] 72%|███████▏  | 582/805 [03:38<01:01,  3.63it/s] 72%|███████▏  | 583/805 [03:39<01:01,  3.63it/s] 73%|███████▎  | 584/805 [03:39<01:03,  3.50it/s] 73%|███████▎  | 585/805 [03:39<01:02,  3.54it/s] 73%|███████▎  | 586/805 [03:40<01:01,  3.57it/s] 73%|███████▎  | 587/805 [03:40<01:00,  3.58it/s] 73%|███████▎  | 588/805 [03:40<01:02,  3.49it/s] 73%|███████▎  | 589/805 [03:40<01:01,  3.52it/s] 73%|███████▎  | 590/805 [03:41<01:00,  3.55it/s] 73%|███████▎  | 591/805 [03:41<00:59,  3.58it/s] 74%|███████▎  | 592/805 [03:41<00:59,  3.59it/s] 74%|███████▎  | 593/805 [03:42<00:58,  3.61it/s] 74%|███████▍  | 594/805 [03:42<00:58,  3.61it/s] 74%|███████▍  | 595/805 [03:42<00:59,  3.53it/s] 74%|███████▍  | 596/805 [03:42<00:58,  3.56it/s] 74%|███████▍  | 597/805 [03:43<00:58,  3.58it/s] 74%|███████▍  | 598/805 [03:43<00:57,  3.60it/s] 74%|███████▍  | 599/805 [03:43<01:11,  2.86it/s] 75%|███████▍  | 600/805 [03:44<01:09,  2.96it/s] 75%|███████▍  | 601/805 [03:44<01:05,  3.12it/s] 75%|███████▍  | 602/805 [03:44<01:02,  3.26it/s] 75%|███████▍  | 603/805 [03:45<01:00,  3.36it/s] 75%|███████▌  | 604/805 [03:45<00:58,  3.44it/s] 75%|███████▌  | 605/805 [03:45<00:58,  3.39it/s] 75%|███████▌  | 606/805 [03:45<00:57,  3.46it/s] 75%|███████▌  | 607/805 [03:46<00:56,  3.51it/s] 76%|███████▌  | 608/805 [03:46<00:55,  3.55it/s] 76%|███████▌  | 609/805 [03:46<00:54,  3.57it/s] 76%|███████▌  | 610/805 [03:47<00:54,  3.59it/s] 76%|███████▌  | 611/805 [03:47<00:53,  3.60it/s] 76%|███████▌  | 612/805 [03:47<00:53,  3.60it/s] 76%|███████▌  | 613/805 [03:47<00:53,  3.61it/s] 76%|███████▋  | 614/805 [03:48<00:52,  3.62it/s] 76%|███████▋  | 615/805 [03:48<00:52,  3.62it/s] 77%|███████▋  | 616/805 [03:48<01:02,  3.04it/s] 77%|███████▋  | 617/805 [03:49<00:58,  3.20it/s] 77%|███████▋  | 618/805 [03:49<00:56,  3.32it/s] 77%|███████▋  | 619/805 [03:49<00:54,  3.41it/s] 77%|███████▋  | 620/805 [03:49<00:53,  3.47it/s] 77%|███████▋  | 621/805 [03:50<00:52,  3.50it/s] 77%|███████▋  | 622/805 [03:50<00:51,  3.53it/s] 77%|███████▋  | 623/805 [03:50<00:51,  3.54it/s] 78%|███████▊  | 624/805 [03:51<00:50,  3.55it/s] 78%|███████▊  | 625/805 [03:51<00:50,  3.56it/s] 78%|███████▊  | 626/805 [03:51<00:50,  3.56it/s] 78%|███████▊  | 627/805 [03:51<00:52,  3.37it/s] 78%|███████▊  | 628/805 [03:52<00:51,  3.42it/s] 78%|███████▊  | 629/805 [03:52<00:50,  3.47it/s] 78%|███████▊  | 630/805 [03:52<00:50,  3.50it/s] 78%|███████▊  | 631/805 [03:53<00:49,  3.52it/s] 79%|███████▊  | 632/805 [03:53<00:48,  3.53it/s] 79%|███████▊  | 633/805 [03:53<00:48,  3.55it/s] 79%|███████▉  | 634/805 [03:53<00:48,  3.55it/s] 79%|███████▉  | 635/805 [03:54<00:47,  3.56it/s] 79%|███████▉  | 636/805 [03:54<00:47,  3.56it/s] 79%|███████▉  | 637/805 [03:54<00:47,  3.57it/s] 79%|███████▉  | 638/805 [03:55<00:48,  3.41it/s] 79%|███████▉  | 639/805 [03:55<00:47,  3.46it/s] 80%|███████▉  | 640/805 [03:55<00:47,  3.49it/s] 80%|███████▉  | 641/805 [03:55<00:46,  3.52it/s] 80%|███████▉  | 642/805 [03:56<00:46,  3.53it/s] 80%|███████▉  | 643/805 [03:56<00:45,  3.55it/s] 80%|████████  | 644/805 [03:56<00:45,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 03:20:18,235 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:20:18,235 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 03:20:18,235 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.6703, 'eval_samples_per_second': 355.807, 'eval_steps_per_second': 44.476, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.1940993788819876e-05, 'epoch': 3.11}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.43it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.50it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.90it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.19it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.37it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.73it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.21it/s][A
  7%|▋         | 43/608 [00:00<00:12, 44.95it/s][A
  8%|▊         | 48/608 [00:01<00:13, 40.21it/s][A
  9%|▊         | 53/608 [00:01<00:13, 41.79it/s][A
 10%|▉         | 58/608 [00:01<00:12, 42.92it/s][A
 10%|█         | 63/608 [00:01<00:12, 43.77it/s][A
 11%|█         | 68/608 [00:01<00:12, 44.34it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 44.77it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.03it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.17it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.80it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.66it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 44.92it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.15it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.34it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.41it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.47it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.52it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.34it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.12it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.03it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.09it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.24it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.31it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.52it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.51it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.53it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.31it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.15it/s][A
 30%|███       | 183/608 [00:04<00:09, 42.61it/s][A
 31%|███       | 188/608 [00:04<00:09, 43.53it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.11it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 44.53it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 44.93it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.11it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.16it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.06it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 44.80it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 44.87it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.09it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.19it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.42it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.44it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.55it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.39it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.28it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.04it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.03it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.18it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.35it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.43it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.37it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.41it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.47it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.24it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.12it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 44.13it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 44.54it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 44.93it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.15it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.32it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.35it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.31it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.18it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 44.96it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.00it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.11it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.35it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.51it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.58it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.54it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.43it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.25it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.05it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.02it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.14it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.34it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.50it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.60it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.56it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.42it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.21it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.12it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.12it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 44.64it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 44.86it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.24it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.35it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.43it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.29it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.24it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.09it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.08it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.10it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.22it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.37it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.55it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.55it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.45it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.26it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.21it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.12it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 44.86it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.29it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.35it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.53it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.54it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.55it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.34it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.33it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.10it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.17it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.16it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.37it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.41it/s][A                                                 
                                                 [A 80%|████████  | 644/805 [04:10<00:45,  3.55it/s]
100%|██████████| 608/608 [00:13<00:00, 45.41it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:20:31,957 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644
[INFO|configuration_utils.py:351] 2023-08-28 03:20:32,121 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:20:34,953 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:20:35,073 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:20:35,128 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644/special_tokens_map.json
 80%|████████  | 645/805 [04:14<14:56,  5.60s/it] 80%|████████  | 646/805 [04:15<10:37,  4.01s/it] 80%|████████  | 647/805 [04:15<07:36,  2.89s/it] 80%|████████  | 648/805 [04:15<05:30,  2.10s/it] 81%|████████  | 649/805 [04:15<04:02,  1.56s/it] 81%|████████  | 650/805 [04:16<03:01,  1.17s/it] 81%|████████  | 651/805 [04:16<02:19,  1.11it/s] 81%|████████  | 652/805 [04:16<01:49,  1.40it/s] 81%|████████  | 653/805 [04:17<01:33,  1.62it/s] 81%|████████  | 654/805 [04:17<01:17,  1.94it/s] 81%|████████▏ | 655/805 [04:17<01:06,  2.25it/s] 81%|████████▏ | 656/805 [04:17<00:58,  2.53it/s] 82%|████████▏ | 657/805 [04:18<00:53,  2.78it/s] 82%|████████▏ | 658/805 [04:18<00:49,  2.97it/s] 82%|████████▏ | 659/805 [04:18<00:46,  3.13it/s] 82%|████████▏ | 660/805 [04:19<00:44,  3.25it/s] 82%|████████▏ | 661/805 [04:19<00:43,  3.35it/s] 82%|████████▏ | 662/805 [04:19<00:41,  3.41it/s] 82%|████████▏ | 663/805 [04:19<00:41,  3.46it/s] 82%|████████▏ | 664/805 [04:20<00:40,  3.49it/s] 83%|████████▎ | 665/805 [04:20<00:39,  3.52it/s] 83%|████████▎ | 666/805 [04:20<00:39,  3.54it/s] 83%|████████▎ | 667/805 [04:21<00:38,  3.55it/s] 83%|████████▎ | 668/805 [04:21<00:38,  3.56it/s] 83%|████████▎ | 669/805 [04:21<00:38,  3.56it/s] 83%|████████▎ | 670/805 [04:21<00:37,  3.57it/s] 83%|████████▎ | 671/805 [04:22<00:37,  3.57it/s] 83%|████████▎ | 672/805 [04:22<00:38,  3.43it/s] 84%|████████▎ | 673/805 [04:22<00:37,  3.48it/s] 84%|████████▎ | 674/805 [04:23<00:37,  3.51it/s] 84%|████████▍ | 675/805 [04:23<00:36,  3.53it/s] 84%|████████▍ | 676/805 [04:23<00:36,  3.54it/s] 84%|████████▍ | 677/805 [04:23<00:36,  3.55it/s] 84%|████████▍ | 678/805 [04:24<00:35,  3.56it/s] 84%|████████▍ | 679/805 [04:24<00:35,  3.57it/s] 84%|████████▍ | 680/805 [04:24<00:35,  3.57it/s] 85%|████████▍ | 681/805 [04:24<00:34,  3.57it/s] 85%|████████▍ | 682/805 [04:25<00:34,  3.57it/s] 85%|████████▍ | 683/805 [04:25<00:35,  3.45it/s] 85%|████████▍ | 684/805 [04:25<00:34,  3.49it/s] 85%|████████▌ | 685/805 [04:26<00:34,  3.52it/s] 85%|████████▌ | 686/805 [04:26<00:33,  3.54it/s] 85%|████████▌ | 687/805 [04:26<00:33,  3.55it/s] 85%|████████▌ | 688/805 [04:26<00:32,  3.56it/s] 86%|████████▌ | 689/805 [04:27<00:32,  3.57it/s] 86%|████████▌ | 690/805 [04:27<00:32,  3.57it/s] 86%|████████▌ | 691/805 [04:27<00:31,  3.57it/s] 86%|████████▌ | 692/805 [04:28<00:31,  3.58it/s] 86%|████████▌ | 693/805 [04:28<00:31,  3.57it/s] 86%|████████▌ | 694/805 [04:28<00:31,  3.49it/s] 86%|████████▋ | 695/805 [04:28<00:31,  3.52it/s] 86%|████████▋ | 696/805 [04:29<00:30,  3.53it/s] 87%|████████▋ | 697/805 [04:29<00:30,  3.54it/s] 87%|████████▋ | 698/805 [04:29<00:30,  3.55it/s] 87%|████████▋ | 699/805 [04:30<00:29,  3.56it/s] 87%|████████▋ | 700/805 [04:30<00:29,  3.57it/s] 87%|████████▋ | 701/805 [04:30<00:29,  3.57it/s] 87%|████████▋ | 702/805 [04:30<00:28,  3.57it/s] 87%|████████▋ | 703/805 [04:31<00:28,  3.57it/s] 87%|████████▋ | 704/805 [04:31<00:28,  3.57it/s] 88%|████████▊ | 705/805 [04:31<00:28,  3.52it/s] 88%|████████▊ | 706/805 [04:32<00:27,  3.56it/s] 88%|████████▊ | 707/805 [04:32<00:27,  3.58it/s] 88%|████████▊ | 708/805 [04:32<00:26,  3.60it/s] 88%|████████▊ | 709/805 [04:32<00:26,  3.61it/s] 88%|████████▊ | 710/805 [04:33<00:26,  3.62it/s] 88%|████████▊ | 711/805 [04:33<00:25,  3.62it/s] 88%|████████▊ | 712/805 [04:33<00:25,  3.62it/s] 89%|████████▊ | 713/805 [04:33<00:25,  3.63it/s] 89%|████████▊ | 714/805 [04:34<00:25,  3.63it/s] 89%|████████▉ | 715/805 [04:34<00:24,  3.63it/s] 89%|████████▉ | 716/805 [04:34<00:25,  3.52it/s] 89%|████████▉ | 717/805 [04:35<00:24,  3.56it/s] 89%|████████▉ | 718/805 [04:35<00:24,  3.58it/s] 89%|████████▉ | 719/805 [04:35<00:23,  3.60it/s] 89%|████████▉ | 720/805 [04:35<00:23,  3.61it/s] 90%|████████▉ | 721/805 [04:36<00:23,  3.62it/s] 90%|████████▉ | 722/805 [04:36<00:22,  3.62it/s] 90%|████████▉ | 723/805 [04:36<00:22,  3.63it/s] 90%|████████▉ | 724/805 [04:37<00:22,  3.63it/s] 90%|█████████ | 725/805 [04:37<00:22,  3.63it/s] 90%|█████████ | 726/805 [04:37<00:21,  3.63it/s] 90%|█████████ | 727/805 [04:38<00:24,  3.13it/s] 90%|█████████ | 728/805 [04:38<00:23,  3.27it/s] 91%|█████████ | 729/805 [04:38<00:22,  3.37it/s] 91%|█████████ | 730/805 [04:38<00:21,  3.45it/s] 91%|█████████ | 731/805 [04:39<00:21,  3.51it/s] 91%|█████████ | 732/805 [04:39<00:20,  3.55it/s] 91%|█████████ | 733/805 [04:39<00:20,  3.58it/s] 91%|█████████ | 734/805 [04:39<00:19,  3.60it/s] 91%|█████████▏| 735/805 [04:40<00:19,  3.61it/s] 91%|█████████▏| 736/805 [04:40<00:19,  3.62it/s] 92%|█████████▏| 737/805 [04:40<00:18,  3.63it/s] 92%|█████████▏| 738/805 [04:41<00:18,  3.53it/s] 92%|█████████▏| 739/805 [04:41<00:18,  3.57it/s] 92%|█████████▏| 740/805 [04:41<00:18,  3.59it/s] 92%|█████████▏| 741/805 [04:41<00:17,  3.61it/s] 92%|█████████▏| 742/805 [04:42<00:17,  3.62it/s] 92%|█████████▏| 743/805 [04:42<00:17,  3.63it/s] 92%|█████████▏| 744/805 [04:42<00:16,  3.63it/s] 93%|█████████▎| 745/805 [04:42<00:16,  3.64it/s] 93%|█████████▎| 746/805 [04:43<00:16,  3.64it/s] 93%|█████████▎| 747/805 [04:43<00:15,  3.64it/s] 93%|█████████▎| 748/805 [04:43<00:17,  3.30it/s] 93%|█████████▎| 749/805 [04:44<00:17,  3.24it/s] 93%|█████████▎| 750/805 [04:44<00:16,  3.34it/s] 93%|█████████▎| 751/805 [04:44<00:15,  3.42it/s] 93%|█████████▎| 752/805 [04:45<00:15,  3.49it/s] 94%|█████████▎| 753/805 [04:45<00:14,  3.53it/s] 94%|█████████▎| 754/805 [04:45<00:14,  3.56it/s] 94%|█████████▍| 755/805 [04:45<00:13,  3.58it/s] 94%|█████████▍| 756/805 [04:46<00:13,  3.60it/s] 94%|█████████▍| 757/805 [04:46<00:13,  3.61it/s] 94%|█████████▍| 758/805 [04:46<00:12,  3.62it/s] 94%|█████████▍| 759/805 [04:46<00:12,  3.63it/s] 94%|█████████▍| 760/805 [04:47<00:13,  3.37it/s] 95%|█████████▍| 761/805 [04:47<00:12,  3.44it/s] 95%|█████████▍| 762/805 [04:47<00:12,  3.50it/s] 95%|█████████▍| 763/805 [04:48<00:11,  3.54it/s] 95%|█████████▍| 764/805 [04:48<00:11,  3.57it/s] 95%|█████████▌| 765/805 [04:48<00:11,  3.59it/s] 95%|█████████▌| 766/805 [04:48<00:10,  3.61it/s] 95%|█████████▌| 767/805 [04:49<00:10,  3.62it/s] 95%|█████████▌| 768/805 [04:49<00:10,  3.62it/s] 96%|█████████▌| 769/805 [04:49<00:09,  3.62it/s] 96%|█████████▌| 770/805 [04:50<00:09,  3.63it/s] 96%|█████████▌| 771/805 [04:50<00:09,  3.63it/s] 96%|█████████▌| 772/805 [04:50<00:09,  3.63it/s] 96%|█████████▌| 773/805 [04:50<00:08,  3.63it/s] 96%|█████████▌| 774/805 [04:51<00:08,  3.64it/s] 96%|█████████▋| 775/805 [04:51<00:08,  3.64it/s] 96%|█████████▋| 776/805 [04:51<00:07,  3.64it/s] 97%|█████████▋| 777/805 [04:51<00:07,  3.64it/s] 97%|█████████▋| 778/805 [04:52<00:07,  3.64it/s] 97%|█████████▋| 779/805 [04:52<00:07,  3.64it/s] 97%|█████████▋| 780/805 [04:52<00:06,  3.64it/s] 97%|█████████▋| 781/805 [04:53<00:06,  3.64it/s] 97%|█████████▋| 782/805 [04:53<00:06,  3.41it/s] 97%|█████████▋| 783/805 [04:53<00:06,  3.48it/s] 97%|█████████▋| 784/805 [04:53<00:05,  3.52it/s] 98%|█████████▊| 785/805 [04:54<00:05,  3.56it/s] 98%|█████████▊| 786/805 [04:54<00:05,  3.58it/s] 98%|█████████▊| 787/805 [04:54<00:04,  3.60it/s] 98%|█████████▊| 788/805 [04:55<00:04,  3.61it/s] 98%|█████████▊| 789/805 [04:55<00:04,  3.62it/s] 98%|█████████▊| 790/805 [04:55<00:04,  3.63it/s] 98%|█████████▊| 791/805 [04:55<00:03,  3.63it/s] 98%|█████████▊| 792/805 [04:56<00:03,  3.63it/s] 99%|█████████▊| 793/805 [04:56<00:03,  3.56it/s] 99%|█████████▊| 794/805 [04:56<00:03,  3.58it/s] 99%|█████████▉| 795/805 [04:56<00:02,  3.60it/s] 99%|█████████▉| 796/805 [04:57<00:02,  3.61it/s] 99%|█████████▉| 797/805 [04:57<00:02,  3.62it/s] 99%|█████████▉| 798/805 [04:57<00:01,  3.63it/s] 99%|█████████▉| 799/805 [04:58<00:01,  3.63it/s] 99%|█████████▉| 800/805 [04:58<00:01,  3.63it/s]100%|█████████▉| 801/805 [04:58<00:01,  3.63it/s]100%|█████████▉| 802/805 [04:58<00:00,  3.63it/s]100%|█████████▉| 803/805 [04:59<00:00,  3.64it/s]100%|█████████▉| 804/805 [04:59<00:00,  3.46it/s]100%|██████████| 805/805 [04:59<00:00,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 03:21:21,213 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:21:21,213 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 03:21:21,213 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.5255, 'eval_samples_per_second': 359.617, 'eval_steps_per_second': 44.952, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.46it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.28it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.81it/s][A
  4%|▎         | 22/608 [00:00<00:12, 47.11it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.46it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.92it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.41it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.19it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.12it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.24it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.39it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.45it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.56it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.46it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.38it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.07it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.07it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.98it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.12it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.24it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.42it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.42it/s][A
 19%|█▉        | 117/608 [00:02<00:11, 43.36it/s][A
 20%|██        | 122/608 [00:02<00:11, 43.93it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.26it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.48it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.66it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.80it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.95it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.11it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 45.05it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.16it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.31it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.27it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 45.11it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 45.26it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.18it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.20it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.17it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.30it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.33it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.33it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.26it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.21it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.17it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.16it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.13it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.19it/s][A
 41%|████      | 247/608 [00:05<00:07, 45.25it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.53it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.84it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.90it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.93it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.02it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.05it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.14it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.14it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.19it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.23it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.15it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.23it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.20it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 45.24it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.21it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.20it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.12it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 45.16it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.28it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.30it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.30it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.22it/s][A
 60%|█████▉    | 362/608 [00:07<00:05, 45.21it/s][A
 60%|██████    | 367/608 [00:08<00:05, 45.16it/s][A
 61%|██████    | 372/608 [00:08<00:05, 45.16it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.09it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 45.08it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.20it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.34it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 44.73it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.84it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.93it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.99it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.05it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.11it/s][A
 70%|███████   | 427/608 [00:09<00:04, 45.11it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.09it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.17it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.29it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.31it/s][A
 74%|███████▍  | 452/608 [00:09<00:03, 45.37it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 45.28it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.29it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.23it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.21it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.13it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.21it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.21it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.26it/s][A
 82%|████████▏ | 497/608 [00:10<00:02, 45.27it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.27it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.27it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.12it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.19it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.11it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.03it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.24it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.30it/s][A
 89%|████████▉ | 542/608 [00:11<00:01, 45.22it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.30it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.24it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.19it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.12it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.17it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.14it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.23it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.21it/s][A
 97%|█████████▋| 587/608 [00:12<00:00, 45.19it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.33it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 45.26it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 45.28it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.16it/s][A                                                 
                                                 [A100%|██████████| 805/805 [05:13<00:00,  3.51it/s]
100%|██████████| 608/608 [00:13<00:00, 45.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 03:21:34,842 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805
[INFO|configuration_utils.py:351] 2023-08-28 03:21:34,960 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:21:38,203 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:21:38,296 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:21:38,351 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 03:21:39,246 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 03:21:39,247 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161 (score: 0.9957473874092102).
                                                 100%|██████████| 805/805 [05:24<00:00,  3.51it/s]100%|██████████| 805/805 [05:24<00:00,  2.48it/s]
[INFO|trainer.py:1894] 2023-08-28 03:21:45,681 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 03:21:45,777 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 03:21:49,401 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 03:21:49,573 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 03:21:49,648 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:21:50,187 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:21:50,187 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:21:50,187 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:21:50,187 >>   train_runtime            = 0:05:24.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:21:50,187 >>   train_samples            =      10320
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:21:50,187 >>   train_samples_per_second =     159.14
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:21:50,187 >>   train_steps_per_second   =      2.483
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.4728, 'eval_samples_per_second': 361.025, 'eval_steps_per_second': 45.128, 'epoch': 5.0}
{'train_runtime': 324.2437, 'train_samples_per_second': 159.14, 'train_steps_per_second': 2.483, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 03:21:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 03:21:50,442 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 03:21:50,442 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 03:21:50,442 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 56.42it/s]  2%|▏         | 12/608 [00:00<00:11, 50.02it/s]  3%|▎         | 18/608 [00:00<00:12, 48.19it/s]  4%|▍         | 23/608 [00:00<00:12, 47.38it/s]  5%|▍         | 28/608 [00:00<00:12, 46.98it/s]  5%|▌         | 33/608 [00:00<00:12, 46.75it/s]  6%|▋         | 38/608 [00:00<00:12, 46.52it/s]  7%|▋         | 43/608 [00:00<00:12, 46.09it/s]  8%|▊         | 48/608 [00:01<00:12, 45.47it/s]  9%|▊         | 53/608 [00:01<00:12, 45.28it/s] 10%|▉         | 58/608 [00:01<00:12, 45.36it/s] 10%|█         | 63/608 [00:01<00:11, 45.49it/s] 11%|█         | 68/608 [00:01<00:11, 45.61it/s] 12%|█▏        | 73/608 [00:01<00:11, 45.76it/s] 13%|█▎        | 78/608 [00:01<00:11, 45.88it/s] 14%|█▎        | 83/608 [00:01<00:11, 45.79it/s] 14%|█▍        | 88/608 [00:01<00:11, 45.50it/s] 15%|█▌        | 93/608 [00:02<00:11, 45.12it/s] 16%|█▌        | 98/608 [00:02<00:11, 43.96it/s] 17%|█▋        | 103/608 [00:02<00:11, 44.38it/s] 18%|█▊        | 108/608 [00:02<00:11, 44.89it/s] 19%|█▊        | 113/608 [00:02<00:10, 45.16it/s] 19%|█▉        | 118/608 [00:02<00:10, 45.40it/s] 20%|██        | 123/608 [00:02<00:10, 45.56it/s] 21%|██        | 128/608 [00:02<00:10, 45.61it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.50it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.22it/s] 24%|██▎       | 143/608 [00:03<00:10, 45.01it/s] 24%|██▍       | 148/608 [00:03<00:10, 45.13it/s] 25%|██▌       | 153/608 [00:03<00:10, 45.36it/s] 26%|██▌       | 158/608 [00:03<00:09, 45.59it/s] 27%|██▋       | 163/608 [00:03<00:09, 45.68it/s] 28%|██▊       | 168/608 [00:03<00:09, 45.76it/s] 28%|██▊       | 173/608 [00:03<00:09, 45.75it/s] 29%|██▉       | 178/608 [00:03<00:09, 45.53it/s] 30%|███       | 183/608 [00:04<00:09, 45.33it/s] 31%|███       | 188/608 [00:04<00:09, 45.20it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.17it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.11it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.38it/s] 34%|███▍      | 208/608 [00:04<00:08, 45.58it/s] 35%|███▌      | 213/608 [00:04<00:08, 45.67it/s] 36%|███▌      | 218/608 [00:04<00:08, 45.80it/s] 37%|███▋      | 223/608 [00:04<00:08, 45.64it/s] 38%|███▊      | 228/608 [00:04<00:08, 45.48it/s] 38%|███▊      | 233/608 [00:05<00:08, 45.27it/s] 39%|███▉      | 238/608 [00:05<00:08, 45.22it/s] 40%|███▉      | 243/608 [00:05<00:08, 45.22it/s] 41%|████      | 248/608 [00:05<00:07, 45.38it/s] 42%|████▏     | 253/608 [00:05<00:07, 45.42it/s] 42%|████▏     | 258/608 [00:05<00:07, 45.70it/s] 43%|████▎     | 263/608 [00:05<00:07, 45.56it/s] 44%|████▍     | 268/608 [00:05<00:07, 45.70it/s] 45%|████▍     | 273/608 [00:05<00:07, 45.43it/s] 46%|████▌     | 278/608 [00:06<00:07, 45.39it/s] 47%|████▋     | 283/608 [00:06<00:07, 45.35it/s] 47%|████▋     | 288/608 [00:06<00:07, 45.43it/s] 48%|████▊     | 293/608 [00:06<00:06, 45.44it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.46it/s] 50%|████▉     | 303/608 [00:06<00:06, 45.52it/s] 51%|█████     | 308/608 [00:06<00:06, 45.62it/s] 51%|█████▏    | 313/608 [00:06<00:06, 45.63it/s] 52%|█████▏    | 318/608 [00:06<00:06, 45.47it/s] 53%|█████▎    | 323/608 [00:07<00:06, 45.42it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.45it/s] 55%|█████▍    | 333/608 [00:07<00:06, 45.34it/s] 56%|█████▌    | 338/608 [00:07<00:05, 45.45it/s] 56%|█████▋    | 343/608 [00:07<00:05, 45.44it/s] 57%|█████▋    | 348/608 [00:07<00:05, 45.53it/s] 58%|█████▊    | 353/608 [00:07<00:05, 45.50it/s] 59%|█████▉    | 358/608 [00:07<00:05, 45.62it/s] 60%|█████▉    | 363/608 [00:07<00:05, 45.48it/s] 61%|██████    | 368/608 [00:08<00:05, 45.46it/s] 61%|██████▏   | 373/608 [00:08<00:05, 45.34it/s] 62%|██████▏   | 378/608 [00:08<00:05, 44.84it/s] 63%|██████▎   | 383/608 [00:08<00:04, 45.02it/s] 64%|██████▍   | 388/608 [00:08<00:04, 45.26it/s] 65%|██████▍   | 393/608 [00:08<00:04, 45.29it/s] 65%|██████▌   | 398/608 [00:08<00:04, 45.44it/s] 66%|██████▋   | 403/608 [00:08<00:04, 45.38it/s] 67%|██████▋   | 408/608 [00:08<00:04, 45.47it/s] 68%|██████▊   | 413/608 [00:09<00:04, 45.39it/s] 69%|██████▉   | 418/608 [00:09<00:04, 45.36it/s] 70%|██████▉   | 423/608 [00:09<00:04, 45.29it/s] 70%|███████   | 428/608 [00:09<00:03, 45.40it/s] 71%|███████   | 433/608 [00:09<00:03, 45.48it/s] 72%|███████▏  | 438/608 [00:09<00:03, 45.51it/s] 73%|███████▎  | 443/608 [00:09<00:03, 45.53it/s] 74%|███████▎  | 448/608 [00:09<00:03, 45.53it/s] 75%|███████▍  | 453/608 [00:09<00:03, 45.55it/s] 75%|███████▌  | 458/608 [00:10<00:03, 45.50it/s] 76%|███████▌  | 463/608 [00:10<00:03, 45.41it/s] 77%|███████▋  | 468/608 [00:10<00:03, 45.34it/s] 78%|███████▊  | 473/608 [00:10<00:02, 45.42it/s] 79%|███████▊  | 478/608 [00:10<00:02, 45.46it/s] 79%|███████▉  | 483/608 [00:10<00:02, 45.48it/s] 80%|████████  | 488/608 [00:10<00:02, 45.41it/s] 81%|████████  | 493/608 [00:10<00:02, 45.42it/s] 82%|████████▏ | 498/608 [00:10<00:02, 45.39it/s] 83%|████████▎ | 503/608 [00:11<00:02, 45.41it/s] 84%|████████▎ | 508/608 [00:11<00:02, 45.32it/s] 84%|████████▍ | 513/608 [00:11<00:02, 45.31it/s] 85%|████████▌ | 518/608 [00:11<00:02, 44.29it/s] 86%|████████▌ | 523/608 [00:11<00:01, 44.73it/s] 87%|████████▋ | 528/608 [00:11<00:01, 45.08it/s] 88%|████████▊ | 533/608 [00:11<00:01, 45.00it/s] 88%|████████▊ | 538/608 [00:11<00:01, 45.35it/s] 89%|████████▉ | 543/608 [00:11<00:01, 45.35it/s] 90%|█████████ | 548/608 [00:12<00:01, 45.38it/s] 91%|█████████ | 553/608 [00:12<00:01, 45.23it/s] 92%|█████████▏| 558/608 [00:12<00:01, 45.12it/s] 93%|█████████▎| 563/608 [00:12<00:00, 45.13it/s] 93%|█████████▎| 568/608 [00:12<00:00, 45.33it/s] 94%|█████████▍| 573/608 [00:12<00:00, 45.42it/s] 95%|█████████▌| 578/608 [00:12<00:00, 45.58it/s] 96%|█████████▌| 583/608 [00:12<00:00, 45.56it/s] 97%|█████████▋| 588/608 [00:12<00:00, 45.61it/s] 98%|█████████▊| 593/608 [00:13<00:00, 45.50it/s] 98%|█████████▊| 598/608 [00:13<00:00, 45.43it/s] 99%|█████████▉| 603/608 [00:13<00:00, 45.32it/s]100%|██████████| 608/608 [00:13<00:00, 45.23it/s]100%|██████████| 608/608 [00:13<00:00, 45.44it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 03:22:03,841 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:22:03,841 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:22:03,841 >>   eval_loss               =     0.9957
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:22:03,841 >>   eval_runtime            = 0:00:13.39
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:22:03,841 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:22:03,841 >>   eval_samples_per_second =    363.012
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:22:03,841 >>   eval_steps_per_second   =     45.376
[INFO|trainer_pt_utils.py:913] 2023-08-28 03:22:03,842 >>   perplexity              =     2.7067
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:14,636 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:14,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:14,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:14,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:14,638 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:22:15,803 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:22:15,851 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:22:16,480 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:22:17,639 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:22:17,639 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:20,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:20,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:20,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:20,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:22:20,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:22:22,121 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:22:22,122 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:22:22,844 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:22:23,171 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:22:23,171 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-483
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-644
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-322
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-805
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/checkpoint-161
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.87it/s]Extractor Predicting: 5it [00:02,  1.91it/s]Extractor Predicting: 6it [00:03,  1.97it/s]Extractor Predicting: 7it [00:03,  1.90it/s]Extractor Predicting: 8it [00:04,  1.86it/s]Extractor Predicting: 9it [00:04,  1.89it/s]Extractor Predicting: 10it [00:05,  1.97it/s]Extractor Predicting: 11it [00:05,  1.96it/s]Extractor Predicting: 12it [00:06,  2.01it/s]Extractor Predicting: 13it [00:06,  1.98it/s]Extractor Predicting: 14it [00:07,  1.94it/s]Extractor Predicting: 15it [00:07,  2.01it/s]Extractor Predicting: 16it [00:08,  1.99it/s]Extractor Predicting: 17it [00:08,  1.97it/s]Extractor Predicting: 18it [00:09,  1.96it/s]Extractor Predicting: 19it [00:09,  1.90it/s]Extractor Predicting: 20it [00:10,  1.98it/s]Extractor Predicting: 21it [00:10,  1.97it/s]Extractor Predicting: 22it [00:11,  1.95it/s]Extractor Predicting: 23it [00:12,  1.86it/s]Extractor Predicting: 24it [00:12,  1.79it/s]Extractor Predicting: 25it [00:13,  1.72it/s]Extractor Predicting: 26it [00:13,  1.72it/s]Extractor Predicting: 27it [00:14,  1.75it/s]Extractor Predicting: 28it [00:14,  1.76it/s]Extractor Predicting: 29it [00:15,  1.68it/s]Extractor Predicting: 30it [00:16,  1.74it/s]Extractor Predicting: 31it [00:16,  1.78it/s]Extractor Predicting: 32it [00:17,  1.80it/s]Extractor Predicting: 33it [00:17,  1.82it/s]Extractor Predicting: 34it [00:18,  1.84it/s]Extractor Predicting: 35it [00:18,  1.82it/s]Extractor Predicting: 36it [00:19,  1.83it/s]Extractor Predicting: 37it [00:19,  1.81it/s]Extractor Predicting: 38it [00:20,  1.64it/s]Extractor Predicting: 39it [00:21,  1.69it/s]Extractor Predicting: 40it [00:21,  1.73it/s]Extractor Predicting: 41it [00:22,  1.77it/s]Extractor Predicting: 42it [00:22,  1.77it/s]Extractor Predicting: 43it [00:23,  1.77it/s]Extractor Predicting: 44it [00:23,  1.81it/s]Extractor Predicting: 45it [00:24,  1.79it/s]Extractor Predicting: 46it [00:25,  1.81it/s]Extractor Predicting: 47it [00:25,  1.82it/s]Extractor Predicting: 48it [00:26,  1.83it/s]Extractor Predicting: 49it [00:26,  1.81it/s]Extractor Predicting: 50it [00:27,  1.79it/s]Extractor Predicting: 51it [00:27,  1.79it/s]Extractor Predicting: 52it [00:28,  1.79it/s]Extractor Predicting: 53it [00:28,  1.79it/s]Extractor Predicting: 54it [00:29,  1.85it/s]Extractor Predicting: 55it [00:29,  1.88it/s]Extractor Predicting: 56it [00:30,  1.81it/s]Extractor Predicting: 57it [00:31,  1.81it/s]Extractor Predicting: 58it [00:31,  1.80it/s]Extractor Predicting: 59it [00:32,  1.81it/s]Extractor Predicting: 60it [00:32,  1.81it/s]Extractor Predicting: 61it [00:33,  1.75it/s]Extractor Predicting: 62it [00:33,  1.74it/s]Extractor Predicting: 63it [00:34,  1.76it/s]Extractor Predicting: 64it [00:35,  1.79it/s]Extractor Predicting: 65it [00:35,  1.79it/s]Extractor Predicting: 66it [00:36,  1.81it/s]Extractor Predicting: 67it [00:36,  1.82it/s]Extractor Predicting: 68it [00:37,  1.74it/s]Extractor Predicting: 69it [00:37,  1.75it/s]Extractor Predicting: 70it [00:38,  1.80it/s]Extractor Predicting: 71it [00:38,  1.86it/s]Extractor Predicting: 72it [00:39,  1.84it/s]Extractor Predicting: 73it [00:40,  1.85it/s]Extractor Predicting: 74it [00:40,  1.81it/s]Extractor Predicting: 75it [00:41,  1.83it/s]Extractor Predicting: 76it [00:41,  1.82it/s]Extractor Predicting: 77it [00:42,  1.81it/s]Extractor Predicting: 78it [00:42,  1.85it/s]Extractor Predicting: 79it [00:43,  1.85it/s]Extractor Predicting: 80it [00:43,  1.85it/s]Extractor Predicting: 81it [00:44,  1.82it/s]Extractor Predicting: 82it [00:44,  1.80it/s]Extractor Predicting: 83it [00:45,  1.79it/s]Extractor Predicting: 84it [00:46,  1.81it/s]Extractor Predicting: 85it [00:46,  1.80it/s]Extractor Predicting: 86it [00:47,  1.78it/s]Extractor Predicting: 87it [00:47,  1.76it/s]Extractor Predicting: 88it [00:48,  1.77it/s]Extractor Predicting: 89it [00:49,  1.68it/s]Extractor Predicting: 90it [00:49,  1.74it/s]Extractor Predicting: 91it [00:50,  1.78it/s]Extractor Predicting: 92it [00:50,  1.80it/s]Extractor Predicting: 93it [00:51,  1.79it/s]Extractor Predicting: 94it [00:51,  1.79it/s]Extractor Predicting: 95it [00:52,  1.73it/s]Extractor Predicting: 96it [00:52,  1.76it/s]Extractor Predicting: 97it [00:53,  1.77it/s]Extractor Predicting: 98it [00:54,  1.80it/s]Extractor Predicting: 99it [00:54,  1.76it/s]Extractor Predicting: 100it [00:55,  1.76it/s]Extractor Predicting: 101it [00:55,  1.78it/s]Extractor Predicting: 102it [00:56,  1.82it/s]Extractor Predicting: 103it [00:56,  1.85it/s]Extractor Predicting: 104it [00:57,  1.84it/s]Extractor Predicting: 105it [00:57,  1.76it/s]Extractor Predicting: 106it [00:58,  1.78it/s]Extractor Predicting: 107it [00:59,  1.80it/s]Extractor Predicting: 108it [00:59,  1.84it/s]Extractor Predicting: 109it [01:00,  1.79it/s]Extractor Predicting: 110it [01:00,  1.76it/s]Extractor Predicting: 111it [01:01,  1.79it/s]Extractor Predicting: 112it [01:01,  1.79it/s]Extractor Predicting: 113it [01:02,  1.81it/s]Extractor Predicting: 114it [01:02,  1.86it/s]Extractor Predicting: 115it [01:03,  1.93it/s]Extractor Predicting: 116it [01:03,  1.93it/s]Extractor Predicting: 117it [01:04,  1.85it/s]Extractor Predicting: 118it [01:04,  1.89it/s]Extractor Predicting: 119it [01:05,  1.87it/s]Extractor Predicting: 120it [01:06,  1.90it/s]Extractor Predicting: 121it [01:06,  1.87it/s]Extractor Predicting: 122it [01:07,  1.82it/s]Extractor Predicting: 123it [01:07,  1.75it/s]Extractor Predicting: 124it [01:08,  1.71it/s]Extractor Predicting: 125it [01:08,  1.74it/s]Extractor Predicting: 126it [01:09,  1.78it/s]Extractor Predicting: 127it [01:10,  1.78it/s]Extractor Predicting: 128it [01:10,  1.73it/s]Extractor Predicting: 129it [01:11,  1.73it/s]Extractor Predicting: 130it [01:11,  1.77it/s]Extractor Predicting: 131it [01:12,  1.65it/s]Extractor Predicting: 132it [01:13,  1.66it/s]Extractor Predicting: 133it [01:13,  1.69it/s]Extractor Predicting: 134it [01:14,  1.73it/s]Extractor Predicting: 135it [01:14,  1.77it/s]Extractor Predicting: 136it [01:15,  1.78it/s]Extractor Predicting: 137it [01:15,  1.81it/s]Extractor Predicting: 138it [01:16,  1.80it/s]Extractor Predicting: 139it [01:16,  1.80it/s]Extractor Predicting: 140it [01:17,  1.79it/s]Extractor Predicting: 141it [01:17,  1.84it/s]Extractor Predicting: 142it [01:18,  1.78it/s]Extractor Predicting: 143it [01:19,  1.83it/s]Extractor Predicting: 144it [01:19,  1.84it/s]Extractor Predicting: 145it [01:20,  1.85it/s]Extractor Predicting: 146it [01:20,  1.78it/s]Extractor Predicting: 147it [01:21,  1.76it/s]Extractor Predicting: 148it [01:21,  1.75it/s]Extractor Predicting: 149it [01:22,  1.74it/s]Extractor Predicting: 150it [01:23,  1.76it/s]Extractor Predicting: 151it [01:23,  1.75it/s]Extractor Predicting: 152it [01:24,  1.81it/s]Extractor Predicting: 153it [01:24,  1.78it/s]Extractor Predicting: 154it [01:25,  1.74it/s]Extractor Predicting: 155it [01:25,  1.78it/s]Extractor Predicting: 156it [01:26,  1.83it/s]Extractor Predicting: 157it [01:26,  1.83it/s]Extractor Predicting: 158it [01:27,  1.86it/s]Extractor Predicting: 159it [01:28,  1.82it/s]Extractor Predicting: 160it [01:28,  1.80it/s]Extractor Predicting: 161it [01:29,  1.78it/s]Extractor Predicting: 162it [01:29,  1.76it/s]Extractor Predicting: 163it [01:30,  1.78it/s]Extractor Predicting: 164it [01:30,  1.77it/s]Extractor Predicting: 165it [01:31,  1.81it/s]Extractor Predicting: 166it [01:32,  1.75it/s]Extractor Predicting: 167it [01:32,  1.74it/s]Extractor Predicting: 168it [01:33,  1.72it/s]Extractor Predicting: 169it [01:33,  1.68it/s]Extractor Predicting: 170it [01:34,  1.70it/s]Extractor Predicting: 171it [01:34,  1.70it/s]Extractor Predicting: 172it [01:35,  1.70it/s]Extractor Predicting: 173it [01:36,  1.68it/s]Extractor Predicting: 174it [01:36,  1.66it/s]Extractor Predicting: 175it [01:37,  1.72it/s]Extractor Predicting: 176it [01:37,  1.70it/s]Extractor Predicting: 177it [01:38,  1.67it/s]Extractor Predicting: 178it [01:39,  1.62it/s]Extractor Predicting: 179it [01:39,  1.60it/s]Extractor Predicting: 180it [01:40,  1.61it/s]Extractor Predicting: 181it [01:41,  1.65it/s]Extractor Predicting: 182it [01:41,  1.64it/s]Extractor Predicting: 183it [01:42,  1.60it/s]Extractor Predicting: 184it [01:42,  1.62it/s]Extractor Predicting: 185it [01:43,  1.68it/s]Extractor Predicting: 186it [01:44,  1.69it/s]Extractor Predicting: 187it [01:44,  1.84it/s]Extractor Predicting: 187it [01:44,  1.79it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:29,870 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:29,939 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:29,939 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:29,939 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:29,939 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:24:30,959 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:24:30,960 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:24:31,622 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:24:32,688 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:24:32,688 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:35,665 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:35,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:35,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:35,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:24:35,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:24:36,458 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:24:36,460 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:24:37,085 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:24:37,287 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:24:37,287 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.78it/s]Extractor Predicting: 7it [00:04,  1.76it/s]Extractor Predicting: 8it [00:04,  1.77it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 10it [00:05,  1.78it/s]Extractor Predicting: 11it [00:06,  1.82it/s]Extractor Predicting: 12it [00:06,  1.85it/s]Extractor Predicting: 13it [00:07,  1.83it/s]Extractor Predicting: 14it [00:07,  1.84it/s]Extractor Predicting: 15it [00:08,  1.83it/s]Extractor Predicting: 16it [00:08,  1.82it/s]Extractor Predicting: 17it [00:09,  1.82it/s]Extractor Predicting: 18it [00:10,  1.77it/s]Extractor Predicting: 19it [00:10,  1.79it/s]Extractor Predicting: 20it [00:11,  1.78it/s]Extractor Predicting: 21it [00:11,  1.74it/s]Extractor Predicting: 22it [00:12,  1.72it/s]Extractor Predicting: 23it [00:12,  1.76it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.80it/s]Extractor Predicting: 26it [00:14,  1.80it/s]Extractor Predicting: 27it [00:15,  1.77it/s]Extractor Predicting: 28it [00:15,  1.77it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:16,  1.81it/s]Extractor Predicting: 31it [00:17,  1.76it/s]Extractor Predicting: 32it [00:18,  1.74it/s]Extractor Predicting: 33it [00:18,  1.77it/s]Extractor Predicting: 34it [00:19,  1.80it/s]Extractor Predicting: 35it [00:19,  1.79it/s]Extractor Predicting: 36it [00:20,  1.82it/s]Extractor Predicting: 37it [00:20,  1.79it/s]Extractor Predicting: 38it [00:21,  1.77it/s]Extractor Predicting: 39it [00:21,  1.77it/s]Extractor Predicting: 40it [00:22,  1.78it/s]Extractor Predicting: 41it [00:23,  1.82it/s]Extractor Predicting: 42it [00:23,  1.80it/s]Extractor Predicting: 43it [00:24,  1.84it/s]Extractor Predicting: 44it [00:24,  1.82it/s]Extractor Predicting: 45it [00:25,  1.74it/s]Extractor Predicting: 46it [00:25,  1.75it/s]Extractor Predicting: 47it [00:26,  1.71it/s]Extractor Predicting: 48it [00:27,  1.71it/s]Extractor Predicting: 49it [00:27,  1.69it/s]Extractor Predicting: 50it [00:28,  1.70it/s]Extractor Predicting: 51it [00:28,  1.70it/s]Extractor Predicting: 52it [00:29,  1.71it/s]Extractor Predicting: 53it [00:29,  1.79it/s]Extractor Predicting: 54it [00:30,  1.81it/s]Extractor Predicting: 55it [00:30,  1.83it/s]Extractor Predicting: 56it [00:31,  1.78it/s]Extractor Predicting: 57it [00:32,  1.76it/s]Extractor Predicting: 58it [00:32,  1.80it/s]Extractor Predicting: 59it [00:33,  1.82it/s]Extractor Predicting: 60it [00:33,  1.85it/s]Extractor Predicting: 61it [00:34,  1.81it/s]Extractor Predicting: 62it [00:34,  1.78it/s]Extractor Predicting: 63it [00:35,  1.79it/s]Extractor Predicting: 64it [00:36,  1.79it/s]Extractor Predicting: 65it [00:36,  1.81it/s]Extractor Predicting: 66it [00:37,  1.84it/s]Extractor Predicting: 67it [00:37,  1.86it/s]Extractor Predicting: 68it [00:38,  1.74it/s]Extractor Predicting: 69it [00:38,  1.74it/s]Extractor Predicting: 70it [00:39,  1.70it/s]Extractor Predicting: 71it [00:40,  1.75it/s]Extractor Predicting: 72it [00:40,  1.77it/s]Extractor Predicting: 73it [00:41,  1.84it/s]Extractor Predicting: 74it [00:41,  1.86it/s]Extractor Predicting: 75it [00:42,  1.82it/s]Extractor Predicting: 76it [00:42,  1.78it/s]Extractor Predicting: 77it [00:43,  1.81it/s]Extractor Predicting: 78it [00:43,  1.87it/s]Extractor Predicting: 79it [00:44,  1.85it/s]Extractor Predicting: 80it [00:44,  1.87it/s]Extractor Predicting: 81it [00:45,  1.81it/s]Extractor Predicting: 82it [00:45,  1.83it/s]Extractor Predicting: 83it [00:46,  1.84it/s]Extractor Predicting: 84it [00:47,  1.79it/s]Extractor Predicting: 85it [00:47,  1.84it/s]Extractor Predicting: 86it [00:48,  1.87it/s]Extractor Predicting: 87it [00:48,  1.88it/s]Extractor Predicting: 88it [00:49,  1.93it/s]Extractor Predicting: 89it [00:49,  1.91it/s]Extractor Predicting: 90it [00:50,  1.86it/s]Extractor Predicting: 91it [00:50,  1.92it/s]Extractor Predicting: 92it [00:51,  1.95it/s]Extractor Predicting: 93it [00:51,  1.74it/s]Extractor Predicting: 94it [00:52,  1.81it/s]Extractor Predicting: 95it [00:52,  1.84it/s]Extractor Predicting: 96it [00:53,  1.89it/s]Extractor Predicting: 97it [00:53,  1.88it/s]Extractor Predicting: 98it [00:54,  1.87it/s]Extractor Predicting: 99it [00:55,  1.83it/s]Extractor Predicting: 100it [00:55,  1.87it/s]Extractor Predicting: 101it [00:56,  1.88it/s]Extractor Predicting: 102it [00:56,  1.88it/s]Extractor Predicting: 103it [00:57,  1.85it/s]Extractor Predicting: 104it [00:57,  1.86it/s]Extractor Predicting: 105it [00:58,  1.77it/s]Extractor Predicting: 106it [00:58,  1.78it/s]Extractor Predicting: 107it [00:59,  1.81it/s]Extractor Predicting: 108it [01:00,  1.82it/s]Extractor Predicting: 109it [01:00,  1.79it/s]Extractor Predicting: 110it [01:01,  1.79it/s]Extractor Predicting: 111it [01:01,  1.76it/s]Extractor Predicting: 112it [01:02,  1.81it/s]Extractor Predicting: 113it [01:02,  1.82it/s]Extractor Predicting: 114it [01:03,  1.85it/s]Extractor Predicting: 115it [01:03,  1.83it/s]Extractor Predicting: 116it [01:04,  1.82it/s]Extractor Predicting: 117it [01:05,  1.80it/s]Extractor Predicting: 118it [01:05,  1.80it/s]Extractor Predicting: 119it [01:06,  1.83it/s]Extractor Predicting: 120it [01:06,  1.81it/s]Extractor Predicting: 121it [01:07,  1.83it/s]Extractor Predicting: 122it [01:07,  1.84it/s]Extractor Predicting: 123it [01:08,  1.85it/s]Extractor Predicting: 124it [01:08,  1.86it/s]Extractor Predicting: 125it [01:09,  1.84it/s]Extractor Predicting: 126it [01:10,  1.57it/s]Extractor Predicting: 127it [01:10,  1.65it/s]Extractor Predicting: 128it [01:11,  1.67it/s]Extractor Predicting: 129it [01:11,  1.68it/s]Extractor Predicting: 130it [01:12,  1.75it/s]Extractor Predicting: 131it [01:12,  1.74it/s]Extractor Predicting: 132it [01:13,  1.75it/s]Extractor Predicting: 133it [01:14,  1.80it/s]Extractor Predicting: 134it [01:14,  1.78it/s]Extractor Predicting: 135it [01:15,  1.81it/s]Extractor Predicting: 136it [01:15,  1.82it/s]Extractor Predicting: 137it [01:16,  1.79it/s]Extractor Predicting: 138it [01:16,  1.78it/s]Extractor Predicting: 139it [01:17,  1.81it/s]Extractor Predicting: 140it [01:17,  1.81it/s]Extractor Predicting: 141it [01:18,  1.80it/s]Extractor Predicting: 142it [01:19,  1.79it/s]Extractor Predicting: 143it [01:19,  1.78it/s]Extractor Predicting: 144it [01:20,  1.78it/s]Extractor Predicting: 145it [01:20,  1.77it/s]Extractor Predicting: 146it [01:21,  1.78it/s]Extractor Predicting: 147it [01:21,  1.78it/s]Extractor Predicting: 148it [01:22,  1.81it/s]Extractor Predicting: 149it [01:23,  1.76it/s]Extractor Predicting: 150it [01:23,  1.73it/s]Extractor Predicting: 151it [01:24,  1.73it/s]Extractor Predicting: 152it [01:24,  1.75it/s]Extractor Predicting: 153it [01:25,  1.81it/s]Extractor Predicting: 154it [01:25,  1.83it/s]Extractor Predicting: 155it [01:26,  1.81it/s]Extractor Predicting: 156it [01:26,  1.79it/s]Extractor Predicting: 157it [01:27,  1.80it/s]Extractor Predicting: 158it [01:28,  1.80it/s]Extractor Predicting: 159it [01:28,  1.78it/s]Extractor Predicting: 160it [01:29,  1.78it/s]Extractor Predicting: 161it [01:29,  1.78it/s]Extractor Predicting: 162it [01:30,  1.77it/s]Extractor Predicting: 163it [01:30,  1.80it/s]Extractor Predicting: 164it [01:31,  1.80it/s]Extractor Predicting: 165it [01:31,  1.82it/s]Extractor Predicting: 166it [01:32,  1.82it/s]Extractor Predicting: 167it [01:33,  1.80it/s]Extractor Predicting: 168it [01:33,  1.78it/s]Extractor Predicting: 169it [01:34,  1.78it/s]Extractor Predicting: 170it [01:34,  1.82it/s]Extractor Predicting: 171it [01:35,  1.89it/s]Extractor Predicting: 172it [01:35,  1.87it/s]Extractor Predicting: 173it [01:36,  1.81it/s]Extractor Predicting: 174it [01:36,  1.79it/s]Extractor Predicting: 175it [01:37,  1.73it/s]Extractor Predicting: 176it [01:38,  1.72it/s]Extractor Predicting: 177it [01:38,  1.76it/s]Extractor Predicting: 178it [01:39,  1.75it/s]Extractor Predicting: 179it [01:39,  1.76it/s]Extractor Predicting: 180it [01:40,  1.68it/s]Extractor Predicting: 181it [01:41,  1.70it/s]Extractor Predicting: 182it [01:41,  1.71it/s]Extractor Predicting: 183it [01:42,  1.71it/s]Extractor Predicting: 184it [01:42,  1.71it/s]Extractor Predicting: 185it [01:43,  1.72it/s]Extractor Predicting: 186it [01:43,  1.69it/s]Extractor Predicting: 187it [01:44,  1.68it/s]Extractor Predicting: 188it [01:45,  1.73it/s]Extractor Predicting: 189it [01:45,  1.75it/s]Extractor Predicting: 190it [01:46,  1.73it/s]Extractor Predicting: 191it [01:46,  1.76it/s]Extractor Predicting: 192it [01:47,  1.72it/s]Extractor Predicting: 193it [01:48,  1.72it/s]Extractor Predicting: 194it [01:48,  1.75it/s]Extractor Predicting: 195it [01:49,  1.80it/s]Extractor Predicting: 196it [01:49,  1.84it/s]Extractor Predicting: 197it [01:50,  1.82it/s]Extractor Predicting: 198it [01:50,  1.79it/s]Extractor Predicting: 199it [01:51,  1.80it/s]Extractor Predicting: 200it [01:51,  1.83it/s]Extractor Predicting: 201it [01:52,  1.81it/s]Extractor Predicting: 202it [01:52,  1.80it/s]Extractor Predicting: 203it [01:53,  1.80it/s]Extractor Predicting: 204it [01:54,  1.78it/s]Extractor Predicting: 205it [01:54,  1.78it/s]Extractor Predicting: 206it [01:55,  1.81it/s]Extractor Predicting: 207it [01:55,  1.79it/s]Extractor Predicting: 208it [01:56,  1.84it/s]Extractor Predicting: 209it [01:56,  1.86it/s]Extractor Predicting: 210it [01:57,  1.80it/s]Extractor Predicting: 211it [01:57,  1.81it/s]Extractor Predicting: 212it [01:58,  1.83it/s]Extractor Predicting: 213it [01:59,  1.79it/s]Extractor Predicting: 214it [01:59,  1.77it/s]Extractor Predicting: 215it [02:00,  1.78it/s]Extractor Predicting: 216it [02:00,  1.75it/s]Extractor Predicting: 217it [02:01,  1.81it/s]Extractor Predicting: 218it [02:01,  1.78it/s]Extractor Predicting: 219it [02:02,  1.76it/s]Extractor Predicting: 220it [02:02,  1.82it/s]Extractor Predicting: 221it [02:03,  1.76it/s]Extractor Predicting: 222it [02:04,  1.74it/s]Extractor Predicting: 223it [02:04,  1.75it/s]Extractor Predicting: 224it [02:05,  1.79it/s]Extractor Predicting: 225it [02:05,  1.83it/s]Extractor Predicting: 226it [02:06,  1.82it/s]Extractor Predicting: 227it [02:06,  1.84it/s]Extractor Predicting: 228it [02:07,  1.82it/s]Extractor Predicting: 229it [02:07,  1.81it/s]Extractor Predicting: 230it [02:08,  1.85it/s]Extractor Predicting: 231it [02:08,  1.86it/s]Extractor Predicting: 232it [02:09,  1.86it/s]Extractor Predicting: 233it [02:10,  1.85it/s]Extractor Predicting: 234it [02:10,  1.81it/s]Extractor Predicting: 235it [02:11,  1.79it/s]Extractor Predicting: 236it [02:11,  1.81it/s]Extractor Predicting: 237it [02:12,  1.79it/s]Extractor Predicting: 238it [02:12,  1.80it/s]Extractor Predicting: 239it [02:13,  1.79it/s]Extractor Predicting: 240it [02:13,  1.82it/s]Extractor Predicting: 241it [02:14,  1.84it/s]Extractor Predicting: 242it [02:15,  1.56it/s]Extractor Predicting: 243it [02:15,  1.67it/s]Extractor Predicting: 244it [02:16,  1.73it/s]Extractor Predicting: 245it [02:16,  1.74it/s]Extractor Predicting: 246it [02:17,  1.79it/s]Extractor Predicting: 247it [02:18,  1.76it/s]Extractor Predicting: 248it [02:18,  1.81it/s]Extractor Predicting: 249it [02:19,  1.81it/s]Extractor Predicting: 250it [02:19,  1.89it/s]Extractor Predicting: 251it [02:20,  1.85it/s]Extractor Predicting: 252it [02:20,  1.87it/s]Extractor Predicting: 253it [02:21,  1.90it/s]Extractor Predicting: 254it [02:21,  1.85it/s]Extractor Predicting: 255it [02:22,  1.86it/s]Extractor Predicting: 256it [02:22,  1.86it/s]Extractor Predicting: 257it [02:23,  1.86it/s]Extractor Predicting: 258it [02:23,  1.87it/s]Extractor Predicting: 259it [02:24,  1.85it/s]Extractor Predicting: 260it [02:25,  1.84it/s]Extractor Predicting: 261it [02:25,  1.81it/s]Extractor Predicting: 262it [02:26,  1.83it/s]Extractor Predicting: 263it [02:26,  1.76it/s]Extractor Predicting: 264it [02:27,  1.81it/s]Extractor Predicting: 265it [02:27,  1.77it/s]Extractor Predicting: 266it [02:28,  1.75it/s]Extractor Predicting: 267it [02:29,  1.76it/s]Extractor Predicting: 268it [02:29,  1.76it/s]Extractor Predicting: 269it [02:30,  1.73it/s]Extractor Predicting: 270it [02:30,  1.76it/s]Extractor Predicting: 271it [02:31,  1.75it/s]Extractor Predicting: 272it [02:31,  1.81it/s]Extractor Predicting: 273it [02:32,  1.79it/s]Extractor Predicting: 274it [02:33,  1.76it/s]Extractor Predicting: 275it [02:33,  1.76it/s]Extractor Predicting: 276it [02:34,  1.80it/s]Extractor Predicting: 277it [02:34,  1.75it/s]Extractor Predicting: 278it [02:35,  1.77it/s]Extractor Predicting: 279it [02:35,  1.76it/s]Extractor Predicting: 280it [02:36,  1.77it/s]Extractor Predicting: 281it [02:36,  1.78it/s]Extractor Predicting: 282it [02:37,  1.78it/s]Extractor Predicting: 283it [02:38,  1.73it/s]Extractor Predicting: 284it [02:38,  1.77it/s]Extractor Predicting: 285it [02:39,  1.81it/s]Extractor Predicting: 286it [02:39,  1.80it/s]Extractor Predicting: 287it [02:40,  1.77it/s]Extractor Predicting: 288it [02:40,  1.77it/s]Extractor Predicting: 289it [02:41,  1.76it/s]Extractor Predicting: 290it [02:42,  1.57it/s]Extractor Predicting: 291it [02:42,  1.62it/s]Extractor Predicting: 292it [02:43,  1.68it/s]Extractor Predicting: 293it [02:43,  1.69it/s]Extractor Predicting: 294it [02:44,  1.67it/s]Extractor Predicting: 295it [02:45,  1.67it/s]Extractor Predicting: 296it [02:45,  1.70it/s]Extractor Predicting: 297it [02:46,  1.71it/s]Extractor Predicting: 298it [02:46,  1.72it/s]Extractor Predicting: 299it [02:47,  1.72it/s]Extractor Predicting: 300it [02:48,  1.67it/s]Extractor Predicting: 301it [02:48,  1.56it/s]Extractor Predicting: 302it [02:49,  1.57it/s]Extractor Predicting: 303it [02:50,  1.62it/s]Extractor Predicting: 304it [02:50,  1.58it/s]Extractor Predicting: 305it [02:51,  1.55it/s]Extractor Predicting: 306it [02:51,  1.60it/s]Extractor Predicting: 307it [02:52,  1.66it/s]Extractor Predicting: 308it [02:53,  1.71it/s]Extractor Predicting: 309it [02:53,  1.79it/s]Extractor Predicting: 310it [02:54,  1.78it/s]Extractor Predicting: 311it [02:54,  1.73it/s]Extractor Predicting: 312it [02:55,  1.72it/s]Extractor Predicting: 313it [02:55,  1.75it/s]Extractor Predicting: 314it [02:56,  1.73it/s]Extractor Predicting: 315it [02:56,  1.82it/s]Extractor Predicting: 316it [02:57,  1.83it/s]Extractor Predicting: 317it [02:58,  1.80it/s]Extractor Predicting: 318it [02:58,  1.78it/s]Extractor Predicting: 319it [02:59,  1.76it/s]Extractor Predicting: 320it [02:59,  1.79it/s]Extractor Predicting: 321it [03:00,  1.81it/s]Extractor Predicting: 322it [03:00,  1.85it/s]Extractor Predicting: 323it [03:01,  1.85it/s]Extractor Predicting: 324it [03:01,  1.84it/s]Extractor Predicting: 325it [03:02,  1.82it/s]Extractor Predicting: 326it [03:03,  1.85it/s]Extractor Predicting: 327it [03:03,  1.85it/s]Extractor Predicting: 328it [03:04,  1.83it/s]Extractor Predicting: 329it [03:04,  1.82it/s]Extractor Predicting: 330it [03:05,  1.78it/s]Extractor Predicting: 331it [03:05,  1.78it/s]Extractor Predicting: 332it [03:06,  1.81it/s]Extractor Predicting: 333it [03:06,  1.79it/s]Extractor Predicting: 334it [03:07,  1.80it/s]Extractor Predicting: 335it [03:08,  1.81it/s]Extractor Predicting: 336it [03:08,  1.60it/s]Extractor Predicting: 337it [03:09,  1.66it/s]Extractor Predicting: 338it [03:09,  1.72it/s]Extractor Predicting: 339it [03:10,  1.74it/s]Extractor Predicting: 340it [03:10,  1.78it/s]Extractor Predicting: 341it [03:11,  1.80it/s]Extractor Predicting: 342it [03:12,  1.78it/s]Extractor Predicting: 343it [03:12,  1.77it/s]Extractor Predicting: 344it [03:13,  1.83it/s]Extractor Predicting: 345it [03:13,  1.87it/s]Extractor Predicting: 346it [03:14,  1.90it/s]Extractor Predicting: 347it [03:14,  1.96it/s]Extractor Predicting: 348it [03:15,  1.92it/s]Extractor Predicting: 349it [03:15,  1.68it/s]Extractor Predicting: 350it [03:16,  1.73it/s]Extractor Predicting: 351it [03:17,  1.79it/s]Extractor Predicting: 352it [03:17,  1.82it/s]Extractor Predicting: 353it [03:18,  1.86it/s]Extractor Predicting: 354it [03:18,  1.89it/s]Extractor Predicting: 355it [03:19,  1.88it/s]Extractor Predicting: 356it [03:19,  1.93it/s]Extractor Predicting: 357it [03:20,  1.64it/s]Extractor Predicting: 358it [03:20,  1.71it/s]Extractor Predicting: 359it [03:21,  1.76it/s]Extractor Predicting: 360it [03:22,  1.79it/s]Extractor Predicting: 361it [03:22,  1.80it/s]Extractor Predicting: 362it [03:23,  1.84it/s]Extractor Predicting: 363it [03:23,  1.89it/s]Extractor Predicting: 364it [03:24,  1.95it/s]Extractor Predicting: 365it [03:24,  1.88it/s]Extractor Predicting: 366it [03:25,  1.84it/s]Extractor Predicting: 367it [03:25,  1.76it/s]Extractor Predicting: 368it [03:26,  1.74it/s]Extractor Predicting: 369it [03:26,  1.80it/s]Extractor Predicting: 370it [03:27,  1.76it/s]Extractor Predicting: 371it [03:28,  1.78it/s]Extractor Predicting: 372it [03:28,  1.81it/s]Extractor Predicting: 373it [03:29,  1.79it/s]Extractor Predicting: 374it [03:29,  1.80it/s]Extractor Predicting: 375it [03:30,  1.80it/s]Extractor Predicting: 376it [03:30,  1.80it/s]Extractor Predicting: 377it [03:31,  1.78it/s]Extractor Predicting: 378it [03:31,  1.77it/s]Extractor Predicting: 379it [03:32,  1.74it/s]Extractor Predicting: 380it [03:33,  1.75it/s]Extractor Predicting: 381it [03:33,  1.80it/s]Extractor Predicting: 382it [03:34,  1.83it/s]Extractor Predicting: 383it [03:34,  1.84it/s]Extractor Predicting: 384it [03:35,  1.86it/s]Extractor Predicting: 385it [03:35,  1.80it/s]Extractor Predicting: 386it [03:36,  1.84it/s]Extractor Predicting: 387it [03:36,  1.85it/s]Extractor Predicting: 388it [03:37,  1.87it/s]Extractor Predicting: 389it [03:38,  1.81it/s]Extractor Predicting: 390it [03:38,  1.79it/s]Extractor Predicting: 391it [03:39,  1.78it/s]Extractor Predicting: 392it [03:39,  1.86it/s]Extractor Predicting: 393it [03:40,  1.90it/s]Extractor Predicting: 394it [03:40,  1.91it/s]Extractor Predicting: 395it [03:41,  1.93it/s]Extractor Predicting: 396it [03:41,  1.95it/s]Extractor Predicting: 397it [03:42,  1.90it/s]Extractor Predicting: 398it [03:42,  1.88it/s]Extractor Predicting: 399it [03:43,  1.84it/s]Extractor Predicting: 400it [03:43,  1.82it/s]Extractor Predicting: 401it [03:44,  1.87it/s]Extractor Predicting: 402it [03:44,  1.90it/s]Extractor Predicting: 403it [03:45,  1.90it/s]Extractor Predicting: 404it [03:45,  1.91it/s]Extractor Predicting: 405it [03:46,  1.82it/s]Extractor Predicting: 406it [03:47,  1.84it/s]Extractor Predicting: 407it [03:47,  1.86it/s]Extractor Predicting: 408it [03:48,  1.87it/s]Extractor Predicting: 409it [03:48,  1.69it/s]Extractor Predicting: 410it [03:49,  1.73it/s]Extractor Predicting: 411it [03:50,  1.71it/s]Extractor Predicting: 412it [03:50,  1.75it/s]Extractor Predicting: 413it [03:51,  1.69it/s]Extractor Predicting: 414it [03:51,  1.67it/s]Extractor Predicting: 415it [03:52,  1.63it/s]Extractor Predicting: 416it [03:53,  1.59it/s]Extractor Predicting: 417it [03:53,  1.63it/s]Extractor Predicting: 418it [03:54,  1.68it/s]Extractor Predicting: 419it [03:54,  1.70it/s]Extractor Predicting: 420it [03:55,  1.67it/s]Extractor Predicting: 421it [03:56,  1.68it/s]Extractor Predicting: 422it [03:56,  1.67it/s]Extractor Predicting: 423it [03:57,  1.72it/s]Extractor Predicting: 424it [03:57,  1.71it/s]Extractor Predicting: 425it [03:58,  1.70it/s]Extractor Predicting: 426it [03:58,  1.69it/s]Extractor Predicting: 427it [03:59,  1.71it/s]Extractor Predicting: 428it [04:00,  1.72it/s]Extractor Predicting: 429it [04:00,  1.75it/s]Extractor Predicting: 430it [04:01,  1.76it/s]Extractor Predicting: 431it [04:01,  1.77it/s]Extractor Predicting: 432it [04:02,  1.76it/s]Extractor Predicting: 433it [04:02,  1.76it/s]Extractor Predicting: 434it [04:03,  1.74it/s]Extractor Predicting: 435it [04:04,  1.77it/s]Extractor Predicting: 436it [04:04,  1.82it/s]Extractor Predicting: 437it [04:05,  1.79it/s]Extractor Predicting: 438it [04:05,  1.81it/s]Extractor Predicting: 439it [04:06,  1.80it/s]Extractor Predicting: 440it [04:06,  1.75it/s]Extractor Predicting: 441it [04:07,  1.74it/s]Extractor Predicting: 442it [04:07,  1.76it/s]Extractor Predicting: 443it [04:08,  1.73it/s]Extractor Predicting: 444it [04:09,  1.74it/s]Extractor Predicting: 445it [04:09,  1.72it/s]Extractor Predicting: 446it [04:10,  1.71it/s]Extractor Predicting: 447it [04:10,  1.73it/s]Extractor Predicting: 448it [04:11,  1.73it/s]Extractor Predicting: 449it [04:12,  1.77it/s]Extractor Predicting: 450it [04:12,  1.81it/s]Extractor Predicting: 451it [04:13,  1.78it/s]Extractor Predicting: 452it [04:13,  1.76it/s]Extractor Predicting: 453it [04:14,  1.73it/s]Extractor Predicting: 454it [04:14,  1.74it/s]Extractor Predicting: 455it [04:15,  1.71it/s]Extractor Predicting: 456it [04:16,  1.74it/s]Extractor Predicting: 457it [04:16,  1.71it/s]Extractor Predicting: 458it [04:17,  1.73it/s]Extractor Predicting: 459it [04:17,  1.70it/s]Extractor Predicting: 460it [04:18,  1.69it/s]Extractor Predicting: 461it [04:18,  1.73it/s]Extractor Predicting: 462it [04:19,  1.74it/s]Extractor Predicting: 463it [04:20,  1.70it/s]Extractor Predicting: 464it [04:20,  1.67it/s]Extractor Predicting: 465it [04:21,  1.65it/s]Extractor Predicting: 466it [04:21,  1.68it/s]Extractor Predicting: 467it [04:22,  1.75it/s]Extractor Predicting: 468it [04:23,  1.76it/s]Extractor Predicting: 469it [04:23,  1.82it/s]Extractor Predicting: 470it [04:24,  1.80it/s]Extractor Predicting: 471it [04:24,  1.75it/s]Extractor Predicting: 472it [04:25,  1.76it/s]Extractor Predicting: 473it [04:25,  1.74it/s]Extractor Predicting: 474it [04:26,  1.50it/s]Extractor Predicting: 475it [04:27,  1.56it/s]Extractor Predicting: 476it [04:27,  1.59it/s]Extractor Predicting: 477it [04:28,  1.64it/s]Extractor Predicting: 478it [04:29,  1.67it/s]Extractor Predicting: 479it [04:29,  1.63it/s]Extractor Predicting: 480it [04:30,  1.60it/s]Extractor Predicting: 481it [04:31,  1.58it/s]Extractor Predicting: 482it [04:31,  1.60it/s]Extractor Predicting: 483it [04:32,  1.58it/s]Extractor Predicting: 484it [04:32,  1.57it/s]Extractor Predicting: 485it [04:33,  1.58it/s]Extractor Predicting: 486it [04:34,  1.59it/s]Extractor Predicting: 487it [04:34,  1.64it/s]Extractor Predicting: 488it [04:35,  1.62it/s]Extractor Predicting: 489it [04:35,  1.65it/s]Extractor Predicting: 490it [04:36,  1.62it/s]Extractor Predicting: 491it [04:37,  1.58it/s]Extractor Predicting: 492it [04:37,  1.58it/s]Extractor Predicting: 493it [04:38,  1.60it/s]Extractor Predicting: 494it [04:39,  1.60it/s]Extractor Predicting: 495it [04:39,  1.60it/s]Extractor Predicting: 496it [04:40,  1.59it/s]Extractor Predicting: 497it [04:40,  1.61it/s]Extractor Predicting: 498it [04:41,  1.44it/s]Extractor Predicting: 499it [04:42,  1.47it/s]Extractor Predicting: 500it [04:43,  1.52it/s]Extractor Predicting: 501it [04:43,  1.57it/s]Extractor Predicting: 502it [04:44,  1.56it/s]Extractor Predicting: 503it [04:45,  1.52it/s]Extractor Predicting: 504it [04:45,  1.52it/s]Extractor Predicting: 505it [04:46,  1.53it/s]Extractor Predicting: 506it [04:46,  1.53it/s]Extractor Predicting: 507it [04:47,  1.58it/s]Extractor Predicting: 508it [04:48,  1.75it/s]Extractor Predicting: 508it [04:48,  1.76it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:41,716 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:41,738 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:41,738 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:41,738 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:41,738 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:29:42,514 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:29:42,515 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:29:43,098 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:29:44,188 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:29:44,188 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:47,194 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:47,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:47,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:47,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:29:47,218 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:29:48,029 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:29:48,031 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:29:48,636 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:29:48,839 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:29:48,839 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.73it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 7it [00:04,  1.70it/s]Extractor Predicting: 8it [00:04,  1.72it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.71it/s]Extractor Predicting: 14it [00:08,  1.70it/s]Extractor Predicting: 15it [00:08,  1.75it/s]Extractor Predicting: 16it [00:09,  1.71it/s]Extractor Predicting: 17it [00:09,  1.72it/s]Extractor Predicting: 18it [00:10,  1.71it/s]Extractor Predicting: 19it [00:11,  1.70it/s]Extractor Predicting: 20it [00:11,  1.70it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:12,  1.69it/s]Extractor Predicting: 23it [00:13,  1.68it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:14,  1.73it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:15,  1.70it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:17,  1.70it/s]Extractor Predicting: 31it [00:18,  1.75it/s]Extractor Predicting: 32it [00:18,  1.80it/s]Extractor Predicting: 33it [00:19,  1.82it/s]Extractor Predicting: 34it [00:19,  1.84it/s]Extractor Predicting: 35it [00:20,  1.82it/s]Extractor Predicting: 36it [00:20,  1.81it/s]Extractor Predicting: 37it [00:21,  1.77it/s]Extractor Predicting: 38it [00:21,  1.80it/s]Extractor Predicting: 39it [00:22,  1.80it/s]Extractor Predicting: 40it [00:23,  1.82it/s]Extractor Predicting: 41it [00:23,  1.81it/s]Extractor Predicting: 42it [00:24,  1.83it/s]Extractor Predicting: 43it [00:24,  1.70it/s]Extractor Predicting: 44it [00:25,  1.79it/s]Extractor Predicting: 45it [00:25,  1.77it/s]Extractor Predicting: 46it [00:26,  1.80it/s]Extractor Predicting: 47it [00:27,  1.81it/s]Extractor Predicting: 48it [00:27,  1.77it/s]Extractor Predicting: 49it [00:28,  1.78it/s]Extractor Predicting: 50it [00:28,  1.77it/s]Extractor Predicting: 51it [00:29,  1.76it/s]Extractor Predicting: 52it [00:29,  1.76it/s]Extractor Predicting: 53it [00:30,  1.77it/s]Extractor Predicting: 54it [00:31,  1.69it/s]Extractor Predicting: 55it [00:31,  1.73it/s]Extractor Predicting: 56it [00:32,  1.76it/s]Extractor Predicting: 57it [00:32,  1.77it/s]Extractor Predicting: 58it [00:33,  1.78it/s]Extractor Predicting: 59it [00:33,  1.79it/s]Extractor Predicting: 60it [00:34,  1.73it/s]Extractor Predicting: 61it [00:35,  1.75it/s]Extractor Predicting: 62it [00:35,  1.76it/s]Extractor Predicting: 63it [00:36,  1.77it/s]Extractor Predicting: 64it [00:36,  1.78it/s]Extractor Predicting: 65it [00:37,  1.74it/s]Extractor Predicting: 66it [00:37,  1.72it/s]Extractor Predicting: 67it [00:38,  1.69it/s]Extractor Predicting: 68it [00:39,  1.69it/s]Extractor Predicting: 69it [00:39,  1.64it/s]Extractor Predicting: 70it [00:40,  1.65it/s]Extractor Predicting: 71it [00:40,  1.63it/s]Extractor Predicting: 72it [00:41,  1.65it/s]Extractor Predicting: 73it [00:42,  1.68it/s]Extractor Predicting: 74it [00:42,  1.63it/s]Extractor Predicting: 75it [00:43,  1.88it/s]Extractor Predicting: 75it [00:43,  1.74it/s]
[INFO|configuration_utils.py:515] 2023-08-28 03:30:35,485 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:30:35,541 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 03:30:35,646 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:30:35,647 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 03:30:35,697 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 03:30:45,944 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 03:30:45,959 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 03:30:46,087 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 03:30:46,088 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 03:30:46,175 >> Didn't find file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:30:46,246 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:30:46,246 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:30:46,246 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:30:46,246 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:30:46,246 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 03:30:46,246 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 03:30:46,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:47,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:47,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:48,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:48,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:49,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:50,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:50,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:51,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:51,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:52,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:52,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:53,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:54,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:54,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:55,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:55,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:56,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:56,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:57,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:57,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:58,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<03:55, 12.39s/it][WARNING|generation_utils.py:914] 2023-08-28 03:30:58,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:30:59,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:00,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:00,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:01,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:01,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:02,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:03,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:03,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:04,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:05,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:05,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:06,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:06,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:07,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:07,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:08,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:08,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:09,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:10,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:10,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:11,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:11,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:12,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:12,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:26<04:04, 13.61s/it][WARNING|generation_utils.py:914] 2023-08-28 03:31:13,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:13,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:14,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:15,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:16,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:17,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:17,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:18,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:19,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:19,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:20,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:21,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:22,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:22,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:23,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:23,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:24,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:25,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:26,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:26,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:27,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:28,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:28,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:29,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:30,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:30,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:31,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:45<04:30, 15.90s/it][WARNING|generation_utils.py:914] 2023-08-28 03:31:32,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:32,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:33,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:33,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:34,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:35,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:35,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:36,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:37,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:37,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:38,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:38,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:39,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:40,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:40,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:41,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:41,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:42,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:43,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:43,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:44,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:44,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:45,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:59<04:03, 15.22s/it][WARNING|generation_utils.py:914] 2023-08-28 03:31:46,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:46,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:47,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:47,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:48,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:49,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:49,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:50,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:50,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:51,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:52,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:52,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:53,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:53,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:54,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:55,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:55,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:56,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:56,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:57,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:57,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:58,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:59,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:31:59,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:00,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:00,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:14<03:48, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 03:32:01,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:02,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:02,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:03,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:03,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:04,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:05,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:05,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:06,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:06,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:07,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:07,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:08,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:09,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:09,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:10,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:10,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:11,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:12,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:12,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:13,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:13,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:14,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:15,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:15,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:16,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:16,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:30<03:36, 15.49s/it][WARNING|generation_utils.py:914] 2023-08-28 03:32:17,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:18,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:18,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:19,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:19,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:20,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:21,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:21,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:22,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:23,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:23,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:24,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:24,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:25,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:25,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:26,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:27,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:27,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:28,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:28,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:29,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:29,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:30,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:44<03:13, 14.89s/it][WARNING|generation_utils.py:914] 2023-08-28 03:32:31,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:31,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:32,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:32,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:33,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:34,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:34,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:35,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:35,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:36,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:37,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:37,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:38,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:38,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:39,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:39,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:40,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:41,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:41,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:42,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:42,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:43,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:44,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:44,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:45,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:59<02:57, 14.81s/it][WARNING|generation_utils.py:914] 2023-08-28 03:32:45,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:46,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:46,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:47,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:48,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:48,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:49,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:50,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:50,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:51,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:51,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:52,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:52,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:53,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:54,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:54,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:55,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:55,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:56,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:56,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:57,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:58,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:58,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:32:59,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:00,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:14<02:43, 14.84s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:00,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:01,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:01,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:02,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:03,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:03,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:04,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:04,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:05,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:06,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:06,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:07,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:07,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:08,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:08,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:09,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:10,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:10,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:11,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:11,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:12,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:12,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:13,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:14,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:28<02:26, 14.70s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:15,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:15,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:16,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:17,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:17,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:18,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:18,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:19,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:20,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:20,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:21,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:22,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:22,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:23,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:23,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:24,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:25,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:25,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:26,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:27,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:27,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:28,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:28,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:42<02:11, 14.61s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:29,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:30,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:30,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:31,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:31,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:32,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:32,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:33,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:34,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:35,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:35,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:36,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:37,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:37,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:38,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:38,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:39,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:40,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:40,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:41,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:41,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:42,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:43,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:57<01:56, 14.56s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:43,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:44,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:45,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:45,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:46,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:46,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:47,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:47,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:48,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:49,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:49,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:50,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:50,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:51,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:52,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:52,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:53,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:54,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:54,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:55,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:55,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:56,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:56,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:10<01:39, 14.23s/it][WARNING|generation_utils.py:914] 2023-08-28 03:33:57,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:57,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:58,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:59,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:33:59,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:00,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:00,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:01,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:01,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:02,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:02,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:03,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:04,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:04,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:05,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:05,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:06,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:06,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:07,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:08,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:09,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:09,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:10,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:24<01:23, 13.96s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:10,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:11,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:11,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:12,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:13,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:13,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:14,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:14,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:15,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:15,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:16,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:17,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:17,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:18,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:18,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:19,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:19,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:20,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:21,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:21,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:22,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:36<01:07, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:23,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:23,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:24,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:24,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:25,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:26,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:26,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:27,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:28,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:29,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:29,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:30,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:31,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:31,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:32,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:32,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:33,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:34,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:34,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:35,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:36,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:36,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:37,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:38,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:38,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:39,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:40,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:40,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:41,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:55<01:00, 15.09s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:41,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:42,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:43,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:43,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:44,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:44,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:45,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:45,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:46,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:47,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:47,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:48,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:48,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:49,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:49,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:50,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:51,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:51,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:52,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:52,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:53,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:07<00:42, 14.19s/it][WARNING|generation_utils.py:914] 2023-08-28 03:34:54,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:54,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:55,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:55,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:56,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:56,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:57,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:57,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:58,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:59,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:34:59,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:00,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:00,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:01,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:02,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:02,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:03,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:03,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:04,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:04,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:05,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:06,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:06,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:20<00:27, 13.90s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:07,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:07,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:08,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:08,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:09,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:10,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:10,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:11,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:11,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:12,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:13,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:13,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:14,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:14,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:15,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:16,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:16,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:17,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:17,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:18,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:19,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:19,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:20,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:34<00:13, 13.91s/it][WARNING|generation_utils.py:914] 2023-08-28 03:35:21,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:21,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:22,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:23,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:23,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:24,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:24,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:25,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:26,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:26,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:27,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:27,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:28,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:29,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:29,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:30,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:30,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:31,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:32,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:32,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:33,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:34,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:34,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:35,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:35:35,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:49<00:00, 14.33s/it]Generating: 100%|██████████| 20/20 [04:49<00:00, 14.49s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:47,941 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:47,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:47,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:47,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:47,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:35:49,008 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:35:49,009 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:35:49,852 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:35:50,952 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:35:50,976 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:54,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:54,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:54,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:54,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:35:54,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:35:55,557 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:35:55,558 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:35:56,419 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:35:56,635 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:35:56,635 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 358, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7118055555555556, 'errors': {'', "('συῆνος', 'said to be the same as', '', 'They are known as a form of Greek σάρτικρος ( συῆνος ) , τεῖς ( συθροτος ) , τίνρας ( συῆλώηνος ) ; and of the forms σατος τικρος in Latin .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : student .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 331, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 503, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7391826923076923, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 303, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 466, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 531, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 576, 'raw': 800}
{'target': 600, 'success': 598, 'raw': 832}
{'target': 600, 'success': 620, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7175925925925926, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8233695652173914, 'errors': {'', "('', 'continent', 'Aegean Sea', 'The island of Aranet , located in the Aegean Sea , is one of two islands that the Greek island of Argos has in common with the Arabian sea .')"}}
['Relation : field of this occupation . Context : The main characters in the series are The New Guy , The New York City Police Detective and the Los Angeles Police Department . Head Entity : Los Angeles Police Department , Tail Entity : police .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Walter L. Cusack', 'field of this occupation', '', 'Walter L. Cusack ( June 23 , 1924 February 21 , 2002 ) was an English cricket er who played six Tests in England .')"}}
['Relation : field of work . Context : Following his graduation in 1957 , he founded the PSA ( now in association with B. C. ) , an academic association for linguistics , linguistics , and other fields , and served as a member of the Princeton University Research Committee . Head Entity : linguistics , Tail Entity : graduate .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.76875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n']
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n', 'Relation : given name . Context : James M. Young ( February 14 , 1821 October 19 , 1924 ) was an American poet and writer specializing in the arts . Head Entity : James M. Young , Tail Entity : named name .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8206521739130435, 'errors': {'', "('Doctor Who', 'given name', '', 'He is best known for his play as a character on Doctor Who , in which he played the name of the villainess in the Doctor Who special , The One and Only Thing .')", 'too many values to unpack (expected 2)', "('Joseph L. M. Rennelly', 'given name', '', 'Joseph L. M. Rennelly is a former New York City mayor who served as the 62nd governor of New York City .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Never Say Goodbye', 'lyrics by', '', 'After graduating from Berklee College of Music , he started to write a single called Never Say Goodbye that debuted at No. 1 on the Billboard 200 single Hot Hot Song in July 1991 .')", "('Never Let Me Go', 'lyrics by', '', 'She followed this up with her sixth studio album , Never Let Me Go , which is now a cult classic on the Billboard R&B Top 40 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Central railway', 'owned by', '', 'It was built between 1941 and 1943 under the direction of the Central railway company ( D.')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9077380952380952, 'errors': {'', "('GMAW', 'performer', '', 'He also performed with the band Pampers on the opening night of WMAW in Los Angeles and the opening night of the ABC drama GMAW in Atlanta .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 128, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 185, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 223, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 288, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 371, 'raw': 576}
{'target': 600, 'success': 393, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 433, 'raw': 672}
{'target': 600, 'success': 451, 'raw': 704}
{'target': 600, 'success': 471, 'raw': 736}
{'target': 600, 'success': 495, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 583, 'raw': 896}
{'target': 600, 'success': 601, 'raw': 928}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.6476293103448276, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : replaces .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 17944
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18044, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.63it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:01,  1.52it/s]Extractor Estimating: 4it [00:02,  1.62it/s]Extractor Estimating: 5it [00:03,  1.64it/s]Extractor Estimating: 6it [00:03,  1.65it/s]Extractor Estimating: 7it [00:04,  1.71it/s]Extractor Estimating: 8it [00:04,  1.70it/s]Extractor Estimating: 9it [00:05,  1.68it/s]Extractor Estimating: 10it [00:06,  1.69it/s]Extractor Estimating: 11it [00:06,  1.72it/s]Extractor Estimating: 12it [00:07,  1.70it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.74it/s]Extractor Estimating: 15it [00:08,  1.70it/s]Extractor Estimating: 16it [00:09,  1.62it/s]Extractor Estimating: 17it [00:10,  1.68it/s]Extractor Estimating: 18it [00:10,  1.65it/s]Extractor Estimating: 19it [00:11,  1.63it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:12,  1.64it/s]Extractor Estimating: 22it [00:13,  1.69it/s]Extractor Estimating: 23it [00:13,  1.66it/s]Extractor Estimating: 24it [00:14,  1.70it/s]Extractor Estimating: 25it [00:15,  1.65it/s]Extractor Estimating: 26it [00:15,  1.58it/s]Extractor Estimating: 27it [00:16,  1.58it/s]Extractor Estimating: 28it [00:16,  1.63it/s]Extractor Estimating: 29it [00:17,  1.70it/s]Extractor Estimating: 30it [00:18,  1.72it/s]Extractor Estimating: 31it [00:18,  1.72it/s]Extractor Estimating: 32it [00:19,  1.77it/s]Extractor Estimating: 33it [00:19,  1.70it/s]Extractor Estimating: 34it [00:20,  1.67it/s]Extractor Estimating: 35it [00:20,  1.68it/s]Extractor Estimating: 36it [00:21,  1.70it/s]Extractor Estimating: 37it [00:22,  1.73it/s]Extractor Estimating: 38it [00:22,  1.71it/s]Extractor Estimating: 39it [00:23,  1.64it/s]Extractor Estimating: 40it [00:23,  1.68it/s]Extractor Estimating: 41it [00:24,  1.74it/s]Extractor Estimating: 42it [00:25,  1.74it/s]Extractor Estimating: 43it [00:25,  1.73it/s]Extractor Estimating: 44it [00:26,  1.74it/s]Extractor Estimating: 45it [00:26,  1.74it/s]Extractor Estimating: 46it [00:27,  1.72it/s]Extractor Estimating: 47it [00:27,  1.67it/s]Extractor Estimating: 48it [00:28,  1.73it/s]Extractor Estimating: 49it [00:29,  1.73it/s]Extractor Estimating: 50it [00:29,  1.73it/s]Extractor Estimating: 51it [00:30,  1.71it/s]Extractor Estimating: 52it [00:30,  1.70it/s]Extractor Estimating: 53it [00:31,  1.67it/s]Extractor Estimating: 54it [00:32,  1.70it/s]Extractor Estimating: 55it [00:32,  1.71it/s]Extractor Estimating: 56it [00:33,  1.61it/s]Extractor Estimating: 57it [00:33,  1.60it/s]Extractor Estimating: 58it [00:34,  1.60it/s]Extractor Estimating: 59it [00:35,  1.59it/s]Extractor Estimating: 60it [00:35,  1.59it/s]Extractor Estimating: 61it [00:36,  1.58it/s]Extractor Estimating: 62it [00:37,  1.61it/s]Extractor Estimating: 63it [00:37,  1.65it/s]Extractor Estimating: 64it [00:38,  1.70it/s]Extractor Estimating: 65it [00:38,  1.67it/s]Extractor Estimating: 66it [00:39,  1.64it/s]Extractor Estimating: 67it [00:40,  1.58it/s]Extractor Estimating: 68it [00:40,  1.59it/s]Extractor Estimating: 69it [00:41,  1.59it/s]Extractor Estimating: 70it [00:42,  1.58it/s]Extractor Estimating: 71it [00:42,  1.53it/s]Extractor Estimating: 72it [00:43,  1.55it/s]Extractor Estimating: 73it [00:44,  1.56it/s]Extractor Estimating: 74it [00:44,  1.59it/s]Extractor Estimating: 75it [00:45,  1.59it/s]Extractor Estimating: 76it [00:45,  1.62it/s]Extractor Estimating: 77it [00:46,  1.61it/s]Extractor Estimating: 78it [00:47,  1.60it/s]Extractor Estimating: 79it [00:47,  1.64it/s]Extractor Estimating: 80it [00:48,  1.59it/s]Extractor Estimating: 81it [00:48,  1.61it/s]Extractor Estimating: 82it [00:49,  1.63it/s]Extractor Estimating: 83it [00:50,  1.66it/s]Extractor Estimating: 84it [00:50,  1.62it/s]Extractor Estimating: 85it [00:51,  1.61it/s]Extractor Estimating: 86it [00:52,  1.61it/s]Extractor Estimating: 87it [00:52,  1.47it/s]Extractor Estimating: 88it [00:53,  1.50it/s]Extractor Estimating: 89it [00:54,  1.55it/s]Extractor Estimating: 90it [00:54,  1.54it/s]Extractor Estimating: 91it [00:55,  1.60it/s]Extractor Estimating: 92it [00:55,  1.55it/s]Extractor Estimating: 93it [00:56,  1.60it/s]Extractor Estimating: 94it [00:57,  1.59it/s]Extractor Estimating: 95it [00:57,  1.60it/s]Extractor Estimating: 96it [00:58,  1.61it/s]Extractor Estimating: 97it [00:59,  1.59it/s]Extractor Estimating: 98it [00:59,  1.59it/s]Extractor Estimating: 99it [01:00,  1.61it/s]Extractor Estimating: 100it [01:01,  1.55it/s]Extractor Estimating: 101it [01:01,  1.58it/s]Extractor Estimating: 102it [01:02,  1.57it/s]Extractor Estimating: 103it [01:02,  1.58it/s]Extractor Estimating: 104it [01:03,  1.56it/s]Extractor Estimating: 105it [01:04,  1.55it/s]Extractor Estimating: 106it [01:04,  1.58it/s]Extractor Estimating: 107it [01:05,  1.59it/s]Extractor Estimating: 108it [01:06,  1.63it/s]Extractor Estimating: 109it [01:06,  1.61it/s]Extractor Estimating: 110it [01:07,  1.60it/s]Extractor Estimating: 111it [01:07,  1.63it/s]Extractor Estimating: 112it [01:08,  1.62it/s]Extractor Estimating: 113it [01:09,  1.65it/s]Extractor Estimating: 114it [01:09,  1.62it/s]Extractor Estimating: 115it [01:10,  1.61it/s]Extractor Estimating: 116it [01:10,  1.64it/s]Extractor Estimating: 117it [01:11,  1.60it/s]Extractor Estimating: 118it [01:12,  1.63it/s]Extractor Estimating: 119it [01:12,  1.62it/s]Extractor Estimating: 120it [01:13,  1.62it/s]Extractor Estimating: 121it [01:14,  1.65it/s]Extractor Estimating: 122it [01:14,  1.62it/s]Extractor Estimating: 123it [01:15,  1.63it/s]Extractor Estimating: 124it [01:15,  1.65it/s]Extractor Estimating: 125it [01:16,  1.59it/s]Extractor Estimating: 126it [01:17,  1.58it/s]Extractor Estimating: 127it [01:17,  1.58it/s]Extractor Estimating: 128it [01:18,  1.59it/s]Extractor Estimating: 129it [01:19,  1.61it/s]Extractor Estimating: 130it [01:19,  1.58it/s]Extractor Estimating: 131it [01:20,  1.58it/s]Extractor Estimating: 132it [01:20,  1.62it/s]Extractor Estimating: 133it [01:21,  1.67it/s]Extractor Estimating: 134it [01:22,  1.67it/s]Extractor Estimating: 135it [01:22,  1.68it/s]Extractor Estimating: 136it [01:23,  1.64it/s]Extractor Estimating: 137it [01:23,  1.66it/s]Extractor Estimating: 138it [01:24,  1.60it/s]Extractor Estimating: 139it [01:25,  1.63it/s]Extractor Estimating: 140it [01:25,  1.61it/s]Extractor Estimating: 141it [01:26,  1.61it/s]Extractor Estimating: 142it [01:26,  1.66it/s]Extractor Estimating: 143it [01:27,  1.62it/s]Extractor Estimating: 144it [01:28,  1.61it/s]Extractor Estimating: 145it [01:28,  1.62it/s]Extractor Estimating: 146it [01:29,  1.61it/s]Extractor Estimating: 147it [01:30,  1.56it/s]Extractor Estimating: 148it [01:30,  1.57it/s]Extractor Estimating: 149it [01:31,  1.56it/s]Extractor Estimating: 150it [01:32,  1.59it/s]Extractor Estimating: 151it [01:32,  1.63it/s]Extractor Estimating: 152it [01:33,  1.59it/s]Extractor Estimating: 153it [01:33,  1.68it/s]Extractor Estimating: 154it [01:34,  1.68it/s]Extractor Estimating: 155it [01:35,  1.51it/s]Extractor Estimating: 156it [01:35,  1.56it/s]Extractor Estimating: 157it [01:36,  1.63it/s]Extractor Estimating: 158it [01:37,  1.56it/s]Extractor Estimating: 159it [01:37,  1.59it/s]Extractor Estimating: 160it [01:38,  1.56it/s]Extractor Estimating: 161it [01:38,  1.61it/s]Extractor Estimating: 162it [01:39,  1.69it/s]Extractor Estimating: 163it [01:39,  1.73it/s]Extractor Estimating: 164it [01:40,  1.76it/s]Extractor Estimating: 165it [01:41,  1.76it/s]Extractor Estimating: 166it [01:41,  1.72it/s]Extractor Estimating: 167it [01:42,  1.74it/s]Extractor Estimating: 168it [01:42,  1.77it/s]Extractor Estimating: 169it [01:43,  1.82it/s]Extractor Estimating: 170it [01:43,  1.76it/s]Extractor Estimating: 171it [01:44,  1.80it/s]Extractor Estimating: 172it [01:45,  1.78it/s]Extractor Estimating: 173it [01:45,  1.79it/s]Extractor Estimating: 174it [01:46,  1.80it/s]Extractor Estimating: 175it [01:46,  1.75it/s]Extractor Estimating: 176it [01:47,  1.71it/s]Extractor Estimating: 177it [01:47,  1.68it/s]Extractor Estimating: 178it [01:48,  1.66it/s]Extractor Estimating: 179it [01:49,  1.46it/s]Extractor Estimating: 180it [01:50,  1.51it/s]Extractor Estimating: 181it [01:50,  1.49it/s]Extractor Estimating: 182it [01:51,  1.57it/s]Extractor Estimating: 183it [01:51,  1.57it/s]Extractor Estimating: 184it [01:52,  1.57it/s]Extractor Estimating: 185it [01:53,  1.57it/s]Extractor Estimating: 186it [01:53,  1.58it/s]Extractor Estimating: 187it [01:54,  1.59it/s]Extractor Estimating: 188it [01:55,  1.63it/s]Extractor Estimating: 189it [01:55,  1.67it/s]Extractor Estimating: 190it [01:56,  1.67it/s]Extractor Estimating: 191it [01:56,  1.68it/s]Extractor Estimating: 192it [01:57,  1.64it/s]Extractor Estimating: 193it [01:58,  1.67it/s]Extractor Estimating: 194it [01:58,  1.64it/s]Extractor Estimating: 195it [01:59,  1.64it/s]Extractor Estimating: 196it [01:59,  1.63it/s]Extractor Estimating: 197it [02:00,  1.64it/s]Extractor Estimating: 198it [02:01,  1.62it/s]Extractor Estimating: 199it [02:01,  1.58it/s]Extractor Estimating: 200it [02:02,  1.65it/s]Extractor Estimating: 201it [02:02,  1.62it/s]Extractor Estimating: 202it [02:03,  1.62it/s]Extractor Estimating: 203it [02:04,  1.60it/s]Extractor Estimating: 204it [02:04,  1.60it/s]Extractor Estimating: 205it [02:05,  1.57it/s]Extractor Estimating: 206it [02:06,  1.54it/s]Extractor Estimating: 207it [02:06,  1.55it/s]Extractor Estimating: 208it [02:07,  1.57it/s]Extractor Estimating: 209it [02:08,  1.54it/s]Extractor Estimating: 210it [02:08,  1.56it/s]Extractor Estimating: 211it [02:09,  1.57it/s]Extractor Estimating: 212it [02:10,  1.56it/s]Extractor Estimating: 213it [02:10,  1.58it/s]Extractor Estimating: 214it [02:11,  1.60it/s]Extractor Estimating: 215it [02:12,  1.52it/s]Extractor Estimating: 216it [02:12,  1.54it/s]Extractor Estimating: 217it [02:13,  1.55it/s]Extractor Estimating: 218it [02:13,  1.62it/s]Extractor Estimating: 219it [02:14,  1.64it/s]Extractor Estimating: 220it [02:15,  1.59it/s]Extractor Estimating: 221it [02:15,  1.64it/s]Extractor Estimating: 222it [02:16,  1.61it/s]Extractor Estimating: 223it [02:16,  1.62it/s]Extractor Estimating: 224it [02:17,  1.59it/s]Extractor Estimating: 225it [02:18,  1.59it/s]Extractor Estimating: 226it [02:18,  1.60it/s]Extractor Estimating: 227it [02:19,  1.60it/s]Extractor Estimating: 228it [02:20,  1.62it/s]Extractor Estimating: 229it [02:20,  1.64it/s]Extractor Estimating: 230it [02:21,  1.60it/s]Extractor Estimating: 231it [02:21,  1.64it/s]Extractor Estimating: 232it [02:22,  1.58it/s]Extractor Estimating: 233it [02:23,  1.60it/s]Extractor Estimating: 234it [02:23,  1.60it/s]Extractor Estimating: 235it [02:24,  1.60it/s]Extractor Estimating: 236it [02:24,  1.64it/s]Extractor Estimating: 237it [02:25,  1.62it/s]Extractor Estimating: 238it [02:26,  1.61it/s]Extractor Estimating: 239it [02:26,  1.63it/s]Extractor Estimating: 240it [02:27,  1.63it/s]Extractor Estimating: 241it [02:28,  1.64it/s]Extractor Estimating: 242it [02:28,  1.63it/s]Extractor Estimating: 243it [02:29,  1.62it/s]Extractor Estimating: 244it [02:29,  1.62it/s]Extractor Estimating: 245it [02:30,  1.60it/s]Extractor Estimating: 246it [02:31,  1.62it/s]Extractor Estimating: 247it [02:31,  1.53it/s]Extractor Estimating: 248it [02:32,  1.59it/s]Extractor Estimating: 249it [02:33,  1.60it/s]Extractor Estimating: 250it [02:33,  1.60it/s]Extractor Estimating: 251it [02:34,  1.64it/s]Extractor Estimating: 252it [02:34,  1.60it/s]Extractor Estimating: 253it [02:35,  1.54it/s]Extractor Estimating: 254it [02:36,  1.52it/s]Extractor Estimating: 255it [02:36,  1.56it/s]Extractor Estimating: 256it [02:37,  1.58it/s]Extractor Estimating: 257it [02:38,  1.45it/s]Extractor Estimating: 258it [02:39,  1.49it/s]Extractor Estimating: 259it [02:40,  1.27it/s]Extractor Estimating: 260it [02:40,  1.34it/s]Extractor Estimating: 261it [02:41,  1.40it/s]Extractor Estimating: 262it [02:41,  1.46it/s]Extractor Estimating: 263it [02:42,  1.51it/s]Extractor Estimating: 264it [02:43,  1.51it/s]Extractor Estimating: 265it [02:43,  1.53it/s]Extractor Estimating: 266it [02:44,  1.53it/s]Extractor Estimating: 267it [02:45,  1.56it/s]Extractor Estimating: 268it [02:45,  1.54it/s]Extractor Estimating: 269it [02:46,  1.50it/s]Extractor Estimating: 270it [02:47,  1.55it/s]Extractor Estimating: 271it [02:47,  1.54it/s]Extractor Estimating: 272it [02:48,  1.56it/s]Extractor Estimating: 273it [02:49,  1.53it/s]Extractor Estimating: 274it [02:49,  1.53it/s]Extractor Estimating: 275it [02:50,  1.50it/s]Extractor Estimating: 276it [02:50,  1.57it/s]Extractor Estimating: 277it [02:51,  1.57it/s]Extractor Estimating: 278it [02:52,  1.53it/s]Extractor Estimating: 279it [02:52,  1.54it/s]Extractor Estimating: 280it [02:53,  1.53it/s]Extractor Estimating: 281it [02:54,  1.52it/s]Extractor Estimating: 282it [02:54,  1.57it/s]Extractor Estimating: 283it [02:55,  1.58it/s]Extractor Estimating: 284it [02:56,  1.56it/s]Extractor Estimating: 285it [02:56,  1.56it/s]Extractor Estimating: 286it [02:57,  1.53it/s]Extractor Estimating: 287it [02:58,  1.57it/s]Extractor Estimating: 288it [02:58,  1.55it/s]Extractor Estimating: 289it [02:59,  1.57it/s]Extractor Estimating: 290it [02:59,  1.59it/s]Extractor Estimating: 291it [03:00,  1.57it/s]Extractor Estimating: 292it [03:01,  1.59it/s]Extractor Estimating: 293it [03:01,  1.55it/s]Extractor Estimating: 294it [03:02,  1.51it/s]Extractor Estimating: 295it [03:03,  1.49it/s]Extractor Estimating: 296it [03:03,  1.51it/s]Extractor Estimating: 297it [03:04,  1.54it/s]Extractor Estimating: 298it [03:05,  1.52it/s]Extractor Estimating: 299it [03:06,  1.46it/s]Extractor Estimating: 300it [03:06,  1.51it/s]Extractor Estimating: 301it [03:07,  1.51it/s]Extractor Estimating: 302it [03:07,  1.54it/s]Extractor Estimating: 303it [03:08,  1.57it/s]Extractor Estimating: 304it [03:09,  1.58it/s]Extractor Estimating: 305it [03:09,  1.58it/s]Extractor Estimating: 306it [03:10,  1.58it/s]Extractor Estimating: 307it [03:10,  1.64it/s]Extractor Estimating: 308it [03:11,  1.62it/s]Extractor Estimating: 309it [03:12,  1.64it/s]Extractor Estimating: 310it [03:12,  1.63it/s]Extractor Estimating: 311it [03:13,  1.55it/s]Extractor Estimating: 312it [03:14,  1.57it/s]Extractor Estimating: 313it [03:14,  1.60it/s]Extractor Estimating: 314it [03:15,  1.52it/s]Extractor Estimating: 315it [03:16,  1.52it/s]Extractor Estimating: 316it [03:16,  1.51it/s]Extractor Estimating: 317it [03:17,  1.55it/s]Extractor Estimating: 318it [03:17,  1.61it/s]Extractor Estimating: 319it [03:18,  1.55it/s]Extractor Estimating: 320it [03:19,  1.60it/s]Extractor Estimating: 321it [03:19,  1.61it/s]Extractor Estimating: 322it [03:20,  1.65it/s]Extractor Estimating: 323it [03:21,  1.64it/s]Extractor Estimating: 324it [03:21,  1.62it/s]Extractor Estimating: 325it [03:22,  1.63it/s]Extractor Estimating: 326it [03:22,  1.64it/s]Extractor Estimating: 327it [03:23,  1.65it/s]Extractor Estimating: 328it [03:24,  1.66it/s]Extractor Estimating: 329it [03:24,  1.65it/s]Extractor Estimating: 330it [03:25,  1.67it/s]Extractor Estimating: 331it [03:25,  1.64it/s]Extractor Estimating: 332it [03:26,  1.62it/s]Extractor Estimating: 333it [03:27,  1.58it/s]Extractor Estimating: 334it [03:27,  1.63it/s]Extractor Estimating: 335it [03:28,  1.65it/s]Extractor Estimating: 336it [03:29,  1.63it/s]Extractor Estimating: 337it [03:29,  1.62it/s]Extractor Estimating: 338it [03:30,  1.47it/s]Extractor Estimating: 339it [03:31,  1.56it/s]Extractor Estimating: 340it [03:31,  1.60it/s]Extractor Estimating: 341it [03:32,  1.62it/s]Extractor Estimating: 342it [03:32,  1.65it/s]Extractor Estimating: 343it [03:33,  1.59it/s]Extractor Estimating: 344it [03:34,  1.61it/s]Extractor Estimating: 345it [03:34,  1.62it/s]Extractor Estimating: 346it [03:35,  1.67it/s]Extractor Estimating: 347it [03:35,  1.61it/s]Extractor Estimating: 348it [03:36,  1.65it/s]Extractor Estimating: 349it [03:37,  1.64it/s]Extractor Estimating: 350it [03:37,  1.66it/s]Extractor Estimating: 351it [03:38,  1.66it/s]Extractor Estimating: 352it [03:38,  1.65it/s]Extractor Estimating: 353it [03:39,  1.62it/s]Extractor Estimating: 354it [03:40,  1.60it/s]Extractor Estimating: 355it [03:40,  1.60it/s]Extractor Estimating: 356it [03:41,  1.57it/s]Extractor Estimating: 357it [03:42,  1.60it/s]Extractor Estimating: 358it [03:42,  1.62it/s]Extractor Estimating: 359it [03:43,  1.58it/s]Extractor Estimating: 360it [03:43,  1.59it/s]Extractor Estimating: 361it [03:44,  1.61it/s]Extractor Estimating: 362it [03:45,  1.65it/s]Extractor Estimating: 363it [03:45,  1.60it/s]Extractor Estimating: 364it [03:46,  1.62it/s]Extractor Estimating: 365it [03:47,  1.59it/s]Extractor Estimating: 366it [03:47,  1.58it/s]Extractor Estimating: 367it [03:48,  1.60it/s]Extractor Estimating: 368it [03:48,  1.67it/s]Extractor Estimating: 369it [03:49,  1.61it/s]Extractor Estimating: 370it [03:50,  1.53it/s]Extractor Estimating: 371it [03:50,  1.55it/s]Extractor Estimating: 372it [03:51,  1.54it/s]Extractor Estimating: 373it [03:52,  1.54it/s]Extractor Estimating: 374it [03:52,  1.57it/s]Extractor Estimating: 375it [03:53,  1.60it/s]Extractor Estimating: 376it [03:54,  1.54it/s]Extractor Estimating: 377it [03:54,  1.54it/s]Extractor Estimating: 378it [03:55,  1.54it/s]Extractor Estimating: 379it [03:55,  1.57it/s]Extractor Estimating: 380it [03:56,  1.54it/s]Extractor Estimating: 381it [03:57,  1.54it/s]Extractor Estimating: 382it [03:57,  1.55it/s]Extractor Estimating: 383it [03:58,  1.52it/s]Extractor Estimating: 384it [03:59,  1.47it/s]Extractor Estimating: 385it [04:00,  1.49it/s]Extractor Estimating: 386it [04:00,  1.53it/s]Extractor Estimating: 387it [04:01,  1.53it/s]Extractor Estimating: 388it [04:01,  1.56it/s]Extractor Estimating: 389it [04:02,  1.54it/s]Extractor Estimating: 390it [04:03,  1.48it/s]Extractor Estimating: 391it [04:03,  1.54it/s]Extractor Estimating: 392it [04:04,  1.57it/s]Extractor Estimating: 393it [04:05,  1.51it/s]Extractor Estimating: 394it [04:05,  1.50it/s]Extractor Estimating: 395it [04:06,  1.51it/s]Extractor Estimating: 396it [04:07,  1.55it/s]Extractor Estimating: 397it [04:07,  1.53it/s]Extractor Estimating: 398it [04:08,  1.52it/s]Extractor Estimating: 399it [04:09,  1.51it/s]Extractor Estimating: 400it [04:09,  1.48it/s]Extractor Estimating: 401it [04:10,  1.48it/s]Extractor Estimating: 402it [04:11,  1.55it/s]Extractor Estimating: 403it [04:11,  1.54it/s]Extractor Estimating: 404it [04:12,  1.56it/s]Extractor Estimating: 405it [04:12,  1.59it/s]Extractor Estimating: 406it [04:13,  1.60it/s]Extractor Estimating: 407it [04:14,  1.59it/s]Extractor Estimating: 408it [04:14,  1.55it/s]Extractor Estimating: 409it [04:15,  1.55it/s]Extractor Estimating: 410it [04:16,  1.59it/s]Extractor Estimating: 411it [04:16,  1.60it/s]Extractor Estimating: 412it [04:17,  1.62it/s]Extractor Estimating: 413it [04:18,  1.59it/s]Extractor Estimating: 414it [04:18,  1.56it/s]Extractor Estimating: 415it [04:19,  1.55it/s]Extractor Estimating: 416it [04:19,  1.58it/s]Extractor Estimating: 417it [04:20,  1.45it/s]Extractor Estimating: 418it [04:21,  1.50it/s]Extractor Estimating: 419it [04:22,  1.55it/s]Extractor Estimating: 420it [04:22,  1.52it/s]Extractor Estimating: 421it [04:23,  1.58it/s]Extractor Estimating: 422it [04:23,  1.56it/s]Extractor Estimating: 423it [04:24,  1.59it/s]Extractor Estimating: 424it [04:25,  1.62it/s]Extractor Estimating: 425it [04:25,  1.64it/s]Extractor Estimating: 426it [04:26,  1.59it/s]Extractor Estimating: 427it [04:26,  1.62it/s]Extractor Estimating: 428it [04:27,  1.60it/s]Extractor Estimating: 429it [04:28,  1.63it/s]Extractor Estimating: 430it [04:28,  1.61it/s]Extractor Estimating: 431it [04:29,  1.65it/s]Extractor Estimating: 432it [04:30,  1.65it/s]Extractor Estimating: 433it [04:30,  1.63it/s]Extractor Estimating: 434it [04:31,  1.59it/s]Extractor Estimating: 435it [04:31,  1.59it/s]Extractor Estimating: 436it [04:32,  1.58it/s]Extractor Estimating: 437it [04:33,  1.59it/s]Extractor Estimating: 438it [04:33,  1.55it/s]Extractor Estimating: 439it [04:34,  1.51it/s]Extractor Estimating: 440it [04:35,  1.57it/s]Extractor Estimating: 441it [04:35,  1.55it/s]Extractor Estimating: 442it [04:36,  1.60it/s]Extractor Estimating: 443it [04:37,  1.59it/s]Extractor Estimating: 444it [04:37,  1.61it/s]Extractor Estimating: 445it [04:38,  1.61it/s]Extractor Estimating: 446it [04:38,  1.56it/s]Extractor Estimating: 447it [04:39,  1.58it/s]Extractor Estimating: 448it [04:40,  1.56it/s]Extractor Estimating: 449it [04:40,  1.60it/s]Extractor Estimating: 450it [04:41,  1.63it/s]Extractor Estimating: 451it [04:42,  1.59it/s]Extractor Estimating: 452it [04:42,  1.55it/s]Extractor Estimating: 453it [04:43,  1.58it/s]Extractor Estimating: 454it [04:44,  1.53it/s]Extractor Estimating: 455it [04:44,  1.55it/s]Extractor Estimating: 456it [04:45,  1.55it/s]Extractor Estimating: 457it [04:46,  1.50it/s]Extractor Estimating: 458it [04:46,  1.51it/s]Extractor Estimating: 459it [04:47,  1.49it/s]Extractor Estimating: 460it [04:48,  1.52it/s]Extractor Estimating: 461it [04:48,  1.57it/s]Extractor Estimating: 462it [04:49,  1.50it/s]Extractor Estimating: 463it [04:50,  1.47it/s]Extractor Estimating: 464it [04:50,  1.50it/s]Extractor Estimating: 465it [04:51,  1.51it/s]Extractor Estimating: 466it [04:51,  1.52it/s]Extractor Estimating: 467it [04:52,  1.53it/s]Extractor Estimating: 468it [04:53,  1.62it/s]Extractor Estimating: 469it [04:53,  1.59it/s]Extractor Estimating: 470it [04:54,  1.64it/s]Extractor Estimating: 471it [04:55,  1.55it/s]Extractor Estimating: 472it [04:55,  1.54it/s]Extractor Estimating: 473it [04:56,  1.49it/s]Extractor Estimating: 474it [04:57,  1.50it/s]Extractor Estimating: 475it [04:57,  1.52it/s]Extractor Estimating: 476it [04:58,  1.53it/s]Extractor Estimating: 477it [04:59,  1.49it/s]Extractor Estimating: 478it [04:59,  1.52it/s]Extractor Estimating: 479it [05:00,  1.56it/s]Extractor Estimating: 480it [05:00,  1.58it/s]Extractor Estimating: 481it [05:01,  1.57it/s]Extractor Estimating: 482it [05:02,  1.54it/s]Extractor Estimating: 483it [05:02,  1.55it/s]Extractor Estimating: 484it [05:03,  1.56it/s]Extractor Estimating: 485it [05:04,  1.56it/s]Extractor Estimating: 486it [05:04,  1.56it/s]Extractor Estimating: 487it [05:05,  1.55it/s]Extractor Estimating: 488it [05:06,  1.59it/s]Extractor Estimating: 489it [05:06,  1.56it/s]Extractor Estimating: 490it [05:07,  1.50it/s]Extractor Estimating: 491it [05:08,  1.55it/s]Extractor Estimating: 492it [05:08,  1.54it/s]Extractor Estimating: 493it [05:09,  1.60it/s]Extractor Estimating: 494it [05:09,  1.59it/s]Extractor Estimating: 495it [05:10,  1.56it/s]Extractor Estimating: 496it [05:11,  1.55it/s]Extractor Estimating: 497it [05:11,  1.57it/s]Extractor Estimating: 498it [05:12,  1.41it/s]Extractor Estimating: 499it [05:13,  1.41it/s]Extractor Estimating: 500it [05:13,  1.75it/s]Extractor Estimating: 500it [05:13,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:33,172 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:33,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:33,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:33,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:33,195 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:41:33,812 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:41:33,813 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:41:34,404 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:41:35,467 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:41:35,468 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:38,550 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:38,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:38,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:38,585 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:41:38,585 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:41:39,240 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:41:39,241 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:41:39,860 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:41:40,034 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:41:40,071 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:38:42,606 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:38:43,249 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 10380 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 30988
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31088, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31088, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.964, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.976, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.953, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.962, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 67, avg_time 0.966, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 167, avg_time 2.288, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 267, avg_time 0.974, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 367, avg_time 0.974, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 34, avg_time 0.967, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 134, avg_time 0.957, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 234, avg_time 2.255, loss:nan
g_step 1200, step 334, avg_time 0.982, loss:nan
g_step 1300, step 1, avg_time 0.972, loss:nan
g_step 1400, step 101, avg_time 0.977, loss:nan
g_step 1500, step 201, avg_time 0.971, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 301, avg_time 2.241, loss:nan
g_step 1700, step 401, avg_time 0.979, loss:nan
g_step 1800, step 68, avg_time 0.970, loss:nan
g_step 1900, step 168, avg_time 0.982, loss:nan
g_step 2000, step 268, avg_time 0.965, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 368, avg_time 2.259, loss:nan
g_step 2200, step 35, avg_time 0.971, loss:nan
g_step 2300, step 135, avg_time 0.976, loss:nan
g_step 2400, step 235, avg_time 0.964, loss:nan
g_step 2500, step 335, avg_time 0.980, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 2, avg_time 2.262, loss:nan
g_step 2700, step 102, avg_time 0.971, loss:nan
g_step 2800, step 202, avg_time 0.980, loss:nan
g_step 2900, step 302, avg_time 0.966, loss:nan
g_step 3000, step 402, avg_time 0.985, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 69, avg_time 2.257, loss:nan
g_step 3200, step 169, avg_time 0.974, loss:nan
g_step 3300, step 269, avg_time 0.975, loss:nan
g_step 3400, step 369, avg_time 0.968, loss:nan
g_step 3500, step 36, avg_time 0.978, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 136, avg_time 2.261, loss:nan
g_step 3700, step 236, avg_time 0.968, loss:nan
g_step 3800, step 336, avg_time 0.961, loss:nan
g_step 3900, step 3, avg_time 0.983, loss:nan
g_step 4000, step 103, avg_time 0.972, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 203, avg_time 2.257, loss:nan
g_step 4200, step 303, avg_time 0.978, loss:nan
g_step 4300, step 403, avg_time 0.977, loss:nan
g_step 4400, step 70, avg_time 0.963, loss:nan
g_step 4500, step 170, avg_time 0.977, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 270, avg_time 2.262, loss:nan
g_step 4700, step 370, avg_time 0.973, loss:nan
g_step 4800, step 37, avg_time 0.967, loss:nan
g_step 4900, step 137, avg_time 0.961, loss:nan
g_step 5000, step 237, avg_time 0.972, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 337, avg_time 2.272, loss:nan
g_step 5200, step 4, avg_time 0.979, loss:nan
g_step 5300, step 104, avg_time 0.989, loss:nan
g_step 5400, step 204, avg_time 0.965, loss:nan
g_step 5500, step 304, avg_time 0.980, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 404, avg_time 2.253, loss:nan
g_step 5700, step 71, avg_time 0.975, loss:nan
g_step 5800, step 171, avg_time 0.974, loss:nan
g_step 5900, step 271, avg_time 0.975, loss:nan
g_step 6000, step 371, avg_time 0.973, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 38, avg_time 2.255, loss:nan
g_step 6200, step 138, avg_time 0.977, loss:nan
g_step 6300, step 238, avg_time 0.983, loss:nan
g_step 6400, step 338, avg_time 0.977, loss:nan
g_step 6500, step 5, avg_time 0.961, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 105, avg_time 2.261, loss:nan
g_step 6700, step 205, avg_time 0.971, loss:nan
g_step 6800, step 305, avg_time 0.977, loss:nan
g_step 6900, step 405, avg_time 0.971, loss:nan
g_step 7000, step 72, avg_time 0.976, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 172, avg_time 2.252, loss:nan
g_step 7200, step 272, avg_time 0.965, loss:nan
g_step 7300, step 372, avg_time 0.983, loss:nan
g_step 7400, step 39, avg_time 0.979, loss:nan
g_step 7500, step 139, avg_time 0.982, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 239, avg_time 2.247, loss:nan
g_step 7700, step 339, avg_time 0.977, loss:nan
g_step 7800, step 6, avg_time 0.972, loss:nan
g_step 7900, step 106, avg_time 0.970, loss:nan
g_step 8000, step 206, avg_time 0.980, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 306, avg_time 2.261, loss:nan
g_step 8200, step 406, avg_time 0.976, loss:nan
g_step 8300, step 73, avg_time 0.970, loss:nan
g_step 8400, step 173, avg_time 0.976, loss:nan
g_step 8500, step 273, avg_time 0.987, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 373, avg_time 2.257, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:38:43 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:38:43 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-38-42_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:38:44 - WARNING - datasets.builder -   Using custom data configuration default-2b5b243cfa080a6a
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2b5b243cfa080a6a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:38:46,995 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:38:46,996 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:38:46,997 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:38:46,998 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:38:47,062 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:38:47,101 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:38:47,101 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:38:47,101 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:38:47,101 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:38:47,101 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:38:47,101 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:38:47,345 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:38:50,376 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:38:50,389 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2b5b243cfa080a6a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.86ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.73ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.11ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.29ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.40ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.49ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.54ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.56ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.58ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.59ba/s]100%|██████████| 11/11 [00:02<00:00,  5.37ba/s]100%|██████████| 11/11 [00:02<00:00,  4.58ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.51ba/s] 40%|████      | 2/5 [00:00<00:00,  3.42ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.87ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.13ba/s]100%|██████████| 5/5 [00:01<00:00,  4.47ba/s]100%|██████████| 5/5 [00:01<00:00,  4.02ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.64ba/s] 27%|██▋       | 3/11 [00:00<00:01,  6.16ba/s] 45%|████▌     | 5/11 [00:00<00:00,  8.10ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.19ba/s] 82%|████████▏ | 9/11 [00:01<00:00,  9.00ba/s]100%|██████████| 11/11 [00:01<00:00, 10.45ba/s]100%|██████████| 11/11 [00:01<00:00,  8.68ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.87ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.56ba/s]100%|██████████| 5/5 [00:00<00:00,  9.39ba/s]100%|██████████| 5/5 [00:00<00:00,  8.35ba/s]
[INFO|trainer.py:414] 2023-08-28 06:38:58,667 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:38:58,809 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:38:58,809 >>   Num examples = 10520
[INFO|trainer.py:1149] 2023-08-28 06:38:58,809 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:38:58,809 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:38:58,809 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:38:58,809 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:38:58,809 >>   Total optimization steps = 820
  0%|          | 0/820 [00:00<?, ?it/s]  0%|          | 1/820 [00:00<03:55,  3.48it/s]  0%|          | 2/820 [00:00<03:47,  3.60it/s]  0%|          | 3/820 [00:00<03:44,  3.64it/s]  0%|          | 4/820 [00:01<04:05,  3.33it/s]  1%|          | 5/820 [00:01<03:58,  3.42it/s]  1%|          | 6/820 [00:01<03:53,  3.49it/s]  1%|          | 7/820 [00:01<03:50,  3.53it/s]  1%|          | 8/820 [00:02<03:48,  3.56it/s]  1%|          | 9/820 [00:02<03:46,  3.57it/s]  1%|          | 10/820 [00:02<03:45,  3.59it/s]  1%|▏         | 11/820 [00:03<03:45,  3.59it/s]  1%|▏         | 12/820 [00:03<03:44,  3.60it/s]  2%|▏         | 13/820 [00:03<03:43,  3.61it/s]  2%|▏         | 14/820 [00:03<03:43,  3.61it/s]  2%|▏         | 15/820 [00:04<03:55,  3.42it/s]  2%|▏         | 16/820 [00:04<03:51,  3.48it/s]  2%|▏         | 17/820 [00:04<03:48,  3.52it/s]  2%|▏         | 18/820 [00:05<03:46,  3.54it/s]  2%|▏         | 19/820 [00:05<03:44,  3.56it/s]  2%|▏         | 20/820 [00:05<03:43,  3.57it/s]  3%|▎         | 21/820 [00:05<03:42,  3.58it/s]  3%|▎         | 22/820 [00:06<03:42,  3.59it/s]  3%|▎         | 23/820 [00:06<03:41,  3.60it/s]  3%|▎         | 24/820 [00:06<03:40,  3.61it/s]  3%|▎         | 25/820 [00:07<03:40,  3.61it/s]  3%|▎         | 26/820 [00:07<03:44,  3.53it/s]  3%|▎         | 27/820 [00:07<03:43,  3.55it/s]  3%|▎         | 28/820 [00:07<03:42,  3.57it/s]  4%|▎         | 29/820 [00:08<03:41,  3.58it/s]  4%|▎         | 30/820 [00:08<03:40,  3.59it/s]  4%|▍         | 31/820 [00:08<03:39,  3.60it/s]  4%|▍         | 32/820 [00:08<03:38,  3.60it/s]  4%|▍         | 33/820 [00:09<03:38,  3.60it/s]  4%|▍         | 34/820 [00:09<03:38,  3.60it/s]  4%|▍         | 35/820 [00:09<03:37,  3.60it/s]  4%|▍         | 36/820 [00:10<03:37,  3.60it/s]  5%|▍         | 37/820 [00:10<03:43,  3.50it/s]  5%|▍         | 38/820 [00:10<03:41,  3.53it/s]  5%|▍         | 39/820 [00:10<03:39,  3.55it/s]  5%|▍         | 40/820 [00:11<03:38,  3.57it/s]  5%|▌         | 41/820 [00:11<03:37,  3.58it/s]  5%|▌         | 42/820 [00:11<03:36,  3.59it/s]  5%|▌         | 43/820 [00:12<03:36,  3.59it/s]  5%|▌         | 44/820 [00:12<03:35,  3.60it/s]  5%|▌         | 45/820 [00:12<03:35,  3.60it/s]  6%|▌         | 46/820 [00:12<03:34,  3.60it/s]  6%|▌         | 47/820 [00:13<03:34,  3.60it/s]  6%|▌         | 48/820 [00:13<03:39,  3.51it/s]  6%|▌         | 49/820 [00:13<03:37,  3.54it/s]  6%|▌         | 50/820 [00:14<03:36,  3.56it/s]  6%|▌         | 51/820 [00:14<03:34,  3.58it/s]  6%|▋         | 52/820 [00:14<03:34,  3.58it/s]  6%|▋         | 53/820 [00:14<03:33,  3.59it/s]  7%|▋         | 54/820 [00:15<03:33,  3.59it/s]  7%|▋         | 55/820 [00:15<03:32,  3.59it/s]  7%|▋         | 56/820 [00:15<03:32,  3.60it/s]  7%|▋         | 57/820 [00:15<03:32,  3.60it/s]  7%|▋         | 58/820 [00:16<03:31,  3.60it/s]  7%|▋         | 59/820 [00:16<03:31,  3.60it/s]  7%|▋         | 60/820 [00:16<03:30,  3.60it/s]  7%|▋         | 61/820 [00:17<03:30,  3.60it/s]  8%|▊         | 62/820 [00:17<03:33,  3.55it/s]  8%|▊         | 63/820 [00:17<03:32,  3.56it/s]  8%|▊         | 64/820 [00:17<03:31,  3.58it/s]  8%|▊         | 65/820 [00:18<03:30,  3.58it/s]  8%|▊         | 66/820 [00:18<03:30,  3.59it/s]  8%|▊         | 67/820 [00:18<03:29,  3.59it/s]  8%|▊         | 68/820 [00:19<03:29,  3.60it/s]  8%|▊         | 69/820 [00:19<03:28,  3.60it/s]  9%|▊         | 70/820 [00:19<03:28,  3.60it/s]  9%|▊         | 71/820 [00:19<03:28,  3.60it/s]  9%|▉         | 72/820 [00:20<03:27,  3.60it/s]  9%|▉         | 73/820 [00:20<03:31,  3.53it/s]  9%|▉         | 74/820 [00:20<03:29,  3.56it/s]  9%|▉         | 75/820 [00:21<03:28,  3.57it/s]  9%|▉         | 76/820 [00:21<03:28,  3.58it/s]  9%|▉         | 77/820 [00:21<03:27,  3.58it/s] 10%|▉         | 78/820 [00:21<03:26,  3.59it/s] 10%|▉         | 79/820 [00:22<03:26,  3.59it/s] 10%|▉         | 80/820 [00:22<03:26,  3.59it/s] 10%|▉         | 81/820 [00:22<03:25,  3.60it/s] 10%|█         | 82/820 [00:22<03:25,  3.60it/s] 10%|█         | 83/820 [00:23<03:24,  3.60it/s] 10%|█         | 84/820 [00:23<03:32,  3.47it/s] 10%|█         | 85/820 [00:23<03:29,  3.50it/s] 10%|█         | 86/820 [00:24<03:27,  3.53it/s] 11%|█         | 87/820 [00:24<03:26,  3.55it/s] 11%|█         | 88/820 [00:24<03:25,  3.57it/s] 11%|█         | 89/820 [00:24<03:24,  3.58it/s] 11%|█         | 90/820 [00:25<03:23,  3.58it/s] 11%|█         | 91/820 [00:25<03:22,  3.59it/s] 11%|█         | 92/820 [00:25<03:22,  3.59it/s] 11%|█▏        | 93/820 [00:26<03:22,  3.59it/s] 11%|█▏        | 94/820 [00:26<03:22,  3.59it/s] 12%|█▏        | 95/820 [00:26<03:29,  3.46it/s] 12%|█▏        | 96/820 [00:26<03:26,  3.50it/s] 12%|█▏        | 97/820 [00:27<03:24,  3.53it/s] 12%|█▏        | 98/820 [00:27<03:23,  3.55it/s] 12%|█▏        | 99/820 [00:27<03:22,  3.57it/s] 12%|█▏        | 100/820 [00:28<03:21,  3.57it/s] 12%|█▏        | 101/820 [00:28<03:20,  3.58it/s] 12%|█▏        | 102/820 [00:28<03:20,  3.59it/s] 13%|█▎        | 103/820 [00:28<03:20,  3.58it/s] 13%|█▎        | 104/820 [00:29<03:19,  3.58it/s] 13%|█▎        | 105/820 [00:29<03:19,  3.59it/s] 13%|█▎        | 106/820 [00:29<03:23,  3.51it/s] 13%|█▎        | 107/820 [00:29<03:21,  3.53it/s] 13%|█▎        | 108/820 [00:30<03:20,  3.55it/s] 13%|█▎        | 109/820 [00:30<03:19,  3.57it/s] 13%|█▎        | 110/820 [00:30<03:18,  3.58it/s] 14%|█▎        | 111/820 [00:31<03:17,  3.59it/s] 14%|█▎        | 112/820 [00:31<03:17,  3.59it/s] 14%|█▍        | 113/820 [00:31<03:16,  3.59it/s] 14%|█▍        | 114/820 [00:31<03:16,  3.59it/s] 14%|█▍        | 115/820 [00:32<03:16,  3.59it/s] 14%|█▍        | 116/820 [00:32<03:16,  3.59it/s] 14%|█▍        | 117/820 [00:32<03:21,  3.50it/s] 14%|█▍        | 118/820 [00:33<03:19,  3.52it/s] 15%|█▍        | 119/820 [00:33<03:18,  3.54it/s] 15%|█▍        | 120/820 [00:33<03:16,  3.56it/s] 15%|█▍        | 121/820 [00:33<03:16,  3.57it/s] 15%|█▍        | 122/820 [00:34<03:15,  3.57it/s] 15%|█▌        | 123/820 [00:34<03:14,  3.58it/s] 15%|█▌        | 124/820 [00:34<03:14,  3.59it/s] 15%|█▌        | 125/820 [00:35<03:13,  3.59it/s] 15%|█▌        | 126/820 [00:35<03:13,  3.59it/s] 15%|█▌        | 127/820 [00:35<03:12,  3.60it/s] 16%|█▌        | 128/820 [00:35<03:17,  3.50it/s] 16%|█▌        | 129/820 [00:36<03:15,  3.53it/s] 16%|█▌        | 130/820 [00:36<03:14,  3.55it/s] 16%|█▌        | 131/820 [00:36<03:13,  3.56it/s] 16%|█▌        | 132/820 [00:37<03:12,  3.57it/s] 16%|█▌        | 133/820 [00:37<03:12,  3.58it/s] 16%|█▋        | 134/820 [00:37<03:11,  3.58it/s] 16%|█▋        | 135/820 [00:37<03:11,  3.58it/s] 17%|█▋        | 136/820 [00:38<03:10,  3.58it/s] 17%|█▋        | 137/820 [00:38<03:10,  3.59it/s] 17%|█▋        | 138/820 [00:38<03:10,  3.59it/s] 17%|█▋        | 139/820 [00:38<03:13,  3.53it/s] 17%|█▋        | 140/820 [00:39<03:11,  3.55it/s] 17%|█▋        | 141/820 [00:39<03:10,  3.56it/s] 17%|█▋        | 142/820 [00:39<03:09,  3.57it/s] 17%|█▋        | 143/820 [00:40<03:09,  3.58it/s] 18%|█▊        | 144/820 [00:40<03:08,  3.58it/s] 18%|█▊        | 145/820 [00:40<03:08,  3.59it/s] 18%|█▊        | 146/820 [00:40<03:07,  3.59it/s] 18%|█▊        | 147/820 [00:41<03:07,  3.58it/s] 18%|█▊        | 148/820 [00:41<03:11,  3.51it/s] 18%|█▊        | 149/820 [00:41<03:10,  3.52it/s] 18%|█▊        | 150/820 [00:42<03:16,  3.41it/s] 18%|█▊        | 151/820 [00:42<03:12,  3.47it/s] 19%|█▊        | 152/820 [00:42<03:10,  3.51it/s] 19%|█▊        | 153/820 [00:42<03:08,  3.53it/s] 19%|█▉        | 154/820 [00:43<03:07,  3.55it/s] 19%|█▉        | 155/820 [00:43<03:06,  3.57it/s] 19%|█▉        | 156/820 [00:43<03:05,  3.57it/s] 19%|█▉        | 157/820 [00:44<03:55,  2.81it/s] 19%|█▉        | 158/820 [00:44<03:52,  2.85it/s] 19%|█▉        | 159/820 [00:44<03:43,  2.96it/s] 20%|█▉        | 160/820 [00:45<03:31,  3.12it/s] 20%|█▉        | 161/820 [00:45<03:22,  3.25it/s] 20%|█▉        | 162/820 [00:45<03:16,  3.34it/s] 20%|█▉        | 163/820 [00:46<03:12,  3.41it/s] 20%|██        | 164/820 [00:46<03:09,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 06:39:45,191 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:39:45,191 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 06:39:45,191 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.82it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.72it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.83it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.84it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.00it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.54it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.24it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.11it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.19it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.45it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.58it/s][A
 10%|█         | 63/608 [00:01<00:12, 43.42it/s][A
 11%|█         | 68/608 [00:01<00:12, 44.02it/s][A
 12%|█▏        | 73/608 [00:01<00:12, 44.29it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 44.33it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 44.45it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.43it/s][A
 15%|█▌        | 93/608 [00:02<00:12, 42.05it/s][A
 16%|█▋        | 99/608 [00:02<00:11, 44.51it/s][A
 17%|█▋        | 104/608 [00:02<00:11, 44.91it/s][A
 18%|█▊        | 109/608 [00:02<00:11, 45.10it/s][A
 19%|█▉        | 114/608 [00:02<00:10, 45.32it/s][A
 20%|█▉        | 119/608 [00:02<00:10, 45.18it/s][A
 20%|██        | 124/608 [00:02<00:10, 45.14it/s][A
 21%|██        | 129/608 [00:02<00:10, 44.93it/s][A
 22%|██▏       | 134/608 [00:02<00:10, 44.90it/s][A
 23%|██▎       | 139/608 [00:03<00:10, 44.95it/s][A
 24%|██▎       | 144/608 [00:03<00:10, 45.07it/s][A
 25%|██▍       | 149/608 [00:03<00:10, 45.22it/s][A
 25%|██▌       | 154/608 [00:03<00:10, 45.28it/s][A
 26%|██▌       | 159/608 [00:03<00:09, 45.45it/s][A
 27%|██▋       | 164/608 [00:03<00:09, 45.44it/s][A
 28%|██▊       | 169/608 [00:03<00:09, 45.34it/s][A
 29%|██▊       | 174/608 [00:03<00:09, 44.98it/s][A
 29%|██▉       | 179/608 [00:03<00:09, 45.03it/s][A
 30%|███       | 184/608 [00:04<00:09, 45.05it/s][A
 31%|███       | 189/608 [00:04<00:09, 45.13it/s][A
 32%|███▏      | 194/608 [00:04<00:09, 45.23it/s][A
 33%|███▎      | 199/608 [00:04<00:09, 44.79it/s][A
 34%|███▎      | 204/608 [00:04<00:08, 45.11it/s][A
 34%|███▍      | 209/608 [00:04<00:08, 45.20it/s][A
 35%|███▌      | 214/608 [00:04<00:08, 45.20it/s][A
 36%|███▌      | 219/608 [00:04<00:08, 44.95it/s][A
 37%|███▋      | 224/608 [00:04<00:08, 45.01it/s][A
 38%|███▊      | 229/608 [00:05<00:08, 45.01it/s][A
 38%|███▊      | 234/608 [00:05<00:08, 45.13it/s][A
 39%|███▉      | 239/608 [00:05<00:08, 45.10it/s][A
 40%|████      | 244/608 [00:05<00:08, 45.30it/s][A
 41%|████      | 249/608 [00:05<00:07, 45.41it/s][A
 42%|████▏     | 254/608 [00:05<00:07, 45.43it/s][A
 43%|████▎     | 259/608 [00:05<00:07, 45.29it/s][A
 43%|████▎     | 264/608 [00:05<00:07, 45.02it/s][A
 44%|████▍     | 269/608 [00:05<00:07, 44.90it/s][A
 45%|████▌     | 274/608 [00:06<00:07, 44.94it/s][A
 46%|████▌     | 279/608 [00:06<00:07, 45.05it/s][A
 47%|████▋     | 284/608 [00:06<00:07, 45.11it/s][A
 48%|████▊     | 289/608 [00:06<00:07, 45.24it/s][A
 48%|████▊     | 294/608 [00:06<00:06, 45.47it/s][A
 49%|████▉     | 299/608 [00:06<00:06, 45.53it/s][A
 50%|█████     | 304/608 [00:06<00:06, 45.27it/s][A
 51%|█████     | 309/608 [00:06<00:06, 45.07it/s][A
 52%|█████▏    | 314/608 [00:06<00:06, 44.91it/s][A
 52%|█████▏    | 319/608 [00:07<00:06, 44.97it/s][A
 53%|█████▎    | 324/608 [00:07<00:06, 45.06it/s][A
 54%|█████▍    | 329/608 [00:07<00:06, 45.05it/s][A
 55%|█████▍    | 334/608 [00:07<00:06, 45.15it/s][A
 56%|█████▌    | 339/608 [00:07<00:06, 41.86it/s][A
 57%|█████▋    | 344/608 [00:07<00:06, 42.99it/s][A
 57%|█████▋    | 349/608 [00:07<00:05, 43.72it/s][A
 58%|█████▊    | 354/608 [00:07<00:05, 44.29it/s][A
 59%|█████▉    | 359/608 [00:07<00:05, 44.54it/s][A
 60%|█████▉    | 364/608 [00:08<00:05, 44.66it/s][A
 61%|██████    | 369/608 [00:08<00:05, 44.83it/s][A
 62%|██████▏   | 374/608 [00:08<00:05, 44.96it/s][A
 62%|██████▏   | 379/608 [00:08<00:05, 44.71it/s][A
 63%|██████▎   | 384/608 [00:08<00:04, 44.83it/s][A
 64%|██████▍   | 389/608 [00:08<00:04, 45.05it/s][A
 65%|██████▍   | 394/608 [00:08<00:04, 45.16it/s][A
 66%|██████▌   | 399/608 [00:08<00:04, 45.36it/s][A
 66%|██████▋   | 404/608 [00:08<00:04, 45.31it/s][A
 67%|██████▋   | 409/608 [00:09<00:04, 45.30it/s][A
 68%|██████▊   | 414/608 [00:09<00:04, 45.29it/s][A
 69%|██████▉   | 419/608 [00:09<00:04, 45.12it/s][A
 70%|██████▉   | 424/608 [00:09<00:04, 45.01it/s][A
 71%|███████   | 429/608 [00:09<00:03, 44.89it/s][A
 71%|███████▏  | 434/608 [00:09<00:03, 45.02it/s][A
 72%|███████▏  | 439/608 [00:09<00:03, 45.25it/s][A
 73%|███████▎  | 444/608 [00:09<00:03, 45.44it/s][A
 74%|███████▍  | 449/608 [00:09<00:03, 45.39it/s][A
 75%|███████▍  | 454/608 [00:10<00:03, 45.43it/s][A
 75%|███████▌  | 459/608 [00:10<00:03, 45.32it/s][A
 76%|███████▋  | 464/608 [00:10<00:03, 45.08it/s][A
 77%|███████▋  | 469/608 [00:10<00:03, 45.05it/s][A
 78%|███████▊  | 474/608 [00:10<00:03, 42.23it/s][A
 79%|███████▉  | 479/608 [00:10<00:02, 43.25it/s][A
 80%|███████▉  | 484/608 [00:10<00:02, 43.99it/s][A
 80%|████████  | 489/608 [00:10<00:02, 44.54it/s][A
 81%|████████▏ | 494/608 [00:10<00:02, 44.79it/s][A
 82%|████████▏ | 499/608 [00:11<00:02, 44.98it/s][A
 83%|████████▎ | 504/608 [00:11<00:02, 44.96it/s][A
 84%|████████▎ | 509/608 [00:11<00:02, 44.88it/s][A
 85%|████████▍ | 514/608 [00:11<00:02, 44.62it/s][A
 85%|████████▌ | 519/608 [00:11<00:01, 44.67it/s][A
 86%|████████▌ | 524/608 [00:11<00:01, 44.89it/s][A
 87%|████████▋ | 529/608 [00:11<00:01, 45.13it/s][A
 88%|████████▊ | 534/608 [00:11<00:01, 45.18it/s][A
 89%|████████▊ | 539/608 [00:11<00:01, 45.43it/s][A
 89%|████████▉ | 544/608 [00:12<00:01, 45.48it/s][A
 90%|█████████ | 549/608 [00:12<00:01, 45.34it/s][A
 91%|█████████ | 554/608 [00:12<00:01, 45.16it/s][A
 92%|█████████▏| 559/608 [00:12<00:01, 44.91it/s][A
 93%|█████████▎| 564/608 [00:12<00:00, 44.87it/s][A
 94%|█████████▎| 569/608 [00:12<00:00, 44.92it/s][A
 94%|█████████▍| 574/608 [00:12<00:00, 45.16it/s][A
 95%|█████████▌| 579/608 [00:12<00:00, 45.23it/s][A
 96%|█████████▌| 584/608 [00:12<00:00, 45.45it/s][A
 97%|█████████▋| 589/608 [00:13<00:00, 45.39it/s][A
 98%|█████████▊| 594/608 [00:13<00:00, 45.34it/s][A
 99%|█████████▊| 599/608 [00:13<00:00, 45.15it/s][A
 99%|█████████▉| 604/608 [00:13<00:00, 44.97it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.97it/s][A 20%|██        | 164/820 [00:59<03:09,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:39:59,281 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 06:39:59,563 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:40:03,641 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:40:03,797 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:40:03,856 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164/special_tokens_map.json
 20%|██        | 165/820 [01:06<1:06:51,  6.12s/it] 20%|██        | 166/820 [01:06<47:38,  4.37s/it]  /cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|██        | 167/820 [01:06<34:12,  3.14s/it] 20%|██        | 168/820 [01:06<24:48,  2.28s/it] 21%|██        | 169/820 [01:07<18:14,  1.68s/it] 21%|██        | 170/820 [01:07<13:39,  1.26s/it] 21%|██        | 171/820 [01:07<10:26,  1.04it/s] 21%|██        | 172/820 [01:08<08:12,  1.32it/s] 21%|██        | 173/820 [01:08<06:40,  1.62it/s] 21%|██        | 174/820 [01:08<05:33,  1.94it/s] 21%|██▏       | 175/820 [01:08<04:46,  2.25it/s] 21%|██▏       | 176/820 [01:09<04:14,  2.53it/s] 22%|██▏       | 177/820 [01:09<03:51,  2.78it/s] 22%|██▏       | 178/820 [01:09<03:35,  2.98it/s] 22%|██▏       | 179/820 [01:09<03:24,  3.14it/s] 22%|██▏       | 180/820 [01:10<03:15,  3.27it/s] 22%|██▏       | 181/820 [01:10<03:10,  3.36it/s] 22%|██▏       | 182/820 [01:10<03:06,  3.43it/s] 22%|██▏       | 183/820 [01:11<03:03,  3.47it/s] 22%|██▏       | 184/820 [01:11<03:05,  3.43it/s] 23%|██▎       | 185/820 [01:11<03:02,  3.47it/s] 23%|██▎       | 186/820 [01:11<03:00,  3.51it/s] 23%|██▎       | 187/820 [01:12<02:59,  3.53it/s] 23%|██▎       | 188/820 [01:12<02:58,  3.54it/s] 23%|██▎       | 189/820 [01:12<02:57,  3.55it/s] 23%|██▎       | 190/820 [01:13<02:56,  3.56it/s] 23%|██▎       | 191/820 [01:13<02:56,  3.57it/s] 23%|██▎       | 192/820 [01:13<02:55,  3.57it/s] 24%|██▎       | 193/820 [01:13<02:55,  3.57it/s] 24%|██▎       | 194/820 [01:14<02:54,  3.58it/s] 24%|██▍       | 195/820 [01:14<02:59,  3.49it/s] 24%|██▍       | 196/820 [01:14<02:57,  3.52it/s] 24%|██▍       | 197/820 [01:15<02:56,  3.54it/s] 24%|██▍       | 198/820 [01:15<02:55,  3.55it/s] 24%|██▍       | 199/820 [01:15<02:54,  3.56it/s] 24%|██▍       | 200/820 [01:15<02:53,  3.57it/s] 25%|██▍       | 201/820 [01:16<02:53,  3.57it/s] 25%|██▍       | 202/820 [01:16<02:52,  3.58it/s] 25%|██▍       | 203/820 [01:16<02:52,  3.58it/s] 25%|██▍       | 204/820 [01:17<02:52,  3.58it/s] 25%|██▌       | 205/820 [01:17<02:51,  3.58it/s] 25%|██▌       | 206/820 [01:17<02:51,  3.59it/s] 25%|██▌       | 207/820 [01:17<02:50,  3.59it/s] 25%|██▌       | 208/820 [01:18<02:50,  3.59it/s] 25%|██▌       | 209/820 [01:18<02:50,  3.59it/s] 26%|██▌       | 210/820 [01:18<02:52,  3.53it/s] 26%|██▌       | 211/820 [01:18<02:51,  3.55it/s] 26%|██▌       | 212/820 [01:19<02:50,  3.56it/s] 26%|██▌       | 213/820 [01:19<02:50,  3.57it/s] 26%|██▌       | 214/820 [01:19<02:49,  3.57it/s] 26%|██▌       | 215/820 [01:20<02:49,  3.58it/s] 26%|██▋       | 216/820 [01:20<02:48,  3.58it/s] 26%|██▋       | 217/820 [01:20<02:48,  3.58it/s] 27%|██▋       | 218/820 [01:20<02:47,  3.59it/s] 27%|██▋       | 219/820 [01:21<02:47,  3.59it/s] 27%|██▋       | 220/820 [01:21<02:47,  3.59it/s] 27%|██▋       | 221/820 [01:21<02:48,  3.56it/s] 27%|██▋       | 222/820 [01:22<02:47,  3.57it/s] 27%|██▋       | 223/820 [01:22<02:46,  3.58it/s] 27%|██▋       | 224/820 [01:22<02:46,  3.58it/s] 27%|██▋       | 225/820 [01:22<02:45,  3.59it/s] 28%|██▊       | 226/820 [01:23<02:45,  3.59it/s] 28%|██▊       | 227/820 [01:23<02:45,  3.59it/s] 28%|██▊       | 228/820 [01:23<02:45,  3.59it/s] 28%|██▊       | 229/820 [01:23<02:44,  3.59it/s] 28%|██▊       | 230/820 [01:24<02:44,  3.59it/s] 28%|██▊       | 231/820 [01:24<02:44,  3.59it/s] 28%|██▊       | 232/820 [01:24<02:46,  3.52it/s] 28%|██▊       | 233/820 [01:25<02:45,  3.54it/s] 29%|██▊       | 234/820 [01:25<02:44,  3.56it/s] 29%|██▊       | 235/820 [01:25<02:43,  3.57it/s] 29%|██▉       | 236/820 [01:25<02:43,  3.58it/s] 29%|██▉       | 237/820 [01:26<02:42,  3.58it/s] 29%|██▉       | 238/820 [01:26<02:42,  3.59it/s] 29%|██▉       | 239/820 [01:26<02:41,  3.59it/s] 29%|██▉       | 240/820 [01:27<02:41,  3.59it/s] 29%|██▉       | 241/820 [01:27<02:41,  3.59it/s] 30%|██▉       | 242/820 [01:27<02:41,  3.59it/s] 30%|██▉       | 243/820 [01:27<02:46,  3.46it/s] 30%|██▉       | 244/820 [01:28<02:44,  3.50it/s] 30%|██▉       | 245/820 [01:28<02:42,  3.53it/s] 30%|███       | 246/820 [01:28<02:41,  3.55it/s] 30%|███       | 247/820 [01:29<02:40,  3.56it/s] 30%|███       | 248/820 [01:29<02:40,  3.57it/s] 30%|███       | 249/820 [01:29<02:39,  3.57it/s] 30%|███       | 250/820 [01:29<02:39,  3.58it/s] 31%|███       | 251/820 [01:30<02:38,  3.58it/s] 31%|███       | 252/820 [01:30<02:38,  3.58it/s] 31%|███       | 253/820 [01:30<02:38,  3.59it/s] 31%|███       | 254/820 [01:31<02:40,  3.53it/s] 31%|███       | 255/820 [01:31<02:39,  3.55it/s] 31%|███       | 256/820 [01:31<02:38,  3.57it/s] 31%|███▏      | 257/820 [01:31<02:37,  3.57it/s] 31%|███▏      | 258/820 [01:32<02:37,  3.58it/s] 32%|███▏      | 259/820 [01:32<02:36,  3.58it/s] 32%|███▏      | 260/820 [01:32<02:36,  3.58it/s] 32%|███▏      | 261/820 [01:32<02:35,  3.59it/s] 32%|███▏      | 262/820 [01:33<02:35,  3.58it/s] 32%|███▏      | 263/820 [01:33<02:35,  3.59it/s] 32%|███▏      | 264/820 [01:33<02:35,  3.59it/s] 32%|███▏      | 265/820 [01:34<02:38,  3.51it/s] 32%|███▏      | 266/820 [01:34<02:36,  3.54it/s] 33%|███▎      | 267/820 [01:34<02:35,  3.55it/s] 33%|███▎      | 268/820 [01:34<02:34,  3.57it/s] 33%|███▎      | 269/820 [01:35<02:34,  3.57it/s] 33%|███▎      | 270/820 [01:35<02:33,  3.58it/s] 33%|███▎      | 271/820 [01:35<02:33,  3.58it/s] 33%|███▎      | 272/820 [01:36<02:33,  3.58it/s] 33%|███▎      | 273/820 [01:36<02:32,  3.58it/s] 33%|███▎      | 274/820 [01:36<02:32,  3.58it/s] 34%|███▎      | 275/820 [01:36<02:32,  3.58it/s] 34%|███▎      | 276/820 [01:37<02:35,  3.51it/s] 34%|███▍      | 277/820 [01:37<02:33,  3.53it/s] 34%|███▍      | 278/820 [01:37<02:32,  3.54it/s] 34%|███▍      | 279/820 [01:38<02:32,  3.55it/s] 34%|███▍      | 280/820 [01:38<02:31,  3.56it/s] 34%|███▍      | 281/820 [01:38<02:31,  3.56it/s] 34%|███▍      | 282/820 [01:38<02:30,  3.57it/s] 35%|███▍      | 283/820 [01:39<02:30,  3.57it/s] 35%|███▍      | 284/820 [01:39<02:29,  3.57it/s] 35%|███▍      | 285/820 [01:39<02:29,  3.58it/s] 35%|███▍      | 286/820 [01:39<02:29,  3.58it/s] 35%|███▌      | 287/820 [01:40<02:30,  3.54it/s] 35%|███▌      | 288/820 [01:40<02:29,  3.55it/s] 35%|███▌      | 289/820 [01:40<02:29,  3.56it/s] 35%|███▌      | 290/820 [01:41<02:28,  3.57it/s] 35%|███▌      | 291/820 [01:41<02:27,  3.58it/s] 36%|███▌      | 292/820 [01:41<02:30,  3.52it/s] 36%|███▌      | 293/820 [01:41<02:28,  3.54it/s] 36%|███▌      | 294/820 [01:42<02:27,  3.57it/s] 36%|███▌      | 295/820 [01:42<02:26,  3.59it/s] 36%|███▌      | 296/820 [01:42<02:25,  3.61it/s] 36%|███▌      | 297/820 [01:43<02:24,  3.61it/s] 36%|███▋      | 298/820 [01:43<02:26,  3.57it/s] 36%|███▋      | 299/820 [01:43<02:25,  3.59it/s] 37%|███▋      | 300/820 [01:43<02:24,  3.60it/s] 37%|███▋      | 301/820 [01:44<02:48,  3.08it/s] 37%|███▋      | 302/820 [01:44<02:44,  3.15it/s] 37%|███▋      | 303/820 [01:44<02:37,  3.28it/s] 37%|███▋      | 304/820 [01:45<02:32,  3.38it/s] 37%|███▋      | 305/820 [01:45<02:29,  3.45it/s] 37%|███▋      | 306/820 [01:45<02:26,  3.51it/s] 37%|███▋      | 307/820 [01:46<02:24,  3.55it/s] 38%|███▊      | 308/820 [01:46<02:23,  3.58it/s] 38%|███▊      | 309/820 [01:46<02:22,  3.59it/s] 38%|███▊      | 310/820 [01:46<02:21,  3.61it/s] 38%|███▊      | 311/820 [01:47<02:20,  3.62it/s] 38%|███▊      | 312/820 [01:47<02:20,  3.63it/s] 38%|███▊      | 313/820 [01:47<02:19,  3.63it/s] 38%|███▊      | 314/820 [01:47<02:19,  3.64it/s] 38%|███▊      | 315/820 [01:48<02:18,  3.64it/s] 39%|███▊      | 316/820 [01:48<02:18,  3.64it/s] 39%|███▊      | 317/820 [01:48<02:18,  3.64it/s] 39%|███▉      | 318/820 [01:49<02:17,  3.64it/s] 39%|███▉      | 319/820 [01:49<02:17,  3.64it/s] 39%|███▉      | 320/820 [01:49<02:19,  3.58it/s] 39%|███▉      | 321/820 [01:49<02:18,  3.60it/s] 39%|███▉      | 322/820 [01:50<02:18,  3.61it/s] 39%|███▉      | 323/820 [01:50<02:17,  3.61it/s] 40%|███▉      | 324/820 [01:50<02:16,  3.62it/s] 40%|███▉      | 325/820 [01:50<02:16,  3.63it/s] 40%|███▉      | 326/820 [01:51<02:15,  3.63it/s] 40%|███▉      | 327/820 [01:51<02:15,  3.63it/s] 40%|████      | 328/820 [01:51<02:15,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 06:40:50,646 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:40:50,646 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 06:40:50,646 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.6122, 'eval_samples_per_second': 357.326, 'eval_steps_per_second': 44.666, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.46it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.25it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.27it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.12it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.66it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.41it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.27it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.28it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.27it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.33it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.40it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.44it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.35it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.25it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.13it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.09it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.19it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 45.13it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.16it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.23it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.27it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.21it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.03it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.12it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.13it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 45.01it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 45.18it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.26it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.21it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.25it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.30it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.18it/s][A
 27%|██▋       | 167/608 [00:03<00:10, 41.49it/s][A
 28%|██▊       | 172/608 [00:03<00:10, 42.68it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 43.58it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.15it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.61it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 44.78it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 44.98it/s][A
 33%|███▎      | 202/608 [00:04<00:09, 44.92it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 44.78it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 44.69it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.85it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.17it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.33it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.35it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.32it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.36it/s][A
 41%|████      | 247/608 [00:05<00:07, 45.23it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.08it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.88it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.06it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.12it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.26it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.26it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.45it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.38it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.30it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.04it/s][A
 50%|████▉     | 302/608 [00:06<00:07, 41.54it/s][A
 50%|█████     | 307/608 [00:06<00:07, 42.72it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 43.63it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.19it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.69it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.83it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.96it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 44.90it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 44.65it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 44.65it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 44.90it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.10it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 45.36it/s][A
 60%|██████    | 367/608 [00:08<00:05, 45.47it/s][A
 61%|██████    | 372/608 [00:08<00:05, 45.48it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.33it/s][A
 63%|██████▎   | 382/608 [00:08<00:04, 45.20it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.94it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.85it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 44.88it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 45.11it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 45.31it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.30it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.49it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.36it/s][A
 70%|███████   | 427/608 [00:09<00:04, 45.16it/s][A
 71%|███████   | 432/608 [00:09<00:03, 44.95it/s][A
 72%|███████▏  | 437/608 [00:09<00:04, 41.92it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 43.03it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 43.87it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.33it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.79it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.00it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.00it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 44.91it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 44.65it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 44.60it/s][A
 80%|████████  | 487/608 [00:10<00:02, 44.91it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.24it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 45.36it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.46it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.45it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.33it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.12it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.91it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.89it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.03it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.26it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 45.24it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.30it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.26it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.18it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 44.90it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 44.76it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 41.02it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 42.37it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 43.30it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 44.06it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.55it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.95it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.96it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 44.88it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.88it/s][A 40%|████      | 328/820 [02:05<02:15,  3.63it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:41:04,568 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 06:41:04,807 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:41:07,749 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:41:07,989 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:41:08,134 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328/special_tokens_map.json
 40%|████      | 329/820 [02:10<47:29,  5.80s/it] 40%|████      | 330/820 [02:10<33:52,  4.15s/it] 40%|████      | 331/820 [02:11<24:21,  2.99s/it] 40%|████      | 332/820 [02:11<17:41,  2.18s/it] 41%|████      | 333/820 [02:11<13:02,  1.61s/it] 41%|████      | 334/820 [02:11<09:47,  1.21s/it] 41%|████      | 335/820 [02:12<07:30,  1.08it/s] 41%|████      | 336/820 [02:12<05:55,  1.36it/s] 41%|████      | 337/820 [02:12<04:48,  1.67it/s] 41%|████      | 338/820 [02:13<04:02,  1.99it/s] 41%|████▏     | 339/820 [02:13<03:29,  2.30it/s] 41%|████▏     | 340/820 [02:13<03:06,  2.57it/s] 42%|████▏     | 341/820 [02:13<02:54,  2.75it/s] 42%|████▏     | 342/820 [02:14<02:41,  2.96it/s] 42%|████▏     | 343/820 [02:14<02:32,  3.12it/s] 42%|████▏     | 344/820 [02:14<02:26,  3.24it/s] 42%|████▏     | 345/820 [02:14<02:22,  3.34it/s] 42%|████▏     | 346/820 [02:15<02:19,  3.41it/s] 42%|████▏     | 347/820 [02:15<02:16,  3.46it/s] 42%|████▏     | 348/820 [02:15<02:15,  3.49it/s] 43%|████▎     | 349/820 [02:16<02:13,  3.52it/s] 43%|████▎     | 350/820 [02:16<02:12,  3.53it/s] 43%|████▎     | 351/820 [02:16<02:12,  3.55it/s] 43%|████▎     | 352/820 [02:16<02:14,  3.47it/s] 43%|████▎     | 353/820 [02:17<02:13,  3.50it/s] 43%|████▎     | 354/820 [02:17<02:12,  3.53it/s] 43%|████▎     | 355/820 [02:17<02:11,  3.54it/s] 43%|████▎     | 356/820 [02:18<02:10,  3.55it/s] 44%|████▎     | 357/820 [02:18<02:10,  3.55it/s] 44%|████▎     | 358/820 [02:18<02:09,  3.56it/s] 44%|████▍     | 359/820 [02:18<02:09,  3.56it/s] 44%|████▍     | 360/820 [02:19<02:08,  3.57it/s] 44%|████▍     | 361/820 [02:19<02:08,  3.57it/s] 44%|████▍     | 362/820 [02:19<02:08,  3.57it/s] 44%|████▍     | 363/820 [02:20<02:11,  3.48it/s] 44%|████▍     | 364/820 [02:20<02:09,  3.51it/s] 45%|████▍     | 365/820 [02:20<02:08,  3.54it/s] 45%|████▍     | 366/820 [02:20<02:07,  3.55it/s] 45%|████▍     | 367/820 [02:21<02:07,  3.56it/s] 45%|████▍     | 368/820 [02:21<02:06,  3.56it/s] 45%|████▌     | 369/820 [02:21<02:06,  3.57it/s] 45%|████▌     | 370/820 [02:22<02:06,  3.57it/s] 45%|████▌     | 371/820 [02:22<02:05,  3.57it/s] 45%|████▌     | 372/820 [02:22<02:05,  3.57it/s] 45%|████▌     | 373/820 [02:22<02:05,  3.57it/s] 46%|████▌     | 374/820 [02:23<02:07,  3.51it/s] 46%|████▌     | 375/820 [02:23<02:06,  3.53it/s] 46%|████▌     | 376/820 [02:23<02:05,  3.54it/s] 46%|████▌     | 377/820 [02:24<02:04,  3.55it/s] 46%|████▌     | 378/820 [02:24<02:04,  3.56it/s] 46%|████▌     | 379/820 [02:24<02:03,  3.57it/s] 46%|████▋     | 380/820 [02:24<02:03,  3.57it/s] 46%|████▋     | 381/820 [02:25<02:02,  3.57it/s] 47%|████▋     | 382/820 [02:25<02:02,  3.57it/s] 47%|████▋     | 383/820 [02:25<02:02,  3.57it/s] 47%|████▋     | 384/820 [02:25<02:02,  3.57it/s] 47%|████▋     | 385/820 [02:26<02:05,  3.47it/s] 47%|████▋     | 386/820 [02:26<02:03,  3.51it/s] 47%|████▋     | 387/820 [02:26<02:02,  3.53it/s] 47%|████▋     | 388/820 [02:27<02:01,  3.54it/s] 47%|████▋     | 389/820 [02:27<02:01,  3.55it/s] 48%|████▊     | 390/820 [02:27<02:00,  3.56it/s] 48%|████▊     | 391/820 [02:27<02:00,  3.56it/s] 48%|████▊     | 392/820 [02:28<02:00,  3.56it/s] 48%|████▊     | 393/820 [02:28<01:59,  3.57it/s] 48%|████▊     | 394/820 [02:28<01:59,  3.57it/s] 48%|████▊     | 395/820 [02:29<01:58,  3.57it/s] 48%|████▊     | 396/820 [02:29<02:01,  3.49it/s] 48%|████▊     | 397/820 [02:29<02:00,  3.52it/s] 49%|████▊     | 398/820 [02:29<01:59,  3.53it/s] 49%|████▊     | 399/820 [02:30<01:58,  3.55it/s] 49%|████▉     | 400/820 [02:30<01:58,  3.56it/s] 49%|████▉     | 401/820 [02:30<01:57,  3.56it/s] 49%|████▉     | 402/820 [02:31<01:57,  3.56it/s] 49%|████▉     | 403/820 [02:31<01:56,  3.57it/s] 49%|████▉     | 404/820 [02:31<01:56,  3.57it/s] 49%|████▉     | 405/820 [02:31<01:56,  3.57it/s] 50%|████▉     | 406/820 [02:32<01:55,  3.57it/s] 50%|████▉     | 407/820 [02:32<01:58,  3.48it/s] 50%|████▉     | 408/820 [02:32<01:57,  3.51it/s] 50%|████▉     | 409/820 [02:33<01:56,  3.53it/s] 50%|█████     | 410/820 [02:33<01:55,  3.54it/s] 50%|█████     | 411/820 [02:33<01:54,  3.56it/s] 50%|█████     | 412/820 [02:33<01:54,  3.56it/s] 50%|█████     | 413/820 [02:34<01:54,  3.57it/s] 50%|█████     | 414/820 [02:34<01:53,  3.57it/s] 51%|█████     | 415/820 [02:34<01:53,  3.57it/s] 51%|█████     | 416/820 [02:34<01:53,  3.57it/s] 51%|█████     | 417/820 [02:35<01:52,  3.57it/s] 51%|█████     | 418/820 [02:35<01:57,  3.42it/s] 51%|█████     | 419/820 [02:35<01:55,  3.47it/s] 51%|█████     | 420/820 [02:36<01:54,  3.50it/s] 51%|█████▏    | 421/820 [02:36<01:53,  3.52it/s] 51%|█████▏    | 422/820 [02:36<01:52,  3.54it/s] 52%|█████▏    | 423/820 [02:36<01:51,  3.55it/s] 52%|█████▏    | 424/820 [02:37<01:51,  3.56it/s] 52%|█████▏    | 425/820 [02:37<01:55,  3.43it/s] 52%|█████▏    | 426/820 [02:37<01:53,  3.48it/s] 52%|█████▏    | 427/820 [02:38<01:52,  3.51it/s] 52%|█████▏    | 428/820 [02:38<01:51,  3.53it/s] 52%|█████▏    | 429/820 [02:38<01:50,  3.54it/s] 52%|█████▏    | 430/820 [02:38<01:49,  3.56it/s] 53%|█████▎    | 431/820 [02:39<01:49,  3.56it/s] 53%|█████▎    | 432/820 [02:39<01:48,  3.57it/s] 53%|█████▎    | 433/820 [02:39<01:48,  3.57it/s] 53%|█████▎    | 434/820 [02:40<01:47,  3.58it/s] 53%|█████▎    | 435/820 [02:40<01:47,  3.58it/s] 53%|█████▎    | 436/820 [02:40<01:50,  3.49it/s] 53%|█████▎    | 437/820 [02:40<01:48,  3.51it/s] 53%|█████▎    | 438/820 [02:41<01:47,  3.54it/s] 54%|█████▎    | 439/820 [02:41<01:47,  3.55it/s] 54%|█████▎    | 440/820 [02:41<01:50,  3.44it/s] 54%|█████▍    | 441/820 [02:42<01:49,  3.47it/s] 54%|█████▍    | 442/820 [02:42<01:47,  3.50it/s] 54%|█████▍    | 443/820 [02:42<01:47,  3.52it/s] 54%|█████▍    | 444/820 [02:42<01:46,  3.54it/s] 54%|█████▍    | 445/820 [02:43<01:45,  3.55it/s] 54%|█████▍    | 446/820 [02:43<01:45,  3.56it/s] 55%|█████▍    | 447/820 [02:43<01:46,  3.49it/s] 55%|█████▍    | 448/820 [02:44<01:45,  3.51it/s] 55%|█████▍    | 449/820 [02:44<02:04,  2.99it/s] 55%|█████▍    | 450/820 [02:44<02:03,  3.00it/s] 55%|█████▌    | 451/820 [02:45<01:57,  3.14it/s] 55%|█████▌    | 452/820 [02:45<01:52,  3.26it/s] 55%|█████▌    | 453/820 [02:45<01:49,  3.35it/s] 55%|█████▌    | 454/820 [02:45<01:47,  3.42it/s] 55%|█████▌    | 455/820 [02:46<01:45,  3.47it/s] 56%|█████▌    | 456/820 [02:46<01:44,  3.50it/s] 56%|█████▌    | 457/820 [02:46<01:45,  3.45it/s] 56%|█████▌    | 458/820 [02:47<01:43,  3.49it/s] 56%|█████▌    | 459/820 [02:47<01:42,  3.51it/s] 56%|█████▌    | 460/820 [02:47<01:41,  3.53it/s] 56%|█████▌    | 461/820 [02:47<01:41,  3.54it/s] 56%|█████▋    | 462/820 [02:48<01:40,  3.56it/s] 56%|█████▋    | 463/820 [02:48<01:43,  3.45it/s] 57%|█████▋    | 464/820 [02:48<01:42,  3.47it/s] 57%|█████▋    | 465/820 [02:49<01:41,  3.50it/s] 57%|█████▋    | 466/820 [02:49<01:40,  3.52it/s] 57%|█████▋    | 467/820 [02:49<01:39,  3.54it/s] 57%|█████▋    | 468/820 [02:49<01:42,  3.45it/s] 57%|█████▋    | 469/820 [02:50<01:40,  3.49it/s] 57%|█████▋    | 470/820 [02:50<01:39,  3.51it/s] 57%|█████▋    | 471/820 [02:50<01:38,  3.53it/s] 58%|█████▊    | 472/820 [02:51<01:38,  3.54it/s] 58%|█████▊    | 473/820 [02:51<01:37,  3.55it/s] 58%|█████▊    | 474/820 [02:51<01:37,  3.56it/s] 58%|█████▊    | 475/820 [02:51<01:36,  3.56it/s] 58%|█████▊    | 476/820 [02:52<01:36,  3.57it/s] 58%|█████▊    | 477/820 [02:52<01:36,  3.57it/s] 58%|█████▊    | 478/820 [02:52<01:35,  3.58it/s] 58%|█████▊    | 479/820 [02:53<01:37,  3.50it/s] 59%|█████▊    | 480/820 [02:53<01:36,  3.54it/s] 59%|█████▊    | 481/820 [02:53<01:34,  3.57it/s] 59%|█████▉    | 482/820 [02:53<01:34,  3.59it/s] 59%|█████▉    | 483/820 [02:54<01:33,  3.60it/s] 59%|█████▉    | 484/820 [02:54<01:33,  3.61it/s] 59%|█████▉    | 485/820 [02:54<01:32,  3.62it/s] 59%|█████▉    | 486/820 [02:55<01:32,  3.63it/s] 59%|█████▉    | 487/820 [02:55<01:31,  3.62it/s] 60%|█████▉    | 488/820 [02:55<01:31,  3.62it/s] 60%|█████▉    | 489/820 [02:55<01:31,  3.63it/s] 60%|█████▉    | 490/820 [02:56<01:34,  3.50it/s] 60%|█████▉    | 491/820 [02:56<01:33,  3.54it/s] 60%|██████    | 492/820 [02:56<01:32,  3.56it/s][INFO|trainer.py:2140] 2023-08-28 06:41:55,547 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:41:55,547 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 06:41:55,547 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.6303, 'eval_samples_per_second': 356.852, 'eval_steps_per_second': 44.606, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.90it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.33it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.59it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.47it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.76it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.21it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.04it/s][A
  7%|▋         | 42/608 [00:00<00:12, 44.97it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.10it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.18it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.30it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.42it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.32it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.15it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 44.95it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.76it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.83it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.93it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.06it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 42.23it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 43.17it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 43.69it/s][A
 19%|█▉        | 117/608 [00:02<00:11, 44.28it/s][A
 20%|██        | 122/608 [00:02<00:10, 44.44it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.50it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.61it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.77it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.65it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.82it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 44.93it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.13it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.09it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.08it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.01it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.96it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.85it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.88it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.00it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.07it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.23it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.18it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.16it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.01it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.97it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.93it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 44.90it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 42.64it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 43.49it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.06it/s][A
 41%|████▏     | 252/608 [00:05<00:08, 44.47it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.63it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.68it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.79it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.76it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.59it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.66it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.88it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 45.08it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.23it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.13it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.15it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.11it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.91it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.77it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.80it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.98it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 45.08it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.17it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.26it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.21it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.07it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.76it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.79it/s][A
 61%|██████    | 372/608 [00:08<00:05, 42.30it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 43.24it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 43.93it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.37it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.76it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 44.81it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.84it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.72it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.48it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.55it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 44.61it/s][A
 70%|███████   | 427/608 [00:09<00:04, 44.95it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.07it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.27it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.25it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.20it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 45.01it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.75it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.72it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 44.77it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 44.98it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.09it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.24it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.18it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.17it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.91it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 44.82it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 36.30it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 38.70it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 40.53it/s][A
 86%|████████▌ | 522/608 [00:11<00:02, 41.94it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 42.96it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 43.72it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 44.25it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.57it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.24it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.11it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 44.22it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 44.52it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 44.83it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.01it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.21it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 45.28it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.20it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.82it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.69it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.60it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 44.75it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.75it/s][A 60%|██████    | 492/820 [03:10<01:32,  3.56it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:42:09,349 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 06:42:09,447 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:42:12,077 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:42:12,177 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:42:12,230 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492/special_tokens_map.json
 60%|██████    | 493/820 [03:14<30:30,  5.60s/it] 60%|██████    | 494/820 [03:14<21:45,  4.00s/it] 60%|██████    | 495/820 [03:15<15:37,  2.89s/it] 60%|██████    | 496/820 [03:15<11:21,  2.10s/it] 61%|██████    | 497/820 [03:15<08:23,  1.56s/it] 61%|██████    | 498/820 [03:16<06:17,  1.17s/it] 61%|██████    | 499/820 [03:16<04:50,  1.10it/s] 61%|██████    | 500/820 [03:16<03:51,  1.38it/s]                                                  61%|██████    | 500/820 [03:16<03:51,  1.38it/s] 61%|██████    | 501/820 [03:16<03:08,  1.69it/s] 61%|██████    | 502/820 [03:17<02:38,  2.01it/s] 61%|██████▏   | 503/820 [03:17<02:16,  2.32it/s] 61%|██████▏   | 504/820 [03:17<02:02,  2.59it/s] 62%|██████▏   | 505/820 [03:18<01:51,  2.82it/s] 62%|██████▏   | 506/820 [03:18<01:44,  3.01it/s] 62%|██████▏   | 507/820 [03:18<01:38,  3.16it/s] 62%|██████▏   | 508/820 [03:18<01:35,  3.28it/s] 62%|██████▏   | 509/820 [03:19<01:32,  3.36it/s] 62%|██████▏   | 510/820 [03:19<01:30,  3.42it/s] 62%|██████▏   | 511/820 [03:19<01:30,  3.40it/s] 62%|██████▏   | 512/820 [03:20<01:29,  3.45it/s] 63%|██████▎   | 513/820 [03:20<01:27,  3.49it/s] 63%|██████▎   | 514/820 [03:20<01:27,  3.51it/s] 63%|██████▎   | 515/820 [03:20<01:26,  3.53it/s] 63%|██████▎   | 516/820 [03:21<01:25,  3.54it/s] 63%|██████▎   | 517/820 [03:21<01:25,  3.55it/s] 63%|██████▎   | 518/820 [03:21<01:24,  3.56it/s] 63%|██████▎   | 519/820 [03:22<01:24,  3.56it/s] 63%|██████▎   | 520/820 [03:22<01:24,  3.56it/s] 64%|██████▎   | 521/820 [03:22<01:23,  3.57it/s] 64%|██████▎   | 522/820 [03:22<01:25,  3.48it/s] 64%|██████▍   | 523/820 [03:23<01:24,  3.51it/s] 64%|██████▍   | 524/820 [03:23<01:23,  3.53it/s] 64%|██████▍   | 525/820 [03:23<01:23,  3.54it/s] 64%|██████▍   | 526/820 [03:23<01:22,  3.55it/s] 64%|██████▍   | 527/820 [03:24<01:22,  3.56it/s] 64%|██████▍   | 528/820 [03:24<01:21,  3.56it/s] 65%|██████▍   | 529/820 [03:24<01:21,  3.57it/s] 65%|██████▍   | 530/820 [03:25<01:21,  3.57it/s] 65%|██████▍   | 531/820 [03:25<01:20,  3.57it/s] 65%|██████▍   | 532/820 [03:25<01:20,  3.57it/s] 65%|██████▌   | 533/820 [03:25<01:23,  3.45it/s] 65%|██████▌   | 534/820 [03:26<01:22,  3.48it/s] 65%|██████▌   | 535/820 [03:26<01:21,  3.51it/s] 65%|██████▌   | 536/820 [03:26<01:20,  3.53it/s] 65%|██████▌   | 537/820 [03:27<01:19,  3.54it/s] 66%|██████▌   | 538/820 [03:27<01:19,  3.55it/s] 66%|██████▌   | 539/820 [03:27<01:18,  3.56it/s] 66%|██████▌   | 540/820 [03:27<01:18,  3.56it/s] 66%|██████▌   | 541/820 [03:28<01:18,  3.57it/s] 66%|██████▌   | 542/820 [03:28<01:17,  3.57it/s] 66%|██████▌   | 543/820 [03:28<01:17,  3.58it/s] 66%|██████▋   | 544/820 [03:29<01:18,  3.53it/s] 66%|██████▋   | 545/820 [03:29<01:17,  3.54it/s] 67%|██████▋   | 546/820 [03:29<01:17,  3.55it/s] 67%|██████▋   | 547/820 [03:29<01:16,  3.56it/s] 67%|██████▋   | 548/820 [03:30<01:16,  3.57it/s] 67%|██████▋   | 549/820 [03:30<01:15,  3.57it/s] 67%|██████▋   | 550/820 [03:30<01:15,  3.57it/s] 67%|██████▋   | 551/820 [03:31<01:15,  3.57it/s] 67%|██████▋   | 552/820 [03:31<01:15,  3.57it/s] 67%|██████▋   | 553/820 [03:31<01:14,  3.58it/s] 68%|██████▊   | 554/820 [03:31<01:14,  3.58it/s] 68%|██████▊   | 555/820 [03:32<01:16,  3.47it/s] 68%|██████▊   | 556/820 [03:32<01:15,  3.50it/s] 68%|██████▊   | 557/820 [03:32<01:14,  3.53it/s] 68%|██████▊   | 558/820 [03:33<01:13,  3.54it/s] 68%|██████▊   | 559/820 [03:33<01:13,  3.55it/s] 68%|██████▊   | 560/820 [03:33<01:12,  3.56it/s] 68%|██████▊   | 561/820 [03:33<01:12,  3.57it/s] 69%|██████▊   | 562/820 [03:34<01:12,  3.57it/s] 69%|██████▊   | 563/820 [03:34<01:11,  3.58it/s] 69%|██████▉   | 564/820 [03:34<01:11,  3.57it/s] 69%|██████▉   | 565/820 [03:34<01:11,  3.57it/s] 69%|██████▉   | 566/820 [03:35<01:12,  3.49it/s] 69%|██████▉   | 567/820 [03:35<01:11,  3.52it/s] 69%|██████▉   | 568/820 [03:35<01:11,  3.53it/s] 69%|██████▉   | 569/820 [03:36<01:10,  3.55it/s] 70%|██████▉   | 570/820 [03:36<01:10,  3.56it/s] 70%|██████▉   | 571/820 [03:36<01:09,  3.56it/s] 70%|██████▉   | 572/820 [03:36<01:09,  3.57it/s] 70%|██████▉   | 573/820 [03:37<01:09,  3.57it/s] 70%|███████   | 574/820 [03:37<01:08,  3.57it/s] 70%|███████   | 575/820 [03:37<01:08,  3.57it/s] 70%|███████   | 576/820 [03:38<01:08,  3.57it/s] 70%|███████   | 577/820 [03:38<01:11,  3.42it/s] 70%|███████   | 578/820 [03:38<01:09,  3.46it/s] 71%|███████   | 579/820 [03:38<01:08,  3.50it/s] 71%|███████   | 580/820 [03:39<01:08,  3.52it/s] 71%|███████   | 581/820 [03:39<01:07,  3.54it/s] 71%|███████   | 582/820 [03:39<01:07,  3.55it/s] 71%|███████   | 583/820 [03:40<01:06,  3.56it/s] 71%|███████   | 584/820 [03:40<01:06,  3.56it/s] 71%|███████▏  | 585/820 [03:40<01:05,  3.56it/s] 71%|███████▏  | 586/820 [03:40<01:05,  3.57it/s] 72%|███████▏  | 587/820 [03:41<01:05,  3.57it/s] 72%|███████▏  | 588/820 [03:41<01:05,  3.57it/s] 72%|███████▏  | 589/820 [03:41<01:04,  3.57it/s] 72%|███████▏  | 590/820 [03:42<01:05,  3.51it/s] 72%|███████▏  | 591/820 [03:42<01:04,  3.53it/s] 72%|███████▏  | 592/820 [03:42<01:04,  3.54it/s] 72%|███████▏  | 593/820 [03:42<01:03,  3.55it/s] 72%|███████▏  | 594/820 [03:43<01:03,  3.56it/s] 73%|███████▎  | 595/820 [03:43<01:03,  3.56it/s] 73%|███████▎  | 596/820 [03:43<01:02,  3.56it/s] 73%|███████▎  | 597/820 [03:44<01:02,  3.56it/s] 73%|███████▎  | 598/820 [03:44<01:02,  3.57it/s] 73%|███████▎  | 599/820 [03:44<01:26,  2.56it/s] 73%|███████▎  | 600/820 [03:45<01:21,  2.69it/s] 73%|███████▎  | 601/820 [03:45<01:15,  2.91it/s] 73%|███████▎  | 602/820 [03:45<01:10,  3.08it/s] 74%|███████▎  | 603/820 [03:46<01:07,  3.21it/s] 74%|███████▎  | 604/820 [03:46<01:05,  3.31it/s] 74%|███████▍  | 605/820 [03:46<01:03,  3.38it/s] 74%|███████▍  | 606/820 [03:46<01:02,  3.43it/s] 74%|███████▍  | 607/820 [03:47<01:01,  3.47it/s] 74%|███████▍  | 608/820 [03:47<01:00,  3.50it/s] 74%|███████▍  | 609/820 [03:47<01:00,  3.51it/s] 74%|███████▍  | 610/820 [03:48<00:59,  3.53it/s] 75%|███████▍  | 611/820 [03:48<01:00,  3.46it/s] 75%|███████▍  | 612/820 [03:48<00:59,  3.49it/s] 75%|███████▍  | 613/820 [03:48<00:58,  3.52it/s] 75%|███████▍  | 614/820 [03:49<00:58,  3.53it/s] 75%|███████▌  | 615/820 [03:49<00:57,  3.54it/s] 75%|███████▌  | 616/820 [03:49<00:57,  3.55it/s] 75%|███████▌  | 617/820 [03:50<00:57,  3.56it/s] 75%|███████▌  | 618/820 [03:50<00:56,  3.56it/s] 75%|███████▌  | 619/820 [03:50<00:56,  3.56it/s] 76%|███████▌  | 620/820 [03:50<00:56,  3.57it/s] 76%|███████▌  | 621/820 [03:51<00:55,  3.57it/s] 76%|███████▌  | 622/820 [03:51<00:56,  3.48it/s] 76%|███████▌  | 623/820 [03:51<00:56,  3.50it/s] 76%|███████▌  | 624/820 [03:52<00:55,  3.52it/s] 76%|███████▌  | 625/820 [03:52<00:55,  3.54it/s] 76%|███████▋  | 626/820 [03:52<00:54,  3.55it/s] 76%|███████▋  | 627/820 [03:52<00:54,  3.55it/s] 77%|███████▋  | 628/820 [03:53<00:53,  3.56it/s] 77%|███████▋  | 629/820 [03:53<00:53,  3.56it/s] 77%|███████▋  | 630/820 [03:53<00:53,  3.56it/s] 77%|███████▋  | 631/820 [03:53<00:53,  3.56it/s] 77%|███████▋  | 632/820 [03:54<00:52,  3.56it/s] 77%|███████▋  | 633/820 [03:54<00:53,  3.50it/s] 77%|███████▋  | 634/820 [03:54<00:52,  3.52it/s] 77%|███████▋  | 635/820 [03:55<00:52,  3.54it/s] 78%|███████▊  | 636/820 [03:55<00:51,  3.55it/s] 78%|███████▊  | 637/820 [03:55<00:51,  3.55it/s] 78%|███████▊  | 638/820 [03:55<00:51,  3.55it/s] 78%|███████▊  | 639/820 [03:56<00:50,  3.55it/s] 78%|███████▊  | 640/820 [03:56<00:50,  3.55it/s] 78%|███████▊  | 641/820 [03:56<00:50,  3.55it/s] 78%|███████▊  | 642/820 [03:57<00:50,  3.56it/s] 78%|███████▊  | 643/820 [03:57<00:49,  3.56it/s] 79%|███████▊  | 644/820 [03:57<00:50,  3.48it/s] 79%|███████▊  | 645/820 [03:57<00:49,  3.51it/s] 79%|███████▉  | 646/820 [03:58<00:49,  3.52it/s] 79%|███████▉  | 647/820 [03:58<00:48,  3.54it/s] 79%|███████▉  | 648/820 [03:58<00:48,  3.54it/s] 79%|███████▉  | 649/820 [03:59<00:48,  3.55it/s] 79%|███████▉  | 650/820 [03:59<00:47,  3.56it/s] 79%|███████▉  | 651/820 [03:59<00:47,  3.56it/s] 80%|███████▉  | 652/820 [03:59<00:47,  3.56it/s] 80%|███████▉  | 653/820 [04:00<00:46,  3.57it/s] 80%|███████▉  | 654/820 [04:00<00:46,  3.57it/s] 80%|███████▉  | 655/820 [04:00<00:48,  3.42it/s] 80%|████████  | 656/820 [04:01<00:47,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 06:42:59,943 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:42:59,943 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 06:42:59,943 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.7134, 'eval_samples_per_second': 354.69, 'eval_steps_per_second': 44.336, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.222560975609756e-05, 'epoch': 3.05}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.22it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.09it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.62it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.84it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.28it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.68it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.25it/s][A
  7%|▋         | 42/608 [00:00<00:12, 44.74it/s][A
  8%|▊         | 47/608 [00:01<00:12, 44.84it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.18it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.20it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.24it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.33it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.31it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.07it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.69it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.78it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.84it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.09it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.15it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.22it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 41.92it/s][A
 19%|█▉        | 117/608 [00:02<00:11, 42.97it/s][A
 20%|██        | 122/608 [00:02<00:11, 43.51it/s][A
 21%|██        | 127/608 [00:02<00:10, 43.84it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.13it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.41it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.73it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.91it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 44.72it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 44.88it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.00it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.07it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 44.96it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.93it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.94it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.02it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.08it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.04it/s][A
 33%|███▎      | 202/608 [00:04<00:09, 45.09it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.19it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.10it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.97it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.98it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.03it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.11it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.03it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.07it/s][A
 41%|████      | 247/608 [00:05<00:08, 41.46it/s][A
 41%|████▏     | 252/608 [00:05<00:08, 42.63it/s][A
 42%|████▏     | 257/608 [00:05<00:08, 43.38it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 43.99it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.35it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.45it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.66it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.74it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.58it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 44.61it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 44.82it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.00it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.14it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.20it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 45.20it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.14it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.96it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.85it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 44.76it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 44.85it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 44.98it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.24it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.31it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 45.36it/s][A
 60%|██████    | 367/608 [00:08<00:05, 45.16it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.91it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.81it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.81it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.91it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.95it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.14it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 45.24it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 45.30it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.05it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.73it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 44.72it/s][A
 70%|███████   | 427/608 [00:09<00:04, 44.81it/s][A
 71%|███████   | 432/608 [00:09<00:03, 44.91it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 44.89it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.10it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.27it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 45.24it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 45.17it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.94it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 44.83it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 44.89it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.02it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.05it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.20it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.29it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 45.29it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.12it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.06it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.13it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.36it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.64it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.72it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 44.99it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 44.94it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 45.08it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.00it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.65it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 44.81it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 44.82it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 44.96it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.08it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.23it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.06it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.09it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.03it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.95it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.99it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 44.96it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.96it/s][A 80%|████████  | 656/820 [04:14<00:47,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:43:13,714 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656
[INFO|configuration_utils.py:351] 2023-08-28 06:43:13,835 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:43:16,087 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:43:16,196 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:43:16,250 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656/special_tokens_map.json
 80%|████████  | 657/820 [04:18<14:35,  5.37s/it] 80%|████████  | 658/820 [04:18<10:22,  3.84s/it] 80%|████████  | 659/820 [04:18<07:26,  2.77s/it] 80%|████████  | 660/820 [04:19<05:24,  2.03s/it] 81%|████████  | 661/820 [04:19<03:58,  1.50s/it] 81%|████████  | 662/820 [04:19<02:59,  1.13s/it] 81%|████████  | 663/820 [04:19<02:17,  1.14it/s] 81%|████████  | 664/820 [04:20<01:48,  1.43it/s] 81%|████████  | 665/820 [04:20<01:28,  1.75it/s] 81%|████████  | 666/820 [04:20<01:15,  2.04it/s] 81%|████████▏ | 667/820 [04:21<01:05,  2.34it/s] 81%|████████▏ | 668/820 [04:21<00:58,  2.61it/s] 82%|████████▏ | 669/820 [04:21<00:53,  2.85it/s] 82%|████████▏ | 670/820 [04:21<00:49,  3.03it/s] 82%|████████▏ | 671/820 [04:22<00:46,  3.18it/s] 82%|████████▏ | 672/820 [04:22<00:45,  3.29it/s] 82%|████████▏ | 673/820 [04:22<00:43,  3.37it/s] 82%|████████▏ | 674/820 [04:23<00:42,  3.43it/s] 82%|████████▏ | 675/820 [04:23<00:41,  3.47it/s] 82%|████████▏ | 676/820 [04:23<00:41,  3.50it/s] 83%|████████▎ | 677/820 [04:23<00:40,  3.50it/s] 83%|████████▎ | 678/820 [04:24<00:40,  3.52it/s] 83%|████████▎ | 679/820 [04:24<00:39,  3.54it/s] 83%|████████▎ | 680/820 [04:24<00:39,  3.56it/s] 83%|████████▎ | 681/820 [04:25<00:39,  3.56it/s] 83%|████████▎ | 682/820 [04:25<00:38,  3.57it/s] 83%|████████▎ | 683/820 [04:25<00:38,  3.57it/s] 83%|████████▎ | 684/820 [04:25<00:38,  3.57it/s] 84%|████████▎ | 685/820 [04:26<00:37,  3.57it/s] 84%|████████▎ | 686/820 [04:26<00:37,  3.57it/s] 84%|████████▍ | 687/820 [04:26<00:37,  3.58it/s] 84%|████████▍ | 688/820 [04:27<00:37,  3.55it/s] 84%|████████▍ | 689/820 [04:27<00:36,  3.56it/s] 84%|████████▍ | 690/820 [04:27<00:36,  3.57it/s] 84%|████████▍ | 691/820 [04:27<00:36,  3.57it/s] 84%|████████▍ | 692/820 [04:28<00:35,  3.58it/s] 85%|████████▍ | 693/820 [04:28<00:35,  3.58it/s] 85%|████████▍ | 694/820 [04:28<00:35,  3.58it/s] 85%|████████▍ | 695/820 [04:28<00:34,  3.58it/s] 85%|████████▍ | 696/820 [04:29<00:34,  3.58it/s] 85%|████████▌ | 697/820 [04:29<00:34,  3.58it/s] 85%|████████▌ | 698/820 [04:29<00:34,  3.58it/s] 85%|████████▌ | 699/820 [04:30<00:34,  3.55it/s] 85%|████████▌ | 700/820 [04:30<00:33,  3.56it/s] 85%|████████▌ | 701/820 [04:30<00:33,  3.57it/s] 86%|████████▌ | 702/820 [04:30<00:33,  3.57it/s] 86%|████████▌ | 703/820 [04:31<00:32,  3.58it/s] 86%|████████▌ | 704/820 [04:31<00:32,  3.58it/s] 86%|████████▌ | 705/820 [04:31<00:32,  3.58it/s] 86%|████████▌ | 706/820 [04:32<00:31,  3.58it/s] 86%|████████▌ | 707/820 [04:32<00:31,  3.58it/s] 86%|████████▋ | 708/820 [04:32<00:31,  3.58it/s] 86%|████████▋ | 709/820 [04:32<00:31,  3.58it/s] 87%|████████▋ | 710/820 [04:33<00:30,  3.57it/s] 87%|████████▋ | 711/820 [04:33<00:30,  3.57it/s] 87%|████████▋ | 712/820 [04:33<00:30,  3.59it/s] 87%|████████▋ | 713/820 [04:33<00:29,  3.60it/s] 87%|████████▋ | 714/820 [04:34<00:29,  3.62it/s] 87%|████████▋ | 715/820 [04:34<00:28,  3.62it/s] 87%|████████▋ | 716/820 [04:34<00:28,  3.62it/s] 87%|████████▋ | 717/820 [04:35<00:28,  3.63it/s] 88%|████████▊ | 718/820 [04:35<00:28,  3.63it/s] 88%|████████▊ | 719/820 [04:35<00:27,  3.63it/s] 88%|████████▊ | 720/820 [04:35<00:27,  3.64it/s] 88%|████████▊ | 721/820 [04:36<00:27,  3.61it/s] 88%|████████▊ | 722/820 [04:36<00:27,  3.62it/s] 88%|████████▊ | 723/820 [04:36<00:26,  3.62it/s] 88%|████████▊ | 724/820 [04:37<00:26,  3.63it/s] 88%|████████▊ | 725/820 [04:37<00:26,  3.63it/s] 89%|████████▊ | 726/820 [04:37<00:25,  3.63it/s] 89%|████████▊ | 727/820 [04:37<00:25,  3.64it/s] 89%|████████▉ | 728/820 [04:38<00:25,  3.64it/s] 89%|████████▉ | 729/820 [04:38<00:25,  3.64it/s] 89%|████████▉ | 730/820 [04:38<00:24,  3.64it/s] 89%|████████▉ | 731/820 [04:38<00:24,  3.64it/s] 89%|████████▉ | 732/820 [04:39<00:24,  3.63it/s] 89%|████████▉ | 733/820 [04:39<00:23,  3.63it/s] 90%|████████▉ | 734/820 [04:39<00:23,  3.63it/s] 90%|████████▉ | 735/820 [04:40<00:23,  3.63it/s] 90%|████████▉ | 736/820 [04:40<00:23,  3.63it/s] 90%|████████▉ | 737/820 [04:40<00:22,  3.63it/s] 90%|█████████ | 738/820 [04:40<00:22,  3.63it/s] 90%|█████████ | 739/820 [04:41<00:22,  3.63it/s] 90%|█████████ | 740/820 [04:41<00:22,  3.63it/s] 90%|█████████ | 741/820 [04:41<00:21,  3.63it/s] 90%|█████████ | 742/820 [04:41<00:21,  3.64it/s] 91%|█████████ | 743/820 [04:42<00:21,  3.63it/s] 91%|█████████ | 744/820 [04:42<00:20,  3.63it/s] 91%|█████████ | 745/820 [04:42<00:20,  3.64it/s] 91%|█████████ | 746/820 [04:43<00:20,  3.64it/s] 91%|█████████ | 747/820 [04:43<00:20,  3.63it/s] 91%|█████████ | 748/820 [04:43<00:19,  3.62it/s] 91%|█████████▏| 749/820 [04:43<00:19,  3.63it/s] 91%|█████████▏| 750/820 [04:44<00:19,  3.63it/s] 92%|█████████▏| 751/820 [04:44<00:18,  3.63it/s] 92%|█████████▏| 752/820 [04:44<00:18,  3.58it/s] 92%|█████████▏| 753/820 [04:45<00:18,  3.54it/s] 92%|█████████▏| 754/820 [04:45<00:18,  3.56it/s] 92%|█████████▏| 755/820 [04:45<00:18,  3.58it/s] 92%|█████████▏| 756/820 [04:45<00:17,  3.60it/s] 92%|█████████▏| 757/820 [04:46<00:17,  3.61it/s] 92%|█████████▏| 758/820 [04:46<00:17,  3.62it/s] 93%|█████████▎| 759/820 [04:46<00:17,  3.58it/s] 93%|█████████▎| 760/820 [04:46<00:16,  3.60it/s] 93%|█████████▎| 761/820 [04:47<00:16,  3.61it/s] 93%|█████████▎| 762/820 [04:47<00:16,  3.62it/s] 93%|█████████▎| 763/820 [04:47<00:15,  3.63it/s] 93%|█████████▎| 764/820 [04:48<00:15,  3.63it/s] 93%|█████████▎| 765/820 [04:48<00:15,  3.63it/s] 93%|█████████▎| 766/820 [04:48<00:14,  3.64it/s] 94%|█████████▎| 767/820 [04:48<00:14,  3.63it/s] 94%|█████████▎| 768/820 [04:49<00:14,  3.63it/s] 94%|█████████▍| 769/820 [04:49<00:14,  3.64it/s] 94%|█████████▍| 770/820 [04:49<00:13,  3.59it/s] 94%|█████████▍| 771/820 [04:49<00:13,  3.61it/s] 94%|█████████▍| 772/820 [04:50<00:13,  3.62it/s] 94%|█████████▍| 773/820 [04:50<00:12,  3.62it/s] 94%|█████████▍| 774/820 [04:50<00:12,  3.63it/s] 95%|█████████▍| 775/820 [04:51<00:12,  3.63it/s] 95%|█████████▍| 776/820 [04:51<00:12,  3.63it/s] 95%|█████████▍| 777/820 [04:51<00:11,  3.64it/s] 95%|█████████▍| 778/820 [04:51<00:11,  3.63it/s] 95%|█████████▌| 779/820 [04:52<00:11,  3.64it/s] 95%|█████████▌| 780/820 [04:52<00:10,  3.64it/s] 95%|█████████▌| 781/820 [04:52<00:10,  3.61it/s] 95%|█████████▌| 782/820 [04:53<00:10,  3.62it/s] 95%|█████████▌| 783/820 [04:53<00:10,  3.62it/s] 96%|█████████▌| 784/820 [04:53<00:09,  3.63it/s] 96%|█████████▌| 785/820 [04:53<00:09,  3.63it/s] 96%|█████████▌| 786/820 [04:54<00:09,  3.63it/s] 96%|█████████▌| 787/820 [04:54<00:09,  3.63it/s] 96%|█████████▌| 788/820 [04:54<00:08,  3.64it/s] 96%|█████████▌| 789/820 [04:54<00:08,  3.64it/s] 96%|█████████▋| 790/820 [04:55<00:08,  3.63it/s] 96%|█████████▋| 791/820 [04:55<00:07,  3.64it/s] 97%|█████████▋| 792/820 [04:55<00:07,  3.55it/s] 97%|█████████▋| 793/820 [04:56<00:07,  3.57it/s] 97%|█████████▋| 794/820 [04:56<00:07,  3.60it/s] 97%|█████████▋| 795/820 [04:56<00:06,  3.61it/s] 97%|█████████▋| 796/820 [04:56<00:06,  3.62it/s] 97%|█████████▋| 797/820 [04:57<00:06,  3.62it/s] 97%|█████████▋| 798/820 [04:57<00:06,  3.64it/s] 97%|█████████▋| 799/820 [04:57<00:05,  3.63it/s] 98%|█████████▊| 800/820 [04:57<00:05,  3.64it/s] 98%|█████████▊| 801/820 [04:58<00:05,  3.63it/s] 98%|█████████▊| 802/820 [04:58<00:04,  3.63it/s] 98%|█████████▊| 803/820 [04:58<00:04,  3.57it/s] 98%|█████████▊| 804/820 [04:59<00:04,  3.59it/s] 98%|█████████▊| 805/820 [04:59<00:04,  3.60it/s] 98%|█████████▊| 806/820 [04:59<00:03,  3.61it/s] 98%|█████████▊| 807/820 [04:59<00:03,  3.62it/s] 99%|█████████▊| 808/820 [05:00<00:03,  3.63it/s] 99%|█████████▊| 809/820 [05:00<00:03,  3.63it/s] 99%|█████████▉| 810/820 [05:00<00:02,  3.63it/s] 99%|█████████▉| 811/820 [05:01<00:02,  3.63it/s] 99%|█████████▉| 812/820 [05:01<00:02,  3.64it/s] 99%|█████████▉| 813/820 [05:01<00:01,  3.64it/s] 99%|█████████▉| 814/820 [05:01<00:01,  3.44it/s] 99%|█████████▉| 815/820 [05:02<00:01,  3.50it/s]100%|█████████▉| 816/820 [05:02<00:01,  3.54it/s]100%|█████████▉| 817/820 [05:02<00:00,  3.57it/s]100%|█████████▉| 818/820 [05:03<00:00,  3.59it/s]100%|█████████▉| 819/820 [05:03<00:00,  3.59it/s]100%|██████████| 820/820 [05:03<00:00,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 06:44:02,381 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:44:02,381 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 06:44:02,381 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.6209, 'eval_samples_per_second': 357.098, 'eval_steps_per_second': 44.637, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.77it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.58it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.73it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.70it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.24it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.80it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.40it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.27it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.29it/s][A
  9%|▊         | 53/608 [00:01<00:13, 42.22it/s][A
 10%|▉         | 58/608 [00:01<00:12, 43.24it/s][A
 10%|█         | 63/608 [00:01<00:12, 44.03it/s][A
 11%|█         | 68/608 [00:01<00:12, 44.51it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 44.82it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 44.82it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 44.85it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.90it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.68it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 44.79it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 44.99it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.12it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.30it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.47it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.36it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.30it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.16it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.99it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.04it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.13it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.14it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.30it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.43it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.40it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.28it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.13it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.08it/s][A
 31%|███       | 188/608 [00:04<00:09, 43.63it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.06it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 44.54it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 44.86it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.12it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.13it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.15it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.07it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 44.82it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 44.93it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.07it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.23it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.35it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.43it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.38it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.27it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.10it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 44.98it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.05it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.16it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.25it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.37it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.40it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.38it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.26it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 44.98it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.07it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.13it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.00it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 43.65it/s][A
 56%|█████▌    | 338/608 [00:07<00:06, 44.25it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 44.67it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 44.90it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 44.91it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 44.89it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 44.89it/s][A
 61%|██████    | 368/608 [00:08<00:05, 44.97it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 44.91it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.11it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.27it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.41it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.46it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.31it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.22it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.16it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.11it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.12it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.16it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.22it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.37it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.40it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.37it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.28it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.13it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.11it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.13it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.17it/s][A
 78%|███████▊  | 473/608 [00:10<00:03, 43.98it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 44.60it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 44.88it/s][A
 80%|████████  | 488/608 [00:10<00:02, 44.85it/s][A
 81%|████████  | 493/608 [00:10<00:02, 44.87it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 44.95it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 44.93it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.02it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.01it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.04it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.22it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.37it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.37it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.27it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.24it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.19it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.08it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.02it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.22it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.32it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.44it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.32it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.36it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.24it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.21it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.02it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.16it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.26it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.26it/s][A100%|██████████| 820/820 [05:17<00:00,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:44:16,114 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820
[INFO|configuration_utils.py:351] 2023-08-28 06:44:16,261 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:44:19,301 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:44:19,423 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:44:19,473 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:44:20,239 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:44:20,240 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164 (score: 0.9957473874092102).
                                                 100%|██████████| 820/820 [05:29<00:00,  3.59it/s]100%|██████████| 820/820 [05:29<00:00,  2.49it/s]
[INFO|trainer.py:1894] 2023-08-28 06:44:28,063 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 06:44:28,203 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:44:32,768 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:44:33,058 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:44:33,113 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:44:33,552 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:33,553 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:33,553 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:33,553 >>   train_runtime            = 0:05:29.21
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:33,553 >>   train_samples            =      10520
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:33,553 >>   train_samples_per_second =    159.776
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:33,553 >>   train_steps_per_second   =      2.491
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.5057, 'eval_samples_per_second': 360.144, 'eval_steps_per_second': 45.018, 'epoch': 5.0}
{'train_runtime': 329.2104, 'train_samples_per_second': 159.776, 'train_steps_per_second': 2.491, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:44:33 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:44:33,835 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:44:33,835 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 06:44:33,835 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 55.79it/s]  2%|▏         | 12/608 [00:00<00:11, 49.90it/s]  3%|▎         | 18/608 [00:00<00:12, 48.09it/s]  4%|▍         | 23/608 [00:00<00:12, 47.32it/s]  5%|▍         | 28/608 [00:00<00:12, 46.80it/s]  5%|▌         | 33/608 [00:00<00:12, 46.43it/s]  6%|▋         | 38/608 [00:00<00:12, 46.34it/s]  7%|▋         | 43/608 [00:00<00:12, 45.79it/s]  8%|▊         | 48/608 [00:01<00:12, 45.19it/s]  9%|▊         | 53/608 [00:01<00:12, 45.17it/s] 10%|▉         | 58/608 [00:01<00:12, 45.26it/s] 10%|█         | 63/608 [00:01<00:11, 45.44it/s] 11%|█         | 68/608 [00:01<00:11, 45.54it/s] 12%|█▏        | 73/608 [00:01<00:11, 45.66it/s] 13%|█▎        | 78/608 [00:01<00:11, 45.74it/s] 14%|█▎        | 83/608 [00:01<00:11, 45.73it/s] 14%|█▍        | 88/608 [00:01<00:11, 45.44it/s] 15%|█▌        | 93/608 [00:02<00:11, 45.11it/s] 16%|█▌        | 98/608 [00:02<00:11, 44.46it/s] 17%|█▋        | 103/608 [00:02<00:11, 44.86it/s] 18%|█▊        | 108/608 [00:02<00:11, 45.09it/s] 19%|█▊        | 113/608 [00:02<00:10, 45.28it/s] 19%|█▉        | 118/608 [00:02<00:10, 45.48it/s] 20%|██        | 123/608 [00:02<00:10, 45.61it/s] 21%|██        | 128/608 [00:02<00:10, 45.60it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.29it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.10it/s] 24%|██▎       | 143/608 [00:03<00:10, 45.04it/s] 24%|██▍       | 148/608 [00:03<00:10, 45.16it/s] 25%|██▌       | 153/608 [00:03<00:10, 45.29it/s] 26%|██▌       | 158/608 [00:03<00:09, 45.49it/s] 27%|██▋       | 163/608 [00:03<00:09, 45.62it/s] 28%|██▊       | 168/608 [00:03<00:09, 45.70it/s] 28%|██▊       | 173/608 [00:03<00:09, 45.63it/s] 29%|██▉       | 178/608 [00:03<00:09, 45.33it/s] 30%|███       | 183/608 [00:04<00:09, 45.13it/s] 31%|███       | 188/608 [00:04<00:09, 45.10it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.10it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.27it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.47it/s] 34%|███▍      | 208/608 [00:04<00:08, 45.36it/s] 35%|███▌      | 213/608 [00:04<00:08, 45.54it/s] 36%|███▌      | 218/608 [00:04<00:08, 45.46it/s] 37%|███▋      | 223/608 [00:04<00:08, 45.34it/s] 38%|███▊      | 228/608 [00:04<00:08, 45.19it/s] 38%|███▊      | 233/608 [00:05<00:08, 45.10it/s] 39%|███▉      | 238/608 [00:05<00:08, 44.88it/s] 40%|███▉      | 243/608 [00:05<00:08, 45.10it/s] 41%|████      | 248/608 [00:05<00:07, 45.34it/s] 42%|████▏     | 253/608 [00:05<00:07, 45.48it/s] 42%|████▏     | 258/608 [00:05<00:07, 45.56it/s] 43%|████▎     | 263/608 [00:05<00:07, 45.54it/s] 44%|████▍     | 268/608 [00:05<00:07, 45.27it/s] 45%|████▍     | 273/608 [00:05<00:07, 45.20it/s] 46%|████▌     | 278/608 [00:06<00:07, 45.08it/s] 47%|████▋     | 283/608 [00:06<00:07, 45.04it/s] 47%|████▋     | 288/608 [00:06<00:07, 45.08it/s] 48%|████▊     | 293/608 [00:06<00:06, 45.32it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.45it/s] 50%|████▉     | 303/608 [00:06<00:06, 44.55it/s] 51%|█████     | 308/608 [00:06<00:06, 44.96it/s] 51%|█████▏    | 313/608 [00:06<00:06, 45.08it/s] 52%|█████▏    | 318/608 [00:06<00:06, 45.02it/s] 53%|█████▎    | 323/608 [00:07<00:06, 44.99it/s] 54%|█████▍    | 328/608 [00:07<00:06, 43.61it/s] 55%|█████▍    | 334/608 [00:07<00:06, 45.46it/s] 56%|█████▌    | 339/608 [00:07<00:05, 45.55it/s] 57%|█████▋    | 344/608 [00:07<00:05, 45.39it/s] 57%|█████▋    | 349/608 [00:07<00:05, 45.58it/s] 58%|█████▊    | 354/608 [00:07<00:05, 45.61it/s] 59%|█████▉    | 359/608 [00:07<00:05, 45.52it/s] 60%|█████▉    | 364/608 [00:08<00:05, 45.35it/s] 61%|██████    | 369/608 [00:08<00:05, 45.22it/s] 62%|██████▏   | 374/608 [00:08<00:05, 45.15it/s] 62%|██████▏   | 379/608 [00:08<00:05, 45.21it/s] 63%|██████▎   | 384/608 [00:08<00:04, 45.29it/s] 64%|██████▍   | 389/608 [00:08<00:04, 45.29it/s] 65%|██████▍   | 394/608 [00:08<00:04, 45.43it/s] 66%|██████▌   | 399/608 [00:08<00:04, 45.55it/s] 66%|██████▋   | 404/608 [00:08<00:04, 45.47it/s] 67%|██████▋   | 409/608 [00:09<00:04, 45.37it/s] 68%|██████▊   | 414/608 [00:09<00:04, 45.22it/s] 69%|██████▉   | 419/608 [00:09<00:04, 45.25it/s] 70%|██████▉   | 424/608 [00:09<00:04, 45.25it/s] 71%|███████   | 429/608 [00:09<00:03, 45.30it/s] 71%|███████▏  | 434/608 [00:09<00:03, 45.23it/s] 72%|███████▏  | 439/608 [00:09<00:03, 45.34it/s] 73%|███████▎  | 444/608 [00:09<00:04, 37.92it/s] 74%|███████▍  | 449/608 [00:09<00:03, 40.05it/s] 75%|███████▍  | 454/608 [00:10<00:04, 34.91it/s] 75%|███████▌  | 459/608 [00:10<00:03, 37.62it/s] 76%|███████▋  | 464/608 [00:10<00:03, 39.80it/s] 77%|███████▋  | 469/608 [00:10<00:03, 41.38it/s] 78%|███████▊  | 474/608 [00:10<00:03, 42.65it/s] 79%|███████▉  | 479/608 [00:10<00:02, 43.50it/s] 80%|███████▉  | 484/608 [00:10<00:02, 44.23it/s] 80%|████████  | 489/608 [00:10<00:02, 44.69it/s] 81%|████████▏ | 494/608 [00:11<00:02, 44.57it/s] 82%|████████▏ | 499/608 [00:11<00:02, 44.41it/s] 83%|████████▎ | 504/608 [00:11<00:02, 44.47it/s] 84%|████████▎ | 509/608 [00:11<00:02, 44.81it/s] 85%|████████▍ | 514/608 [00:11<00:02, 45.02it/s] 85%|████████▌ | 519/608 [00:11<00:01, 45.17it/s] 86%|████████▌ | 524/608 [00:11<00:01, 45.41it/s] 87%|████████▋ | 529/608 [00:11<00:01, 45.48it/s] 88%|████████▊ | 534/608 [00:11<00:01, 45.59it/s] 89%|████████▊ | 539/608 [00:12<00:01, 45.31it/s] 89%|████████▉ | 544/608 [00:12<00:01, 45.04it/s] 90%|█████████ | 549/608 [00:12<00:01, 45.01it/s] 91%|█████████ | 554/608 [00:12<00:01, 45.02it/s] 92%|█████████▏| 559/608 [00:12<00:01, 45.15it/s] 93%|█████████▎| 564/608 [00:12<00:00, 45.36it/s] 94%|█████████▎| 569/608 [00:12<00:00, 44.43it/s] 94%|█████████▍| 574/608 [00:12<00:00, 44.87it/s] 95%|█████████▌| 579/608 [00:12<00:00, 44.73it/s] 96%|█████████▌| 584/608 [00:13<00:00, 45.01it/s] 97%|█████████▋| 589/608 [00:13<00:00, 44.80it/s] 98%|█████████▊| 594/608 [00:13<00:00, 44.89it/s] 99%|█████████▊| 599/608 [00:13<00:00, 44.87it/s] 99%|█████████▉| 604/608 [00:13<00:00, 45.09it/s]100%|██████████| 608/608 [00:13<00:00, 44.82it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:44:47,416 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:47,416 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:47,416 >>   eval_loss               =     0.9957
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:47,416 >>   eval_runtime            = 0:00:13.58
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:47,416 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:47,416 >>   eval_samples_per_second =    358.137
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:47,416 >>   eval_steps_per_second   =     44.767
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:44:47,416 >>   perplexity              =     2.7067
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:44:57,772 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:44:57,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:44:57,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:44:57,787 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:44:57,788 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:44:58,630 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:44:58,631 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:44:59,253 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:45:00,378 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:45:00,379 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:03,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:03,666 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:03,667 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:03,667 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:45:03,667 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:45:04,589 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:45:04,591 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:45:05,578 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:45:05,825 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:45:05,825 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-492
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-820
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-328
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-656
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/checkpoint-164
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.86it/s]Extractor Predicting: 5it [00:02,  1.91it/s]Extractor Predicting: 6it [00:03,  1.97it/s]Extractor Predicting: 7it [00:03,  1.91it/s]Extractor Predicting: 8it [00:04,  1.86it/s]Extractor Predicting: 9it [00:04,  1.88it/s]Extractor Predicting: 10it [00:05,  1.96it/s]Extractor Predicting: 11it [00:05,  1.95it/s]Extractor Predicting: 12it [00:06,  2.01it/s]Extractor Predicting: 13it [00:06,  1.98it/s]Extractor Predicting: 14it [00:07,  1.92it/s]Extractor Predicting: 15it [00:07,  2.00it/s]Extractor Predicting: 16it [00:08,  1.98it/s]Extractor Predicting: 17it [00:08,  1.96it/s]Extractor Predicting: 18it [00:09,  1.94it/s]Extractor Predicting: 19it [00:09,  1.89it/s]Extractor Predicting: 20it [00:10,  1.97it/s]Extractor Predicting: 21it [00:10,  1.97it/s]Extractor Predicting: 22it [00:11,  1.95it/s]Extractor Predicting: 23it [00:12,  1.73it/s]Extractor Predicting: 24it [00:12,  1.70it/s]Extractor Predicting: 25it [00:13,  1.65it/s]Extractor Predicting: 26it [00:14,  1.66it/s]Extractor Predicting: 27it [00:14,  1.71it/s]Extractor Predicting: 28it [00:15,  1.73it/s]Extractor Predicting: 29it [00:15,  1.77it/s]Extractor Predicting: 30it [00:16,  1.81it/s]Extractor Predicting: 31it [00:16,  1.80it/s]Extractor Predicting: 32it [00:17,  1.83it/s]Extractor Predicting: 33it [00:17,  1.85it/s]Extractor Predicting: 34it [00:18,  1.85it/s]Extractor Predicting: 35it [00:18,  1.82it/s]Extractor Predicting: 36it [00:19,  1.83it/s]Extractor Predicting: 37it [00:20,  1.77it/s]Extractor Predicting: 38it [00:20,  1.79it/s]Extractor Predicting: 39it [00:21,  1.80it/s]Extractor Predicting: 40it [00:21,  1.80it/s]Extractor Predicting: 41it [00:22,  1.82it/s]Extractor Predicting: 42it [00:22,  1.80it/s]Extractor Predicting: 43it [00:23,  1.76it/s]Extractor Predicting: 44it [00:23,  1.81it/s]Extractor Predicting: 45it [00:24,  1.79it/s]Extractor Predicting: 46it [00:25,  1.80it/s]Extractor Predicting: 47it [00:25,  1.81it/s]Extractor Predicting: 48it [00:26,  1.81it/s]Extractor Predicting: 49it [00:26,  1.74it/s]Extractor Predicting: 50it [00:27,  1.76it/s]Extractor Predicting: 51it [00:27,  1.76it/s]Extractor Predicting: 52it [00:28,  1.77it/s]Extractor Predicting: 53it [00:29,  1.76it/s]Extractor Predicting: 54it [00:29,  1.83it/s]Extractor Predicting: 55it [00:30,  1.84it/s]Extractor Predicting: 56it [00:30,  1.83it/s]Extractor Predicting: 57it [00:31,  1.80it/s]Extractor Predicting: 58it [00:31,  1.78it/s]Extractor Predicting: 59it [00:32,  1.81it/s]Extractor Predicting: 60it [00:32,  1.79it/s]Extractor Predicting: 61it [00:33,  1.74it/s]Extractor Predicting: 62it [00:34,  1.74it/s]Extractor Predicting: 63it [00:34,  1.73it/s]Extractor Predicting: 64it [00:35,  1.76it/s]Extractor Predicting: 65it [00:35,  1.76it/s]Extractor Predicting: 66it [00:36,  1.78it/s]Extractor Predicting: 67it [00:36,  1.81it/s]Extractor Predicting: 68it [00:37,  1.76it/s]Extractor Predicting: 69it [00:38,  1.74it/s]Extractor Predicting: 70it [00:38,  1.78it/s]Extractor Predicting: 71it [00:39,  1.83it/s]Extractor Predicting: 72it [00:39,  1.83it/s]Extractor Predicting: 73it [00:40,  1.85it/s]Extractor Predicting: 74it [00:40,  1.83it/s]Extractor Predicting: 75it [00:41,  1.81it/s]Extractor Predicting: 76it [00:41,  1.81it/s]Extractor Predicting: 77it [00:42,  1.81it/s]Extractor Predicting: 78it [00:42,  1.85it/s]Extractor Predicting: 79it [00:43,  1.84it/s]Extractor Predicting: 80it [00:43,  1.87it/s]Extractor Predicting: 81it [00:44,  1.80it/s]Extractor Predicting: 82it [00:45,  1.78it/s]Extractor Predicting: 83it [00:45,  1.78it/s]Extractor Predicting: 84it [00:46,  1.80it/s]Extractor Predicting: 85it [00:46,  1.79it/s]Extractor Predicting: 86it [00:47,  1.78it/s]Extractor Predicting: 87it [00:47,  1.79it/s]Extractor Predicting: 88it [00:48,  1.78it/s]Extractor Predicting: 89it [00:49,  1.70it/s]Extractor Predicting: 90it [00:49,  1.76it/s]Extractor Predicting: 91it [00:50,  1.79it/s]Extractor Predicting: 92it [00:50,  1.80it/s]Extractor Predicting: 93it [00:51,  1.76it/s]Extractor Predicting: 94it [00:51,  1.76it/s]Extractor Predicting: 95it [00:52,  1.70it/s]Extractor Predicting: 96it [00:53,  1.57it/s]Extractor Predicting: 97it [00:53,  1.62it/s]Extractor Predicting: 98it [00:54,  1.65it/s]Extractor Predicting: 99it [00:54,  1.71it/s]Extractor Predicting: 100it [00:55,  1.73it/s]Extractor Predicting: 101it [00:56,  1.76it/s]Extractor Predicting: 102it [00:56,  1.80it/s]Extractor Predicting: 103it [00:57,  1.84it/s]Extractor Predicting: 104it [00:57,  1.77it/s]Extractor Predicting: 105it [00:58,  1.75it/s]Extractor Predicting: 106it [00:58,  1.77it/s]Extractor Predicting: 107it [00:59,  1.78it/s]Extractor Predicting: 108it [00:59,  1.83it/s]Extractor Predicting: 109it [01:00,  1.79it/s]Extractor Predicting: 110it [01:01,  1.71it/s]Extractor Predicting: 111it [01:01,  1.75it/s]Extractor Predicting: 112it [01:02,  1.77it/s]Extractor Predicting: 113it [01:02,  1.79it/s]Extractor Predicting: 114it [01:03,  1.83it/s]Extractor Predicting: 115it [01:03,  1.91it/s]Extractor Predicting: 116it [01:04,  1.90it/s]Extractor Predicting: 117it [01:04,  1.87it/s]Extractor Predicting: 118it [01:05,  1.89it/s]Extractor Predicting: 119it [01:05,  1.87it/s]Extractor Predicting: 120it [01:06,  1.90it/s]Extractor Predicting: 121it [01:07,  1.87it/s]Extractor Predicting: 122it [01:07,  1.81it/s]Extractor Predicting: 123it [01:08,  1.75it/s]Extractor Predicting: 124it [01:08,  1.71it/s]Extractor Predicting: 125it [01:09,  1.74it/s]Extractor Predicting: 126it [01:09,  1.78it/s]Extractor Predicting: 127it [01:10,  1.78it/s]Extractor Predicting: 128it [01:11,  1.73it/s]Extractor Predicting: 129it [01:11,  1.75it/s]Extractor Predicting: 130it [01:12,  1.78it/s]Extractor Predicting: 131it [01:12,  1.84it/s]Extractor Predicting: 132it [01:13,  1.78it/s]Extractor Predicting: 133it [01:13,  1.76it/s]Extractor Predicting: 134it [01:14,  1.78it/s]Extractor Predicting: 135it [01:15,  1.76it/s]Extractor Predicting: 136it [01:15,  1.78it/s]Extractor Predicting: 137it [01:16,  1.81it/s]Extractor Predicting: 138it [01:16,  1.80it/s]Extractor Predicting: 139it [01:17,  1.79it/s]Extractor Predicting: 140it [01:17,  1.78it/s]Extractor Predicting: 141it [01:18,  1.81it/s]Extractor Predicting: 142it [01:18,  1.78it/s]Extractor Predicting: 143it [01:19,  1.83it/s]Extractor Predicting: 144it [01:19,  1.84it/s]Extractor Predicting: 145it [01:20,  1.85it/s]Extractor Predicting: 146it [01:21,  1.78it/s]Extractor Predicting: 147it [01:21,  1.73it/s]Extractor Predicting: 148it [01:22,  1.75it/s]Extractor Predicting: 149it [01:22,  1.75it/s]Extractor Predicting: 150it [01:23,  1.78it/s]Extractor Predicting: 151it [01:23,  1.76it/s]Extractor Predicting: 152it [01:24,  1.81it/s]Extractor Predicting: 153it [01:25,  1.76it/s]Extractor Predicting: 154it [01:25,  1.77it/s]Extractor Predicting: 155it [01:26,  1.80it/s]Extractor Predicting: 156it [01:26,  1.84it/s]Extractor Predicting: 157it [01:27,  1.84it/s]Extractor Predicting: 158it [01:27,  1.86it/s]Extractor Predicting: 159it [01:28,  1.81it/s]Extractor Predicting: 160it [01:28,  1.82it/s]Extractor Predicting: 161it [01:29,  1.80it/s]Extractor Predicting: 162it [01:30,  1.77it/s]Extractor Predicting: 163it [01:30,  1.79it/s]Extractor Predicting: 164it [01:31,  1.78it/s]Extractor Predicting: 165it [01:31,  1.79it/s]Extractor Predicting: 166it [01:32,  1.76it/s]Extractor Predicting: 167it [01:32,  1.75it/s]Extractor Predicting: 168it [01:33,  1.72it/s]Extractor Predicting: 169it [01:34,  1.69it/s]Extractor Predicting: 170it [01:34,  1.70it/s]Extractor Predicting: 171it [01:35,  1.68it/s]Extractor Predicting: 172it [01:35,  1.69it/s]Extractor Predicting: 173it [01:36,  1.68it/s]Extractor Predicting: 174it [01:37,  1.64it/s]Extractor Predicting: 175it [01:37,  1.71it/s]Extractor Predicting: 176it [01:38,  1.70it/s]Extractor Predicting: 177it [01:38,  1.67it/s]Extractor Predicting: 178it [01:39,  1.63it/s]Extractor Predicting: 179it [01:40,  1.61it/s]Extractor Predicting: 180it [01:40,  1.61it/s]Extractor Predicting: 181it [01:41,  1.65it/s]Extractor Predicting: 182it [01:41,  1.64it/s]Extractor Predicting: 183it [01:42,  1.46it/s]Extractor Predicting: 184it [01:43,  1.51it/s]Extractor Predicting: 185it [01:43,  1.59it/s]Extractor Predicting: 186it [01:44,  1.62it/s]Extractor Predicting: 187it [01:45,  1.77it/s]Extractor Predicting: 187it [01:45,  1.78it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:08,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:08,338 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:08,338 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:08,339 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:08,339 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:47:09,300 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:47:09,301 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:47:09,944 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:47:11,045 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:47:11,045 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:14,461 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:14,464 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:14,464 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:14,465 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:47:14,465 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:47:15,233 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:47:15,234 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:47:15,831 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:47:16,056 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:47:16,056 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 4it [00:02,  1.81it/s]Extractor Predicting: 5it [00:02,  1.84it/s]Extractor Predicting: 6it [00:03,  1.82it/s]Extractor Predicting: 7it [00:03,  1.77it/s]Extractor Predicting: 8it [00:04,  1.78it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 10it [00:05,  1.78it/s]Extractor Predicting: 11it [00:06,  1.82it/s]Extractor Predicting: 12it [00:06,  1.85it/s]Extractor Predicting: 13it [00:07,  1.80it/s]Extractor Predicting: 14it [00:07,  1.82it/s]Extractor Predicting: 15it [00:08,  1.85it/s]Extractor Predicting: 16it [00:08,  1.84it/s]Extractor Predicting: 17it [00:09,  1.82it/s]Extractor Predicting: 18it [00:10,  1.78it/s]Extractor Predicting: 19it [00:10,  1.76it/s]Extractor Predicting: 20it [00:11,  1.76it/s]Extractor Predicting: 21it [00:11,  1.76it/s]Extractor Predicting: 22it [00:12,  1.74it/s]Extractor Predicting: 23it [00:12,  1.77it/s]Extractor Predicting: 24it [00:13,  1.78it/s]Extractor Predicting: 25it [00:13,  1.78it/s]Extractor Predicting: 26it [00:14,  1.79it/s]Extractor Predicting: 27it [00:15,  1.79it/s]Extractor Predicting: 28it [00:15,  1.79it/s]Extractor Predicting: 29it [00:16,  1.80it/s]Extractor Predicting: 30it [00:16,  1.81it/s]Extractor Predicting: 31it [00:17,  1.75it/s]Extractor Predicting: 32it [00:17,  1.73it/s]Extractor Predicting: 33it [00:18,  1.80it/s]Extractor Predicting: 34it [00:18,  1.82it/s]Extractor Predicting: 35it [00:19,  1.80it/s]Extractor Predicting: 36it [00:20,  1.83it/s]Extractor Predicting: 37it [00:20,  1.77it/s]Extractor Predicting: 38it [00:21,  1.75it/s]Extractor Predicting: 39it [00:21,  1.79it/s]Extractor Predicting: 40it [00:22,  1.80it/s]Extractor Predicting: 41it [00:22,  1.83it/s]Extractor Predicting: 42it [00:23,  1.81it/s]Extractor Predicting: 43it [00:23,  1.82it/s]Extractor Predicting: 44it [00:24,  1.80it/s]Extractor Predicting: 45it [00:25,  1.84it/s]Extractor Predicting: 46it [00:25,  1.81it/s]Extractor Predicting: 47it [00:26,  1.58it/s]Extractor Predicting: 48it [00:27,  1.63it/s]Extractor Predicting: 49it [00:27,  1.64it/s]Extractor Predicting: 50it [00:28,  1.68it/s]Extractor Predicting: 51it [00:28,  1.69it/s]Extractor Predicting: 52it [00:29,  1.72it/s]Extractor Predicting: 53it [00:29,  1.79it/s]Extractor Predicting: 54it [00:30,  1.80it/s]Extractor Predicting: 55it [00:30,  1.81it/s]Extractor Predicting: 56it [00:31,  1.80it/s]Extractor Predicting: 57it [00:32,  1.76it/s]Extractor Predicting: 58it [00:32,  1.80it/s]Extractor Predicting: 59it [00:33,  1.80it/s]Extractor Predicting: 60it [00:33,  1.83it/s]Extractor Predicting: 61it [00:34,  1.78it/s]Extractor Predicting: 62it [00:34,  1.73it/s]Extractor Predicting: 63it [00:35,  1.76it/s]Extractor Predicting: 64it [00:36,  1.74it/s]Extractor Predicting: 65it [00:36,  1.76it/s]Extractor Predicting: 66it [00:37,  1.79it/s]Extractor Predicting: 67it [00:37,  1.82it/s]Extractor Predicting: 68it [00:38,  1.68it/s]Extractor Predicting: 69it [00:38,  1.74it/s]Extractor Predicting: 70it [00:39,  1.70it/s]Extractor Predicting: 71it [00:40,  1.75it/s]Extractor Predicting: 72it [00:40,  1.76it/s]Extractor Predicting: 73it [00:41,  1.83it/s]Extractor Predicting: 74it [00:41,  1.81it/s]Extractor Predicting: 75it [00:42,  1.83it/s]Extractor Predicting: 76it [00:42,  1.78it/s]Extractor Predicting: 77it [00:43,  1.81it/s]Extractor Predicting: 78it [00:43,  1.86it/s]Extractor Predicting: 79it [00:44,  1.84it/s]Extractor Predicting: 80it [00:44,  1.82it/s]Extractor Predicting: 81it [00:45,  1.83it/s]Extractor Predicting: 82it [00:46,  1.84it/s]Extractor Predicting: 83it [00:46,  1.84it/s]Extractor Predicting: 84it [00:47,  1.78it/s]Extractor Predicting: 85it [00:47,  1.83it/s]Extractor Predicting: 86it [00:48,  1.82it/s]Extractor Predicting: 87it [00:48,  1.88it/s]Extractor Predicting: 88it [00:49,  1.91it/s]Extractor Predicting: 89it [00:49,  1.89it/s]Extractor Predicting: 90it [00:50,  1.84it/s]Extractor Predicting: 91it [00:50,  1.91it/s]Extractor Predicting: 92it [00:51,  1.89it/s]Extractor Predicting: 93it [00:51,  1.87it/s]Extractor Predicting: 94it [00:52,  1.90it/s]Extractor Predicting: 95it [00:52,  1.90it/s]Extractor Predicting: 96it [00:53,  1.93it/s]Extractor Predicting: 97it [00:53,  1.90it/s]Extractor Predicting: 98it [00:54,  1.87it/s]Extractor Predicting: 99it [00:55,  1.84it/s]Extractor Predicting: 100it [00:55,  1.88it/s]Extractor Predicting: 101it [00:56,  1.87it/s]Extractor Predicting: 102it [00:56,  1.88it/s]Extractor Predicting: 103it [00:57,  1.85it/s]Extractor Predicting: 104it [00:57,  1.85it/s]Extractor Predicting: 105it [00:58,  1.82it/s]Extractor Predicting: 106it [00:58,  1.79it/s]Extractor Predicting: 107it [00:59,  1.82it/s]Extractor Predicting: 108it [01:00,  1.83it/s]Extractor Predicting: 109it [01:00,  1.79it/s]Extractor Predicting: 110it [01:01,  1.79it/s]Extractor Predicting: 111it [01:01,  1.78it/s]Extractor Predicting: 112it [01:02,  1.80it/s]Extractor Predicting: 113it [01:02,  1.82it/s]Extractor Predicting: 114it [01:03,  1.84it/s]Extractor Predicting: 115it [01:03,  1.82it/s]Extractor Predicting: 116it [01:04,  1.81it/s]Extractor Predicting: 117it [01:04,  1.81it/s]Extractor Predicting: 118it [01:05,  1.79it/s]Extractor Predicting: 119it [01:06,  1.81it/s]Extractor Predicting: 120it [01:06,  1.79it/s]Extractor Predicting: 121it [01:07,  1.81it/s]Extractor Predicting: 122it [01:07,  1.83it/s]Extractor Predicting: 123it [01:08,  1.84it/s]Extractor Predicting: 124it [01:08,  1.82it/s]Extractor Predicting: 125it [01:09,  1.81it/s]Extractor Predicting: 126it [01:09,  1.82it/s]Extractor Predicting: 127it [01:10,  1.83it/s]Extractor Predicting: 128it [01:11,  1.80it/s]Extractor Predicting: 129it [01:11,  1.76it/s]Extractor Predicting: 130it [01:12,  1.81it/s]Extractor Predicting: 131it [01:12,  1.81it/s]Extractor Predicting: 132it [01:13,  1.80it/s]Extractor Predicting: 133it [01:13,  1.83it/s]Extractor Predicting: 134it [01:14,  1.80it/s]Extractor Predicting: 135it [01:14,  1.81it/s]Extractor Predicting: 136it [01:15,  1.81it/s]Extractor Predicting: 137it [01:16,  1.79it/s]Extractor Predicting: 138it [01:16,  1.78it/s]Extractor Predicting: 139it [01:17,  1.81it/s]Extractor Predicting: 140it [01:17,  1.80it/s]Extractor Predicting: 141it [01:18,  1.79it/s]Extractor Predicting: 142it [01:18,  1.77it/s]Extractor Predicting: 143it [01:19,  1.77it/s]Extractor Predicting: 144it [01:20,  1.78it/s]Extractor Predicting: 145it [01:20,  1.76it/s]Extractor Predicting: 146it [01:21,  1.77it/s]Extractor Predicting: 147it [01:21,  1.76it/s]Extractor Predicting: 148it [01:22,  1.77it/s]Extractor Predicting: 149it [01:22,  1.76it/s]Extractor Predicting: 150it [01:23,  1.73it/s]Extractor Predicting: 151it [01:24,  1.73it/s]Extractor Predicting: 152it [01:24,  1.75it/s]Extractor Predicting: 153it [01:25,  1.82it/s]Extractor Predicting: 154it [01:25,  1.81it/s]Extractor Predicting: 155it [01:26,  1.80it/s]Extractor Predicting: 156it [01:26,  1.77it/s]Extractor Predicting: 157it [01:27,  1.79it/s]Extractor Predicting: 158it [01:27,  1.79it/s]Extractor Predicting: 159it [01:28,  1.78it/s]Extractor Predicting: 160it [01:29,  1.78it/s]Extractor Predicting: 161it [01:29,  1.78it/s]Extractor Predicting: 162it [01:30,  1.76it/s]Extractor Predicting: 163it [01:30,  1.79it/s]Extractor Predicting: 164it [01:31,  1.78it/s]Extractor Predicting: 165it [01:31,  1.80it/s]Extractor Predicting: 166it [01:32,  1.81it/s]Extractor Predicting: 167it [01:32,  1.79it/s]Extractor Predicting: 168it [01:33,  1.77it/s]Extractor Predicting: 169it [01:34,  1.77it/s]Extractor Predicting: 170it [01:34,  1.81it/s]Extractor Predicting: 171it [01:35,  1.87it/s]Extractor Predicting: 172it [01:35,  1.86it/s]Extractor Predicting: 173it [01:36,  1.78it/s]Extractor Predicting: 174it [01:36,  1.77it/s]Extractor Predicting: 175it [01:37,  1.72it/s]Extractor Predicting: 176it [01:38,  1.72it/s]Extractor Predicting: 177it [01:38,  1.75it/s]Extractor Predicting: 178it [01:39,  1.75it/s]Extractor Predicting: 179it [01:39,  1.75it/s]Extractor Predicting: 180it [01:40,  1.72it/s]Extractor Predicting: 181it [01:40,  1.73it/s]Extractor Predicting: 182it [01:41,  1.73it/s]Extractor Predicting: 183it [01:42,  1.50it/s]Extractor Predicting: 184it [01:42,  1.54it/s]Extractor Predicting: 185it [01:43,  1.60it/s]Extractor Predicting: 186it [01:44,  1.65it/s]Extractor Predicting: 187it [01:44,  1.65it/s]Extractor Predicting: 188it [01:45,  1.70it/s]Extractor Predicting: 189it [01:45,  1.73it/s]Extractor Predicting: 190it [01:46,  1.67it/s]Extractor Predicting: 191it [01:46,  1.72it/s]Extractor Predicting: 192it [01:47,  1.73it/s]Extractor Predicting: 193it [01:48,  1.73it/s]Extractor Predicting: 194it [01:48,  1.75it/s]Extractor Predicting: 195it [01:49,  1.81it/s]Extractor Predicting: 196it [01:49,  1.79it/s]Extractor Predicting: 197it [01:50,  1.79it/s]Extractor Predicting: 198it [01:50,  1.80it/s]Extractor Predicting: 199it [01:51,  1.81it/s]Extractor Predicting: 200it [01:51,  1.83it/s]Extractor Predicting: 201it [01:52,  1.82it/s]Extractor Predicting: 202it [01:53,  1.76it/s]Extractor Predicting: 203it [01:53,  1.78it/s]Extractor Predicting: 204it [01:54,  1.78it/s]Extractor Predicting: 205it [01:54,  1.79it/s]Extractor Predicting: 206it [01:55,  1.81it/s]Extractor Predicting: 207it [01:55,  1.80it/s]Extractor Predicting: 208it [01:56,  1.78it/s]Extractor Predicting: 209it [01:56,  1.82it/s]Extractor Predicting: 210it [01:57,  1.79it/s]Extractor Predicting: 211it [01:58,  1.82it/s]Extractor Predicting: 212it [01:58,  1.83it/s]Extractor Predicting: 213it [01:59,  1.80it/s]Extractor Predicting: 214it [01:59,  1.77it/s]Extractor Predicting: 215it [02:00,  1.74it/s]Extractor Predicting: 216it [02:00,  1.75it/s]Extractor Predicting: 217it [02:01,  1.80it/s]Extractor Predicting: 218it [02:02,  1.78it/s]Extractor Predicting: 219it [02:02,  1.76it/s]Extractor Predicting: 220it [02:03,  1.81it/s]Extractor Predicting: 221it [02:03,  1.75it/s]Extractor Predicting: 222it [02:04,  1.74it/s]Extractor Predicting: 223it [02:04,  1.75it/s]Extractor Predicting: 224it [02:05,  1.78it/s]Extractor Predicting: 225it [02:05,  1.83it/s]Extractor Predicting: 226it [02:06,  1.83it/s]Extractor Predicting: 227it [02:07,  1.82it/s]Extractor Predicting: 228it [02:07,  1.83it/s]Extractor Predicting: 229it [02:08,  1.81it/s]Extractor Predicting: 230it [02:08,  1.85it/s]Extractor Predicting: 231it [02:09,  1.85it/s]Extractor Predicting: 232it [02:09,  1.85it/s]Extractor Predicting: 233it [02:10,  1.80it/s]Extractor Predicting: 234it [02:10,  1.77it/s]Extractor Predicting: 235it [02:11,  1.76it/s]Extractor Predicting: 236it [02:12,  1.78it/s]Extractor Predicting: 237it [02:12,  1.76it/s]Extractor Predicting: 238it [02:13,  1.78it/s]Extractor Predicting: 239it [02:13,  1.74it/s]Extractor Predicting: 240it [02:14,  1.78it/s]Extractor Predicting: 241it [02:14,  1.82it/s]Extractor Predicting: 242it [02:15,  1.81it/s]Extractor Predicting: 243it [02:15,  1.85it/s]Extractor Predicting: 244it [02:16,  1.86it/s]Extractor Predicting: 245it [02:17,  1.81it/s]Extractor Predicting: 246it [02:17,  1.83it/s]Extractor Predicting: 247it [02:18,  1.81it/s]Extractor Predicting: 248it [02:18,  1.85it/s]Extractor Predicting: 249it [02:19,  1.84it/s]Extractor Predicting: 250it [02:19,  1.90it/s]Extractor Predicting: 251it [02:20,  1.82it/s]Extractor Predicting: 252it [02:20,  1.85it/s]Extractor Predicting: 253it [02:21,  1.92it/s]Extractor Predicting: 254it [02:21,  1.86it/s]Extractor Predicting: 255it [02:22,  1.87it/s]Extractor Predicting: 256it [02:22,  1.86it/s]Extractor Predicting: 257it [02:23,  1.84it/s]Extractor Predicting: 258it [02:24,  1.85it/s]Extractor Predicting: 259it [02:24,  1.86it/s]Extractor Predicting: 260it [02:25,  1.84it/s]Extractor Predicting: 261it [02:25,  1.81it/s]Extractor Predicting: 262it [02:26,  1.83it/s]Extractor Predicting: 263it [02:26,  1.73it/s]Extractor Predicting: 264it [02:27,  1.78it/s]Extractor Predicting: 265it [02:27,  1.78it/s]Extractor Predicting: 266it [02:28,  1.76it/s]Extractor Predicting: 267it [02:29,  1.77it/s]Extractor Predicting: 268it [02:29,  1.76it/s]Extractor Predicting: 269it [02:30,  1.73it/s]Extractor Predicting: 270it [02:30,  1.72it/s]Extractor Predicting: 271it [02:31,  1.75it/s]Extractor Predicting: 272it [02:31,  1.80it/s]Extractor Predicting: 273it [02:32,  1.78it/s]Extractor Predicting: 274it [02:33,  1.76it/s]Extractor Predicting: 275it [02:33,  1.76it/s]Extractor Predicting: 276it [02:34,  1.78it/s]Extractor Predicting: 277it [02:34,  1.77it/s]Extractor Predicting: 278it [02:35,  1.78it/s]Extractor Predicting: 279it [02:35,  1.77it/s]Extractor Predicting: 280it [02:36,  1.77it/s]Extractor Predicting: 281it [02:37,  1.78it/s]Extractor Predicting: 282it [02:37,  1.77it/s]Extractor Predicting: 283it [02:38,  1.78it/s]Extractor Predicting: 284it [02:38,  1.81it/s]Extractor Predicting: 285it [02:39,  1.84it/s]Extractor Predicting: 286it [02:39,  1.83it/s]Extractor Predicting: 287it [02:40,  1.79it/s]Extractor Predicting: 288it [02:40,  1.78it/s]Extractor Predicting: 289it [02:41,  1.77it/s]Extractor Predicting: 290it [02:42,  1.75it/s]Extractor Predicting: 291it [02:42,  1.76it/s]Extractor Predicting: 292it [02:43,  1.78it/s]Extractor Predicting: 293it [02:43,  1.77it/s]Extractor Predicting: 294it [02:44,  1.71it/s]Extractor Predicting: 295it [02:44,  1.74it/s]Extractor Predicting: 296it [02:45,  1.74it/s]Extractor Predicting: 297it [02:46,  1.73it/s]Extractor Predicting: 298it [02:46,  1.74it/s]Extractor Predicting: 299it [02:47,  1.73it/s]Extractor Predicting: 300it [02:47,  1.62it/s]Extractor Predicting: 301it [02:48,  1.59it/s]Extractor Predicting: 302it [02:49,  1.59it/s]Extractor Predicting: 303it [02:49,  1.63it/s]Extractor Predicting: 304it [02:50,  1.59it/s]Extractor Predicting: 305it [02:51,  1.54it/s]Extractor Predicting: 306it [02:52,  1.42it/s]Extractor Predicting: 307it [02:52,  1.53it/s]Extractor Predicting: 308it [02:53,  1.60it/s]Extractor Predicting: 309it [02:53,  1.71it/s]Extractor Predicting: 310it [02:54,  1.68it/s]Extractor Predicting: 311it [02:54,  1.66it/s]Extractor Predicting: 312it [02:55,  1.70it/s]Extractor Predicting: 313it [02:55,  1.73it/s]Extractor Predicting: 314it [02:56,  1.72it/s]Extractor Predicting: 315it [02:57,  1.81it/s]Extractor Predicting: 316it [02:57,  1.77it/s]Extractor Predicting: 317it [02:58,  1.77it/s]Extractor Predicting: 318it [02:58,  1.78it/s]Extractor Predicting: 319it [02:59,  1.76it/s]Extractor Predicting: 320it [02:59,  1.78it/s]Extractor Predicting: 321it [03:00,  1.81it/s]Extractor Predicting: 322it [03:00,  1.85it/s]Extractor Predicting: 323it [03:01,  1.78it/s]Extractor Predicting: 324it [03:02,  1.81it/s]Extractor Predicting: 325it [03:02,  1.81it/s]Extractor Predicting: 326it [03:03,  1.84it/s]Extractor Predicting: 327it [03:03,  1.84it/s]Extractor Predicting: 328it [03:04,  1.82it/s]Extractor Predicting: 329it [03:04,  1.80it/s]Extractor Predicting: 330it [03:05,  1.78it/s]Extractor Predicting: 331it [03:05,  1.78it/s]Extractor Predicting: 332it [03:06,  1.82it/s]Extractor Predicting: 333it [03:07,  1.80it/s]Extractor Predicting: 334it [03:07,  1.80it/s]Extractor Predicting: 335it [03:08,  1.78it/s]Extractor Predicting: 336it [03:08,  1.81it/s]Extractor Predicting: 337it [03:09,  1.81it/s]Extractor Predicting: 338it [03:09,  1.82it/s]Extractor Predicting: 339it [03:10,  1.82it/s]Extractor Predicting: 340it [03:10,  1.83it/s]Extractor Predicting: 341it [03:11,  1.81it/s]Extractor Predicting: 342it [03:12,  1.78it/s]Extractor Predicting: 343it [03:12,  1.79it/s]Extractor Predicting: 344it [03:13,  1.84it/s]Extractor Predicting: 345it [03:13,  1.87it/s]Extractor Predicting: 346it [03:14,  1.90it/s]Extractor Predicting: 347it [03:14,  1.94it/s]Extractor Predicting: 348it [03:15,  1.91it/s]Extractor Predicting: 349it [03:15,  1.92it/s]Extractor Predicting: 350it [03:16,  1.91it/s]Extractor Predicting: 351it [03:16,  1.91it/s]Extractor Predicting: 352it [03:17,  1.90it/s]Extractor Predicting: 353it [03:17,  1.89it/s]Extractor Predicting: 354it [03:18,  1.89it/s]Extractor Predicting: 355it [03:18,  1.89it/s]Extractor Predicting: 356it [03:19,  1.93it/s]Extractor Predicting: 357it [03:19,  1.90it/s]Extractor Predicting: 358it [03:20,  1.90it/s]Extractor Predicting: 359it [03:20,  1.87it/s]Extractor Predicting: 360it [03:21,  1.87it/s]Extractor Predicting: 361it [03:21,  1.87it/s]Extractor Predicting: 362it [03:22,  1.89it/s]Extractor Predicting: 363it [03:23,  1.91it/s]Extractor Predicting: 364it [03:23,  1.96it/s]Extractor Predicting: 365it [03:24,  1.85it/s]Extractor Predicting: 366it [03:24,  1.81it/s]Extractor Predicting: 367it [03:25,  1.76it/s]Extractor Predicting: 368it [03:25,  1.73it/s]Extractor Predicting: 369it [03:26,  1.79it/s]Extractor Predicting: 370it [03:27,  1.75it/s]Extractor Predicting: 371it [03:27,  1.74it/s]Extractor Predicting: 372it [03:28,  1.77it/s]Extractor Predicting: 373it [03:28,  1.80it/s]Extractor Predicting: 374it [03:29,  1.81it/s]Extractor Predicting: 375it [03:29,  1.79it/s]Extractor Predicting: 376it [03:30,  1.78it/s]Extractor Predicting: 377it [03:30,  1.77it/s]Extractor Predicting: 378it [03:31,  1.77it/s]Extractor Predicting: 379it [03:32,  1.77it/s]Extractor Predicting: 380it [03:32,  1.75it/s]Extractor Predicting: 381it [03:33,  1.80it/s]Extractor Predicting: 382it [03:33,  1.83it/s]Extractor Predicting: 383it [03:34,  1.84it/s]Extractor Predicting: 384it [03:34,  1.86it/s]Extractor Predicting: 385it [03:35,  1.83it/s]Extractor Predicting: 386it [03:35,  1.82it/s]Extractor Predicting: 387it [03:36,  1.83it/s]Extractor Predicting: 388it [03:36,  1.88it/s]Extractor Predicting: 389it [03:37,  1.83it/s]Extractor Predicting: 390it [03:38,  1.80it/s]Extractor Predicting: 391it [03:38,  1.82it/s]Extractor Predicting: 392it [03:39,  1.85it/s]Extractor Predicting: 393it [03:39,  1.89it/s]Extractor Predicting: 394it [03:40,  1.90it/s]Extractor Predicting: 395it [03:40,  1.93it/s]Extractor Predicting: 396it [03:41,  1.95it/s]Extractor Predicting: 397it [03:41,  1.91it/s]Extractor Predicting: 398it [03:42,  1.87it/s]Extractor Predicting: 399it [03:42,  1.88it/s]Extractor Predicting: 400it [03:43,  1.86it/s]Extractor Predicting: 401it [03:43,  1.89it/s]Extractor Predicting: 402it [03:44,  1.93it/s]Extractor Predicting: 403it [03:44,  1.92it/s]Extractor Predicting: 404it [03:45,  1.87it/s]Extractor Predicting: 405it [03:45,  1.84it/s]Extractor Predicting: 406it [03:46,  1.86it/s]Extractor Predicting: 407it [03:47,  1.87it/s]Extractor Predicting: 408it [03:47,  1.89it/s]Extractor Predicting: 409it [03:48,  1.68it/s]Extractor Predicting: 410it [03:48,  1.69it/s]Extractor Predicting: 411it [03:49,  1.72it/s]Extractor Predicting: 412it [03:50,  1.75it/s]Extractor Predicting: 413it [03:50,  1.70it/s]Extractor Predicting: 414it [03:51,  1.67it/s]Extractor Predicting: 415it [03:51,  1.63it/s]Extractor Predicting: 416it [03:52,  1.60it/s]Extractor Predicting: 417it [03:53,  1.63it/s]Extractor Predicting: 418it [03:53,  1.68it/s]Extractor Predicting: 419it [03:54,  1.70it/s]Extractor Predicting: 420it [03:54,  1.67it/s]Extractor Predicting: 421it [03:55,  1.68it/s]Extractor Predicting: 422it [03:56,  1.66it/s]Extractor Predicting: 423it [03:56,  1.71it/s]Extractor Predicting: 424it [03:57,  1.70it/s]Extractor Predicting: 425it [03:58,  1.48it/s]Extractor Predicting: 426it [03:58,  1.53it/s]Extractor Predicting: 427it [03:59,  1.57it/s]Extractor Predicting: 428it [03:59,  1.62it/s]Extractor Predicting: 429it [04:00,  1.68it/s]Extractor Predicting: 430it [04:00,  1.72it/s]Extractor Predicting: 431it [04:01,  1.74it/s]Extractor Predicting: 432it [04:02,  1.75it/s]Extractor Predicting: 433it [04:02,  1.75it/s]Extractor Predicting: 434it [04:03,  1.73it/s]Extractor Predicting: 435it [04:03,  1.76it/s]Extractor Predicting: 436it [04:04,  1.80it/s]Extractor Predicting: 437it [04:04,  1.78it/s]Extractor Predicting: 438it [04:05,  1.80it/s]Extractor Predicting: 439it [04:06,  1.80it/s]Extractor Predicting: 440it [04:06,  1.77it/s]Extractor Predicting: 441it [04:07,  1.75it/s]Extractor Predicting: 442it [04:07,  1.77it/s]Extractor Predicting: 443it [04:08,  1.75it/s]Extractor Predicting: 444it [04:08,  1.75it/s]Extractor Predicting: 445it [04:09,  1.74it/s]Extractor Predicting: 446it [04:10,  1.71it/s]Extractor Predicting: 447it [04:10,  1.72it/s]Extractor Predicting: 448it [04:11,  1.72it/s]Extractor Predicting: 449it [04:11,  1.76it/s]Extractor Predicting: 450it [04:12,  1.80it/s]Extractor Predicting: 451it [04:12,  1.79it/s]Extractor Predicting: 452it [04:13,  1.74it/s]Extractor Predicting: 453it [04:14,  1.74it/s]Extractor Predicting: 454it [04:14,  1.74it/s]Extractor Predicting: 455it [04:15,  1.71it/s]Extractor Predicting: 456it [04:15,  1.74it/s]Extractor Predicting: 457it [04:16,  1.71it/s]Extractor Predicting: 458it [04:16,  1.71it/s]Extractor Predicting: 459it [04:17,  1.72it/s]Extractor Predicting: 460it [04:18,  1.71it/s]Extractor Predicting: 461it [04:18,  1.74it/s]Extractor Predicting: 462it [04:19,  1.74it/s]Extractor Predicting: 463it [04:19,  1.69it/s]Extractor Predicting: 464it [04:20,  1.64it/s]Extractor Predicting: 465it [04:21,  1.65it/s]Extractor Predicting: 466it [04:21,  1.69it/s]Extractor Predicting: 467it [04:22,  1.75it/s]Extractor Predicting: 468it [04:22,  1.78it/s]Extractor Predicting: 469it [04:23,  1.83it/s]Extractor Predicting: 470it [04:23,  1.79it/s]Extractor Predicting: 471it [04:24,  1.76it/s]Extractor Predicting: 472it [04:25,  1.77it/s]Extractor Predicting: 473it [04:25,  1.74it/s]Extractor Predicting: 474it [04:26,  1.73it/s]Extractor Predicting: 475it [04:26,  1.72it/s]Extractor Predicting: 476it [04:27,  1.72it/s]Extractor Predicting: 477it [04:27,  1.73it/s]Extractor Predicting: 478it [04:28,  1.74it/s]Extractor Predicting: 479it [04:29,  1.67it/s]Extractor Predicting: 480it [04:29,  1.63it/s]Extractor Predicting: 481it [04:30,  1.59it/s]Extractor Predicting: 482it [04:31,  1.61it/s]Extractor Predicting: 483it [04:31,  1.59it/s]Extractor Predicting: 484it [04:32,  1.57it/s]Extractor Predicting: 485it [04:32,  1.59it/s]Extractor Predicting: 486it [04:33,  1.59it/s]Extractor Predicting: 487it [04:34,  1.64it/s]Extractor Predicting: 488it [04:34,  1.62it/s]Extractor Predicting: 489it [04:35,  1.66it/s]Extractor Predicting: 490it [04:36,  1.63it/s]Extractor Predicting: 491it [04:36,  1.60it/s]Extractor Predicting: 492it [04:37,  1.61it/s]Extractor Predicting: 493it [04:37,  1.63it/s]Extractor Predicting: 494it [04:38,  1.64it/s]Extractor Predicting: 495it [04:39,  1.64it/s]Extractor Predicting: 496it [04:39,  1.61it/s]Extractor Predicting: 497it [04:40,  1.61it/s]Extractor Predicting: 498it [04:40,  1.64it/s]Extractor Predicting: 499it [04:41,  1.61it/s]Extractor Predicting: 500it [04:42,  1.64it/s]Extractor Predicting: 501it [04:42,  1.64it/s]Extractor Predicting: 502it [04:43,  1.61it/s]Extractor Predicting: 503it [04:44,  1.60it/s]Extractor Predicting: 504it [04:44,  1.58it/s]Extractor Predicting: 505it [04:45,  1.57it/s]Extractor Predicting: 506it [04:46,  1.55it/s]Extractor Predicting: 507it [04:46,  1.60it/s]Extractor Predicting: 508it [04:47,  1.80it/s]Extractor Predicting: 508it [04:47,  1.77it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:17,402 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:17,447 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:17,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:17,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:17,448 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:52:18,092 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:52:18,093 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:52:18,707 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:52:19,765 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:52:19,765 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:23,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:23,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:23,085 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:23,085 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:52:23,085 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:52:23,754 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:52:23,756 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:52:24,350 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:52:24,547 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:52:24,547 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:04,  1.70it/s]Extractor Predicting: 8it [00:04,  1.71it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.67it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.71it/s]Extractor Predicting: 14it [00:08,  1.70it/s]Extractor Predicting: 15it [00:08,  1.73it/s]Extractor Predicting: 16it [00:09,  1.70it/s]Extractor Predicting: 17it [00:09,  1.71it/s]Extractor Predicting: 18it [00:10,  1.73it/s]Extractor Predicting: 19it [00:11,  1.71it/s]Extractor Predicting: 20it [00:11,  1.71it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:12,  1.68it/s]Extractor Predicting: 23it [00:13,  1.66it/s]Extractor Predicting: 24it [00:14,  1.70it/s]Extractor Predicting: 25it [00:14,  1.75it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:15,  1.67it/s]Extractor Predicting: 28it [00:16,  1.67it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:17,  1.71it/s]Extractor Predicting: 31it [00:18,  1.76it/s]Extractor Predicting: 32it [00:18,  1.80it/s]Extractor Predicting: 33it [00:19,  1.80it/s]Extractor Predicting: 34it [00:19,  1.83it/s]Extractor Predicting: 35it [00:20,  1.81it/s]Extractor Predicting: 36it [00:20,  1.83it/s]Extractor Predicting: 37it [00:21,  1.80it/s]Extractor Predicting: 38it [00:22,  1.82it/s]Extractor Predicting: 39it [00:22,  1.78it/s]Extractor Predicting: 40it [00:23,  1.81it/s]Extractor Predicting: 41it [00:23,  1.80it/s]Extractor Predicting: 42it [00:24,  1.85it/s]Extractor Predicting: 43it [00:24,  1.84it/s]Extractor Predicting: 44it [00:25,  1.89it/s]Extractor Predicting: 45it [00:25,  1.82it/s]Extractor Predicting: 46it [00:26,  1.83it/s]Extractor Predicting: 47it [00:26,  1.84it/s]Extractor Predicting: 48it [00:27,  1.82it/s]Extractor Predicting: 49it [00:28,  1.81it/s]Extractor Predicting: 50it [00:28,  1.78it/s]Extractor Predicting: 51it [00:29,  1.74it/s]Extractor Predicting: 52it [00:29,  1.75it/s]Extractor Predicting: 53it [00:30,  1.77it/s]Extractor Predicting: 54it [00:30,  1.71it/s]Extractor Predicting: 55it [00:31,  1.74it/s]Extractor Predicting: 56it [00:32,  1.78it/s]Extractor Predicting: 57it [00:32,  1.75it/s]Extractor Predicting: 58it [00:33,  1.76it/s]Extractor Predicting: 59it [00:33,  1.79it/s]Extractor Predicting: 60it [00:34,  1.76it/s]Extractor Predicting: 61it [00:34,  1.78it/s]Extractor Predicting: 62it [00:35,  1.78it/s]Extractor Predicting: 63it [00:36,  1.76it/s]Extractor Predicting: 64it [00:36,  1.61it/s]Extractor Predicting: 65it [00:37,  1.63it/s]Extractor Predicting: 66it [00:37,  1.68it/s]Extractor Predicting: 67it [00:38,  1.66it/s]Extractor Predicting: 68it [00:39,  1.65it/s]Extractor Predicting: 69it [00:39,  1.62it/s]Extractor Predicting: 70it [00:40,  1.63it/s]Extractor Predicting: 71it [00:41,  1.64it/s]Extractor Predicting: 72it [00:41,  1.65it/s]Extractor Predicting: 73it [00:42,  1.67it/s]Extractor Predicting: 74it [00:42,  1.62it/s]Extractor Predicting: 75it [00:43,  1.87it/s]Extractor Predicting: 75it [00:43,  1.74it/s]
[INFO|configuration_utils.py:515] 2023-08-28 06:53:10,257 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:53:10,258 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:53:10,291 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:53:10,292 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 06:53:10,316 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:53:25,545 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 06:53:25,545 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 06:53:25,724 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:53:25,743 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:53:25,794 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:53:25,839 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:53:25,839 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:53:25,839 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:53:25,839 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:53:25,839 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:53:25,840 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 06:53:26,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:26,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:27,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:27,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:28,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:29,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:29,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:30,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:30,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:31,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:32,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:32,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:33,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:33,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:34,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:34,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:35,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:35,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:36,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:36,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:37,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:38,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<03:56, 12.42s/it][WARNING|generation_utils.py:914] 2023-08-28 06:53:38,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:39,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:39,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:40,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:40,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:41,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:42,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:43,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:43,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:44,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:44,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:45,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:45,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:46,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:46,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:47,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:48,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:48,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:49,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:49,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:50,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:50,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:51,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:51,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:52,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:26<04:05, 13.62s/it][WARNING|generation_utils.py:914] 2023-08-28 06:53:53,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:53,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:54,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:55,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:55,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:56,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:57,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:58,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:58,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:59,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:00,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:01,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:01,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:02,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:02,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:03,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:04,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:04,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:05,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:06,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:07,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:07,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:08,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:08,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:09,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:10,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:10,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:45<04:28, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-28 06:54:11,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:12,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:12,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:13,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:14,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:14,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:15,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:15,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:16,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:16,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:17,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:18,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:18,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:19,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:20,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:20,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:21,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:21,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:22,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:23,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:23,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:24,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:25,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:59<04:03, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 06:54:25,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:26,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:26,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:27,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:28,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:29,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:29,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:30,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:30,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:31,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:31,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:32,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:33,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:33,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:34,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:35,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:35,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:36,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:36,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:37,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:38,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:38,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:39,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:39,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:40,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:40,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:15<03:51, 15.44s/it][WARNING|generation_utils.py:914] 2023-08-28 06:54:41,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:42,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:42,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:43,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:43,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:44,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:45,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:45,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:46,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:47,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:47,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:48,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:48,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:49,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:50,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:50,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:51,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:51,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:52,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:53,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:53,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:54,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:54,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:55,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:56,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:56,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:57,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:31<03:40, 15.73s/it][WARNING|generation_utils.py:914] 2023-08-28 06:54:57,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:58,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:59,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:59,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:00,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:00,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:01,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:02,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:03,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:03,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:04,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:04,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:05,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:05,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:06,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:07,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:07,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:08,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:08,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:09,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:09,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:10,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:11,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:45<03:17, 15.16s/it][WARNING|generation_utils.py:914] 2023-08-28 06:55:11,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:12,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:13,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:13,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:14,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:14,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:15,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:16,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:16,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:17,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:17,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:18,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:19,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:19,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:20,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:20,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:21,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:21,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:22,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:23,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:23,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:24,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:25,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:25,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:26,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:00<03:01, 15.09s/it][WARNING|generation_utils.py:914] 2023-08-28 06:55:26,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:27,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:27,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:28,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:29,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:29,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:30,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:31,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:31,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:32,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:32,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:33,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:34,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:34,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:35,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:35,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:36,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:37,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:37,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:38,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:38,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:39,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:40,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:40,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:41,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:16<02:46, 15.16s/it][WARNING|generation_utils.py:914] 2023-08-28 06:55:42,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:42,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:43,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:43,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:44,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:45,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:45,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:46,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:47,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:47,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:48,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:48,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:49,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:49,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:50,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:51,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:51,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:52,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:52,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:53,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:54,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:54,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:55,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:55,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:30<02:29, 14.99s/it][WARNING|generation_utils.py:914] 2023-08-28 06:55:56,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:57,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:58,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:58,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:55:59,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:00,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:00,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:01,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:01,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:02,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:03,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:03,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:04,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:05,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:05,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:06,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:06,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:07,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:08,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:08,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:09,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:10,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:10,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:45<02:13, 14.87s/it][WARNING|generation_utils.py:914] 2023-08-28 06:56:11,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:11,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:12,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:13,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:13,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:14,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:14,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:15,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:16,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:17,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:17,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:18,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:19,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:19,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:20,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:20,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:21,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:22,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:22,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:23,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:23,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:24,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:25,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:59<01:58, 14.80s/it][WARNING|generation_utils.py:914] 2023-08-28 06:56:26,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:26,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:27,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:27,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:28,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:28,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:29,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:30,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:30,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:31,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:31,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:32,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:33,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:33,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:34,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:35,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:35,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:36,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:36,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:37,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:38,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:38,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:39,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:13<01:41, 14.48s/it][WARNING|generation_utils.py:914] 2023-08-28 06:56:39,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:40,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:40,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:41,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:42,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:42,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:43,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:43,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:44,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:44,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:45,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:46,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:46,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:47,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:47,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:48,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:48,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:49,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:50,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:51,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:51,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:52,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:52,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:27<01:25, 14.22s/it][WARNING|generation_utils.py:914] 2023-08-28 06:56:53,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:53,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:54,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:55,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:55,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:56,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:56,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:57,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:58,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:58,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:59,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:56:59,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:00,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:01,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:01,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:02,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:02,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:03,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:03,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:04,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:05,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:39<01:08, 13.71s/it][WARNING|generation_utils.py:914] 2023-08-28 06:57:05,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:06,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:07,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:07,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:08,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:09,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:09,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:10,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:11,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:11,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:12,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:13,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:13,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:14,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:15,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:15,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:16,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:16,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:17,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:18,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:18,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:19,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:20,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:20,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:21,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:22,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:22,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:23,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:23,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:58<01:00, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 06:57:24,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:25,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:25,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:26,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:26,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:27,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:28,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:28,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:29,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:29,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:30,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:31,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:31,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:32,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:32,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:33,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:33,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:34,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:35,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:35,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:36,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:10<00:42, 14.33s/it][WARNING|generation_utils.py:914] 2023-08-28 06:57:36,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:37,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:38,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:38,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:39,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:39,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:40,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:40,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:41,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:42,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:42,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:43,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:43,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:44,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:45,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:45,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:46,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:46,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:47,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:48,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:48,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:49,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:49,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:24<00:28, 14.08s/it][WARNING|generation_utils.py:914] 2023-08-28 06:57:50,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:51,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:51,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:52,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:52,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:53,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:53,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:54,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:55,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:55,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:56,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:56,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:57,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:58,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:58,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:59,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:57:59,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:00,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:00,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:01,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:02,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:03,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:03,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:38<00:14, 14.12s/it][WARNING|generation_utils.py:914] 2023-08-28 06:58:04,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:05,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:05,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:06,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:07,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:07,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:08,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:09,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:09,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:10,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:10,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:11,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:12,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:12,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:13,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:14,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:14,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:15,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:15,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:16,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:17,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:17,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:18,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:19,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:58:19,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:54<00:00, 14.59s/it]Generating: 100%|██████████| 20/20 [04:54<00:00, 14.71s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:30,747 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:30,750 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:30,750 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:30,750 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:30,750 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:58:31,436 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:58:31,437 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:58:32,085 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:58:33,194 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:58:33,211 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:36,180 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:36,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:36,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:36,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:36,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:58:36,897 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:58:36,910 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:58:37,515 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:58:37,734 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:58:37,734 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 358, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7118055555555556, 'errors': {'', "('συῆνος', 'said to be the same as', '', 'They are known as a form of Greek σάρτικρος ( συῆνος ) , τεῖς ( συθροτος ) , τίνρας ( συῆλώηνος ) ; and of the forms σατος τικρος in Latin .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : student .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 331, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 503, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7391826923076923, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 303, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 466, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 531, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 576, 'raw': 800}
{'target': 600, 'success': 598, 'raw': 832}
{'target': 600, 'success': 620, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7175925925925926, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8233695652173914, 'errors': {'', "('', 'continent', 'Aegean Sea', 'The island of Aranet , located in the Aegean Sea , is one of two islands that the Greek island of Argos has in common with the Arabian sea .')"}}
['Relation : field of this occupation . Context : The main characters in the series are The New Guy , The New York City Police Detective and the Los Angeles Police Department . Head Entity : Los Angeles Police Department , Tail Entity : police .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Walter L. Cusack', 'field of this occupation', '', 'Walter L. Cusack ( June 23 , 1924 February 21 , 2002 ) was an English cricket er who played six Tests in England .')"}}
['Relation : field of work . Context : Following his graduation in 1957 , he founded the PSA ( now in association with B. C. ) , an academic association for linguistics , linguistics , and other fields , and served as a member of the Princeton University Research Committee . Head Entity : linguistics , Tail Entity : graduate .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.76875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n']
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n', 'Relation : given name . Context : James M. Young ( February 14 , 1821 October 19 , 1924 ) was an American poet and writer specializing in the arts . Head Entity : James M. Young , Tail Entity : named name .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8206521739130435, 'errors': {'', "('Doctor Who', 'given name', '', 'He is best known for his play as a character on Doctor Who , in which he played the name of the villainess in the Doctor Who special , The One and Only Thing .')", 'too many values to unpack (expected 2)', "('Joseph L. M. Rennelly', 'given name', '', 'Joseph L. M. Rennelly is a former New York City mayor who served as the 62nd governor of New York City .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Never Say Goodbye', 'lyrics by', '', 'After graduating from Berklee College of Music , he started to write a single called Never Say Goodbye that debuted at No. 1 on the Billboard 200 single Hot Hot Song in July 1991 .')", "('Never Let Me Go', 'lyrics by', '', 'She followed this up with her sixth studio album , Never Let Me Go , which is now a cult classic on the Billboard R&B Top 40 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Central railway', 'owned by', '', 'It was built between 1941 and 1943 under the direction of the Central railway company ( D.')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9077380952380952, 'errors': {'', "('GMAW', 'performer', '', 'He also performed with the band Pampers on the opening night of WMAW in Los Angeles and the opening night of the ABC drama GMAW in Atlanta .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 128, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 185, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 223, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 288, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 371, 'raw': 576}
{'target': 600, 'success': 393, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 433, 'raw': 672}
{'target': 600, 'success': 451, 'raw': 704}
{'target': 600, 'success': 471, 'raw': 736}
{'target': 600, 'success': 495, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 583, 'raw': 896}
{'target': 600, 'success': 601, 'raw': 928}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.6476293103448276, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : replaces .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 17944
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18044, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.60it/s]Extractor Estimating: 2it [00:01,  1.53it/s]Extractor Estimating: 3it [00:01,  1.62it/s]Extractor Estimating: 4it [00:02,  1.68it/s]Extractor Estimating: 5it [00:03,  1.56it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.66it/s]Extractor Estimating: 8it [00:04,  1.66it/s]Extractor Estimating: 9it [00:05,  1.66it/s]Extractor Estimating: 10it [00:06,  1.71it/s]Extractor Estimating: 11it [00:06,  1.73it/s]Extractor Estimating: 12it [00:07,  1.69it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.74it/s]Extractor Estimating: 15it [00:08,  1.71it/s]Extractor Estimating: 16it [00:09,  1.65it/s]Extractor Estimating: 17it [00:10,  1.68it/s]Extractor Estimating: 18it [00:10,  1.66it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:12,  1.64it/s]Extractor Estimating: 21it [00:12,  1.68it/s]Extractor Estimating: 22it [00:13,  1.73it/s]Extractor Estimating: 23it [00:13,  1.66it/s]Extractor Estimating: 24it [00:14,  1.70it/s]Extractor Estimating: 25it [00:15,  1.65it/s]Extractor Estimating: 26it [00:15,  1.58it/s]Extractor Estimating: 27it [00:16,  1.60it/s]Extractor Estimating: 28it [00:16,  1.63it/s]Extractor Estimating: 29it [00:17,  1.70it/s]Extractor Estimating: 30it [00:18,  1.72it/s]Extractor Estimating: 31it [00:18,  1.73it/s]Extractor Estimating: 32it [00:19,  1.77it/s]Extractor Estimating: 33it [00:19,  1.73it/s]Extractor Estimating: 34it [00:20,  1.69it/s]Extractor Estimating: 35it [00:20,  1.71it/s]Extractor Estimating: 36it [00:21,  1.72it/s]Extractor Estimating: 37it [00:22,  1.75it/s]Extractor Estimating: 38it [00:22,  1.72it/s]Extractor Estimating: 39it [00:23,  1.65it/s]Extractor Estimating: 40it [00:23,  1.69it/s]Extractor Estimating: 41it [00:24,  1.76it/s]Extractor Estimating: 42it [00:24,  1.76it/s]Extractor Estimating: 43it [00:25,  1.74it/s]Extractor Estimating: 44it [00:26,  1.75it/s]Extractor Estimating: 45it [00:26,  1.74it/s]Extractor Estimating: 46it [00:27,  1.72it/s]Extractor Estimating: 47it [00:27,  1.68it/s]Extractor Estimating: 48it [00:28,  1.73it/s]Extractor Estimating: 49it [00:29,  1.73it/s]Extractor Estimating: 50it [00:29,  1.74it/s]Extractor Estimating: 51it [00:30,  1.70it/s]Extractor Estimating: 52it [00:30,  1.70it/s]Extractor Estimating: 53it [00:31,  1.67it/s]Extractor Estimating: 54it [00:31,  1.70it/s]Extractor Estimating: 55it [00:32,  1.71it/s]Extractor Estimating: 56it [00:33,  1.62it/s]Extractor Estimating: 57it [00:33,  1.58it/s]Extractor Estimating: 58it [00:34,  1.60it/s]Extractor Estimating: 59it [00:35,  1.58it/s]Extractor Estimating: 60it [00:35,  1.59it/s]Extractor Estimating: 61it [00:36,  1.62it/s]Extractor Estimating: 62it [00:36,  1.63it/s]Extractor Estimating: 63it [00:37,  1.65it/s]Extractor Estimating: 64it [00:38,  1.70it/s]Extractor Estimating: 65it [00:38,  1.66it/s]Extractor Estimating: 66it [00:39,  1.64it/s]Extractor Estimating: 67it [00:40,  1.58it/s]Extractor Estimating: 68it [00:40,  1.59it/s]Extractor Estimating: 69it [00:41,  1.59it/s]Extractor Estimating: 70it [00:41,  1.58it/s]Extractor Estimating: 71it [00:42,  1.53it/s]Extractor Estimating: 72it [00:43,  1.52it/s]Extractor Estimating: 73it [00:43,  1.54it/s]Extractor Estimating: 74it [00:44,  1.58it/s]Extractor Estimating: 75it [00:45,  1.58it/s]Extractor Estimating: 76it [00:45,  1.48it/s]Extractor Estimating: 77it [00:46,  1.50it/s]Extractor Estimating: 78it [00:47,  1.51it/s]Extractor Estimating: 79it [00:47,  1.58it/s]Extractor Estimating: 80it [00:48,  1.55it/s]Extractor Estimating: 81it [00:49,  1.59it/s]Extractor Estimating: 82it [00:49,  1.60it/s]Extractor Estimating: 83it [00:50,  1.63it/s]Extractor Estimating: 84it [00:50,  1.61it/s]Extractor Estimating: 85it [00:51,  1.60it/s]Extractor Estimating: 86it [00:52,  1.61it/s]Extractor Estimating: 87it [00:52,  1.62it/s]Extractor Estimating: 88it [00:53,  1.61it/s]Extractor Estimating: 89it [00:54,  1.59it/s]Extractor Estimating: 90it [00:54,  1.58it/s]Extractor Estimating: 91it [00:55,  1.63it/s]Extractor Estimating: 92it [00:55,  1.60it/s]Extractor Estimating: 93it [00:56,  1.64it/s]Extractor Estimating: 94it [00:57,  1.59it/s]Extractor Estimating: 95it [00:57,  1.61it/s]Extractor Estimating: 96it [00:58,  1.61it/s]Extractor Estimating: 97it [00:59,  1.62it/s]Extractor Estimating: 98it [00:59,  1.62it/s]Extractor Estimating: 99it [01:00,  1.60it/s]Extractor Estimating: 100it [01:00,  1.55it/s]Extractor Estimating: 101it [01:01,  1.59it/s]Extractor Estimating: 102it [01:02,  1.59it/s]Extractor Estimating: 103it [01:02,  1.59it/s]Extractor Estimating: 104it [01:03,  1.55it/s]Extractor Estimating: 105it [01:04,  1.55it/s]Extractor Estimating: 106it [01:04,  1.58it/s]Extractor Estimating: 107it [01:05,  1.59it/s]Extractor Estimating: 108it [01:05,  1.63it/s]Extractor Estimating: 109it [01:06,  1.61it/s]Extractor Estimating: 110it [01:07,  1.61it/s]Extractor Estimating: 111it [01:07,  1.63it/s]Extractor Estimating: 112it [01:08,  1.62it/s]Extractor Estimating: 113it [01:08,  1.65it/s]Extractor Estimating: 114it [01:09,  1.61it/s]Extractor Estimating: 115it [01:10,  1.63it/s]Extractor Estimating: 116it [01:10,  1.65it/s]Extractor Estimating: 117it [01:11,  1.61it/s]Extractor Estimating: 118it [01:12,  1.64it/s]Extractor Estimating: 119it [01:12,  1.61it/s]Extractor Estimating: 120it [01:13,  1.63it/s]Extractor Estimating: 121it [01:13,  1.65it/s]Extractor Estimating: 122it [01:14,  1.63it/s]Extractor Estimating: 123it [01:15,  1.64it/s]Extractor Estimating: 124it [01:15,  1.64it/s]Extractor Estimating: 125it [01:16,  1.64it/s]Extractor Estimating: 126it [01:17,  1.61it/s]Extractor Estimating: 127it [01:17,  1.61it/s]Extractor Estimating: 128it [01:18,  1.63it/s]Extractor Estimating: 129it [01:18,  1.61it/s]Extractor Estimating: 130it [01:19,  1.59it/s]Extractor Estimating: 131it [01:20,  1.59it/s]Extractor Estimating: 132it [01:20,  1.63it/s]Extractor Estimating: 133it [01:21,  1.69it/s]Extractor Estimating: 134it [01:21,  1.68it/s]Extractor Estimating: 135it [01:22,  1.71it/s]Extractor Estimating: 136it [01:23,  1.65it/s]Extractor Estimating: 137it [01:23,  1.67it/s]Extractor Estimating: 138it [01:24,  1.61it/s]Extractor Estimating: 139it [01:25,  1.48it/s]Extractor Estimating: 140it [01:25,  1.52it/s]Extractor Estimating: 141it [01:26,  1.55it/s]Extractor Estimating: 142it [01:26,  1.62it/s]Extractor Estimating: 143it [01:27,  1.60it/s]Extractor Estimating: 144it [01:28,  1.58it/s]Extractor Estimating: 145it [01:28,  1.61it/s]Extractor Estimating: 146it [01:29,  1.61it/s]Extractor Estimating: 147it [01:30,  1.55it/s]Extractor Estimating: 148it [01:30,  1.56it/s]Extractor Estimating: 149it [01:31,  1.54it/s]Extractor Estimating: 150it [01:32,  1.59it/s]Extractor Estimating: 151it [01:32,  1.62it/s]Extractor Estimating: 152it [01:33,  1.59it/s]Extractor Estimating: 153it [01:33,  1.67it/s]Extractor Estimating: 154it [01:34,  1.66it/s]Extractor Estimating: 155it [01:34,  1.68it/s]Extractor Estimating: 156it [01:35,  1.68it/s]Extractor Estimating: 157it [01:36,  1.73it/s]Extractor Estimating: 158it [01:36,  1.62it/s]Extractor Estimating: 159it [01:37,  1.62it/s]Extractor Estimating: 160it [01:37,  1.69it/s]Extractor Estimating: 161it [01:38,  1.55it/s]Extractor Estimating: 162it [01:39,  1.64it/s]Extractor Estimating: 163it [01:39,  1.70it/s]Extractor Estimating: 164it [01:40,  1.74it/s]Extractor Estimating: 165it [01:40,  1.70it/s]Extractor Estimating: 166it [01:41,  1.74it/s]Extractor Estimating: 167it [01:42,  1.75it/s]Extractor Estimating: 168it [01:42,  1.78it/s]Extractor Estimating: 169it [01:43,  1.83it/s]Extractor Estimating: 170it [01:43,  1.77it/s]Extractor Estimating: 171it [01:44,  1.76it/s]Extractor Estimating: 172it [01:44,  1.79it/s]Extractor Estimating: 173it [01:45,  1.80it/s]Extractor Estimating: 174it [01:45,  1.81it/s]Extractor Estimating: 175it [01:46,  1.76it/s]Extractor Estimating: 176it [01:47,  1.72it/s]Extractor Estimating: 177it [01:47,  1.66it/s]Extractor Estimating: 178it [01:48,  1.69it/s]Extractor Estimating: 179it [01:49,  1.62it/s]Extractor Estimating: 180it [01:49,  1.63it/s]Extractor Estimating: 181it [01:50,  1.57it/s]Extractor Estimating: 182it [01:50,  1.61it/s]Extractor Estimating: 183it [01:51,  1.63it/s]Extractor Estimating: 184it [01:52,  1.62it/s]Extractor Estimating: 185it [01:52,  1.61it/s]Extractor Estimating: 186it [01:53,  1.61it/s]Extractor Estimating: 187it [01:54,  1.62it/s]Extractor Estimating: 188it [01:54,  1.66it/s]Extractor Estimating: 189it [01:55,  1.67it/s]Extractor Estimating: 190it [01:55,  1.67it/s]Extractor Estimating: 191it [01:56,  1.68it/s]Extractor Estimating: 192it [01:56,  1.65it/s]Extractor Estimating: 193it [01:57,  1.68it/s]Extractor Estimating: 194it [01:58,  1.65it/s]Extractor Estimating: 195it [01:58,  1.62it/s]Extractor Estimating: 196it [01:59,  1.61it/s]Extractor Estimating: 197it [02:00,  1.62it/s]Extractor Estimating: 198it [02:00,  1.61it/s]Extractor Estimating: 199it [02:01,  1.59it/s]Extractor Estimating: 200it [02:01,  1.64it/s]Extractor Estimating: 201it [02:02,  1.62it/s]Extractor Estimating: 202it [02:03,  1.62it/s]Extractor Estimating: 203it [02:03,  1.60it/s]Extractor Estimating: 204it [02:04,  1.64it/s]Extractor Estimating: 205it [02:05,  1.58it/s]Extractor Estimating: 206it [02:05,  1.55it/s]Extractor Estimating: 207it [02:06,  1.56it/s]Extractor Estimating: 208it [02:06,  1.58it/s]Extractor Estimating: 209it [02:07,  1.55it/s]Extractor Estimating: 210it [02:08,  1.56it/s]Extractor Estimating: 211it [02:08,  1.57it/s]Extractor Estimating: 212it [02:09,  1.57it/s]Extractor Estimating: 213it [02:10,  1.59it/s]Extractor Estimating: 214it [02:10,  1.61it/s]Extractor Estimating: 215it [02:11,  1.53it/s]Extractor Estimating: 216it [02:12,  1.55it/s]Extractor Estimating: 217it [02:12,  1.56it/s]Extractor Estimating: 218it [02:13,  1.62it/s]Extractor Estimating: 219it [02:13,  1.64it/s]Extractor Estimating: 220it [02:14,  1.61it/s]Extractor Estimating: 221it [02:15,  1.65it/s]Extractor Estimating: 222it [02:15,  1.62it/s]Extractor Estimating: 223it [02:16,  1.63it/s]Extractor Estimating: 224it [02:17,  1.60it/s]Extractor Estimating: 225it [02:17,  1.60it/s]Extractor Estimating: 226it [02:18,  1.61it/s]Extractor Estimating: 227it [02:18,  1.61it/s]Extractor Estimating: 228it [02:19,  1.63it/s]Extractor Estimating: 229it [02:20,  1.65it/s]Extractor Estimating: 230it [02:20,  1.58it/s]Extractor Estimating: 231it [02:21,  1.63it/s]Extractor Estimating: 232it [02:22,  1.57it/s]Extractor Estimating: 233it [02:22,  1.59it/s]Extractor Estimating: 234it [02:23,  1.60it/s]Extractor Estimating: 235it [02:23,  1.64it/s]Extractor Estimating: 236it [02:24,  1.67it/s]Extractor Estimating: 237it [02:25,  1.48it/s]Extractor Estimating: 238it [02:25,  1.51it/s]Extractor Estimating: 239it [02:26,  1.54it/s]Extractor Estimating: 240it [02:27,  1.59it/s]Extractor Estimating: 241it [02:27,  1.61it/s]Extractor Estimating: 242it [02:28,  1.61it/s]Extractor Estimating: 243it [02:28,  1.61it/s]Extractor Estimating: 244it [02:29,  1.59it/s]Extractor Estimating: 245it [02:30,  1.61it/s]Extractor Estimating: 246it [02:30,  1.62it/s]Extractor Estimating: 247it [02:31,  1.53it/s]Extractor Estimating: 248it [02:32,  1.60it/s]Extractor Estimating: 249it [02:32,  1.58it/s]Extractor Estimating: 250it [02:33,  1.63it/s]Extractor Estimating: 251it [02:33,  1.66it/s]Extractor Estimating: 252it [02:34,  1.62it/s]Extractor Estimating: 253it [02:35,  1.55it/s]Extractor Estimating: 254it [02:35,  1.49it/s]Extractor Estimating: 255it [02:36,  1.54it/s]Extractor Estimating: 256it [02:37,  1.58it/s]Extractor Estimating: 257it [02:37,  1.60it/s]Extractor Estimating: 258it [02:38,  1.61it/s]Extractor Estimating: 259it [02:39,  1.56it/s]Extractor Estimating: 260it [02:39,  1.55it/s]Extractor Estimating: 261it [02:40,  1.56it/s]Extractor Estimating: 262it [02:40,  1.58it/s]Extractor Estimating: 263it [02:41,  1.60it/s]Extractor Estimating: 264it [02:42,  1.57it/s]Extractor Estimating: 265it [02:42,  1.58it/s]Extractor Estimating: 266it [02:43,  1.56it/s]Extractor Estimating: 267it [02:44,  1.58it/s]Extractor Estimating: 268it [02:44,  1.56it/s]Extractor Estimating: 269it [02:45,  1.54it/s]Extractor Estimating: 270it [02:46,  1.58it/s]Extractor Estimating: 271it [02:46,  1.56it/s]Extractor Estimating: 272it [02:47,  1.58it/s]Extractor Estimating: 273it [02:48,  1.55it/s]Extractor Estimating: 274it [02:48,  1.55it/s]Extractor Estimating: 275it [02:49,  1.52it/s]Extractor Estimating: 276it [02:49,  1.58it/s]Extractor Estimating: 277it [02:50,  1.58it/s]Extractor Estimating: 278it [02:51,  1.55it/s]Extractor Estimating: 279it [02:51,  1.54it/s]Extractor Estimating: 280it [02:52,  1.54it/s]Extractor Estimating: 281it [02:53,  1.52it/s]Extractor Estimating: 282it [02:53,  1.57it/s]Extractor Estimating: 283it [02:54,  1.59it/s]Extractor Estimating: 284it [02:55,  1.59it/s]Extractor Estimating: 285it [02:55,  1.59it/s]Extractor Estimating: 286it [02:56,  1.56it/s]Extractor Estimating: 287it [02:56,  1.58it/s]Extractor Estimating: 288it [02:57,  1.56it/s]Extractor Estimating: 289it [02:58,  1.59it/s]Extractor Estimating: 290it [02:58,  1.61it/s]Extractor Estimating: 291it [02:59,  1.58it/s]Extractor Estimating: 292it [03:00,  1.58it/s]Extractor Estimating: 293it [03:00,  1.54it/s]Extractor Estimating: 294it [03:01,  1.53it/s]Extractor Estimating: 295it [03:02,  1.50it/s]Extractor Estimating: 296it [03:02,  1.52it/s]Extractor Estimating: 297it [03:03,  1.54it/s]Extractor Estimating: 298it [03:04,  1.51it/s]Extractor Estimating: 299it [03:04,  1.51it/s]Extractor Estimating: 300it [03:05,  1.56it/s]Extractor Estimating: 301it [03:06,  1.54it/s]Extractor Estimating: 302it [03:06,  1.54it/s]Extractor Estimating: 303it [03:07,  1.57it/s]Extractor Estimating: 304it [03:07,  1.58it/s]Extractor Estimating: 305it [03:08,  1.59it/s]Extractor Estimating: 306it [03:09,  1.61it/s]Extractor Estimating: 307it [03:09,  1.62it/s]Extractor Estimating: 308it [03:10,  1.61it/s]Extractor Estimating: 309it [03:10,  1.63it/s]Extractor Estimating: 310it [03:11,  1.63it/s]Extractor Estimating: 311it [03:12,  1.58it/s]Extractor Estimating: 312it [03:12,  1.57it/s]Extractor Estimating: 313it [03:13,  1.61it/s]Extractor Estimating: 314it [03:14,  1.52it/s]Extractor Estimating: 315it [03:14,  1.52it/s]Extractor Estimating: 316it [03:15,  1.52it/s]Extractor Estimating: 317it [03:16,  1.54it/s]Extractor Estimating: 318it [03:16,  1.45it/s]Extractor Estimating: 319it [03:17,  1.44it/s]Extractor Estimating: 320it [03:18,  1.53it/s]Extractor Estimating: 321it [03:18,  1.58it/s]Extractor Estimating: 322it [03:19,  1.61it/s]Extractor Estimating: 323it [03:20,  1.60it/s]Extractor Estimating: 324it [03:20,  1.60it/s]Extractor Estimating: 325it [03:21,  1.61it/s]Extractor Estimating: 326it [03:21,  1.65it/s]Extractor Estimating: 327it [03:22,  1.65it/s]Extractor Estimating: 328it [03:23,  1.66it/s]Extractor Estimating: 329it [03:23,  1.66it/s]Extractor Estimating: 330it [03:24,  1.68it/s]Extractor Estimating: 331it [03:24,  1.65it/s]Extractor Estimating: 332it [03:25,  1.65it/s]Extractor Estimating: 333it [03:26,  1.60it/s]Extractor Estimating: 334it [03:26,  1.65it/s]Extractor Estimating: 335it [03:27,  1.65it/s]Extractor Estimating: 336it [03:27,  1.63it/s]Extractor Estimating: 337it [03:28,  1.63it/s]Extractor Estimating: 338it [03:29,  1.63it/s]Extractor Estimating: 339it [03:29,  1.68it/s]Extractor Estimating: 340it [03:30,  1.68it/s]Extractor Estimating: 341it [03:30,  1.68it/s]Extractor Estimating: 342it [03:31,  1.71it/s]Extractor Estimating: 343it [03:32,  1.63it/s]Extractor Estimating: 344it [03:32,  1.65it/s]Extractor Estimating: 345it [03:33,  1.63it/s]Extractor Estimating: 346it [03:33,  1.68it/s]Extractor Estimating: 347it [03:34,  1.68it/s]Extractor Estimating: 348it [03:35,  1.70it/s]Extractor Estimating: 349it [03:35,  1.69it/s]Extractor Estimating: 350it [03:36,  1.70it/s]Extractor Estimating: 351it [03:36,  1.67it/s]Extractor Estimating: 352it [03:37,  1.66it/s]Extractor Estimating: 353it [03:38,  1.63it/s]Extractor Estimating: 354it [03:38,  1.61it/s]Extractor Estimating: 355it [03:39,  1.64it/s]Extractor Estimating: 356it [03:40,  1.59it/s]Extractor Estimating: 357it [03:40,  1.62it/s]Extractor Estimating: 358it [03:41,  1.63it/s]Extractor Estimating: 359it [03:41,  1.58it/s]Extractor Estimating: 360it [03:42,  1.63it/s]Extractor Estimating: 361it [03:43,  1.57it/s]Extractor Estimating: 362it [03:43,  1.61it/s]Extractor Estimating: 363it [03:44,  1.57it/s]Extractor Estimating: 364it [03:45,  1.60it/s]Extractor Estimating: 365it [03:45,  1.62it/s]Extractor Estimating: 366it [03:46,  1.56it/s]Extractor Estimating: 367it [03:46,  1.59it/s]Extractor Estimating: 368it [03:47,  1.67it/s]Extractor Estimating: 369it [03:48,  1.61it/s]Extractor Estimating: 370it [03:48,  1.56it/s]Extractor Estimating: 371it [03:49,  1.55it/s]Extractor Estimating: 372it [03:50,  1.53it/s]Extractor Estimating: 373it [03:50,  1.53it/s]Extractor Estimating: 374it [03:51,  1.57it/s]Extractor Estimating: 375it [03:51,  1.62it/s]Extractor Estimating: 376it [03:52,  1.53it/s]Extractor Estimating: 377it [03:53,  1.54it/s]Extractor Estimating: 378it [03:54,  1.54it/s]Extractor Estimating: 379it [03:54,  1.57it/s]Extractor Estimating: 380it [03:55,  1.56it/s]Extractor Estimating: 381it [03:55,  1.54it/s]Extractor Estimating: 382it [03:56,  1.55it/s]Extractor Estimating: 383it [03:57,  1.52it/s]Extractor Estimating: 384it [03:57,  1.47it/s]Extractor Estimating: 385it [03:58,  1.49it/s]Extractor Estimating: 386it [03:59,  1.53it/s]Extractor Estimating: 387it [03:59,  1.53it/s]Extractor Estimating: 388it [04:00,  1.56it/s]Extractor Estimating: 389it [04:01,  1.54it/s]Extractor Estimating: 390it [04:01,  1.49it/s]Extractor Estimating: 391it [04:02,  1.54it/s]Extractor Estimating: 392it [04:03,  1.56it/s]Extractor Estimating: 393it [04:03,  1.49it/s]Extractor Estimating: 394it [04:04,  1.48it/s]Extractor Estimating: 395it [04:05,  1.50it/s]Extractor Estimating: 396it [04:05,  1.54it/s]Extractor Estimating: 397it [04:06,  1.53it/s]Extractor Estimating: 398it [04:07,  1.52it/s]Extractor Estimating: 399it [04:07,  1.50it/s]Extractor Estimating: 400it [04:08,  1.48it/s]Extractor Estimating: 401it [04:09,  1.35it/s]Extractor Estimating: 402it [04:10,  1.44it/s]Extractor Estimating: 403it [04:10,  1.49it/s]Extractor Estimating: 404it [04:11,  1.53it/s]Extractor Estimating: 405it [04:11,  1.55it/s]Extractor Estimating: 406it [04:12,  1.57it/s]Extractor Estimating: 407it [04:13,  1.57it/s]Extractor Estimating: 408it [04:13,  1.56it/s]Extractor Estimating: 409it [04:14,  1.55it/s]Extractor Estimating: 410it [04:15,  1.57it/s]Extractor Estimating: 411it [04:15,  1.60it/s]Extractor Estimating: 412it [04:16,  1.62it/s]Extractor Estimating: 413it [04:16,  1.61it/s]Extractor Estimating: 414it [04:17,  1.57it/s]Extractor Estimating: 415it [04:18,  1.53it/s]Extractor Estimating: 416it [04:18,  1.57it/s]Extractor Estimating: 417it [04:19,  1.59it/s]Extractor Estimating: 418it [04:20,  1.62it/s]Extractor Estimating: 419it [04:20,  1.63it/s]Extractor Estimating: 420it [04:21,  1.56it/s]Extractor Estimating: 421it [04:21,  1.61it/s]Extractor Estimating: 422it [04:22,  1.59it/s]Extractor Estimating: 423it [04:23,  1.63it/s]Extractor Estimating: 424it [04:23,  1.67it/s]Extractor Estimating: 425it [04:24,  1.66it/s]Extractor Estimating: 426it [04:24,  1.61it/s]Extractor Estimating: 427it [04:25,  1.64it/s]Extractor Estimating: 428it [04:26,  1.64it/s]Extractor Estimating: 429it [04:26,  1.66it/s]Extractor Estimating: 430it [04:27,  1.64it/s]Extractor Estimating: 431it [04:27,  1.67it/s]Extractor Estimating: 432it [04:28,  1.67it/s]Extractor Estimating: 433it [04:29,  1.66it/s]Extractor Estimating: 434it [04:29,  1.61it/s]Extractor Estimating: 435it [04:30,  1.61it/s]Extractor Estimating: 436it [04:31,  1.59it/s]Extractor Estimating: 437it [04:31,  1.60it/s]Extractor Estimating: 438it [04:32,  1.56it/s]Extractor Estimating: 439it [04:33,  1.51it/s]Extractor Estimating: 440it [04:33,  1.57it/s]Extractor Estimating: 441it [04:34,  1.56it/s]Extractor Estimating: 442it [04:34,  1.61it/s]Extractor Estimating: 443it [04:35,  1.61it/s]Extractor Estimating: 444it [04:36,  1.62it/s]Extractor Estimating: 445it [04:36,  1.62it/s]Extractor Estimating: 446it [04:37,  1.57it/s]Extractor Estimating: 447it [04:38,  1.59it/s]Extractor Estimating: 448it [04:38,  1.57it/s]Extractor Estimating: 449it [04:39,  1.60it/s]Extractor Estimating: 450it [04:39,  1.63it/s]Extractor Estimating: 451it [04:40,  1.60it/s]Extractor Estimating: 452it [04:41,  1.59it/s]Extractor Estimating: 453it [04:41,  1.58it/s]Extractor Estimating: 454it [04:42,  1.53it/s]Extractor Estimating: 455it [04:43,  1.56it/s]Extractor Estimating: 456it [04:43,  1.55it/s]Extractor Estimating: 457it [04:44,  1.53it/s]Extractor Estimating: 458it [04:45,  1.51it/s]Extractor Estimating: 459it [04:45,  1.49it/s]Extractor Estimating: 460it [04:46,  1.51it/s]Extractor Estimating: 461it [04:47,  1.57it/s]Extractor Estimating: 462it [04:47,  1.53it/s]Extractor Estimating: 463it [04:48,  1.46it/s]Extractor Estimating: 464it [04:49,  1.49it/s]Extractor Estimating: 465it [04:49,  1.51it/s]Extractor Estimating: 466it [04:50,  1.52it/s]Extractor Estimating: 467it [04:51,  1.58it/s]Extractor Estimating: 468it [04:51,  1.61it/s]Extractor Estimating: 469it [04:52,  1.58it/s]Extractor Estimating: 470it [04:52,  1.63it/s]Extractor Estimating: 471it [04:53,  1.54it/s]Extractor Estimating: 472it [04:54,  1.56it/s]Extractor Estimating: 473it [04:54,  1.45it/s]Extractor Estimating: 474it [04:55,  1.47it/s]Extractor Estimating: 475it [04:56,  1.50it/s]Extractor Estimating: 476it [04:57,  1.39it/s]Extractor Estimating: 477it [04:57,  1.42it/s]Extractor Estimating: 478it [04:58,  1.46it/s]Extractor Estimating: 479it [04:59,  1.53it/s]Extractor Estimating: 480it [04:59,  1.53it/s]Extractor Estimating: 481it [05:00,  1.53it/s]Extractor Estimating: 482it [05:00,  1.53it/s]Extractor Estimating: 483it [05:01,  1.55it/s]Extractor Estimating: 484it [05:02,  1.56it/s]Extractor Estimating: 485it [05:02,  1.55it/s]Extractor Estimating: 486it [05:03,  1.56it/s]Extractor Estimating: 487it [05:04,  1.57it/s]Extractor Estimating: 488it [05:04,  1.61it/s]Extractor Estimating: 489it [05:05,  1.57it/s]Extractor Estimating: 490it [05:06,  1.49it/s]Extractor Estimating: 491it [05:06,  1.55it/s]Extractor Estimating: 492it [05:07,  1.55it/s]Extractor Estimating: 493it [05:07,  1.61it/s]Extractor Estimating: 494it [05:08,  1.61it/s]Extractor Estimating: 495it [05:09,  1.56it/s]Extractor Estimating: 496it [05:09,  1.54it/s]Extractor Estimating: 497it [05:10,  1.57it/s]Extractor Estimating: 498it [05:11,  1.55it/s]Extractor Estimating: 499it [05:11,  1.52it/s]Extractor Estimating: 500it [05:12,  1.83it/s]Extractor Estimating: 500it [05:12,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:10,833 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:10,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:10,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:10,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:10,855 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:04:11,312 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:04:11,313 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:04:12,025 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:04:13,128 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:04:13,128 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:16,210 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:16,245 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:16,245 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:16,245 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:04:16,246 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:04:17,179 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:04:17,180 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:04:17,546 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:04:17,796 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:04:17,796 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 10:07:03,177 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 10:07:03,388 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 10537 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 29242
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 29342, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=29342, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.992, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.990, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.008, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.996, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 60, avg_time 0.994, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 160, avg_time 2.294, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 260, avg_time 1.008, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 360, avg_time 0.994, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 20, avg_time 0.986, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 120, avg_time 0.999, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 220, avg_time 2.293, loss:nan
g_step 1200, step 320, avg_time 1.004, loss:nan
g_step 1300, step 420, avg_time 0.991, loss:nan
g_step 1400, step 80, avg_time 0.981, loss:nan
g_step 1500, step 180, avg_time 1.001, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 280, avg_time 2.273, loss:nan
g_step 1700, step 380, avg_time 0.994, loss:nan
g_step 1800, step 40, avg_time 0.977, loss:nan
g_step 1900, step 140, avg_time 0.989, loss:nan
g_step 2000, step 240, avg_time 1.002, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 340, avg_time 2.283, loss:nan
g_step 2200, step 440, avg_time 1.002, loss:nan
g_step 2300, step 100, avg_time 1.001, loss:nan
g_step 2400, step 200, avg_time 0.990, loss:nan
g_step 2500, step 300, avg_time 0.992, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 400, avg_time 2.288, loss:nan
g_step 2700, step 60, avg_time 0.981, loss:nan
g_step 2800, step 160, avg_time 1.000, loss:nan
g_step 2900, step 260, avg_time 0.989, loss:nan
g_step 3000, step 360, avg_time 1.007, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 20, avg_time 2.286, loss:nan
g_step 3200, step 120, avg_time 0.996, loss:nan
g_step 3300, step 220, avg_time 0.997, loss:nan
g_step 3400, step 320, avg_time 1.001, loss:nan
g_step 3500, step 420, avg_time 0.990, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 80, avg_time 2.291, loss:nan
g_step 3700, step 180, avg_time 0.997, loss:nan
g_step 3800, step 280, avg_time 1.001, loss:nan
g_step 3900, step 380, avg_time 1.002, loss:nan
g_step 4000, step 40, avg_time 0.980, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 140, avg_time 2.294, loss:nan
g_step 4200, step 240, avg_time 0.988, loss:nan
g_step 4300, step 340, avg_time 1.004, loss:nan
g_step 4400, step 440, avg_time 1.000, loss:nan
g_step 4500, step 100, avg_time 0.996, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 200, avg_time 2.296, loss:nan
g_step 4700, step 300, avg_time 0.990, loss:nan
g_step 4800, step 400, avg_time 1.006, loss:nan
g_step 4900, step 60, avg_time 0.996, loss:nan
g_step 5000, step 160, avg_time 0.997, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 260, avg_time 2.273, loss:nan
g_step 5200, step 360, avg_time 1.011, loss:nan
g_step 5300, step 20, avg_time 0.982, loss:nan
g_step 5400, step 120, avg_time 1.001, loss:nan
g_step 5500, step 220, avg_time 1.001, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 320, avg_time 2.282, loss:nan
g_step 5700, step 420, avg_time 1.003, loss:nan
g_step 5800, step 80, avg_time 0.987, loss:nan
g_step 5900, step 180, avg_time 1.003, loss:nan
g_step 6000, step 280, avg_time 0.991, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 380, avg_time 2.288, loss:nan
g_step 6200, step 40, avg_time 1.015, loss:nan
g_step 6300, step 140, avg_time 0.994, loss:nan
g_step 6400, step 240, avg_time 1.001, loss:nan
g_step 6500, step 340, avg_time 1.006, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 440, avg_time 2.262, loss:nan
g_step 6700, step 100, avg_time 1.002, loss:nan
g_step 6800, step 200, avg_time 0.992, loss:nan
g_step 6900, step 300, avg_time 0.995, loss:nan
g_step 7000, step 400, avg_time 0.984, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 60, avg_time 2.284, loss:nan
g_step 7200, step 160, avg_time 0.992, loss:nan
g_step 7300, step 260, avg_time 1.009, loss:nan
g_step 7400, step 360, avg_time 0.992, loss:nan
g_step 7500, step 20, avg_time 0.993, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 120, avg_time 2.286, loss:nan
g_step 7700, step 220, avg_time 0.992, loss:nan
g_step 7800, step 320, avg_time 1.004, loss:nan
g_step 7900, step 420, avg_time 0.992, loss:nan
g_step 8000, step 80, avg_time 1.003, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 180, avg_time 2.284, loss:nan
g_step 8200, step 280, avg_time 0.990, loss:nan
g_step 8300, step 380, avg_time 1.004, loss:nan
g_step 8400, step 40, avg_time 0.990, loss:nan
g_step 8500, step 140, avg_time 1.005, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 240, avg_time 2.284, loss:nan
g_step 8700, step 340, avg_time 0.999, loss:nan
g_step 8800, step 440, avg_time 0.988, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:07:03 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:07:03 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-07-03_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:07:04 - WARNING - datasets.builder -   Using custom data configuration default-b1ce3e058e5402bb
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b1ce3e058e5402bb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:07:06,643 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:07:06,672 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:07:06,672 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:07:06,673 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:07:06,809 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:07:06,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:07:06,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:07:06,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:07:06,892 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:07:06,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:07:06,892 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:07:07,367 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:07:10,462 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:07:10,478 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b1ce3e058e5402bb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  3.08ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.91ba/s] 27%|██▋       | 3/11 [00:00<00:02,  3.32ba/s] 36%|███▋      | 4/11 [00:01<00:01,  3.72ba/s] 45%|████▌     | 5/11 [00:01<00:01,  3.99ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.18ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.31ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.39ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.46ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.51ba/s]100%|██████████| 11/11 [00:02<00:00,  5.14ba/s]100%|██████████| 11/11 [00:02<00:00,  4.32ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.51ba/s] 40%|████      | 2/5 [00:00<00:00,  3.90ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.20ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.35ba/s]100%|██████████| 5/5 [00:01<00:00,  4.64ba/s]100%|██████████| 5/5 [00:01<00:00,  4.37ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  5.84ba/s] 27%|██▋       | 3/11 [00:00<00:00,  9.08ba/s] 45%|████▌     | 5/11 [00:00<00:00, 10.15ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 10.69ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.83ba/s]100%|██████████| 11/11 [00:01<00:00, 11.84ba/s]100%|██████████| 11/11 [00:01<00:00, 10.81ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.48ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.19ba/s]100%|██████████| 5/5 [00:00<00:00,  9.75ba/s]100%|██████████| 5/5 [00:00<00:00,  8.85ba/s]
[INFO|trainer.py:414] 2023-08-28 10:07:16,917 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:07:17,047 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:07:17,047 >>   Num examples = 10600
[INFO|trainer.py:1149] 2023-08-28 10:07:17,047 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:07:17,047 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:07:17,047 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:07:17,047 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:07:17,047 >>   Total optimization steps = 830
  0%|          | 0/830 [00:00<?, ?it/s]  0%|          | 1/830 [00:00<03:55,  3.52it/s]  0%|          | 2/830 [00:00<03:49,  3.60it/s]  0%|          | 3/830 [00:00<03:47,  3.63it/s]  0%|          | 4/830 [00:01<03:48,  3.62it/s]  1%|          | 5/830 [00:01<03:48,  3.61it/s]  1%|          | 6/830 [00:01<03:48,  3.61it/s]  1%|          | 7/830 [00:01<03:48,  3.61it/s]  1%|          | 8/830 [00:02<03:47,  3.61it/s]  1%|          | 9/830 [00:02<03:47,  3.60it/s]  1%|          | 10/830 [00:02<03:47,  3.60it/s]  1%|▏         | 11/830 [00:03<03:47,  3.60it/s]  1%|▏         | 12/830 [00:03<03:54,  3.48it/s]  2%|▏         | 13/830 [00:03<03:52,  3.52it/s]  2%|▏         | 14/830 [00:03<03:50,  3.55it/s]  2%|▏         | 15/830 [00:04<03:48,  3.56it/s]  2%|▏         | 16/830 [00:04<03:48,  3.57it/s]  2%|▏         | 17/830 [00:04<03:47,  3.58it/s]  2%|▏         | 18/830 [00:05<03:46,  3.58it/s]  2%|▏         | 19/830 [00:05<03:46,  3.59it/s]  2%|▏         | 20/830 [00:05<03:45,  3.59it/s]  3%|▎         | 21/830 [00:05<03:45,  3.59it/s]  3%|▎         | 22/830 [00:06<03:44,  3.60it/s]  3%|▎         | 23/830 [00:06<03:49,  3.52it/s]  3%|▎         | 24/830 [00:06<03:47,  3.55it/s]  3%|▎         | 25/830 [00:06<03:46,  3.56it/s]  3%|▎         | 26/830 [00:07<03:45,  3.57it/s]  3%|▎         | 27/830 [00:07<03:44,  3.57it/s]  3%|▎         | 28/830 [00:07<03:44,  3.58it/s]  3%|▎         | 29/830 [00:08<03:43,  3.58it/s]  4%|▎         | 30/830 [00:08<03:43,  3.58it/s]  4%|▎         | 31/830 [00:08<03:42,  3.59it/s]  4%|▍         | 32/830 [00:08<03:42,  3.59it/s]  4%|▍         | 33/830 [00:09<03:41,  3.60it/s]  4%|▍         | 34/830 [00:09<03:53,  3.41it/s]  4%|▍         | 35/830 [00:09<03:49,  3.46it/s]  4%|▍         | 36/830 [00:10<03:47,  3.50it/s]  4%|▍         | 37/830 [00:10<03:45,  3.52it/s]  5%|▍         | 38/830 [00:10<03:43,  3.54it/s]  5%|▍         | 39/830 [00:10<03:42,  3.56it/s]  5%|▍         | 40/830 [00:11<03:41,  3.57it/s]  5%|▍         | 41/830 [00:11<03:40,  3.58it/s]  5%|▌         | 42/830 [00:11<03:39,  3.58it/s]  5%|▌         | 43/830 [00:12<03:39,  3.59it/s]  5%|▌         | 44/830 [00:12<03:39,  3.59it/s]  5%|▌         | 45/830 [00:12<03:49,  3.43it/s]  6%|▌         | 46/830 [00:12<03:45,  3.47it/s]  6%|▌         | 47/830 [00:13<03:43,  3.51it/s]  6%|▌         | 48/830 [00:13<03:41,  3.53it/s]  6%|▌         | 49/830 [00:13<03:40,  3.55it/s]  6%|▌         | 50/830 [00:14<03:39,  3.56it/s]  6%|▌         | 51/830 [00:14<03:38,  3.57it/s]  6%|▋         | 52/830 [00:14<03:37,  3.57it/s]  6%|▋         | 53/830 [00:14<03:37,  3.58it/s]  7%|▋         | 54/830 [00:15<03:36,  3.58it/s]  7%|▋         | 55/830 [00:15<03:35,  3.59it/s]  7%|▋         | 56/830 [00:15<03:41,  3.50it/s]  7%|▋         | 57/830 [00:16<03:39,  3.52it/s]  7%|▋         | 58/830 [00:16<03:38,  3.54it/s]  7%|▋         | 59/830 [00:16<03:36,  3.56it/s]  7%|▋         | 60/830 [00:16<03:35,  3.57it/s]  7%|▋         | 61/830 [00:17<03:34,  3.58it/s]  7%|▋         | 62/830 [00:17<03:34,  3.58it/s]  8%|▊         | 63/830 [00:17<03:33,  3.59it/s]  8%|▊         | 64/830 [00:17<03:33,  3.59it/s]  8%|▊         | 65/830 [00:18<03:33,  3.59it/s]  8%|▊         | 66/830 [00:18<03:32,  3.59it/s]  8%|▊         | 67/830 [00:18<03:33,  3.58it/s]  8%|▊         | 68/830 [00:19<03:32,  3.58it/s]  8%|▊         | 69/830 [00:19<03:32,  3.58it/s]  8%|▊         | 70/830 [00:19<03:32,  3.58it/s]  9%|▊         | 71/830 [00:19<03:38,  3.48it/s]  9%|▊         | 72/830 [00:20<03:35,  3.51it/s]  9%|▉         | 73/830 [00:20<03:34,  3.53it/s]  9%|▉         | 74/830 [00:20<03:33,  3.55it/s]  9%|▉         | 75/830 [00:21<03:32,  3.56it/s]  9%|▉         | 76/830 [00:21<03:31,  3.57it/s]  9%|▉         | 77/830 [00:21<03:30,  3.57it/s]  9%|▉         | 78/830 [00:21<03:30,  3.58it/s] 10%|▉         | 79/830 [00:22<03:29,  3.58it/s] 10%|▉         | 80/830 [00:22<03:29,  3.58it/s] 10%|▉         | 81/830 [00:22<03:28,  3.58it/s] 10%|▉         | 82/830 [00:23<03:37,  3.44it/s] 10%|█         | 83/830 [00:23<03:34,  3.48it/s] 10%|█         | 84/830 [00:23<03:32,  3.51it/s] 10%|█         | 85/830 [00:23<03:30,  3.54it/s] 10%|█         | 86/830 [00:24<03:29,  3.55it/s] 10%|█         | 87/830 [00:24<03:28,  3.57it/s] 11%|█         | 88/830 [00:24<03:27,  3.57it/s] 11%|█         | 89/830 [00:25<03:27,  3.58it/s] 11%|█         | 90/830 [00:25<03:26,  3.58it/s] 11%|█         | 91/830 [00:25<03:26,  3.58it/s] 11%|█         | 92/830 [00:25<03:26,  3.58it/s] 11%|█         | 93/830 [00:26<03:34,  3.43it/s] 11%|█▏        | 94/830 [00:26<03:31,  3.47it/s] 11%|█▏        | 95/830 [00:26<03:29,  3.51it/s] 12%|█▏        | 96/830 [00:27<03:28,  3.53it/s] 12%|█▏        | 97/830 [00:27<03:26,  3.55it/s] 12%|█▏        | 98/830 [00:27<03:25,  3.56it/s] 12%|█▏        | 99/830 [00:27<03:25,  3.57it/s] 12%|█▏        | 100/830 [00:28<03:24,  3.57it/s] 12%|█▏        | 101/830 [00:28<03:23,  3.58it/s] 12%|█▏        | 102/830 [00:28<03:23,  3.58it/s] 12%|█▏        | 103/830 [00:28<03:22,  3.59it/s] 13%|█▎        | 104/830 [00:29<03:35,  3.38it/s] 13%|█▎        | 105/830 [00:29<03:30,  3.44it/s] 13%|█▎        | 106/830 [00:29<03:27,  3.48it/s] 13%|█▎        | 107/830 [00:30<03:25,  3.52it/s] 13%|█▎        | 108/830 [00:30<03:24,  3.54it/s] 13%|█▎        | 109/830 [00:30<03:23,  3.55it/s] 13%|█▎        | 110/830 [00:30<03:22,  3.56it/s] 13%|█▎        | 111/830 [00:31<03:21,  3.57it/s] 13%|█▎        | 112/830 [00:31<03:20,  3.57it/s] 14%|█▎        | 113/830 [00:31<03:20,  3.58it/s] 14%|█▎        | 114/830 [00:32<03:19,  3.58it/s] 14%|█▍        | 115/830 [00:32<03:32,  3.37it/s] 14%|█▍        | 116/830 [00:32<03:27,  3.44it/s] 14%|█▍        | 117/830 [00:32<03:24,  3.48it/s] 14%|█▍        | 118/830 [00:33<03:22,  3.52it/s] 14%|█▍        | 119/830 [00:33<03:21,  3.54it/s] 14%|█▍        | 120/830 [00:33<03:19,  3.55it/s] 15%|█▍        | 121/830 [00:34<03:18,  3.57it/s] 15%|█▍        | 122/830 [00:34<03:18,  3.57it/s] 15%|█▍        | 123/830 [00:34<03:17,  3.58it/s] 15%|█▍        | 124/830 [00:34<03:17,  3.58it/s] 15%|█▌        | 125/830 [00:35<03:16,  3.58it/s] 15%|█▌        | 126/830 [00:35<03:25,  3.43it/s] 15%|█▌        | 127/830 [00:35<03:22,  3.47it/s] 15%|█▌        | 128/830 [00:36<03:19,  3.51it/s] 16%|█▌        | 129/830 [00:36<03:17,  3.54it/s] 16%|█▌        | 130/830 [00:36<03:15,  3.58it/s] 16%|█▌        | 131/830 [00:36<03:14,  3.60it/s] 16%|█▌        | 132/830 [00:37<03:13,  3.61it/s] 16%|█▌        | 133/830 [00:37<03:12,  3.63it/s] 16%|█▌        | 134/830 [00:37<03:11,  3.63it/s] 16%|█▋        | 135/830 [00:37<03:11,  3.63it/s] 16%|█▋        | 136/830 [00:38<03:10,  3.64it/s] 17%|█▋        | 137/830 [00:38<03:20,  3.46it/s] 17%|█▋        | 138/830 [00:38<03:16,  3.52it/s] 17%|█▋        | 139/830 [00:39<03:14,  3.56it/s] 17%|█▋        | 140/830 [00:39<03:12,  3.58it/s] 17%|█▋        | 141/830 [00:39<03:11,  3.60it/s] 17%|█▋        | 142/830 [00:39<03:10,  3.61it/s] 17%|█▋        | 143/830 [00:40<03:09,  3.62it/s] 17%|█▋        | 144/830 [00:40<03:09,  3.63it/s] 17%|█▋        | 145/830 [00:40<03:08,  3.63it/s] 18%|█▊        | 146/830 [00:41<03:07,  3.64it/s] 18%|█▊        | 147/830 [00:41<03:07,  3.64it/s] 18%|█▊        | 148/830 [00:41<03:11,  3.57it/s] 18%|█▊        | 149/830 [00:41<03:09,  3.59it/s] 18%|█▊        | 150/830 [00:42<03:08,  3.61it/s] 18%|█▊        | 151/830 [00:42<03:07,  3.62it/s] 18%|█▊        | 152/830 [00:42<03:06,  3.63it/s] 18%|█▊        | 153/830 [00:43<03:06,  3.63it/s] 19%|█▊        | 154/830 [00:43<03:06,  3.63it/s] 19%|█▊        | 155/830 [00:43<03:05,  3.64it/s] 19%|█▉        | 156/830 [00:43<03:05,  3.64it/s] 19%|█▉        | 157/830 [00:44<03:10,  3.53it/s] 19%|█▉        | 158/830 [00:44<03:09,  3.55it/s] 19%|█▉        | 159/830 [00:44<03:13,  3.46it/s] 19%|█▉        | 160/830 [00:44<03:11,  3.50it/s] 19%|█▉        | 161/830 [00:45<03:09,  3.52it/s] 20%|█▉        | 162/830 [00:45<03:08,  3.55it/s] 20%|█▉        | 163/830 [00:45<03:07,  3.56it/s] 20%|█▉        | 164/830 [00:46<03:06,  3.57it/s] 20%|█▉        | 165/830 [00:46<04:00,  2.76it/s] 20%|██        | 166/830 [00:46<03:26,  3.22it/s][INFO|trainer.py:2140] 2023-08-28 10:08:03,901 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:08:03,901 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 10:08:03,902 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.98it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.75it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.03it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.21it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.80it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.27it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.44it/s][A
  7%|▋         | 43/608 [00:00<00:12, 44.98it/s][A
  8%|▊         | 48/608 [00:01<00:12, 44.98it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.19it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.32it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.48it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.38it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.55it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.41it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.04it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.77it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.81it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.07it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.26it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.40it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.55it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.60it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.43it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.07it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 44.90it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.83it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 44.98it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.19it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.34it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.53it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.59it/s][A
 28%|██▊       | 168/608 [00:03<00:10, 43.68it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 44.11it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.18it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.35it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.63it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.83it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.15it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.34it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.31it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.21it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.15it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.04it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 44.93it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 44.97it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.09it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.26it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.45it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.40it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.36it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.15it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.11it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.01it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.00it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.13it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.17it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.42it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.40it/s][A
 50%|████▉     | 303/608 [00:06<00:07, 41.86it/s][A
 51%|█████     | 308/608 [00:06<00:06, 42.94it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 43.66it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 44.10it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 44.33it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 44.59it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 44.87it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.03it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 44.87it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 44.96it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.17it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.23it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.26it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.21it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.15it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.11it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.18it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.15it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.21it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.24it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.25it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.25it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.26it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.16it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.07it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.19it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.20it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 44.89it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.04it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.09it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.15it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.17it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.17it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.09it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.17it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.16it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.16it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.19it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.27it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.17it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 44.94it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.25it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.22it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.18it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.23it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.21it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.27it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.23it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.22it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.15it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.20it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.24it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.27it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.25it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.18it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 41.13it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 42.45it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 43.37it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 43.98it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 44.40it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 44.68it/s][A
100%|██████████| 608/608 [00:13<00:00, 44.86it/s][A                                                 
                                                 [A 20%|██        | 166/830 [01:00<03:26,  3.22it/s]
100%|██████████| 608/608 [00:13<00:00, 44.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:08:17,844 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 10:08:18,064 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:08:21,746 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:08:21,941 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:08:22,040 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|██        | 167/830 [01:06<1:08:46,  6.22s/it] 20%|██        | 168/830 [01:07<49:00,  4.44s/it]   20%|██        | 169/830 [01:07<35:10,  3.19s/it] 20%|██        | 170/830 [01:07<25:30,  2.32s/it] 21%|██        | 171/830 [01:08<18:50,  1.72s/it] 21%|██        | 172/830 [01:08<14:04,  1.28s/it] 21%|██        | 173/830 [01:08<10:44,  1.02it/s] 21%|██        | 174/830 [01:08<08:24,  1.30it/s] 21%|██        | 175/830 [01:09<06:46,  1.61it/s] 21%|██        | 176/830 [01:09<05:38,  1.93it/s] 21%|██▏       | 177/830 [01:09<04:50,  2.25it/s] 21%|██▏       | 178/830 [01:09<04:16,  2.54it/s] 22%|██▏       | 179/830 [01:10<03:52,  2.80it/s] 22%|██▏       | 180/830 [01:10<03:36,  3.00it/s] 22%|██▏       | 181/830 [01:10<03:24,  3.17it/s] 22%|██▏       | 182/830 [01:11<03:22,  3.20it/s] 22%|██▏       | 183/830 [01:11<03:15,  3.32it/s] 22%|██▏       | 184/830 [01:11<03:09,  3.41it/s] 22%|██▏       | 185/830 [01:11<03:05,  3.47it/s] 22%|██▏       | 186/830 [01:12<03:02,  3.52it/s] 23%|██▎       | 187/830 [01:12<03:00,  3.55it/s] 23%|██▎       | 188/830 [01:12<02:59,  3.58it/s] 23%|██▎       | 189/830 [01:12<02:58,  3.59it/s] 23%|██▎       | 190/830 [01:13<02:57,  3.61it/s] 23%|██▎       | 191/830 [01:13<02:56,  3.62it/s] 23%|██▎       | 192/830 [01:13<02:56,  3.62it/s] 23%|██▎       | 193/830 [01:14<02:58,  3.56it/s] 23%|██▎       | 194/830 [01:14<02:57,  3.58it/s] 23%|██▎       | 195/830 [01:14<02:56,  3.60it/s] 24%|██▎       | 196/830 [01:14<02:55,  3.61it/s] 24%|██▎       | 197/830 [01:15<02:54,  3.62it/s] 24%|██▍       | 198/830 [01:15<02:54,  3.63it/s] 24%|██▍       | 199/830 [01:15<02:53,  3.63it/s] 24%|██▍       | 200/830 [01:16<02:53,  3.63it/s] 24%|██▍       | 201/830 [01:16<02:53,  3.63it/s] 24%|██▍       | 202/830 [01:16<02:52,  3.64it/s] 24%|██▍       | 203/830 [01:16<02:52,  3.64it/s] 25%|██▍       | 204/830 [01:17<02:57,  3.53it/s] 25%|██▍       | 205/830 [01:17<02:55,  3.56it/s] 25%|██▍       | 206/830 [01:17<02:54,  3.58it/s] 25%|██▍       | 207/830 [01:17<02:53,  3.60it/s] 25%|██▌       | 208/830 [01:18<02:52,  3.61it/s] 25%|██▌       | 209/830 [01:18<02:51,  3.62it/s] 25%|██▌       | 210/830 [01:18<02:51,  3.62it/s] 25%|██▌       | 211/830 [01:19<02:50,  3.63it/s] 26%|██▌       | 212/830 [01:19<02:50,  3.63it/s] 26%|██▌       | 213/830 [01:19<02:50,  3.63it/s] 26%|██▌       | 214/830 [01:19<02:49,  3.63it/s] 26%|██▌       | 215/830 [01:20<02:49,  3.63it/s] 26%|██▌       | 216/830 [01:20<02:49,  3.63it/s] 26%|██▌       | 217/830 [01:20<02:48,  3.63it/s] 26%|██▋       | 218/830 [01:21<02:48,  3.63it/s] 26%|██▋       | 219/830 [01:21<02:48,  3.63it/s] 27%|██▋       | 220/830 [01:21<02:52,  3.54it/s] 27%|██▋       | 221/830 [01:21<02:50,  3.56it/s] 27%|██▋       | 222/830 [01:22<02:49,  3.58it/s] 27%|██▋       | 223/830 [01:22<02:48,  3.60it/s] 27%|██▋       | 224/830 [01:22<02:47,  3.61it/s] 27%|██▋       | 225/830 [01:22<02:47,  3.62it/s] 27%|██▋       | 226/830 [01:23<02:46,  3.62it/s] 27%|██▋       | 227/830 [01:23<02:46,  3.62it/s] 27%|██▋       | 228/830 [01:23<02:46,  3.62it/s] 28%|██▊       | 229/830 [01:24<02:45,  3.63it/s] 28%|██▊       | 230/830 [01:24<02:45,  3.63it/s] 28%|██▊       | 231/830 [01:24<02:52,  3.48it/s] 28%|██▊       | 232/830 [01:24<02:49,  3.52it/s] 28%|██▊       | 233/830 [01:25<02:47,  3.55it/s] 28%|██▊       | 234/830 [01:25<02:46,  3.58it/s] 28%|██▊       | 235/830 [01:25<02:45,  3.59it/s] 28%|██▊       | 236/830 [01:26<02:45,  3.60it/s] 29%|██▊       | 237/830 [01:26<02:44,  3.61it/s] 29%|██▊       | 238/830 [01:26<02:43,  3.61it/s] 29%|██▉       | 239/830 [01:26<02:43,  3.62it/s] 29%|██▉       | 240/830 [01:27<02:43,  3.62it/s] 29%|██▉       | 241/830 [01:27<02:42,  3.62it/s] 29%|██▉       | 242/830 [01:27<02:47,  3.51it/s] 29%|██▉       | 243/830 [01:28<02:45,  3.54it/s] 29%|██▉       | 244/830 [01:28<02:44,  3.57it/s] 30%|██▉       | 245/830 [01:28<02:43,  3.58it/s] 30%|██▉       | 246/830 [01:28<02:42,  3.60it/s] 30%|██▉       | 247/830 [01:29<02:41,  3.60it/s] 30%|██▉       | 248/830 [01:29<02:41,  3.61it/s] 30%|███       | 249/830 [01:29<02:40,  3.61it/s] 30%|███       | 250/830 [01:29<02:40,  3.62it/s] 30%|███       | 251/830 [01:30<02:39,  3.62it/s] 30%|███       | 252/830 [01:30<02:39,  3.62it/s] 30%|███       | 253/830 [01:30<02:50,  3.39it/s] 31%|███       | 254/830 [01:31<02:46,  3.45it/s] 31%|███       | 255/830 [01:31<02:44,  3.50it/s] 31%|███       | 256/830 [01:31<02:42,  3.54it/s] 31%|███       | 257/830 [01:31<02:40,  3.57it/s] 31%|███       | 258/830 [01:32<02:39,  3.58it/s] 31%|███       | 259/830 [01:32<02:39,  3.59it/s] 31%|███▏      | 260/830 [01:32<02:38,  3.60it/s] 31%|███▏      | 261/830 [01:33<02:37,  3.61it/s] 32%|███▏      | 262/830 [01:33<02:37,  3.61it/s] 32%|███▏      | 263/830 [01:33<02:36,  3.62it/s] 32%|███▏      | 264/830 [01:33<02:46,  3.41it/s] 32%|███▏      | 265/830 [01:34<02:42,  3.47it/s] 32%|███▏      | 266/830 [01:34<02:40,  3.51it/s] 32%|███▏      | 267/830 [01:34<02:38,  3.55it/s] 32%|███▏      | 268/830 [01:35<02:37,  3.57it/s] 32%|███▏      | 269/830 [01:35<02:36,  3.58it/s] 33%|███▎      | 270/830 [01:35<02:35,  3.59it/s] 33%|███▎      | 271/830 [01:35<02:34,  3.61it/s] 33%|███▎      | 272/830 [01:36<02:34,  3.61it/s] 33%|███▎      | 273/830 [01:36<02:34,  3.62it/s] 33%|███▎      | 274/830 [01:36<02:33,  3.61it/s] 33%|███▎      | 275/830 [01:37<02:43,  3.39it/s] 33%|███▎      | 276/830 [01:37<02:40,  3.46it/s] 33%|███▎      | 277/830 [01:37<02:37,  3.51it/s] 33%|███▎      | 278/830 [01:37<02:35,  3.54it/s] 34%|███▎      | 279/830 [01:38<02:34,  3.56it/s] 34%|███▎      | 280/830 [01:38<02:33,  3.58it/s] 34%|███▍      | 281/830 [01:38<02:32,  3.59it/s] 34%|███▍      | 282/830 [01:38<02:32,  3.60it/s] 34%|███▍      | 283/830 [01:39<02:31,  3.61it/s] 34%|███▍      | 284/830 [01:39<02:31,  3.61it/s] 34%|███▍      | 285/830 [01:39<02:30,  3.61it/s] 34%|███▍      | 286/830 [01:40<02:39,  3.41it/s] 35%|███▍      | 287/830 [01:40<02:36,  3.47it/s] 35%|███▍      | 288/830 [01:40<02:34,  3.52it/s] 35%|███▍      | 289/830 [01:40<02:32,  3.55it/s] 35%|███▍      | 290/830 [01:41<02:31,  3.57it/s] 35%|███▌      | 291/830 [01:41<02:30,  3.59it/s] 35%|███▌      | 292/830 [01:41<02:29,  3.60it/s] 35%|███▌      | 293/830 [01:42<02:28,  3.61it/s] 35%|███▌      | 294/830 [01:42<02:28,  3.61it/s] 36%|███▌      | 295/830 [01:42<02:28,  3.61it/s] 36%|███▌      | 296/830 [01:42<02:27,  3.62it/s] 36%|███▌      | 297/830 [01:43<02:32,  3.48it/s] 36%|███▌      | 298/830 [01:43<02:30,  3.52it/s] 36%|███▌      | 299/830 [01:43<02:29,  3.55it/s] 36%|███▌      | 300/830 [01:44<02:28,  3.57it/s] 36%|███▋      | 301/830 [01:44<02:31,  3.48it/s] 36%|███▋      | 302/830 [01:44<02:29,  3.52it/s] 37%|███▋      | 303/830 [01:44<02:28,  3.55it/s] 37%|███▋      | 304/830 [01:45<02:27,  3.57it/s] 37%|███▋      | 305/830 [01:45<02:26,  3.59it/s] 37%|███▋      | 306/830 [01:45<02:25,  3.60it/s] 37%|███▋      | 307/830 [01:45<02:25,  3.60it/s] 37%|███▋      | 308/830 [01:46<02:29,  3.48it/s] 37%|███▋      | 309/830 [01:46<03:09,  2.75it/s] 37%|███▋      | 310/830 [01:47<02:55,  2.96it/s] 37%|███▋      | 311/830 [01:47<02:45,  3.13it/s] 38%|███▊      | 312/830 [01:47<02:38,  3.26it/s] 38%|███▊      | 313/830 [01:47<02:33,  3.36it/s] 38%|███▊      | 314/830 [01:48<02:30,  3.44it/s] 38%|███▊      | 315/830 [01:48<02:27,  3.49it/s] 38%|███▊      | 316/830 [01:48<02:25,  3.53it/s] 38%|███▊      | 317/830 [01:49<02:24,  3.55it/s] 38%|███▊      | 318/830 [01:49<02:23,  3.57it/s] 38%|███▊      | 319/830 [01:49<02:22,  3.59it/s] 39%|███▊      | 320/830 [01:49<02:21,  3.60it/s] 39%|███▊      | 321/830 [01:50<02:21,  3.61it/s] 39%|███▉      | 322/830 [01:50<02:20,  3.61it/s] 39%|███▉      | 323/830 [01:50<02:20,  3.61it/s] 39%|███▉      | 324/830 [01:50<02:20,  3.61it/s] 39%|███▉      | 325/830 [01:51<02:19,  3.62it/s] 39%|███▉      | 326/830 [01:51<02:19,  3.62it/s] 39%|███▉      | 327/830 [01:51<02:19,  3.62it/s] 40%|███▉      | 328/830 [01:52<02:18,  3.62it/s] 40%|███▉      | 329/830 [01:52<02:23,  3.50it/s] 40%|███▉      | 330/830 [01:52<02:21,  3.53it/s] 40%|███▉      | 331/830 [01:52<02:20,  3.56it/s] 40%|████      | 332/830 [01:53<02:05,  3.96it/s][INFO|trainer.py:2140] 2023-08-28 10:09:10,175 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:09:10,175 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 10:09:10,175 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.523, 'eval_samples_per_second': 359.682, 'eval_steps_per_second': 44.96, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.82it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.48it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.68it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.01it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.52it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.02it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.41it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.01it/s][A
  8%|▊         | 48/608 [00:01<00:12, 44.91it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.11it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.18it/s][A
 10%|█         | 63/608 [00:01<00:12, 45.27it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.43it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.56it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.43it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.20it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.91it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.92it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 44.43it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 44.77it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 44.98it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.11it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.21it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.15it/s][A
 21%|██        | 128/608 [00:02<00:10, 44.96it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 44.80it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.79it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 44.93it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.13it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.33it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.42it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.52it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.39it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.17it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.87it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.91it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.94it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.09it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.25it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.29it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.39it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.44it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.29it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.13it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 44.94it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.04it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 42.97it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 43.83it/s][A
 41%|████      | 248/608 [00:05<00:08, 44.34it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 44.76it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 44.96it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 44.98it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 44.87it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 44.87it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 44.68it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 44.85it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.04it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.18it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.38it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.39it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.34it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.09it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.04it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 44.93it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 44.95it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.09it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.15it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.37it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.39it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.35it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.22it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.03it/s][A
 61%|██████    | 368/608 [00:08<00:05, 44.96it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 43.43it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 44.08it/s][A
 63%|██████▎   | 383/608 [00:08<00:05, 44.41it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 44.73it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.03it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.14it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.12it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 44.93it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 44.82it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 44.88it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.05it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.16it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.30it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.15it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.45it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.27it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.11it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 44.96it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 44.91it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.03it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.22it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.33it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.37it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.38it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.22it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.13it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 44.98it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 42.53it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 43.46it/s][A
 85%|████████▌ | 518/608 [00:11<00:02, 44.10it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 44.54it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 44.78it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 44.82it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 44.82it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 44.74it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 44.58it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 44.66it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 44.92it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.17it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.39it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.38it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.41it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.21it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.02it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 44.85it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 44.91it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 44.93it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.12it/s][A                                                 
                                                 [A 40%|████      | 332/830 [02:06<02:05,  3.96it/s]
100%|██████████| 608/608 [00:13<00:00, 45.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:09:24,134 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 10:09:24,335 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:09:28,314 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:09:28,622 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:09:28,757 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332/special_tokens_map.json
 40%|████      | 333/830 [02:13<52:17,  6.31s/it] 40%|████      | 334/830 [02:13<37:13,  4.50s/it] 40%|████      | 335/830 [02:14<26:41,  3.24s/it] 40%|████      | 336/830 [02:14<19:20,  2.35s/it] 41%|████      | 337/830 [02:14<14:18,  1.74s/it] 41%|████      | 338/830 [02:15<10:41,  1.30s/it] 41%|████      | 339/830 [02:15<08:09,  1.00it/s] 41%|████      | 340/830 [02:15<06:22,  1.28it/s] 41%|████      | 341/830 [02:15<05:08,  1.58it/s] 41%|████      | 342/830 [02:16<04:16,  1.90it/s] 41%|████▏     | 343/830 [02:16<03:40,  2.21it/s] 41%|████▏     | 344/830 [02:16<03:14,  2.50it/s] 42%|████▏     | 345/830 [02:16<02:56,  2.75it/s] 42%|████▏     | 346/830 [02:17<02:44,  2.95it/s] 42%|████▏     | 347/830 [02:17<02:35,  3.11it/s] 42%|████▏     | 348/830 [02:17<02:34,  3.12it/s] 42%|████▏     | 349/830 [02:18<02:28,  3.24it/s] 42%|████▏     | 350/830 [02:18<02:23,  3.33it/s] 42%|████▏     | 351/830 [02:18<02:20,  3.40it/s] 42%|████▏     | 352/830 [02:18<02:18,  3.45it/s] 43%|████▎     | 353/830 [02:19<02:16,  3.49it/s] 43%|████▎     | 354/830 [02:19<02:15,  3.51it/s] 43%|████▎     | 355/830 [02:19<02:18,  3.43it/s] 43%|████▎     | 356/830 [02:20<02:16,  3.46it/s] 43%|████▎     | 357/830 [02:20<02:15,  3.49it/s] 43%|████▎     | 358/830 [02:20<02:14,  3.51it/s] 43%|████▎     | 359/830 [02:20<02:13,  3.53it/s] 43%|████▎     | 360/830 [02:21<02:12,  3.54it/s] 43%|████▎     | 361/830 [02:21<02:12,  3.55it/s] 44%|████▎     | 362/830 [02:21<02:11,  3.55it/s] 44%|████▎     | 363/830 [02:22<02:11,  3.56it/s] 44%|████▍     | 364/830 [02:22<02:10,  3.56it/s] 44%|████▍     | 365/830 [02:22<02:10,  3.56it/s] 44%|████▍     | 366/830 [02:22<02:12,  3.50it/s] 44%|████▍     | 367/830 [02:23<02:11,  3.52it/s] 44%|████▍     | 368/830 [02:23<02:10,  3.53it/s] 44%|████▍     | 369/830 [02:23<02:10,  3.54it/s] 45%|████▍     | 370/830 [02:24<02:09,  3.55it/s] 45%|████▍     | 371/830 [02:24<02:09,  3.56it/s] 45%|████▍     | 372/830 [02:24<02:08,  3.56it/s] 45%|████▍     | 373/830 [02:24<02:08,  3.56it/s] 45%|████▌     | 374/830 [02:25<02:07,  3.56it/s] 45%|████▌     | 375/830 [02:25<02:07,  3.57it/s] 45%|████▌     | 376/830 [02:25<02:07,  3.57it/s] 45%|████▌     | 377/830 [02:26<02:11,  3.44it/s] 46%|████▌     | 378/830 [02:26<02:10,  3.47it/s] 46%|████▌     | 379/830 [02:26<02:08,  3.50it/s] 46%|████▌     | 380/830 [02:26<02:07,  3.52it/s] 46%|████▌     | 381/830 [02:27<02:07,  3.53it/s] 46%|████▌     | 382/830 [02:27<02:06,  3.54it/s] 46%|████▌     | 383/830 [02:27<02:05,  3.55it/s] 46%|████▋     | 384/830 [02:28<02:05,  3.55it/s] 46%|████▋     | 385/830 [02:28<02:05,  3.56it/s] 47%|████▋     | 386/830 [02:28<02:04,  3.56it/s] 47%|████▋     | 387/830 [02:28<02:04,  3.56it/s] 47%|████▋     | 388/830 [02:29<02:07,  3.47it/s] 47%|████▋     | 389/830 [02:29<02:06,  3.50it/s] 47%|████▋     | 390/830 [02:29<02:05,  3.52it/s] 47%|████▋     | 391/830 [02:30<02:04,  3.53it/s] 47%|████▋     | 392/830 [02:30<02:03,  3.54it/s] 47%|████▋     | 393/830 [02:30<02:03,  3.54it/s] 47%|████▋     | 394/830 [02:30<02:02,  3.55it/s] 48%|████▊     | 395/830 [02:31<02:02,  3.55it/s] 48%|████▊     | 396/830 [02:31<02:02,  3.55it/s] 48%|████▊     | 397/830 [02:31<02:01,  3.56it/s] 48%|████▊     | 398/830 [02:31<02:01,  3.56it/s] 48%|████▊     | 399/830 [02:32<02:06,  3.39it/s] 48%|████▊     | 400/830 [02:32<02:04,  3.45it/s] 48%|████▊     | 401/830 [02:32<02:03,  3.48it/s] 48%|████▊     | 402/830 [02:33<02:02,  3.51it/s] 49%|████▊     | 403/830 [02:33<02:01,  3.53it/s] 49%|████▊     | 404/830 [02:33<02:00,  3.54it/s] 49%|████▉     | 405/830 [02:33<01:59,  3.55it/s] 49%|████▉     | 406/830 [02:34<01:59,  3.55it/s] 49%|████▉     | 407/830 [02:34<01:58,  3.55it/s] 49%|████▉     | 408/830 [02:34<01:58,  3.56it/s] 49%|████▉     | 409/830 [02:35<01:58,  3.56it/s] 49%|████▉     | 410/830 [02:35<02:07,  3.29it/s] 50%|████▉     | 411/830 [02:35<02:04,  3.37it/s] 50%|████▉     | 412/830 [02:36<02:01,  3.43it/s] 50%|████▉     | 413/830 [02:36<02:00,  3.47it/s] 50%|████▉     | 414/830 [02:36<01:58,  3.50it/s] 50%|█████     | 415/830 [02:36<01:58,  3.51it/s] 50%|█████     | 416/830 [02:37<01:57,  3.53it/s] 50%|█████     | 417/830 [02:37<01:56,  3.54it/s] 50%|█████     | 418/830 [02:37<01:56,  3.54it/s] 50%|█████     | 419/830 [02:38<01:55,  3.55it/s] 51%|█████     | 420/830 [02:38<01:55,  3.55it/s] 51%|█████     | 421/830 [02:38<02:02,  3.34it/s] 51%|█████     | 422/830 [02:38<01:59,  3.40it/s] 51%|█████     | 423/830 [02:39<01:58,  3.45it/s] 51%|█████     | 424/830 [02:39<01:56,  3.48it/s] 51%|█████     | 425/830 [02:39<01:55,  3.51it/s] 51%|█████▏    | 426/830 [02:40<01:54,  3.52it/s] 51%|█████▏    | 427/830 [02:40<01:54,  3.53it/s] 52%|█████▏    | 428/830 [02:40<01:53,  3.54it/s] 52%|█████▏    | 429/830 [02:40<01:52,  3.55it/s] 52%|█████▏    | 430/830 [02:41<01:52,  3.56it/s] 52%|█████▏    | 431/830 [02:41<01:51,  3.56it/s] 52%|█████▏    | 432/830 [02:41<01:57,  3.37it/s] 52%|█████▏    | 433/830 [02:42<01:55,  3.43it/s] 52%|█████▏    | 434/830 [02:42<01:54,  3.46it/s] 52%|█████▏    | 435/830 [02:42<01:53,  3.49it/s] 53%|█████▎    | 436/830 [02:42<01:52,  3.51it/s] 53%|█████▎    | 437/830 [02:43<01:51,  3.53it/s] 53%|█████▎    | 438/830 [02:43<01:50,  3.54it/s] 53%|█████▎    | 439/830 [02:43<01:50,  3.55it/s] 53%|█████▎    | 440/830 [02:44<01:49,  3.55it/s] 53%|█████▎    | 441/830 [02:44<01:51,  3.47it/s] 53%|█████▎    | 442/830 [02:44<01:51,  3.48it/s] 53%|█████▎    | 443/830 [02:44<01:53,  3.41it/s] 53%|█████▎    | 444/830 [02:45<01:51,  3.45it/s] 54%|█████▎    | 445/830 [02:45<01:50,  3.49it/s] 54%|█████▎    | 446/830 [02:45<01:49,  3.51it/s] 54%|█████▍    | 447/830 [02:46<01:48,  3.52it/s] 54%|█████▍    | 448/830 [02:46<01:48,  3.54it/s] 54%|█████▍    | 449/830 [02:46<02:17,  2.78it/s] 54%|█████▍    | 450/830 [02:47<02:08,  2.97it/s] 54%|█████▍    | 451/830 [02:47<02:01,  3.13it/s] 54%|█████▍    | 452/830 [02:47<01:56,  3.25it/s] 55%|█████▍    | 453/830 [02:47<01:55,  3.25it/s] 55%|█████▍    | 454/830 [02:48<01:52,  3.34it/s] 55%|█████▍    | 455/830 [02:48<01:50,  3.40it/s] 55%|█████▍    | 456/830 [02:48<01:48,  3.45it/s] 55%|█████▌    | 457/830 [02:49<01:47,  3.48it/s] 55%|█████▌    | 458/830 [02:49<01:46,  3.50it/s] 55%|█████▌    | 459/830 [02:49<01:45,  3.52it/s] 55%|█████▌    | 460/830 [02:49<01:44,  3.53it/s] 56%|█████▌    | 461/830 [02:50<01:44,  3.54it/s] 56%|█████▌    | 462/830 [02:50<01:47,  3.42it/s] 56%|█████▌    | 463/830 [02:50<01:46,  3.46it/s] 56%|█████▌    | 464/830 [02:51<01:44,  3.49it/s] 56%|█████▌    | 465/830 [02:51<01:44,  3.51it/s] 56%|█████▌    | 466/830 [02:51<01:43,  3.53it/s] 56%|█████▋    | 467/830 [02:51<01:42,  3.53it/s] 56%|█████▋    | 468/830 [02:52<01:42,  3.54it/s] 57%|█████▋    | 469/830 [02:52<01:41,  3.55it/s] 57%|█████▋    | 470/830 [02:52<01:41,  3.56it/s] 57%|█████▋    | 471/830 [02:53<01:40,  3.56it/s] 57%|█████▋    | 472/830 [02:53<01:40,  3.56it/s] 57%|█████▋    | 473/830 [02:53<01:43,  3.45it/s] 57%|█████▋    | 474/830 [02:53<01:42,  3.48it/s] 57%|█████▋    | 475/830 [02:54<01:41,  3.50it/s] 57%|█████▋    | 476/830 [02:54<01:40,  3.52it/s] 57%|█████▋    | 477/830 [02:54<01:39,  3.53it/s] 58%|█████▊    | 478/830 [02:55<01:39,  3.54it/s] 58%|█████▊    | 479/830 [02:55<01:39,  3.54it/s] 58%|█████▊    | 480/830 [02:55<01:38,  3.55it/s] 58%|█████▊    | 481/830 [02:55<01:38,  3.56it/s] 58%|█████▊    | 482/830 [02:56<01:37,  3.56it/s] 58%|█████▊    | 483/830 [02:56<01:37,  3.56it/s] 58%|█████▊    | 484/830 [02:56<01:40,  3.45it/s] 58%|█████▊    | 485/830 [02:57<01:39,  3.48it/s] 59%|█████▊    | 486/830 [02:57<01:38,  3.50it/s] 59%|█████▊    | 487/830 [02:57<01:37,  3.52it/s] 59%|█████▉    | 488/830 [02:57<01:36,  3.53it/s] 59%|█████▉    | 489/830 [02:58<01:36,  3.54it/s] 59%|█████▉    | 490/830 [02:58<01:35,  3.54it/s] 59%|█████▉    | 491/830 [02:58<01:35,  3.55it/s] 59%|█████▉    | 492/830 [02:59<01:35,  3.56it/s] 59%|█████▉    | 493/830 [02:59<01:34,  3.56it/s] 60%|█████▉    | 494/830 [02:59<01:34,  3.56it/s] 60%|█████▉    | 495/830 [02:59<01:38,  3.41it/s] 60%|█████▉    | 496/830 [03:00<01:36,  3.45it/s] 60%|█████▉    | 497/830 [03:00<01:35,  3.48it/s] 60%|██████    | 498/830 [03:00<01:25,  3.88it/s][INFO|trainer.py:2140] 2023-08-28 10:10:17,732 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:10:17,733 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 10:10:17,733 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.5167, 'eval_samples_per_second': 359.852, 'eval_steps_per_second': 44.982, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 55.91it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.09it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.48it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.77it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.30it/s][A
  5%|▌         | 32/608 [00:00<00:12, 46.01it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.54it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.08it/s][A
  8%|▊         | 47/608 [00:01<00:12, 44.98it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.03it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.09it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.19it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.35it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.39it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.32it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.04it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.83it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.90it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 42.61it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 43.44it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.00it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 44.54it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 44.87it/s][A
 20%|██        | 122/608 [00:02<00:10, 44.93it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.88it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.78it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.59it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.64it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.90it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.06it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.27it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.30it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.30it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.12it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.92it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.76it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.74it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 44.96it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.13it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.36it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.00it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.42it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.18it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.02it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.87it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 43.65it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 44.12it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.63it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.93it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.07it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 45.09it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.02it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.85it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.58it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.72it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.86it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.04it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.23it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.38it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.30it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.12it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 44.81it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.58it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.67it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.71it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.05it/s][A
 55%|█████▌    | 337/608 [00:07<00:05, 45.27it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.34it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.38it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.13it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 44.93it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.77it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.28it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.53it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.81it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.94it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.20it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 45.28it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.24it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 45.04it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.76it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.84it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.85it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.05it/s][A
 70%|███████   | 427/608 [00:09<00:04, 45.05it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.20it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.24it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.23it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.01it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.96it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.83it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.93it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.04it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.12it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.27it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.14it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.07it/s][A
 81%|████████  | 492/608 [00:10<00:02, 44.98it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 45.01it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 42.77it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 43.53it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.13it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.44it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.74it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.81it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 44.77it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 44.78it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.67it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.62it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.91it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.03it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.23it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.32it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.22it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.10it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 44.92it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 44.82it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.81it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.90it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 45.11it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.27it/s][A                                                 
                                                 [A 60%|██████    | 498/830 [03:14<01:25,  3.88it/s]
100%|██████████| 608/608 [00:13<00:00, 45.27it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:10:31,555 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498
[INFO|configuration_utils.py:351] 2023-08-28 10:10:31,749 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:10:34,407 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:10:34,627 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:10:34,728 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498/special_tokens_map.json
 60%|██████    | 499/830 [03:18<31:03,  5.63s/it] 60%|██████    | 500/830 [03:19<22:07,  4.02s/it]                                                  60%|██████    | 500/830 [03:19<22:07,  4.02s/it] 60%|██████    | 501/830 [03:19<15:54,  2.90s/it] 60%|██████    | 502/830 [03:19<11:32,  2.11s/it] 61%|██████    | 503/830 [03:19<08:30,  1.56s/it] 61%|██████    | 504/830 [03:20<06:23,  1.18s/it] 61%|██████    | 505/830 [03:20<04:54,  1.10it/s] 61%|██████    | 506/830 [03:20<03:55,  1.38it/s] 61%|██████    | 507/830 [03:21<03:11,  1.69it/s] 61%|██████    | 508/830 [03:21<02:40,  2.01it/s] 61%|██████▏   | 509/830 [03:21<02:18,  2.32it/s] 61%|██████▏   | 510/830 [03:21<02:02,  2.60it/s] 62%|██████▏   | 511/830 [03:22<01:52,  2.84it/s] 62%|██████▏   | 512/830 [03:22<01:44,  3.04it/s] 62%|██████▏   | 513/830 [03:22<01:39,  3.19it/s] 62%|██████▏   | 514/830 [03:23<01:35,  3.31it/s] 62%|██████▏   | 515/830 [03:23<01:32,  3.40it/s] 62%|██████▏   | 516/830 [03:23<01:30,  3.46it/s] 62%|██████▏   | 517/830 [03:23<01:29,  3.51it/s] 62%|██████▏   | 518/830 [03:24<01:28,  3.54it/s] 63%|██████▎   | 519/830 [03:24<01:28,  3.50it/s] 63%|██████▎   | 520/830 [03:24<01:27,  3.53it/s] 63%|██████▎   | 521/830 [03:24<01:26,  3.56it/s] 63%|██████▎   | 522/830 [03:25<01:26,  3.58it/s] 63%|██████▎   | 523/830 [03:25<01:25,  3.59it/s] 63%|██████▎   | 524/830 [03:25<01:25,  3.60it/s] 63%|██████▎   | 525/830 [03:26<01:24,  3.61it/s] 63%|██████▎   | 526/830 [03:26<01:24,  3.61it/s] 63%|██████▎   | 527/830 [03:26<01:23,  3.61it/s] 64%|██████▎   | 528/830 [03:26<01:23,  3.62it/s] 64%|██████▎   | 529/830 [03:27<01:23,  3.62it/s] 64%|██████▍   | 530/830 [03:27<01:25,  3.50it/s] 64%|██████▍   | 531/830 [03:27<01:24,  3.54it/s] 64%|██████▍   | 532/830 [03:28<01:23,  3.56it/s] 64%|██████▍   | 533/830 [03:28<01:22,  3.58it/s] 64%|██████▍   | 534/830 [03:28<01:22,  3.59it/s] 64%|██████▍   | 535/830 [03:28<01:21,  3.60it/s] 65%|██████▍   | 536/830 [03:29<01:21,  3.61it/s] 65%|██████▍   | 537/830 [03:29<01:21,  3.61it/s] 65%|██████▍   | 538/830 [03:29<01:20,  3.62it/s] 65%|██████▍   | 539/830 [03:29<01:20,  3.62it/s] 65%|██████▌   | 540/830 [03:30<01:20,  3.62it/s] 65%|██████▌   | 541/830 [03:30<01:23,  3.48it/s] 65%|██████▌   | 542/830 [03:30<01:21,  3.52it/s] 65%|██████▌   | 543/830 [03:31<01:20,  3.55it/s] 66%|██████▌   | 544/830 [03:31<01:20,  3.57it/s] 66%|██████▌   | 545/830 [03:31<01:19,  3.59it/s] 66%|██████▌   | 546/830 [03:31<01:18,  3.60it/s] 66%|██████▌   | 547/830 [03:32<01:18,  3.61it/s] 66%|██████▌   | 548/830 [03:32<01:18,  3.61it/s] 66%|██████▌   | 549/830 [03:32<01:17,  3.61it/s] 66%|██████▋   | 550/830 [03:33<01:17,  3.61it/s] 66%|██████▋   | 551/830 [03:33<01:17,  3.62it/s] 67%|██████▋   | 552/830 [03:33<01:20,  3.44it/s] 67%|██████▋   | 553/830 [03:33<01:19,  3.50it/s] 67%|██████▋   | 554/830 [03:34<01:18,  3.53it/s] 67%|██████▋   | 555/830 [03:34<01:17,  3.56it/s] 67%|██████▋   | 556/830 [03:34<01:16,  3.58it/s] 67%|██████▋   | 557/830 [03:35<01:16,  3.59it/s] 67%|██████▋   | 558/830 [03:35<01:15,  3.60it/s] 67%|██████▋   | 559/830 [03:35<01:15,  3.60it/s] 67%|██████▋   | 560/830 [03:35<01:14,  3.61it/s] 68%|██████▊   | 561/830 [03:36<01:14,  3.61it/s] 68%|██████▊   | 562/830 [03:36<01:14,  3.61it/s] 68%|██████▊   | 563/830 [03:36<01:18,  3.42it/s] 68%|██████▊   | 564/830 [03:37<01:16,  3.48it/s] 68%|██████▊   | 565/830 [03:37<01:15,  3.52it/s] 68%|██████▊   | 566/830 [03:37<01:14,  3.55it/s] 68%|██████▊   | 567/830 [03:37<01:13,  3.57it/s] 68%|██████▊   | 568/830 [03:38<01:13,  3.58it/s] 69%|██████▊   | 569/830 [03:38<01:12,  3.59it/s] 69%|██████▊   | 570/830 [03:38<01:12,  3.60it/s] 69%|██████▉   | 571/830 [03:38<01:11,  3.60it/s] 69%|██████▉   | 572/830 [03:39<01:11,  3.61it/s] 69%|██████▉   | 573/830 [03:39<01:11,  3.61it/s] 69%|██████▉   | 574/830 [03:39<01:14,  3.44it/s] 69%|██████▉   | 575/830 [03:40<01:12,  3.49it/s] 69%|██████▉   | 576/830 [03:40<01:11,  3.53it/s] 70%|██████▉   | 577/830 [03:40<01:11,  3.56it/s] 70%|██████▉   | 578/830 [03:40<01:10,  3.58it/s] 70%|██████▉   | 579/830 [03:41<01:09,  3.59it/s] 70%|██████▉   | 580/830 [03:41<01:09,  3.60it/s] 70%|███████   | 581/830 [03:41<01:09,  3.61it/s] 70%|███████   | 582/830 [03:42<01:08,  3.61it/s] 70%|███████   | 583/830 [03:42<01:08,  3.62it/s] 70%|███████   | 584/830 [03:42<01:07,  3.62it/s] 70%|███████   | 585/830 [03:42<01:09,  3.55it/s] 71%|███████   | 586/830 [03:43<01:08,  3.57it/s] 71%|███████   | 587/830 [03:43<01:07,  3.59it/s] 71%|███████   | 588/830 [03:43<01:07,  3.60it/s] 71%|███████   | 589/830 [03:43<01:06,  3.60it/s] 71%|███████   | 590/830 [03:44<01:06,  3.61it/s] 71%|███████   | 591/830 [03:44<01:07,  3.52it/s] 71%|███████▏  | 592/830 [03:44<01:07,  3.53it/s] 71%|███████▏  | 593/830 [03:45<01:06,  3.56it/s] 72%|███████▏  | 594/830 [03:45<01:05,  3.58it/s] 72%|███████▏  | 595/830 [03:45<01:05,  3.59it/s] 72%|███████▏  | 596/830 [03:45<01:05,  3.56it/s] 72%|███████▏  | 597/830 [03:46<01:05,  3.58it/s] 72%|███████▏  | 598/830 [03:46<01:04,  3.59it/s] 72%|███████▏  | 599/830 [03:47<01:25,  2.70it/s] 72%|███████▏  | 600/830 [03:47<01:18,  2.92it/s] 72%|███████▏  | 601/830 [03:47<01:14,  3.09it/s] 73%|███████▎  | 602/830 [03:47<01:10,  3.23it/s] 73%|███████▎  | 603/830 [03:48<01:07,  3.34it/s] 73%|███████▎  | 604/830 [03:48<01:05,  3.42it/s] 73%|███████▎  | 605/830 [03:48<01:04,  3.48it/s] 73%|███████▎  | 606/830 [03:49<01:06,  3.36it/s] 73%|███████▎  | 607/830 [03:49<01:04,  3.43it/s] 73%|███████▎  | 608/830 [03:49<01:03,  3.49it/s] 73%|███████▎  | 609/830 [03:49<01:02,  3.52it/s] 73%|███████▎  | 610/830 [03:50<01:01,  3.55it/s] 74%|███████▎  | 611/830 [03:50<01:01,  3.57it/s] 74%|███████▎  | 612/830 [03:50<01:00,  3.59it/s] 74%|███████▍  | 613/830 [03:51<01:00,  3.60it/s] 74%|███████▍  | 614/830 [03:51<00:59,  3.61it/s] 74%|███████▍  | 615/830 [03:51<00:59,  3.61it/s] 74%|███████▍  | 616/830 [03:51<00:59,  3.62it/s] 74%|███████▍  | 617/830 [03:52<00:58,  3.62it/s] 74%|███████▍  | 618/830 [03:52<00:58,  3.62it/s] 75%|███████▍  | 619/830 [03:52<00:58,  3.62it/s] 75%|███████▍  | 620/830 [03:52<00:58,  3.62it/s] 75%|███████▍  | 621/830 [03:53<00:57,  3.63it/s] 75%|███████▍  | 622/830 [03:53<00:57,  3.62it/s] 75%|███████▌  | 623/830 [03:53<00:57,  3.62it/s] 75%|███████▌  | 624/830 [03:54<00:56,  3.62it/s] 75%|███████▌  | 625/830 [03:54<00:56,  3.62it/s] 75%|███████▌  | 626/830 [03:54<00:56,  3.62it/s] 76%|███████▌  | 627/830 [03:54<00:55,  3.63it/s] 76%|███████▌  | 628/830 [03:55<00:56,  3.56it/s] 76%|███████▌  | 629/830 [03:55<00:56,  3.58it/s] 76%|███████▌  | 630/830 [03:55<00:55,  3.59it/s] 76%|███████▌  | 631/830 [03:55<00:55,  3.60it/s] 76%|███████▌  | 632/830 [03:56<00:54,  3.61it/s] 76%|███████▋  | 633/830 [03:56<00:54,  3.61it/s] 76%|███████▋  | 634/830 [03:56<00:54,  3.61it/s] 77%|███████▋  | 635/830 [03:57<00:53,  3.62it/s] 77%|███████▋  | 636/830 [03:57<00:53,  3.62it/s] 77%|███████▋  | 637/830 [03:57<00:53,  3.62it/s] 77%|███████▋  | 638/830 [03:57<00:53,  3.62it/s] 77%|███████▋  | 639/830 [03:58<00:54,  3.47it/s] 77%|███████▋  | 640/830 [03:58<00:54,  3.52it/s] 77%|███████▋  | 641/830 [03:58<00:53,  3.55it/s] 77%|███████▋  | 642/830 [03:59<00:52,  3.57it/s] 77%|███████▋  | 643/830 [03:59<00:52,  3.59it/s] 78%|███████▊  | 644/830 [03:59<00:51,  3.60it/s] 78%|███████▊  | 645/830 [03:59<00:51,  3.60it/s] 78%|███████▊  | 646/830 [04:00<00:50,  3.61it/s] 78%|███████▊  | 647/830 [04:00<00:50,  3.61it/s] 78%|███████▊  | 648/830 [04:00<00:50,  3.62it/s] 78%|███████▊  | 649/830 [04:00<00:50,  3.62it/s] 78%|███████▊  | 650/830 [04:01<00:50,  3.54it/s] 78%|███████▊  | 651/830 [04:01<00:50,  3.57it/s] 79%|███████▊  | 652/830 [04:01<00:49,  3.58it/s] 79%|███████▊  | 653/830 [04:02<00:49,  3.59it/s] 79%|███████▉  | 654/830 [04:02<00:48,  3.60it/s] 79%|███████▉  | 655/830 [04:02<00:48,  3.61it/s] 79%|███████▉  | 656/830 [04:02<00:48,  3.61it/s] 79%|███████▉  | 657/830 [04:03<00:47,  3.62it/s] 79%|███████▉  | 658/830 [04:03<00:47,  3.62it/s] 79%|███████▉  | 659/830 [04:03<00:47,  3.62it/s] 80%|███████▉  | 660/830 [04:04<00:46,  3.62it/s] 80%|███████▉  | 661/830 [04:04<00:47,  3.57it/s] 80%|███████▉  | 662/830 [04:04<00:46,  3.59it/s] 80%|███████▉  | 663/830 [04:04<00:46,  3.60it/s] 80%|████████  | 664/830 [04:05<00:41,  4.00it/s][INFO|trainer.py:2140] 2023-08-28 10:11:22,128 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:11:22,128 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 10:11:22,128 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.5436, 'eval_samples_per_second': 359.136, 'eval_steps_per_second': 44.892, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.2409638554216866e-05, 'epoch': 3.01}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.68it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.57it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.85it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.06it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.31it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.78it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.09it/s][A
  7%|▋         | 43/608 [00:00<00:12, 44.80it/s][A
  8%|▊         | 48/608 [00:01<00:12, 44.93it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.00it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.23it/s][A
 10%|█         | 63/608 [00:01<00:12, 45.34it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.54it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.55it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.41it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.10it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.95it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.94it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 44.39it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 44.89it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.12it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.34it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.37it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.36it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.03it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.00it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.80it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.04it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.19it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.36it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.51it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.57it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.39it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.16it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.99it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.94it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.05it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.11it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.35it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.52it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.58it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.42it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.17it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.10it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.00it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 44.98it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 44.98it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.18it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.41it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.50it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.41it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.20it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.03it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 44.98it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 44.95it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.16it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.28it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.47it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.48it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.47it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.21it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.11it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 44.98it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 44.96it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.10it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.24it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.38it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.55it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.43it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.28it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.14it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 44.97it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.03it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.01it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.13it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.26it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.47it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.48it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.43it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.18it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.05it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 44.99it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.06it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.19it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.31it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.46it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.54it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.40it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.27it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 45.12it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.04it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.11it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.20it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.24it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.42it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.43it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.44it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.27it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 45.16it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.00it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.00it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.03it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.16it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.38it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.43it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.48it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.42it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 45.37it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.18it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.06it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.11it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.20it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.32it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.41it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.36it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.44it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 45.05it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.09it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 44.96it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.03it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.10it/s][A                                                 
                                                 [A 80%|████████  | 664/830 [04:18<00:41,  4.00it/s]
100%|██████████| 608/608 [00:13<00:00, 45.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:11:35,653 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664
[INFO|configuration_utils.py:351] 2023-08-28 10:11:35,770 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:11:38,335 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:11:38,426 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:11:38,466 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664/special_tokens_map.json
 80%|████████  | 665/830 [04:22<14:57,  5.44s/it] 80%|████████  | 666/830 [04:22<10:38,  3.89s/it] 80%|████████  | 667/830 [04:23<07:37,  2.81s/it] 80%|████████  | 668/830 [04:23<05:31,  2.05s/it] 81%|████████  | 669/830 [04:23<04:04,  1.52s/it] 81%|████████  | 670/830 [04:24<03:03,  1.15s/it] 81%|████████  | 671/830 [04:24<02:20,  1.13it/s] 81%|████████  | 672/830 [04:24<01:51,  1.42it/s] 81%|████████  | 673/830 [04:24<01:30,  1.73it/s] 81%|████████  | 674/830 [04:25<01:16,  2.05it/s] 81%|████████▏ | 675/830 [04:25<01:05,  2.35it/s] 81%|████████▏ | 676/830 [04:25<01:01,  2.52it/s] 82%|████████▏ | 677/830 [04:26<00:55,  2.77it/s] 82%|████████▏ | 678/830 [04:26<00:51,  2.97it/s] 82%|████████▏ | 679/830 [04:26<00:48,  3.12it/s] 82%|████████▏ | 680/830 [04:26<00:46,  3.24it/s] 82%|████████▏ | 681/830 [04:27<00:44,  3.33it/s] 82%|████████▏ | 682/830 [04:27<00:43,  3.40it/s] 82%|████████▏ | 683/830 [04:27<00:42,  3.45it/s] 82%|████████▏ | 684/830 [04:27<00:41,  3.48it/s] 83%|████████▎ | 685/830 [04:28<00:41,  3.51it/s] 83%|████████▎ | 686/830 [04:28<00:40,  3.52it/s] 83%|████████▎ | 687/830 [04:28<00:42,  3.36it/s] 83%|████████▎ | 688/830 [04:29<00:41,  3.42it/s] 83%|████████▎ | 689/830 [04:29<00:40,  3.46it/s] 83%|████████▎ | 690/830 [04:29<00:40,  3.49it/s] 83%|████████▎ | 691/830 [04:30<00:39,  3.52it/s] 83%|████████▎ | 692/830 [04:30<00:39,  3.54it/s] 83%|████████▎ | 693/830 [04:30<00:38,  3.55it/s] 84%|████████▎ | 694/830 [04:30<00:38,  3.55it/s] 84%|████████▎ | 695/830 [04:31<00:37,  3.56it/s] 84%|████████▍ | 696/830 [04:31<00:37,  3.57it/s] 84%|████████▍ | 697/830 [04:31<00:37,  3.57it/s] 84%|████████▍ | 698/830 [04:31<00:37,  3.48it/s] 84%|████████▍ | 699/830 [04:32<00:37,  3.51it/s] 84%|████████▍ | 700/830 [04:32<00:36,  3.53it/s] 84%|████████▍ | 701/830 [04:32<00:36,  3.54it/s] 85%|████████▍ | 702/830 [04:33<00:36,  3.55it/s] 85%|████████▍ | 703/830 [04:33<00:35,  3.56it/s] 85%|████████▍ | 704/830 [04:33<00:35,  3.56it/s] 85%|████████▍ | 705/830 [04:33<00:35,  3.56it/s] 85%|████████▌ | 706/830 [04:34<00:34,  3.57it/s] 85%|████████▌ | 707/830 [04:34<00:34,  3.57it/s] 85%|████████▌ | 708/830 [04:34<00:34,  3.57it/s] 85%|████████▌ | 709/830 [04:35<00:35,  3.44it/s] 86%|████████▌ | 710/830 [04:35<00:34,  3.48it/s] 86%|████████▌ | 711/830 [04:35<00:33,  3.50it/s] 86%|████████▌ | 712/830 [04:35<00:33,  3.52it/s] 86%|████████▌ | 713/830 [04:36<00:33,  3.54it/s] 86%|████████▌ | 714/830 [04:36<00:32,  3.55it/s] 86%|████████▌ | 715/830 [04:36<00:32,  3.55it/s] 86%|████████▋ | 716/830 [04:37<00:32,  3.56it/s] 86%|████████▋ | 717/830 [04:37<00:31,  3.56it/s] 87%|████████▋ | 718/830 [04:37<00:31,  3.57it/s] 87%|████████▋ | 719/830 [04:37<00:31,  3.57it/s] 87%|████████▋ | 720/830 [04:38<00:32,  3.43it/s] 87%|████████▋ | 721/830 [04:38<00:31,  3.47it/s] 87%|████████▋ | 722/830 [04:38<00:30,  3.50it/s] 87%|████████▋ | 723/830 [04:39<00:30,  3.52it/s] 87%|████████▋ | 724/830 [04:39<00:29,  3.54it/s] 87%|████████▋ | 725/830 [04:39<00:29,  3.55it/s] 87%|████████▋ | 726/830 [04:39<00:29,  3.56it/s] 88%|████████▊ | 727/830 [04:40<00:28,  3.56it/s] 88%|████████▊ | 728/830 [04:40<00:28,  3.56it/s] 88%|████████▊ | 729/830 [04:40<00:28,  3.57it/s] 88%|████████▊ | 730/830 [04:41<00:28,  3.57it/s] 88%|████████▊ | 731/830 [04:41<00:28,  3.43it/s] 88%|████████▊ | 732/830 [04:41<00:28,  3.47it/s] 88%|████████▊ | 733/830 [04:41<00:27,  3.50it/s] 88%|████████▊ | 734/830 [04:42<00:27,  3.52it/s] 89%|████████▊ | 735/830 [04:42<00:26,  3.54it/s] 89%|████████▊ | 736/830 [04:42<00:26,  3.55it/s] 89%|████████▉ | 737/830 [04:43<00:26,  3.56it/s] 89%|████████▉ | 738/830 [04:43<00:25,  3.56it/s] 89%|████████▉ | 739/830 [04:43<00:25,  3.57it/s] 89%|████████▉ | 740/830 [04:43<00:25,  3.57it/s] 89%|████████▉ | 741/830 [04:44<00:24,  3.57it/s] 89%|████████▉ | 742/830 [04:44<00:25,  3.48it/s] 90%|████████▉ | 743/830 [04:44<00:25,  3.41it/s] 90%|████████▉ | 744/830 [04:45<00:24,  3.46it/s] 90%|████████▉ | 745/830 [04:45<00:24,  3.51it/s] 90%|████████▉ | 746/830 [04:45<00:23,  3.54it/s] 90%|█████████ | 747/830 [04:45<00:23,  3.57it/s] 90%|█████████ | 748/830 [04:46<00:22,  3.58it/s] 90%|█████████ | 749/830 [04:46<00:22,  3.59it/s] 90%|█████████ | 750/830 [04:46<00:22,  3.60it/s] 90%|█████████ | 751/830 [04:47<00:26,  2.98it/s] 91%|█████████ | 752/830 [04:47<00:25,  3.06it/s] 91%|█████████ | 753/830 [04:47<00:23,  3.21it/s] 91%|█████████ | 754/830 [04:48<00:22,  3.33it/s] 91%|█████████ | 755/830 [04:48<00:22,  3.41it/s] 91%|█████████ | 756/830 [04:48<00:21,  3.47it/s] 91%|█████████ | 757/830 [04:48<00:20,  3.51it/s] 91%|█████████▏| 758/830 [04:49<00:20,  3.55it/s] 91%|█████████▏| 759/830 [04:49<00:19,  3.57it/s] 92%|█████████▏| 760/830 [04:49<00:19,  3.58it/s] 92%|█████████▏| 761/830 [04:49<00:19,  3.59it/s] 92%|█████████▏| 762/830 [04:50<00:18,  3.60it/s] 92%|█████████▏| 763/830 [04:50<00:19,  3.50it/s] 92%|█████████▏| 764/830 [04:50<00:18,  3.53it/s] 92%|█████████▏| 765/830 [04:51<00:18,  3.56it/s] 92%|█████████▏| 766/830 [04:51<00:17,  3.58it/s] 92%|█████████▏| 767/830 [04:51<00:17,  3.59it/s] 93%|█████████▎| 768/830 [04:51<00:17,  3.60it/s] 93%|█████████▎| 769/830 [04:52<00:16,  3.61it/s] 93%|█████████▎| 770/830 [04:52<00:16,  3.61it/s] 93%|█████████▎| 771/830 [04:52<00:16,  3.62it/s] 93%|█████████▎| 772/830 [04:53<00:16,  3.62it/s] 93%|█████████▎| 773/830 [04:53<00:15,  3.62it/s] 93%|█████████▎| 774/830 [04:53<00:15,  3.62it/s] 93%|█████████▎| 775/830 [04:53<00:15,  3.62it/s] 93%|█████████▎| 776/830 [04:54<00:14,  3.62it/s] 94%|█████████▎| 777/830 [04:54<00:14,  3.63it/s] 94%|█████████▎| 778/830 [04:54<00:14,  3.62it/s] 94%|█████████▍| 779/830 [04:54<00:14,  3.62it/s] 94%|█████████▍| 780/830 [04:55<00:13,  3.62it/s] 94%|█████████▍| 781/830 [04:55<00:13,  3.62it/s] 94%|█████████▍| 782/830 [04:55<00:13,  3.62it/s] 94%|█████████▍| 783/830 [04:56<00:12,  3.62it/s] 94%|█████████▍| 784/830 [04:56<00:12,  3.62it/s] 95%|█████████▍| 785/830 [04:56<00:12,  3.53it/s] 95%|█████████▍| 786/830 [04:56<00:12,  3.56it/s] 95%|█████████▍| 787/830 [04:57<00:12,  3.58it/s] 95%|█████████▍| 788/830 [04:57<00:11,  3.59it/s] 95%|█████████▌| 789/830 [04:57<00:11,  3.60it/s] 95%|█████████▌| 790/830 [04:57<00:11,  3.61it/s] 95%|█████████▌| 791/830 [04:58<00:10,  3.61it/s] 95%|█████████▌| 792/830 [04:58<00:10,  3.62it/s] 96%|█████████▌| 793/830 [04:58<00:10,  3.62it/s] 96%|█████████▌| 794/830 [04:59<00:09,  3.62it/s] 96%|█████████▌| 795/830 [04:59<00:09,  3.62it/s] 96%|█████████▌| 796/830 [04:59<00:09,  3.49it/s] 96%|█████████▌| 797/830 [04:59<00:09,  3.53it/s] 96%|█████████▌| 798/830 [05:00<00:09,  3.55it/s] 96%|█████████▋| 799/830 [05:00<00:08,  3.57it/s] 96%|█████████▋| 800/830 [05:00<00:08,  3.58it/s] 97%|█████████▋| 801/830 [05:01<00:08,  3.60it/s] 97%|█████████▋| 802/830 [05:01<00:07,  3.60it/s] 97%|█████████▋| 803/830 [05:01<00:07,  3.61it/s] 97%|█████████▋| 804/830 [05:01<00:07,  3.61it/s] 97%|█████████▋| 805/830 [05:02<00:06,  3.61it/s] 97%|█████████▋| 806/830 [05:02<00:06,  3.62it/s] 97%|█████████▋| 807/830 [05:02<00:06,  3.48it/s] 97%|█████████▋| 808/830 [05:03<00:06,  3.52it/s] 97%|█████████▋| 809/830 [05:03<00:05,  3.55it/s] 98%|█████████▊| 810/830 [05:03<00:05,  3.57it/s] 98%|█████████▊| 811/830 [05:03<00:05,  3.59it/s] 98%|█████████▊| 812/830 [05:04<00:05,  3.60it/s] 98%|█████████▊| 813/830 [05:04<00:04,  3.61it/s] 98%|█████████▊| 814/830 [05:04<00:04,  3.61it/s] 98%|█████████▊| 815/830 [05:04<00:04,  3.61it/s] 98%|█████████▊| 816/830 [05:05<00:03,  3.62it/s] 98%|█████████▊| 817/830 [05:05<00:03,  3.62it/s] 99%|█████████▊| 818/830 [05:05<00:03,  3.51it/s] 99%|█████████▊| 819/830 [05:06<00:03,  3.54it/s] 99%|█████████▉| 820/830 [05:06<00:02,  3.56it/s] 99%|█████████▉| 821/830 [05:06<00:02,  3.58it/s] 99%|█████████▉| 822/830 [05:06<00:02,  3.59it/s] 99%|█████████▉| 823/830 [05:07<00:01,  3.60it/s] 99%|█████████▉| 824/830 [05:07<00:01,  3.61it/s] 99%|█████████▉| 825/830 [05:07<00:01,  3.62it/s]100%|█████████▉| 826/830 [05:08<00:01,  3.61it/s]100%|█████████▉| 827/830 [05:08<00:00,  3.62it/s]100%|█████████▉| 828/830 [05:08<00:00,  3.62it/s]100%|█████████▉| 829/830 [05:08<00:00,  3.54it/s]100%|██████████| 830/830 [05:09<00:00,  3.95it/s][INFO|trainer.py:2140] 2023-08-28 10:12:26,124 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:12:26,125 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 10:12:26,125 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.4497, 'eval_samples_per_second': 361.644, 'eval_steps_per_second': 45.205, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.82it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.75it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.95it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.00it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.31it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.80it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.34it/s][A
  7%|▋         | 43/608 [00:00<00:12, 44.98it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.12it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.32it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.52it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.60it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.64it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.63it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.36it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.11it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.94it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.03it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.21it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.46it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.57it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.64it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.57it/s][A
 20%|██        | 123/608 [00:02<00:11, 43.46it/s][A
 21%|██        | 128/608 [00:02<00:10, 43.83it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 44.21it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.46it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 44.68it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 44.98it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.24it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.35it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.25it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.34it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.24it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.18it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.10it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.21it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.30it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.47it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.44it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.37it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.34it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.14it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.11it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.07it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.14it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.28it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.39it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.42it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.36it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.37it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 44.34it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 44.61it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 44.74it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 44.91it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.05it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.29it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.35it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.31it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.23it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.22it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.23it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.22it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.21it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.34it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.40it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.13it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.49it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.38it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.29it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.25it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.26it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.33it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.39it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.40it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.36it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.39it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.29it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.38it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.24it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.22it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.29it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.38it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.41it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.48it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.42it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.42it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.30it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.33it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 45.30it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.26it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.22it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.38it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.39it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.41it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.30it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.21it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.24it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 45.29it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.33it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.28it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.38it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.47it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.41it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.33it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.14it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.19it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 44.16it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 44.59it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 44.87it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.15it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.22it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.34it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.19it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.22it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.05it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 44.96it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.24it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.37it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.48it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.49it/s][A                                                 
                                                 [A100%|██████████| 830/830 [05:22<00:00,  3.95it/s]
100%|██████████| 608/608 [00:13<00:00, 45.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:12:39,833 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830
[INFO|configuration_utils.py:351] 2023-08-28 10:12:40,064 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:12:42,909 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:12:43,077 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:12:43,147 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:12:43,955 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:12:43,964 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166 (score: 0.9957473874092102).
                                                 100%|██████████| 830/830 [05:33<00:00,  3.95it/s]100%|██████████| 830/830 [05:33<00:00,  2.49it/s]
[INFO|trainer.py:1894] 2023-08-28 10:12:50,831 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 10:12:50,943 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:12:54,848 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:12:55,144 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:12:55,300 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:12:56,124 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:12:56,124 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:12:56,124 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:12:56,124 >>   train_runtime            = 0:05:33.71
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:12:56,124 >>   train_samples            =      10600
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:12:56,124 >>   train_samples_per_second =    158.816
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:12:56,124 >>   train_steps_per_second   =      2.487
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.4471, 'eval_samples_per_second': 361.714, 'eval_steps_per_second': 45.214, 'epoch': 5.0}
{'train_runtime': 333.7193, 'train_samples_per_second': 158.816, 'train_steps_per_second': 2.487, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 10:12:56 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:12:56,577 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:12:56,577 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 10:12:56,577 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 56.09it/s]  2%|▏         | 12/608 [00:00<00:11, 50.03it/s]  3%|▎         | 18/608 [00:00<00:12, 48.04it/s]  4%|▍         | 23/608 [00:00<00:12, 47.35it/s]  5%|▍         | 28/608 [00:00<00:12, 46.93it/s]  5%|▌         | 33/608 [00:00<00:12, 46.67it/s]  6%|▋         | 38/608 [00:00<00:12, 46.42it/s]  7%|▋         | 43/608 [00:00<00:12, 46.03it/s]  8%|▊         | 48/608 [00:01<00:12, 45.40it/s]  9%|▊         | 53/608 [00:01<00:12, 45.22it/s] 10%|▉         | 58/608 [00:01<00:12, 45.28it/s] 10%|█         | 63/608 [00:01<00:11, 45.57it/s] 11%|█         | 68/608 [00:01<00:12, 43.17it/s] 12%|█▏        | 73/608 [00:01<00:12, 44.04it/s] 13%|█▎        | 78/608 [00:01<00:11, 44.64it/s] 14%|█▎        | 83/608 [00:01<00:11, 45.08it/s] 14%|█▍        | 88/608 [00:01<00:11, 45.14it/s] 15%|█▌        | 93/608 [00:02<00:11, 45.01it/s] 16%|█▌        | 98/608 [00:02<00:11, 44.90it/s] 17%|█▋        | 103/608 [00:02<00:11, 44.98it/s] 18%|█▊        | 108/608 [00:02<00:11, 44.95it/s] 19%|█▊        | 113/608 [00:02<00:10, 45.17it/s] 19%|█▉        | 118/608 [00:02<00:10, 45.21it/s] 20%|██        | 123/608 [00:02<00:10, 45.44it/s] 21%|██        | 128/608 [00:02<00:10, 45.58it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.65it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.55it/s] 24%|██▎       | 143/608 [00:03<00:10, 45.47it/s] 24%|██▍       | 148/608 [00:03<00:10, 45.30it/s] 25%|██▌       | 153/608 [00:03<00:10, 45.31it/s] 26%|██▌       | 158/608 [00:03<00:09, 45.32it/s] 27%|██▋       | 163/608 [00:03<00:09, 45.34it/s] 28%|██▊       | 168/608 [00:03<00:09, 45.42it/s] 28%|██▊       | 173/608 [00:03<00:09, 45.62it/s] 29%|██▉       | 178/608 [00:03<00:09, 45.67it/s] 30%|███       | 183/608 [00:04<00:09, 45.63it/s] 31%|███       | 188/608 [00:04<00:09, 45.48it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.43it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.33it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.35it/s] 34%|███▍      | 208/608 [00:04<00:09, 41.30it/s] 35%|███▌      | 213/608 [00:04<00:09, 42.65it/s] 36%|███▌      | 218/608 [00:04<00:08, 43.67it/s] 37%|███▋      | 223/608 [00:04<00:08, 44.35it/s] 38%|███▊      | 228/608 [00:05<00:08, 44.85it/s] 38%|███▊      | 233/608 [00:05<00:08, 45.02it/s] 39%|███▉      | 238/608 [00:05<00:09, 41.05it/s] 40%|███▉      | 243/608 [00:05<00:08, 42.26it/s] 41%|████      | 248/608 [00:05<00:08, 43.02it/s] 42%|████▏     | 253/608 [00:05<00:08, 43.62it/s] 42%|████▏     | 258/608 [00:05<00:07, 44.30it/s] 43%|████▎     | 263/608 [00:05<00:07, 44.74it/s] 44%|████▍     | 268/608 [00:05<00:07, 45.15it/s] 45%|████▍     | 273/608 [00:06<00:07, 45.31it/s] 46%|████▌     | 278/608 [00:06<00:07, 45.22it/s] 47%|████▋     | 283/608 [00:06<00:07, 45.11it/s] 47%|████▋     | 288/608 [00:06<00:07, 45.12it/s] 48%|████▊     | 293/608 [00:06<00:06, 45.14it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.17it/s] 50%|████▉     | 303/608 [00:06<00:06, 45.24it/s] 51%|█████     | 308/608 [00:06<00:06, 45.42it/s] 51%|█████▏    | 313/608 [00:06<00:06, 45.56it/s] 52%|█████▏    | 318/608 [00:07<00:06, 45.57it/s] 53%|█████▎    | 323/608 [00:07<00:06, 45.56it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.38it/s] 55%|█████▍    | 333/608 [00:07<00:06, 45.22it/s] 56%|█████▌    | 338/608 [00:07<00:11, 22.83it/s] 56%|█████▋    | 342/608 [00:07<00:10, 25.34it/s] 57%|█████▋    | 347/608 [00:08<00:08, 29.52it/s] 58%|█████▊    | 352/608 [00:08<00:07, 33.20it/s] 59%|█████▊    | 357/608 [00:08<00:06, 36.26it/s] 60%|█████▉    | 362/608 [00:08<00:06, 38.69it/s] 60%|██████    | 367/608 [00:08<00:05, 40.64it/s] 61%|██████    | 372/608 [00:08<00:05, 42.07it/s] 62%|██████▏   | 377/608 [00:08<00:05, 42.77it/s] 63%|██████▎   | 382/608 [00:08<00:05, 43.15it/s] 64%|██████▎   | 387/608 [00:08<00:05, 43.67it/s] 64%|██████▍   | 392/608 [00:09<00:04, 44.23it/s] 65%|██████▌   | 397/608 [00:09<00:04, 44.79it/s] 66%|██████▌   | 402/608 [00:09<00:04, 45.05it/s] 67%|██████▋   | 407/608 [00:09<00:04, 45.31it/s] 68%|██████▊   | 412/608 [00:09<00:04, 45.46it/s] 69%|██████▊   | 417/608 [00:09<00:04, 45.53it/s] 69%|██████▉   | 422/608 [00:09<00:04, 45.31it/s] 70%|███████   | 427/608 [00:09<00:04, 45.09it/s] 71%|███████   | 432/608 [00:09<00:03, 44.97it/s] 72%|███████▏  | 437/608 [00:10<00:03, 45.10it/s] 73%|███████▎  | 442/608 [00:10<00:03, 45.17it/s] 74%|███████▎  | 447/608 [00:10<00:03, 45.43it/s] 74%|███████▍  | 452/608 [00:10<00:03, 45.51it/s] 75%|███████▌  | 457/608 [00:10<00:03, 45.66it/s] 76%|███████▌  | 462/608 [00:10<00:03, 45.62it/s] 77%|███████▋  | 467/608 [00:10<00:03, 45.41it/s] 78%|███████▊  | 472/608 [00:10<00:03, 45.20it/s] 78%|███████▊  | 477/608 [00:10<00:02, 45.11it/s] 79%|███████▉  | 482/608 [00:11<00:02, 42.56it/s] 80%|████████  | 487/608 [00:11<00:02, 43.42it/s] 81%|████████  | 492/608 [00:11<00:02, 43.96it/s] 82%|████████▏ | 497/608 [00:11<00:02, 44.63it/s] 83%|████████▎ | 502/608 [00:11<00:02, 44.94it/s] 83%|████████▎ | 507/608 [00:11<00:02, 45.20it/s] 84%|████████▍ | 512/608 [00:11<00:02, 45.26it/s] 85%|████████▌ | 517/608 [00:11<00:02, 45.32it/s] 86%|████████▌ | 522/608 [00:11<00:01, 45.03it/s] 87%|████████▋ | 527/608 [00:12<00:01, 44.98it/s] 88%|████████▊ | 532/608 [00:12<00:01, 45.12it/s] 88%|████████▊ | 537/608 [00:12<00:01, 45.28it/s] 89%|████████▉ | 542/608 [00:12<00:01, 45.38it/s] 90%|████████▉ | 547/608 [00:12<00:01, 45.57it/s] 91%|█████████ | 552/608 [00:12<00:01, 45.54it/s] 92%|█████████▏| 557/608 [00:12<00:01, 45.55it/s] 92%|█████████▏| 562/608 [00:12<00:01, 45.44it/s] 93%|█████████▎| 567/608 [00:12<00:00, 45.16it/s] 94%|█████████▍| 572/608 [00:13<00:00, 45.11it/s] 95%|█████████▍| 577/608 [00:13<00:00, 45.02it/s] 96%|█████████▌| 582/608 [00:13<00:00, 45.27it/s] 97%|█████████▋| 587/608 [00:13<00:00, 45.26it/s] 97%|█████████▋| 592/608 [00:13<00:00, 45.43it/s] 98%|█████████▊| 597/608 [00:13<00:00, 45.56it/s] 99%|█████████▉| 602/608 [00:13<00:00, 45.61it/s]100%|█████████▉| 607/608 [00:13<00:00, 45.45it/s]100%|██████████| 608/608 [00:13<00:00, 43.87it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:13:10,452 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:13:10,452 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:13:10,452 >>   eval_loss               =     0.9957
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:13:10,452 >>   eval_runtime            = 0:00:13.87
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:13:10,452 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:13:10,452 >>   eval_samples_per_second =    350.567
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:13:10,452 >>   eval_steps_per_second   =     43.821
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:13:10,452 >>   perplexity              =     2.7067
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:20,210 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:20,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:20,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:20,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:20,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:13:21,010 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:13:21,011 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:13:21,614 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:13:22,715 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:13:22,715 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:25,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:25,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:25,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:25,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:13:25,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:13:26,497 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:13:26,499 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:13:27,105 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:13:27,323 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:13:27,323 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-498
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-664
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-830
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-332
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/checkpoint-166
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.84it/s]Extractor Predicting: 5it [00:02,  1.89it/s]Extractor Predicting: 6it [00:03,  1.95it/s]Extractor Predicting: 7it [00:03,  1.88it/s]Extractor Predicting: 8it [00:04,  1.84it/s]Extractor Predicting: 9it [00:04,  1.86it/s]Extractor Predicting: 10it [00:05,  1.94it/s]Extractor Predicting: 11it [00:05,  1.93it/s]Extractor Predicting: 12it [00:06,  1.98it/s]Extractor Predicting: 13it [00:06,  1.95it/s]Extractor Predicting: 14it [00:07,  1.89it/s]Extractor Predicting: 15it [00:07,  1.97it/s]Extractor Predicting: 16it [00:08,  1.96it/s]Extractor Predicting: 17it [00:08,  1.94it/s]Extractor Predicting: 18it [00:09,  1.92it/s]Extractor Predicting: 19it [00:10,  1.89it/s]Extractor Predicting: 20it [00:10,  1.96it/s]Extractor Predicting: 21it [00:11,  1.96it/s]Extractor Predicting: 22it [00:11,  1.93it/s]Extractor Predicting: 23it [00:12,  1.84it/s]Extractor Predicting: 24it [00:12,  1.77it/s]Extractor Predicting: 25it [00:13,  1.69it/s]Extractor Predicting: 26it [00:14,  1.69it/s]Extractor Predicting: 27it [00:14,  1.72it/s]Extractor Predicting: 28it [00:15,  1.63it/s]Extractor Predicting: 29it [00:15,  1.69it/s]Extractor Predicting: 30it [00:16,  1.71it/s]Extractor Predicting: 31it [00:16,  1.75it/s]Extractor Predicting: 32it [00:17,  1.80it/s]Extractor Predicting: 33it [00:17,  1.81it/s]Extractor Predicting: 34it [00:18,  1.81it/s]Extractor Predicting: 35it [00:19,  1.79it/s]Extractor Predicting: 36it [00:19,  1.77it/s]Extractor Predicting: 37it [00:20,  1.76it/s]Extractor Predicting: 38it [00:20,  1.77it/s]Extractor Predicting: 39it [00:21,  1.77it/s]Extractor Predicting: 40it [00:21,  1.78it/s]Extractor Predicting: 41it [00:22,  1.80it/s]Extractor Predicting: 42it [00:23,  1.73it/s]Extractor Predicting: 43it [00:23,  1.73it/s]Extractor Predicting: 44it [00:24,  1.79it/s]Extractor Predicting: 45it [00:24,  1.77it/s]Extractor Predicting: 46it [00:25,  1.79it/s]Extractor Predicting: 47it [00:25,  1.79it/s]Extractor Predicting: 48it [00:26,  1.74it/s]Extractor Predicting: 49it [00:27,  1.74it/s]Extractor Predicting: 50it [00:27,  1.76it/s]Extractor Predicting: 51it [00:28,  1.76it/s]Extractor Predicting: 52it [00:28,  1.77it/s]Extractor Predicting: 53it [00:29,  1.76it/s]Extractor Predicting: 54it [00:29,  1.79it/s]Extractor Predicting: 55it [00:30,  1.82it/s]Extractor Predicting: 56it [00:30,  1.82it/s]Extractor Predicting: 57it [00:31,  1.81it/s]Extractor Predicting: 58it [00:32,  1.79it/s]Extractor Predicting: 59it [00:32,  1.80it/s]Extractor Predicting: 60it [00:33,  1.73it/s]Extractor Predicting: 61it [00:33,  1.69it/s]Extractor Predicting: 62it [00:34,  1.72it/s]Extractor Predicting: 63it [00:35,  1.75it/s]Extractor Predicting: 64it [00:35,  1.78it/s]Extractor Predicting: 65it [00:36,  1.77it/s]Extractor Predicting: 66it [00:36,  1.77it/s]Extractor Predicting: 67it [00:37,  1.79it/s]Extractor Predicting: 68it [00:37,  1.74it/s]Extractor Predicting: 69it [00:38,  1.75it/s]Extractor Predicting: 70it [00:38,  1.79it/s]Extractor Predicting: 71it [00:39,  1.85it/s]Extractor Predicting: 72it [00:39,  1.81it/s]Extractor Predicting: 73it [00:40,  1.84it/s]Extractor Predicting: 74it [00:41,  1.82it/s]Extractor Predicting: 75it [00:41,  1.84it/s]Extractor Predicting: 76it [00:42,  1.82it/s]Extractor Predicting: 77it [00:42,  1.82it/s]Extractor Predicting: 78it [00:43,  1.86it/s]Extractor Predicting: 79it [00:43,  1.85it/s]Extractor Predicting: 80it [00:44,  1.85it/s]Extractor Predicting: 81it [00:44,  1.82it/s]Extractor Predicting: 82it [00:45,  1.81it/s]Extractor Predicting: 83it [00:46,  1.79it/s]Extractor Predicting: 84it [00:46,  1.81it/s]Extractor Predicting: 85it [00:47,  1.80it/s]Extractor Predicting: 86it [00:47,  1.76it/s]Extractor Predicting: 87it [00:48,  1.79it/s]Extractor Predicting: 88it [00:48,  1.78it/s]Extractor Predicting: 89it [00:49,  1.70it/s]Extractor Predicting: 90it [00:50,  1.75it/s]Extractor Predicting: 91it [00:50,  1.78it/s]Extractor Predicting: 92it [00:51,  1.78it/s]Extractor Predicting: 93it [00:51,  1.81it/s]Extractor Predicting: 94it [00:52,  1.80it/s]Extractor Predicting: 95it [00:52,  1.74it/s]Extractor Predicting: 96it [00:53,  1.77it/s]Extractor Predicting: 97it [00:53,  1.77it/s]Extractor Predicting: 98it [00:54,  1.79it/s]Extractor Predicting: 99it [00:55,  1.82it/s]Extractor Predicting: 100it [00:55,  1.80it/s]Extractor Predicting: 101it [00:56,  1.82it/s]Extractor Predicting: 102it [00:56,  1.84it/s]Extractor Predicting: 103it [00:57,  1.87it/s]Extractor Predicting: 104it [00:57,  1.85it/s]Extractor Predicting: 105it [00:58,  1.81it/s]Extractor Predicting: 106it [00:58,  1.82it/s]Extractor Predicting: 107it [00:59,  1.82it/s]Extractor Predicting: 108it [00:59,  1.86it/s]Extractor Predicting: 109it [01:00,  1.81it/s]Extractor Predicting: 110it [01:01,  1.58it/s]Extractor Predicting: 111it [01:01,  1.66it/s]Extractor Predicting: 112it [01:02,  1.69it/s]Extractor Predicting: 113it [01:02,  1.73it/s]Extractor Predicting: 114it [01:03,  1.80it/s]Extractor Predicting: 115it [01:03,  1.88it/s]Extractor Predicting: 116it [01:04,  1.88it/s]Extractor Predicting: 117it [01:04,  1.87it/s]Extractor Predicting: 118it [01:05,  1.90it/s]Extractor Predicting: 119it [01:06,  1.87it/s]Extractor Predicting: 120it [01:06,  1.90it/s]Extractor Predicting: 121it [01:07,  1.86it/s]Extractor Predicting: 122it [01:07,  1.79it/s]Extractor Predicting: 123it [01:08,  1.75it/s]Extractor Predicting: 124it [01:08,  1.70it/s]Extractor Predicting: 125it [01:09,  1.74it/s]Extractor Predicting: 126it [01:10,  1.76it/s]Extractor Predicting: 127it [01:10,  1.77it/s]Extractor Predicting: 128it [01:11,  1.72it/s]Extractor Predicting: 129it [01:11,  1.75it/s]Extractor Predicting: 130it [01:12,  1.78it/s]Extractor Predicting: 131it [01:12,  1.83it/s]Extractor Predicting: 132it [01:13,  1.77it/s]Extractor Predicting: 133it [01:14,  1.75it/s]Extractor Predicting: 134it [01:14,  1.77it/s]Extractor Predicting: 135it [01:15,  1.81it/s]Extractor Predicting: 136it [01:15,  1.81it/s]Extractor Predicting: 137it [01:16,  1.83it/s]Extractor Predicting: 138it [01:16,  1.80it/s]Extractor Predicting: 139it [01:17,  1.80it/s]Extractor Predicting: 140it [01:17,  1.79it/s]Extractor Predicting: 141it [01:18,  1.82it/s]Extractor Predicting: 142it [01:18,  1.79it/s]Extractor Predicting: 143it [01:19,  1.83it/s]Extractor Predicting: 144it [01:20,  1.84it/s]Extractor Predicting: 145it [01:20,  1.85it/s]Extractor Predicting: 146it [01:21,  1.78it/s]Extractor Predicting: 147it [01:21,  1.75it/s]Extractor Predicting: 148it [01:22,  1.76it/s]Extractor Predicting: 149it [01:22,  1.75it/s]Extractor Predicting: 150it [01:23,  1.78it/s]Extractor Predicting: 151it [01:24,  1.76it/s]Extractor Predicting: 152it [01:24,  1.81it/s]Extractor Predicting: 153it [01:25,  1.72it/s]Extractor Predicting: 154it [01:25,  1.74it/s]Extractor Predicting: 155it [01:26,  1.78it/s]Extractor Predicting: 156it [01:26,  1.82it/s]Extractor Predicting: 157it [01:27,  1.82it/s]Extractor Predicting: 158it [01:27,  1.86it/s]Extractor Predicting: 159it [01:28,  1.78it/s]Extractor Predicting: 160it [01:29,  1.81it/s]Extractor Predicting: 161it [01:29,  1.79it/s]Extractor Predicting: 162it [01:30,  1.77it/s]Extractor Predicting: 163it [01:30,  1.79it/s]Extractor Predicting: 164it [01:31,  1.78it/s]Extractor Predicting: 165it [01:31,  1.76it/s]Extractor Predicting: 166it [01:32,  1.74it/s]Extractor Predicting: 167it [01:33,  1.73it/s]Extractor Predicting: 168it [01:33,  1.71it/s]Extractor Predicting: 169it [01:34,  1.68it/s]Extractor Predicting: 170it [01:34,  1.70it/s]Extractor Predicting: 171it [01:35,  1.64it/s]Extractor Predicting: 172it [01:36,  1.69it/s]Extractor Predicting: 173it [01:36,  1.66it/s]Extractor Predicting: 174it [01:37,  1.63it/s]Extractor Predicting: 175it [01:37,  1.69it/s]Extractor Predicting: 176it [01:38,  1.64it/s]Extractor Predicting: 177it [01:39,  1.62it/s]Extractor Predicting: 178it [01:39,  1.61it/s]Extractor Predicting: 179it [01:40,  1.60it/s]Extractor Predicting: 180it [01:41,  1.61it/s]Extractor Predicting: 181it [01:41,  1.62it/s]Extractor Predicting: 182it [01:42,  1.62it/s]Extractor Predicting: 183it [01:42,  1.63it/s]Extractor Predicting: 184it [01:43,  1.64it/s]Extractor Predicting: 185it [01:44,  1.69it/s]Extractor Predicting: 186it [01:44,  1.69it/s]Extractor Predicting: 187it [01:45,  1.82it/s]Extractor Predicting: 187it [01:45,  1.78it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:29,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:29,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:29,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:29,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:29,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:15:30,349 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:15:30,350 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:15:30,950 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:15:32,028 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:15:32,028 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:34,987 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:35,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:35,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:35,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:15:35,008 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:15:35,766 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:15:35,767 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:15:36,387 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:15:36,589 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:15:36,589 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.75it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.83it/s]Extractor Predicting: 6it [00:03,  1.80it/s]Extractor Predicting: 7it [00:03,  1.78it/s]Extractor Predicting: 8it [00:04,  1.78it/s]Extractor Predicting: 9it [00:05,  1.76it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.80it/s]Extractor Predicting: 12it [00:06,  1.83it/s]Extractor Predicting: 13it [00:07,  1.78it/s]Extractor Predicting: 14it [00:07,  1.80it/s]Extractor Predicting: 15it [00:08,  1.83it/s]Extractor Predicting: 16it [00:08,  1.83it/s]Extractor Predicting: 17it [00:09,  1.82it/s]Extractor Predicting: 18it [00:10,  1.76it/s]Extractor Predicting: 19it [00:10,  1.74it/s]Extractor Predicting: 20it [00:11,  1.74it/s]Extractor Predicting: 21it [00:11,  1.75it/s]Extractor Predicting: 22it [00:12,  1.73it/s]Extractor Predicting: 23it [00:12,  1.75it/s]Extractor Predicting: 24it [00:13,  1.76it/s]Extractor Predicting: 25it [00:14,  1.76it/s]Extractor Predicting: 26it [00:14,  1.77it/s]Extractor Predicting: 27it [00:15,  1.77it/s]Extractor Predicting: 28it [00:15,  1.77it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:16,  1.80it/s]Extractor Predicting: 31it [00:17,  1.69it/s]Extractor Predicting: 32it [00:18,  1.69it/s]Extractor Predicting: 33it [00:18,  1.77it/s]Extractor Predicting: 34it [00:19,  1.80it/s]Extractor Predicting: 35it [00:19,  1.79it/s]Extractor Predicting: 36it [00:20,  1.81it/s]Extractor Predicting: 37it [00:20,  1.73it/s]Extractor Predicting: 38it [00:21,  1.73it/s]Extractor Predicting: 39it [00:22,  1.77it/s]Extractor Predicting: 40it [00:22,  1.78it/s]Extractor Predicting: 41it [00:23,  1.82it/s]Extractor Predicting: 42it [00:23,  1.80it/s]Extractor Predicting: 43it [00:24,  1.78it/s]Extractor Predicting: 44it [00:24,  1.77it/s]Extractor Predicting: 45it [00:25,  1.81it/s]Extractor Predicting: 46it [00:26,  1.59it/s]Extractor Predicting: 47it [00:26,  1.60it/s]Extractor Predicting: 48it [00:27,  1.60it/s]Extractor Predicting: 49it [00:27,  1.60it/s]Extractor Predicting: 50it [00:28,  1.65it/s]Extractor Predicting: 51it [00:29,  1.67it/s]Extractor Predicting: 52it [00:29,  1.70it/s]Extractor Predicting: 53it [00:30,  1.77it/s]Extractor Predicting: 54it [00:30,  1.76it/s]Extractor Predicting: 55it [00:31,  1.78it/s]Extractor Predicting: 56it [00:31,  1.80it/s]Extractor Predicting: 57it [00:32,  1.76it/s]Extractor Predicting: 58it [00:33,  1.79it/s]Extractor Predicting: 59it [00:33,  1.80it/s]Extractor Predicting: 60it [00:34,  1.79it/s]Extractor Predicting: 61it [00:34,  1.76it/s]Extractor Predicting: 62it [00:35,  1.77it/s]Extractor Predicting: 63it [00:35,  1.78it/s]Extractor Predicting: 64it [00:36,  1.78it/s]Extractor Predicting: 65it [00:36,  1.79it/s]Extractor Predicting: 66it [00:37,  1.81it/s]Extractor Predicting: 67it [00:38,  1.79it/s]Extractor Predicting: 68it [00:38,  1.68it/s]Extractor Predicting: 69it [00:39,  1.74it/s]Extractor Predicting: 70it [00:39,  1.68it/s]Extractor Predicting: 71it [00:40,  1.73it/s]Extractor Predicting: 72it [00:40,  1.75it/s]Extractor Predicting: 73it [00:41,  1.79it/s]Extractor Predicting: 74it [00:42,  1.81it/s]Extractor Predicting: 75it [00:42,  1.83it/s]Extractor Predicting: 76it [00:43,  1.77it/s]Extractor Predicting: 77it [00:43,  1.80it/s]Extractor Predicting: 78it [00:44,  1.86it/s]Extractor Predicting: 79it [00:44,  1.80it/s]Extractor Predicting: 80it [00:45,  1.84it/s]Extractor Predicting: 81it [00:45,  1.85it/s]Extractor Predicting: 82it [00:46,  1.85it/s]Extractor Predicting: 83it [00:46,  1.85it/s]Extractor Predicting: 84it [00:47,  1.79it/s]Extractor Predicting: 85it [00:48,  1.80it/s]Extractor Predicting: 86it [00:48,  1.84it/s]Extractor Predicting: 87it [00:49,  1.89it/s]Extractor Predicting: 88it [00:49,  1.93it/s]Extractor Predicting: 89it [00:50,  1.91it/s]Extractor Predicting: 90it [00:50,  1.86it/s]Extractor Predicting: 91it [00:51,  1.88it/s]Extractor Predicting: 92it [00:51,  1.92it/s]Extractor Predicting: 93it [00:52,  1.88it/s]Extractor Predicting: 94it [00:52,  1.91it/s]Extractor Predicting: 95it [00:53,  1.92it/s]Extractor Predicting: 96it [00:53,  1.94it/s]Extractor Predicting: 97it [00:54,  1.89it/s]Extractor Predicting: 98it [00:54,  1.87it/s]Extractor Predicting: 99it [00:55,  1.86it/s]Extractor Predicting: 100it [00:55,  1.89it/s]Extractor Predicting: 101it [00:56,  1.89it/s]Extractor Predicting: 102it [00:57,  1.88it/s]Extractor Predicting: 103it [00:57,  1.83it/s]Extractor Predicting: 104it [00:58,  1.83it/s]Extractor Predicting: 105it [00:58,  1.81it/s]Extractor Predicting: 106it [00:59,  1.80it/s]Extractor Predicting: 107it [00:59,  1.83it/s]Extractor Predicting: 108it [01:00,  1.84it/s]Extractor Predicting: 109it [01:00,  1.79it/s]Extractor Predicting: 110it [01:01,  1.79it/s]Extractor Predicting: 111it [01:02,  1.77it/s]Extractor Predicting: 112it [01:02,  1.81it/s]Extractor Predicting: 113it [01:03,  1.83it/s]Extractor Predicting: 114it [01:03,  1.85it/s]Extractor Predicting: 115it [01:04,  1.79it/s]Extractor Predicting: 116it [01:04,  1.79it/s]Extractor Predicting: 117it [01:05,  1.81it/s]Extractor Predicting: 118it [01:05,  1.81it/s]Extractor Predicting: 119it [01:06,  1.83it/s]Extractor Predicting: 120it [01:07,  1.81it/s]Extractor Predicting: 121it [01:07,  1.84it/s]Extractor Predicting: 122it [01:08,  1.85it/s]Extractor Predicting: 123it [01:08,  1.84it/s]Extractor Predicting: 124it [01:09,  1.85it/s]Extractor Predicting: 125it [01:09,  1.84it/s]Extractor Predicting: 126it [01:10,  1.83it/s]Extractor Predicting: 127it [01:10,  1.85it/s]Extractor Predicting: 128it [01:11,  1.80it/s]Extractor Predicting: 129it [01:12,  1.73it/s]Extractor Predicting: 130it [01:12,  1.79it/s]Extractor Predicting: 131it [01:13,  1.80it/s]Extractor Predicting: 132it [01:13,  1.79it/s]Extractor Predicting: 133it [01:14,  1.83it/s]Extractor Predicting: 134it [01:14,  1.80it/s]Extractor Predicting: 135it [01:15,  1.79it/s]Extractor Predicting: 136it [01:15,  1.80it/s]Extractor Predicting: 137it [01:16,  1.79it/s]Extractor Predicting: 138it [01:17,  1.77it/s]Extractor Predicting: 139it [01:17,  1.80it/s]Extractor Predicting: 140it [01:18,  1.79it/s]Extractor Predicting: 141it [01:18,  1.72it/s]Extractor Predicting: 142it [01:19,  1.73it/s]Extractor Predicting: 143it [01:19,  1.76it/s]Extractor Predicting: 144it [01:20,  1.77it/s]Extractor Predicting: 145it [01:20,  1.76it/s]Extractor Predicting: 146it [01:21,  1.77it/s]Extractor Predicting: 147it [01:22,  1.71it/s]Extractor Predicting: 148it [01:22,  1.76it/s]Extractor Predicting: 149it [01:23,  1.75it/s]Extractor Predicting: 150it [01:23,  1.72it/s]Extractor Predicting: 151it [01:24,  1.72it/s]Extractor Predicting: 152it [01:25,  1.75it/s]Extractor Predicting: 153it [01:25,  1.76it/s]Extractor Predicting: 154it [01:26,  1.79it/s]Extractor Predicting: 155it [01:26,  1.82it/s]Extractor Predicting: 156it [01:27,  1.80it/s]Extractor Predicting: 157it [01:27,  1.82it/s]Extractor Predicting: 158it [01:28,  1.81it/s]Extractor Predicting: 159it [01:28,  1.78it/s]Extractor Predicting: 160it [01:29,  1.78it/s]Extractor Predicting: 161it [01:30,  1.80it/s]Extractor Predicting: 162it [01:30,  1.78it/s]Extractor Predicting: 163it [01:31,  1.80it/s]Extractor Predicting: 164it [01:31,  1.79it/s]Extractor Predicting: 165it [01:32,  1.78it/s]Extractor Predicting: 166it [01:32,  1.79it/s]Extractor Predicting: 167it [01:33,  1.81it/s]Extractor Predicting: 168it [01:33,  1.79it/s]Extractor Predicting: 169it [01:34,  1.78it/s]Extractor Predicting: 170it [01:35,  1.82it/s]Extractor Predicting: 171it [01:35,  1.86it/s]Extractor Predicting: 172it [01:36,  1.85it/s]Extractor Predicting: 173it [01:36,  1.83it/s]Extractor Predicting: 174it [01:37,  1.80it/s]Extractor Predicting: 175it [01:37,  1.74it/s]Extractor Predicting: 176it [01:38,  1.50it/s]Extractor Predicting: 177it [01:39,  1.57it/s]Extractor Predicting: 178it [01:39,  1.60it/s]Extractor Predicting: 179it [01:40,  1.65it/s]Extractor Predicting: 180it [01:41,  1.65it/s]Extractor Predicting: 181it [01:41,  1.67it/s]Extractor Predicting: 182it [01:42,  1.68it/s]Extractor Predicting: 183it [01:42,  1.64it/s]Extractor Predicting: 184it [01:43,  1.65it/s]Extractor Predicting: 185it [01:44,  1.67it/s]Extractor Predicting: 186it [01:44,  1.70it/s]Extractor Predicting: 187it [01:45,  1.67it/s]Extractor Predicting: 188it [01:45,  1.69it/s]Extractor Predicting: 189it [01:46,  1.71it/s]Extractor Predicting: 190it [01:46,  1.70it/s]Extractor Predicting: 191it [01:47,  1.73it/s]Extractor Predicting: 192it [01:48,  1.73it/s]Extractor Predicting: 193it [01:48,  1.73it/s]Extractor Predicting: 194it [01:49,  1.72it/s]Extractor Predicting: 195it [01:49,  1.78it/s]Extractor Predicting: 196it [01:50,  1.82it/s]Extractor Predicting: 197it [01:50,  1.81it/s]Extractor Predicting: 198it [01:51,  1.81it/s]Extractor Predicting: 199it [01:51,  1.81it/s]Extractor Predicting: 200it [01:52,  1.81it/s]Extractor Predicting: 201it [01:53,  1.80it/s]Extractor Predicting: 202it [01:53,  1.80it/s]Extractor Predicting: 203it [01:54,  1.80it/s]Extractor Predicting: 204it [01:54,  1.79it/s]Extractor Predicting: 205it [01:55,  1.79it/s]Extractor Predicting: 206it [01:55,  1.78it/s]Extractor Predicting: 207it [01:56,  1.77it/s]Extractor Predicting: 208it [01:56,  1.82it/s]Extractor Predicting: 209it [01:57,  1.84it/s]Extractor Predicting: 210it [01:58,  1.80it/s]Extractor Predicting: 211it [01:58,  1.82it/s]Extractor Predicting: 212it [01:59,  1.82it/s]Extractor Predicting: 213it [01:59,  1.79it/s]Extractor Predicting: 214it [02:00,  1.76it/s]Extractor Predicting: 215it [02:00,  1.77it/s]Extractor Predicting: 216it [02:01,  1.77it/s]Extractor Predicting: 217it [02:01,  1.82it/s]Extractor Predicting: 218it [02:02,  1.77it/s]Extractor Predicting: 219it [02:03,  1.75it/s]Extractor Predicting: 220it [02:03,  1.81it/s]Extractor Predicting: 221it [02:04,  1.76it/s]Extractor Predicting: 222it [02:04,  1.76it/s]Extractor Predicting: 223it [02:05,  1.77it/s]Extractor Predicting: 224it [02:05,  1.77it/s]Extractor Predicting: 225it [02:06,  1.82it/s]Extractor Predicting: 226it [02:07,  1.82it/s]Extractor Predicting: 227it [02:07,  1.84it/s]Extractor Predicting: 228it [02:08,  1.84it/s]Extractor Predicting: 229it [02:08,  1.82it/s]Extractor Predicting: 230it [02:09,  1.86it/s]Extractor Predicting: 231it [02:09,  1.82it/s]Extractor Predicting: 232it [02:10,  1.83it/s]Extractor Predicting: 233it [02:10,  1.83it/s]Extractor Predicting: 234it [02:11,  1.79it/s]Extractor Predicting: 235it [02:11,  1.79it/s]Extractor Predicting: 236it [02:12,  1.80it/s]Extractor Predicting: 237it [02:13,  1.73it/s]Extractor Predicting: 238it [02:13,  1.76it/s]Extractor Predicting: 239it [02:14,  1.77it/s]Extractor Predicting: 240it [02:14,  1.80it/s]Extractor Predicting: 241it [02:15,  1.85it/s]Extractor Predicting: 242it [02:15,  1.82it/s]Extractor Predicting: 243it [02:16,  1.83it/s]Extractor Predicting: 244it [02:16,  1.84it/s]Extractor Predicting: 245it [02:17,  1.81it/s]Extractor Predicting: 246it [02:18,  1.84it/s]Extractor Predicting: 247it [02:18,  1.82it/s]Extractor Predicting: 248it [02:19,  1.85it/s]Extractor Predicting: 249it [02:19,  1.79it/s]Extractor Predicting: 250it [02:20,  1.87it/s]Extractor Predicting: 251it [02:20,  1.83it/s]Extractor Predicting: 252it [02:21,  1.86it/s]Extractor Predicting: 253it [02:21,  1.92it/s]Extractor Predicting: 254it [02:22,  1.87it/s]Extractor Predicting: 255it [02:22,  1.83it/s]Extractor Predicting: 256it [02:23,  1.84it/s]Extractor Predicting: 257it [02:23,  1.84it/s]Extractor Predicting: 258it [02:24,  1.86it/s]Extractor Predicting: 259it [02:25,  1.86it/s]Extractor Predicting: 260it [02:25,  1.84it/s]Extractor Predicting: 261it [02:26,  1.77it/s]Extractor Predicting: 262it [02:26,  1.80it/s]Extractor Predicting: 263it [02:27,  1.74it/s]Extractor Predicting: 264it [02:27,  1.79it/s]Extractor Predicting: 265it [02:28,  1.78it/s]Extractor Predicting: 266it [02:29,  1.76it/s]Extractor Predicting: 267it [02:29,  1.73it/s]Extractor Predicting: 268it [02:30,  1.74it/s]Extractor Predicting: 269it [02:30,  1.72it/s]Extractor Predicting: 270it [02:31,  1.74it/s]Extractor Predicting: 271it [02:31,  1.76it/s]Extractor Predicting: 272it [02:32,  1.81it/s]Extractor Predicting: 273it [02:33,  1.77it/s]Extractor Predicting: 274it [02:33,  1.75it/s]Extractor Predicting: 275it [02:34,  1.75it/s]Extractor Predicting: 276it [02:34,  1.79it/s]Extractor Predicting: 277it [02:35,  1.77it/s]Extractor Predicting: 278it [02:35,  1.78it/s]Extractor Predicting: 279it [02:36,  1.74it/s]Extractor Predicting: 280it [02:37,  1.75it/s]Extractor Predicting: 281it [02:37,  1.77it/s]Extractor Predicting: 282it [02:38,  1.77it/s]Extractor Predicting: 283it [02:38,  1.77it/s]Extractor Predicting: 284it [02:39,  1.80it/s]Extractor Predicting: 285it [02:39,  1.83it/s]Extractor Predicting: 286it [02:40,  1.82it/s]Extractor Predicting: 287it [02:40,  1.73it/s]Extractor Predicting: 288it [02:41,  1.75it/s]Extractor Predicting: 289it [02:42,  1.74it/s]Extractor Predicting: 290it [02:43,  1.49it/s]Extractor Predicting: 291it [02:43,  1.56it/s]Extractor Predicting: 292it [02:44,  1.59it/s]Extractor Predicting: 293it [02:44,  1.62it/s]Extractor Predicting: 294it [02:45,  1.62it/s]Extractor Predicting: 295it [02:45,  1.66it/s]Extractor Predicting: 296it [02:46,  1.68it/s]Extractor Predicting: 297it [02:47,  1.68it/s]Extractor Predicting: 298it [02:47,  1.67it/s]Extractor Predicting: 299it [02:48,  1.68it/s]Extractor Predicting: 300it [02:48,  1.63it/s]Extractor Predicting: 301it [02:49,  1.59it/s]Extractor Predicting: 302it [02:50,  1.58it/s]Extractor Predicting: 303it [02:50,  1.60it/s]Extractor Predicting: 304it [02:51,  1.55it/s]Extractor Predicting: 305it [02:52,  1.53it/s]Extractor Predicting: 306it [02:52,  1.61it/s]Extractor Predicting: 307it [02:53,  1.67it/s]Extractor Predicting: 308it [02:53,  1.68it/s]Extractor Predicting: 309it [02:54,  1.76it/s]Extractor Predicting: 310it [02:55,  1.75it/s]Extractor Predicting: 311it [02:55,  1.70it/s]Extractor Predicting: 312it [02:56,  1.72it/s]Extractor Predicting: 313it [02:56,  1.74it/s]Extractor Predicting: 314it [02:57,  1.69it/s]Extractor Predicting: 315it [02:57,  1.78it/s]Extractor Predicting: 316it [02:58,  1.79it/s]Extractor Predicting: 317it [02:59,  1.77it/s]Extractor Predicting: 318it [02:59,  1.78it/s]Extractor Predicting: 319it [03:00,  1.76it/s]Extractor Predicting: 320it [03:00,  1.75it/s]Extractor Predicting: 321it [03:01,  1.78it/s]Extractor Predicting: 322it [03:01,  1.82it/s]Extractor Predicting: 323it [03:02,  1.83it/s]Extractor Predicting: 324it [03:02,  1.84it/s]Extractor Predicting: 325it [03:03,  1.82it/s]Extractor Predicting: 326it [03:03,  1.83it/s]Extractor Predicting: 327it [03:04,  1.84it/s]Extractor Predicting: 328it [03:05,  1.82it/s]Extractor Predicting: 329it [03:05,  1.81it/s]Extractor Predicting: 330it [03:06,  1.79it/s]Extractor Predicting: 331it [03:06,  1.79it/s]Extractor Predicting: 332it [03:07,  1.80it/s]Extractor Predicting: 333it [03:07,  1.78it/s]Extractor Predicting: 334it [03:08,  1.78it/s]Extractor Predicting: 335it [03:09,  1.79it/s]Extractor Predicting: 336it [03:09,  1.83it/s]Extractor Predicting: 337it [03:10,  1.82it/s]Extractor Predicting: 338it [03:10,  1.83it/s]Extractor Predicting: 339it [03:11,  1.80it/s]Extractor Predicting: 340it [03:11,  1.82it/s]Extractor Predicting: 341it [03:12,  1.82it/s]Extractor Predicting: 342it [03:12,  1.79it/s]Extractor Predicting: 343it [03:13,  1.79it/s]Extractor Predicting: 344it [03:13,  1.85it/s]Extractor Predicting: 345it [03:14,  1.85it/s]Extractor Predicting: 346it [03:14,  1.89it/s]Extractor Predicting: 347it [03:15,  1.94it/s]Extractor Predicting: 348it [03:15,  1.91it/s]Extractor Predicting: 349it [03:16,  1.93it/s]Extractor Predicting: 350it [03:17,  1.92it/s]Extractor Predicting: 351it [03:17,  1.89it/s]Extractor Predicting: 352it [03:18,  1.89it/s]Extractor Predicting: 353it [03:18,  1.91it/s]Extractor Predicting: 354it [03:19,  1.92it/s]Extractor Predicting: 355it [03:19,  1.92it/s]Extractor Predicting: 356it [03:20,  1.95it/s]Extractor Predicting: 357it [03:20,  1.87it/s]Extractor Predicting: 358it [03:21,  1.88it/s]Extractor Predicting: 359it [03:21,  1.89it/s]Extractor Predicting: 360it [03:22,  1.88it/s]Extractor Predicting: 361it [03:22,  1.89it/s]Extractor Predicting: 362it [03:23,  1.90it/s]Extractor Predicting: 363it [03:23,  1.87it/s]Extractor Predicting: 364it [03:24,  1.93it/s]Extractor Predicting: 365it [03:24,  1.86it/s]Extractor Predicting: 366it [03:25,  1.83it/s]Extractor Predicting: 367it [03:26,  1.77it/s]Extractor Predicting: 368it [03:26,  1.75it/s]Extractor Predicting: 369it [03:27,  1.76it/s]Extractor Predicting: 370it [03:27,  1.72it/s]Extractor Predicting: 371it [03:28,  1.76it/s]Extractor Predicting: 372it [03:28,  1.78it/s]Extractor Predicting: 373it [03:29,  1.81it/s]Extractor Predicting: 374it [03:30,  1.81it/s]Extractor Predicting: 375it [03:30,  1.74it/s]Extractor Predicting: 376it [03:31,  1.74it/s]Extractor Predicting: 377it [03:31,  1.74it/s]Extractor Predicting: 378it [03:32,  1.74it/s]Extractor Predicting: 379it [03:32,  1.75it/s]Extractor Predicting: 380it [03:33,  1.76it/s]Extractor Predicting: 381it [03:34,  1.74it/s]Extractor Predicting: 382it [03:34,  1.78it/s]Extractor Predicting: 383it [03:35,  1.80it/s]Extractor Predicting: 384it [03:35,  1.83it/s]Extractor Predicting: 385it [03:36,  1.81it/s]Extractor Predicting: 386it [03:36,  1.84it/s]Extractor Predicting: 387it [03:37,  1.82it/s]Extractor Predicting: 388it [03:37,  1.87it/s]Extractor Predicting: 389it [03:38,  1.82it/s]Extractor Predicting: 390it [03:39,  1.79it/s]Extractor Predicting: 391it [03:39,  1.82it/s]Extractor Predicting: 392it [03:40,  1.89it/s]Extractor Predicting: 393it [03:40,  1.92it/s]Extractor Predicting: 394it [03:41,  1.92it/s]Extractor Predicting: 395it [03:41,  1.94it/s]Extractor Predicting: 396it [03:42,  1.95it/s]Extractor Predicting: 397it [03:42,  1.91it/s]Extractor Predicting: 398it [03:43,  1.89it/s]Extractor Predicting: 399it [03:43,  1.89it/s]Extractor Predicting: 400it [03:44,  1.86it/s]Extractor Predicting: 401it [03:44,  1.89it/s]Extractor Predicting: 402it [03:45,  1.89it/s]Extractor Predicting: 403it [03:45,  1.89it/s]Extractor Predicting: 404it [03:46,  1.90it/s]Extractor Predicting: 405it [03:46,  1.85it/s]Extractor Predicting: 406it [03:47,  1.87it/s]Extractor Predicting: 407it [03:47,  1.87it/s]Extractor Predicting: 408it [03:48,  1.87it/s]Extractor Predicting: 409it [03:49,  1.68it/s]Extractor Predicting: 410it [03:49,  1.72it/s]Extractor Predicting: 411it [03:50,  1.75it/s]Extractor Predicting: 412it [03:50,  1.77it/s]Extractor Predicting: 413it [03:51,  1.71it/s]Extractor Predicting: 414it [03:52,  1.66it/s]Extractor Predicting: 415it [03:52,  1.62it/s]Extractor Predicting: 416it [03:53,  1.63it/s]Extractor Predicting: 417it [03:54,  1.66it/s]Extractor Predicting: 418it [03:54,  1.70it/s]Extractor Predicting: 419it [03:55,  1.68it/s]Extractor Predicting: 420it [03:55,  1.66it/s]Extractor Predicting: 421it [03:56,  1.45it/s]Extractor Predicting: 422it [03:57,  1.52it/s]Extractor Predicting: 423it [03:57,  1.59it/s]Extractor Predicting: 424it [03:58,  1.59it/s]Extractor Predicting: 425it [03:59,  1.61it/s]Extractor Predicting: 426it [03:59,  1.62it/s]Extractor Predicting: 427it [04:00,  1.65it/s]Extractor Predicting: 428it [04:00,  1.69it/s]Extractor Predicting: 429it [04:01,  1.72it/s]Extractor Predicting: 430it [04:01,  1.71it/s]Extractor Predicting: 431it [04:02,  1.73it/s]Extractor Predicting: 432it [04:03,  1.72it/s]Extractor Predicting: 433it [04:03,  1.73it/s]Extractor Predicting: 434it [04:04,  1.72it/s]Extractor Predicting: 435it [04:04,  1.75it/s]Extractor Predicting: 436it [04:05,  1.77it/s]Extractor Predicting: 437it [04:05,  1.76it/s]Extractor Predicting: 438it [04:06,  1.78it/s]Extractor Predicting: 439it [04:07,  1.77it/s]Extractor Predicting: 440it [04:07,  1.77it/s]Extractor Predicting: 441it [04:08,  1.75it/s]Extractor Predicting: 442it [04:08,  1.74it/s]Extractor Predicting: 443it [04:09,  1.72it/s]Extractor Predicting: 444it [04:09,  1.73it/s]Extractor Predicting: 445it [04:10,  1.71it/s]Extractor Predicting: 446it [04:11,  1.72it/s]Extractor Predicting: 447it [04:11,  1.73it/s]Extractor Predicting: 448it [04:12,  1.72it/s]Extractor Predicting: 449it [04:12,  1.72it/s]Extractor Predicting: 450it [04:13,  1.76it/s]Extractor Predicting: 451it [04:14,  1.75it/s]Extractor Predicting: 452it [04:14,  1.74it/s]Extractor Predicting: 453it [04:15,  1.74it/s]Extractor Predicting: 454it [04:15,  1.75it/s]Extractor Predicting: 455it [04:16,  1.67it/s]Extractor Predicting: 456it [04:16,  1.71it/s]Extractor Predicting: 457it [04:17,  1.69it/s]Extractor Predicting: 458it [04:18,  1.72it/s]Extractor Predicting: 459it [04:18,  1.72it/s]Extractor Predicting: 460it [04:19,  1.71it/s]Extractor Predicting: 461it [04:19,  1.72it/s]Extractor Predicting: 462it [04:20,  1.73it/s]Extractor Predicting: 463it [04:21,  1.69it/s]Extractor Predicting: 464it [04:21,  1.66it/s]Extractor Predicting: 465it [04:22,  1.67it/s]Extractor Predicting: 466it [04:22,  1.70it/s]Extractor Predicting: 467it [04:23,  1.72it/s]Extractor Predicting: 468it [04:23,  1.75it/s]Extractor Predicting: 469it [04:24,  1.81it/s]Extractor Predicting: 470it [04:25,  1.79it/s]Extractor Predicting: 471it [04:25,  1.75it/s]Extractor Predicting: 472it [04:26,  1.77it/s]Extractor Predicting: 473it [04:26,  1.70it/s]Extractor Predicting: 474it [04:27,  1.70it/s]Extractor Predicting: 475it [04:27,  1.70it/s]Extractor Predicting: 476it [04:28,  1.73it/s]Extractor Predicting: 477it [04:29,  1.73it/s]Extractor Predicting: 478it [04:29,  1.74it/s]Extractor Predicting: 479it [04:30,  1.63it/s]Extractor Predicting: 480it [04:31,  1.60it/s]Extractor Predicting: 481it [04:31,  1.61it/s]Extractor Predicting: 482it [04:32,  1.62it/s]Extractor Predicting: 483it [04:32,  1.61it/s]Extractor Predicting: 484it [04:33,  1.49it/s]Extractor Predicting: 485it [04:34,  1.53it/s]Extractor Predicting: 486it [04:34,  1.59it/s]Extractor Predicting: 487it [04:35,  1.64it/s]Extractor Predicting: 488it [04:36,  1.62it/s]Extractor Predicting: 489it [04:36,  1.63it/s]Extractor Predicting: 490it [04:37,  1.60it/s]Extractor Predicting: 491it [04:37,  1.60it/s]Extractor Predicting: 492it [04:38,  1.60it/s]Extractor Predicting: 493it [04:39,  1.62it/s]Extractor Predicting: 494it [04:39,  1.59it/s]Extractor Predicting: 495it [04:40,  1.60it/s]Extractor Predicting: 496it [04:41,  1.61it/s]Extractor Predicting: 497it [04:41,  1.62it/s]Extractor Predicting: 498it [04:42,  1.64it/s]Extractor Predicting: 499it [04:42,  1.59it/s]Extractor Predicting: 500it [04:43,  1.59it/s]Extractor Predicting: 501it [04:44,  1.62it/s]Extractor Predicting: 502it [04:44,  1.60it/s]Extractor Predicting: 503it [04:45,  1.58it/s]Extractor Predicting: 504it [04:46,  1.56it/s]Extractor Predicting: 505it [04:46,  1.54it/s]Extractor Predicting: 506it [04:47,  1.54it/s]Extractor Predicting: 507it [04:48,  1.57it/s]Extractor Predicting: 508it [04:48,  1.77it/s]Extractor Predicting: 508it [04:48,  1.76it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:39,401 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:39,418 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:39,418 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:39,418 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:39,418 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:20:40,171 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:20:40,172 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:20:40,769 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:20:41,870 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:20:41,870 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:44,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:44,837 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:44,837 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:44,837 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:20:44,837 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:20:45,616 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:20:45,617 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:20:46,228 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:20:46,455 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:20:46,455 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.67it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:08,  1.71it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:10,  1.70it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.66it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.64it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:14,  1.70it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:16,  1.67it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:17,  1.65it/s]Extractor Predicting: 31it [00:18,  1.71it/s]Extractor Predicting: 32it [00:18,  1.76it/s]Extractor Predicting: 33it [00:19,  1.79it/s]Extractor Predicting: 34it [00:20,  1.82it/s]Extractor Predicting: 35it [00:20,  1.80it/s]Extractor Predicting: 36it [00:21,  1.76it/s]Extractor Predicting: 37it [00:21,  1.75it/s]Extractor Predicting: 38it [00:22,  1.78it/s]Extractor Predicting: 39it [00:22,  1.79it/s]Extractor Predicting: 40it [00:23,  1.82it/s]Extractor Predicting: 41it [00:23,  1.80it/s]Extractor Predicting: 42it [00:24,  1.80it/s]Extractor Predicting: 43it [00:25,  1.80it/s]Extractor Predicting: 44it [00:25,  1.86it/s]Extractor Predicting: 45it [00:26,  1.82it/s]Extractor Predicting: 46it [00:26,  1.83it/s]Extractor Predicting: 47it [00:27,  1.83it/s]Extractor Predicting: 48it [00:27,  1.77it/s]Extractor Predicting: 49it [00:28,  1.64it/s]Extractor Predicting: 50it [00:29,  1.66it/s]Extractor Predicting: 51it [00:29,  1.67it/s]Extractor Predicting: 52it [00:30,  1.69it/s]Extractor Predicting: 53it [00:30,  1.69it/s]Extractor Predicting: 54it [00:31,  1.65it/s]Extractor Predicting: 55it [00:32,  1.70it/s]Extractor Predicting: 56it [00:32,  1.74it/s]Extractor Predicting: 57it [00:33,  1.75it/s]Extractor Predicting: 58it [00:33,  1.75it/s]Extractor Predicting: 59it [00:34,  1.73it/s]Extractor Predicting: 60it [00:34,  1.72it/s]Extractor Predicting: 61it [00:35,  1.74it/s]Extractor Predicting: 62it [00:36,  1.74it/s]Extractor Predicting: 63it [00:36,  1.74it/s]Extractor Predicting: 64it [00:37,  1.75it/s]Extractor Predicting: 65it [00:37,  1.72it/s]Extractor Predicting: 66it [00:38,  1.74it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:39,  1.67it/s]Extractor Predicting: 69it [00:40,  1.64it/s]Extractor Predicting: 70it [00:40,  1.64it/s]Extractor Predicting: 71it [00:41,  1.64it/s]Extractor Predicting: 72it [00:42,  1.61it/s]Extractor Predicting: 73it [00:42,  1.65it/s]Extractor Predicting: 74it [00:43,  1.61it/s]Extractor Predicting: 75it [00:43,  1.83it/s]Extractor Predicting: 75it [00:43,  1.71it/s]
[INFO|configuration_utils.py:515] 2023-08-28 10:21:32,781 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:21:32,810 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:21:32,864 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:21:32,865 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 10:21:32,898 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:21:42,822 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 10:21:42,829 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 10:21:42,893 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:21:42,894 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:21:42,948 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:21:42,987 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:21:42,987 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:21:42,987 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:21:42,987 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:21:42,987 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:21:42,987 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 10:21:43,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:43,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:44,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:45,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:45,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:46,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:46,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:47,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:48,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:48,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:49,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:49,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:50,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:50,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:51,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:51,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:52,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:53,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:53,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:54,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:54,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:55,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<03:56, 12.44s/it][WARNING|generation_utils.py:914] 2023-08-28 10:21:55,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:56,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:56,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:57,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:58,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:58,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:21:59,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:00,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:00,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:01,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:01,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:02,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:03,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:03,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:04,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:04,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:05,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:05,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:06,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:06,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:07,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:07,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:08,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:09,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:09,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:26<04:05, 13.62s/it][WARNING|generation_utils.py:914] 2023-08-28 10:22:10,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:10,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:11,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:12,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:13,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:14,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:14,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:15,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:16,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:16,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:17,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:18,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:19,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:19,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:20,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:21,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:21,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:22,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:23,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:23,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:24,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:25,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:26,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:26,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:27,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:27,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:28,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:45<04:33, 16.11s/it][WARNING|generation_utils.py:914] 2023-08-28 10:22:29,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:29,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:30,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:30,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:31,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:32,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:33,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:33,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:34,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:34,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:35,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:36,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:36,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:37,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:37,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:38,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:39,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:39,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:40,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:41,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:41,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:42,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:42,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:00<04:06, 15.40s/it][WARNING|generation_utils.py:914] 2023-08-28 10:22:43,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:44,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:44,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:45,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:45,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:46,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:47,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:47,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:48,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:48,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:49,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:50,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:50,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:51,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:51,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:52,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:53,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:53,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:54,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:54,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:55,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:56,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:56,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:57,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:57,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:58,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:15<03:51, 15.44s/it][WARNING|generation_utils.py:914] 2023-08-28 10:22:59,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:22:59,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:00,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:00,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:01,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:01,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:02,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:03,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:03,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:04,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:05,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:05,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:06,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:06,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:07,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:07,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:08,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:09,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:09,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:10,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:10,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:11,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:12,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:12,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:13,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:14,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:14,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:32<03:40, 15.72s/it][WARNING|generation_utils.py:914] 2023-08-28 10:23:15,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:15,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:16,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:17,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:17,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:18,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:19,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:19,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:20,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:21,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:21,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:22,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:22,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:23,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:23,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:24,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:25,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:25,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:26,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:26,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:27,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:27,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:28,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:45<03:16, 15.09s/it][WARNING|generation_utils.py:914] 2023-08-28 10:23:29,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:29,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:30,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:30,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:31,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:32,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:32,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:33,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:34,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:34,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:35,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:35,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:36,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:36,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:37,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:37,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:38,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:39,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:39,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:40,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:41,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:41,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:42,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:42,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:43,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:00<03:00, 15.01s/it][WARNING|generation_utils.py:914] 2023-08-28 10:23:43,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:44,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:45,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:45,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:46,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:46,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:47,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:48,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:48,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:49,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:50,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:50,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:51,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:51,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:52,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:52,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:53,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:54,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:54,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:55,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:56,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:56,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:57,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:57,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:58,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:15<02:45, 15.06s/it][WARNING|generation_utils.py:914] 2023-08-28 10:23:59,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:23:59,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:00,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:00,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:01,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:02,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:02,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:03,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:04,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:04,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:05,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:05,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:06,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:06,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:07,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:07,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:08,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:09,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:09,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:10,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:10,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:11,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:12,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:12,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:30<02:28, 14.84s/it][WARNING|generation_utils.py:914] 2023-08-28 10:24:13,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:14,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:14,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:15,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:16,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:16,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:17,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:18,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:18,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:19,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:19,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:20,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:21,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:21,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:22,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:22,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:23,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:24,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:24,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:25,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:26,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:26,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:27,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:44<02:12, 14.76s/it][WARNING|generation_utils.py:914] 2023-08-28 10:24:28,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:28,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:29,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:29,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:30,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:31,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:31,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:32,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:33,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:33,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:34,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:35,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:35,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:36,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:36,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:37,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:38,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:38,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:39,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:40,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:40,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:41,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:42,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:59<01:57, 14.69s/it][WARNING|generation_utils.py:914] 2023-08-28 10:24:42,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:43,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:43,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:44,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:44,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:45,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:46,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:46,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:47,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:47,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:48,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:49,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:49,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:50,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:51,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:51,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:52,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:52,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:53,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:54,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:54,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:55,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:55,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:13<01:40, 14.43s/it][WARNING|generation_utils.py:914] 2023-08-28 10:24:56,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:56,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:57,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:58,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:58,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:59,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:24:59,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:00,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:00,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:01,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:02,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:02,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:03,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:03,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:04,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:04,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:05,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:06,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:07,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:07,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:08,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:08,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:09,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:26<01:25, 14.18s/it][WARNING|generation_utils.py:914] 2023-08-28 10:25:10,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:10,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:11,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:11,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:12,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:13,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:13,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:14,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:14,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:15,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:16,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:16,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:17,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:17,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:18,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:18,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:19,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:20,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:20,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:21,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:21,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:39<01:08, 13.72s/it][WARNING|generation_utils.py:914] 2023-08-28 10:25:22,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:23,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:23,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:24,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:25,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:25,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:26,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:27,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:27,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:28,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:29,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:29,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:30,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:31,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:31,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:32,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:32,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:33,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:34,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:34,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:35,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:36,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:36,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:37,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:38,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:38,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:39,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:40,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:40,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:58<01:00, 15.20s/it][WARNING|generation_utils.py:914] 2023-08-28 10:25:41,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:41,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:42,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:43,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:43,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:44,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:44,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:45,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:46,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:46,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:47,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:47,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:48,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:48,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:49,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:50,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:50,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:51,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:51,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:52,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:52,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:10<00:42, 14.29s/it][WARNING|generation_utils.py:914] 2023-08-28 10:25:53,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:54,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:54,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:55,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:55,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:56,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:57,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:57,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:58,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:58,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:59,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:25:59,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:00,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:01,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:01,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:02,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:02,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:03,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:04,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:04,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:05,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:05,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:06,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:23<00:28, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-28 10:26:07,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:07,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:08,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:08,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:09,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:09,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:10,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:11,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:11,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:12,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:12,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:13,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:14,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:14,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:15,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:16,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:16,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:17,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:17,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:18,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:19,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:19,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:20,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:38<00:14, 14.12s/it][WARNING|generation_utils.py:914] 2023-08-28 10:26:21,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:21,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:22,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:23,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:24,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:24,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:25,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:25,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:26,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:27,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:27,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:28,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:28,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:29,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:29,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:30,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:31,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:31,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:32,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:33,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:33,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:34,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:35,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:35,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:26:36,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:53<00:00, 14.58s/it]Generating: 100%|██████████| 20/20 [04:53<00:00, 14.69s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:46,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:46,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:46,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:46,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:46,314 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:26:47,153 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:26:47,155 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:26:47,789 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:26:48,911 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:26:48,911 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:51,896 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:51,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:51,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:51,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:26:51,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:26:52,702 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:26:52,704 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:26:53,317 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:26:53,552 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:26:53,552 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 358, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7118055555555556, 'errors': {'', "('συῆνος', 'said to be the same as', '', 'They are known as a form of Greek σάρτικρος ( συῆνος ) , τεῖς ( συθροτος ) , τίνρας ( συῆλώηνος ) ; and of the forms σατος τικρος in Latin .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : student .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 331, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 503, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7391826923076923, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 303, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 466, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 531, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 576, 'raw': 800}
{'target': 600, 'success': 598, 'raw': 832}
{'target': 600, 'success': 620, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7175925925925926, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8233695652173914, 'errors': {'', "('', 'continent', 'Aegean Sea', 'The island of Aranet , located in the Aegean Sea , is one of two islands that the Greek island of Argos has in common with the Arabian sea .')"}}
['Relation : field of this occupation . Context : The main characters in the series are The New Guy , The New York City Police Detective and the Los Angeles Police Department . Head Entity : Los Angeles Police Department , Tail Entity : police .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Walter L. Cusack', 'field of this occupation', '', 'Walter L. Cusack ( June 23 , 1924 February 21 , 2002 ) was an English cricket er who played six Tests in England .')"}}
['Relation : field of work . Context : Following his graduation in 1957 , he founded the PSA ( now in association with B. C. ) , an academic association for linguistics , linguistics , and other fields , and served as a member of the Princeton University Research Committee . Head Entity : linguistics , Tail Entity : graduate .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.76875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n']
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n', 'Relation : given name . Context : James M. Young ( February 14 , 1821 October 19 , 1924 ) was an American poet and writer specializing in the arts . Head Entity : James M. Young , Tail Entity : named name .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8206521739130435, 'errors': {'', "('Doctor Who', 'given name', '', 'He is best known for his play as a character on Doctor Who , in which he played the name of the villainess in the Doctor Who special , The One and Only Thing .')", 'too many values to unpack (expected 2)', "('Joseph L. M. Rennelly', 'given name', '', 'Joseph L. M. Rennelly is a former New York City mayor who served as the 62nd governor of New York City .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Never Say Goodbye', 'lyrics by', '', 'After graduating from Berklee College of Music , he started to write a single called Never Say Goodbye that debuted at No. 1 on the Billboard 200 single Hot Hot Song in July 1991 .')", "('Never Let Me Go', 'lyrics by', '', 'She followed this up with her sixth studio album , Never Let Me Go , which is now a cult classic on the Billboard R&B Top 40 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Central railway', 'owned by', '', 'It was built between 1941 and 1943 under the direction of the Central railway company ( D.')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9077380952380952, 'errors': {'', "('GMAW', 'performer', '', 'He also performed with the band Pampers on the opening night of WMAW in Los Angeles and the opening night of the ABC drama GMAW in Atlanta .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 128, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 185, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 223, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 288, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 371, 'raw': 576}
{'target': 600, 'success': 393, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 433, 'raw': 672}
{'target': 600, 'success': 451, 'raw': 704}
{'target': 600, 'success': 471, 'raw': 736}
{'target': 600, 'success': 495, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 583, 'raw': 896}
{'target': 600, 'success': 601, 'raw': 928}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.6476293103448276, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : replaces .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 17944
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18044, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.58it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:01,  1.52it/s]Extractor Estimating: 4it [00:02,  1.61it/s]Extractor Estimating: 5it [00:03,  1.63it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.68it/s]Extractor Estimating: 8it [00:04,  1.67it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.70it/s]Extractor Estimating: 11it [00:06,  1.71it/s]Extractor Estimating: 12it [00:07,  1.69it/s]Extractor Estimating: 13it [00:07,  1.69it/s]Extractor Estimating: 14it [00:08,  1.72it/s]Extractor Estimating: 15it [00:09,  1.69it/s]Extractor Estimating: 16it [00:09,  1.63it/s]Extractor Estimating: 17it [00:10,  1.68it/s]Extractor Estimating: 18it [00:10,  1.65it/s]Extractor Estimating: 19it [00:11,  1.60it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:12,  1.65it/s]Extractor Estimating: 22it [00:13,  1.69it/s]Extractor Estimating: 23it [00:13,  1.66it/s]Extractor Estimating: 24it [00:14,  1.67it/s]Extractor Estimating: 25it [00:15,  1.63it/s]Extractor Estimating: 26it [00:15,  1.55it/s]Extractor Estimating: 27it [00:16,  1.58it/s]Extractor Estimating: 28it [00:17,  1.63it/s]Extractor Estimating: 29it [00:17,  1.66it/s]Extractor Estimating: 30it [00:18,  1.69it/s]Extractor Estimating: 31it [00:18,  1.71it/s]Extractor Estimating: 32it [00:19,  1.75it/s]Extractor Estimating: 33it [00:19,  1.71it/s]Extractor Estimating: 34it [00:20,  1.68it/s]Extractor Estimating: 35it [00:21,  1.64it/s]Extractor Estimating: 36it [00:21,  1.68it/s]Extractor Estimating: 37it [00:22,  1.71it/s]Extractor Estimating: 38it [00:22,  1.69it/s]Extractor Estimating: 39it [00:23,  1.65it/s]Extractor Estimating: 40it [00:24,  1.64it/s]Extractor Estimating: 41it [00:24,  1.71it/s]Extractor Estimating: 42it [00:25,  1.72it/s]Extractor Estimating: 43it [00:25,  1.72it/s]Extractor Estimating: 44it [00:26,  1.74it/s]Extractor Estimating: 45it [00:26,  1.75it/s]Extractor Estimating: 46it [00:27,  1.71it/s]Extractor Estimating: 47it [00:28,  1.67it/s]Extractor Estimating: 48it [00:28,  1.73it/s]Extractor Estimating: 49it [00:29,  1.73it/s]Extractor Estimating: 50it [00:29,  1.73it/s]Extractor Estimating: 51it [00:30,  1.74it/s]Extractor Estimating: 52it [00:31,  1.70it/s]Extractor Estimating: 53it [00:31,  1.67it/s]Extractor Estimating: 54it [00:32,  1.70it/s]Extractor Estimating: 55it [00:32,  1.72it/s]Extractor Estimating: 56it [00:33,  1.62it/s]Extractor Estimating: 57it [00:34,  1.58it/s]Extractor Estimating: 58it [00:34,  1.59it/s]Extractor Estimating: 59it [00:35,  1.59it/s]Extractor Estimating: 60it [00:36,  1.59it/s]Extractor Estimating: 61it [00:36,  1.62it/s]Extractor Estimating: 62it [00:37,  1.65it/s]Extractor Estimating: 63it [00:37,  1.67it/s]Extractor Estimating: 64it [00:38,  1.70it/s]Extractor Estimating: 65it [00:39,  1.67it/s]Extractor Estimating: 66it [00:39,  1.64it/s]Extractor Estimating: 67it [00:40,  1.63it/s]Extractor Estimating: 68it [00:40,  1.62it/s]Extractor Estimating: 69it [00:41,  1.60it/s]Extractor Estimating: 70it [00:42,  1.59it/s]Extractor Estimating: 71it [00:42,  1.53it/s]Extractor Estimating: 72it [00:43,  1.58it/s]Extractor Estimating: 73it [00:44,  1.59it/s]Extractor Estimating: 74it [00:44,  1.59it/s]Extractor Estimating: 75it [00:45,  1.59it/s]Extractor Estimating: 76it [00:45,  1.61it/s]Extractor Estimating: 77it [00:46,  1.63it/s]Extractor Estimating: 78it [00:47,  1.60it/s]Extractor Estimating: 79it [00:47,  1.63it/s]Extractor Estimating: 80it [00:48,  1.58it/s]Extractor Estimating: 81it [00:49,  1.60it/s]Extractor Estimating: 82it [00:49,  1.65it/s]Extractor Estimating: 83it [00:50,  1.67it/s]Extractor Estimating: 84it [00:50,  1.61it/s]Extractor Estimating: 85it [00:51,  1.60it/s]Extractor Estimating: 86it [00:52,  1.60it/s]Extractor Estimating: 87it [00:53,  1.47it/s]Extractor Estimating: 88it [00:53,  1.50it/s]Extractor Estimating: 89it [00:54,  1.52it/s]Extractor Estimating: 90it [00:54,  1.52it/s]Extractor Estimating: 91it [00:55,  1.58it/s]Extractor Estimating: 92it [00:56,  1.56it/s]Extractor Estimating: 93it [00:56,  1.61it/s]Extractor Estimating: 94it [00:57,  1.55it/s]Extractor Estimating: 95it [00:58,  1.57it/s]Extractor Estimating: 96it [00:58,  1.58it/s]Extractor Estimating: 97it [00:59,  1.60it/s]Extractor Estimating: 98it [00:59,  1.59it/s]Extractor Estimating: 99it [01:00,  1.59it/s]Extractor Estimating: 100it [01:01,  1.53it/s]Extractor Estimating: 101it [01:01,  1.56it/s]Extractor Estimating: 102it [01:02,  1.57it/s]Extractor Estimating: 103it [01:03,  1.58it/s]Extractor Estimating: 104it [01:03,  1.53it/s]Extractor Estimating: 105it [01:04,  1.53it/s]Extractor Estimating: 106it [01:05,  1.56it/s]Extractor Estimating: 107it [01:05,  1.57it/s]Extractor Estimating: 108it [01:06,  1.61it/s]Extractor Estimating: 109it [01:06,  1.60it/s]Extractor Estimating: 110it [01:07,  1.61it/s]Extractor Estimating: 111it [01:08,  1.62it/s]Extractor Estimating: 112it [01:08,  1.61it/s]Extractor Estimating: 113it [01:09,  1.61it/s]Extractor Estimating: 114it [01:10,  1.59it/s]Extractor Estimating: 115it [01:10,  1.61it/s]Extractor Estimating: 116it [01:11,  1.63it/s]Extractor Estimating: 117it [01:11,  1.60it/s]Extractor Estimating: 118it [01:12,  1.62it/s]Extractor Estimating: 119it [01:13,  1.62it/s]Extractor Estimating: 120it [01:13,  1.63it/s]Extractor Estimating: 121it [01:14,  1.65it/s]Extractor Estimating: 122it [01:14,  1.62it/s]Extractor Estimating: 123it [01:15,  1.61it/s]Extractor Estimating: 124it [01:16,  1.63it/s]Extractor Estimating: 125it [01:16,  1.63it/s]Extractor Estimating: 126it [01:17,  1.60it/s]Extractor Estimating: 127it [01:18,  1.60it/s]Extractor Estimating: 128it [01:18,  1.57it/s]Extractor Estimating: 129it [01:19,  1.60it/s]Extractor Estimating: 130it [01:19,  1.57it/s]Extractor Estimating: 131it [01:20,  1.57it/s]Extractor Estimating: 132it [01:21,  1.62it/s]Extractor Estimating: 133it [01:21,  1.63it/s]Extractor Estimating: 134it [01:22,  1.64it/s]Extractor Estimating: 135it [01:22,  1.68it/s]Extractor Estimating: 136it [01:23,  1.63it/s]Extractor Estimating: 137it [01:24,  1.66it/s]Extractor Estimating: 138it [01:24,  1.61it/s]Extractor Estimating: 139it [01:25,  1.59it/s]Extractor Estimating: 140it [01:26,  1.60it/s]Extractor Estimating: 141it [01:26,  1.61it/s]Extractor Estimating: 142it [01:27,  1.66it/s]Extractor Estimating: 143it [01:27,  1.62it/s]Extractor Estimating: 144it [01:28,  1.59it/s]Extractor Estimating: 145it [01:29,  1.62it/s]Extractor Estimating: 146it [01:29,  1.62it/s]Extractor Estimating: 147it [01:30,  1.56it/s]Extractor Estimating: 148it [01:31,  1.57it/s]Extractor Estimating: 149it [01:31,  1.54it/s]Extractor Estimating: 150it [01:32,  1.59it/s]Extractor Estimating: 151it [01:33,  1.62it/s]Extractor Estimating: 152it [01:33,  1.59it/s]Extractor Estimating: 153it [01:34,  1.68it/s]Extractor Estimating: 154it [01:34,  1.65it/s]Extractor Estimating: 155it [01:35,  1.67it/s]Extractor Estimating: 156it [01:35,  1.68it/s]Extractor Estimating: 157it [01:36,  1.72it/s]Extractor Estimating: 158it [01:37,  1.61it/s]Extractor Estimating: 159it [01:37,  1.63it/s]Extractor Estimating: 160it [01:38,  1.69it/s]Extractor Estimating: 161it [01:38,  1.71it/s]Extractor Estimating: 162it [01:39,  1.76it/s]Extractor Estimating: 163it [01:40,  1.78it/s]Extractor Estimating: 164it [01:40,  1.79it/s]Extractor Estimating: 165it [01:41,  1.78it/s]Extractor Estimating: 166it [01:41,  1.80it/s]Extractor Estimating: 167it [01:42,  1.79it/s]Extractor Estimating: 168it [01:42,  1.81it/s]Extractor Estimating: 169it [01:43,  1.82it/s]Extractor Estimating: 170it [01:43,  1.77it/s]Extractor Estimating: 171it [01:44,  1.81it/s]Extractor Estimating: 172it [01:45,  1.81it/s]Extractor Estimating: 173it [01:45,  1.82it/s]Extractor Estimating: 174it [01:46,  1.82it/s]Extractor Estimating: 175it [01:46,  1.74it/s]Extractor Estimating: 176it [01:47,  1.71it/s]Extractor Estimating: 177it [01:47,  1.68it/s]Extractor Estimating: 178it [01:48,  1.71it/s]Extractor Estimating: 179it [01:49,  1.47it/s]Extractor Estimating: 180it [01:50,  1.50it/s]Extractor Estimating: 181it [01:50,  1.48it/s]Extractor Estimating: 182it [01:51,  1.56it/s]Extractor Estimating: 183it [01:51,  1.58it/s]Extractor Estimating: 184it [01:52,  1.58it/s]Extractor Estimating: 185it [01:53,  1.55it/s]Extractor Estimating: 186it [01:53,  1.56it/s]Extractor Estimating: 187it [01:54,  1.57it/s]Extractor Estimating: 188it [01:55,  1.62it/s]Extractor Estimating: 189it [01:55,  1.65it/s]Extractor Estimating: 190it [01:56,  1.62it/s]Extractor Estimating: 191it [01:56,  1.64it/s]Extractor Estimating: 192it [01:57,  1.61it/s]Extractor Estimating: 193it [01:58,  1.64it/s]Extractor Estimating: 194it [01:58,  1.62it/s]Extractor Estimating: 195it [01:59,  1.59it/s]Extractor Estimating: 196it [02:00,  1.59it/s]Extractor Estimating: 197it [02:00,  1.60it/s]Extractor Estimating: 198it [02:01,  1.59it/s]Extractor Estimating: 199it [02:01,  1.57it/s]Extractor Estimating: 200it [02:02,  1.62it/s]Extractor Estimating: 201it [02:03,  1.60it/s]Extractor Estimating: 202it [02:03,  1.61it/s]Extractor Estimating: 203it [02:04,  1.59it/s]Extractor Estimating: 204it [02:04,  1.63it/s]Extractor Estimating: 205it [02:05,  1.57it/s]Extractor Estimating: 206it [02:06,  1.54it/s]Extractor Estimating: 207it [02:07,  1.54it/s]Extractor Estimating: 208it [02:07,  1.57it/s]Extractor Estimating: 209it [02:08,  1.53it/s]Extractor Estimating: 210it [02:08,  1.56it/s]Extractor Estimating: 211it [02:09,  1.58it/s]Extractor Estimating: 212it [02:10,  1.57it/s]Extractor Estimating: 213it [02:10,  1.56it/s]Extractor Estimating: 214it [02:11,  1.58it/s]Extractor Estimating: 215it [02:12,  1.53it/s]Extractor Estimating: 216it [02:12,  1.55it/s]Extractor Estimating: 217it [02:13,  1.56it/s]Extractor Estimating: 218it [02:13,  1.59it/s]Extractor Estimating: 219it [02:14,  1.62it/s]Extractor Estimating: 220it [02:15,  1.61it/s]Extractor Estimating: 221it [02:15,  1.65it/s]Extractor Estimating: 222it [02:16,  1.61it/s]Extractor Estimating: 223it [02:17,  1.60it/s]Extractor Estimating: 224it [02:17,  1.58it/s]Extractor Estimating: 225it [02:18,  1.61it/s]Extractor Estimating: 226it [02:18,  1.61it/s]Extractor Estimating: 227it [02:19,  1.61it/s]Extractor Estimating: 228it [02:20,  1.59it/s]Extractor Estimating: 229it [02:20,  1.62it/s]Extractor Estimating: 230it [02:21,  1.60it/s]Extractor Estimating: 231it [02:22,  1.64it/s]Extractor Estimating: 232it [02:22,  1.58it/s]Extractor Estimating: 233it [02:23,  1.56it/s]Extractor Estimating: 234it [02:23,  1.57it/s]Extractor Estimating: 235it [02:24,  1.61it/s]Extractor Estimating: 236it [02:25,  1.65it/s]Extractor Estimating: 237it [02:25,  1.63it/s]Extractor Estimating: 238it [02:26,  1.57it/s]Extractor Estimating: 239it [02:27,  1.61it/s]Extractor Estimating: 240it [02:27,  1.63it/s]Extractor Estimating: 241it [02:28,  1.64it/s]Extractor Estimating: 242it [02:28,  1.62it/s]Extractor Estimating: 243it [02:29,  1.59it/s]Extractor Estimating: 244it [02:30,  1.60it/s]Extractor Estimating: 245it [02:30,  1.61it/s]Extractor Estimating: 246it [02:31,  1.63it/s]Extractor Estimating: 247it [02:32,  1.54it/s]Extractor Estimating: 248it [02:32,  1.54it/s]Extractor Estimating: 249it [02:33,  1.57it/s]Extractor Estimating: 250it [02:33,  1.61it/s]Extractor Estimating: 251it [02:34,  1.64it/s]Extractor Estimating: 252it [02:35,  1.60it/s]Extractor Estimating: 253it [02:35,  1.52it/s]Extractor Estimating: 254it [02:36,  1.51it/s]Extractor Estimating: 255it [02:37,  1.55it/s]Extractor Estimating: 256it [02:37,  1.58it/s]Extractor Estimating: 257it [02:38,  1.44it/s]Extractor Estimating: 258it [02:39,  1.49it/s]Extractor Estimating: 259it [02:39,  1.50it/s]Extractor Estimating: 260it [02:40,  1.50it/s]Extractor Estimating: 261it [02:41,  1.49it/s]Extractor Estimating: 262it [02:41,  1.52it/s]Extractor Estimating: 263it [02:42,  1.55it/s]Extractor Estimating: 264it [02:43,  1.57it/s]Extractor Estimating: 265it [02:43,  1.57it/s]Extractor Estimating: 266it [02:44,  1.51it/s]Extractor Estimating: 267it [02:45,  1.54it/s]Extractor Estimating: 268it [02:45,  1.52it/s]Extractor Estimating: 269it [02:46,  1.54it/s]Extractor Estimating: 270it [02:47,  1.57it/s]Extractor Estimating: 271it [02:47,  1.52it/s]Extractor Estimating: 272it [02:48,  1.54it/s]Extractor Estimating: 273it [02:49,  1.52it/s]Extractor Estimating: 274it [02:49,  1.55it/s]Extractor Estimating: 275it [02:50,  1.52it/s]Extractor Estimating: 276it [02:50,  1.54it/s]Extractor Estimating: 277it [02:51,  1.55it/s]Extractor Estimating: 278it [02:52,  1.52it/s]Extractor Estimating: 279it [02:52,  1.55it/s]Extractor Estimating: 280it [02:53,  1.54it/s]Extractor Estimating: 281it [02:54,  1.50it/s]Extractor Estimating: 282it [02:54,  1.56it/s]Extractor Estimating: 283it [02:55,  1.57it/s]Extractor Estimating: 284it [02:56,  1.58it/s]Extractor Estimating: 285it [02:56,  1.58it/s]Extractor Estimating: 286it [02:57,  1.52it/s]Extractor Estimating: 287it [02:58,  1.56it/s]Extractor Estimating: 288it [02:58,  1.54it/s]Extractor Estimating: 289it [02:59,  1.58it/s]Extractor Estimating: 290it [02:59,  1.60it/s]Extractor Estimating: 291it [03:00,  1.56it/s]Extractor Estimating: 292it [03:01,  1.58it/s]Extractor Estimating: 293it [03:01,  1.54it/s]Extractor Estimating: 294it [03:02,  1.53it/s]Extractor Estimating: 295it [03:03,  1.51it/s]Extractor Estimating: 296it [03:03,  1.51it/s]Extractor Estimating: 297it [03:04,  1.55it/s]Extractor Estimating: 298it [03:05,  1.52it/s]Extractor Estimating: 299it [03:05,  1.53it/s]Extractor Estimating: 300it [03:06,  1.57it/s]Extractor Estimating: 301it [03:07,  1.52it/s]Extractor Estimating: 302it [03:07,  1.55it/s]Extractor Estimating: 303it [03:08,  1.58it/s]Extractor Estimating: 304it [03:08,  1.59it/s]Extractor Estimating: 305it [03:09,  1.59it/s]Extractor Estimating: 306it [03:10,  1.62it/s]Extractor Estimating: 307it [03:10,  1.67it/s]Extractor Estimating: 308it [03:11,  1.65it/s]Extractor Estimating: 309it [03:12,  1.63it/s]Extractor Estimating: 310it [03:12,  1.62it/s]Extractor Estimating: 311it [03:13,  1.58it/s]Extractor Estimating: 312it [03:13,  1.59it/s]Extractor Estimating: 313it [03:14,  1.62it/s]Extractor Estimating: 314it [03:15,  1.51it/s]Extractor Estimating: 315it [03:15,  1.51it/s]Extractor Estimating: 316it [03:16,  1.51it/s]Extractor Estimating: 317it [03:17,  1.56it/s]Extractor Estimating: 318it [03:17,  1.61it/s]Extractor Estimating: 319it [03:18,  1.53it/s]Extractor Estimating: 320it [03:19,  1.59it/s]Extractor Estimating: 321it [03:19,  1.62it/s]Extractor Estimating: 322it [03:20,  1.66it/s]Extractor Estimating: 323it [03:20,  1.64it/s]Extractor Estimating: 324it [03:21,  1.58it/s]Extractor Estimating: 325it [03:22,  1.60it/s]Extractor Estimating: 326it [03:22,  1.64it/s]Extractor Estimating: 327it [03:23,  1.65it/s]Extractor Estimating: 328it [03:23,  1.66it/s]Extractor Estimating: 329it [03:24,  1.61it/s]Extractor Estimating: 330it [03:25,  1.63it/s]Extractor Estimating: 331it [03:25,  1.61it/s]Extractor Estimating: 332it [03:26,  1.62it/s]Extractor Estimating: 333it [03:27,  1.58it/s]Extractor Estimating: 334it [03:27,  1.58it/s]Extractor Estimating: 335it [03:28,  1.62it/s]Extractor Estimating: 336it [03:28,  1.60it/s]Extractor Estimating: 337it [03:29,  1.62it/s]Extractor Estimating: 338it [03:30,  1.46it/s]Extractor Estimating: 339it [03:30,  1.52it/s]Extractor Estimating: 340it [03:31,  1.57it/s]Extractor Estimating: 341it [03:32,  1.59it/s]Extractor Estimating: 342it [03:32,  1.64it/s]Extractor Estimating: 343it [03:33,  1.58it/s]Extractor Estimating: 344it [03:34,  1.57it/s]Extractor Estimating: 345it [03:34,  1.59it/s]Extractor Estimating: 346it [03:35,  1.64it/s]Extractor Estimating: 347it [03:35,  1.64it/s]Extractor Estimating: 348it [03:36,  1.66it/s]Extractor Estimating: 349it [03:37,  1.62it/s]Extractor Estimating: 350it [03:37,  1.65it/s]Extractor Estimating: 351it [03:38,  1.64it/s]Extractor Estimating: 352it [03:38,  1.64it/s]Extractor Estimating: 353it [03:39,  1.58it/s]Extractor Estimating: 354it [03:40,  1.58it/s]Extractor Estimating: 355it [03:40,  1.61it/s]Extractor Estimating: 356it [03:41,  1.57it/s]Extractor Estimating: 357it [03:42,  1.60it/s]Extractor Estimating: 358it [03:42,  1.60it/s]Extractor Estimating: 359it [03:43,  1.56it/s]Extractor Estimating: 360it [03:43,  1.61it/s]Extractor Estimating: 361it [03:44,  1.62it/s]Extractor Estimating: 362it [03:45,  1.65it/s]Extractor Estimating: 363it [03:45,  1.57it/s]Extractor Estimating: 364it [03:46,  1.60it/s]Extractor Estimating: 365it [03:47,  1.62it/s]Extractor Estimating: 366it [03:47,  1.59it/s]Extractor Estimating: 367it [03:48,  1.61it/s]Extractor Estimating: 368it [03:48,  1.66it/s]Extractor Estimating: 369it [03:49,  1.60it/s]Extractor Estimating: 370it [03:50,  1.56it/s]Extractor Estimating: 371it [03:50,  1.57it/s]Extractor Estimating: 372it [03:51,  1.55it/s]Extractor Estimating: 373it [03:52,  1.53it/s]Extractor Estimating: 374it [03:52,  1.57it/s]Extractor Estimating: 375it [03:53,  1.62it/s]Extractor Estimating: 376it [03:54,  1.56it/s]Extractor Estimating: 377it [03:54,  1.56it/s]Extractor Estimating: 378it [03:55,  1.54it/s]Extractor Estimating: 379it [03:55,  1.57it/s]Extractor Estimating: 380it [03:56,  1.55it/s]Extractor Estimating: 381it [03:57,  1.55it/s]Extractor Estimating: 382it [03:57,  1.56it/s]Extractor Estimating: 383it [03:58,  1.51it/s]Extractor Estimating: 384it [03:59,  1.47it/s]Extractor Estimating: 385it [04:00,  1.50it/s]Extractor Estimating: 386it [04:00,  1.54it/s]Extractor Estimating: 387it [04:01,  1.54it/s]Extractor Estimating: 388it [04:01,  1.54it/s]Extractor Estimating: 389it [04:02,  1.53it/s]Extractor Estimating: 390it [04:03,  1.50it/s]Extractor Estimating: 391it [04:03,  1.55it/s]Extractor Estimating: 392it [04:04,  1.56it/s]Extractor Estimating: 393it [04:05,  1.48it/s]Extractor Estimating: 394it [04:05,  1.47it/s]Extractor Estimating: 395it [04:06,  1.50it/s]Extractor Estimating: 396it [04:07,  1.54it/s]Extractor Estimating: 397it [04:07,  1.53it/s]Extractor Estimating: 398it [04:08,  1.50it/s]Extractor Estimating: 399it [04:09,  1.49it/s]Extractor Estimating: 400it [04:09,  1.48it/s]Extractor Estimating: 401it [04:10,  1.49it/s]Extractor Estimating: 402it [04:11,  1.55it/s]Extractor Estimating: 403it [04:11,  1.57it/s]Extractor Estimating: 404it [04:12,  1.58it/s]Extractor Estimating: 405it [04:13,  1.61it/s]Extractor Estimating: 406it [04:13,  1.59it/s]Extractor Estimating: 407it [04:14,  1.58it/s]Extractor Estimating: 408it [04:14,  1.57it/s]Extractor Estimating: 409it [04:15,  1.56it/s]Extractor Estimating: 410it [04:16,  1.59it/s]Extractor Estimating: 411it [04:16,  1.59it/s]Extractor Estimating: 412it [04:17,  1.62it/s]Extractor Estimating: 413it [04:18,  1.60it/s]Extractor Estimating: 414it [04:18,  1.57it/s]Extractor Estimating: 415it [04:19,  1.55it/s]Extractor Estimating: 416it [04:20,  1.56it/s]Extractor Estimating: 417it [04:20,  1.42it/s]Extractor Estimating: 418it [04:21,  1.49it/s]Extractor Estimating: 419it [04:22,  1.53it/s]Extractor Estimating: 420it [04:22,  1.50it/s]Extractor Estimating: 421it [04:23,  1.52it/s]Extractor Estimating: 422it [04:24,  1.52it/s]Extractor Estimating: 423it [04:24,  1.58it/s]Extractor Estimating: 424it [04:25,  1.62it/s]Extractor Estimating: 425it [04:25,  1.63it/s]Extractor Estimating: 426it [04:26,  1.55it/s]Extractor Estimating: 427it [04:27,  1.60it/s]Extractor Estimating: 428it [04:27,  1.60it/s]Extractor Estimating: 429it [04:28,  1.62it/s]Extractor Estimating: 430it [04:28,  1.61it/s]Extractor Estimating: 431it [04:29,  1.60it/s]Extractor Estimating: 432it [04:30,  1.62it/s]Extractor Estimating: 433it [04:30,  1.64it/s]Extractor Estimating: 434it [04:31,  1.60it/s]Extractor Estimating: 435it [04:32,  1.59it/s]Extractor Estimating: 436it [04:32,  1.56it/s]Extractor Estimating: 437it [04:33,  1.58it/s]Extractor Estimating: 438it [04:34,  1.56it/s]Extractor Estimating: 439it [04:34,  1.52it/s]Extractor Estimating: 440it [04:35,  1.58it/s]Extractor Estimating: 441it [04:36,  1.55it/s]Extractor Estimating: 442it [04:36,  1.60it/s]Extractor Estimating: 443it [04:37,  1.62it/s]Extractor Estimating: 444it [04:37,  1.63it/s]Extractor Estimating: 445it [04:38,  1.62it/s]Extractor Estimating: 446it [04:39,  1.55it/s]Extractor Estimating: 447it [04:39,  1.58it/s]Extractor Estimating: 448it [04:40,  1.57it/s]Extractor Estimating: 449it [04:40,  1.59it/s]Extractor Estimating: 450it [04:41,  1.62it/s]Extractor Estimating: 451it [04:42,  1.59it/s]Extractor Estimating: 452it [04:42,  1.58it/s]Extractor Estimating: 453it [04:43,  1.60it/s]Extractor Estimating: 454it [04:44,  1.52it/s]Extractor Estimating: 455it [04:44,  1.55it/s]Extractor Estimating: 456it [04:45,  1.54it/s]Extractor Estimating: 457it [04:46,  1.54it/s]Extractor Estimating: 458it [04:46,  1.54it/s]Extractor Estimating: 459it [04:47,  1.49it/s]Extractor Estimating: 460it [04:48,  1.51it/s]Extractor Estimating: 461it [04:48,  1.56it/s]Extractor Estimating: 462it [04:49,  1.52it/s]Extractor Estimating: 463it [04:50,  1.49it/s]Extractor Estimating: 464it [04:50,  1.49it/s]Extractor Estimating: 465it [04:51,  1.50it/s]Extractor Estimating: 466it [04:52,  1.52it/s]Extractor Estimating: 467it [04:52,  1.58it/s]Extractor Estimating: 468it [04:53,  1.65it/s]Extractor Estimating: 469it [04:53,  1.60it/s]Extractor Estimating: 470it [04:54,  1.64it/s]Extractor Estimating: 471it [04:55,  1.55it/s]Extractor Estimating: 472it [04:55,  1.56it/s]Extractor Estimating: 473it [04:56,  1.50it/s]Extractor Estimating: 474it [04:57,  1.49it/s]Extractor Estimating: 475it [04:57,  1.51it/s]Extractor Estimating: 476it [04:58,  1.53it/s]Extractor Estimating: 477it [04:59,  1.52it/s]Extractor Estimating: 478it [04:59,  1.53it/s]Extractor Estimating: 479it [05:00,  1.56it/s]Extractor Estimating: 480it [05:01,  1.58it/s]Extractor Estimating: 481it [05:01,  1.58it/s]Extractor Estimating: 482it [05:02,  1.56it/s]Extractor Estimating: 483it [05:02,  1.57it/s]Extractor Estimating: 484it [05:03,  1.55it/s]Extractor Estimating: 485it [05:04,  1.56it/s]Extractor Estimating: 486it [05:04,  1.55it/s]Extractor Estimating: 487it [05:05,  1.57it/s]Extractor Estimating: 488it [05:06,  1.60it/s]Extractor Estimating: 489it [05:06,  1.54it/s]Extractor Estimating: 490it [05:07,  1.49it/s]Extractor Estimating: 491it [05:08,  1.54it/s]Extractor Estimating: 492it [05:08,  1.55it/s]Extractor Estimating: 493it [05:09,  1.60it/s]Extractor Estimating: 494it [05:10,  1.57it/s]Extractor Estimating: 495it [05:10,  1.55it/s]Extractor Estimating: 496it [05:11,  1.54it/s]Extractor Estimating: 497it [05:11,  1.57it/s]Extractor Estimating: 498it [05:12,  1.39it/s]Extractor Estimating: 499it [05:13,  1.40it/s]Extractor Estimating: 500it [05:13,  1.72it/s]Extractor Estimating: 500it [05:13,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:34,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:34,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:34,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:34,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:34,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:32:35,114 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:32:35,115 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:32:35,715 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:32:36,813 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:32:36,813 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:40,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:40,085 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:40,085 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:40,085 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:32:40,085 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:32:40,831 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:32:40,832 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:32:41,437 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:32:41,625 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:32:41,625 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 13:37:05,979 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 13:37:06,413 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 10452 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 27224
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27324, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27324, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.013, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.011, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.026, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.004, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.048, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 2.326, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 264, avg_time 1.007, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 364, avg_time 1.013, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.021, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.007, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 228, avg_time 2.307, loss:nan
g_step 1200, step 328, avg_time 1.010, loss:nan
g_step 1300, step 428, avg_time 1.023, loss:nan
g_step 1400, step 92, avg_time 1.014, loss:nan
g_step 1500, step 192, avg_time 1.021, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 292, avg_time 2.313, loss:nan
g_step 1700, step 392, avg_time 1.024, loss:nan
g_step 1800, step 56, avg_time 1.013, loss:nan
g_step 1900, step 156, avg_time 1.007, loss:nan
g_step 2000, step 256, avg_time 1.016, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 356, avg_time 2.299, loss:nan
g_step 2200, step 20, avg_time 1.001, loss:nan
g_step 2300, step 120, avg_time 1.023, loss:nan
g_step 2400, step 220, avg_time 1.004, loss:nan
g_step 2500, step 320, avg_time 1.023, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 420, avg_time 2.299, loss:nan
g_step 2700, step 84, avg_time 0.998, loss:nan
g_step 2800, step 184, avg_time 1.012, loss:nan
g_step 2900, step 284, avg_time 1.017, loss:nan
g_step 3000, step 384, avg_time 1.026, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.318, loss:nan
g_step 3200, step 148, avg_time 1.005, loss:nan
g_step 3300, step 248, avg_time 1.018, loss:nan
g_step 3400, step 348, avg_time 1.019, loss:nan
g_step 3500, step 12, avg_time 1.012, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 2.325, loss:nan
g_step 3700, step 212, avg_time 1.024, loss:nan
g_step 3800, step 312, avg_time 1.014, loss:nan
g_step 3900, step 412, avg_time 1.013, loss:nan
g_step 4000, step 76, avg_time 1.014, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 2.307, loss:nan
g_step 4200, step 276, avg_time 1.003, loss:nan
g_step 4300, step 376, avg_time 1.019, loss:nan
g_step 4400, step 40, avg_time 1.024, loss:nan
g_step 4500, step 140, avg_time 1.012, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 240, avg_time 2.290, loss:nan
g_step 4700, step 340, avg_time 1.017, loss:nan
g_step 4800, step 4, avg_time 1.029, loss:nan
g_step 4900, step 104, avg_time 1.006, loss:nan
g_step 5000, step 204, avg_time 1.019, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 304, avg_time 2.320, loss:nan
g_step 5200, step 404, avg_time 1.014, loss:nan
g_step 5300, step 68, avg_time 1.017, loss:nan
g_step 5400, step 168, avg_time 1.014, loss:nan
g_step 5500, step 268, avg_time 1.010, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 368, avg_time 2.302, loss:nan
g_step 5700, step 32, avg_time 1.019, loss:nan
g_step 5800, step 132, avg_time 1.014, loss:nan
g_step 5900, step 232, avg_time 0.998, loss:nan
g_step 6000, step 332, avg_time 1.025, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 432, avg_time 2.304, loss:nan
g_step 6200, step 96, avg_time 1.015, loss:nan
g_step 6300, step 196, avg_time 1.023, loss:nan
g_step 6400, step 296, avg_time 0.999, loss:nan
g_step 6500, step 396, avg_time 1.010, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 60, avg_time 2.313, loss:nan
g_step 6700, step 160, avg_time 1.031, loss:nan
g_step 6800, step 260, avg_time 1.011, loss:nan
g_step 6900, step 360, avg_time 1.007, loss:nan
g_step 7000, step 24, avg_time 0.997, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 124, avg_time 2.313, loss:nan
g_step 7200, step 224, avg_time 1.012, loss:nan
g_step 7300, step 324, avg_time 1.019, loss:nan
g_step 7400, step 424, avg_time 1.032, loss:nan
g_step 7500, step 88, avg_time 0.996, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 188, avg_time 2.324, loss:nan
g_step 7700, step 288, avg_time 1.020, loss:nan
g_step 7800, step 388, avg_time 1.024, loss:nan
g_step 7900, step 52, avg_time 1.012, loss:nan
g_step 8000, step 152, avg_time 1.014, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 252, avg_time 2.320, loss:nan
g_step 8200, step 352, avg_time 1.026, loss:nan
g_step 8300, step 16, avg_time 1.027, loss:nan
g_step 8400, step 116, avg_time 1.029, loss:nan
g_step 8500, step 216, avg_time 1.029, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 316, avg_time 2.296, loss:nan
g_step 8700, step 416, avg_time 1.024, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:37:06 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:37:06 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-37-05_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:37:07 - WARNING - datasets.builder -   Using custom data configuration default-cc83f68ad913dae9
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-cc83f68ad913dae9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:37:09,462 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:37:09,480 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:37:09,480 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:37:09,481 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:37:09,550 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:37:09,576 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:37:09,576 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:37:09,576 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:37:09,576 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:37:09,576 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:37:09,576 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:37:09,909 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:37:13,016 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:37:13,017 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-cc83f68ad913dae9/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.79ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.68ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.10ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.31ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.42ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.49ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.55ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.56ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.60ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.60ba/s]100%|██████████| 11/11 [00:02<00:00,  5.45ba/s]100%|██████████| 11/11 [00:02<00:00,  4.59ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.40ba/s] 40%|████      | 2/5 [00:00<00:00,  4.00ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.26ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.39ba/s]100%|██████████| 5/5 [00:01<00:00,  4.67ba/s]100%|██████████| 5/5 [00:01<00:00,  4.40ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  3.28ba/s] 27%|██▋       | 3/11 [00:00<00:01,  6.96ba/s] 45%|████▌     | 5/11 [00:00<00:00,  8.74ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.69ba/s] 82%|████████▏ | 9/11 [00:01<00:00, 10.31ba/s]100%|██████████| 11/11 [00:01<00:00, 11.63ba/s]100%|██████████| 11/11 [00:01<00:00,  9.70ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.52ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.53ba/s]100%|██████████| 5/5 [00:00<00:00, 10.79ba/s]100%|██████████| 5/5 [00:00<00:00, 10.17ba/s]
[INFO|trainer.py:414] 2023-08-28 13:37:19,489 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:37:19,616 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:37:19,616 >>   Num examples = 10479
[INFO|trainer.py:1149] 2023-08-28 13:37:19,616 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:37:19,616 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:37:19,617 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:37:19,617 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:37:19,617 >>   Total optimization steps = 820
  0%|          | 0/820 [00:00<?, ?it/s]  0%|          | 1/820 [00:00<03:57,  3.46it/s]  0%|          | 2/820 [00:00<03:48,  3.58it/s]  0%|          | 3/820 [00:00<03:45,  3.62it/s]  0%|          | 4/820 [00:01<03:45,  3.62it/s]  1%|          | 5/820 [00:01<03:45,  3.61it/s]  1%|          | 6/820 [00:01<03:45,  3.61it/s]  1%|          | 7/820 [00:01<03:45,  3.61it/s]  1%|          | 8/820 [00:02<03:44,  3.61it/s]  1%|          | 9/820 [00:02<03:44,  3.61it/s]  1%|          | 10/820 [00:02<03:44,  3.61it/s]  1%|▏         | 11/820 [00:03<03:44,  3.61it/s]  1%|▏         | 12/820 [00:03<03:44,  3.61it/s]  2%|▏         | 13/820 [00:03<03:43,  3.61it/s]  2%|▏         | 14/820 [00:03<03:43,  3.60it/s]  2%|▏         | 15/820 [00:04<03:43,  3.61it/s]  2%|▏         | 16/820 [00:04<03:42,  3.61it/s]  2%|▏         | 17/820 [00:04<03:42,  3.61it/s]  2%|▏         | 18/820 [00:04<03:42,  3.61it/s]  2%|▏         | 19/820 [00:05<03:42,  3.61it/s]  2%|▏         | 20/820 [00:05<03:41,  3.61it/s]  3%|▎         | 21/820 [00:05<03:41,  3.60it/s]  3%|▎         | 22/820 [00:06<03:41,  3.61it/s]  3%|▎         | 23/820 [00:06<03:40,  3.61it/s]  3%|▎         | 24/820 [00:06<03:40,  3.61it/s]  3%|▎         | 25/820 [00:06<03:40,  3.61it/s]  3%|▎         | 26/820 [00:07<03:40,  3.61it/s]  3%|▎         | 27/820 [00:07<03:40,  3.60it/s]  3%|▎         | 28/820 [00:07<03:40,  3.59it/s]  4%|▎         | 29/820 [00:08<03:39,  3.60it/s]  4%|▎         | 30/820 [00:08<03:39,  3.59it/s]  4%|▍         | 31/820 [00:08<03:45,  3.50it/s]  4%|▍         | 32/820 [00:08<03:43,  3.53it/s]  4%|▍         | 33/820 [00:09<03:41,  3.55it/s]  4%|▍         | 34/820 [00:09<03:40,  3.56it/s]  4%|▍         | 35/820 [00:09<03:39,  3.57it/s]  4%|▍         | 36/820 [00:10<03:38,  3.58it/s]  5%|▍         | 37/820 [00:10<03:38,  3.58it/s]  5%|▍         | 38/820 [00:10<03:38,  3.59it/s]  5%|▍         | 39/820 [00:10<03:37,  3.59it/s]  5%|▍         | 40/820 [00:11<03:36,  3.59it/s]  5%|▌         | 41/820 [00:11<03:36,  3.60it/s]  5%|▌         | 42/820 [00:11<03:36,  3.60it/s]  5%|▌         | 43/820 [00:11<03:36,  3.59it/s]  5%|▌         | 44/820 [00:12<03:36,  3.59it/s]  5%|▌         | 45/820 [00:12<03:35,  3.59it/s]  6%|▌         | 46/820 [00:12<03:35,  3.59it/s]  6%|▌         | 47/820 [00:13<03:34,  3.60it/s]  6%|▌         | 48/820 [00:13<03:34,  3.60it/s]  6%|▌         | 49/820 [00:13<03:34,  3.60it/s]  6%|▌         | 50/820 [00:13<03:33,  3.60it/s]  6%|▌         | 51/820 [00:14<03:33,  3.60it/s]  6%|▋         | 52/820 [00:14<03:33,  3.60it/s]  6%|▋         | 53/820 [00:14<03:33,  3.59it/s]  7%|▋         | 54/820 [00:15<03:32,  3.60it/s]  7%|▋         | 55/820 [00:15<03:32,  3.60it/s]  7%|▋         | 56/820 [00:15<03:32,  3.60it/s]  7%|▋         | 57/820 [00:15<03:32,  3.60it/s]  7%|▋         | 58/820 [00:16<03:31,  3.60it/s]  7%|▋         | 59/820 [00:16<03:31,  3.60it/s]  7%|▋         | 60/820 [00:16<03:30,  3.62it/s]  7%|▋         | 61/820 [00:16<03:29,  3.63it/s]  8%|▊         | 62/820 [00:17<03:28,  3.64it/s]  8%|▊         | 63/820 [00:17<03:27,  3.64it/s]  8%|▊         | 64/820 [00:17<03:27,  3.65it/s]  8%|▊         | 65/820 [00:18<03:26,  3.65it/s]  8%|▊         | 66/820 [00:18<03:26,  3.65it/s]  8%|▊         | 67/820 [00:18<03:26,  3.65it/s]  8%|▊         | 68/820 [00:18<03:25,  3.66it/s]  8%|▊         | 69/820 [00:19<03:25,  3.65it/s]  9%|▊         | 70/820 [00:19<03:25,  3.65it/s]  9%|▊         | 71/820 [00:19<03:25,  3.65it/s]  9%|▉         | 72/820 [00:19<03:24,  3.65it/s]  9%|▉         | 73/820 [00:20<03:24,  3.65it/s]  9%|▉         | 74/820 [00:20<03:25,  3.64it/s]  9%|▉         | 75/820 [00:20<03:25,  3.62it/s]  9%|▉         | 76/820 [00:21<03:25,  3.61it/s]  9%|▉         | 77/820 [00:21<03:26,  3.61it/s] 10%|▉         | 78/820 [00:21<03:25,  3.61it/s] 10%|▉         | 79/820 [00:21<03:25,  3.60it/s] 10%|▉         | 80/820 [00:22<03:25,  3.60it/s] 10%|▉         | 81/820 [00:22<03:25,  3.60it/s] 10%|█         | 82/820 [00:22<03:24,  3.60it/s] 10%|█         | 83/820 [00:23<03:24,  3.60it/s] 10%|█         | 84/820 [00:23<03:25,  3.59it/s] 10%|█         | 85/820 [00:23<03:24,  3.59it/s] 10%|█         | 86/820 [00:23<03:24,  3.59it/s] 11%|█         | 87/820 [00:24<03:24,  3.59it/s] 11%|█         | 88/820 [00:24<03:24,  3.59it/s] 11%|█         | 89/820 [00:24<03:23,  3.59it/s] 11%|█         | 90/820 [00:24<03:23,  3.59it/s] 11%|█         | 91/820 [00:25<03:23,  3.59it/s] 11%|█         | 92/820 [00:25<03:22,  3.59it/s] 11%|█▏        | 93/820 [00:25<03:22,  3.60it/s] 11%|█▏        | 94/820 [00:26<03:21,  3.60it/s] 12%|█▏        | 95/820 [00:26<03:21,  3.59it/s] 12%|█▏        | 96/820 [00:26<03:21,  3.60it/s] 12%|█▏        | 97/820 [00:26<03:21,  3.60it/s] 12%|█▏        | 98/820 [00:27<03:21,  3.59it/s] 12%|█▏        | 99/820 [00:27<03:20,  3.59it/s] 12%|█▏        | 100/820 [00:27<03:20,  3.59it/s] 12%|█▏        | 101/820 [00:28<03:20,  3.59it/s] 12%|█▏        | 102/820 [00:28<03:20,  3.59it/s] 13%|█▎        | 103/820 [00:28<03:19,  3.59it/s] 13%|█▎        | 104/820 [00:28<03:19,  3.59it/s] 13%|█▎        | 105/820 [00:29<03:19,  3.59it/s] 13%|█▎        | 106/820 [00:29<03:18,  3.59it/s] 13%|█▎        | 107/820 [00:29<03:18,  3.60it/s] 13%|█▎        | 108/820 [00:29<03:18,  3.59it/s] 13%|█▎        | 109/820 [00:30<03:17,  3.59it/s] 13%|█▎        | 110/820 [00:30<03:17,  3.59it/s] 14%|█▎        | 111/820 [00:30<03:17,  3.59it/s] 14%|█▎        | 112/820 [00:31<03:17,  3.59it/s] 14%|█▍        | 113/820 [00:31<03:17,  3.59it/s] 14%|█▍        | 114/820 [00:31<03:16,  3.59it/s] 14%|█▍        | 115/820 [00:31<03:16,  3.59it/s] 14%|█▍        | 116/820 [00:32<03:16,  3.59it/s] 14%|█▍        | 117/820 [00:32<03:15,  3.59it/s] 14%|█▍        | 118/820 [00:32<03:15,  3.59it/s] 15%|█▍        | 119/820 [00:33<03:15,  3.59it/s] 15%|█▍        | 120/820 [00:33<03:14,  3.59it/s] 15%|█▍        | 121/820 [00:33<03:14,  3.59it/s] 15%|█▍        | 122/820 [00:33<03:14,  3.59it/s] 15%|█▌        | 123/820 [00:34<03:13,  3.60it/s] 15%|█▌        | 124/820 [00:34<03:13,  3.59it/s] 15%|█▌        | 125/820 [00:34<03:13,  3.60it/s] 15%|█▌        | 126/820 [00:34<03:13,  3.59it/s] 15%|█▌        | 127/820 [00:35<03:12,  3.60it/s] 16%|█▌        | 128/820 [00:35<03:11,  3.61it/s] 16%|█▌        | 129/820 [00:35<03:10,  3.62it/s] 16%|█▌        | 130/820 [00:36<03:09,  3.63it/s] 16%|█▌        | 131/820 [00:36<03:09,  3.64it/s] 16%|█▌        | 132/820 [00:36<03:08,  3.64it/s] 16%|█▌        | 133/820 [00:36<03:08,  3.64it/s] 16%|█▋        | 134/820 [00:37<03:08,  3.64it/s] 16%|█▋        | 135/820 [00:37<03:08,  3.64it/s] 17%|█▋        | 136/820 [00:37<03:07,  3.64it/s] 17%|█▋        | 137/820 [00:38<03:07,  3.65it/s] 17%|█▋        | 138/820 [00:38<03:07,  3.64it/s] 17%|█▋        | 139/820 [00:38<03:06,  3.65it/s] 17%|█▋        | 140/820 [00:38<03:06,  3.65it/s] 17%|█▋        | 141/820 [00:39<03:06,  3.64it/s] 17%|█▋        | 142/820 [00:39<03:05,  3.65it/s] 17%|█▋        | 143/820 [00:39<03:05,  3.64it/s] 18%|█▊        | 144/820 [00:39<03:05,  3.65it/s] 18%|█▊        | 145/820 [00:40<03:05,  3.65it/s] 18%|█▊        | 146/820 [00:40<03:04,  3.65it/s] 18%|█▊        | 147/820 [00:40<03:04,  3.65it/s] 18%|█▊        | 148/820 [00:41<03:11,  3.51it/s] 18%|█▊        | 149/820 [00:41<03:08,  3.55it/s] 18%|█▊        | 150/820 [00:41<03:07,  3.58it/s] 18%|█▊        | 151/820 [00:41<03:05,  3.60it/s] 19%|█▊        | 152/820 [00:42<03:04,  3.61it/s] 19%|█▊        | 153/820 [00:42<03:03,  3.63it/s] 19%|█▉        | 154/820 [00:42<03:03,  3.63it/s] 19%|█▉        | 155/820 [00:42<03:02,  3.64it/s] 19%|█▉        | 156/820 [00:43<03:02,  3.64it/s] 19%|█▉        | 157/820 [00:43<03:02,  3.64it/s] 19%|█▉        | 158/820 [00:43<03:10,  3.47it/s] 19%|█▉        | 159/820 [00:44<03:20,  3.30it/s] 20%|█▉        | 160/820 [00:44<03:13,  3.40it/s] 20%|█▉        | 161/820 [00:44<03:09,  3.47it/s] 20%|█▉        | 162/820 [00:45<03:06,  3.52it/s] 20%|█▉        | 163/820 [00:45<03:04,  3.56it/s] 20%|██        | 164/820 [00:45<02:50,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 13:38:05,120 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:38:05,120 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 13:38:05,120 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.02it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.74it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.89it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.85it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.22it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.73it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.45it/s][A
  7%|▋         | 43/608 [00:01<00:26, 21.13it/s][A
  8%|▊         | 48/608 [00:01<00:21, 25.65it/s][A
  9%|▊         | 52/608 [00:01<00:20, 27.34it/s][A
  9%|▉         | 57/608 [00:01<00:17, 31.40it/s][A
 10%|█         | 62/608 [00:01<00:15, 34.82it/s][A
 11%|█         | 67/608 [00:01<00:14, 37.61it/s][A
 12%|█▏        | 72/608 [00:01<00:13, 39.79it/s][A
 13%|█▎        | 77/608 [00:02<00:12, 41.47it/s][A
 13%|█▎        | 82/608 [00:02<00:12, 42.58it/s][A
 14%|█▍        | 87/608 [00:02<00:12, 43.10it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 43.50it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 43.89it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 44.45it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.84it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.10it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.31it/s][A
 20%|██        | 122/608 [00:03<00:10, 45.35it/s][A
 21%|██        | 127/608 [00:03<00:10, 45.27it/s][A
 22%|██▏       | 132/608 [00:03<00:10, 44.95it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.74it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.87it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.06it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.31it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.46it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.59it/s][A
 27%|██▋       | 167/608 [00:04<00:09, 45.62it/s][A
 28%|██▊       | 172/608 [00:04<00:09, 45.49it/s][A
 29%|██▉       | 177/608 [00:04<00:09, 45.16it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.98it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.03it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.03it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.17it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.24it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.43it/s][A
 35%|███▍      | 212/608 [00:05<00:08, 44.05it/s][A
 36%|███▌      | 217/608 [00:05<00:08, 44.54it/s][A
 37%|███▋      | 222/608 [00:05<00:08, 44.61it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.65it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 44.77it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 44.91it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.13it/s][A
 41%|████      | 247/608 [00:05<00:07, 45.27it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.28it/s][A
 42%|████▏     | 257/608 [00:06<00:07, 45.34it/s][A
 43%|████▎     | 262/608 [00:06<00:07, 45.44it/s][A
 44%|████▍     | 267/608 [00:06<00:07, 45.26it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.12it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.18it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.20it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.36it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.41it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.40it/s][A
 50%|████▉     | 302/608 [00:07<00:06, 45.44it/s][A
 50%|█████     | 307/608 [00:07<00:06, 45.44it/s][A
 51%|█████▏    | 312/608 [00:07<00:06, 45.24it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 45.19it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.22it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.20it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.24it/s][A
 55%|█████▌    | 337/608 [00:07<00:05, 45.33it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.41it/s][A
 57%|█████▋    | 347/608 [00:08<00:05, 45.48it/s][A
 58%|█████▊    | 352/608 [00:08<00:05, 43.72it/s][A
 59%|█████▊    | 357/608 [00:08<00:05, 44.10it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.52it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.71it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.89it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.05it/s][A
 63%|██████▎   | 382/608 [00:08<00:04, 45.21it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.27it/s][A
 64%|██████▍   | 392/608 [00:09<00:04, 45.19it/s][A
 65%|██████▌   | 397/608 [00:09<00:04, 45.23it/s][A
 66%|██████▌   | 402/608 [00:09<00:04, 45.23it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 45.26it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.22it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.25it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.31it/s][A
 70%|███████   | 427/608 [00:09<00:03, 45.37it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.29it/s][A
 72%|███████▏  | 437/608 [00:10<00:03, 45.30it/s][A
 73%|███████▎  | 442/608 [00:10<00:03, 45.32it/s][A
 74%|███████▎  | 447/608 [00:10<00:03, 45.35it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 45.25it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 45.29it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.29it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.36it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.31it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.31it/s][A
 79%|███████▉  | 482/608 [00:11<00:02, 45.32it/s][A
 80%|████████  | 487/608 [00:11<00:02, 45.32it/s][A
 81%|████████  | 492/608 [00:11<00:02, 44.54it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.78it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.01it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.08it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.15it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.12it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.27it/s][A
 87%|████████▋ | 527/608 [00:12<00:01, 45.24it/s][A
 88%|████████▊ | 532/608 [00:12<00:01, 45.16it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 45.19it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 45.25it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.32it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.39it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.35it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.31it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.33it/s][A
 94%|█████████▍| 572/608 [00:13<00:00, 45.36it/s][A
 95%|█████████▍| 577/608 [00:13<00:00, 45.21it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 45.22it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.22it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.32it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 45.38it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 45.35it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.25it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.25it/s][A 20%|██        | 164/820 [00:59<02:50,  3.86it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:38:19,232 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 13:38:19,439 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:38:22,632 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:38:22,765 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:38:22,835 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164/special_tokens_map.json
 20%|██        | 165/820 [01:04<1:04:46,  5.93s/it] 20%|██        | 166/820 [01:04<46:11,  4.24s/it]  /cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 20%|██        | 167/820 [01:05<33:11,  3.05s/it] 20%|██        | 168/820 [01:05<24:06,  2.22s/it] 21%|██        | 169/820 [01:05<17:45,  1.64s/it] 21%|██        | 170/820 [01:06<13:19,  1.23s/it] 21%|██        | 171/820 [01:06<10:17,  1.05it/s] 21%|██        | 172/820 [01:06<08:05,  1.33it/s] 21%|██        | 173/820 [01:06<06:33,  1.64it/s] 21%|██        | 174/820 [01:07<05:29,  1.96it/s] 21%|██▏       | 175/820 [01:07<04:44,  2.27it/s] 21%|██▏       | 176/820 [01:07<04:12,  2.55it/s] 22%|██▏       | 177/820 [01:08<03:50,  2.79it/s] 22%|██▏       | 178/820 [01:08<03:34,  2.99it/s] 22%|██▏       | 179/820 [01:08<03:23,  3.15it/s] 22%|██▏       | 180/820 [01:08<03:15,  3.27it/s] 22%|██▏       | 181/820 [01:09<03:10,  3.36it/s] 22%|██▏       | 182/820 [01:09<03:12,  3.31it/s] 22%|██▏       | 183/820 [01:09<03:07,  3.39it/s] 22%|██▏       | 184/820 [01:10<03:04,  3.45it/s] 23%|██▎       | 185/820 [01:10<03:02,  3.49it/s] 23%|██▎       | 186/820 [01:10<03:00,  3.51it/s] 23%|██▎       | 187/820 [01:10<02:59,  3.53it/s] 23%|██▎       | 188/820 [01:11<02:58,  3.55it/s] 23%|██▎       | 189/820 [01:11<02:57,  3.56it/s] 23%|██▎       | 190/820 [01:11<02:56,  3.57it/s] 23%|██▎       | 191/820 [01:11<02:55,  3.58it/s] 23%|██▎       | 192/820 [01:12<02:55,  3.58it/s] 24%|██▎       | 193/820 [01:12<03:01,  3.45it/s] 24%|██▎       | 194/820 [01:12<02:59,  3.49it/s] 24%|██▍       | 195/820 [01:13<02:57,  3.52it/s] 24%|██▍       | 196/820 [01:13<02:56,  3.54it/s] 24%|██▍       | 197/820 [01:13<02:55,  3.55it/s] 24%|██▍       | 198/820 [01:13<02:54,  3.56it/s] 24%|██▍       | 199/820 [01:14<02:54,  3.56it/s] 24%|██▍       | 200/820 [01:14<02:53,  3.57it/s] 25%|██▍       | 201/820 [01:14<02:53,  3.57it/s] 25%|██▍       | 202/820 [01:15<02:52,  3.57it/s] 25%|██▍       | 203/820 [01:15<02:52,  3.58it/s] 25%|██▍       | 204/820 [01:15<02:59,  3.43it/s] 25%|██▌       | 205/820 [01:15<02:56,  3.48it/s] 25%|██▌       | 206/820 [01:16<02:54,  3.51it/s] 25%|██▌       | 207/820 [01:16<02:53,  3.53it/s] 25%|██▌       | 208/820 [01:16<02:52,  3.54it/s] 25%|██▌       | 209/820 [01:17<02:51,  3.55it/s] 26%|██▌       | 210/820 [01:17<02:51,  3.57it/s] 26%|██▌       | 211/820 [01:17<02:50,  3.57it/s] 26%|██▌       | 212/820 [01:17<02:49,  3.58it/s] 26%|██▌       | 213/820 [01:18<02:49,  3.58it/s] 26%|██▌       | 214/820 [01:18<03:10,  3.17it/s] 26%|██▌       | 215/820 [01:18<03:04,  3.29it/s] 26%|██▋       | 216/820 [01:19<02:59,  3.37it/s] 26%|██▋       | 217/820 [01:19<02:55,  3.43it/s] 27%|██▋       | 218/820 [01:19<02:53,  3.48it/s] 27%|██▋       | 219/820 [01:19<02:51,  3.51it/s] 27%|██▋       | 220/820 [01:20<02:50,  3.53it/s] 27%|██▋       | 221/820 [01:20<02:48,  3.55it/s] 27%|██▋       | 222/820 [01:20<02:48,  3.55it/s] 27%|██▋       | 223/820 [01:21<02:47,  3.56it/s] 27%|██▋       | 224/820 [01:21<02:47,  3.57it/s] 27%|██▋       | 225/820 [01:21<02:53,  3.42it/s] 28%|██▊       | 226/820 [01:21<02:50,  3.48it/s] 28%|██▊       | 227/820 [01:22<02:48,  3.53it/s] 28%|██▊       | 228/820 [01:22<02:46,  3.56it/s] 28%|██▊       | 229/820 [01:22<02:44,  3.58it/s] 28%|██▊       | 230/820 [01:23<02:44,  3.60it/s] 28%|██▊       | 231/820 [01:23<02:42,  3.62it/s] 28%|██▊       | 232/820 [01:23<02:42,  3.62it/s] 28%|██▊       | 233/820 [01:23<02:41,  3.63it/s] 29%|██▊       | 234/820 [01:24<02:41,  3.63it/s] 29%|██▊       | 235/820 [01:24<02:41,  3.63it/s] 29%|██▉       | 236/820 [01:24<02:51,  3.41it/s] 29%|██▉       | 237/820 [01:25<02:48,  3.47it/s] 29%|██▉       | 238/820 [01:25<02:45,  3.52it/s] 29%|██▉       | 239/820 [01:25<02:43,  3.55it/s] 29%|██▉       | 240/820 [01:25<02:41,  3.58it/s] 29%|██▉       | 241/820 [01:26<02:40,  3.60it/s] 30%|██▉       | 242/820 [01:26<02:40,  3.61it/s] 30%|██▉       | 243/820 [01:26<02:39,  3.62it/s] 30%|██▉       | 244/820 [01:26<02:38,  3.62it/s] 30%|██▉       | 245/820 [01:27<02:38,  3.63it/s] 30%|███       | 246/820 [01:27<02:38,  3.63it/s] 30%|███       | 247/820 [01:27<02:46,  3.45it/s] 30%|███       | 248/820 [01:28<02:43,  3.50it/s] 30%|███       | 249/820 [01:28<02:41,  3.54it/s] 30%|███       | 250/820 [01:28<02:39,  3.57it/s] 31%|███       | 251/820 [01:28<02:38,  3.60it/s] 31%|███       | 252/820 [01:29<02:37,  3.61it/s] 31%|███       | 253/820 [01:29<02:36,  3.62it/s] 31%|███       | 254/820 [01:29<02:35,  3.63it/s] 31%|███       | 255/820 [01:30<02:35,  3.63it/s] 31%|███       | 256/820 [01:30<02:35,  3.63it/s] 31%|███▏      | 257/820 [01:30<02:34,  3.64it/s] 31%|███▏      | 258/820 [01:30<02:37,  3.56it/s] 32%|███▏      | 259/820 [01:31<02:36,  3.58it/s] 32%|███▏      | 260/820 [01:31<02:35,  3.60it/s] 32%|███▏      | 261/820 [01:31<02:34,  3.61it/s] 32%|███▏      | 262/820 [01:32<02:34,  3.61it/s] 32%|███▏      | 263/820 [01:32<02:33,  3.62it/s] 32%|███▏      | 264/820 [01:32<02:33,  3.62it/s] 32%|███▏      | 265/820 [01:32<02:32,  3.63it/s] 32%|███▏      | 266/820 [01:33<02:32,  3.63it/s] 33%|███▎      | 267/820 [01:33<02:32,  3.63it/s] 33%|███▎      | 268/820 [01:33<02:31,  3.64it/s] 33%|███▎      | 269/820 [01:33<02:36,  3.52it/s] 33%|███▎      | 270/820 [01:34<02:34,  3.55it/s] 33%|███▎      | 271/820 [01:34<02:33,  3.58it/s] 33%|███▎      | 272/820 [01:34<02:32,  3.60it/s] 33%|███▎      | 273/820 [01:35<02:31,  3.61it/s] 33%|███▎      | 274/820 [01:35<02:30,  3.62it/s] 34%|███▎      | 275/820 [01:35<02:30,  3.62it/s] 34%|███▎      | 276/820 [01:35<02:29,  3.63it/s] 34%|███▍      | 277/820 [01:36<02:29,  3.63it/s] 34%|███▍      | 278/820 [01:36<02:29,  3.63it/s] 34%|███▍      | 279/820 [01:36<02:28,  3.63it/s] 34%|███▍      | 280/820 [01:37<02:32,  3.54it/s] 34%|███▍      | 281/820 [01:37<02:30,  3.57it/s] 34%|███▍      | 282/820 [01:37<02:29,  3.59it/s] 35%|███▍      | 283/820 [01:37<02:29,  3.60it/s] 35%|███▍      | 284/820 [01:38<02:27,  3.62it/s] 35%|███▍      | 285/820 [01:38<02:27,  3.62it/s] 35%|███▍      | 286/820 [01:38<02:27,  3.63it/s] 35%|███▌      | 287/820 [01:38<02:26,  3.63it/s] 35%|███▌      | 288/820 [01:39<02:26,  3.63it/s] 35%|███▌      | 289/820 [01:39<02:26,  3.63it/s] 35%|███▌      | 290/820 [01:39<02:25,  3.63it/s] 35%|███▌      | 291/820 [01:40<02:30,  3.51it/s] 36%|███▌      | 292/820 [01:40<02:28,  3.55it/s] 36%|███▌      | 293/820 [01:40<02:27,  3.57it/s] 36%|███▌      | 294/820 [01:40<02:26,  3.60it/s] 36%|███▌      | 295/820 [01:41<02:25,  3.61it/s] 36%|███▌      | 296/820 [01:41<02:24,  3.62it/s] 36%|███▌      | 297/820 [01:41<02:24,  3.62it/s] 36%|███▋      | 298/820 [01:41<02:23,  3.63it/s] 36%|███▋      | 299/820 [01:42<02:23,  3.63it/s] 37%|███▋      | 300/820 [01:42<02:23,  3.63it/s] 37%|███▋      | 301/820 [01:42<02:22,  3.64it/s] 37%|███▋      | 302/820 [01:43<02:27,  3.52it/s] 37%|███▋      | 303/820 [01:43<02:25,  3.56it/s] 37%|███▋      | 304/820 [01:43<02:24,  3.58it/s] 37%|███▋      | 305/820 [01:43<02:25,  3.54it/s] 37%|███▋      | 306/820 [01:44<02:24,  3.56it/s] 37%|███▋      | 307/820 [01:44<02:23,  3.58it/s] 38%|███▊      | 308/820 [01:44<02:22,  3.60it/s] 38%|███▊      | 309/820 [01:45<02:21,  3.61it/s] 38%|███▊      | 310/820 [01:45<02:21,  3.61it/s] 38%|███▊      | 311/820 [01:45<02:20,  3.62it/s] 38%|███▊      | 312/820 [01:45<02:20,  3.63it/s] 38%|███▊      | 313/820 [01:46<02:23,  3.53it/s] 38%|███▊      | 314/820 [01:46<02:26,  3.44it/s] 38%|███▊      | 315/820 [01:47<03:00,  2.80it/s] 39%|███▊      | 316/820 [01:47<02:47,  3.00it/s] 39%|███▊      | 317/820 [01:47<02:38,  3.17it/s] 39%|███▉      | 318/820 [01:47<02:32,  3.30it/s] 39%|███▉      | 319/820 [01:48<02:27,  3.39it/s] 39%|███▉      | 320/820 [01:48<02:24,  3.46it/s] 39%|███▉      | 321/820 [01:48<02:22,  3.51it/s] 39%|███▉      | 322/820 [01:48<02:20,  3.54it/s] 39%|███▉      | 323/820 [01:49<02:19,  3.57it/s] 40%|███▉      | 324/820 [01:49<02:17,  3.60it/s] 40%|███▉      | 325/820 [01:49<02:17,  3.61it/s] 40%|███▉      | 326/820 [01:50<02:16,  3.62it/s] 40%|███▉      | 327/820 [01:50<02:16,  3.62it/s] 40%|████      | 328/820 [01:50<02:05,  3.91it/s][INFO|trainer.py:2140] 2023-08-28 13:39:10,138 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:39:10,138 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 13:39:10,138 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.8769, 'eval_samples_per_second': 350.511, 'eval_steps_per_second': 43.814, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.75it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.50it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.98it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.01it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.28it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.88it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.62it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.36it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.48it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.60it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.65it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.74it/s][A
 11%|█         | 68/608 [00:01<00:12, 43.14it/s][A
 12%|█▏        | 73/608 [00:01<00:12, 43.82it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 44.26it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 44.48it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.71it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.87it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.18it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.43it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.27it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.34it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.42it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.43it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.32it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.34it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.36it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.45it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.43it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.39it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.42it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.41it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.39it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.37it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.36it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.44it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.48it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.42it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.44it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.32it/s][A
 34%|███▍      | 208/608 [00:04<00:09, 43.18it/s][A
 35%|███▌      | 213/608 [00:04<00:09, 43.84it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 44.21it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 44.53it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 44.91it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.13it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.17it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.32it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.11it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.21it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.28it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.31it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.42it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.43it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.53it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.46it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.41it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.34it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.28it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.32it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.38it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.42it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.47it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.49it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.49it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.43it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.30it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.26it/s][A
 57%|█████▋    | 348/608 [00:07<00:06, 42.61it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 43.57it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 44.22it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 44.69it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.00it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.12it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.18it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.10it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 44.89it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 44.93it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.12it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.31it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.51it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.61it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.62it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.53it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.35it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.20it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.05it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.21it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.32it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.47it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.62it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.67it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.59it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.30it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.16it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.11it/s][A
 80%|████████  | 488/608 [00:10<00:02, 42.21it/s][A
 81%|████████  | 493/608 [00:10<00:02, 43.26it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 44.02it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 44.60it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 44.94it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.14it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.13it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.03it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 44.84it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 44.91it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.14it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.36it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.49it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.59it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.50it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.55it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.47it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.30it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.24it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.26it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.42it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.52it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.56it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.44it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.49it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.49it/s][A 40%|████      | 328/820 [02:03<02:05,  3.91it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:39:23,918 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 13:39:24,190 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:39:27,794 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:39:28,009 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:39:28,110 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328/special_tokens_map.json
 40%|████      | 329/820 [02:10<49:30,  6.05s/it] 40%|████      | 330/820 [02:10<35:16,  4.32s/it] 40%|████      | 331/820 [02:10<25:19,  3.11s/it] 40%|████      | 332/820 [02:10<18:22,  2.26s/it] 41%|████      | 333/820 [02:11<13:30,  1.66s/it] 41%|████      | 334/820 [02:11<10:13,  1.26s/it] 41%|████      | 335/820 [02:11<07:48,  1.03it/s] 41%|████      | 336/820 [02:12<06:07,  1.32it/s] 41%|████      | 337/820 [02:12<04:57,  1.62it/s] 41%|████      | 338/820 [02:12<04:07,  1.94it/s] 41%|████▏     | 339/820 [02:12<03:33,  2.25it/s] 41%|████▏     | 340/820 [02:13<03:09,  2.54it/s] 42%|████▏     | 341/820 [02:13<02:52,  2.78it/s] 42%|████▏     | 342/820 [02:13<02:40,  2.98it/s] 42%|████▏     | 343/820 [02:14<02:31,  3.14it/s] 42%|████▏     | 344/820 [02:14<02:25,  3.26it/s] 42%|████▏     | 345/820 [02:14<02:24,  3.29it/s] 42%|████▏     | 346/820 [02:14<02:20,  3.37it/s] 42%|████▏     | 347/820 [02:15<02:17,  3.43it/s] 42%|████▏     | 348/820 [02:15<02:15,  3.47it/s] 43%|████▎     | 349/820 [02:15<02:14,  3.51it/s] 43%|████▎     | 350/820 [02:16<02:13,  3.53it/s] 43%|████▎     | 351/820 [02:16<02:12,  3.55it/s] 43%|████▎     | 352/820 [02:16<02:11,  3.55it/s] 43%|████▎     | 353/820 [02:16<02:11,  3.56it/s] 43%|████▎     | 354/820 [02:17<02:10,  3.57it/s] 43%|████▎     | 355/820 [02:17<02:10,  3.57it/s] 43%|████▎     | 356/820 [02:17<02:21,  3.27it/s] 44%|████▎     | 357/820 [02:18<02:17,  3.36it/s] 44%|████▎     | 358/820 [02:18<02:15,  3.42it/s] 44%|████▍     | 359/820 [02:18<02:12,  3.47it/s] 44%|████▍     | 360/820 [02:18<02:11,  3.50it/s] 44%|████▍     | 361/820 [02:19<02:10,  3.52it/s] 44%|████▍     | 362/820 [02:19<02:09,  3.54it/s] 44%|████▍     | 363/820 [02:19<02:08,  3.55it/s] 44%|████▍     | 364/820 [02:20<02:08,  3.56it/s] 45%|████▍     | 365/820 [02:20<02:07,  3.56it/s] 45%|████▍     | 366/820 [02:20<02:07,  3.56it/s] 45%|████▍     | 367/820 [02:20<02:06,  3.57it/s] 45%|████▍     | 368/820 [02:21<02:06,  3.58it/s] 45%|████▌     | 369/820 [02:21<02:06,  3.58it/s] 45%|████▌     | 370/820 [02:21<02:05,  3.58it/s] 45%|████▌     | 371/820 [02:21<02:05,  3.58it/s] 45%|████▌     | 372/820 [02:22<02:05,  3.58it/s] 45%|████▌     | 373/820 [02:22<02:04,  3.58it/s] 46%|████▌     | 374/820 [02:22<02:04,  3.58it/s] 46%|████▌     | 375/820 [02:23<02:13,  3.33it/s] 46%|████▌     | 376/820 [02:23<02:10,  3.40it/s] 46%|████▌     | 377/820 [02:23<02:08,  3.45it/s] 46%|████▌     | 378/820 [02:23<02:06,  3.49it/s] 46%|████▌     | 379/820 [02:24<02:05,  3.52it/s] 46%|████▋     | 380/820 [02:24<02:04,  3.53it/s] 46%|████▋     | 381/820 [02:24<02:03,  3.55it/s] 47%|████▋     | 382/820 [02:25<02:03,  3.56it/s] 47%|████▋     | 383/820 [02:25<02:02,  3.57it/s] 47%|████▋     | 384/820 [02:25<02:02,  3.57it/s] 47%|████▋     | 385/820 [02:25<02:01,  3.57it/s] 47%|████▋     | 386/820 [02:26<02:10,  3.33it/s] 47%|████▋     | 387/820 [02:26<02:07,  3.40it/s] 47%|████▋     | 388/820 [02:26<02:05,  3.45it/s] 47%|████▋     | 389/820 [02:27<02:03,  3.49it/s] 48%|████▊     | 390/820 [02:27<02:02,  3.52it/s] 48%|████▊     | 391/820 [02:27<02:01,  3.54it/s] 48%|████▊     | 392/820 [02:27<02:00,  3.55it/s] 48%|████▊     | 393/820 [02:28<01:59,  3.56it/s] 48%|████▊     | 394/820 [02:28<01:59,  3.57it/s] 48%|████▊     | 395/820 [02:28<01:58,  3.57it/s] 48%|████▊     | 396/820 [02:29<01:58,  3.58it/s] 48%|████▊     | 397/820 [02:29<02:01,  3.48it/s] 49%|████▊     | 398/820 [02:29<02:00,  3.51it/s] 49%|████▊     | 399/820 [02:29<01:59,  3.53it/s] 49%|████▉     | 400/820 [02:30<01:58,  3.54it/s] 49%|████▉     | 401/820 [02:30<01:57,  3.55it/s] 49%|████▉     | 402/820 [02:30<01:57,  3.56it/s] 49%|████▉     | 403/820 [02:31<01:56,  3.57it/s] 49%|████▉     | 404/820 [02:31<01:56,  3.57it/s] 49%|████▉     | 405/820 [02:31<01:56,  3.57it/s] 50%|████▉     | 406/820 [02:31<01:55,  3.57it/s] 50%|████▉     | 407/820 [02:32<01:55,  3.58it/s] 50%|████▉     | 408/820 [02:32<01:57,  3.52it/s] 50%|████▉     | 409/820 [02:32<01:56,  3.54it/s] 50%|█████     | 410/820 [02:33<01:55,  3.55it/s] 50%|█████     | 411/820 [02:33<01:54,  3.57it/s] 50%|█████     | 412/820 [02:33<01:54,  3.57it/s] 50%|█████     | 413/820 [02:33<01:53,  3.58it/s] 50%|█████     | 414/820 [02:34<01:53,  3.58it/s] 51%|█████     | 415/820 [02:34<01:53,  3.58it/s] 51%|█████     | 416/820 [02:34<01:52,  3.58it/s] 51%|█████     | 417/820 [02:34<01:52,  3.58it/s] 51%|█████     | 418/820 [02:35<01:52,  3.58it/s] 51%|█████     | 419/820 [02:35<01:54,  3.52it/s] 51%|█████     | 420/820 [02:35<01:53,  3.53it/s] 51%|█████▏    | 421/820 [02:36<01:52,  3.55it/s] 51%|█████▏    | 422/820 [02:36<01:51,  3.56it/s] 52%|█████▏    | 423/820 [02:36<01:51,  3.57it/s] 52%|█████▏    | 424/820 [02:36<01:50,  3.57it/s] 52%|█████▏    | 425/820 [02:37<01:50,  3.57it/s] 52%|█████▏    | 426/820 [02:37<01:50,  3.58it/s] 52%|█████▏    | 427/820 [02:37<01:49,  3.58it/s] 52%|█████▏    | 428/820 [02:38<01:49,  3.58it/s] 52%|█████▏    | 429/820 [02:38<01:49,  3.58it/s] 52%|█████▏    | 430/820 [02:38<01:52,  3.48it/s] 53%|█████▎    | 431/820 [02:38<01:50,  3.51it/s] 53%|█████▎    | 432/820 [02:39<01:49,  3.53it/s] 53%|█████▎    | 433/820 [02:39<01:49,  3.55it/s] 53%|█████▎    | 434/820 [02:39<01:48,  3.56it/s] 53%|█████▎    | 435/820 [02:40<01:48,  3.56it/s] 53%|█████▎    | 436/820 [02:40<01:47,  3.57it/s] 53%|█████▎    | 437/820 [02:40<01:47,  3.58it/s] 53%|█████▎    | 438/820 [02:40<01:46,  3.58it/s] 54%|█████▎    | 439/820 [02:41<01:46,  3.58it/s] 54%|█████▎    | 440/820 [02:41<01:46,  3.58it/s] 54%|█████▍    | 441/820 [02:41<01:50,  3.44it/s] 54%|█████▍    | 442/820 [02:42<01:48,  3.48it/s] 54%|█████▍    | 443/820 [02:42<01:47,  3.51it/s] 54%|█████▍    | 444/820 [02:42<01:46,  3.53it/s] 54%|█████▍    | 445/820 [02:42<01:45,  3.55it/s] 54%|█████▍    | 446/820 [02:43<01:45,  3.56it/s] 55%|█████▍    | 447/820 [02:43<01:44,  3.56it/s] 55%|█████▍    | 448/820 [02:43<01:44,  3.57it/s] 55%|█████▍    | 449/820 [02:44<01:47,  3.46it/s] 55%|█████▍    | 450/820 [02:44<01:46,  3.48it/s] 55%|█████▌    | 451/820 [02:44<01:45,  3.51it/s] 55%|█████▌    | 452/820 [02:44<01:48,  3.40it/s] 55%|█████▌    | 453/820 [02:45<01:48,  3.37it/s] 55%|█████▌    | 454/820 [02:45<01:47,  3.42it/s] 55%|█████▌    | 455/820 [02:45<01:45,  3.46it/s] 56%|█████▌    | 456/820 [02:46<01:44,  3.49it/s] 56%|█████▌    | 457/820 [02:46<01:42,  3.53it/s] 56%|█████▌    | 458/820 [02:46<01:47,  3.35it/s] 56%|█████▌    | 459/820 [02:47<02:11,  2.75it/s] 56%|█████▌    | 460/820 [02:47<02:01,  2.96it/s] 56%|█████▌    | 461/820 [02:47<01:54,  3.13it/s] 56%|█████▋    | 462/820 [02:48<01:50,  3.23it/s] 56%|█████▋    | 463/820 [02:48<01:46,  3.34it/s] 57%|█████▋    | 464/820 [02:48<01:43,  3.42it/s] 57%|█████▋    | 465/820 [02:48<01:41,  3.49it/s] 57%|█████▋    | 466/820 [02:49<01:40,  3.53it/s] 57%|█████▋    | 467/820 [02:49<01:39,  3.56it/s] 57%|█████▋    | 468/820 [02:49<01:38,  3.59it/s] 57%|█████▋    | 469/820 [02:49<01:37,  3.60it/s] 57%|█████▋    | 470/820 [02:50<01:36,  3.61it/s] 57%|█████▋    | 471/820 [02:50<01:36,  3.62it/s] 58%|█████▊    | 472/820 [02:50<01:36,  3.62it/s] 58%|█████▊    | 473/820 [02:51<01:35,  3.62it/s] 58%|█████▊    | 474/820 [02:51<01:35,  3.63it/s] 58%|█████▊    | 475/820 [02:51<01:34,  3.63it/s] 58%|█████▊    | 476/820 [02:51<01:34,  3.63it/s] 58%|█████▊    | 477/820 [02:52<01:34,  3.64it/s] 58%|█████▊    | 478/820 [02:52<01:34,  3.64it/s] 58%|█████▊    | 479/820 [02:52<01:33,  3.64it/s] 59%|█████▊    | 480/820 [02:52<01:33,  3.64it/s] 59%|█████▊    | 481/820 [02:53<01:33,  3.64it/s] 59%|█████▉    | 482/820 [02:53<01:32,  3.64it/s] 59%|█████▉    | 483/820 [02:53<01:35,  3.54it/s] 59%|█████▉    | 484/820 [02:54<01:34,  3.57it/s] 59%|█████▉    | 485/820 [02:54<01:33,  3.59it/s] 59%|█████▉    | 486/820 [02:54<01:32,  3.60it/s] 59%|█████▉    | 487/820 [02:54<01:32,  3.61it/s] 60%|█████▉    | 488/820 [02:55<01:31,  3.62it/s] 60%|█████▉    | 489/820 [02:55<01:31,  3.62it/s] 60%|█████▉    | 490/820 [02:55<01:31,  3.62it/s] 60%|█████▉    | 491/820 [02:56<01:30,  3.63it/s] 60%|██████    | 492/820 [02:56<01:23,  3.91it/s][INFO|trainer.py:2140] 2023-08-28 13:40:15,855 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:40:15,855 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 13:40:15,855 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.4761, 'eval_samples_per_second': 360.935, 'eval_steps_per_second': 45.117, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.78it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.83it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.87it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.04it/s][A
  5%|▍         | 28/608 [00:00<00:12, 45.64it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.47it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.31it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.14it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.33it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.51it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.60it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.60it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.56it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.39it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.31it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.19it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.14it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.17it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.40it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.55it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.55it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.48it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.52it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.32it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.28it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.16it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.22it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.28it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.50it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 45.54it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.63it/s][A
 27%|██▋       | 163/608 [00:03<00:10, 43.38it/s][A
 28%|██▊       | 168/608 [00:03<00:10, 43.88it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 44.40it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.65it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.81it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.92it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.09it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.33it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.19it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.41it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.44it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.45it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.35it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.39it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.29it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.26it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.36it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.45it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.50it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.53it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.40it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.46it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.35it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.28it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.27it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.38it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.42it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.43it/s][A
 50%|████▉     | 303/608 [00:06<00:07, 42.15it/s][A
 51%|█████     | 308/608 [00:06<00:06, 43.20it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 43.99it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 44.41it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 44.66it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 44.91it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.11it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.25it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 44.99it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 44.96it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.12it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.36it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.48it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.41it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.52it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.52it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.41it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.18it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.13it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.24it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.31it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.48it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.50it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.51it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.48it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.42it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.25it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 43.94it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 44.41it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 44.87it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.17it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.30it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.34it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.35it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.26it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 44.99it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.09it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.12it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.34it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.45it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.57it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.58it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.59it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.42it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.27it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.12it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.32it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.40it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 45.48it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.50it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.55it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.48it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.36it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.24it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.18it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 44.72it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 44.98it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 45.24it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.30it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.38it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.39it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.29it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.29it/s][A 60%|██████    | 492/820 [03:09<01:23,  3.91it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:40:29,554 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 13:40:29,709 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:40:33,722 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:40:33,873 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:40:33,981 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492/special_tokens_map.json
 60%|██████    | 493/820 [03:15<33:12,  6.09s/it] 60%|██████    | 494/820 [03:16<23:37,  4.35s/it] 60%|██████    | 495/820 [03:16<16:56,  3.13s/it] 60%|██████    | 496/820 [03:16<12:16,  2.27s/it] 61%|██████    | 497/820 [03:17<09:01,  1.67s/it] 61%|██████    | 498/820 [03:17<06:44,  1.26s/it] 61%|██████    | 499/820 [03:17<05:12,  1.03it/s] 61%|██████    | 500/820 [03:17<04:04,  1.31it/s]                                                  61%|██████    | 500/820 [03:17<04:04,  1.31it/s] 61%|██████    | 501/820 [03:18<03:17,  1.62it/s] 61%|██████    | 502/820 [03:18<02:44,  1.93it/s] 61%|██████▏   | 503/820 [03:18<02:21,  2.24it/s] 61%|██████▏   | 504/820 [03:19<02:05,  2.53it/s] 62%|██████▏   | 505/820 [03:19<01:53,  2.77it/s] 62%|██████▏   | 506/820 [03:19<01:45,  2.97it/s] 62%|██████▏   | 507/820 [03:19<01:39,  3.13it/s] 62%|██████▏   | 508/820 [03:20<01:35,  3.25it/s] 62%|██████▏   | 509/820 [03:20<01:33,  3.34it/s] 62%|██████▏   | 510/820 [03:20<01:37,  3.17it/s] 62%|██████▏   | 511/820 [03:21<01:34,  3.28it/s] 62%|██████▏   | 512/820 [03:21<01:34,  3.24it/s] 63%|██████▎   | 513/820 [03:21<01:31,  3.34it/s] 63%|██████▎   | 514/820 [03:21<01:29,  3.41it/s] 63%|██████▎   | 515/820 [03:22<01:28,  3.46it/s] 63%|██████▎   | 516/820 [03:22<01:26,  3.50it/s] 63%|██████▎   | 517/820 [03:22<01:25,  3.52it/s] 63%|██████▎   | 518/820 [03:23<01:25,  3.54it/s] 63%|██████▎   | 519/820 [03:23<01:24,  3.55it/s] 63%|██████▎   | 520/820 [03:23<01:24,  3.56it/s] 64%|██████▎   | 521/820 [03:23<01:23,  3.57it/s] 64%|██████▎   | 522/820 [03:24<01:23,  3.57it/s] 64%|██████▍   | 523/820 [03:24<01:26,  3.42it/s] 64%|██████▍   | 524/820 [03:24<01:25,  3.46it/s] 64%|██████▍   | 525/820 [03:25<01:24,  3.50it/s] 64%|██████▍   | 526/820 [03:25<01:23,  3.52it/s] 64%|██████▍   | 527/820 [03:25<01:22,  3.54it/s] 64%|██████▍   | 528/820 [03:25<01:22,  3.55it/s] 65%|██████▍   | 529/820 [03:26<01:21,  3.56it/s] 65%|██████▍   | 530/820 [03:26<01:21,  3.56it/s] 65%|██████▍   | 531/820 [03:26<01:20,  3.57it/s] 65%|██████▍   | 532/820 [03:27<01:20,  3.57it/s] 65%|██████▌   | 533/820 [03:27<01:20,  3.57it/s] 65%|██████▌   | 534/820 [03:27<01:25,  3.35it/s] 65%|██████▌   | 535/820 [03:27<01:23,  3.42it/s] 65%|██████▌   | 536/820 [03:28<01:22,  3.46it/s] 65%|██████▌   | 537/820 [03:28<01:20,  3.49it/s] 66%|██████▌   | 538/820 [03:28<01:20,  3.52it/s] 66%|██████▌   | 539/820 [03:29<01:19,  3.54it/s] 66%|██████▌   | 540/820 [03:29<01:18,  3.55it/s] 66%|██████▌   | 541/820 [03:29<01:18,  3.56it/s] 66%|██████▌   | 542/820 [03:29<01:17,  3.57it/s] 66%|██████▌   | 543/820 [03:30<01:17,  3.57it/s] 66%|██████▋   | 544/820 [03:30<01:17,  3.57it/s] 66%|██████▋   | 545/820 [03:30<01:20,  3.40it/s] 67%|██████▋   | 546/820 [03:31<01:19,  3.46it/s] 67%|██████▋   | 547/820 [03:31<01:18,  3.49it/s] 67%|██████▋   | 548/820 [03:31<01:17,  3.52it/s] 67%|██████▋   | 549/820 [03:31<01:16,  3.54it/s] 67%|██████▋   | 550/820 [03:32<01:16,  3.55it/s] 67%|██████▋   | 551/820 [03:32<01:15,  3.56it/s] 67%|██████▋   | 552/820 [03:32<01:15,  3.56it/s] 67%|██████▋   | 553/820 [03:33<01:14,  3.57it/s] 68%|██████▊   | 554/820 [03:33<01:14,  3.57it/s] 68%|██████▊   | 555/820 [03:33<01:14,  3.57it/s] 68%|██████▊   | 556/820 [03:33<01:17,  3.43it/s] 68%|██████▊   | 557/820 [03:34<01:15,  3.47it/s] 68%|██████▊   | 558/820 [03:34<01:14,  3.50it/s] 68%|██████▊   | 559/820 [03:34<01:14,  3.53it/s] 68%|██████▊   | 560/820 [03:34<01:13,  3.54it/s] 68%|██████▊   | 561/820 [03:35<01:12,  3.56it/s] 69%|██████▊   | 562/820 [03:35<01:12,  3.58it/s] 69%|██████▊   | 563/820 [03:35<01:11,  3.60it/s] 69%|██████▉   | 564/820 [03:36<01:10,  3.61it/s] 69%|██████▉   | 565/820 [03:36<01:10,  3.61it/s] 69%|██████▉   | 566/820 [03:36<01:10,  3.62it/s] 69%|██████▉   | 567/820 [03:36<01:11,  3.54it/s] 69%|██████▉   | 568/820 [03:37<01:10,  3.57it/s] 69%|██████▉   | 569/820 [03:37<01:10,  3.58it/s] 70%|██████▉   | 570/820 [03:37<01:09,  3.60it/s] 70%|██████▉   | 571/820 [03:38<01:09,  3.61it/s] 70%|██████▉   | 572/820 [03:38<01:08,  3.62it/s] 70%|██████▉   | 573/820 [03:38<01:08,  3.62it/s] 70%|███████   | 574/820 [03:38<01:07,  3.63it/s] 70%|███████   | 575/820 [03:39<01:07,  3.63it/s] 70%|███████   | 576/820 [03:39<01:07,  3.63it/s] 70%|███████   | 577/820 [03:39<01:06,  3.63it/s] 70%|███████   | 578/820 [03:40<01:08,  3.54it/s] 71%|███████   | 579/820 [03:40<01:07,  3.56it/s] 71%|███████   | 580/820 [03:40<01:06,  3.59it/s] 71%|███████   | 581/820 [03:40<01:06,  3.60it/s] 71%|███████   | 582/820 [03:41<01:05,  3.61it/s] 71%|███████   | 583/820 [03:41<01:05,  3.62it/s] 71%|███████   | 584/820 [03:41<01:05,  3.62it/s] 71%|███████▏  | 585/820 [03:41<01:04,  3.63it/s] 71%|███████▏  | 586/820 [03:42<01:04,  3.63it/s] 72%|███████▏  | 587/820 [03:42<01:04,  3.63it/s] 72%|███████▏  | 588/820 [03:42<01:03,  3.63it/s] 72%|███████▏  | 589/820 [03:43<01:06,  3.50it/s] 72%|███████▏  | 590/820 [03:43<01:05,  3.54it/s] 72%|███████▏  | 591/820 [03:43<01:04,  3.56it/s] 72%|███████▏  | 592/820 [03:43<01:03,  3.59it/s] 72%|███████▏  | 593/820 [03:44<01:05,  3.49it/s] 72%|███████▏  | 594/820 [03:44<01:04,  3.52it/s] 73%|███████▎  | 595/820 [03:44<01:03,  3.55it/s] 73%|███████▎  | 596/820 [03:45<01:02,  3.58it/s] 73%|███████▎  | 597/820 [03:45<01:02,  3.59it/s] 73%|███████▎  | 598/820 [03:45<01:01,  3.61it/s] 73%|███████▎  | 599/820 [03:45<01:01,  3.61it/s] 73%|███████▎  | 600/820 [03:46<01:02,  3.50it/s] 73%|███████▎  | 601/820 [03:46<01:01,  3.54it/s] 73%|███████▎  | 602/820 [03:46<01:04,  3.40it/s] 74%|███████▎  | 603/820 [03:47<01:08,  3.18it/s] 74%|███████▎  | 604/820 [03:47<01:08,  3.15it/s] 74%|███████▍  | 605/820 [03:47<01:06,  3.26it/s] 74%|███████▍  | 606/820 [03:47<01:03,  3.36it/s] 74%|███████▍  | 607/820 [03:48<01:01,  3.44it/s] 74%|███████▍  | 608/820 [03:48<01:00,  3.49it/s] 74%|███████▍  | 609/820 [03:48<00:59,  3.54it/s] 74%|███████▍  | 610/820 [03:49<00:58,  3.56it/s] 75%|███████▍  | 611/820 [03:49<00:59,  3.49it/s] 75%|███████▍  | 612/820 [03:49<00:58,  3.53it/s] 75%|███████▍  | 613/820 [03:49<00:58,  3.56it/s] 75%|███████▍  | 614/820 [03:50<00:57,  3.58it/s] 75%|███████▌  | 615/820 [03:50<00:56,  3.60it/s] 75%|███████▌  | 616/820 [03:50<00:56,  3.61it/s] 75%|███████▌  | 617/820 [03:51<00:56,  3.61it/s] 75%|███████▌  | 618/820 [03:51<00:55,  3.62it/s] 75%|███████▌  | 619/820 [03:51<00:55,  3.62it/s] 76%|███████▌  | 620/820 [03:51<00:55,  3.63it/s] 76%|███████▌  | 621/820 [03:52<00:54,  3.63it/s] 76%|███████▌  | 622/820 [03:52<00:54,  3.63it/s] 76%|███████▌  | 623/820 [03:52<00:54,  3.63it/s] 76%|███████▌  | 624/820 [03:52<00:53,  3.63it/s] 76%|███████▌  | 625/820 [03:53<00:53,  3.63it/s] 76%|███████▋  | 626/820 [03:53<00:53,  3.63it/s] 76%|███████▋  | 627/820 [03:53<00:53,  3.63it/s] 77%|███████▋  | 628/820 [03:54<00:52,  3.63it/s] 77%|███████▋  | 629/820 [03:54<00:52,  3.63it/s] 77%|███████▋  | 630/820 [03:54<00:52,  3.63it/s] 77%|███████▋  | 631/820 [03:54<00:52,  3.63it/s] 77%|███████▋  | 632/820 [03:55<00:53,  3.50it/s] 77%|███████▋  | 633/820 [03:55<00:52,  3.54it/s] 77%|███████▋  | 634/820 [03:55<00:52,  3.57it/s] 77%|███████▋  | 635/820 [03:56<00:51,  3.59it/s] 78%|███████▊  | 636/820 [03:56<00:51,  3.60it/s] 78%|███████▊  | 637/820 [03:56<00:50,  3.61it/s] 78%|███████▊  | 638/820 [03:56<00:50,  3.62it/s] 78%|███████▊  | 639/820 [03:57<00:49,  3.62it/s] 78%|███████▊  | 640/820 [03:57<00:49,  3.62it/s] 78%|███████▊  | 641/820 [03:57<00:49,  3.63it/s] 78%|███████▊  | 642/820 [03:57<00:49,  3.63it/s] 78%|███████▊  | 643/820 [03:58<00:50,  3.51it/s] 79%|███████▊  | 644/820 [03:58<00:49,  3.54it/s] 79%|███████▊  | 645/820 [03:58<00:49,  3.57it/s] 79%|███████▉  | 646/820 [03:59<00:48,  3.59it/s] 79%|███████▉  | 647/820 [03:59<00:48,  3.60it/s] 79%|███████▉  | 648/820 [03:59<00:47,  3.60it/s] 79%|███████▉  | 649/820 [03:59<00:47,  3.61it/s] 79%|███████▉  | 650/820 [04:00<00:46,  3.62it/s] 79%|███████▉  | 651/820 [04:00<00:46,  3.62it/s] 80%|███████▉  | 652/820 [04:00<00:46,  3.63it/s] 80%|███████▉  | 653/820 [04:01<00:46,  3.63it/s] 80%|███████▉  | 654/820 [04:01<00:46,  3.55it/s] 80%|███████▉  | 655/820 [04:01<00:46,  3.57it/s] 80%|████████  | 656/820 [04:01<00:42,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 13:41:21,433 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:41:21,433 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 13:41:21,433 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.4595, 'eval_samples_per_second': 361.38, 'eval_steps_per_second': 45.172, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.222560975609756e-05, 'epoch': 3.05}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.06it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.60it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.50it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.59it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.00it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.64it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.45it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.21it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.34it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.31it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.38it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.44it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.42it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.32it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.21it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.00it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.15it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.19it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.30it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.27it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 41.94it/s][A
 19%|█▊        | 113/608 [00:02<00:11, 43.07it/s][A
 19%|█▉        | 118/608 [00:02<00:11, 43.84it/s][A
 20%|██        | 123/608 [00:02<00:10, 44.24it/s][A
 21%|██        | 128/608 [00:02<00:10, 44.49it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 44.74it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.88it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.06it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 44.68it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 44.82it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.09it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.31it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.42it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.30it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.35it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.31it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.25it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.98it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.13it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.17it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.35it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.37it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.45it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.37it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.32it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.15it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.06it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 42.85it/s][A
 41%|████      | 248/608 [00:05<00:08, 43.77it/s][A
 42%|████▏     | 253/608 [00:05<00:08, 44.31it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 44.74it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 44.90it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.08it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.07it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 44.98it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 44.66it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 44.80it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.12it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.29it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.43it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.44it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.42it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.29it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.14it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 44.90it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 44.93it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.13it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.38it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.45it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.40it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.44it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.28it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.08it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 44.97it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 44.07it/s][A
 63%|██████▎   | 383/608 [00:08<00:05, 44.64it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 44.99it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.25it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.27it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.29it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.23it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.08it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 44.74it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 44.94it/s][A
 70%|███████   | 428/608 [00:09<00:04, 44.99it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.19it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.46it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.57it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.44it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.37it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.12it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 44.93it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 44.84it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.13it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.32it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.45it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.52it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.46it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.32it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.08it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 44.92it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 44.90it/s][A
 85%|████████▌ | 518/608 [00:11<00:02, 43.15it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 43.96it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 44.54it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 44.88it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.08it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.06it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 44.99it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 44.80it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 44.74it/s][A
 93%|█████████▎| 563/608 [00:12<00:01, 44.92it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.07it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.23it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.40it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.56it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.55it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.34it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 44.99it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 44.97it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.06it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.06it/s][A 80%|████████  | 656/820 [04:15<00:42,  3.86it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:41:35,294 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656
[INFO|configuration_utils.py:351] 2023-08-28 13:41:35,479 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:41:38,673 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:41:38,798 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:41:38,906 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656/special_tokens_map.json
 80%|████████  | 657/820 [04:21<16:11,  5.96s/it] 80%|████████  | 658/820 [04:21<11:29,  4.26s/it] 80%|████████  | 659/820 [04:21<08:13,  3.06s/it] 80%|████████  | 660/820 [04:21<05:56,  2.23s/it] 81%|████████  | 661/820 [04:22<04:21,  1.64s/it] 81%|████████  | 662/820 [04:22<03:17,  1.25s/it] 81%|████████  | 663/820 [04:22<02:31,  1.04it/s] 81%|████████  | 664/820 [04:23<01:58,  1.32it/s] 81%|████████  | 665/820 [04:23<01:35,  1.62it/s] 81%|████████  | 666/820 [04:23<01:19,  1.94it/s] 81%|████████▏ | 667/820 [04:23<01:07,  2.25it/s] 81%|████████▏ | 668/820 [04:24<00:59,  2.53it/s] 82%|████████▏ | 669/820 [04:24<00:54,  2.77it/s] 82%|████████▏ | 670/820 [04:24<00:50,  2.97it/s] 82%|████████▏ | 671/820 [04:25<00:47,  3.13it/s] 82%|████████▏ | 672/820 [04:25<00:45,  3.25it/s] 82%|████████▏ | 673/820 [04:25<00:43,  3.34it/s] 82%|████████▏ | 674/820 [04:25<00:44,  3.26it/s] 82%|████████▏ | 675/820 [04:26<00:43,  3.36it/s] 82%|████████▏ | 676/820 [04:26<00:42,  3.42it/s] 83%|████████▎ | 677/820 [04:26<00:41,  3.46it/s] 83%|████████▎ | 678/820 [04:27<00:40,  3.50it/s] 83%|████████▎ | 679/820 [04:27<00:40,  3.52it/s] 83%|████████▎ | 680/820 [04:27<00:39,  3.54it/s] 83%|████████▎ | 681/820 [04:27<00:39,  3.55it/s] 83%|████████▎ | 682/820 [04:28<00:38,  3.56it/s] 83%|████████▎ | 683/820 [04:28<00:38,  3.56it/s] 83%|████████▎ | 684/820 [04:28<00:38,  3.57it/s] 84%|████████▎ | 685/820 [04:29<00:39,  3.41it/s] 84%|████████▎ | 686/820 [04:29<00:38,  3.46it/s] 84%|████████▍ | 687/820 [04:29<00:38,  3.49it/s] 84%|████████▍ | 688/820 [04:29<00:37,  3.52it/s] 84%|████████▍ | 689/820 [04:30<00:37,  3.54it/s] 84%|████████▍ | 690/820 [04:30<00:36,  3.55it/s] 84%|████████▍ | 691/820 [04:30<00:36,  3.56it/s] 84%|████████▍ | 692/820 [04:31<00:35,  3.56it/s] 85%|████████▍ | 693/820 [04:31<00:35,  3.57it/s] 85%|████████▍ | 694/820 [04:31<00:35,  3.57it/s] 85%|████████▍ | 695/820 [04:31<00:35,  3.57it/s] 85%|████████▍ | 696/820 [04:32<00:35,  3.49it/s] 85%|████████▌ | 697/820 [04:32<00:34,  3.52it/s] 85%|████████▌ | 698/820 [04:32<00:34,  3.53it/s] 85%|████████▌ | 699/820 [04:33<00:34,  3.55it/s] 85%|████████▌ | 700/820 [04:33<00:33,  3.55it/s] 85%|████████▌ | 701/820 [04:33<00:33,  3.56it/s] 86%|████████▌ | 702/820 [04:33<00:33,  3.56it/s] 86%|████████▌ | 703/820 [04:34<00:32,  3.56it/s] 86%|████████▌ | 704/820 [04:34<00:32,  3.57it/s] 86%|████████▌ | 705/820 [04:34<00:32,  3.57it/s] 86%|████████▌ | 706/820 [04:34<00:31,  3.57it/s] 86%|████████▌ | 707/820 [04:35<00:32,  3.46it/s] 86%|████████▋ | 708/820 [04:35<00:32,  3.50it/s] 86%|████████▋ | 709/820 [04:35<00:31,  3.52it/s] 87%|████████▋ | 710/820 [04:36<00:31,  3.54it/s] 87%|████████▋ | 711/820 [04:36<00:30,  3.55it/s] 87%|████████▋ | 712/820 [04:36<00:30,  3.56it/s] 87%|████████▋ | 713/820 [04:36<00:29,  3.57it/s] 87%|████████▋ | 714/820 [04:37<00:29,  3.57it/s] 87%|████████▋ | 715/820 [04:37<00:29,  3.57it/s] 87%|████████▋ | 716/820 [04:37<00:29,  3.58it/s] 87%|████████▋ | 717/820 [04:38<00:28,  3.58it/s] 88%|████████▊ | 718/820 [04:38<00:29,  3.50it/s] 88%|████████▊ | 719/820 [04:38<00:28,  3.52it/s] 88%|████████▊ | 720/820 [04:38<00:28,  3.54it/s] 88%|████████▊ | 721/820 [04:39<00:27,  3.55it/s] 88%|████████▊ | 722/820 [04:39<00:27,  3.56it/s] 88%|████████▊ | 723/820 [04:39<00:27,  3.56it/s] 88%|████████▊ | 724/820 [04:40<00:26,  3.57it/s] 88%|████████▊ | 725/820 [04:40<00:26,  3.57it/s] 89%|████████▊ | 726/820 [04:40<00:26,  3.57it/s] 89%|████████▊ | 727/820 [04:40<00:26,  3.57it/s] 89%|████████▉ | 728/820 [04:41<00:25,  3.57it/s] 89%|████████▉ | 729/820 [04:41<00:26,  3.50it/s] 89%|████████▉ | 730/820 [04:41<00:25,  3.52it/s] 89%|████████▉ | 731/820 [04:42<00:25,  3.54it/s] 89%|████████▉ | 732/820 [04:42<00:24,  3.55it/s] 89%|████████▉ | 733/820 [04:42<00:24,  3.55it/s] 90%|████████▉ | 734/820 [04:42<00:24,  3.56it/s] 90%|████████▉ | 735/820 [04:43<00:23,  3.57it/s] 90%|████████▉ | 736/820 [04:43<00:23,  3.57it/s] 90%|████████▉ | 737/820 [04:43<00:23,  3.57it/s] 90%|█████████ | 738/820 [04:43<00:22,  3.57it/s] 90%|█████████ | 739/820 [04:44<00:23,  3.49it/s] 90%|█████████ | 740/820 [04:44<00:23,  3.40it/s] 90%|█████████ | 741/820 [04:44<00:22,  3.45it/s] 90%|█████████ | 742/820 [04:45<00:22,  3.48it/s] 91%|█████████ | 743/820 [04:45<00:21,  3.51it/s] 91%|█████████ | 744/820 [04:45<00:21,  3.53it/s] 91%|█████████ | 745/820 [04:46<00:21,  3.54it/s] 91%|█████████ | 746/820 [04:46<00:20,  3.56it/s] 91%|█████████ | 747/820 [04:46<00:20,  3.58it/s] 91%|█████████ | 748/820 [04:46<00:21,  3.41it/s] 91%|█████████▏| 749/820 [04:47<00:23,  2.96it/s] 91%|█████████▏| 750/820 [04:47<00:24,  2.91it/s] 92%|█████████▏| 751/820 [04:47<00:22,  3.09it/s] 92%|█████████▏| 752/820 [04:48<00:21,  3.23it/s] 92%|█████████▏| 753/820 [04:48<00:20,  3.34it/s] 92%|█████████▏| 754/820 [04:48<00:19,  3.42it/s] 92%|█████████▏| 755/820 [04:49<00:18,  3.48it/s] 92%|█████████▏| 756/820 [04:49<00:18,  3.52it/s] 92%|█████████▏| 757/820 [04:49<00:17,  3.56it/s] 92%|█████████▏| 758/820 [04:49<00:17,  3.58it/s] 93%|█████████▎| 759/820 [04:50<00:16,  3.59it/s] 93%|█████████▎| 760/820 [04:50<00:16,  3.60it/s] 93%|█████████▎| 761/820 [04:50<00:17,  3.46it/s] 93%|█████████▎| 762/820 [04:51<00:16,  3.51it/s] 93%|█████████▎| 763/820 [04:51<00:16,  3.54it/s] 93%|█████████▎| 764/820 [04:51<00:15,  3.57it/s] 93%|█████████▎| 765/820 [04:51<00:15,  3.59it/s] 93%|█████████▎| 766/820 [04:52<00:14,  3.60it/s] 94%|█████████▎| 767/820 [04:52<00:14,  3.61it/s] 94%|█████████▎| 768/820 [04:52<00:14,  3.62it/s] 94%|█████████▍| 769/820 [04:52<00:14,  3.62it/s] 94%|█████████▍| 770/820 [04:53<00:13,  3.62it/s] 94%|█████████▍| 771/820 [04:53<00:13,  3.62it/s] 94%|█████████▍| 772/820 [04:53<00:13,  3.63it/s] 94%|█████████▍| 773/820 [04:54<00:12,  3.63it/s] 94%|█████████▍| 774/820 [04:54<00:12,  3.63it/s] 95%|█████████▍| 775/820 [04:54<00:12,  3.63it/s] 95%|█████████▍| 776/820 [04:54<00:12,  3.63it/s] 95%|█████████▍| 777/820 [04:55<00:11,  3.63it/s] 95%|█████████▍| 778/820 [04:55<00:11,  3.63it/s] 95%|█████████▌| 779/820 [04:55<00:11,  3.63it/s] 95%|█████████▌| 780/820 [04:55<00:11,  3.63it/s] 95%|█████████▌| 781/820 [04:56<00:10,  3.63it/s] 95%|█████████▌| 782/820 [04:56<00:10,  3.55it/s] 95%|█████████▌| 783/820 [04:56<00:10,  3.57it/s] 96%|█████████▌| 784/820 [04:57<00:10,  3.59it/s] 96%|█████████▌| 785/820 [04:57<00:09,  3.60it/s] 96%|█████████▌| 786/820 [04:57<00:09,  3.61it/s] 96%|█████████▌| 787/820 [04:57<00:09,  3.62it/s] 96%|█████████▌| 788/820 [04:58<00:08,  3.62it/s] 96%|█████████▌| 789/820 [04:58<00:08,  3.62it/s] 96%|█████████▋| 790/820 [04:58<00:08,  3.62it/s] 96%|█████████▋| 791/820 [04:59<00:07,  3.63it/s] 97%|█████████▋| 792/820 [04:59<00:07,  3.63it/s] 97%|█████████▋| 793/820 [04:59<00:07,  3.52it/s] 97%|█████████▋| 794/820 [04:59<00:07,  3.54it/s] 97%|█████████▋| 795/820 [05:00<00:07,  3.57it/s] 97%|█████████▋| 796/820 [05:00<00:06,  3.59it/s] 97%|█████████▋| 797/820 [05:00<00:06,  3.60it/s] 97%|█████████▋| 798/820 [05:00<00:06,  3.61it/s] 97%|█████████▋| 799/820 [05:01<00:05,  3.61it/s] 98%|█████████▊| 800/820 [05:01<00:05,  3.62it/s] 98%|█████████▊| 801/820 [05:01<00:05,  3.62it/s] 98%|█████████▊| 802/820 [05:02<00:04,  3.63it/s] 98%|█████████▊| 803/820 [05:02<00:04,  3.63it/s] 98%|█████████▊| 804/820 [05:02<00:04,  3.51it/s] 98%|█████████▊| 805/820 [05:02<00:04,  3.54it/s] 98%|█████████▊| 806/820 [05:03<00:03,  3.56it/s] 98%|█████████▊| 807/820 [05:03<00:03,  3.58it/s] 99%|█████████▊| 808/820 [05:03<00:03,  3.60it/s] 99%|█████████▊| 809/820 [05:04<00:03,  3.61it/s] 99%|█████████▉| 810/820 [05:04<00:02,  3.61it/s] 99%|█████████▉| 811/820 [05:04<00:02,  3.62it/s] 99%|█████████▉| 812/820 [05:04<00:02,  3.62it/s] 99%|█████████▉| 813/820 [05:05<00:01,  3.62it/s] 99%|█████████▉| 814/820 [05:05<00:01,  3.62it/s] 99%|█████████▉| 815/820 [05:05<00:01,  3.47it/s]100%|█████████▉| 816/820 [05:06<00:01,  3.51it/s]100%|█████████▉| 817/820 [05:06<00:00,  3.54it/s]100%|█████████▉| 818/820 [05:06<00:00,  3.57it/s]100%|█████████▉| 819/820 [05:06<00:00,  3.59it/s]100%|██████████| 820/820 [05:07<00:00,  3.87it/s][INFO|trainer.py:2140] 2023-08-28 13:42:26,691 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:42:26,691 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 13:42:26,691 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.5098, 'eval_samples_per_second': 360.034, 'eval_steps_per_second': 45.004, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.86it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.63it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.55it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.56it/s][A
  5%|▍         | 28/608 [00:00<00:12, 45.89it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.47it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.21it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.06it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.12it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.25it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.36it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.54it/s][A
 11%|█         | 68/608 [00:01<00:12, 44.86it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 44.84it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 44.74it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 44.76it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.89it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.97it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.06it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.20it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.27it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.34it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.23it/s][A
 20%|██        | 123/608 [00:02<00:10, 44.98it/s][A
 21%|██        | 128/608 [00:02<00:10, 44.97it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 44.97it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.05it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.14it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.17it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.35it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.37it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.30it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.06it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 44.96it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.96it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.03it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.11it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.19it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.32it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.37it/s][A
 34%|███▍      | 208/608 [00:04<00:09, 42.32it/s][A
 35%|███▌      | 213/608 [00:04<00:09, 43.22it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 43.74it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 44.15it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 44.43it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 44.64it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 44.89it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.04it/s][A
 41%|████      | 248/608 [00:05<00:08, 44.90it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 44.87it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.00it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.03it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.11it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.15it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.19it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.24it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.19it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.11it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.03it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.12it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.13it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.15it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.03it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.16it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.24it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.17it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.18it/s][A
 56%|█████▋    | 343/608 [00:07<00:06, 41.90it/s][A
 57%|█████▋    | 348/608 [00:07<00:06, 42.98it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 43.71it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 44.22it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 44.52it/s][A
 61%|██████    | 368/608 [00:08<00:05, 44.84it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 44.93it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 44.97it/s][A
 63%|██████▎   | 383/608 [00:08<00:05, 44.70it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 44.76it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 44.85it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.12it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.17it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.19it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.21it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.25it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.09it/s][A
 70%|███████   | 428/608 [00:09<00:04, 44.89it/s][A
 71%|███████   | 433/608 [00:09<00:03, 44.97it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 44.99it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.15it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.16it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.30it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.27it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.22it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.08it/s][A
 78%|███████▊  | 473/608 [00:10<00:03, 44.98it/s][A
 79%|███████▊  | 478/608 [00:10<00:03, 41.69it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 42.85it/s][A
 80%|████████  | 488/608 [00:10<00:02, 43.72it/s][A
 81%|████████  | 493/608 [00:10<00:02, 44.31it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 44.62it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 44.87it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 44.88it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 44.88it/s][A
 85%|████████▌ | 518/608 [00:11<00:02, 44.59it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 44.60it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 44.84it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.17it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.31it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.36it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.32it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.21it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.09it/s][A
 93%|█████████▎| 563/608 [00:12<00:01, 44.81it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 44.91it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 44.98it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.18it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.26it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.25it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.32it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.23it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.06it/s][A
100%|██████████| 608/608 [00:13<00:00, 44.84it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.84it/s][A100%|██████████| 820/820 [05:20<00:00,  3.87it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:42:40,671 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820
[INFO|configuration_utils.py:351] 2023-08-28 13:42:41,153 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:42:45,864 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:42:46,133 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:42:46,293 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:42:48,567 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:42:48,641 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164 (score: 0.9957473874092102).
                                                 100%|██████████| 820/820 [05:41<00:00,  3.87it/s]100%|██████████| 820/820 [05:41<00:00,  2.40it/s]
[INFO|trainer.py:1894] 2023-08-28 13:43:01,342 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 13:43:01,526 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:43:05,031 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:43:05,158 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:43:05,220 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:43:05,737 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:05,738 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:05,738 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:05,738 >>   train_runtime            = 0:05:41.67
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:05,738 >>   train_samples            =      10479
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:05,738 >>   train_samples_per_second =    153.346
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:05,738 >>   train_steps_per_second   =        2.4
{'eval_loss': 0.9957473874092102, 'eval_runtime': 13.589, 'eval_samples_per_second': 357.935, 'eval_steps_per_second': 44.742, 'epoch': 5.0}
{'train_runtime': 341.679, 'train_samples_per_second': 153.346, 'train_steps_per_second': 2.4, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 13:43:06 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:43:06,294 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:43:06,294 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 13:43:06,294 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 56.31it/s]  2%|▏         | 12/608 [00:00<00:15, 38.47it/s]  3%|▎         | 17/608 [00:00<00:14, 41.44it/s]  4%|▎         | 22/608 [00:00<00:24, 24.26it/s]  4%|▍         | 27/608 [00:00<00:19, 29.14it/s]  5%|▌         | 31/608 [00:00<00:19, 29.59it/s]  6%|▌         | 36/608 [00:01<00:16, 34.17it/s]  7%|▋         | 41/608 [00:01<00:15, 37.33it/s]  8%|▊         | 46/608 [00:01<00:14, 39.66it/s]  8%|▊         | 51/608 [00:01<00:13, 41.52it/s]  9%|▉         | 56/608 [00:01<00:12, 42.82it/s] 10%|█         | 61/608 [00:01<00:12, 43.58it/s] 11%|█         | 66/608 [00:01<00:12, 43.93it/s] 12%|█▏        | 71/608 [00:01<00:12, 43.70it/s] 12%|█▎        | 76/608 [00:01<00:12, 43.96it/s] 13%|█▎        | 81/608 [00:02<00:11, 44.41it/s] 14%|█▍        | 86/608 [00:02<00:11, 44.87it/s] 15%|█▍        | 91/608 [00:02<00:11, 45.22it/s] 16%|█▌        | 96/608 [00:02<00:11, 45.47it/s] 17%|█▋        | 101/608 [00:02<00:11, 45.65it/s] 17%|█▋        | 106/608 [00:02<00:11, 45.58it/s] 18%|█▊        | 111/608 [00:02<00:10, 45.48it/s] 19%|█▉        | 116/608 [00:02<00:10, 45.24it/s] 20%|█▉        | 121/608 [00:02<00:10, 45.17it/s] 21%|██        | 126/608 [00:03<00:10, 45.12it/s] 22%|██▏       | 131/608 [00:03<00:10, 45.27it/s] 22%|██▏       | 136/608 [00:03<00:10, 45.37it/s] 23%|██▎       | 141/608 [00:03<00:10, 45.44it/s] 24%|██▍       | 146/608 [00:03<00:10, 45.50it/s] 25%|██▍       | 151/608 [00:03<00:10, 45.60it/s] 26%|██▌       | 156/608 [00:03<00:09, 45.66it/s] 26%|██▋       | 161/608 [00:03<00:09, 45.52it/s] 27%|██▋       | 166/608 [00:03<00:09, 45.46it/s] 28%|██▊       | 171/608 [00:04<00:09, 45.42it/s] 29%|██▉       | 176/608 [00:04<00:09, 45.35it/s] 30%|██▉       | 181/608 [00:04<00:09, 45.44it/s] 31%|███       | 186/608 [00:04<00:09, 45.47it/s] 31%|███▏      | 191/608 [00:04<00:09, 45.58it/s] 32%|███▏      | 196/608 [00:04<00:09, 45.58it/s] 33%|███▎      | 201/608 [00:04<00:08, 45.62it/s] 34%|███▍      | 206/608 [00:04<00:08, 45.49it/s] 35%|███▍      | 211/608 [00:04<00:09, 41.37it/s] 36%|███▌      | 216/608 [00:05<00:09, 42.67it/s] 36%|███▋      | 221/608 [00:05<00:08, 43.65it/s] 37%|███▋      | 226/608 [00:05<00:08, 44.17it/s] 38%|███▊      | 231/608 [00:05<00:08, 44.65it/s] 39%|███▉      | 236/608 [00:05<00:08, 44.96it/s] 40%|███▉      | 241/608 [00:05<00:08, 45.07it/s] 40%|████      | 246/608 [00:05<00:08, 45.20it/s] 41%|████▏     | 251/608 [00:05<00:07, 44.95it/s] 42%|████▏     | 256/608 [00:05<00:07, 44.92it/s] 43%|████▎     | 261/608 [00:06<00:07, 45.21it/s] 44%|████▍     | 266/608 [00:06<00:07, 45.36it/s] 45%|████▍     | 271/608 [00:06<00:07, 45.51it/s] 45%|████▌     | 276/608 [00:06<00:07, 45.54it/s] 46%|████▌     | 281/608 [00:06<00:07, 45.58it/s] 47%|████▋     | 286/608 [00:06<00:07, 45.51it/s] 48%|████▊     | 291/608 [00:06<00:06, 45.44it/s] 49%|████▊     | 296/608 [00:06<00:06, 45.23it/s] 50%|████▉     | 301/608 [00:06<00:06, 45.17it/s] 50%|█████     | 306/608 [00:07<00:06, 45.22it/s] 51%|█████     | 311/608 [00:07<00:06, 45.33it/s] 52%|█████▏    | 316/608 [00:07<00:06, 45.53it/s] 53%|█████▎    | 321/608 [00:07<00:06, 45.58it/s] 54%|█████▎    | 326/608 [00:07<00:06, 45.60it/s] 54%|█████▍    | 331/608 [00:07<00:06, 45.53it/s] 55%|█████▌    | 336/608 [00:07<00:05, 45.38it/s] 56%|█████▌    | 341/608 [00:07<00:05, 45.28it/s] 57%|█████▋    | 346/608 [00:07<00:06, 40.72it/s] 58%|█████▊    | 351/608 [00:08<00:06, 41.21it/s] 59%|█████▊    | 356/608 [00:08<00:05, 42.44it/s] 59%|█████▉    | 361/608 [00:08<00:05, 43.41it/s] 60%|██████    | 366/608 [00:08<00:05, 44.03it/s] 61%|██████    | 371/608 [00:08<00:05, 44.53it/s] 62%|██████▏   | 376/608 [00:08<00:05, 44.97it/s] 63%|██████▎   | 381/608 [00:08<00:05, 45.14it/s] 63%|██████▎   | 386/608 [00:08<00:04, 44.90it/s] 64%|██████▍   | 391/608 [00:08<00:04, 44.94it/s] 65%|██████▌   | 396/608 [00:09<00:04, 45.12it/s] 66%|██████▌   | 401/608 [00:09<00:04, 45.16it/s] 67%|██████▋   | 406/608 [00:09<00:04, 45.44it/s] 68%|██████▊   | 411/608 [00:09<00:04, 45.49it/s] 68%|██████▊   | 416/608 [00:09<00:04, 45.58it/s] 69%|██████▉   | 421/608 [00:09<00:04, 45.53it/s] 70%|███████   | 426/608 [00:09<00:04, 45.50it/s] 71%|███████   | 431/608 [00:09<00:03, 45.31it/s] 72%|███████▏  | 436/608 [00:09<00:03, 45.24it/s] 73%|███████▎  | 441/608 [00:10<00:03, 45.12it/s] 73%|███████▎  | 446/608 [00:10<00:03, 45.17it/s] 74%|███████▍  | 451/608 [00:10<00:03, 45.21it/s] 75%|███████▌  | 456/608 [00:10<00:03, 45.39it/s] 76%|███████▌  | 461/608 [00:10<00:03, 45.54it/s] 77%|███████▋  | 466/608 [00:10<00:03, 45.62it/s] 77%|███████▋  | 471/608 [00:10<00:03, 45.54it/s] 78%|███████▊  | 476/608 [00:10<00:02, 45.45it/s] 79%|███████▉  | 481/608 [00:10<00:02, 45.31it/s] 80%|███████▉  | 486/608 [00:11<00:02, 44.15it/s] 81%|████████  | 491/608 [00:11<00:02, 44.54it/s] 82%|████████▏ | 496/608 [00:11<00:02, 44.73it/s] 82%|████████▏ | 501/608 [00:11<00:02, 45.13it/s] 83%|████████▎ | 506/608 [00:11<00:02, 45.30it/s] 84%|████████▍ | 511/608 [00:11<00:02, 45.47it/s] 85%|████████▍ | 516/608 [00:11<00:02, 45.36it/s] 86%|████████▌ | 521/608 [00:11<00:01, 45.34it/s] 87%|████████▋ | 526/608 [00:11<00:01, 45.12it/s] 87%|████████▋ | 531/608 [00:12<00:01, 45.12it/s] 88%|████████▊ | 536/608 [00:12<00:01, 45.25it/s] 89%|████████▉ | 541/608 [00:12<00:01, 45.32it/s] 90%|████████▉ | 546/608 [00:12<00:01, 45.46it/s] 91%|█████████ | 551/608 [00:12<00:01, 45.54it/s] 91%|█████████▏| 556/608 [00:12<00:01, 45.52it/s] 92%|█████████▏| 561/608 [00:12<00:01, 45.45it/s] 93%|█████████▎| 566/608 [00:12<00:00, 45.39it/s] 94%|█████████▍| 571/608 [00:12<00:00, 45.21it/s] 95%|█████████▍| 576/608 [00:13<00:00, 45.20it/s] 96%|█████████▌| 581/608 [00:13<00:00, 45.19it/s] 96%|█████████▋| 586/608 [00:13<00:00, 45.35it/s] 97%|█████████▋| 591/608 [00:13<00:00, 45.48it/s] 98%|█████████▊| 596/608 [00:13<00:00, 45.56it/s] 99%|█████████▉| 601/608 [00:13<00:00, 45.53it/s]100%|█████████▉| 606/608 [00:13<00:00, 45.47it/s]100%|██████████| 608/608 [00:13<00:00, 44.09it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:43:20,101 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:20,102 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:20,102 >>   eval_loss               =     0.9957
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:20,102 >>   eval_runtime            = 0:00:13.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:20,102 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:20,102 >>   eval_samples_per_second =    352.276
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:20,102 >>   eval_steps_per_second   =     44.034
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:43:20,102 >>   perplexity              =     2.7067
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:29,393 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:29,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:29,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:29,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:29,408 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:43:30,057 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:43:30,058 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:43:30,651 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:43:31,702 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:43:31,702 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:34,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:34,634 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:34,634 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:34,634 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:34,634 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:43:35,362 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:43:35,363 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:43:35,955 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:43:36,156 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:43:36,156 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-656
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-328
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-820
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-164
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/checkpoint-492
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.86it/s]Extractor Predicting: 5it [00:02,  1.90it/s]Extractor Predicting: 6it [00:03,  1.95it/s]Extractor Predicting: 7it [00:03,  1.90it/s]Extractor Predicting: 8it [00:04,  1.85it/s]Extractor Predicting: 9it [00:04,  1.86it/s]Extractor Predicting: 10it [00:05,  1.94it/s]Extractor Predicting: 11it [00:05,  1.93it/s]Extractor Predicting: 12it [00:06,  1.98it/s]Extractor Predicting: 13it [00:06,  1.98it/s]Extractor Predicting: 14it [00:07,  1.92it/s]Extractor Predicting: 15it [00:07,  1.98it/s]Extractor Predicting: 16it [00:08,  1.96it/s]Extractor Predicting: 17it [00:08,  1.94it/s]Extractor Predicting: 18it [00:09,  1.93it/s]Extractor Predicting: 19it [00:10,  1.85it/s]Extractor Predicting: 20it [00:10,  1.92it/s]Extractor Predicting: 21it [00:11,  1.93it/s]Extractor Predicting: 22it [00:11,  1.91it/s]Extractor Predicting: 23it [00:12,  1.71it/s]Extractor Predicting: 24it [00:12,  1.68it/s]Extractor Predicting: 25it [00:13,  1.65it/s]Extractor Predicting: 26it [00:14,  1.66it/s]Extractor Predicting: 27it [00:14,  1.70it/s]Extractor Predicting: 28it [00:15,  1.71it/s]Extractor Predicting: 29it [00:15,  1.75it/s]Extractor Predicting: 30it [00:16,  1.78it/s]Extractor Predicting: 31it [00:16,  1.80it/s]Extractor Predicting: 32it [00:17,  1.83it/s]Extractor Predicting: 33it [00:17,  1.83it/s]Extractor Predicting: 34it [00:18,  1.83it/s]Extractor Predicting: 35it [00:19,  1.80it/s]Extractor Predicting: 36it [00:19,  1.80it/s]Extractor Predicting: 37it [00:20,  1.78it/s]Extractor Predicting: 38it [00:20,  1.78it/s]Extractor Predicting: 39it [00:21,  1.78it/s]Extractor Predicting: 40it [00:21,  1.78it/s]Extractor Predicting: 41it [00:22,  1.80it/s]Extractor Predicting: 42it [00:23,  1.78it/s]Extractor Predicting: 43it [00:23,  1.74it/s]Extractor Predicting: 44it [00:24,  1.80it/s]Extractor Predicting: 45it [00:24,  1.78it/s]Extractor Predicting: 46it [00:25,  1.79it/s]Extractor Predicting: 47it [00:25,  1.80it/s]Extractor Predicting: 48it [00:26,  1.80it/s]Extractor Predicting: 49it [00:26,  1.77it/s]Extractor Predicting: 50it [00:27,  1.78it/s]Extractor Predicting: 51it [00:28,  1.77it/s]Extractor Predicting: 52it [00:28,  1.77it/s]Extractor Predicting: 53it [00:29,  1.76it/s]Extractor Predicting: 54it [00:29,  1.83it/s]Extractor Predicting: 55it [00:30,  1.85it/s]Extractor Predicting: 56it [00:30,  1.83it/s]Extractor Predicting: 57it [00:31,  1.81it/s]Extractor Predicting: 58it [00:31,  1.79it/s]Extractor Predicting: 59it [00:32,  1.81it/s]Extractor Predicting: 60it [00:33,  1.79it/s]Extractor Predicting: 61it [00:33,  1.72it/s]Extractor Predicting: 62it [00:34,  1.74it/s]Extractor Predicting: 63it [00:34,  1.76it/s]Extractor Predicting: 64it [00:35,  1.78it/s]Extractor Predicting: 65it [00:35,  1.77it/s]Extractor Predicting: 66it [00:36,  1.79it/s]Extractor Predicting: 67it [00:37,  1.80it/s]Extractor Predicting: 68it [00:37,  1.75it/s]Extractor Predicting: 69it [00:38,  1.76it/s]Extractor Predicting: 70it [00:38,  1.80it/s]Extractor Predicting: 71it [00:39,  1.84it/s]Extractor Predicting: 72it [00:39,  1.83it/s]Extractor Predicting: 73it [00:40,  1.85it/s]Extractor Predicting: 74it [00:40,  1.82it/s]Extractor Predicting: 75it [00:41,  1.83it/s]Extractor Predicting: 76it [00:41,  1.82it/s]Extractor Predicting: 77it [00:42,  1.81it/s]Extractor Predicting: 78it [00:43,  1.85it/s]Extractor Predicting: 79it [00:43,  1.84it/s]Extractor Predicting: 80it [00:44,  1.84it/s]Extractor Predicting: 81it [00:44,  1.81it/s]Extractor Predicting: 82it [00:45,  1.80it/s]Extractor Predicting: 83it [00:45,  1.79it/s]Extractor Predicting: 84it [00:46,  1.80it/s]Extractor Predicting: 85it [00:46,  1.80it/s]Extractor Predicting: 86it [00:47,  1.76it/s]Extractor Predicting: 87it [00:48,  1.80it/s]Extractor Predicting: 88it [00:48,  1.79it/s]Extractor Predicting: 89it [00:49,  1.71it/s]Extractor Predicting: 90it [00:49,  1.76it/s]Extractor Predicting: 91it [00:50,  1.80it/s]Extractor Predicting: 92it [00:50,  1.79it/s]Extractor Predicting: 93it [00:51,  1.82it/s]Extractor Predicting: 94it [00:51,  1.80it/s]Extractor Predicting: 95it [00:52,  1.74it/s]Extractor Predicting: 96it [00:53,  1.59it/s]Extractor Predicting: 97it [00:53,  1.62it/s]Extractor Predicting: 98it [00:54,  1.68it/s]Extractor Predicting: 99it [00:55,  1.73it/s]Extractor Predicting: 100it [00:55,  1.74it/s]Extractor Predicting: 101it [00:56,  1.75it/s]Extractor Predicting: 102it [00:56,  1.78it/s]Extractor Predicting: 103it [00:57,  1.78it/s]Extractor Predicting: 104it [00:57,  1.78it/s]Extractor Predicting: 105it [00:58,  1.76it/s]Extractor Predicting: 106it [00:58,  1.76it/s]Extractor Predicting: 107it [00:59,  1.77it/s]Extractor Predicting: 108it [01:00,  1.82it/s]Extractor Predicting: 109it [01:00,  1.75it/s]Extractor Predicting: 110it [01:01,  1.71it/s]Extractor Predicting: 111it [01:01,  1.75it/s]Extractor Predicting: 112it [01:02,  1.75it/s]Extractor Predicting: 113it [01:02,  1.78it/s]Extractor Predicting: 114it [01:03,  1.83it/s]Extractor Predicting: 115it [01:03,  1.84it/s]Extractor Predicting: 116it [01:04,  1.85it/s]Extractor Predicting: 117it [01:05,  1.85it/s]Extractor Predicting: 118it [01:05,  1.88it/s]Extractor Predicting: 119it [01:06,  1.85it/s]Extractor Predicting: 120it [01:06,  1.88it/s]Extractor Predicting: 121it [01:07,  1.80it/s]Extractor Predicting: 122it [01:07,  1.75it/s]Extractor Predicting: 123it [01:08,  1.72it/s]Extractor Predicting: 124it [01:09,  1.68it/s]Extractor Predicting: 125it [01:09,  1.72it/s]Extractor Predicting: 126it [01:10,  1.76it/s]Extractor Predicting: 127it [01:10,  1.75it/s]Extractor Predicting: 128it [01:11,  1.63it/s]Extractor Predicting: 129it [01:12,  1.68it/s]Extractor Predicting: 130it [01:12,  1.73it/s]Extractor Predicting: 131it [01:13,  1.79it/s]Extractor Predicting: 132it [01:13,  1.74it/s]Extractor Predicting: 133it [01:14,  1.73it/s]Extractor Predicting: 134it [01:14,  1.69it/s]Extractor Predicting: 135it [01:15,  1.75it/s]Extractor Predicting: 136it [01:15,  1.76it/s]Extractor Predicting: 137it [01:16,  1.78it/s]Extractor Predicting: 138it [01:17,  1.77it/s]Extractor Predicting: 139it [01:17,  1.77it/s]Extractor Predicting: 140it [01:18,  1.74it/s]Extractor Predicting: 141it [01:18,  1.80it/s]Extractor Predicting: 142it [01:19,  1.77it/s]Extractor Predicting: 143it [01:19,  1.81it/s]Extractor Predicting: 144it [01:20,  1.82it/s]Extractor Predicting: 145it [01:20,  1.84it/s]Extractor Predicting: 146it [01:21,  1.74it/s]Extractor Predicting: 147it [01:22,  1.73it/s]Extractor Predicting: 148it [01:22,  1.74it/s]Extractor Predicting: 149it [01:23,  1.74it/s]Extractor Predicting: 150it [01:23,  1.77it/s]Extractor Predicting: 151it [01:24,  1.75it/s]Extractor Predicting: 152it [01:25,  1.78it/s]Extractor Predicting: 153it [01:25,  1.76it/s]Extractor Predicting: 154it [01:26,  1.77it/s]Extractor Predicting: 155it [01:26,  1.80it/s]Extractor Predicting: 156it [01:27,  1.83it/s]Extractor Predicting: 157it [01:27,  1.84it/s]Extractor Predicting: 158it [01:28,  1.83it/s]Extractor Predicting: 159it [01:28,  1.81it/s]Extractor Predicting: 160it [01:29,  1.82it/s]Extractor Predicting: 161it [01:29,  1.79it/s]Extractor Predicting: 162it [01:30,  1.77it/s]Extractor Predicting: 163it [01:31,  1.79it/s]Extractor Predicting: 164it [01:31,  1.75it/s]Extractor Predicting: 165it [01:32,  1.79it/s]Extractor Predicting: 166it [01:32,  1.75it/s]Extractor Predicting: 167it [01:33,  1.74it/s]Extractor Predicting: 168it [01:34,  1.71it/s]Extractor Predicting: 169it [01:34,  1.68it/s]Extractor Predicting: 170it [01:35,  1.67it/s]Extractor Predicting: 171it [01:35,  1.67it/s]Extractor Predicting: 172it [01:36,  1.70it/s]Extractor Predicting: 173it [01:37,  1.68it/s]Extractor Predicting: 174it [01:37,  1.64it/s]Extractor Predicting: 175it [01:38,  1.67it/s]Extractor Predicting: 176it [01:38,  1.67it/s]Extractor Predicting: 177it [01:39,  1.65it/s]Extractor Predicting: 178it [01:40,  1.62it/s]Extractor Predicting: 179it [01:40,  1.61it/s]Extractor Predicting: 180it [01:41,  1.61it/s]Extractor Predicting: 181it [01:41,  1.63it/s]Extractor Predicting: 182it [01:42,  1.62it/s]Extractor Predicting: 183it [01:43,  1.48it/s]Extractor Predicting: 184it [01:44,  1.52it/s]Extractor Predicting: 185it [01:44,  1.59it/s]Extractor Predicting: 186it [01:45,  1.58it/s]Extractor Predicting: 187it [01:45,  1.72it/s]Extractor Predicting: 187it [01:45,  1.77it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:36,414 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:36,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:36,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:36,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:36,443 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:45:37,259 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:45:37,260 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:45:37,933 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:45:39,047 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:45:39,047 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:42,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:42,191 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:42,192 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:42,192 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:45:42,192 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:45:43,015 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:45:43,016 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:45:43,612 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:45:43,835 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:45:43,835 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.76it/s]Extractor Predicting: 5it [00:02,  1.80it/s]Extractor Predicting: 6it [00:03,  1.79it/s]Extractor Predicting: 7it [00:04,  1.72it/s]Extractor Predicting: 8it [00:04,  1.73it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:05,  1.73it/s]Extractor Predicting: 11it [00:06,  1.77it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.76it/s]Extractor Predicting: 14it [00:07,  1.78it/s]Extractor Predicting: 15it [00:08,  1.81it/s]Extractor Predicting: 16it [00:09,  1.81it/s]Extractor Predicting: 17it [00:09,  1.80it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:10,  1.74it/s]Extractor Predicting: 20it [00:11,  1.74it/s]Extractor Predicting: 21it [00:11,  1.74it/s]Extractor Predicting: 22it [00:12,  1.72it/s]Extractor Predicting: 23it [00:13,  1.75it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:14,  1.74it/s]Extractor Predicting: 26it [00:14,  1.76it/s]Extractor Predicting: 27it [00:15,  1.76it/s]Extractor Predicting: 28it [00:15,  1.76it/s]Extractor Predicting: 29it [00:16,  1.78it/s]Extractor Predicting: 30it [00:17,  1.80it/s]Extractor Predicting: 31it [00:17,  1.74it/s]Extractor Predicting: 32it [00:18,  1.72it/s]Extractor Predicting: 33it [00:18,  1.78it/s]Extractor Predicting: 34it [00:19,  1.81it/s]Extractor Predicting: 35it [00:19,  1.78it/s]Extractor Predicting: 36it [00:20,  1.81it/s]Extractor Predicting: 37it [00:20,  1.77it/s]Extractor Predicting: 38it [00:21,  1.75it/s]Extractor Predicting: 39it [00:22,  1.79it/s]Extractor Predicting: 40it [00:22,  1.79it/s]Extractor Predicting: 41it [00:23,  1.82it/s]Extractor Predicting: 42it [00:23,  1.80it/s]Extractor Predicting: 43it [00:24,  1.80it/s]Extractor Predicting: 44it [00:24,  1.78it/s]Extractor Predicting: 45it [00:25,  1.82it/s]Extractor Predicting: 46it [00:25,  1.80it/s]Extractor Predicting: 47it [00:26,  1.55it/s]Extractor Predicting: 48it [00:27,  1.58it/s]Extractor Predicting: 49it [00:28,  1.59it/s]Extractor Predicting: 50it [00:28,  1.65it/s]Extractor Predicting: 51it [00:29,  1.67it/s]Extractor Predicting: 52it [00:29,  1.69it/s]Extractor Predicting: 53it [00:30,  1.77it/s]Extractor Predicting: 54it [00:30,  1.74it/s]Extractor Predicting: 55it [00:31,  1.77it/s]Extractor Predicting: 56it [00:31,  1.78it/s]Extractor Predicting: 57it [00:32,  1.74it/s]Extractor Predicting: 58it [00:33,  1.78it/s]Extractor Predicting: 59it [00:33,  1.78it/s]Extractor Predicting: 60it [00:34,  1.78it/s]Extractor Predicting: 61it [00:34,  1.75it/s]Extractor Predicting: 62it [00:35,  1.76it/s]Extractor Predicting: 63it [00:35,  1.77it/s]Extractor Predicting: 64it [00:36,  1.76it/s]Extractor Predicting: 65it [00:37,  1.77it/s]Extractor Predicting: 66it [00:37,  1.77it/s]Extractor Predicting: 67it [00:38,  1.79it/s]Extractor Predicting: 68it [00:38,  1.67it/s]Extractor Predicting: 69it [00:39,  1.72it/s]Extractor Predicting: 70it [00:40,  1.68it/s]Extractor Predicting: 71it [00:40,  1.72it/s]Extractor Predicting: 72it [00:41,  1.73it/s]Extractor Predicting: 73it [00:41,  1.77it/s]Extractor Predicting: 74it [00:42,  1.78it/s]Extractor Predicting: 75it [00:42,  1.80it/s]Extractor Predicting: 76it [00:43,  1.75it/s]Extractor Predicting: 77it [00:43,  1.79it/s]Extractor Predicting: 78it [00:44,  1.83it/s]Extractor Predicting: 79it [00:45,  1.74it/s]Extractor Predicting: 80it [00:45,  1.78it/s]Extractor Predicting: 81it [00:46,  1.80it/s]Extractor Predicting: 82it [00:46,  1.81it/s]Extractor Predicting: 83it [00:47,  1.81it/s]Extractor Predicting: 84it [00:47,  1.76it/s]Extractor Predicting: 85it [00:48,  1.77it/s]Extractor Predicting: 86it [00:48,  1.80it/s]Extractor Predicting: 87it [00:49,  1.85it/s]Extractor Predicting: 88it [00:49,  1.89it/s]Extractor Predicting: 89it [00:50,  1.87it/s]Extractor Predicting: 90it [00:51,  1.82it/s]Extractor Predicting: 91it [00:51,  1.85it/s]Extractor Predicting: 92it [00:52,  1.89it/s]Extractor Predicting: 93it [00:52,  1.86it/s]Extractor Predicting: 94it [00:53,  1.90it/s]Extractor Predicting: 95it [00:53,  1.90it/s]Extractor Predicting: 96it [00:54,  1.92it/s]Extractor Predicting: 97it [00:54,  1.87it/s]Extractor Predicting: 98it [00:55,  1.86it/s]Extractor Predicting: 99it [00:55,  1.84it/s]Extractor Predicting: 100it [00:56,  1.87it/s]Extractor Predicting: 101it [00:56,  1.87it/s]Extractor Predicting: 102it [00:57,  1.88it/s]Extractor Predicting: 103it [00:58,  1.78it/s]Extractor Predicting: 104it [00:58,  1.81it/s]Extractor Predicting: 105it [00:59,  1.78it/s]Extractor Predicting: 106it [00:59,  1.79it/s]Extractor Predicting: 107it [01:00,  1.81it/s]Extractor Predicting: 108it [01:00,  1.82it/s]Extractor Predicting: 109it [01:01,  1.73it/s]Extractor Predicting: 110it [01:02,  1.75it/s]Extractor Predicting: 111it [01:02,  1.74it/s]Extractor Predicting: 112it [01:03,  1.79it/s]Extractor Predicting: 113it [01:03,  1.80it/s]Extractor Predicting: 114it [01:04,  1.82it/s]Extractor Predicting: 115it [01:04,  1.75it/s]Extractor Predicting: 116it [01:05,  1.77it/s]Extractor Predicting: 117it [01:05,  1.79it/s]Extractor Predicting: 118it [01:06,  1.79it/s]Extractor Predicting: 119it [01:07,  1.81it/s]Extractor Predicting: 120it [01:07,  1.79it/s]Extractor Predicting: 121it [01:08,  1.76it/s]Extractor Predicting: 122it [01:08,  1.79it/s]Extractor Predicting: 123it [01:09,  1.81it/s]Extractor Predicting: 124it [01:09,  1.83it/s]Extractor Predicting: 125it [01:10,  1.82it/s]Extractor Predicting: 126it [01:10,  1.81it/s]Extractor Predicting: 127it [01:11,  1.83it/s]Extractor Predicting: 128it [01:12,  1.77it/s]Extractor Predicting: 129it [01:12,  1.74it/s]Extractor Predicting: 130it [01:13,  1.80it/s]Extractor Predicting: 131it [01:13,  1.81it/s]Extractor Predicting: 132it [01:14,  1.79it/s]Extractor Predicting: 133it [01:14,  1.82it/s]Extractor Predicting: 134it [01:15,  1.76it/s]Extractor Predicting: 135it [01:15,  1.79it/s]Extractor Predicting: 136it [01:16,  1.80it/s]Extractor Predicting: 137it [01:17,  1.78it/s]Extractor Predicting: 138it [01:17,  1.77it/s]Extractor Predicting: 139it [01:18,  1.80it/s]Extractor Predicting: 140it [01:18,  1.75it/s]Extractor Predicting: 141it [01:19,  1.75it/s]Extractor Predicting: 142it [01:19,  1.75it/s]Extractor Predicting: 143it [01:20,  1.77it/s]Extractor Predicting: 144it [01:21,  1.78it/s]Extractor Predicting: 145it [01:21,  1.76it/s]Extractor Predicting: 146it [01:22,  1.73it/s]Extractor Predicting: 147it [01:22,  1.73it/s]Extractor Predicting: 148it [01:23,  1.77it/s]Extractor Predicting: 149it [01:23,  1.76it/s]Extractor Predicting: 150it [01:24,  1.72it/s]Extractor Predicting: 151it [01:25,  1.72it/s]Extractor Predicting: 152it [01:25,  1.71it/s]Extractor Predicting: 153it [01:26,  1.78it/s]Extractor Predicting: 154it [01:26,  1.80it/s]Extractor Predicting: 155it [01:27,  1.83it/s]Extractor Predicting: 156it [01:27,  1.79it/s]Extractor Predicting: 157it [01:28,  1.81it/s]Extractor Predicting: 158it [01:28,  1.78it/s]Extractor Predicting: 159it [01:29,  1.77it/s]Extractor Predicting: 160it [01:30,  1.78it/s]Extractor Predicting: 161it [01:30,  1.78it/s]Extractor Predicting: 162it [01:31,  1.77it/s]Extractor Predicting: 163it [01:31,  1.79it/s]Extractor Predicting: 164it [01:32,  1.75it/s]Extractor Predicting: 165it [01:32,  1.78it/s]Extractor Predicting: 166it [01:33,  1.79it/s]Extractor Predicting: 167it [01:34,  1.80it/s]Extractor Predicting: 168it [01:34,  1.78it/s]Extractor Predicting: 169it [01:35,  1.77it/s]Extractor Predicting: 170it [01:35,  1.77it/s]Extractor Predicting: 171it [01:36,  1.84it/s]Extractor Predicting: 172it [01:36,  1.84it/s]Extractor Predicting: 173it [01:37,  1.82it/s]Extractor Predicting: 174it [01:37,  1.79it/s]Extractor Predicting: 175it [01:38,  1.72it/s]Extractor Predicting: 176it [01:39,  1.69it/s]Extractor Predicting: 177it [01:39,  1.70it/s]Extractor Predicting: 178it [01:40,  1.70it/s]Extractor Predicting: 179it [01:40,  1.72it/s]Extractor Predicting: 180it [01:41,  1.70it/s]Extractor Predicting: 181it [01:42,  1.71it/s]Extractor Predicting: 182it [01:42,  1.71it/s]Extractor Predicting: 183it [01:43,  1.46it/s]Extractor Predicting: 184it [01:44,  1.52it/s]Extractor Predicting: 185it [01:44,  1.58it/s]Extractor Predicting: 186it [01:45,  1.63it/s]Extractor Predicting: 187it [01:45,  1.63it/s]Extractor Predicting: 188it [01:46,  1.65it/s]Extractor Predicting: 189it [01:47,  1.68it/s]Extractor Predicting: 190it [01:47,  1.68it/s]Extractor Predicting: 191it [01:48,  1.71it/s]Extractor Predicting: 192it [01:48,  1.71it/s]Extractor Predicting: 193it [01:49,  1.71it/s]Extractor Predicting: 194it [01:50,  1.70it/s]Extractor Predicting: 195it [01:50,  1.76it/s]Extractor Predicting: 196it [01:51,  1.80it/s]Extractor Predicting: 197it [01:51,  1.79it/s]Extractor Predicting: 198it [01:52,  1.79it/s]Extractor Predicting: 199it [01:52,  1.79it/s]Extractor Predicting: 200it [01:53,  1.76it/s]Extractor Predicting: 201it [01:53,  1.76it/s]Extractor Predicting: 202it [01:54,  1.76it/s]Extractor Predicting: 203it [01:55,  1.77it/s]Extractor Predicting: 204it [01:55,  1.76it/s]Extractor Predicting: 205it [01:56,  1.77it/s]Extractor Predicting: 206it [01:56,  1.76it/s]Extractor Predicting: 207it [01:57,  1.76it/s]Extractor Predicting: 208it [01:57,  1.81it/s]Extractor Predicting: 209it [01:58,  1.83it/s]Extractor Predicting: 210it [01:58,  1.80it/s]Extractor Predicting: 211it [01:59,  1.82it/s]Extractor Predicting: 212it [02:00,  1.77it/s]Extractor Predicting: 213it [02:00,  1.75it/s]Extractor Predicting: 214it [02:01,  1.73it/s]Extractor Predicting: 215it [02:01,  1.76it/s]Extractor Predicting: 216it [02:02,  1.76it/s]Extractor Predicting: 217it [02:02,  1.82it/s]Extractor Predicting: 218it [02:03,  1.72it/s]Extractor Predicting: 219it [02:04,  1.72it/s]Extractor Predicting: 220it [02:04,  1.79it/s]Extractor Predicting: 221it [02:05,  1.75it/s]Extractor Predicting: 222it [02:05,  1.75it/s]Extractor Predicting: 223it [02:06,  1.76it/s]Extractor Predicting: 224it [02:06,  1.74it/s]Extractor Predicting: 225it [02:07,  1.80it/s]Extractor Predicting: 226it [02:08,  1.81it/s]Extractor Predicting: 227it [02:08,  1.82it/s]Extractor Predicting: 228it [02:09,  1.83it/s]Extractor Predicting: 229it [02:09,  1.82it/s]Extractor Predicting: 230it [02:10,  1.79it/s]Extractor Predicting: 231it [02:10,  1.80it/s]Extractor Predicting: 232it [02:11,  1.82it/s]Extractor Predicting: 233it [02:11,  1.82it/s]Extractor Predicting: 234it [02:12,  1.78it/s]Extractor Predicting: 235it [02:13,  1.78it/s]Extractor Predicting: 236it [02:13,  1.75it/s]Extractor Predicting: 237it [02:14,  1.74it/s]Extractor Predicting: 238it [02:14,  1.77it/s]Extractor Predicting: 239it [02:15,  1.77it/s]Extractor Predicting: 240it [02:15,  1.80it/s]Extractor Predicting: 241it [02:16,  1.84it/s]Extractor Predicting: 242it [02:16,  1.78it/s]Extractor Predicting: 243it [02:17,  1.83it/s]Extractor Predicting: 244it [02:17,  1.84it/s]Extractor Predicting: 245it [02:18,  1.81it/s]Extractor Predicting: 246it [02:19,  1.84it/s]Extractor Predicting: 247it [02:19,  1.82it/s]Extractor Predicting: 248it [02:20,  1.83it/s]Extractor Predicting: 249it [02:20,  1.82it/s]Extractor Predicting: 250it [02:21,  1.89it/s]Extractor Predicting: 251it [02:21,  1.85it/s]Extractor Predicting: 252it [02:22,  1.87it/s]Extractor Predicting: 253it [02:22,  1.93it/s]Extractor Predicting: 254it [02:23,  1.83it/s]Extractor Predicting: 255it [02:23,  1.85it/s]Extractor Predicting: 256it [02:24,  1.84it/s]Extractor Predicting: 257it [02:25,  1.83it/s]Extractor Predicting: 258it [02:25,  1.85it/s]Extractor Predicting: 259it [02:26,  1.85it/s]Extractor Predicting: 260it [02:26,  1.81it/s]Extractor Predicting: 261it [02:27,  1.79it/s]Extractor Predicting: 262it [02:27,  1.81it/s]Extractor Predicting: 263it [02:28,  1.74it/s]Extractor Predicting: 264it [02:28,  1.79it/s]Extractor Predicting: 265it [02:29,  1.79it/s]Extractor Predicting: 266it [02:30,  1.73it/s]Extractor Predicting: 267it [02:30,  1.75it/s]Extractor Predicting: 268it [02:31,  1.74it/s]Extractor Predicting: 269it [02:31,  1.72it/s]Extractor Predicting: 270it [02:32,  1.74it/s]Extractor Predicting: 271it [02:32,  1.75it/s]Extractor Predicting: 272it [02:33,  1.77it/s]Extractor Predicting: 273it [02:34,  1.76it/s]Extractor Predicting: 274it [02:34,  1.74it/s]Extractor Predicting: 275it [02:35,  1.75it/s]Extractor Predicting: 276it [02:35,  1.78it/s]Extractor Predicting: 277it [02:36,  1.77it/s]Extractor Predicting: 278it [02:36,  1.75it/s]Extractor Predicting: 279it [02:37,  1.74it/s]Extractor Predicting: 280it [02:38,  1.75it/s]Extractor Predicting: 281it [02:38,  1.77it/s]Extractor Predicting: 282it [02:39,  1.77it/s]Extractor Predicting: 283it [02:39,  1.77it/s]Extractor Predicting: 284it [02:40,  1.77it/s]Extractor Predicting: 285it [02:40,  1.80it/s]Extractor Predicting: 286it [02:41,  1.79it/s]Extractor Predicting: 287it [02:42,  1.76it/s]Extractor Predicting: 288it [02:42,  1.77it/s]Extractor Predicting: 289it [02:43,  1.76it/s]Extractor Predicting: 290it [02:43,  1.73it/s]Extractor Predicting: 291it [02:44,  1.72it/s]Extractor Predicting: 292it [02:44,  1.75it/s]Extractor Predicting: 293it [02:45,  1.74it/s]Extractor Predicting: 294it [02:46,  1.70it/s]Extractor Predicting: 295it [02:46,  1.73it/s]Extractor Predicting: 296it [02:47,  1.73it/s]Extractor Predicting: 297it [02:47,  1.70it/s]Extractor Predicting: 298it [02:48,  1.72it/s]Extractor Predicting: 299it [02:49,  1.71it/s]Extractor Predicting: 300it [02:49,  1.66it/s]Extractor Predicting: 301it [02:50,  1.62it/s]Extractor Predicting: 302it [02:50,  1.58it/s]Extractor Predicting: 303it [02:51,  1.63it/s]Extractor Predicting: 304it [02:52,  1.59it/s]Extractor Predicting: 305it [02:52,  1.56it/s]Extractor Predicting: 306it [02:53,  1.43it/s]Extractor Predicting: 307it [02:54,  1.49it/s]Extractor Predicting: 308it [02:54,  1.57it/s]Extractor Predicting: 309it [02:55,  1.68it/s]Extractor Predicting: 310it [02:55,  1.69it/s]Extractor Predicting: 311it [02:56,  1.66it/s]Extractor Predicting: 312it [02:57,  1.69it/s]Extractor Predicting: 313it [02:57,  1.68it/s]Extractor Predicting: 314it [02:58,  1.68it/s]Extractor Predicting: 315it [02:58,  1.77it/s]Extractor Predicting: 316it [02:59,  1.79it/s]Extractor Predicting: 317it [02:59,  1.77it/s]Extractor Predicting: 318it [03:00,  1.78it/s]Extractor Predicting: 319it [03:01,  1.69it/s]Extractor Predicting: 320it [03:01,  1.73it/s]Extractor Predicting: 321it [03:02,  1.76it/s]Extractor Predicting: 322it [03:02,  1.80it/s]Extractor Predicting: 323it [03:03,  1.81it/s]Extractor Predicting: 324it [03:03,  1.82it/s]Extractor Predicting: 325it [03:04,  1.73it/s]Extractor Predicting: 326it [03:05,  1.77it/s]Extractor Predicting: 327it [03:05,  1.79it/s]Extractor Predicting: 328it [03:06,  1.78it/s]Extractor Predicting: 329it [03:06,  1.78it/s]Extractor Predicting: 330it [03:07,  1.77it/s]Extractor Predicting: 331it [03:07,  1.68it/s]Extractor Predicting: 332it [03:08,  1.73it/s]Extractor Predicting: 333it [03:09,  1.74it/s]Extractor Predicting: 334it [03:09,  1.74it/s]Extractor Predicting: 335it [03:10,  1.76it/s]Extractor Predicting: 336it [03:10,  1.80it/s]Extractor Predicting: 337it [03:11,  1.75it/s]Extractor Predicting: 338it [03:11,  1.69it/s]Extractor Predicting: 339it [03:12,  1.72it/s]Extractor Predicting: 340it [03:13,  1.76it/s]Extractor Predicting: 341it [03:13,  1.79it/s]Extractor Predicting: 342it [03:14,  1.76it/s]Extractor Predicting: 343it [03:14,  1.78it/s]Extractor Predicting: 344it [03:15,  1.80it/s]Extractor Predicting: 345it [03:15,  1.85it/s]Extractor Predicting: 346it [03:16,  1.89it/s]Extractor Predicting: 347it [03:16,  1.94it/s]Extractor Predicting: 348it [03:17,  1.91it/s]Extractor Predicting: 349it [03:17,  1.92it/s]Extractor Predicting: 350it [03:18,  1.88it/s]Extractor Predicting: 351it [03:18,  1.88it/s]Extractor Predicting: 352it [03:19,  1.88it/s]Extractor Predicting: 353it [03:19,  1.90it/s]Extractor Predicting: 354it [03:20,  1.90it/s]Extractor Predicting: 355it [03:21,  1.91it/s]Extractor Predicting: 356it [03:21,  1.91it/s]Extractor Predicting: 357it [03:22,  1.88it/s]Extractor Predicting: 358it [03:22,  1.88it/s]Extractor Predicting: 359it [03:23,  1.88it/s]Extractor Predicting: 360it [03:23,  1.87it/s]Extractor Predicting: 361it [03:24,  1.87it/s]Extractor Predicting: 362it [03:24,  1.85it/s]Extractor Predicting: 363it [03:25,  1.88it/s]Extractor Predicting: 364it [03:25,  1.94it/s]Extractor Predicting: 365it [03:26,  1.87it/s]Extractor Predicting: 366it [03:26,  1.82it/s]Extractor Predicting: 367it [03:27,  1.76it/s]Extractor Predicting: 368it [03:28,  1.70it/s]Extractor Predicting: 369it [03:28,  1.77it/s]Extractor Predicting: 370it [03:29,  1.74it/s]Extractor Predicting: 371it [03:29,  1.77it/s]Extractor Predicting: 372it [03:30,  1.79it/s]Extractor Predicting: 373it [03:30,  1.81it/s]Extractor Predicting: 374it [03:31,  1.78it/s]Extractor Predicting: 375it [03:32,  1.77it/s]Extractor Predicting: 376it [03:32,  1.77it/s]Extractor Predicting: 377it [03:33,  1.75it/s]Extractor Predicting: 378it [03:33,  1.75it/s]Extractor Predicting: 379it [03:34,  1.75it/s]Extractor Predicting: 380it [03:34,  1.73it/s]Extractor Predicting: 381it [03:35,  1.78it/s]Extractor Predicting: 382it [03:36,  1.81it/s]Extractor Predicting: 383it [03:36,  1.82it/s]Extractor Predicting: 384it [03:37,  1.85it/s]Extractor Predicting: 385it [03:37,  1.82it/s]Extractor Predicting: 386it [03:38,  1.80it/s]Extractor Predicting: 387it [03:38,  1.82it/s]Extractor Predicting: 388it [03:39,  1.87it/s]Extractor Predicting: 389it [03:39,  1.81it/s]Extractor Predicting: 390it [03:40,  1.79it/s]Extractor Predicting: 391it [03:40,  1.81it/s]Extractor Predicting: 392it [03:41,  1.85it/s]Extractor Predicting: 393it [03:41,  1.89it/s]Extractor Predicting: 394it [03:42,  1.90it/s]Extractor Predicting: 395it [03:42,  1.92it/s]Extractor Predicting: 396it [03:43,  1.95it/s]Extractor Predicting: 397it [03:44,  1.91it/s]Extractor Predicting: 398it [03:44,  1.89it/s]Extractor Predicting: 399it [03:45,  1.89it/s]Extractor Predicting: 400it [03:45,  1.84it/s]Extractor Predicting: 401it [03:46,  1.87it/s]Extractor Predicting: 402it [03:46,  1.90it/s]Extractor Predicting: 403it [03:47,  1.90it/s]Extractor Predicting: 404it [03:47,  1.90it/s]Extractor Predicting: 405it [03:48,  1.86it/s]Extractor Predicting: 406it [03:48,  1.84it/s]Extractor Predicting: 407it [03:49,  1.85it/s]Extractor Predicting: 408it [03:49,  1.87it/s]Extractor Predicting: 409it [03:50,  1.68it/s]Extractor Predicting: 410it [03:51,  1.73it/s]Extractor Predicting: 411it [03:51,  1.75it/s]Extractor Predicting: 412it [03:52,  1.75it/s]Extractor Predicting: 413it [03:52,  1.70it/s]Extractor Predicting: 414it [03:53,  1.67it/s]Extractor Predicting: 415it [03:54,  1.62it/s]Extractor Predicting: 416it [03:54,  1.64it/s]Extractor Predicting: 417it [03:55,  1.63it/s]Extractor Predicting: 418it [03:56,  1.68it/s]Extractor Predicting: 419it [03:56,  1.69it/s]Extractor Predicting: 420it [03:57,  1.67it/s]Extractor Predicting: 421it [03:57,  1.68it/s]Extractor Predicting: 422it [03:58,  1.69it/s]Extractor Predicting: 423it [03:58,  1.70it/s]Extractor Predicting: 424it [03:59,  1.70it/s]Extractor Predicting: 425it [04:00,  1.46it/s]Extractor Predicting: 426it [04:01,  1.51it/s]Extractor Predicting: 427it [04:01,  1.57it/s]Extractor Predicting: 428it [04:02,  1.55it/s]Extractor Predicting: 429it [04:02,  1.62it/s]Extractor Predicting: 430it [04:03,  1.66it/s]Extractor Predicting: 431it [04:04,  1.68it/s]Extractor Predicting: 432it [04:04,  1.69it/s]Extractor Predicting: 433it [04:05,  1.70it/s]Extractor Predicting: 434it [04:05,  1.62it/s]Extractor Predicting: 435it [04:06,  1.67it/s]Extractor Predicting: 436it [04:06,  1.74it/s]Extractor Predicting: 437it [04:07,  1.73it/s]Extractor Predicting: 438it [04:08,  1.76it/s]Extractor Predicting: 439it [04:08,  1.76it/s]Extractor Predicting: 440it [04:09,  1.72it/s]Extractor Predicting: 441it [04:09,  1.72it/s]Extractor Predicting: 442it [04:10,  1.73it/s]Extractor Predicting: 443it [04:10,  1.72it/s]Extractor Predicting: 444it [04:11,  1.72it/s]Extractor Predicting: 445it [04:12,  1.71it/s]Extractor Predicting: 446it [04:12,  1.66it/s]Extractor Predicting: 447it [04:13,  1.64it/s]Extractor Predicting: 448it [04:14,  1.66it/s]Extractor Predicting: 449it [04:14,  1.72it/s]Extractor Predicting: 450it [04:15,  1.77it/s]Extractor Predicting: 451it [04:15,  1.76it/s]Extractor Predicting: 452it [04:16,  1.75it/s]Extractor Predicting: 453it [04:16,  1.72it/s]Extractor Predicting: 454it [04:17,  1.73it/s]Extractor Predicting: 455it [04:18,  1.71it/s]Extractor Predicting: 456it [04:18,  1.73it/s]Extractor Predicting: 457it [04:19,  1.70it/s]Extractor Predicting: 458it [04:19,  1.73it/s]Extractor Predicting: 459it [04:20,  1.70it/s]Extractor Predicting: 460it [04:20,  1.70it/s]Extractor Predicting: 461it [04:21,  1.73it/s]Extractor Predicting: 462it [04:22,  1.74it/s]Extractor Predicting: 463it [04:22,  1.70it/s]Extractor Predicting: 464it [04:23,  1.67it/s]Extractor Predicting: 465it [04:23,  1.63it/s]Extractor Predicting: 466it [04:24,  1.67it/s]Extractor Predicting: 467it [04:25,  1.74it/s]Extractor Predicting: 468it [04:25,  1.76it/s]Extractor Predicting: 469it [04:26,  1.82it/s]Extractor Predicting: 470it [04:26,  1.79it/s]Extractor Predicting: 471it [04:27,  1.72it/s]Extractor Predicting: 472it [04:27,  1.74it/s]Extractor Predicting: 473it [04:28,  1.73it/s]Extractor Predicting: 474it [04:29,  1.72it/s]Extractor Predicting: 475it [04:29,  1.71it/s]Extractor Predicting: 476it [04:30,  1.74it/s]Extractor Predicting: 477it [04:30,  1.71it/s]Extractor Predicting: 478it [04:31,  1.73it/s]Extractor Predicting: 479it [04:32,  1.66it/s]Extractor Predicting: 480it [04:32,  1.63it/s]Extractor Predicting: 481it [04:33,  1.62it/s]Extractor Predicting: 482it [04:33,  1.60it/s]Extractor Predicting: 483it [04:34,  1.58it/s]Extractor Predicting: 484it [04:35,  1.57it/s]Extractor Predicting: 485it [04:35,  1.58it/s]Extractor Predicting: 486it [04:36,  1.62it/s]Extractor Predicting: 487it [04:37,  1.63it/s]Extractor Predicting: 488it [04:37,  1.62it/s]Extractor Predicting: 489it [04:38,  1.66it/s]Extractor Predicting: 490it [04:38,  1.62it/s]Extractor Predicting: 491it [04:39,  1.61it/s]Extractor Predicting: 492it [04:40,  1.60it/s]Extractor Predicting: 493it [04:40,  1.62it/s]Extractor Predicting: 494it [04:41,  1.62it/s]Extractor Predicting: 495it [04:41,  1.62it/s]Extractor Predicting: 496it [04:42,  1.63it/s]Extractor Predicting: 497it [04:43,  1.61it/s]Extractor Predicting: 498it [04:43,  1.64it/s]Extractor Predicting: 499it [04:44,  1.60it/s]Extractor Predicting: 500it [04:45,  1.63it/s]Extractor Predicting: 501it [04:45,  1.64it/s]Extractor Predicting: 502it [04:46,  1.61it/s]Extractor Predicting: 503it [04:46,  1.60it/s]Extractor Predicting: 504it [04:47,  1.54it/s]Extractor Predicting: 505it [04:48,  1.55it/s]Extractor Predicting: 506it [04:48,  1.54it/s]Extractor Predicting: 507it [04:49,  1.59it/s]Extractor Predicting: 508it [04:49,  1.77it/s]Extractor Predicting: 508it [04:49,  1.75it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:50,318 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:50,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:50,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:50,347 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:50,347 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:50:51,185 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:50:51,186 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:50:51,815 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:50:52,994 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:50:52,994 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:56,172 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:56,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:56,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:56,232 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:50:56,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:50:57,294 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:50:57,295 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:50:58,009 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:50:58,306 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:50:58,306 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.70it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.67it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:08,  1.72it/s]Extractor Predicting: 16it [00:09,  1.69it/s]Extractor Predicting: 17it [00:10,  1.70it/s]Extractor Predicting: 18it [00:10,  1.72it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:11,  1.67it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:13,  1.65it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:14,  1.70it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:16,  1.67it/s]Extractor Predicting: 29it [00:17,  1.64it/s]Extractor Predicting: 30it [00:17,  1.67it/s]Extractor Predicting: 31it [00:18,  1.73it/s]Extractor Predicting: 32it [00:18,  1.78it/s]Extractor Predicting: 33it [00:19,  1.80it/s]Extractor Predicting: 34it [00:19,  1.83it/s]Extractor Predicting: 35it [00:20,  1.81it/s]Extractor Predicting: 36it [00:21,  1.81it/s]Extractor Predicting: 37it [00:21,  1.78it/s]Extractor Predicting: 38it [00:22,  1.80it/s]Extractor Predicting: 39it [00:22,  1.80it/s]Extractor Predicting: 40it [00:23,  1.82it/s]Extractor Predicting: 41it [00:23,  1.80it/s]Extractor Predicting: 42it [00:24,  1.81it/s]Extractor Predicting: 43it [00:25,  1.81it/s]Extractor Predicting: 44it [00:25,  1.87it/s]Extractor Predicting: 45it [00:26,  1.82it/s]Extractor Predicting: 46it [00:26,  1.83it/s]Extractor Predicting: 47it [00:27,  1.83it/s]Extractor Predicting: 48it [00:27,  1.79it/s]Extractor Predicting: 49it [00:28,  1.79it/s]Extractor Predicting: 50it [00:28,  1.77it/s]Extractor Predicting: 51it [00:29,  1.76it/s]Extractor Predicting: 52it [00:30,  1.76it/s]Extractor Predicting: 53it [00:30,  1.77it/s]Extractor Predicting: 54it [00:31,  1.67it/s]Extractor Predicting: 55it [00:31,  1.71it/s]Extractor Predicting: 56it [00:32,  1.75it/s]Extractor Predicting: 57it [00:32,  1.76it/s]Extractor Predicting: 58it [00:33,  1.77it/s]Extractor Predicting: 59it [00:34,  1.79it/s]Extractor Predicting: 60it [00:34,  1.73it/s]Extractor Predicting: 61it [00:35,  1.75it/s]Extractor Predicting: 62it [00:35,  1.77it/s]Extractor Predicting: 63it [00:36,  1.78it/s]Extractor Predicting: 64it [00:37,  1.62it/s]Extractor Predicting: 65it [00:37,  1.63it/s]Extractor Predicting: 66it [00:38,  1.65it/s]Extractor Predicting: 67it [00:38,  1.64it/s]Extractor Predicting: 68it [00:39,  1.64it/s]Extractor Predicting: 69it [00:40,  1.61it/s]Extractor Predicting: 70it [00:40,  1.63it/s]Extractor Predicting: 71it [00:41,  1.61it/s]Extractor Predicting: 72it [00:41,  1.61it/s]Extractor Predicting: 73it [00:42,  1.65it/s]Extractor Predicting: 74it [00:43,  1.61it/s]Extractor Predicting: 75it [00:43,  1.83it/s]Extractor Predicting: 75it [00:43,  1.72it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:51:45,876 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:51:45,900 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:51:45,948 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:51:45,949 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:51:45,975 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:52:01,057 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:52:01,102 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:52:01,402 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:52:01,403 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:52:01,637 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:52:01,916 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:52:01,916 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:52:01,916 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:52:01,916 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:52:01,916 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:52:01,917 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:52:02,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:03,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:03,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:04,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:04,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:05,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:06,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:06,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:07,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:07,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:08,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:09,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:09,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:10,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:10,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:11,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:11,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:12,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:12,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:13,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:14,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:14,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<04:01, 12.69s/it][WARNING|generation_utils.py:914] 2023-08-28 13:52:15,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:15,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:16,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:16,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:17,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:18,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:19,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:19,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:20,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:20,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:21,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:21,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:22,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:23,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:23,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:24,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:24,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:25,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:25,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:26,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:26,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:27,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:28,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:28,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:29,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:27<04:09, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 13:52:29,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:30,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:30,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:31,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:32,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:33,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:34,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:34,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:35,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:36,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:37,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:37,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:38,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:39,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:39,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:40,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:41,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:41,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:42,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:43,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:44,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:44,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:45,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:45,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:46,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:47,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:47,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:46<04:33, 16.10s/it][WARNING|generation_utils.py:914] 2023-08-28 13:52:48,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:49,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:49,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:50,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:51,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:51,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:52,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:52,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:53,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:54,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:54,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:55,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:56,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:56,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:57,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:57,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:58,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:59,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:52:59,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:00,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:01,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:01,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:02,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:00<04:07, 15.45s/it][WARNING|generation_utils.py:914] 2023-08-28 13:53:03,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:03,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:04,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:04,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:05,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:06,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:06,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:07,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:07,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:08,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:09,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:09,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:10,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:10,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:11,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:12,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:12,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:13,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:13,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:14,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:15,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:15,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:16,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:16,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:17,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:17,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:16<03:52, 15.48s/it][WARNING|generation_utils.py:914] 2023-08-28 13:53:18,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:19,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:19,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:20,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:20,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:21,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:22,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:22,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:23,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:24,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:24,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:25,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:25,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:26,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:27,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:27,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:28,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:28,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:29,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:30,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:30,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:31,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:31,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:32,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:33,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:33,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:34,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:32<03:40, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-28 13:53:34,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:35,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:36,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:36,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:37,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:37,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:38,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:39,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:39,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:40,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:41,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:41,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:42,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:42,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:43,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:44,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:44,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:45,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:45,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:46,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:46,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:47,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:48,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:46<03:16, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-28 13:53:48,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:49,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:49,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:50,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:51,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:51,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:52,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:53,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:53,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:54,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:54,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:55,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:55,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:56,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:57,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:57,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:58,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:58,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:53:59,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:00,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:00,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:01,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:01,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:02,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:03,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:01<03:00, 15.07s/it][WARNING|generation_utils.py:914] 2023-08-28 13:54:03,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:04,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:04,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:05,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:06,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:06,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:07,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:08,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:08,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:09,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:09,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:10,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:10,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:11,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:12,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:12,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:13,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:13,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:14,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:15,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:15,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:16,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:17,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:17,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:18,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:16<02:46, 15.12s/it][WARNING|generation_utils.py:914] 2023-08-28 13:54:18,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:19,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:20,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:20,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:21,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:21,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:22,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:23,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:23,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:24,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:24,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:25,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:26,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:26,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:27,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:27,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:28,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:29,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:29,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:30,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:30,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:31,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:32,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:32,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:30<02:29, 14.94s/it][WARNING|generation_utils.py:914] 2023-08-28 13:54:33,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:34,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:34,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:35,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:36,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:36,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:37,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:37,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:38,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:39,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:39,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:40,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:41,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:41,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:42,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:43,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:43,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:44,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:44,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:45,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:46,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:46,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:47,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:45<02:13, 14.86s/it][WARNING|generation_utils.py:914] 2023-08-28 13:54:48,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:48,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:49,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:49,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:50,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:51,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:51,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:52,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:53,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:53,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:54,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:55,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:55,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:56,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:57,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:57,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:58,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:58,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:54:59,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:00,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:00,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:01,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:02,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:00<01:58, 14.80s/it][WARNING|generation_utils.py:914] 2023-08-28 13:55:02,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:03,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:04,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:04,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:05,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:05,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:06,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:06,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:07,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:08,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:08,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:09,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:09,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:10,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:11,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:12,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:12,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:13,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:13,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:14,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:15,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:15,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:16,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:14<01:41, 14.55s/it][WARNING|generation_utils.py:914] 2023-08-28 13:55:16,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:17,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:17,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:18,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:18,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:19,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:20,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:20,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:21,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:21,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:22,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:22,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:23,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:24,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:24,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:25,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:25,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:26,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:27,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:27,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:28,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:29,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:29,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:27<01:25, 14.21s/it][WARNING|generation_utils.py:914] 2023-08-28 13:55:30,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:30,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:31,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:31,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:32,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:33,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:33,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:34,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:34,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:35,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:36,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:36,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:37,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:37,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:38,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:39,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:39,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:40,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:40,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:41,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:42,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:40<01:08, 13.73s/it][WARNING|generation_utils.py:914] 2023-08-28 13:55:42,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:43,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:44,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:44,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:45,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:46,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:46,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:47,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:48,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:48,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:49,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:50,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:50,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:51,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:51,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:52,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:53,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:53,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:54,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:54,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:55,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:56,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:57,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:57,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:58,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:58,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:55:59,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:00,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:00,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:59<01:00, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-28 13:56:01,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:02,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:02,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:03,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:03,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:04,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:04,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:05,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:06,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:06,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:07,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:07,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:08,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:09,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:09,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:10,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:10,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:11,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:12,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:12,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:13,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:11<00:43, 14.37s/it][WARNING|generation_utils.py:914] 2023-08-28 13:56:13,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:14,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:15,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:15,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:16,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:16,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:17,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:17,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:18,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:19,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:19,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:20,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:20,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:21,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:22,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:22,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:23,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:23,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:24,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:25,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:25,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:26,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:26,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:24<00:28, 14.11s/it][WARNING|generation_utils.py:914] 2023-08-28 13:56:27,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:28,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:28,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:29,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:29,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:30,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:30,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:31,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:32,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:32,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:33,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:33,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:34,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:35,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:35,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:36,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:36,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:37,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:37,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:38,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:39,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:40,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:40,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:39<00:14, 14.14s/it][WARNING|generation_utils.py:914] 2023-08-28 13:56:41,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:42,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:42,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:43,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:44,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:44,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:45,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:46,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:46,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:47,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:47,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:48,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:49,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:49,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:50,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:50,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:51,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:52,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:52,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:53,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:54,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:54,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:55,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:56,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:56:56,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:54<00:00, 14.62s/it]Generating: 100%|██████████| 20/20 [04:54<00:00, 14.74s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:09,223 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:09,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:09,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:09,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:09,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:57:10,321 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:57:10,322 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:57:11,006 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:57:12,185 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:57:12,185 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:14,151 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:14,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:14,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:14,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:57:14,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:57:15,027 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:57:15,028 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:57:15,430 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:57:15,725 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:57:15,725 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 358, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7118055555555556, 'errors': {'', "('συῆνος', 'said to be the same as', '', 'They are known as a form of Greek σάρτικρος ( συῆνος ) , τεῖς ( συθροτος ) , τίνρας ( συῆλώηνος ) ; and of the forms σατος τικρος in Latin .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : student .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 331, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 503, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7391826923076923, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 303, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 466, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 531, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 576, 'raw': 800}
{'target': 600, 'success': 598, 'raw': 832}
{'target': 600, 'success': 620, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7175925925925926, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8233695652173914, 'errors': {'', "('', 'continent', 'Aegean Sea', 'The island of Aranet , located in the Aegean Sea , is one of two islands that the Greek island of Argos has in common with the Arabian sea .')"}}
['Relation : field of this occupation . Context : The main characters in the series are The New Guy , The New York City Police Detective and the Los Angeles Police Department . Head Entity : Los Angeles Police Department , Tail Entity : police .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.76625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Walter L. Cusack', 'field of this occupation', '', 'Walter L. Cusack ( June 23 , 1924 February 21 , 2002 ) was an English cricket er who played six Tests in England .')"}}
['Relation : field of work . Context : Following his graduation in 1957 , he founded the PSA ( now in association with B. C. ) , an academic association for linguistics , linguistics , and other fields , and served as a member of the Princeton University Research Committee . Head Entity : linguistics , Tail Entity : graduate .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.76875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n']
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n', 'Relation : given name . Context : James M. Young ( February 14 , 1821 October 19 , 1924 ) was an American poet and writer specializing in the arts . Head Entity : James M. Young , Tail Entity : named name .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8206521739130435, 'errors': {'', "('Doctor Who', 'given name', '', 'He is best known for his play as a character on Doctor Who , in which he played the name of the villainess in the Doctor Who special , The One and Only Thing .')", 'too many values to unpack (expected 2)', "('Joseph L. M. Rennelly', 'given name', '', 'Joseph L. M. Rennelly is a former New York City mayor who served as the 62nd governor of New York City .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Never Say Goodbye', 'lyrics by', '', 'After graduating from Berklee College of Music , he started to write a single called Never Say Goodbye that debuted at No. 1 on the Billboard 200 single Hot Hot Song in July 1991 .')", "('Never Let Me Go', 'lyrics by', '', 'She followed this up with her sixth studio album , Never Let Me Go , which is now a cult classic on the Billboard R&B Top 40 .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Central railway', 'owned by', '', 'It was built between 1941 and 1943 under the direction of the Central railway company ( D.')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9077380952380952, 'errors': {'', "('GMAW', 'performer', '', 'He also performed with the band Pampers on the opening night of WMAW in Los Angeles and the opening night of the ABC drama GMAW in Atlanta .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 128, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 185, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 223, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 288, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 371, 'raw': 576}
{'target': 600, 'success': 393, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 433, 'raw': 672}
{'target': 600, 'success': 451, 'raw': 704}
{'target': 600, 'success': 471, 'raw': 736}
{'target': 600, 'success': 495, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 583, 'raw': 896}
{'target': 600, 'success': 601, 'raw': 928}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.6476293103448276, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : replaces .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 17944
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18044, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.59it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:01,  1.60it/s]Extractor Estimating: 4it [00:02,  1.66it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.57it/s]Extractor Estimating: 7it [00:04,  1.64it/s]Extractor Estimating: 8it [00:04,  1.65it/s]Extractor Estimating: 9it [00:05,  1.64it/s]Extractor Estimating: 10it [00:06,  1.69it/s]Extractor Estimating: 11it [00:06,  1.71it/s]Extractor Estimating: 12it [00:07,  1.67it/s]Extractor Estimating: 13it [00:07,  1.68it/s]Extractor Estimating: 14it [00:08,  1.72it/s]Extractor Estimating: 15it [00:09,  1.68it/s]Extractor Estimating: 16it [00:09,  1.62it/s]Extractor Estimating: 17it [00:10,  1.68it/s]Extractor Estimating: 18it [00:10,  1.65it/s]Extractor Estimating: 19it [00:11,  1.62it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:12,  1.64it/s]Extractor Estimating: 22it [00:13,  1.69it/s]Extractor Estimating: 23it [00:13,  1.66it/s]Extractor Estimating: 24it [00:14,  1.70it/s]Extractor Estimating: 25it [00:15,  1.65it/s]Extractor Estimating: 26it [00:15,  1.53it/s]Extractor Estimating: 27it [00:16,  1.56it/s]Extractor Estimating: 28it [00:17,  1.62it/s]Extractor Estimating: 29it [00:17,  1.69it/s]Extractor Estimating: 30it [00:18,  1.71it/s]Extractor Estimating: 31it [00:18,  1.67it/s]Extractor Estimating: 32it [00:19,  1.72it/s]Extractor Estimating: 33it [00:20,  1.69it/s]Extractor Estimating: 34it [00:20,  1.67it/s]Extractor Estimating: 35it [00:21,  1.68it/s]Extractor Estimating: 36it [00:21,  1.71it/s]Extractor Estimating: 37it [00:22,  1.70it/s]Extractor Estimating: 38it [00:22,  1.69it/s]Extractor Estimating: 39it [00:23,  1.65it/s]Extractor Estimating: 40it [00:24,  1.69it/s]Extractor Estimating: 41it [00:24,  1.75it/s]Extractor Estimating: 42it [00:25,  1.75it/s]Extractor Estimating: 43it [00:25,  1.72it/s]Extractor Estimating: 44it [00:26,  1.73it/s]Extractor Estimating: 45it [00:26,  1.75it/s]Extractor Estimating: 46it [00:27,  1.73it/s]Extractor Estimating: 47it [00:28,  1.68it/s]Extractor Estimating: 48it [00:28,  1.74it/s]Extractor Estimating: 49it [00:29,  1.72it/s]Extractor Estimating: 50it [00:29,  1.72it/s]Extractor Estimating: 51it [00:30,  1.73it/s]Extractor Estimating: 52it [00:31,  1.71it/s]Extractor Estimating: 53it [00:31,  1.68it/s]Extractor Estimating: 54it [00:32,  1.70it/s]Extractor Estimating: 55it [00:32,  1.70it/s]Extractor Estimating: 56it [00:33,  1.61it/s]Extractor Estimating: 57it [00:34,  1.59it/s]Extractor Estimating: 58it [00:34,  1.60it/s]Extractor Estimating: 59it [00:35,  1.59it/s]Extractor Estimating: 60it [00:36,  1.57it/s]Extractor Estimating: 61it [00:36,  1.60it/s]Extractor Estimating: 62it [00:37,  1.63it/s]Extractor Estimating: 63it [00:37,  1.66it/s]Extractor Estimating: 64it [00:38,  1.70it/s]Extractor Estimating: 65it [00:39,  1.68it/s]Extractor Estimating: 66it [00:39,  1.61it/s]Extractor Estimating: 67it [00:40,  1.61it/s]Extractor Estimating: 68it [00:40,  1.61it/s]Extractor Estimating: 69it [00:41,  1.60it/s]Extractor Estimating: 70it [00:42,  1.59it/s]Extractor Estimating: 71it [00:42,  1.53it/s]Extractor Estimating: 72it [00:43,  1.54it/s]Extractor Estimating: 73it [00:44,  1.56it/s]Extractor Estimating: 74it [00:44,  1.59it/s]Extractor Estimating: 75it [00:45,  1.59it/s]Extractor Estimating: 76it [00:46,  1.46it/s]Extractor Estimating: 77it [00:46,  1.47it/s]Extractor Estimating: 78it [00:47,  1.49it/s]Extractor Estimating: 79it [00:48,  1.56it/s]Extractor Estimating: 80it [00:48,  1.53it/s]Extractor Estimating: 81it [00:49,  1.56it/s]Extractor Estimating: 82it [00:50,  1.56it/s]Extractor Estimating: 83it [00:50,  1.60it/s]Extractor Estimating: 84it [00:51,  1.58it/s]Extractor Estimating: 85it [00:51,  1.58it/s]Extractor Estimating: 86it [00:52,  1.58it/s]Extractor Estimating: 87it [00:53,  1.54it/s]Extractor Estimating: 88it [00:53,  1.55it/s]Extractor Estimating: 89it [00:54,  1.58it/s]Extractor Estimating: 90it [00:55,  1.56it/s]Extractor Estimating: 91it [00:55,  1.61it/s]Extractor Estimating: 92it [00:56,  1.55it/s]Extractor Estimating: 93it [00:57,  1.60it/s]Extractor Estimating: 94it [00:57,  1.58it/s]Extractor Estimating: 95it [00:58,  1.60it/s]Extractor Estimating: 96it [00:58,  1.61it/s]Extractor Estimating: 97it [00:59,  1.60it/s]Extractor Estimating: 98it [01:00,  1.59it/s]Extractor Estimating: 99it [01:00,  1.60it/s]Extractor Estimating: 100it [01:01,  1.54it/s]Extractor Estimating: 101it [01:02,  1.57it/s]Extractor Estimating: 102it [01:02,  1.56it/s]Extractor Estimating: 103it [01:03,  1.57it/s]Extractor Estimating: 104it [01:04,  1.55it/s]Extractor Estimating: 105it [01:04,  1.55it/s]Extractor Estimating: 106it [01:05,  1.58it/s]Extractor Estimating: 107it [01:05,  1.56it/s]Extractor Estimating: 108it [01:06,  1.61it/s]Extractor Estimating: 109it [01:07,  1.60it/s]Extractor Estimating: 110it [01:07,  1.60it/s]Extractor Estimating: 111it [01:08,  1.62it/s]Extractor Estimating: 112it [01:09,  1.59it/s]Extractor Estimating: 113it [01:09,  1.63it/s]Extractor Estimating: 114it [01:10,  1.60it/s]Extractor Estimating: 115it [01:10,  1.62it/s]Extractor Estimating: 116it [01:11,  1.64it/s]Extractor Estimating: 117it [01:12,  1.60it/s]Extractor Estimating: 118it [01:12,  1.63it/s]Extractor Estimating: 119it [01:13,  1.62it/s]Extractor Estimating: 120it [01:13,  1.62it/s]Extractor Estimating: 121it [01:14,  1.64it/s]Extractor Estimating: 122it [01:15,  1.61it/s]Extractor Estimating: 123it [01:15,  1.63it/s]Extractor Estimating: 124it [01:16,  1.65it/s]Extractor Estimating: 125it [01:17,  1.63it/s]Extractor Estimating: 126it [01:17,  1.60it/s]Extractor Estimating: 127it [01:18,  1.61it/s]Extractor Estimating: 128it [01:18,  1.62it/s]Extractor Estimating: 129it [01:19,  1.62it/s]Extractor Estimating: 130it [01:20,  1.57it/s]Extractor Estimating: 131it [01:20,  1.57it/s]Extractor Estimating: 132it [01:21,  1.62it/s]Extractor Estimating: 133it [01:21,  1.67it/s]Extractor Estimating: 134it [01:22,  1.67it/s]Extractor Estimating: 135it [01:23,  1.66it/s]Extractor Estimating: 136it [01:23,  1.62it/s]Extractor Estimating: 137it [01:24,  1.65it/s]Extractor Estimating: 138it [01:25,  1.60it/s]Extractor Estimating: 139it [01:25,  1.63it/s]Extractor Estimating: 140it [01:26,  1.60it/s]Extractor Estimating: 141it [01:26,  1.61it/s]Extractor Estimating: 142it [01:27,  1.67it/s]Extractor Estimating: 143it [01:28,  1.62it/s]Extractor Estimating: 144it [01:28,  1.61it/s]Extractor Estimating: 145it [01:29,  1.61it/s]Extractor Estimating: 146it [01:29,  1.61it/s]Extractor Estimating: 147it [01:30,  1.56it/s]Extractor Estimating: 148it [01:31,  1.57it/s]Extractor Estimating: 149it [01:31,  1.57it/s]Extractor Estimating: 150it [01:32,  1.59it/s]Extractor Estimating: 151it [01:33,  1.62it/s]Extractor Estimating: 152it [01:33,  1.59it/s]Extractor Estimating: 153it [01:34,  1.67it/s]Extractor Estimating: 154it [01:34,  1.68it/s]Extractor Estimating: 155it [01:35,  1.69it/s]Extractor Estimating: 156it [01:36,  1.67it/s]Extractor Estimating: 157it [01:36,  1.71it/s]Extractor Estimating: 158it [01:37,  1.60it/s]Extractor Estimating: 159it [01:37,  1.62it/s]Extractor Estimating: 160it [01:38,  1.69it/s]Extractor Estimating: 161it [01:39,  1.52it/s]Extractor Estimating: 162it [01:39,  1.61it/s]Extractor Estimating: 163it [01:40,  1.67it/s]Extractor Estimating: 164it [01:40,  1.71it/s]Extractor Estimating: 165it [01:41,  1.72it/s]Extractor Estimating: 166it [01:42,  1.75it/s]Extractor Estimating: 167it [01:42,  1.75it/s]Extractor Estimating: 168it [01:43,  1.78it/s]Extractor Estimating: 169it [01:43,  1.82it/s]Extractor Estimating: 170it [01:44,  1.71it/s]Extractor Estimating: 171it [01:44,  1.75it/s]Extractor Estimating: 172it [01:45,  1.78it/s]Extractor Estimating: 173it [01:46,  1.79it/s]Extractor Estimating: 174it [01:46,  1.79it/s]Extractor Estimating: 175it [01:47,  1.73it/s]Extractor Estimating: 176it [01:47,  1.63it/s]Extractor Estimating: 177it [01:48,  1.62it/s]Extractor Estimating: 178it [01:49,  1.65it/s]Extractor Estimating: 179it [01:49,  1.59it/s]Extractor Estimating: 180it [01:50,  1.60it/s]Extractor Estimating: 181it [01:51,  1.51it/s]Extractor Estimating: 182it [01:51,  1.59it/s]Extractor Estimating: 183it [01:52,  1.60it/s]Extractor Estimating: 184it [01:52,  1.60it/s]Extractor Estimating: 185it [01:53,  1.59it/s]Extractor Estimating: 186it [01:54,  1.57it/s]Extractor Estimating: 187it [01:54,  1.59it/s]Extractor Estimating: 188it [01:55,  1.64it/s]Extractor Estimating: 189it [01:55,  1.67it/s]Extractor Estimating: 190it [01:56,  1.67it/s]Extractor Estimating: 191it [01:57,  1.65it/s]Extractor Estimating: 192it [01:57,  1.63it/s]Extractor Estimating: 193it [01:58,  1.67it/s]Extractor Estimating: 194it [01:59,  1.64it/s]Extractor Estimating: 195it [01:59,  1.64it/s]Extractor Estimating: 196it [02:00,  1.60it/s]Extractor Estimating: 197it [02:00,  1.61it/s]Extractor Estimating: 198it [02:01,  1.59it/s]Extractor Estimating: 199it [02:02,  1.57it/s]Extractor Estimating: 200it [02:02,  1.65it/s]Extractor Estimating: 201it [02:03,  1.59it/s]Extractor Estimating: 202it [02:04,  1.59it/s]Extractor Estimating: 203it [02:04,  1.58it/s]Extractor Estimating: 204it [02:05,  1.62it/s]Extractor Estimating: 205it [02:05,  1.58it/s]Extractor Estimating: 206it [02:06,  1.52it/s]Extractor Estimating: 207it [02:07,  1.53it/s]Extractor Estimating: 208it [02:07,  1.56it/s]Extractor Estimating: 209it [02:08,  1.53it/s]Extractor Estimating: 210it [02:09,  1.56it/s]Extractor Estimating: 211it [02:09,  1.56it/s]Extractor Estimating: 212it [02:10,  1.56it/s]Extractor Estimating: 213it [02:11,  1.58it/s]Extractor Estimating: 214it [02:11,  1.60it/s]Extractor Estimating: 215it [02:12,  1.55it/s]Extractor Estimating: 216it [02:13,  1.56it/s]Extractor Estimating: 217it [02:13,  1.57it/s]Extractor Estimating: 218it [02:14,  1.63it/s]Extractor Estimating: 219it [02:14,  1.65it/s]Extractor Estimating: 220it [02:15,  1.60it/s]Extractor Estimating: 221it [02:16,  1.64it/s]Extractor Estimating: 222it [02:16,  1.61it/s]Extractor Estimating: 223it [02:17,  1.63it/s]Extractor Estimating: 224it [02:17,  1.59it/s]Extractor Estimating: 225it [02:18,  1.60it/s]Extractor Estimating: 226it [02:19,  1.60it/s]Extractor Estimating: 227it [02:19,  1.60it/s]Extractor Estimating: 228it [02:20,  1.63it/s]Extractor Estimating: 229it [02:21,  1.65it/s]Extractor Estimating: 230it [02:21,  1.60it/s]Extractor Estimating: 231it [02:22,  1.64it/s]Extractor Estimating: 232it [02:22,  1.58it/s]Extractor Estimating: 233it [02:23,  1.60it/s]Extractor Estimating: 234it [02:24,  1.61it/s]Extractor Estimating: 235it [02:24,  1.61it/s]Extractor Estimating: 236it [02:25,  1.65it/s]Extractor Estimating: 237it [02:26,  1.47it/s]Extractor Estimating: 238it [02:26,  1.50it/s]Extractor Estimating: 239it [02:27,  1.55it/s]Extractor Estimating: 240it [02:28,  1.56it/s]Extractor Estimating: 241it [02:28,  1.59it/s]Extractor Estimating: 242it [02:29,  1.58it/s]Extractor Estimating: 243it [02:29,  1.59it/s]Extractor Estimating: 244it [02:30,  1.59it/s]Extractor Estimating: 245it [02:31,  1.57it/s]Extractor Estimating: 246it [02:31,  1.59it/s]Extractor Estimating: 247it [02:32,  1.50it/s]Extractor Estimating: 248it [02:33,  1.57it/s]Extractor Estimating: 249it [02:33,  1.58it/s]Extractor Estimating: 250it [02:34,  1.59it/s]Extractor Estimating: 251it [02:34,  1.62it/s]Extractor Estimating: 252it [02:35,  1.58it/s]Extractor Estimating: 253it [02:36,  1.52it/s]Extractor Estimating: 254it [02:37,  1.51it/s]Extractor Estimating: 255it [02:37,  1.53it/s]Extractor Estimating: 256it [02:38,  1.56it/s]Extractor Estimating: 257it [02:38,  1.58it/s]Extractor Estimating: 258it [02:39,  1.58it/s]Extractor Estimating: 259it [02:40,  1.56it/s]Extractor Estimating: 260it [02:40,  1.51it/s]Extractor Estimating: 261it [02:41,  1.52it/s]Extractor Estimating: 262it [02:42,  1.55it/s]Extractor Estimating: 263it [02:42,  1.53it/s]Extractor Estimating: 264it [02:43,  1.56it/s]Extractor Estimating: 265it [02:44,  1.56it/s]Extractor Estimating: 266it [02:44,  1.55it/s]Extractor Estimating: 267it [02:45,  1.58it/s]Extractor Estimating: 268it [02:46,  1.51it/s]Extractor Estimating: 269it [02:46,  1.53it/s]Extractor Estimating: 270it [02:47,  1.57it/s]Extractor Estimating: 271it [02:47,  1.55it/s]Extractor Estimating: 272it [02:48,  1.57it/s]Extractor Estimating: 273it [02:49,  1.51it/s]Extractor Estimating: 274it [02:49,  1.55it/s]Extractor Estimating: 275it [02:50,  1.51it/s]Extractor Estimating: 276it [02:51,  1.57it/s]Extractor Estimating: 277it [02:51,  1.57it/s]Extractor Estimating: 278it [02:52,  1.49it/s]Extractor Estimating: 279it [02:53,  1.52it/s]Extractor Estimating: 280it [02:53,  1.52it/s]Extractor Estimating: 281it [02:54,  1.51it/s]Extractor Estimating: 282it [02:55,  1.56it/s]Extractor Estimating: 283it [02:55,  1.54it/s]Extractor Estimating: 284it [02:56,  1.56it/s]Extractor Estimating: 285it [02:57,  1.57it/s]Extractor Estimating: 286it [02:57,  1.54it/s]Extractor Estimating: 287it [02:58,  1.58it/s]Extractor Estimating: 288it [02:59,  1.54it/s]Extractor Estimating: 289it [02:59,  1.58it/s]Extractor Estimating: 290it [03:00,  1.60it/s]Extractor Estimating: 291it [03:00,  1.58it/s]Extractor Estimating: 292it [03:01,  1.60it/s]Extractor Estimating: 293it [03:02,  1.54it/s]Extractor Estimating: 294it [03:02,  1.53it/s]Extractor Estimating: 295it [03:03,  1.51it/s]Extractor Estimating: 296it [03:04,  1.52it/s]Extractor Estimating: 297it [03:04,  1.55it/s]Extractor Estimating: 298it [03:05,  1.51it/s]Extractor Estimating: 299it [03:06,  1.51it/s]Extractor Estimating: 300it [03:06,  1.55it/s]Extractor Estimating: 301it [03:07,  1.54it/s]Extractor Estimating: 302it [03:08,  1.57it/s]Extractor Estimating: 303it [03:08,  1.57it/s]Extractor Estimating: 304it [03:09,  1.58it/s]Extractor Estimating: 305it [03:09,  1.59it/s]Extractor Estimating: 306it [03:10,  1.61it/s]Extractor Estimating: 307it [03:11,  1.66it/s]Extractor Estimating: 308it [03:11,  1.60it/s]Extractor Estimating: 309it [03:12,  1.63it/s]Extractor Estimating: 310it [03:12,  1.62it/s]Extractor Estimating: 311it [03:13,  1.58it/s]Extractor Estimating: 312it [03:14,  1.60it/s]Extractor Estimating: 313it [03:14,  1.62it/s]Extractor Estimating: 314it [03:15,  1.53it/s]Extractor Estimating: 315it [03:16,  1.53it/s]Extractor Estimating: 316it [03:16,  1.51it/s]Extractor Estimating: 317it [03:17,  1.55it/s]Extractor Estimating: 318it [03:18,  1.45it/s]Extractor Estimating: 319it [03:19,  1.43it/s]Extractor Estimating: 320it [03:19,  1.51it/s]Extractor Estimating: 321it [03:20,  1.53it/s]Extractor Estimating: 322it [03:20,  1.59it/s]Extractor Estimating: 323it [03:21,  1.59it/s]Extractor Estimating: 324it [03:22,  1.58it/s]Extractor Estimating: 325it [03:22,  1.60it/s]Extractor Estimating: 326it [03:23,  1.59it/s]Extractor Estimating: 327it [03:23,  1.62it/s]Extractor Estimating: 328it [03:24,  1.63it/s]Extractor Estimating: 329it [03:25,  1.63it/s]Extractor Estimating: 330it [03:25,  1.65it/s]Extractor Estimating: 331it [03:26,  1.59it/s]Extractor Estimating: 332it [03:27,  1.59it/s]Extractor Estimating: 333it [03:27,  1.56it/s]Extractor Estimating: 334it [03:28,  1.61it/s]Extractor Estimating: 335it [03:28,  1.63it/s]Extractor Estimating: 336it [03:29,  1.58it/s]Extractor Estimating: 337it [03:30,  1.59it/s]Extractor Estimating: 338it [03:30,  1.59it/s]Extractor Estimating: 339it [03:31,  1.64it/s]Extractor Estimating: 340it [03:31,  1.67it/s]Extractor Estimating: 341it [03:32,  1.63it/s]Extractor Estimating: 342it [03:33,  1.67it/s]Extractor Estimating: 343it [03:33,  1.60it/s]Extractor Estimating: 344it [03:34,  1.62it/s]Extractor Estimating: 345it [03:35,  1.62it/s]Extractor Estimating: 346it [03:35,  1.65it/s]Extractor Estimating: 347it [03:36,  1.65it/s]Extractor Estimating: 348it [03:36,  1.67it/s]Extractor Estimating: 349it [03:37,  1.66it/s]Extractor Estimating: 350it [03:38,  1.68it/s]Extractor Estimating: 351it [03:38,  1.67it/s]Extractor Estimating: 352it [03:39,  1.61it/s]Extractor Estimating: 353it [03:39,  1.60it/s]Extractor Estimating: 354it [03:40,  1.59it/s]Extractor Estimating: 355it [03:41,  1.62it/s]Extractor Estimating: 356it [03:41,  1.59it/s]Extractor Estimating: 357it [03:42,  1.58it/s]Extractor Estimating: 358it [03:43,  1.61it/s]Extractor Estimating: 359it [03:43,  1.57it/s]Extractor Estimating: 360it [03:44,  1.61it/s]Extractor Estimating: 361it [03:44,  1.62it/s]Extractor Estimating: 362it [03:45,  1.66it/s]Extractor Estimating: 363it [03:46,  1.60it/s]Extractor Estimating: 364it [03:46,  1.62it/s]Extractor Estimating: 365it [03:47,  1.59it/s]Extractor Estimating: 366it [03:48,  1.57it/s]Extractor Estimating: 367it [03:48,  1.60it/s]Extractor Estimating: 368it [03:49,  1.67it/s]Extractor Estimating: 369it [03:49,  1.62it/s]Extractor Estimating: 370it [03:50,  1.53it/s]Extractor Estimating: 371it [03:51,  1.56it/s]Extractor Estimating: 372it [03:51,  1.54it/s]Extractor Estimating: 373it [03:52,  1.54it/s]Extractor Estimating: 374it [03:53,  1.58it/s]Extractor Estimating: 375it [03:53,  1.57it/s]Extractor Estimating: 376it [03:54,  1.53it/s]Extractor Estimating: 377it [03:55,  1.54it/s]Extractor Estimating: 378it [03:55,  1.55it/s]Extractor Estimating: 379it [03:56,  1.58it/s]Extractor Estimating: 380it [03:57,  1.52it/s]Extractor Estimating: 381it [03:57,  1.53it/s]Extractor Estimating: 382it [03:58,  1.55it/s]Extractor Estimating: 383it [03:59,  1.52it/s]Extractor Estimating: 384it [03:59,  1.47it/s]Extractor Estimating: 385it [04:00,  1.49it/s]Extractor Estimating: 386it [04:01,  1.54it/s]Extractor Estimating: 387it [04:01,  1.54it/s]Extractor Estimating: 388it [04:02,  1.57it/s]Extractor Estimating: 389it [04:02,  1.55it/s]Extractor Estimating: 390it [04:03,  1.50it/s]Extractor Estimating: 391it [04:04,  1.55it/s]Extractor Estimating: 392it [04:04,  1.56it/s]Extractor Estimating: 393it [04:05,  1.49it/s]Extractor Estimating: 394it [04:06,  1.48it/s]Extractor Estimating: 395it [04:06,  1.49it/s]Extractor Estimating: 396it [04:07,  1.53it/s]Extractor Estimating: 397it [04:08,  1.52it/s]Extractor Estimating: 398it [04:08,  1.51it/s]Extractor Estimating: 399it [04:09,  1.50it/s]Extractor Estimating: 400it [04:10,  1.47it/s]Extractor Estimating: 401it [04:11,  1.34it/s]Extractor Estimating: 402it [04:11,  1.43it/s]Extractor Estimating: 403it [04:12,  1.47it/s]Extractor Estimating: 404it [04:13,  1.51it/s]Extractor Estimating: 405it [04:13,  1.52it/s]Extractor Estimating: 406it [04:14,  1.54it/s]Extractor Estimating: 407it [04:14,  1.54it/s]Extractor Estimating: 408it [04:15,  1.54it/s]Extractor Estimating: 409it [04:16,  1.53it/s]Extractor Estimating: 410it [04:16,  1.57it/s]Extractor Estimating: 411it [04:17,  1.59it/s]Extractor Estimating: 412it [04:18,  1.59it/s]Extractor Estimating: 413it [04:18,  1.58it/s]Extractor Estimating: 414it [04:19,  1.53it/s]Extractor Estimating: 415it [04:20,  1.52it/s]Extractor Estimating: 416it [04:20,  1.56it/s]Extractor Estimating: 417it [04:21,  1.55it/s]Extractor Estimating: 418it [04:22,  1.59it/s]Extractor Estimating: 419it [04:22,  1.60it/s]Extractor Estimating: 420it [04:23,  1.55it/s]Extractor Estimating: 421it [04:23,  1.60it/s]Extractor Estimating: 422it [04:24,  1.56it/s]Extractor Estimating: 423it [04:25,  1.60it/s]Extractor Estimating: 424it [04:25,  1.64it/s]Extractor Estimating: 425it [04:26,  1.65it/s]Extractor Estimating: 426it [04:26,  1.60it/s]Extractor Estimating: 427it [04:27,  1.59it/s]Extractor Estimating: 428it [04:28,  1.60it/s]Extractor Estimating: 429it [04:28,  1.62it/s]Extractor Estimating: 430it [04:29,  1.60it/s]Extractor Estimating: 431it [04:30,  1.65it/s]Extractor Estimating: 432it [04:30,  1.62it/s]Extractor Estimating: 433it [04:31,  1.64it/s]Extractor Estimating: 434it [04:31,  1.59it/s]Extractor Estimating: 435it [04:32,  1.59it/s]Extractor Estimating: 436it [04:33,  1.58it/s]Extractor Estimating: 437it [04:33,  1.57it/s]Extractor Estimating: 438it [04:34,  1.55it/s]Extractor Estimating: 439it [04:35,  1.50it/s]Extractor Estimating: 440it [04:35,  1.56it/s]Extractor Estimating: 441it [04:36,  1.54it/s]Extractor Estimating: 442it [04:37,  1.57it/s]Extractor Estimating: 443it [04:37,  1.60it/s]Extractor Estimating: 444it [04:38,  1.61it/s]Extractor Estimating: 445it [04:38,  1.61it/s]Extractor Estimating: 446it [04:39,  1.56it/s]Extractor Estimating: 447it [04:40,  1.56it/s]Extractor Estimating: 448it [04:40,  1.55it/s]Extractor Estimating: 449it [04:41,  1.59it/s]Extractor Estimating: 450it [04:42,  1.62it/s]Extractor Estimating: 451it [04:42,  1.59it/s]Extractor Estimating: 452it [04:43,  1.54it/s]Extractor Estimating: 453it [04:44,  1.57it/s]Extractor Estimating: 454it [04:44,  1.52it/s]Extractor Estimating: 455it [04:45,  1.55it/s]Extractor Estimating: 456it [04:46,  1.54it/s]Extractor Estimating: 457it [04:46,  1.53it/s]Extractor Estimating: 458it [04:47,  1.53it/s]Extractor Estimating: 459it [04:48,  1.50it/s]Extractor Estimating: 460it [04:48,  1.48it/s]Extractor Estimating: 461it [04:49,  1.54it/s]Extractor Estimating: 462it [04:50,  1.51it/s]Extractor Estimating: 463it [04:50,  1.47it/s]Extractor Estimating: 464it [04:51,  1.50it/s]Extractor Estimating: 465it [04:52,  1.46it/s]Extractor Estimating: 466it [04:52,  1.49it/s]Extractor Estimating: 467it [04:53,  1.55it/s]Extractor Estimating: 468it [04:53,  1.63it/s]Extractor Estimating: 469it [04:54,  1.60it/s]Extractor Estimating: 470it [04:55,  1.60it/s]Extractor Estimating: 471it [04:55,  1.53it/s]Extractor Estimating: 472it [04:56,  1.54it/s]Extractor Estimating: 473it [04:57,  1.49it/s]Extractor Estimating: 474it [04:57,  1.50it/s]Extractor Estimating: 475it [04:58,  1.49it/s]Extractor Estimating: 476it [04:59,  1.36it/s]Extractor Estimating: 477it [05:00,  1.39it/s]Extractor Estimating: 478it [05:00,  1.44it/s]Extractor Estimating: 479it [05:01,  1.51it/s]Extractor Estimating: 480it [05:02,  1.51it/s]Extractor Estimating: 481it [05:02,  1.52it/s]Extractor Estimating: 482it [05:03,  1.52it/s]Extractor Estimating: 483it [05:03,  1.53it/s]Extractor Estimating: 484it [05:04,  1.53it/s]Extractor Estimating: 485it [05:05,  1.53it/s]Extractor Estimating: 486it [05:05,  1.53it/s]Extractor Estimating: 487it [05:06,  1.54it/s]Extractor Estimating: 488it [05:07,  1.57it/s]Extractor Estimating: 489it [05:07,  1.53it/s]Extractor Estimating: 490it [05:08,  1.45it/s]Extractor Estimating: 491it [05:09,  1.51it/s]Extractor Estimating: 492it [05:09,  1.52it/s]Extractor Estimating: 493it [05:10,  1.58it/s]Extractor Estimating: 494it [05:11,  1.57it/s]Extractor Estimating: 495it [05:11,  1.52it/s]Extractor Estimating: 496it [05:12,  1.51it/s]Extractor Estimating: 497it [05:13,  1.54it/s]Extractor Estimating: 498it [05:13,  1.53it/s]Extractor Estimating: 499it [05:14,  1.50it/s]Extractor Estimating: 500it [05:14,  1.79it/s]Extractor Estimating: 500it [05:14,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:53,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:53,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:53,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:53,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:53,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:02:54,610 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:02:54,611 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:02:55,333 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:02:56,379 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:02:56,379 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:57,834 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:57,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:57,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:57,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:02:57,869 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:02:58,527 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:02:58,528 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:02:59,121 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:02:59,289 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:02:59,289 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:08:09,805 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:08:10,697 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 10282 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 25007
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25107, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25107, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.037, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.052, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.026, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.042, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 71, avg_time 1.035, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 171, avg_time 2.343, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 271, avg_time 1.033, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 371, avg_time 1.048, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 42, avg_time 1.040, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 142, avg_time 1.052, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 242, avg_time 2.330, loss:nan
g_step 1200, step 342, avg_time 1.041, loss:nan
g_step 1300, step 13, avg_time 1.032, loss:nan
g_step 1400, step 113, avg_time 1.034, loss:nan
g_step 1500, step 213, avg_time 1.039, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 313, avg_time 2.319, loss:nan
g_step 1700, step 413, avg_time 1.043, loss:nan
g_step 1800, step 84, avg_time 1.022, loss:nan
g_step 1900, step 184, avg_time 1.050, loss:nan
g_step 2000, step 284, avg_time 1.036, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 384, avg_time 2.337, loss:nan
g_step 2200, step 55, avg_time 1.024, loss:nan
g_step 2300, step 155, avg_time 1.041, loss:nan
g_step 2400, step 255, avg_time 1.034, loss:nan
g_step 2500, step 355, avg_time 1.034, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 26, avg_time 2.342, loss:nan
g_step 2700, step 126, avg_time 1.021, loss:nan
g_step 2800, step 226, avg_time 1.039, loss:nan
g_step 2900, step 326, avg_time 1.060, loss:nan
g_step 3000, step 426, avg_time 1.048, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 97, avg_time 2.334, loss:nan
g_step 3200, step 197, avg_time 1.037, loss:nan
g_step 3300, step 297, avg_time 1.052, loss:nan
g_step 3400, step 397, avg_time 1.040, loss:nan
g_step 3500, step 68, avg_time 1.042, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 168, avg_time 2.332, loss:nan
g_step 3700, step 268, avg_time 1.036, loss:nan
g_step 3800, step 368, avg_time 1.033, loss:nan
g_step 3900, step 39, avg_time 1.032, loss:nan
g_step 4000, step 139, avg_time 1.054, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 239, avg_time 2.325, loss:nan
g_step 4200, step 339, avg_time 1.028, loss:nan
g_step 4300, step 10, avg_time 1.034, loss:nan
g_step 4400, step 110, avg_time 1.030, loss:nan
g_step 4500, step 210, avg_time 1.041, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 310, avg_time 2.327, loss:nan
g_step 4700, step 410, avg_time 1.035, loss:nan
g_step 4800, step 81, avg_time 1.028, loss:nan
g_step 4900, step 181, avg_time 1.061, loss:nan
g_step 5000, step 281, avg_time 1.033, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 381, avg_time 2.312, loss:nan
g_step 5200, step 52, avg_time 1.033, loss:nan
g_step 5300, step 152, avg_time 1.034, loss:nan
g_step 5400, step 252, avg_time 1.025, loss:nan
g_step 5500, step 352, avg_time 1.051, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 23, avg_time 2.322, loss:nan
g_step 5700, step 123, avg_time 1.035, loss:nan
g_step 5800, step 223, avg_time 1.049, loss:nan
g_step 5900, step 323, avg_time 1.030, loss:nan
g_step 6000, step 423, avg_time 1.028, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 94, avg_time 2.317, loss:nan
g_step 6200, step 194, avg_time 1.026, loss:nan
g_step 6300, step 294, avg_time 1.029, loss:nan
g_step 6400, step 394, avg_time 1.048, loss:nan
g_step 6500, step 65, avg_time 1.040, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 165, avg_time 2.332, loss:nan
g_step 6700, step 265, avg_time 1.042, loss:nan
g_step 6800, step 365, avg_time 1.027, loss:nan
g_step 6900, step 36, avg_time 1.041, loss:nan
g_step 7000, step 136, avg_time 1.016, loss:nan
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 236, avg_time 2.351, loss:nan
g_step 7200, step 336, avg_time 1.034, loss:nan
g_step 7300, step 7, avg_time 1.041, loss:nan
g_step 7400, step 107, avg_time 1.038, loss:nan
g_step 7500, step 207, avg_time 1.047, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 307, avg_time 2.342, loss:nan
g_step 7700, step 407, avg_time 1.031, loss:nan
g_step 7800, step 78, avg_time 1.029, loss:nan
g_step 7900, step 178, avg_time 1.035, loss:nan
g_step 8000, step 278, avg_time 1.039, loss:nan
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 378, avg_time 2.332, loss:nan
g_step 8200, step 49, avg_time 1.029, loss:nan
g_step 8300, step 149, avg_time 1.036, loss:nan
g_step 8400, step 249, avg_time 1.038, loss:nan
g_step 8500, step 349, avg_time 1.036, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:08:10 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:08:10 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-08-09_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:08:13 - WARNING - datasets.builder -   Using custom data configuration default-418ebc12ba08558e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-418ebc12ba08558e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:08:20,950 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:08:20,952 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:08:20,952 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:08:20,953 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:08:21,306 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:08:21,525 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:08:21,526 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:08:21,526 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:08:21,526 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:08:21,526 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:08:21,526 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:08:23,074 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:08:26,432 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:08:27,118 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-418ebc12ba08558e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:09,  1.06ba/s] 18%|█▊        | 2/11 [00:01<00:04,  1.95ba/s] 27%|██▋       | 3/11 [00:01<00:02,  2.67ba/s] 36%|███▋      | 4/11 [00:01<00:02,  2.71ba/s] 45%|████▌     | 5/11 [00:01<00:01,  3.20ba/s] 55%|█████▍    | 6/11 [00:02<00:01,  3.58ba/s] 64%|██████▎   | 7/11 [00:02<00:01,  2.77ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  3.17ba/s] 82%|████████▏ | 9/11 [00:03<00:00,  3.51ba/s] 91%|█████████ | 10/11 [00:03<00:00,  3.80ba/s]100%|██████████| 11/11 [00:03<00:00,  3.23ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:03,  1.17ba/s] 40%|████      | 2/5 [00:01<00:01,  2.08ba/s] 60%|██████    | 3/5 [00:01<00:00,  2.77ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.29ba/s]100%|██████████| 5/5 [00:01<00:00,  3.80ba/s]100%|██████████| 5/5 [00:01<00:00,  2.94ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.73ba/s] 27%|██▋       | 3/11 [00:00<00:01,  6.29ba/s] 45%|████▌     | 5/11 [00:00<00:00,  8.21ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.33ba/s] 82%|████████▏ | 9/11 [00:01<00:00, 10.05ba/s]100%|██████████| 11/11 [00:01<00:00, 11.71ba/s]100%|██████████| 11/11 [00:01<00:00,  9.33ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.83ba/s] 40%|████      | 2/5 [00:00<00:00,  5.80ba/s] 80%|████████  | 4/5 [00:00<00:00,  8.32ba/s]100%|██████████| 5/5 [00:00<00:00,  7.99ba/s]
[INFO|trainer.py:414] 2023-08-28 17:08:45,525 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:08:45,692 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:08:45,692 >>   Num examples = 10300
[INFO|trainer.py:1149] 2023-08-28 17:08:45,692 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:08:45,692 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:08:45,692 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:08:45,692 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:08:45,692 >>   Total optimization steps = 805
  0%|          | 0/805 [00:00<?, ?it/s]  0%|          | 1/805 [00:02<35:51,  2.68s/it]  0%|          | 2/805 [00:03<19:43,  1.47s/it]  0%|          | 3/805 [00:04<16:37,  1.24s/it]  0%|          | 4/805 [00:05<15:52,  1.19s/it]  1%|          | 5/805 [00:05<12:15,  1.09it/s]  1%|          | 6/805 [00:06<09:32,  1.40it/s]  1%|          | 7/805 [00:06<07:47,  1.71it/s]  1%|          | 8/805 [00:06<06:56,  1.91it/s]  1%|          | 9/805 [00:07<05:55,  2.24it/s]  1%|          | 10/805 [00:07<06:05,  2.17it/s]  1%|▏         | 11/805 [00:07<05:20,  2.47it/s]  1%|▏         | 12/805 [00:08<05:25,  2.44it/s]  2%|▏         | 13/805 [00:08<04:52,  2.71it/s]  2%|▏         | 14/805 [00:08<04:30,  2.93it/s]  2%|▏         | 15/805 [00:09<04:14,  3.10it/s]  2%|▏         | 16/805 [00:09<04:14,  3.10it/s]  2%|▏         | 17/805 [00:09<04:08,  3.17it/s]  2%|▏         | 18/805 [00:10<03:58,  3.30it/s]  2%|▏         | 19/805 [00:10<03:51,  3.40it/s]  2%|▏         | 20/805 [00:10<03:45,  3.48it/s]  3%|▎         | 21/805 [00:10<03:42,  3.53it/s]  3%|▎         | 22/805 [00:11<03:39,  3.57it/s]  3%|▎         | 23/805 [00:11<03:44,  3.48it/s]  3%|▎         | 24/805 [00:11<03:41,  3.53it/s]  3%|▎         | 25/805 [00:11<03:38,  3.57it/s]  3%|▎         | 26/805 [00:12<05:53,  2.21it/s]  3%|▎         | 27/805 [00:13<05:36,  2.31it/s]  3%|▎         | 28/805 [00:13<04:59,  2.60it/s]  4%|▎         | 29/805 [00:13<04:32,  2.85it/s]  4%|▎         | 30/805 [00:14<04:13,  3.05it/s]  4%|▍         | 31/805 [00:14<04:00,  3.21it/s]  4%|▍         | 32/805 [00:14<03:51,  3.34it/s]  4%|▍         | 33/805 [00:14<03:45,  3.43it/s]  4%|▍         | 34/805 [00:15<03:40,  3.50it/s]  4%|▍         | 35/805 [00:15<03:37,  3.54it/s]  4%|▍         | 36/805 [00:15<03:34,  3.58it/s]  5%|▍         | 37/805 [00:15<03:32,  3.61it/s]  5%|▍         | 38/805 [00:16<03:57,  3.23it/s]  5%|▍         | 39/805 [00:16<03:48,  3.35it/s]  5%|▍         | 40/805 [00:16<03:42,  3.44it/s]  5%|▌         | 41/805 [00:17<03:38,  3.50it/s]  5%|▌         | 42/805 [00:17<03:34,  3.55it/s]  5%|▌         | 43/805 [00:17<03:32,  3.58it/s]  5%|▌         | 44/805 [00:17<03:30,  3.61it/s]  6%|▌         | 45/805 [00:18<03:29,  3.63it/s]  6%|▌         | 46/805 [00:18<03:28,  3.64it/s]  6%|▌         | 47/805 [00:18<03:40,  3.44it/s]  6%|▌         | 48/805 [00:19<03:36,  3.49it/s]  6%|▌         | 49/805 [00:19<03:44,  3.37it/s]  6%|▌         | 50/805 [00:19<03:38,  3.45it/s]  6%|▋         | 51/805 [00:19<03:34,  3.51it/s]  6%|▋         | 52/805 [00:20<03:31,  3.55it/s]  7%|▋         | 53/805 [00:20<03:29,  3.59it/s]  7%|▋         | 54/805 [00:20<03:28,  3.61it/s]  7%|▋         | 55/805 [00:21<03:26,  3.62it/s]  7%|▋         | 56/805 [00:21<03:26,  3.63it/s]  7%|▋         | 57/805 [00:21<03:25,  3.64it/s]  7%|▋         | 58/805 [00:21<03:24,  3.65it/s]  7%|▋         | 59/805 [00:22<03:24,  3.65it/s]  7%|▋         | 60/805 [00:22<03:43,  3.34it/s]  8%|▊         | 61/805 [00:22<03:37,  3.42it/s]  8%|▊         | 62/805 [00:23<03:32,  3.49it/s]  8%|▊         | 63/805 [00:23<03:29,  3.54it/s]  8%|▊         | 64/805 [00:23<03:27,  3.58it/s]  8%|▊         | 65/805 [00:23<03:25,  3.60it/s]  8%|▊         | 66/805 [00:24<03:24,  3.61it/s]  8%|▊         | 67/805 [00:25<06:01,  2.04it/s]  8%|▊         | 68/805 [00:25<05:14,  2.35it/s]  9%|▊         | 69/805 [00:26<06:03,  2.03it/s]  9%|▊         | 70/805 [00:26<05:13,  2.34it/s]  9%|▉         | 71/805 [00:26<04:39,  2.62it/s]  9%|▉         | 72/805 [00:26<04:15,  2.87it/s]  9%|▉         | 73/805 [00:27<03:58,  3.07it/s]  9%|▉         | 74/805 [00:27<03:47,  3.22it/s]  9%|▉         | 75/805 [00:27<04:18,  2.83it/s]  9%|▉         | 76/805 [00:28<04:00,  3.03it/s] 10%|▉         | 77/805 [00:28<03:48,  3.19it/s] 10%|▉         | 78/805 [00:28<03:39,  3.31it/s] 10%|▉         | 79/805 [00:29<03:33,  3.40it/s] 10%|▉         | 80/805 [00:29<03:28,  3.48it/s] 10%|█         | 81/805 [00:29<03:25,  3.52it/s] 10%|█         | 82/805 [00:29<03:23,  3.56it/s] 10%|█         | 83/805 [00:30<03:21,  3.58it/s] 10%|█         | 84/805 [00:30<03:19,  3.61it/s] 11%|█         | 85/805 [00:30<03:18,  3.62it/s] 11%|█         | 86/805 [00:31<03:57,  3.03it/s] 11%|█         | 87/805 [00:31<03:44,  3.19it/s] 11%|█         | 88/805 [00:31<03:36,  3.32it/s] 11%|█         | 89/805 [00:31<03:30,  3.41it/s] 11%|█         | 90/805 [00:32<03:25,  3.47it/s] 11%|█▏        | 91/805 [00:32<03:22,  3.53it/s] 11%|█▏        | 92/805 [00:32<03:20,  3.56it/s] 12%|█▏        | 93/805 [00:33<03:18,  3.58it/s] 12%|█▏        | 94/805 [00:33<03:17,  3.60it/s] 12%|█▏        | 95/805 [00:33<03:16,  3.61it/s] 12%|█▏        | 96/805 [00:33<03:15,  3.62it/s] 12%|█▏        | 97/805 [00:34<04:02,  2.91it/s] 12%|█▏        | 98/805 [00:34<03:48,  3.10it/s] 12%|█▏        | 99/805 [00:34<03:37,  3.24it/s] 12%|█▏        | 100/805 [00:35<03:30,  3.35it/s] 13%|█▎        | 101/805 [00:35<03:25,  3.43it/s] 13%|█▎        | 102/805 [00:35<03:21,  3.49it/s] 13%|█▎        | 103/805 [00:36<03:18,  3.53it/s] 13%|█▎        | 104/805 [00:36<03:16,  3.56it/s] 13%|█▎        | 105/805 [00:36<03:15,  3.59it/s] 13%|█▎        | 106/805 [00:36<03:14,  3.60it/s] 13%|█▎        | 107/805 [00:37<03:13,  3.61it/s] 13%|█▎        | 108/805 [00:37<03:28,  3.34it/s] 14%|█▎        | 109/805 [00:37<03:23,  3.43it/s] 14%|█▎        | 110/805 [00:38<03:19,  3.48it/s] 14%|█▍        | 111/805 [00:38<03:16,  3.53it/s] 14%|█▍        | 112/805 [00:38<03:14,  3.56it/s] 14%|█▍        | 113/805 [00:38<03:13,  3.59it/s] 14%|█▍        | 114/805 [00:39<03:12,  3.60it/s] 14%|█▍        | 115/805 [00:39<03:11,  3.61it/s] 14%|█▍        | 116/805 [00:39<03:10,  3.62it/s] 15%|█▍        | 117/805 [00:39<03:09,  3.62it/s] 15%|█▍        | 118/805 [00:40<03:09,  3.63it/s] 15%|█▍        | 119/805 [00:40<03:55,  2.92it/s] 15%|█▍        | 120/805 [00:40<03:41,  3.10it/s] 15%|█▌        | 121/805 [00:41<03:30,  3.24it/s] 15%|█▌        | 122/805 [00:41<03:23,  3.36it/s] 15%|█▌        | 123/805 [00:41<03:18,  3.43it/s] 15%|█▌        | 124/805 [00:42<03:14,  3.49it/s] 16%|█▌        | 125/805 [00:42<03:12,  3.54it/s] 16%|█▌        | 126/805 [00:42<03:10,  3.57it/s] 16%|█▌        | 127/805 [00:42<03:09,  3.59it/s] 16%|█▌        | 128/805 [00:43<03:08,  3.60it/s] 16%|█▌        | 129/805 [00:43<03:07,  3.61it/s] 16%|█▌        | 130/805 [00:44<04:30,  2.49it/s] 16%|█▋        | 131/805 [00:44<04:04,  2.75it/s] 16%|█▋        | 132/805 [00:44<03:46,  2.97it/s] 17%|█▋        | 133/805 [00:44<03:33,  3.14it/s] 17%|█▋        | 134/805 [00:45<03:25,  3.27it/s] 17%|█▋        | 135/805 [00:45<03:18,  3.37it/s] 17%|█▋        | 136/805 [00:45<03:14,  3.45it/s] 17%|█▋        | 137/805 [00:46<03:10,  3.50it/s] 17%|█▋        | 138/805 [00:46<03:08,  3.54it/s] 17%|█▋        | 139/805 [00:46<03:06,  3.56it/s] 17%|█▋        | 140/805 [00:47<03:51,  2.87it/s] 18%|█▊        | 141/805 [00:47<03:36,  3.06it/s] 18%|█▊        | 142/805 [00:47<03:26,  3.21it/s] 18%|█▊        | 143/805 [00:47<03:18,  3.33it/s] 18%|█▊        | 144/805 [00:48<03:13,  3.41it/s] 18%|█▊        | 145/805 [00:48<03:09,  3.48it/s] 18%|█▊        | 146/805 [00:48<03:07,  3.52it/s] 18%|█▊        | 147/805 [00:49<03:05,  3.55it/s] 18%|█▊        | 148/805 [00:49<03:03,  3.58it/s] 19%|█▊        | 149/805 [00:49<03:02,  3.60it/s] 19%|█▊        | 150/805 [00:49<03:01,  3.61it/s] 19%|█▉        | 151/805 [00:50<03:36,  3.02it/s] 19%|█▉        | 152/805 [00:50<03:25,  3.18it/s] 19%|█▉        | 153/805 [00:50<03:17,  3.31it/s] 19%|█▉        | 154/805 [00:51<03:11,  3.40it/s] 19%|█▉        | 155/805 [00:51<03:07,  3.46it/s] 19%|█▉        | 156/805 [00:51<03:04,  3.51it/s] 20%|█▉        | 157/805 [00:52<03:02,  3.55it/s] 20%|█▉        | 158/805 [00:52<03:01,  3.57it/s] 20%|█▉        | 159/805 [00:52<02:59,  3.59it/s] 20%|█▉        | 160/805 [00:52<02:59,  3.60it/s] 20%|██        | 161/805 [00:53<02:55,  3.66it/s][INFO|trainer.py:2140] 2023-08-28 17:09:38,951 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:09:38,951 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 17:09:38,951 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.83it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.00it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.90it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.25it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.77it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.51it/s][A
  6%|▋         | 38/608 [00:00<00:12, 46.07it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.82it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.28it/s][A
  9%|▊         | 53/608 [00:01<00:16, 34.04it/s][A
 10%|▉         | 58/608 [00:01<00:14, 36.89it/s][A
 10%|█         | 63/608 [00:01<00:13, 39.22it/s][A
 11%|█         | 68/608 [00:01<00:13, 40.94it/s][A
 12%|█▏        | 73/608 [00:01<00:12, 42.35it/s][A
 13%|█▎        | 78/608 [00:01<00:12, 43.34it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 44.09it/s][A
 14%|█▍        | 88/608 [00:02<00:11, 44.64it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.52it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 44.42it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 44.54it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 44.78it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.10it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.29it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.43it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.57it/s][A
 22%|██▏       | 133/608 [00:03<00:11, 41.58it/s][A
 23%|██▎       | 138/608 [00:03<00:11, 42.65it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 43.28it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 43.71it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 44.28it/s][A
 26%|██▌       | 158/608 [00:03<00:10, 44.59it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.03it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.15it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.07it/s][A
 29%|██▉       | 178/608 [00:04<00:17, 25.28it/s][A
 30%|██▉       | 182/608 [00:04<00:16, 25.12it/s][A
 31%|███       | 187/608 [00:04<00:14, 29.64it/s][A
 32%|███▏      | 192/608 [00:04<00:12, 33.32it/s][A
 32%|███▏      | 197/608 [00:04<00:11, 36.39it/s][A
 33%|███▎      | 202/608 [00:04<00:10, 38.91it/s][A
 34%|███▍      | 207/608 [00:05<00:09, 40.84it/s][A
 35%|███▍      | 212/608 [00:05<00:09, 42.20it/s][A
 36%|███▌      | 217/608 [00:05<00:09, 43.00it/s][A
 37%|███▋      | 222/608 [00:05<00:14, 26.73it/s][A
 37%|███▋      | 227/608 [00:05<00:12, 30.61it/s][A
 38%|███▊      | 232/608 [00:05<00:11, 33.97it/s][A
 39%|███▉      | 237/608 [00:05<00:10, 36.90it/s][A
 40%|███▉      | 242/608 [00:06<00:09, 39.16it/s][A
 41%|████      | 247/608 [00:06<00:08, 41.01it/s][A
 41%|████▏     | 252/608 [00:06<00:08, 42.37it/s][A
 42%|████▏     | 257/608 [00:06<00:08, 43.27it/s][A
 43%|████▎     | 262/608 [00:06<00:07, 43.51it/s][A
 44%|████▍     | 267/608 [00:06<00:07, 43.73it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.06it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.51it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.78it/s][A
 47%|████▋     | 287/608 [00:07<00:07, 45.17it/s][A
 48%|████▊     | 292/608 [00:07<00:06, 45.41it/s][A
 49%|████▉     | 297/608 [00:07<00:06, 45.50it/s][A
 50%|████▉     | 302/608 [00:07<00:07, 38.61it/s][A
 50%|█████     | 307/608 [00:07<00:07, 40.57it/s][A
 51%|█████▏    | 312/608 [00:07<00:07, 42.01it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 43.13it/s][A
 53%|█████▎    | 322/608 [00:08<00:15, 18.66it/s][A
 54%|█████▎    | 326/608 [00:08<00:15, 17.76it/s][A
 54%|█████▍    | 331/608 [00:08<00:12, 21.99it/s][A
 55%|█████▌    | 336/608 [00:08<00:10, 26.18it/s][A
 56%|█████▌    | 341/608 [00:08<00:08, 30.15it/s][A
 57%|█████▋    | 346/608 [00:09<00:07, 33.67it/s][A
 58%|█████▊    | 351/608 [00:09<00:07, 36.64it/s][A
 59%|█████▊    | 356/608 [00:09<00:06, 38.44it/s][A
 59%|█████▉    | 361/608 [00:09<00:06, 40.44it/s][A
 60%|██████    | 366/608 [00:09<00:05, 41.62it/s][A
 61%|██████    | 371/608 [00:09<00:05, 42.43it/s][A
 62%|██████▏   | 376/608 [00:09<00:05, 43.19it/s][A
 63%|██████▎   | 381/608 [00:09<00:05, 43.76it/s][A
 63%|██████▎   | 386/608 [00:09<00:05, 44.31it/s][A
 64%|██████▍   | 391/608 [00:10<00:04, 44.75it/s][A
 65%|██████▌   | 396/608 [00:10<00:04, 45.11it/s][A
 66%|██████▌   | 401/608 [00:10<00:04, 45.24it/s][A
 67%|██████▋   | 406/608 [00:10<00:04, 45.28it/s][A
 68%|██████▊   | 411/608 [00:10<00:04, 45.11it/s][A
 68%|██████▊   | 416/608 [00:10<00:04, 45.04it/s][A
 69%|██████▉   | 421/608 [00:10<00:04, 45.02it/s][A
 70%|███████   | 426/608 [00:11<00:07, 25.29it/s][A
 71%|███████   | 431/608 [00:11<00:06, 29.21it/s][A
 72%|███████▏  | 435/608 [00:12<00:06, 27.19it/s][A
 72%|███████▏  | 439/608 [00:12<00:12, 13.98it/s][A
 73%|███████▎  | 444/608 [00:12<00:09, 18.04it/s][A
 74%|███████▍  | 449/608 [00:12<00:07, 22.29it/s][A
 75%|███████▍  | 454/608 [00:12<00:05, 26.52it/s][A
 75%|███████▌  | 459/608 [00:12<00:04, 30.47it/s][A
 76%|███████▋  | 464/608 [00:12<00:04, 33.94it/s][A
 77%|███████▋  | 469/608 [00:12<00:03, 36.89it/s][A
 78%|███████▊  | 474/608 [00:12<00:03, 39.21it/s][A
 79%|███████▉  | 479/608 [00:12<00:03, 40.60it/s][A
 80%|███████▉  | 484/608 [00:13<00:02, 41.60it/s][A
 80%|████████  | 489/608 [00:13<00:02, 42.52it/s][A
 81%|████████▏ | 494/608 [00:13<00:02, 43.36it/s][A
 82%|████████▏ | 499/608 [00:13<00:02, 43.96it/s][A
 83%|████████▎ | 504/608 [00:13<00:02, 44.38it/s][A
 84%|████████▎ | 509/608 [00:13<00:02, 44.80it/s][A
 85%|████████▍ | 514/608 [00:13<00:02, 45.13it/s][A
 85%|████████▌ | 519/608 [00:13<00:01, 45.33it/s][A
 86%|████████▌ | 524/608 [00:13<00:01, 45.18it/s][A
 87%|████████▋ | 529/608 [00:14<00:01, 45.03it/s][A
 88%|████████▊ | 534/608 [00:14<00:01, 44.95it/s][A
 89%|████████▊ | 539/608 [00:14<00:01, 44.99it/s][A
 89%|████████▉ | 544/608 [00:14<00:01, 45.12it/s][A
 90%|█████████ | 549/608 [00:14<00:01, 45.17it/s][A
 91%|█████████ | 554/608 [00:14<00:01, 41.93it/s][A
 92%|█████████▏| 559/608 [00:14<00:01, 43.08it/s][A
 93%|█████████▎| 564/608 [00:14<00:01, 36.46it/s][A
 94%|█████████▎| 569/608 [00:15<00:01, 38.84it/s][A
 94%|█████████▍| 574/608 [00:15<00:00, 40.70it/s][A
 95%|█████████▌| 579/608 [00:15<00:00, 42.15it/s][A
 96%|█████████▌| 584/608 [00:15<00:00, 43.20it/s][A
 97%|█████████▋| 589/608 [00:15<00:00, 43.91it/s][A
 98%|█████████▊| 594/608 [00:15<00:00, 44.52it/s][A
 99%|█████████▊| 599/608 [00:15<00:00, 44.79it/s][A
 99%|█████████▉| 604/608 [00:15<00:00, 44.37it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:15<00:00, 44.37it/s][A 20%|██        | 161/805 [01:09<02:55,  3.66it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:09:56,509 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161
[INFO|configuration_utils.py:351] 2023-08-28 17:09:57,023 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:10:08,843 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:10:09,796 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:10:09,911 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161/special_tokens_map.json
 20%|██        | 162/805 [01:28<1:57:21, 10.95s/it] 20%|██        | 163/805 [01:29<1:23:06,  7.77s/it] 20%|██        | 164/805 [01:29<58:58,  5.52s/it]   20%|██        | 165/805 [01:30<43:05,  4.04s/it] 21%|██        | 166/805 [01:30<31:00,  2.91s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 21%|██        | 167/805 [01:30<22:33,  2.12s/it] 21%|██        | 168/805 [01:31<16:39,  1.57s/it] 21%|██        | 169/805 [01:31<12:31,  1.18s/it] 21%|██        | 170/805 [01:31<09:38,  1.10it/s] 21%|██        | 171/805 [01:31<07:37,  1.39it/s] 21%|██▏       | 172/805 [01:32<06:12,  1.70it/s] 21%|██▏       | 173/805 [01:32<05:13,  2.02it/s] 22%|██▏       | 174/805 [01:32<04:31,  2.33it/s] 22%|██▏       | 175/805 [01:33<04:12,  2.49it/s] 22%|██▏       | 176/805 [01:33<03:49,  2.74it/s] 22%|██▏       | 177/805 [01:33<03:32,  2.95it/s] 22%|██▏       | 178/805 [01:33<03:21,  3.11it/s] 22%|██▏       | 179/805 [01:34<03:13,  3.24it/s] 22%|██▏       | 180/805 [01:34<03:07,  3.33it/s] 22%|██▏       | 181/805 [01:34<03:03,  3.40it/s] 23%|██▎       | 182/805 [01:34<03:00,  3.45it/s] 23%|██▎       | 183/805 [01:35<02:58,  3.49it/s] 23%|██▎       | 184/805 [01:35<02:56,  3.51it/s] 23%|██▎       | 185/805 [01:35<02:55,  3.53it/s] 23%|██▎       | 186/805 [01:36<03:02,  3.40it/s] 23%|██▎       | 187/805 [01:36<02:59,  3.45it/s] 23%|██▎       | 188/805 [01:36<02:57,  3.49it/s] 23%|██▎       | 189/805 [01:36<02:55,  3.51it/s] 24%|██▎       | 190/805 [01:37<02:54,  3.53it/s] 24%|██▎       | 191/805 [01:37<02:53,  3.54it/s] 24%|██▍       | 192/805 [01:37<02:52,  3.55it/s] 24%|██▍       | 193/805 [01:38<02:52,  3.56it/s] 24%|██▍       | 194/805 [01:38<02:51,  3.56it/s] 24%|██▍       | 195/805 [01:38<02:51,  3.57it/s] 24%|██▍       | 196/805 [01:38<02:50,  3.57it/s] 24%|██▍       | 197/805 [01:39<03:29,  2.91it/s] 25%|██▍       | 198/805 [01:39<03:16,  3.08it/s] 25%|██▍       | 199/805 [01:39<03:08,  3.21it/s] 25%|██▍       | 200/805 [01:40<03:02,  3.32it/s] 25%|██▍       | 201/805 [01:40<02:57,  3.39it/s] 25%|██▌       | 202/805 [01:40<02:54,  3.45it/s] 25%|██▌       | 203/805 [01:41<02:52,  3.49it/s] 25%|██▌       | 204/805 [01:41<02:50,  3.52it/s] 25%|██▌       | 205/805 [01:41<02:49,  3.54it/s] 26%|██▌       | 206/805 [01:41<02:48,  3.55it/s] 26%|██▌       | 207/805 [01:42<03:53,  2.57it/s] 26%|██▌       | 208/805 [01:42<03:32,  2.80it/s] 26%|██▌       | 209/805 [01:43<03:18,  3.00it/s] 26%|██▌       | 210/805 [01:43<03:08,  3.16it/s] 26%|██▌       | 211/805 [01:43<03:00,  3.29it/s] 26%|██▋       | 212/805 [01:43<02:55,  3.39it/s] 26%|██▋       | 213/805 [01:44<02:51,  3.46it/s] 27%|██▋       | 214/805 [01:44<02:48,  3.51it/s] 27%|██▋       | 215/805 [01:44<02:46,  3.55it/s] 27%|██▋       | 216/805 [01:45<02:44,  3.57it/s] 27%|██▋       | 217/805 [01:45<03:17,  2.98it/s] 27%|██▋       | 218/805 [01:45<03:06,  3.15it/s] 27%|██▋       | 219/805 [01:46<02:58,  3.28it/s] 27%|██▋       | 220/805 [01:46<02:52,  3.38it/s] 27%|██▋       | 221/805 [01:46<02:49,  3.45it/s] 28%|██▊       | 222/805 [01:46<02:46,  3.50it/s] 28%|██▊       | 223/805 [01:47<02:44,  3.55it/s] 28%|██▊       | 224/805 [01:47<02:42,  3.57it/s] 28%|██▊       | 225/805 [01:47<02:41,  3.59it/s] 28%|██▊       | 226/805 [01:47<02:40,  3.60it/s] 28%|██▊       | 227/805 [01:48<02:39,  3.62it/s] 28%|██▊       | 228/805 [01:48<03:05,  3.10it/s] 28%|██▊       | 229/805 [01:48<02:57,  3.25it/s] 29%|██▊       | 230/805 [01:49<02:51,  3.35it/s] 29%|██▊       | 231/805 [01:49<02:47,  3.44it/s] 29%|██▉       | 232/805 [01:49<02:44,  3.49it/s] 29%|██▉       | 233/805 [01:50<02:41,  3.53it/s] 29%|██▉       | 234/805 [01:50<02:40,  3.57it/s] 29%|██▉       | 235/805 [01:50<02:38,  3.59it/s] 29%|██▉       | 236/805 [01:50<02:38,  3.60it/s] 29%|██▉       | 237/805 [01:51<02:37,  3.61it/s] 30%|██▉       | 238/805 [01:51<02:36,  3.62it/s] 30%|██▉       | 239/805 [01:51<02:56,  3.21it/s] 30%|██▉       | 240/805 [01:52<02:49,  3.32it/s] 30%|██▉       | 241/805 [01:52<02:45,  3.41it/s] 30%|███       | 242/805 [01:52<02:41,  3.48it/s] 30%|███       | 243/805 [01:52<02:39,  3.52it/s] 30%|███       | 244/805 [01:53<02:37,  3.55it/s] 30%|███       | 245/805 [01:53<02:36,  3.58it/s] 31%|███       | 246/805 [01:53<02:35,  3.60it/s] 31%|███       | 247/805 [01:54<02:34,  3.61it/s] 31%|███       | 248/805 [01:54<02:34,  3.62it/s] 31%|███       | 249/805 [01:54<02:33,  3.62it/s] 31%|███       | 250/805 [01:54<02:47,  3.31it/s] 31%|███       | 251/805 [01:55<02:42,  3.40it/s] 31%|███▏      | 252/805 [01:55<02:39,  3.47it/s] 31%|███▏      | 253/805 [01:55<02:37,  3.52it/s] 32%|███▏      | 254/805 [01:56<02:35,  3.55it/s] 32%|███▏      | 255/805 [01:56<02:34,  3.57it/s] 32%|███▏      | 256/805 [01:56<02:32,  3.59it/s] 32%|███▏      | 257/805 [01:56<02:32,  3.60it/s] 32%|███▏      | 258/805 [01:57<02:31,  3.61it/s] 32%|███▏      | 259/805 [01:57<02:30,  3.62it/s] 32%|███▏      | 260/805 [01:57<02:30,  3.62it/s] 32%|███▏      | 261/805 [01:58<02:53,  3.14it/s] 33%|███▎      | 262/805 [01:58<02:45,  3.27it/s] 33%|███▎      | 263/805 [01:58<02:40,  3.37it/s] 33%|███▎      | 264/805 [01:58<02:37,  3.44it/s] 33%|███▎      | 265/805 [01:59<02:34,  3.50it/s] 33%|███▎      | 266/805 [01:59<02:32,  3.54it/s] 33%|███▎      | 267/805 [01:59<02:30,  3.57it/s] 33%|███▎      | 268/805 [02:00<02:29,  3.59it/s] 33%|███▎      | 269/805 [02:00<02:28,  3.61it/s] 34%|███▎      | 270/805 [02:00<02:28,  3.61it/s] 34%|███▎      | 271/805 [02:00<02:27,  3.62it/s] 34%|███▍      | 272/805 [02:01<02:27,  3.62it/s] 34%|███▍      | 273/805 [02:01<02:26,  3.63it/s] 34%|███▍      | 274/805 [02:01<02:26,  3.63it/s] 34%|███▍      | 275/805 [02:01<02:26,  3.63it/s] 34%|███▍      | 276/805 [02:02<02:25,  3.63it/s] 34%|███▍      | 277/805 [02:02<02:25,  3.63it/s] 35%|███▍      | 278/805 [02:02<02:25,  3.63it/s] 35%|███▍      | 279/805 [02:03<02:24,  3.63it/s] 35%|███▍      | 280/805 [02:03<02:24,  3.63it/s] 35%|███▍      | 281/805 [02:03<02:50,  3.07it/s] 35%|███▌      | 282/805 [02:04<02:42,  3.22it/s] 35%|███▌      | 283/805 [02:04<02:36,  3.33it/s] 35%|███▌      | 284/805 [02:04<02:32,  3.42it/s] 35%|███▌      | 285/805 [02:04<02:29,  3.48it/s] 36%|███▌      | 286/805 [02:05<02:27,  3.52it/s] 36%|███▌      | 287/805 [02:05<02:25,  3.55it/s] 36%|███▌      | 288/805 [02:05<02:24,  3.58it/s] 36%|███▌      | 289/805 [02:06<02:23,  3.59it/s] 36%|███▌      | 290/805 [02:06<02:22,  3.61it/s] 36%|███▌      | 291/805 [02:06<02:22,  3.61it/s] 36%|███▋      | 292/805 [02:07<03:13,  2.65it/s] 36%|███▋      | 293/805 [02:07<02:57,  2.88it/s] 37%|███▋      | 294/805 [02:07<02:46,  3.07it/s] 37%|███▋      | 295/805 [02:07<02:38,  3.22it/s] 37%|███▋      | 296/805 [02:08<02:32,  3.33it/s] 37%|███▋      | 297/805 [02:08<02:28,  3.42it/s] 37%|███▋      | 298/805 [02:08<02:25,  3.48it/s] 37%|███▋      | 299/805 [02:09<02:23,  3.52it/s] 37%|███▋      | 300/805 [02:09<02:22,  3.55it/s] 37%|███▋      | 301/805 [02:09<02:20,  3.57it/s] 38%|███▊      | 302/805 [02:09<02:26,  3.43it/s] 38%|███▊      | 303/805 [02:10<02:23,  3.49it/s] 38%|███▊      | 304/805 [02:10<02:21,  3.54it/s] 38%|███▊      | 305/805 [02:10<02:20,  3.57it/s] 38%|███▊      | 306/805 [02:11<02:19,  3.58it/s] 38%|███▊      | 307/805 [02:11<02:18,  3.59it/s] 38%|███▊      | 308/805 [02:11<02:17,  3.61it/s] 38%|███▊      | 309/805 [02:11<02:17,  3.61it/s] 39%|███▊      | 310/805 [02:12<02:17,  3.61it/s] 39%|███▊      | 311/805 [02:12<02:16,  3.62it/s] 39%|███▉      | 312/805 [02:12<02:15,  3.63it/s] 39%|███▉      | 313/805 [02:13<02:54,  2.83it/s] 39%|███▉      | 314/805 [02:13<02:42,  3.03it/s] 39%|███▉      | 315/805 [02:13<02:33,  3.19it/s] 39%|███▉      | 316/805 [02:14<02:28,  3.30it/s] 39%|███▉      | 317/805 [02:14<02:23,  3.39it/s] 40%|███▉      | 318/805 [02:14<02:20,  3.46it/s] 40%|███▉      | 319/805 [02:14<02:18,  3.51it/s] 40%|███▉      | 320/805 [02:15<02:16,  3.54it/s] 40%|███▉      | 321/805 [02:15<02:15,  3.57it/s] 40%|████      | 322/805 [02:15<02:12,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 17:11:01,442 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:11:01,443 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 17:11:01,443 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 15.9425, 'eval_samples_per_second': 305.096, 'eval_steps_per_second': 38.137, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 58.45it/s][A
  2%|▏         | 12/608 [00:00<00:11, 50.01it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.81it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.10it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.69it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.42it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.87it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.32it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.04it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.13it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.20it/s][A
 10%|█         | 63/608 [00:01<00:12, 45.19it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.40it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.54it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.55it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.37it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.99it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.83it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 44.99it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.07it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.12it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.27it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.48it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.53it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.42it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.09it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.98it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 43.00it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 43.79it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 44.29it/s][A
 26%|██▌       | 158/608 [00:03<00:10, 44.66it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 44.96it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.17it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.13it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.01it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.77it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.72it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.97it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.14it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.33it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.41it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.44it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.35it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.19it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 44.89it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 44.85it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 44.92it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.13it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.31it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.46it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.47it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.31it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.07it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 44.73it/s][A
 46%|████▌     | 278/608 [00:06<00:10, 30.19it/s][A
 47%|████▋     | 283/608 [00:06<00:09, 33.55it/s][A
 47%|████▋     | 288/608 [00:06<00:08, 36.44it/s][A
 48%|████▊     | 293/608 [00:06<00:08, 38.78it/s][A
 49%|████▉     | 298/608 [00:06<00:07, 40.69it/s][A
 50%|████▉     | 303/608 [00:06<00:07, 42.01it/s][A
 51%|█████     | 308/608 [00:06<00:06, 43.10it/s][A
 51%|█████▏    | 313/608 [00:07<00:06, 43.58it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 43.73it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 43.87it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 44.02it/s][A
 55%|█████▍    | 333/608 [00:07<00:08, 31.75it/s][A
 56%|█████▌    | 339/608 [00:07<00:07, 36.36it/s][A
 57%|█████▋    | 344/608 [00:07<00:06, 38.63it/s][A
 57%|█████▋    | 349/608 [00:08<00:06, 40.47it/s][A
 58%|█████▊    | 354/608 [00:08<00:06, 41.90it/s][A
 59%|█████▉    | 359/608 [00:08<00:05, 42.89it/s][A
 60%|█████▉    | 364/608 [00:08<00:05, 43.79it/s][A
 61%|██████    | 369/608 [00:08<00:05, 44.40it/s][A
 62%|██████▏   | 374/608 [00:08<00:05, 44.59it/s][A
 62%|██████▏   | 379/608 [00:08<00:05, 44.36it/s][A
 63%|██████▎   | 384/608 [00:08<00:05, 44.31it/s][A
 64%|██████▍   | 389/608 [00:08<00:04, 44.33it/s][A
 65%|██████▍   | 394/608 [00:09<00:04, 44.74it/s][A
 66%|██████▌   | 399/608 [00:09<00:04, 45.05it/s][A
 66%|██████▋   | 404/608 [00:09<00:04, 41.96it/s][A
 67%|██████▋   | 409/608 [00:09<00:04, 43.04it/s][A
 68%|██████▊   | 414/608 [00:09<00:04, 43.84it/s][A
 69%|██████▉   | 419/608 [00:09<00:04, 44.42it/s][A
 70%|██████▉   | 424/608 [00:09<00:04, 44.55it/s][A
 71%|███████   | 429/608 [00:09<00:04, 44.64it/s][A
 71%|███████▏  | 434/608 [00:10<00:14, 11.94it/s][A
 72%|███████▏  | 439/608 [00:11<00:10, 15.38it/s][A
 73%|███████▎  | 444/608 [00:11<00:08, 19.17it/s][A
 74%|███████▍  | 449/608 [00:11<00:06, 23.21it/s][A
 75%|███████▍  | 454/608 [00:11<00:05, 27.23it/s][A
 75%|███████▌  | 459/608 [00:11<00:04, 31.00it/s][A
 76%|███████▋  | 464/608 [00:11<00:04, 34.31it/s][A
 77%|███████▋  | 469/608 [00:11<00:03, 37.15it/s][A
 78%|███████▊  | 474/608 [00:11<00:03, 39.08it/s][A
 79%|███████▉  | 479/608 [00:11<00:03, 40.46it/s][A
 80%|███████▉  | 484/608 [00:12<00:02, 41.50it/s][A
 80%|████████  | 489/608 [00:12<00:02, 42.52it/s][A
 81%|████████▏ | 494/608 [00:12<00:05, 22.18it/s][A
 82%|████████▏ | 499/608 [00:12<00:04, 26.20it/s][A
 83%|████████▎ | 504/608 [00:12<00:03, 29.99it/s][A
 84%|████████▎ | 509/608 [00:12<00:02, 33.43it/s][A
 85%|████████▍ | 514/608 [00:13<00:02, 36.39it/s][A
 85%|████████▌ | 519/608 [00:13<00:02, 38.78it/s][A
 86%|████████▌ | 524/608 [00:13<00:02, 40.70it/s][A
 87%|████████▋ | 529/608 [00:13<00:01, 42.02it/s][A
 88%|████████▊ | 534/608 [00:13<00:01, 42.55it/s][A
 89%|████████▊ | 539/608 [00:13<00:01, 42.99it/s][A
 89%|████████▉ | 544/608 [00:13<00:01, 43.59it/s][A
 90%|█████████ | 549/608 [00:13<00:01, 44.12it/s][A
 91%|█████████ | 554/608 [00:13<00:01, 44.51it/s][A
 92%|█████████▏| 559/608 [00:14<00:01, 44.75it/s][A
 93%|█████████▎| 564/608 [00:14<00:00, 45.05it/s][A
 94%|█████████▎| 569/608 [00:14<00:00, 45.31it/s][A
 94%|█████████▍| 574/608 [00:14<00:00, 45.27it/s][A
 95%|█████████▌| 579/608 [00:14<00:00, 44.99it/s][A
 96%|█████████▌| 584/608 [00:14<00:00, 44.85it/s][A
 97%|█████████▋| 589/608 [00:14<00:00, 44.82it/s][A
 98%|█████████▊| 594/608 [00:14<00:00, 44.97it/s][A
 99%|█████████▊| 599/608 [00:14<00:00, 45.07it/s][A
 99%|█████████▉| 604/608 [00:15<00:00, 45.21it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:15<00:00, 45.21it/s][A 40%|████      | 322/805 [02:31<02:12,  3.64it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:11:18,129 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322
[INFO|configuration_utils.py:351] 2023-08-28 17:11:18,434 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:11:28,520 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:11:29,715 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:11:30,687 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322/special_tokens_map.json
 40%|████      | 323/805 [02:50<1:25:57, 10.70s/it] 40%|████      | 324/805 [02:51<1:02:03,  7.74s/it] 40%|████      | 325/805 [02:51<44:01,  5.50s/it]   40%|████      | 326/805 [02:52<31:25,  3.94s/it] 41%|████      | 327/805 [02:52<22:37,  2.84s/it] 41%|████      | 328/805 [02:52<16:27,  2.07s/it] 41%|████      | 329/805 [02:52<12:10,  1.53s/it] 41%|████      | 330/805 [02:53<09:09,  1.16s/it] 41%|████      | 331/805 [02:53<07:03,  1.12it/s] 41%|████      | 332/805 [02:53<05:35,  1.41it/s] 41%|████▏     | 333/805 [02:54<04:45,  1.65it/s] 41%|████▏     | 334/805 [02:54<03:58,  1.97it/s] 42%|████▏     | 335/805 [02:54<03:26,  2.28it/s] 42%|████▏     | 336/805 [02:55<03:03,  2.56it/s] 42%|████▏     | 337/805 [02:55<02:47,  2.80it/s] 42%|████▏     | 338/805 [02:55<02:36,  2.99it/s] 42%|████▏     | 339/805 [02:55<02:28,  3.15it/s] 42%|████▏     | 340/805 [02:56<02:22,  3.26it/s] 42%|████▏     | 341/805 [02:56<02:18,  3.35it/s] 42%|████▏     | 342/805 [02:56<02:15,  3.42it/s] 43%|████▎     | 343/805 [02:56<02:13,  3.46it/s] 43%|████▎     | 344/805 [02:57<03:07,  2.45it/s] 43%|████▎     | 345/805 [02:57<02:49,  2.71it/s] 43%|████▎     | 346/805 [02:58<02:36,  2.92it/s] 43%|████▎     | 347/805 [02:58<02:28,  3.09it/s] 43%|████▎     | 348/805 [02:58<02:21,  3.22it/s] 43%|████▎     | 349/805 [02:59<02:17,  3.33it/s] 43%|████▎     | 350/805 [02:59<02:14,  3.39it/s] 44%|████▎     | 351/805 [02:59<02:11,  3.45it/s] 44%|████▎     | 352/805 [02:59<02:10,  3.48it/s] 44%|████▍     | 353/805 [03:00<02:08,  3.51it/s] 44%|████▍     | 354/805 [03:00<02:21,  3.18it/s] 44%|████▍     | 355/805 [03:00<02:16,  3.29it/s] 44%|████▍     | 356/805 [03:01<02:13,  3.37it/s] 44%|████▍     | 357/805 [03:01<02:10,  3.43it/s] 44%|████▍     | 358/805 [03:01<02:08,  3.47it/s] 45%|████▍     | 359/805 [03:01<02:07,  3.50it/s] 45%|████▍     | 360/805 [03:02<02:13,  3.34it/s] 45%|████▍     | 361/805 [03:02<02:10,  3.41it/s] 45%|████▍     | 362/805 [03:02<02:08,  3.45it/s] 45%|████▌     | 363/805 [03:03<02:06,  3.49it/s] 45%|████▌     | 364/805 [03:03<02:05,  3.51it/s] 45%|████▌     | 365/805 [03:03<02:04,  3.53it/s] 45%|████▌     | 366/805 [03:03<02:03,  3.54it/s] 46%|████▌     | 367/805 [03:04<02:03,  3.55it/s] 46%|████▌     | 368/805 [03:04<02:02,  3.56it/s] 46%|████▌     | 369/805 [03:04<02:02,  3.56it/s] 46%|████▌     | 370/805 [03:05<02:02,  3.56it/s] 46%|████▌     | 371/805 [03:05<02:52,  2.51it/s] 46%|████▌     | 372/805 [03:06<02:37,  2.76it/s] 46%|████▋     | 373/805 [03:06<02:25,  2.96it/s] 46%|████▋     | 374/805 [03:06<02:17,  3.13it/s] 47%|████▋     | 375/805 [03:06<02:12,  3.25it/s] 47%|████▋     | 376/805 [03:07<02:08,  3.35it/s] 47%|████▋     | 377/805 [03:07<02:05,  3.41it/s] 47%|████▋     | 378/805 [03:07<02:03,  3.46it/s] 47%|████▋     | 379/805 [03:08<02:01,  3.49it/s] 47%|████▋     | 380/805 [03:08<02:00,  3.52it/s] 47%|████▋     | 381/805 [03:08<02:04,  3.40it/s] 47%|████▋     | 382/805 [03:08<02:02,  3.45it/s] 48%|████▊     | 383/805 [03:09<02:01,  3.49it/s] 48%|████▊     | 384/805 [03:09<01:59,  3.51it/s] 48%|████▊     | 385/805 [03:09<01:59,  3.53it/s] 48%|████▊     | 386/805 [03:09<01:58,  3.54it/s] 48%|████▊     | 387/805 [03:10<01:57,  3.55it/s] 48%|████▊     | 388/805 [03:10<01:57,  3.56it/s] 48%|████▊     | 389/805 [03:10<01:56,  3.56it/s] 48%|████▊     | 390/805 [03:11<01:56,  3.56it/s] 49%|████▊     | 391/805 [03:11<01:55,  3.57it/s] 49%|████▊     | 392/805 [03:11<02:01,  3.39it/s] 49%|████▉     | 393/805 [03:12<01:59,  3.45it/s] 49%|████▉     | 394/805 [03:12<01:57,  3.49it/s] 49%|████▉     | 395/805 [03:12<01:56,  3.52it/s] 49%|████▉     | 396/805 [03:12<01:55,  3.54it/s] 49%|████▉     | 397/805 [03:13<01:54,  3.55it/s] 49%|████▉     | 398/805 [03:13<01:54,  3.56it/s] 50%|████▉     | 399/805 [03:13<01:53,  3.57it/s] 50%|████▉     | 400/805 [03:13<01:53,  3.57it/s] 50%|████▉     | 401/805 [03:14<01:53,  3.57it/s] 50%|████▉     | 402/805 [03:14<01:52,  3.57it/s] 50%|█████     | 403/805 [03:14<02:13,  3.00it/s] 50%|█████     | 404/805 [03:15<02:07,  3.15it/s] 50%|█████     | 405/805 [03:15<02:02,  3.27it/s] 50%|█████     | 406/805 [03:15<01:59,  3.35it/s] 51%|█████     | 407/805 [03:16<01:56,  3.43it/s] 51%|█████     | 408/805 [03:16<01:53,  3.49it/s] 51%|█████     | 409/805 [03:16<01:52,  3.53it/s] 51%|█████     | 410/805 [03:16<01:50,  3.56it/s] 51%|█████     | 411/805 [03:17<01:50,  3.58it/s] 51%|█████     | 412/805 [03:17<01:49,  3.60it/s] 51%|█████▏    | 413/805 [03:17<01:48,  3.61it/s] 51%|█████▏    | 414/805 [03:18<02:17,  2.84it/s] 52%|█████▏    | 415/805 [03:18<02:08,  3.04it/s] 52%|█████▏    | 416/805 [03:18<02:01,  3.20it/s] 52%|█████▏    | 417/805 [03:19<01:56,  3.32it/s] 52%|█████▏    | 418/805 [03:19<01:53,  3.40it/s] 52%|█████▏    | 419/805 [03:19<01:51,  3.47it/s] 52%|█████▏    | 420/805 [03:19<01:49,  3.51it/s] 52%|█████▏    | 421/805 [03:20<01:48,  3.55it/s] 52%|█████▏    | 422/805 [03:20<01:47,  3.57it/s] 53%|█████▎    | 423/805 [03:20<01:46,  3.59it/s] 53%|█████▎    | 424/805 [03:21<02:11,  2.89it/s] 53%|█████▎    | 425/805 [03:21<02:03,  3.08it/s] 53%|█████▎    | 426/805 [03:21<01:57,  3.22it/s] 53%|█████▎    | 427/805 [03:22<01:53,  3.34it/s] 53%|█████▎    | 428/805 [03:22<01:50,  3.43it/s] 53%|█████▎    | 429/805 [03:22<01:47,  3.48it/s] 53%|█████▎    | 430/805 [03:22<01:46,  3.53it/s] 54%|█████▎    | 431/805 [03:23<01:44,  3.56it/s] 54%|█████▎    | 432/805 [03:23<01:55,  3.22it/s] 54%|█████▍    | 433/805 [03:23<01:51,  3.32it/s] 54%|█████▍    | 434/805 [03:24<02:08,  2.88it/s] 54%|█████▍    | 435/805 [03:24<02:00,  3.07it/s] 54%|█████▍    | 436/805 [03:24<01:54,  3.22it/s] 54%|█████▍    | 437/805 [03:25<01:50,  3.33it/s] 54%|█████▍    | 438/805 [03:25<01:47,  3.42it/s] 55%|█████▍    | 439/805 [03:27<04:26,  1.37it/s] 55%|█████▍    | 440/805 [03:27<03:54,  1.56it/s] 55%|█████▍    | 441/805 [03:27<03:14,  1.88it/s] 55%|█████▍    | 442/805 [03:28<02:45,  2.19it/s] 55%|█████▌    | 443/805 [03:28<02:26,  2.48it/s] 55%|█████▌    | 444/805 [03:28<02:12,  2.73it/s] 55%|█████▌    | 445/805 [03:28<02:02,  2.94it/s] 55%|█████▌    | 446/805 [03:29<01:55,  3.10it/s] 56%|█████▌    | 447/805 [03:29<01:50,  3.23it/s] 56%|█████▌    | 448/805 [03:29<01:47,  3.33it/s] 56%|█████▌    | 449/805 [03:30<01:44,  3.40it/s] 56%|█████▌    | 450/805 [03:30<01:42,  3.45it/s] 56%|█████▌    | 451/805 [03:30<01:46,  3.33it/s] 56%|█████▌    | 452/805 [03:30<01:43,  3.40it/s] 56%|█████▋    | 453/805 [03:31<01:41,  3.45it/s] 56%|█████▋    | 454/805 [03:31<01:40,  3.49it/s] 57%|█████▋    | 455/805 [03:31<01:39,  3.52it/s] 57%|█████▋    | 456/805 [03:32<01:38,  3.55it/s] 57%|█████▋    | 457/805 [03:32<01:37,  3.57it/s] 57%|█████▋    | 458/805 [03:32<01:36,  3.59it/s] 57%|█████▋    | 459/805 [03:33<01:48,  3.20it/s] 57%|█████▋    | 460/805 [03:33<01:43,  3.32it/s] 57%|█████▋    | 461/805 [03:33<01:40,  3.41it/s] 57%|█████▋    | 462/805 [03:33<01:38,  3.47it/s] 58%|█████▊    | 463/805 [03:34<01:37,  3.51it/s] 58%|█████▊    | 464/805 [03:34<01:36,  3.55it/s] 58%|█████▊    | 465/805 [03:34<01:35,  3.57it/s] 58%|█████▊    | 466/805 [03:34<01:34,  3.59it/s] 58%|█████▊    | 467/805 [03:35<01:33,  3.60it/s] 58%|█████▊    | 468/805 [03:35<01:33,  3.61it/s] 58%|█████▊    | 469/805 [03:35<01:32,  3.62it/s] 58%|█████▊    | 470/805 [03:36<02:14,  2.48it/s] 59%|█████▊    | 471/805 [03:36<02:17,  2.42it/s] 59%|█████▊    | 472/805 [03:37<02:03,  2.69it/s] 59%|█████▉    | 473/805 [03:37<01:53,  2.91it/s] 59%|█████▉    | 474/805 [03:37<01:46,  3.10it/s] 59%|█████▉    | 475/805 [03:38<01:41,  3.24it/s] 59%|█████▉    | 476/805 [03:38<01:38,  3.35it/s] 59%|█████▉    | 477/805 [03:38<01:35,  3.43it/s] 59%|█████▉    | 478/805 [03:38<01:33,  3.49it/s] 60%|█████▉    | 479/805 [03:39<01:52,  2.90it/s] 60%|█████▉    | 480/805 [03:39<01:45,  3.09it/s] 60%|█████▉    | 481/805 [03:39<01:40,  3.23it/s] 60%|█████▉    | 482/805 [03:40<01:36,  3.34it/s] 60%|██████    | 483/805 [03:40<01:32,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 17:12:26,118 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:12:26,118 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 17:12:26,118 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 15.1938, 'eval_samples_per_second': 320.131, 'eval_steps_per_second': 40.016, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.83it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.52it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.81it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.06it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.41it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.73it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.31it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.05it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.19it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.33it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.42it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.55it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.53it/s][A
 12%|█▏        | 73/608 [00:01<00:21, 25.36it/s][A
 13%|█▎        | 78/608 [00:01<00:18, 29.22it/s][A
 14%|█▎        | 83/608 [00:02<00:16, 32.71it/s][A
 14%|█▍        | 88/608 [00:02<00:14, 35.83it/s][A
 15%|█▌        | 93/608 [00:02<00:13, 38.35it/s][A
 16%|█▌        | 98/608 [00:02<00:12, 40.35it/s][A
 17%|█▋        | 103/608 [00:02<00:12, 41.89it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 42.84it/s][A
 19%|█▊        | 113/608 [00:02<00:11, 43.25it/s][A
 19%|█▉        | 118/608 [00:02<00:11, 43.45it/s][A
 20%|██        | 123/608 [00:02<00:11, 43.62it/s][A
 21%|██        | 128/608 [00:03<00:10, 44.06it/s][A
 22%|██▏       | 133/608 [00:03<00:10, 44.48it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.84it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.14it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.39it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.42it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.27it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 44.98it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 44.82it/s][A
 28%|██▊       | 173/608 [00:04<00:09, 44.85it/s][A
 29%|██▉       | 178/608 [00:04<00:09, 45.05it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.31it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.44it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.51it/s][A
 33%|███▎      | 198/608 [00:04<00:12, 33.04it/s][A
 33%|███▎      | 203/608 [00:04<00:11, 35.94it/s][A
 34%|███▍      | 208/608 [00:04<00:10, 38.34it/s][A
 35%|███▌      | 213/608 [00:05<00:09, 40.34it/s][A
 36%|███▌      | 218/608 [00:05<00:09, 41.83it/s][A
 37%|███▋      | 223/608 [00:05<00:08, 42.99it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 43.74it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 44.28it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 44.23it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 44.18it/s][A
 41%|████      | 248/608 [00:05<00:08, 44.11it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 44.38it/s][A
 42%|████▏     | 258/608 [00:06<00:07, 44.73it/s][A
 43%|████▎     | 263/608 [00:06<00:07, 44.98it/s][A
 44%|████▍     | 268/608 [00:06<00:07, 45.24it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.34it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.49it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.34it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.07it/s][A
 48%|████▊     | 293/608 [00:06<00:07, 44.74it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 44.86it/s][A
 50%|████▉     | 303/608 [00:07<00:06, 44.99it/s][A
 51%|█████     | 308/608 [00:07<00:06, 45.17it/s][A
 51%|█████▏    | 313/608 [00:07<00:06, 45.36it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.44it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.53it/s][A
 54%|█████▍    | 328/608 [00:07<00:07, 39.29it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 41.04it/s][A
 56%|█████▌    | 338/608 [00:07<00:06, 42.29it/s][A
 56%|█████▋    | 343/608 [00:08<00:06, 43.20it/s][A
 57%|█████▋    | 348/608 [00:08<00:05, 43.88it/s][A
 58%|█████▊    | 353/608 [00:08<00:05, 44.40it/s][A
 59%|█████▉    | 358/608 [00:08<00:05, 44.74it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.00it/s][A
 61%|██████    | 368/608 [00:08<00:05, 44.67it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 44.56it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 44.71it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.02it/s][A
 64%|██████▍   | 388/608 [00:09<00:04, 45.13it/s][A
 65%|██████▍   | 393/608 [00:09<00:04, 45.24it/s][A
 65%|██████▌   | 398/608 [00:09<00:04, 45.41it/s][A
 66%|██████▋   | 403/608 [00:09<00:04, 45.41it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.31it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.02it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 44.88it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.02it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.13it/s][A
 71%|███████   | 433/608 [00:10<00:03, 45.27it/s][A
 72%|███████▏  | 438/608 [00:10<00:03, 45.25it/s][A
 73%|███████▎  | 443/608 [00:10<00:03, 45.29it/s][A
 74%|███████▎  | 448/608 [00:10<00:03, 45.40it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.33it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.13it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 37.75it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 39.82it/s][A
 78%|███████▊  | 473/608 [00:10<00:03, 41.38it/s][A
 79%|███████▊  | 478/608 [00:11<00:03, 42.55it/s][A
 79%|███████▉  | 483/608 [00:11<00:02, 43.44it/s][A
 80%|████████  | 488/608 [00:11<00:02, 44.02it/s][A
 81%|████████  | 493/608 [00:11<00:02, 44.62it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 44.89it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 44.65it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 44.42it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 44.55it/s][A
 85%|████████▌ | 518/608 [00:11<00:02, 44.76it/s][A
 86%|████████▌ | 523/608 [00:12<00:01, 44.94it/s][A
 87%|████████▋ | 528/608 [00:12<00:01, 45.19it/s][A
 88%|████████▊ | 533/608 [00:12<00:01, 45.39it/s][A
 88%|████████▊ | 538/608 [00:12<00:01, 45.49it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.43it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.05it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 44.86it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 44.83it/s][A
 93%|█████████▎| 563/608 [00:12<00:01, 44.87it/s][A
 93%|█████████▎| 568/608 [00:13<00:00, 45.09it/s][A
 94%|█████████▍| 573/608 [00:13<00:00, 45.22it/s][A
 95%|█████████▌| 578/608 [00:13<00:00, 45.43it/s][A
 96%|█████████▌| 583/608 [00:13<00:00, 45.49it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.32it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.11it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 32.88it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 35.97it/s][A
100%|██████████| 608/608 [00:14<00:00, 38.42it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:14<00:00, 38.42it/s][A 60%|██████    | 483/805 [03:54<01:32,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:12:40,887 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483
[INFO|configuration_utils.py:351] 2023-08-28 17:12:41,209 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:12:50,140 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:12:51,019 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:12:51,183 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483/special_tokens_map.json
 60%|██████    | 484/805 [04:08<46:39,  8.72s/it] 60%|██████    | 485/805 [04:09<33:06,  6.21s/it] 60%|██████    | 486/805 [04:09<23:33,  4.43s/it] 60%|██████    | 487/805 [04:09<16:52,  3.19s/it] 61%|██████    | 488/805 [04:10<12:13,  2.31s/it] 61%|██████    | 489/805 [04:10<08:58,  1.70s/it] 61%|██████    | 490/805 [04:10<06:41,  1.28s/it] 61%|██████    | 491/805 [04:10<05:06,  1.02it/s] 61%|██████    | 492/805 [04:11<04:00,  1.30it/s] 61%|██████    | 493/805 [04:11<03:13,  1.61it/s] 61%|██████▏   | 494/805 [04:11<02:41,  1.93it/s] 61%|██████▏   | 495/805 [04:11<02:18,  2.24it/s] 62%|██████▏   | 496/805 [04:12<02:22,  2.17it/s] 62%|██████▏   | 497/805 [04:12<02:05,  2.46it/s] 62%|██████▏   | 498/805 [04:13<01:53,  2.71it/s] 62%|██████▏   | 499/805 [04:13<01:44,  2.92it/s] 62%|██████▏   | 500/805 [04:13<01:38,  3.09it/s]                                                  62%|██████▏   | 500/805 [04:13<01:38,  3.09it/s] 62%|██████▏   | 501/805 [04:13<01:34,  3.22it/s] 62%|██████▏   | 502/805 [04:14<01:31,  3.32it/s] 62%|██████▏   | 503/805 [04:14<01:29,  3.39it/s] 63%|██████▎   | 504/805 [04:14<01:27,  3.44it/s] 63%|██████▎   | 505/805 [04:14<01:26,  3.48it/s] 63%|██████▎   | 506/805 [04:15<01:31,  3.27it/s] 63%|██████▎   | 507/805 [04:15<01:28,  3.36it/s] 63%|██████▎   | 508/805 [04:15<01:26,  3.42it/s] 63%|██████▎   | 509/805 [04:16<01:25,  3.47it/s] 63%|██████▎   | 510/805 [04:16<01:24,  3.50it/s] 63%|██████▎   | 511/805 [04:16<01:23,  3.52it/s] 64%|██████▎   | 512/805 [04:17<01:22,  3.54it/s] 64%|██████▎   | 513/805 [04:17<01:22,  3.55it/s] 64%|██████▍   | 514/805 [04:17<01:21,  3.56it/s] 64%|██████▍   | 515/805 [04:17<01:21,  3.57it/s] 64%|██████▍   | 516/805 [04:18<01:20,  3.57it/s] 64%|██████▍   | 517/805 [04:18<01:37,  2.97it/s] 64%|██████▍   | 518/805 [04:18<01:31,  3.12it/s] 64%|██████▍   | 519/805 [04:19<01:28,  3.24it/s] 65%|██████▍   | 520/805 [04:19<01:25,  3.34it/s] 65%|██████▍   | 521/805 [04:19<01:23,  3.40it/s] 65%|██████▍   | 522/805 [04:19<01:21,  3.45it/s] 65%|██████▍   | 523/805 [04:20<01:20,  3.49it/s] 65%|██████▌   | 524/805 [04:20<01:19,  3.51it/s] 65%|██████▌   | 525/805 [04:20<01:19,  3.53it/s] 65%|██████▌   | 526/805 [04:21<01:18,  3.54it/s] 65%|██████▌   | 527/805 [04:21<01:18,  3.55it/s] 66%|██████▌   | 528/805 [04:21<01:40,  2.77it/s] 66%|██████▌   | 529/805 [04:22<01:32,  2.97it/s] 66%|██████▌   | 530/805 [04:22<01:27,  3.13it/s] 66%|██████▌   | 531/805 [04:22<01:24,  3.25it/s] 66%|██████▌   | 532/805 [04:23<01:21,  3.34it/s] 66%|██████▌   | 533/805 [04:23<01:19,  3.41it/s] 66%|██████▋   | 534/805 [04:23<01:28,  3.07it/s] 66%|██████▋   | 535/805 [04:24<01:24,  3.21it/s] 67%|██████▋   | 536/805 [04:24<01:21,  3.32it/s] 67%|██████▋   | 537/805 [04:24<01:18,  3.40it/s] 67%|██████▋   | 538/805 [04:24<01:25,  3.11it/s] 67%|██████▋   | 539/805 [04:25<01:21,  3.25it/s] 67%|██████▋   | 540/805 [04:25<01:18,  3.35it/s] 67%|██████▋   | 541/805 [04:25<01:20,  3.28it/s] 67%|██████▋   | 542/805 [04:26<01:18,  3.36it/s] 67%|██████▋   | 543/805 [04:27<02:53,  1.51it/s] 68%|██████▊   | 544/805 [04:27<02:26,  1.78it/s] 68%|██████▊   | 545/805 [04:28<02:03,  2.10it/s] 68%|██████▊   | 546/805 [04:28<01:48,  2.39it/s] 68%|██████▊   | 547/805 [04:28<01:37,  2.65it/s] 68%|██████▊   | 548/805 [04:29<01:29,  2.88it/s] 68%|██████▊   | 549/805 [04:29<01:23,  3.05it/s] 68%|██████▊   | 550/805 [04:29<01:19,  3.19it/s] 68%|██████▊   | 551/805 [04:29<01:17,  3.29it/s] 69%|██████▊   | 552/805 [04:30<01:15,  3.37it/s] 69%|██████▊   | 553/805 [04:30<01:13,  3.43it/s] 69%|██████▉   | 554/805 [04:30<01:12,  3.47it/s] 69%|██████▉   | 555/805 [04:31<01:14,  3.36it/s] 69%|██████▉   | 556/805 [04:31<01:12,  3.43it/s] 69%|██████▉   | 557/805 [04:31<01:11,  3.49it/s] 69%|██████▉   | 558/805 [04:31<01:09,  3.53it/s] 69%|██████▉   | 559/805 [04:32<01:09,  3.56it/s] 70%|██████▉   | 560/805 [04:32<01:08,  3.58it/s] 70%|██████▉   | 561/805 [04:32<01:07,  3.59it/s] 70%|██████▉   | 562/805 [04:33<01:07,  3.60it/s] 70%|██████▉   | 563/805 [04:33<01:07,  3.61it/s] 70%|███████   | 564/805 [04:33<01:06,  3.62it/s] 70%|███████   | 565/805 [04:33<01:06,  3.62it/s] 70%|███████   | 566/805 [04:34<01:07,  3.52it/s] 70%|███████   | 567/805 [04:34<01:07,  3.55it/s] 71%|███████   | 568/805 [04:35<01:29,  2.64it/s] 71%|███████   | 569/805 [04:35<01:22,  2.87it/s] 71%|███████   | 570/805 [04:35<01:16,  3.06it/s] 71%|███████   | 571/805 [04:35<01:12,  3.22it/s] 71%|███████   | 572/805 [04:36<01:09,  3.33it/s] 71%|███████   | 573/805 [04:36<01:07,  3.42it/s] 71%|███████▏  | 574/805 [04:36<01:06,  3.48it/s] 71%|███████▏  | 575/805 [04:36<01:05,  3.52it/s] 72%|███████▏  | 576/805 [04:37<01:04,  3.55it/s] 72%|███████▏  | 577/805 [04:37<01:03,  3.57it/s] 72%|███████▏  | 578/805 [04:37<01:06,  3.42it/s] 72%|███████▏  | 579/805 [04:38<01:04,  3.48it/s] 72%|███████▏  | 580/805 [04:38<01:03,  3.52it/s] 72%|███████▏  | 581/805 [04:38<01:03,  3.55it/s] 72%|███████▏  | 582/805 [04:38<01:02,  3.57it/s] 72%|███████▏  | 583/805 [04:39<01:01,  3.59it/s] 73%|███████▎  | 584/805 [04:39<01:01,  3.60it/s] 73%|███████▎  | 585/805 [04:39<01:01,  3.61it/s] 73%|███████▎  | 586/805 [04:40<01:00,  3.61it/s] 73%|███████▎  | 587/805 [04:40<01:00,  3.61it/s] 73%|███████▎  | 588/805 [04:40<00:59,  3.62it/s] 73%|███████▎  | 589/805 [04:40<01:05,  3.28it/s] 73%|███████▎  | 590/805 [04:41<01:03,  3.38it/s] 73%|███████▎  | 591/805 [04:41<01:02,  3.45it/s] 74%|███████▎  | 592/805 [04:41<01:00,  3.50it/s] 74%|███████▎  | 593/805 [04:42<00:59,  3.54it/s] 74%|███████▍  | 594/805 [04:42<00:59,  3.56it/s] 74%|███████▍  | 595/805 [04:42<00:58,  3.58it/s] 74%|███████▍  | 596/805 [04:42<00:58,  3.60it/s] 74%|███████▍  | 597/805 [04:43<00:57,  3.60it/s] 74%|███████▍  | 598/805 [04:43<00:57,  3.61it/s] 74%|███████▍  | 599/805 [04:43<00:56,  3.61it/s] 75%|███████▍  | 600/805 [04:44<00:59,  3.47it/s] 75%|███████▍  | 601/805 [04:44<00:57,  3.52it/s] 75%|███████▍  | 602/805 [04:44<00:57,  3.55it/s] 75%|███████▍  | 603/805 [04:44<00:56,  3.57it/s] 75%|███████▌  | 604/805 [04:45<00:56,  3.58it/s] 75%|███████▌  | 605/805 [04:45<00:55,  3.59it/s] 75%|███████▌  | 606/805 [04:45<00:55,  3.60it/s] 75%|███████▌  | 607/805 [04:45<00:54,  3.61it/s] 76%|███████▌  | 608/805 [04:46<00:54,  3.61it/s] 76%|███████▌  | 609/805 [04:46<00:54,  3.61it/s] 76%|███████▌  | 610/805 [04:46<00:53,  3.61it/s] 76%|███████▌  | 611/805 [04:47<00:57,  3.38it/s] 76%|███████▌  | 612/805 [04:47<00:55,  3.45it/s] 76%|███████▌  | 613/805 [04:47<00:54,  3.50it/s] 76%|███████▋  | 614/805 [04:47<00:53,  3.54it/s] 76%|███████▋  | 615/805 [04:48<00:53,  3.56it/s] 77%|███████▋  | 616/805 [04:48<00:52,  3.57it/s] 77%|███████▋  | 617/805 [04:48<00:52,  3.59it/s] 77%|███████▋  | 618/805 [04:49<00:51,  3.60it/s] 77%|███████▋  | 619/805 [04:49<00:51,  3.60it/s] 77%|███████▋  | 620/805 [04:49<00:51,  3.61it/s] 77%|███████▋  | 621/805 [04:49<00:50,  3.61it/s] 77%|███████▋  | 622/805 [04:50<00:53,  3.39it/s] 77%|███████▋  | 623/805 [04:50<00:52,  3.46it/s] 78%|███████▊  | 624/805 [04:50<00:51,  3.51it/s] 78%|███████▊  | 625/805 [04:51<00:50,  3.54it/s] 78%|███████▊  | 626/805 [04:51<00:50,  3.56it/s] 78%|███████▊  | 627/805 [04:51<00:49,  3.58it/s] 78%|███████▊  | 628/805 [04:51<00:49,  3.59it/s] 78%|███████▊  | 629/805 [04:52<00:48,  3.61it/s] 78%|███████▊  | 630/805 [04:52<00:48,  3.61it/s] 78%|███████▊  | 631/805 [04:52<00:48,  3.61it/s] 79%|███████▊  | 632/805 [04:52<00:47,  3.62it/s] 79%|███████▊  | 633/805 [04:53<01:20,  2.13it/s] 79%|███████▉  | 634/805 [04:54<01:10,  2.43it/s] 79%|███████▉  | 635/805 [04:54<01:02,  2.70it/s] 79%|███████▉  | 636/805 [04:54<00:57,  2.92it/s] 79%|███████▉  | 637/805 [04:55<00:54,  3.11it/s] 79%|███████▉  | 638/805 [04:55<00:51,  3.24it/s] 79%|███████▉  | 639/805 [04:55<00:49,  3.35it/s] 80%|███████▉  | 640/805 [04:55<00:48,  3.43it/s] 80%|███████▉  | 641/805 [04:56<00:47,  3.48it/s] 80%|███████▉  | 642/805 [04:56<00:47,  3.45it/s] 80%|███████▉  | 643/805 [04:56<00:46,  3.50it/s] 80%|████████  | 644/805 [04:56<00:44,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 17:13:42,639 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:13:42,640 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 17:13:42,640 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 14.1233, 'eval_samples_per_second': 344.394, 'eval_steps_per_second': 43.049, 'epoch': 3.0}
{'loss': nan, 'learning_rate': 2.1940993788819876e-05, 'epoch': 3.11}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.62it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.32it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.88it/s][A
  4%|▎         | 22/608 [00:00<00:12, 47.13it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.69it/s][A
  5%|▌         | 32/608 [00:00<00:12, 46.15it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.62it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.28it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.24it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.34it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.38it/s][A
 10%|█         | 62/608 [00:01<00:11, 45.57it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.63it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.66it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.52it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.31it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.08it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 45.13it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.18it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.25it/s][A
 18%|█▊        | 107/608 [00:02<00:13, 37.43it/s][A
 18%|█▊        | 112/608 [00:02<00:12, 39.64it/s][A
 19%|█▉        | 117/608 [00:02<00:11, 41.33it/s][A
 20%|██        | 122/608 [00:02<00:11, 42.62it/s][A
 21%|██        | 127/608 [00:02<00:11, 43.49it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.21it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.65it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.93it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.65it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 44.48it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 44.71it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 44.91it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.06it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.35it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 45.48it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 45.57it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.51it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.24it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.03it/s][A
 33%|███▎      | 202/608 [00:04<00:09, 44.97it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.03it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.27it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.47it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.59it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.60it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.49it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.32it/s][A
 40%|███▉      | 242/608 [00:05<00:10, 34.73it/s][A
 41%|████      | 247/608 [00:05<00:09, 37.43it/s][A
 41%|████▏     | 252/608 [00:05<00:08, 39.56it/s][A
 42%|████▏     | 257/608 [00:05<00:08, 41.28it/s][A
 43%|████▎     | 262/608 [00:05<00:08, 42.57it/s][A
 44%|████▍     | 267/608 [00:06<00:07, 43.43it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.18it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.60it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.48it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.41it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 44.59it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 44.90it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.16it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.40it/s][A
 51%|█████▏    | 312/608 [00:07<00:06, 45.49it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 45.50it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.39it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.00it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.86it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 44.86it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.00it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.23it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.45it/s][A
 59%|█████▊    | 357/608 [00:08<00:05, 45.62it/s][A
 60%|█████▉    | 362/608 [00:08<00:11, 21.37it/s][A
 60%|██████    | 367/608 [00:08<00:09, 25.41it/s][A
 61%|██████    | 372/608 [00:08<00:08, 29.31it/s][A
 62%|██████▏   | 377/608 [00:08<00:07, 32.85it/s][A
 63%|██████▎   | 382/608 [00:08<00:06, 35.93it/s][A
 64%|██████▎   | 387/608 [00:09<00:05, 38.41it/s][A
 64%|██████▍   | 392/608 [00:09<00:05, 40.40it/s][A
 65%|██████▌   | 397/608 [00:09<00:05, 41.73it/s][A
 66%|██████▌   | 402/608 [00:09<00:04, 42.51it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 43.04it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 43.60it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.13it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 44.65it/s][A
 70%|███████   | 427/608 [00:09<00:04, 44.98it/s][A
 71%|███████   | 432/608 [00:10<00:03, 45.27it/s][A
 72%|███████▏  | 437/608 [00:10<00:03, 45.32it/s][A
 73%|███████▎  | 442/608 [00:10<00:03, 45.42it/s][A
 74%|███████▎  | 447/608 [00:10<00:03, 45.12it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.90it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.93it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.06it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.19it/s][A
 78%|███████▊  | 472/608 [00:10<00:02, 45.44it/s][A
 78%|███████▊  | 477/608 [00:11<00:02, 45.47it/s][A
 79%|███████▉  | 482/608 [00:11<00:04, 28.09it/s][A
 80%|████████  | 487/608 [00:11<00:03, 31.82it/s][A
 81%|████████  | 492/608 [00:11<00:03, 34.86it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 37.77it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 39.86it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 41.47it/s][A
 84%|████████▍ | 512/608 [00:12<00:02, 42.56it/s][A
 85%|████████▌ | 517/608 [00:12<00:02, 43.43it/s][A
 86%|████████▌ | 522/608 [00:12<00:01, 43.63it/s][A
 87%|████████▋ | 527/608 [00:12<00:01, 43.81it/s][A
 88%|████████▊ | 532/608 [00:12<00:01, 44.07it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 44.63it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.95it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.22it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.24it/s][A
 92%|█████████▏| 557/608 [00:13<00:01, 45.38it/s][A
 92%|█████████▏| 562/608 [00:13<00:01, 45.42it/s][A
 93%|█████████▎| 567/608 [00:13<00:00, 45.33it/s][A
 94%|█████████▍| 572/608 [00:13<00:00, 45.22it/s][A
 95%|█████████▍| 577/608 [00:13<00:00, 45.08it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 45.23it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.34it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.30it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 45.33it/s][A
 99%|█████████▉| 602/608 [00:14<00:00, 45.44it/s][A
100%|█████████▉| 607/608 [00:14<00:00, 45.41it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:14<00:00, 45.41it/s][A 80%|████████  | 644/805 [05:11<00:44,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:13:58,416 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644
[INFO|configuration_utils.py:351] 2023-08-28 17:13:58,688 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:14:08,050 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:14:08,705 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:14:08,900 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644/special_tokens_map.json
 80%|████████  | 645/805 [05:29<26:41, 10.01s/it] 80%|████████  | 646/805 [05:30<19:09,  7.23s/it] 80%|████████  | 647/805 [05:30<13:32,  5.14s/it] 80%|████████  | 648/805 [05:30<09:37,  3.68s/it] 81%|████████  | 649/805 [05:31<06:54,  2.66s/it] 81%|████████  | 650/805 [05:31<05:01,  1.94s/it] 81%|████████  | 651/805 [05:31<03:42,  1.44s/it] 81%|████████  | 652/805 [05:32<02:47,  1.09s/it] 81%|████████  | 653/805 [05:32<02:08,  1.18it/s] 81%|████████  | 654/805 [05:32<01:41,  1.48it/s] 81%|████████▏ | 655/805 [05:32<01:23,  1.80it/s] 81%|████████▏ | 656/805 [05:33<01:17,  1.92it/s] 82%|████████▏ | 657/805 [05:33<01:06,  2.24it/s] 82%|████████▏ | 658/805 [05:33<00:58,  2.53it/s] 82%|████████▏ | 659/805 [05:34<00:52,  2.78it/s] 82%|████████▏ | 660/805 [05:34<00:48,  2.99it/s] 82%|████████▏ | 661/805 [05:34<00:45,  3.16it/s] 82%|████████▏ | 662/805 [05:34<00:43,  3.29it/s] 82%|████████▏ | 663/805 [05:35<00:42,  3.38it/s] 82%|████████▏ | 664/805 [05:35<00:40,  3.45it/s] 83%|████████▎ | 665/805 [05:35<00:39,  3.50it/s] 83%|████████▎ | 666/805 [05:36<00:39,  3.54it/s] 83%|████████▎ | 667/805 [05:36<00:43,  3.17it/s] 83%|████████▎ | 668/805 [05:36<00:41,  3.30it/s] 83%|████████▎ | 669/805 [05:37<00:44,  3.06it/s] 83%|████████▎ | 670/805 [05:37<00:42,  3.20it/s] 83%|████████▎ | 671/805 [05:37<00:40,  3.32it/s] 83%|████████▎ | 672/805 [05:37<00:39,  3.40it/s] 84%|████████▎ | 673/805 [05:38<00:38,  3.47it/s] 84%|████████▎ | 674/805 [05:38<00:37,  3.52it/s] 84%|████████▍ | 675/805 [05:38<00:36,  3.55it/s] 84%|████████▍ | 676/805 [05:39<00:36,  3.58it/s] 84%|████████▍ | 677/805 [05:39<00:35,  3.57it/s] 84%|████████▍ | 678/805 [05:39<00:47,  2.68it/s] 84%|████████▍ | 679/805 [05:40<00:43,  2.91it/s] 84%|████████▍ | 680/805 [05:40<00:40,  3.10it/s] 85%|████████▍ | 681/805 [05:40<00:38,  3.24it/s] 85%|████████▍ | 682/805 [05:41<00:36,  3.34it/s] 85%|████████▍ | 683/805 [05:41<00:35,  3.42it/s] 85%|████████▍ | 684/805 [05:41<00:34,  3.47it/s] 85%|████████▌ | 685/805 [05:41<00:34,  3.50it/s] 85%|████████▌ | 686/805 [05:42<00:33,  3.52it/s] 85%|████████▌ | 687/805 [05:42<00:33,  3.54it/s] 85%|████████▌ | 688/805 [05:42<00:40,  2.87it/s] 86%|████████▌ | 689/805 [05:43<00:38,  3.05it/s] 86%|████████▌ | 690/805 [05:43<00:36,  3.19it/s] 86%|████████▌ | 691/805 [05:43<00:34,  3.29it/s] 86%|████████▌ | 692/805 [05:44<00:33,  3.37it/s] 86%|████████▌ | 693/805 [05:44<00:32,  3.43it/s] 86%|████████▌ | 694/805 [05:44<00:31,  3.47it/s] 86%|████████▋ | 695/805 [05:44<00:31,  3.50it/s] 86%|████████▋ | 696/805 [05:45<00:30,  3.52it/s] 87%|████████▋ | 697/805 [05:45<00:30,  3.54it/s] 87%|████████▋ | 698/805 [05:46<00:54,  1.97it/s] 87%|████████▋ | 699/805 [05:46<00:46,  2.28it/s] 87%|████████▋ | 700/805 [05:47<00:41,  2.56it/s] 87%|████████▋ | 701/805 [05:47<00:37,  2.80it/s] 87%|████████▋ | 702/805 [05:47<00:34,  2.99it/s] 87%|████████▋ | 703/805 [05:47<00:32,  3.14it/s] 87%|████████▋ | 704/805 [05:48<00:30,  3.26it/s] 88%|████████▊ | 705/805 [05:48<00:29,  3.35it/s] 88%|████████▊ | 706/805 [05:48<00:29,  3.41it/s] 88%|████████▊ | 707/805 [05:49<00:44,  2.19it/s] 88%|████████▊ | 708/805 [05:49<00:39,  2.48it/s] 88%|████████▊ | 709/805 [05:50<00:35,  2.73it/s] 88%|████████▊ | 710/805 [05:50<00:32,  2.93it/s] 88%|████████▊ | 711/805 [05:50<00:30,  3.10it/s] 88%|████████▊ | 712/805 [05:50<00:28,  3.23it/s] 89%|████████▊ | 713/805 [05:51<00:27,  3.32it/s] 89%|████████▊ | 714/805 [05:51<00:26,  3.39it/s] 89%|████████▉ | 715/805 [05:51<00:26,  3.44it/s] 89%|████████▉ | 716/805 [05:52<00:34,  2.56it/s] 89%|████████▉ | 717/805 [05:52<00:31,  2.79it/s] 89%|████████▉ | 718/805 [05:52<00:29,  2.98it/s] 89%|████████▉ | 719/805 [05:53<00:27,  3.14it/s] 89%|████████▉ | 720/805 [05:53<00:26,  3.25it/s] 90%|████████▉ | 721/805 [05:53<00:25,  3.34it/s] 90%|████████▉ | 722/805 [05:54<00:24,  3.40it/s] 90%|████████▉ | 723/805 [05:54<00:23,  3.45it/s] 90%|████████▉ | 724/805 [05:54<00:23,  3.48it/s] 90%|█████████ | 725/805 [05:54<00:22,  3.51it/s] 90%|█████████ | 726/805 [05:55<00:25,  3.16it/s] 90%|█████████ | 727/805 [05:55<00:23,  3.27it/s] 90%|█████████ | 728/805 [05:55<00:22,  3.36it/s] 91%|█████████ | 729/805 [05:56<00:22,  3.42it/s] 91%|█████████ | 730/805 [05:56<00:21,  3.47it/s] 91%|█████████ | 731/805 [05:56<00:21,  3.49it/s] 91%|█████████ | 732/805 [05:56<00:20,  3.52it/s] 91%|█████████ | 733/805 [05:57<00:20,  3.53it/s] 91%|█████████ | 734/805 [05:57<00:20,  3.55it/s] 91%|█████████▏| 735/805 [05:57<00:19,  3.55it/s] 91%|█████████▏| 736/805 [05:58<00:19,  3.56it/s] 92%|█████████▏| 737/805 [05:58<00:21,  3.15it/s] 92%|█████████▏| 738/805 [05:58<00:20,  3.26it/s] 92%|█████████▏| 739/805 [05:59<00:19,  3.34it/s] 92%|█████████▏| 740/805 [05:59<00:19,  3.41it/s] 92%|█████████▏| 741/805 [05:59<00:18,  3.45it/s] 92%|█████████▏| 742/805 [05:59<00:18,  3.48it/s] 92%|█████████▏| 743/805 [06:00<00:17,  3.51it/s] 92%|█████████▏| 744/805 [06:00<00:17,  3.52it/s] 93%|█████████▎| 745/805 [06:00<00:16,  3.54it/s] 93%|█████████▎| 746/805 [06:01<00:16,  3.54it/s] 93%|█████████▎| 747/805 [06:01<00:16,  3.55it/s] 93%|█████████▎| 748/805 [06:01<00:17,  3.35it/s] 93%|█████████▎| 749/805 [06:01<00:16,  3.41it/s] 93%|█████████▎| 750/805 [06:02<00:15,  3.46it/s] 93%|█████████▎| 751/805 [06:02<00:15,  3.49it/s] 93%|█████████▎| 752/805 [06:02<00:15,  3.51it/s] 94%|█████████▎| 753/805 [06:03<00:14,  3.53it/s] 94%|█████████▎| 754/805 [06:03<00:14,  3.56it/s] 94%|█████████▍| 755/805 [06:03<00:13,  3.57it/s] 94%|█████████▍| 756/805 [06:03<00:13,  3.58it/s] 94%|█████████▍| 757/805 [06:04<00:13,  3.59it/s] 94%|█████████▍| 758/805 [06:04<00:13,  3.60it/s] 94%|█████████▍| 759/805 [06:04<00:15,  2.97it/s] 94%|█████████▍| 760/805 [06:05<00:14,  3.14it/s] 95%|█████████▍| 761/805 [06:05<00:13,  3.27it/s] 95%|█████████▍| 762/805 [06:05<00:12,  3.37it/s] 95%|█████████▍| 763/805 [06:06<00:12,  3.44it/s] 95%|█████████▍| 764/805 [06:06<00:11,  3.49it/s] 95%|█████████▌| 765/805 [06:06<00:11,  3.53it/s] 95%|█████████▌| 766/805 [06:06<00:10,  3.55it/s] 95%|█████████▌| 767/805 [06:07<00:10,  3.57it/s] 95%|█████████▌| 768/805 [06:07<00:10,  3.58it/s] 96%|█████████▌| 769/805 [06:07<00:10,  3.59it/s] 96%|█████████▌| 770/805 [06:07<00:09,  3.60it/s] 96%|█████████▌| 771/805 [06:08<00:09,  3.61it/s] 96%|█████████▌| 772/805 [06:08<00:09,  3.61it/s] 96%|█████████▌| 773/805 [06:08<00:08,  3.61it/s] 96%|█████████▌| 774/805 [06:09<00:08,  3.61it/s] 96%|█████████▋| 775/805 [06:09<00:08,  3.62it/s] 96%|█████████▋| 776/805 [06:09<00:08,  3.62it/s] 97%|█████████▋| 777/805 [06:09<00:07,  3.62it/s] 97%|█████████▋| 778/805 [06:10<00:07,  3.42it/s] 97%|█████████▋| 779/805 [06:10<00:07,  3.48it/s] 97%|█████████▋| 780/805 [06:10<00:07,  3.52it/s] 97%|█████████▋| 781/805 [06:11<00:06,  3.55it/s] 97%|█████████▋| 782/805 [06:11<00:06,  3.57it/s] 97%|█████████▋| 783/805 [06:11<00:06,  3.58it/s] 97%|█████████▋| 784/805 [06:11<00:05,  3.59it/s] 98%|█████████▊| 785/805 [06:12<00:05,  3.60it/s] 98%|█████████▊| 786/805 [06:12<00:05,  3.61it/s] 98%|█████████▊| 787/805 [06:12<00:04,  3.61it/s] 98%|█████████▊| 788/805 [06:12<00:04,  3.61it/s] 98%|█████████▊| 789/805 [06:13<00:05,  3.03it/s] 98%|█████████▊| 790/805 [06:13<00:04,  3.19it/s] 98%|█████████▊| 791/805 [06:14<00:04,  3.31it/s] 98%|█████████▊| 792/805 [06:14<00:03,  3.39it/s] 99%|█████████▊| 793/805 [06:14<00:03,  3.45it/s] 99%|█████████▊| 794/805 [06:14<00:03,  3.50it/s] 99%|█████████▉| 795/805 [06:15<00:02,  3.54it/s] 99%|█████████▉| 796/805 [06:15<00:02,  3.56it/s] 99%|█████████▉| 797/805 [06:15<00:02,  3.58it/s] 99%|█████████▉| 798/805 [06:15<00:01,  3.59it/s] 99%|█████████▉| 799/805 [06:16<00:01,  3.59it/s] 99%|█████████▉| 800/805 [06:16<00:01,  2.96it/s]100%|█████████▉| 801/805 [06:16<00:01,  3.13it/s]100%|█████████▉| 802/805 [06:17<00:00,  3.26it/s]100%|█████████▉| 803/805 [06:17<00:00,  3.36it/s]100%|█████████▉| 804/805 [06:17<00:00,  3.43it/s]100%|██████████| 805/805 [06:18<00:00,  3.53it/s][INFO|trainer.py:2140] 2023-08-28 17:15:03,759 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:15:03,759 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 17:15:03,759 >>   Batch size = 8
{'eval_loss': 0.9957473874092102, 'eval_runtime': 14.3946, 'eval_samples_per_second': 337.904, 'eval_steps_per_second': 42.238, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.07it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.34it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.66it/s][A
  4%|▎         | 22/608 [00:00<00:12, 47.05it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.55it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.93it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.37it/s][A
  7%|▋         | 42/608 [00:00<00:12, 44.95it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.01it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.02it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.19it/s][A
 10%|█         | 62/608 [00:01<00:13, 41.81it/s][A
 11%|█         | 67/608 [00:01<00:12, 42.94it/s][A
 12%|█▏        | 72/608 [00:01<00:12, 43.76it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 44.34it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.54it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.63it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.73it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 44.72it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 44.49it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.75it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 44.91it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.12it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.28it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.35it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 45.26it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 45.24it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.98it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.84it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 44.88it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 45.00it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.19it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.25it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.33it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 45.03it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 45.05it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.91it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 44.76it/s][A
 32%|███▏      | 197/608 [00:04<00:13, 31.44it/s][A
 33%|███▎      | 202/608 [00:04<00:11, 34.59it/s][A
 34%|███▍      | 207/608 [00:04<00:10, 37.29it/s][A
 35%|███▍      | 212/608 [00:04<00:10, 39.44it/s][A
 36%|███▌      | 217/608 [00:04<00:09, 41.14it/s][A
 37%|███▋      | 222/608 [00:05<00:09, 42.50it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 43.40it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 43.99it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 43.94it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 43.93it/s][A
 41%|████      | 247/608 [00:05<00:08, 43.89it/s][A
 41%|████▏     | 252/608 [00:05<00:09, 37.09it/s][A
 42%|████▏     | 257/608 [00:05<00:08, 39.52it/s][A
 43%|████▎     | 262/608 [00:06<00:08, 41.09it/s][A
 44%|████▍     | 267/608 [00:06<00:08, 42.41it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 43.28it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 43.87it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.44it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.80it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 44.69it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 44.46it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 44.53it/s][A
 50%|█████     | 307/608 [00:07<00:06, 44.75it/s][A
 51%|█████▏    | 312/608 [00:07<00:06, 44.93it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 45.00it/s][A
 53%|█████▎    | 322/608 [00:07<00:09, 30.74it/s][A
 54%|█████▍    | 327/608 [00:07<00:08, 34.01it/s][A
 55%|█████▍    | 332/608 [00:07<00:07, 36.78it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 39.11it/s][A
 56%|█████▋    | 342/608 [00:07<00:06, 40.91it/s][A
 57%|█████▋    | 347/608 [00:08<00:06, 42.22it/s][A
 58%|█████▊    | 352/608 [00:08<00:05, 43.18it/s][A
 59%|█████▊    | 357/608 [00:08<00:05, 43.87it/s][A
 60%|█████▉    | 362/608 [00:08<00:07, 33.84it/s][A
 60%|██████    | 366/608 [00:08<00:08, 29.23it/s][A
 61%|██████    | 371/608 [00:08<00:07, 32.94it/s][A
 62%|██████▏   | 376/608 [00:08<00:06, 36.00it/s][A
 62%|██████▎   | 380/608 [00:09<00:09, 24.78it/s][A
 63%|██████▎   | 385/608 [00:09<00:07, 29.03it/s][A
 64%|██████▍   | 389/608 [00:11<00:29,  7.32it/s][A
 64%|██████▍   | 392/608 [00:11<00:24,  8.68it/s][A
 65%|██████▌   | 397/608 [00:11<00:17, 12.04it/s][A
 66%|██████▌   | 402/608 [00:11<00:12, 15.95it/s][A
 67%|██████▋   | 407/608 [00:11<00:09, 20.17it/s][A
 68%|██████▊   | 412/608 [00:11<00:08, 24.46it/s][A
 69%|██████▊   | 417/608 [00:11<00:06, 28.60it/s][A
 69%|██████▉   | 422/608 [00:11<00:05, 32.31it/s][A
 70%|███████   | 427/608 [00:11<00:05, 35.29it/s][A
 71%|███████   | 432/608 [00:12<00:04, 37.64it/s][A
 72%|███████▏  | 437/608 [00:12<00:04, 39.35it/s][A
 73%|███████▎  | 442/608 [00:12<00:04, 40.76it/s][A
 74%|███████▎  | 447/608 [00:12<00:03, 42.04it/s][A
 74%|███████▍  | 452/608 [00:12<00:03, 43.00it/s][A
 75%|███████▌  | 457/608 [00:12<00:03, 43.83it/s][A
 76%|███████▌  | 462/608 [00:12<00:03, 44.36it/s][A
 77%|███████▋  | 467/608 [00:12<00:03, 44.81it/s][A
 78%|███████▊  | 472/608 [00:12<00:03, 44.85it/s][A
 78%|███████▊  | 477/608 [00:13<00:02, 44.83it/s][A
 79%|███████▉  | 482/608 [00:13<00:02, 44.61it/s][A
 80%|████████  | 487/608 [00:13<00:02, 44.63it/s][A
 81%|████████  | 492/608 [00:13<00:02, 44.71it/s][A
 82%|████████▏ | 497/608 [00:13<00:02, 44.85it/s][A
 83%|████████▎ | 502/608 [00:13<00:02, 45.06it/s][A
 83%|████████▎ | 507/608 [00:13<00:02, 45.18it/s][A
 84%|████████▍ | 512/608 [00:13<00:02, 45.40it/s][A
 85%|████████▌ | 517/608 [00:13<00:02, 45.42it/s][A
 86%|████████▌ | 522/608 [00:14<00:01, 45.19it/s][A
 87%|████████▋ | 527/608 [00:14<00:01, 45.02it/s][A
 88%|████████▊ | 532/608 [00:14<00:01, 42.68it/s][A
 88%|████████▊ | 537/608 [00:14<00:01, 43.41it/s][A
 89%|████████▉ | 542/608 [00:14<00:01, 43.97it/s][A
 90%|████████▉ | 547/608 [00:14<00:01, 44.35it/s][A
 91%|█████████ | 552/608 [00:14<00:01, 44.69it/s][A
 92%|█████████▏| 557/608 [00:14<00:01, 44.93it/s][A
 92%|█████████▏| 562/608 [00:14<00:01, 45.06it/s][A
 93%|█████████▎| 567/608 [00:15<00:00, 45.09it/s][A
 94%|█████████▍| 572/608 [00:15<00:00, 44.73it/s][A
 95%|█████████▍| 577/608 [00:15<00:00, 44.83it/s][A
 96%|█████████▌| 582/608 [00:15<00:00, 44.92it/s][A
 97%|█████████▋| 587/608 [00:15<00:00, 45.10it/s][A
 97%|█████████▋| 592/608 [00:15<00:00, 45.07it/s][A
 98%|█████████▊| 597/608 [00:15<00:00, 45.26it/s][A
 99%|█████████▉| 602/608 [00:15<00:00, 45.32it/s][A
100%|█████████▉| 607/608 [00:15<00:00, 45.27it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:15<00:00, 45.27it/s][A100%|██████████| 805/805 [06:34<00:00,  3.53it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:15:21,158 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805
[INFO|configuration_utils.py:351] 2023-08-28 17:15:22,499 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:15:32,038 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:15:33,283 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:15:33,978 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:15:41,476 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:15:41,549 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161 (score: 0.9957473874092102).
                                                 100%|██████████| 805/805 [07:41<00:00,  3.53it/s]100%|██████████| 805/805 [07:41<00:00,  1.74it/s]
[INFO|trainer.py:1894] 2023-08-28 17:16:29,295 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 17:16:29,609 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:16:43,101 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:16:45,146 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:16:45,492 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:16:49,556 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:50,025 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:50,025 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:50,025 >>   train_runtime            = 0:07:41.71
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:50,025 >>   train_samples            =      10300
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:50,025 >>   train_samples_per_second =     111.54
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:16:50,025 >>   train_steps_per_second   =      1.743
{'eval_loss': 0.9957473874092102, 'eval_runtime': 15.9754, 'eval_samples_per_second': 304.467, 'eval_steps_per_second': 38.058, 'epoch': 5.0}
{'train_runtime': 461.7188, 'train_samples_per_second': 111.54, 'train_steps_per_second': 1.743, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 17:16:52 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:16:52,363 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:16:52,364 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 17:16:52,364 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 57.62it/s]  2%|▏         | 12/608 [00:00<00:11, 50.44it/s]  3%|▎         | 18/608 [00:00<00:16, 36.61it/s]  4%|▍         | 23/608 [00:00<00:14, 39.54it/s]  5%|▍         | 28/608 [00:00<00:13, 41.49it/s]  5%|▌         | 33/608 [00:00<00:13, 42.97it/s]  6%|▋         | 38/608 [00:00<00:12, 43.90it/s]  7%|▋         | 43/608 [00:00<00:12, 44.31it/s]  8%|▊         | 48/608 [00:01<00:12, 44.73it/s]  9%|▊         | 53/608 [00:01<00:12, 44.92it/s] 10%|▉         | 58/608 [00:01<00:12, 44.80it/s] 10%|█         | 63/608 [00:01<00:12, 44.67it/s] 11%|█         | 68/608 [00:01<00:12, 44.69it/s] 12%|█▏        | 73/608 [00:01<00:11, 44.87it/s] 13%|█▎        | 78/608 [00:01<00:11, 45.31it/s] 14%|█▎        | 83/608 [00:01<00:11, 45.54it/s] 14%|█▍        | 88/608 [00:01<00:11, 45.64it/s] 15%|█▌        | 93/608 [00:02<00:11, 45.74it/s] 16%|█▌        | 98/608 [00:02<00:11, 45.70it/s] 17%|█▋        | 103/608 [00:02<00:11, 45.33it/s] 18%|█▊        | 108/608 [00:02<00:11, 45.17it/s] 19%|█▊        | 113/608 [00:02<00:11, 44.82it/s] 19%|█▉        | 118/608 [00:02<00:10, 45.11it/s] 20%|██        | 123/608 [00:02<00:10, 45.22it/s] 21%|██        | 128/608 [00:02<00:10, 45.44it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.61it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.70it/s] 24%|██▎       | 143/608 [00:03<00:10, 45.79it/s] 24%|██▍       | 148/608 [00:03<00:10, 45.72it/s] 25%|██▌       | 153/608 [00:03<00:11, 40.49it/s] 26%|██▌       | 158/608 [00:03<00:10, 42.02it/s] 27%|██▋       | 163/608 [00:03<00:10, 43.13it/s] 28%|██▊       | 168/608 [00:03<00:10, 43.85it/s] 28%|██▊       | 173/608 [00:03<00:09, 44.41it/s] 29%|██▉       | 178/608 [00:04<00:09, 44.76it/s] 30%|███       | 183/608 [00:04<00:09, 45.08it/s] 31%|███       | 188/608 [00:04<00:09, 45.29it/s] 32%|███▏      | 193/608 [00:04<00:09, 44.99it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.10it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.26it/s] 34%|███▍      | 208/608 [00:04<00:08, 45.53it/s] 35%|███▌      | 213/608 [00:04<00:08, 45.64it/s] 36%|███▌      | 218/608 [00:04<00:08, 45.78it/s] 37%|███▋      | 223/608 [00:04<00:08, 45.85it/s] 38%|███▊      | 228/608 [00:05<00:08, 45.83it/s] 38%|███▊      | 233/608 [00:05<00:08, 45.75it/s] 39%|███▉      | 238/608 [00:05<00:08, 45.56it/s] 40%|███▉      | 243/608 [00:05<00:08, 45.52it/s] 41%|████      | 248/608 [00:05<00:07, 45.63it/s] 42%|████▏     | 253/608 [00:05<00:07, 45.68it/s] 42%|████▏     | 258/608 [00:05<00:07, 45.88it/s] 43%|████▎     | 263/608 [00:05<00:07, 46.00it/s] 44%|████▍     | 268/608 [00:05<00:07, 46.09it/s] 45%|████▍     | 273/608 [00:06<00:08, 41.70it/s] 46%|████▌     | 278/608 [00:06<00:07, 43.03it/s] 47%|████▋     | 283/608 [00:06<00:07, 43.88it/s] 47%|████▋     | 288/608 [00:06<00:07, 44.53it/s] 48%|████▊     | 293/608 [00:06<00:07, 44.89it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.21it/s] 50%|████▉     | 303/608 [00:06<00:06, 45.44it/s] 51%|█████     | 308/608 [00:06<00:06, 45.71it/s] 51%|█████▏    | 313/608 [00:06<00:06, 45.44it/s] 52%|█████▏    | 318/608 [00:07<00:06, 45.43it/s] 53%|█████▎    | 323/608 [00:07<00:06, 45.54it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.68it/s] 55%|█████▍    | 333/608 [00:07<00:06, 45.82it/s] 56%|█████▌    | 338/608 [00:07<00:05, 45.85it/s] 56%|█████▋    | 343/608 [00:07<00:05, 45.90it/s] 57%|█████▋    | 348/608 [00:07<00:05, 45.98it/s] 58%|█████▊    | 353/608 [00:07<00:05, 45.98it/s] 59%|█████▉    | 358/608 [00:07<00:05, 45.71it/s] 60%|█████▉    | 363/608 [00:08<00:05, 45.64it/s] 61%|██████    | 368/608 [00:08<00:05, 45.60it/s] 61%|██████▏   | 373/608 [00:08<00:05, 45.71it/s] 62%|██████▏   | 378/608 [00:08<00:05, 45.79it/s] 63%|██████▎   | 383/608 [00:08<00:04, 45.85it/s] 64%|██████▍   | 388/608 [00:08<00:04, 45.95it/s] 65%|██████▍   | 393/608 [00:08<00:04, 45.97it/s] 65%|██████▌   | 398/608 [00:08<00:04, 45.92it/s] 66%|██████▋   | 403/608 [00:08<00:04, 45.76it/s] 67%|██████▋   | 408/608 [00:09<00:05, 39.49it/s] 68%|██████▊   | 413/608 [00:09<00:04, 41.35it/s] 69%|██████▉   | 418/608 [00:09<00:04, 42.62it/s] 70%|██████▉   | 423/608 [00:09<00:04, 43.71it/s] 70%|███████   | 428/608 [00:09<00:04, 44.46it/s] 71%|███████   | 433/608 [00:09<00:03, 45.02it/s] 72%|███████▏  | 438/608 [00:09<00:03, 45.36it/s] 73%|███████▎  | 443/608 [00:09<00:03, 45.62it/s] 74%|███████▎  | 448/608 [00:09<00:03, 45.31it/s] 75%|███████▍  | 453/608 [00:10<00:03, 45.16it/s] 75%|███████▌  | 458/608 [00:10<00:03, 45.18it/s] 76%|███████▌  | 463/608 [00:10<00:03, 45.39it/s] 77%|███████▋  | 468/608 [00:10<00:03, 45.65it/s] 78%|███████▊  | 473/608 [00:10<00:02, 45.82it/s] 79%|███████▊  | 478/608 [00:10<00:02, 46.03it/s] 79%|███████▉  | 483/608 [00:10<00:02, 46.08it/s] 80%|████████  | 488/608 [00:10<00:02, 45.91it/s] 81%|████████  | 493/608 [00:10<00:02, 45.66it/s] 82%|████████▏ | 498/608 [00:11<00:02, 45.45it/s] 83%|████████▎ | 503/608 [00:11<00:02, 45.33it/s] 84%|████████▎ | 508/608 [00:11<00:02, 45.53it/s] 84%|████████▍ | 513/608 [00:11<00:02, 45.59it/s] 85%|████████▌ | 518/608 [00:11<00:01, 45.76it/s] 86%|████████▌ | 523/608 [00:11<00:01, 45.96it/s] 87%|████████▋ | 528/608 [00:11<00:01, 46.10it/s] 88%|████████▊ | 533/608 [00:11<00:01, 46.00it/s] 88%|████████▊ | 538/608 [00:11<00:01, 45.67it/s] 89%|████████▉ | 543/608 [00:12<00:01, 41.19it/s] 90%|█████████ | 548/608 [00:12<00:01, 42.62it/s] 91%|█████████ | 553/608 [00:12<00:01, 43.58it/s] 92%|█████████▏| 558/608 [00:12<00:01, 44.24it/s] 93%|█████████▎| 563/608 [00:12<00:01, 44.79it/s] 93%|█████████▎| 568/608 [00:12<00:00, 45.18it/s] 94%|█████████▍| 573/608 [00:12<00:00, 45.40it/s] 95%|█████████▌| 578/608 [00:12<00:00, 45.58it/s] 96%|█████████▌| 583/608 [00:12<00:00, 45.25it/s] 97%|█████████▋| 588/608 [00:13<00:00, 45.34it/s] 98%|█████████▊| 593/608 [00:13<00:00, 45.53it/s] 98%|█████████▊| 598/608 [00:13<00:00, 45.61it/s] 99%|█████████▉| 603/608 [00:13<00:00, 45.75it/s]100%|██████████| 608/608 [00:13<00:00, 45.82it/s]100%|██████████| 608/608 [00:13<00:00, 44.90it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:17:05,940 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:17:05,940 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:17:05,940 >>   eval_loss               =     0.9957
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:17:05,940 >>   eval_runtime            = 0:00:13.57
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:17:05,940 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:17:05,940 >>   eval_samples_per_second =     358.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:17:05,940 >>   eval_steps_per_second   =     44.784
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:17:05,940 >>   perplexity              =     2.7067
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:29,645 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:29,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:29,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:29,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:29,647 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:17:30,550 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:17:30,551 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:17:31,775 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:17:33,192 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:17:33,363 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:38,065 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:38,425 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:38,426 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:38,426 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:17:38,426 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:17:40,417 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:17:40,418 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:17:41,296 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:17:41,780 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:17:41,780 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-805
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-483
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-161
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-322
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/generator/iter5/model/checkpoint-644
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.84it/s]Extractor Predicting: 5it [00:02,  1.89it/s]Extractor Predicting: 6it [00:03,  1.94it/s]Extractor Predicting: 7it [00:03,  1.87it/s]Extractor Predicting: 8it [00:04,  1.83it/s]Extractor Predicting: 9it [00:04,  1.86it/s]Extractor Predicting: 10it [00:05,  1.94it/s]Extractor Predicting: 11it [00:05,  1.92it/s]Extractor Predicting: 12it [00:06,  1.98it/s]Extractor Predicting: 13it [00:06,  1.98it/s]Extractor Predicting: 14it [00:07,  1.93it/s]Extractor Predicting: 15it [00:07,  2.00it/s]Extractor Predicting: 16it [00:08,  1.97it/s]Extractor Predicting: 17it [00:08,  1.95it/s]Extractor Predicting: 18it [00:09,  1.93it/s]Extractor Predicting: 19it [00:10,  1.79it/s]Extractor Predicting: 20it [00:10,  1.88it/s]Extractor Predicting: 21it [00:11,  1.89it/s]Extractor Predicting: 22it [00:11,  1.89it/s]Extractor Predicting: 23it [00:12,  1.81it/s]Extractor Predicting: 24it [00:12,  1.75it/s]Extractor Predicting: 25it [00:13,  1.59it/s]Extractor Predicting: 26it [00:14,  1.62it/s]Extractor Predicting: 27it [00:14,  1.67it/s]Extractor Predicting: 28it [00:15,  1.69it/s]Extractor Predicting: 29it [00:16,  1.62it/s]Extractor Predicting: 30it [00:16,  1.42it/s]Extractor Predicting: 31it [00:17,  1.53it/s]Extractor Predicting: 32it [00:17,  1.62it/s]Extractor Predicting: 33it [00:18,  1.68it/s]Extractor Predicting: 34it [00:19,  1.73it/s]Extractor Predicting: 35it [00:19,  1.51it/s]Extractor Predicting: 36it [00:20,  1.59it/s]Extractor Predicting: 37it [00:21,  1.63it/s]Extractor Predicting: 38it [00:21,  1.68it/s]Extractor Predicting: 39it [00:22,  1.71it/s]Extractor Predicting: 40it [00:22,  1.63it/s]Extractor Predicting: 41it [00:23,  1.69it/s]Extractor Predicting: 42it [00:23,  1.71it/s]Extractor Predicting: 43it [00:24,  1.71it/s]Extractor Predicting: 44it [00:25,  1.78it/s]Extractor Predicting: 45it [00:25,  1.77it/s]Extractor Predicting: 46it [00:26,  1.66it/s]Extractor Predicting: 47it [00:26,  1.71it/s]Extractor Predicting: 48it [00:27,  1.74it/s]Extractor Predicting: 49it [00:27,  1.74it/s]Extractor Predicting: 50it [00:28,  1.76it/s]Extractor Predicting: 51it [00:29,  1.75it/s]Extractor Predicting: 52it [00:29,  1.67it/s]Extractor Predicting: 53it [00:30,  1.69it/s]Extractor Predicting: 54it [00:30,  1.77it/s]Extractor Predicting: 55it [00:31,  1.81it/s]Extractor Predicting: 56it [00:31,  1.80it/s]Extractor Predicting: 57it [00:32,  1.79it/s]Extractor Predicting: 58it [00:33,  1.73it/s]Extractor Predicting: 59it [00:33,  1.76it/s]Extractor Predicting: 60it [00:34,  1.75it/s]Extractor Predicting: 61it [00:34,  1.70it/s]Extractor Predicting: 62it [00:35,  1.72it/s]Extractor Predicting: 63it [00:36,  1.75it/s]Extractor Predicting: 64it [00:36,  1.70it/s]Extractor Predicting: 65it [00:37,  1.71it/s]Extractor Predicting: 66it [00:37,  1.74it/s]Extractor Predicting: 67it [00:38,  1.77it/s]Extractor Predicting: 68it [00:38,  1.73it/s]Extractor Predicting: 69it [00:39,  1.73it/s]Extractor Predicting: 70it [00:40,  1.77it/s]Extractor Predicting: 71it [00:41,  1.42it/s]Extractor Predicting: 72it [00:41,  1.51it/s]Extractor Predicting: 73it [00:42,  1.61it/s]Extractor Predicting: 74it [00:42,  1.65it/s]Extractor Predicting: 75it [00:43,  1.71it/s]Extractor Predicting: 76it [00:44,  1.39it/s]Extractor Predicting: 77it [00:44,  1.49it/s]Extractor Predicting: 78it [00:45,  1.60it/s]Extractor Predicting: 79it [00:45,  1.66it/s]Extractor Predicting: 80it [00:46,  1.72it/s]Extractor Predicting: 81it [00:47,  1.51it/s]Extractor Predicting: 82it [00:47,  1.57it/s]Extractor Predicting: 83it [00:48,  1.63it/s]Extractor Predicting: 84it [00:48,  1.68it/s]Extractor Predicting: 85it [00:49,  1.71it/s]Extractor Predicting: 86it [00:50,  1.45it/s]Extractor Predicting: 87it [00:50,  1.56it/s]Extractor Predicting: 88it [00:51,  1.62it/s]Extractor Predicting: 89it [00:52,  1.59it/s]Extractor Predicting: 90it [00:52,  1.68it/s]Extractor Predicting: 91it [00:53,  1.59it/s]Extractor Predicting: 92it [00:53,  1.66it/s]Extractor Predicting: 93it [00:54,  1.72it/s]Extractor Predicting: 94it [00:55,  1.74it/s]Extractor Predicting: 95it [00:55,  1.70it/s]Extractor Predicting: 96it [00:56,  1.73it/s]Extractor Predicting: 97it [00:56,  1.68it/s]Extractor Predicting: 98it [00:57,  1.73it/s]Extractor Predicting: 99it [00:57,  1.77it/s]Extractor Predicting: 100it [00:58,  1.77it/s]Extractor Predicting: 101it [00:59,  1.79it/s]Extractor Predicting: 102it [00:59,  1.82it/s]Extractor Predicting: 103it [01:00,  1.79it/s]Extractor Predicting: 104it [01:00,  1.80it/s]Extractor Predicting: 105it [01:01,  1.77it/s]Extractor Predicting: 106it [01:01,  1.79it/s]Extractor Predicting: 107it [01:02,  1.80it/s]Extractor Predicting: 108it [01:02,  1.84it/s]Extractor Predicting: 109it [01:03,  1.59it/s]Extractor Predicting: 110it [01:04,  1.61it/s]Extractor Predicting: 111it [01:04,  1.68it/s]Extractor Predicting: 112it [01:05,  1.72it/s]Extractor Predicting: 113it [01:05,  1.76it/s]Extractor Predicting: 114it [01:06,  1.64it/s]Extractor Predicting: 115it [01:07,  1.76it/s]Extractor Predicting: 116it [01:07,  1.80it/s]Extractor Predicting: 117it [01:08,  1.83it/s]Extractor Predicting: 118it [01:08,  1.88it/s]Extractor Predicting: 119it [01:09,  1.86it/s]Extractor Predicting: 120it [01:09,  1.90it/s]Extractor Predicting: 121it [01:10,  1.86it/s]Extractor Predicting: 122it [01:10,  1.81it/s]Extractor Predicting: 123it [01:11,  1.60it/s]Extractor Predicting: 124it [01:12,  1.61it/s]Extractor Predicting: 125it [01:12,  1.67it/s]Extractor Predicting: 126it [01:13,  1.73it/s]Extractor Predicting: 127it [01:13,  1.76it/s]Extractor Predicting: 128it [01:14,  1.55it/s]Extractor Predicting: 129it [01:15,  1.63it/s]Extractor Predicting: 130it [01:15,  1.70it/s]Extractor Predicting: 131it [01:16,  1.60it/s]Extractor Predicting: 132it [01:17,  1.62it/s]Extractor Predicting: 133it [01:18,  1.22it/s]Extractor Predicting: 134it [01:18,  1.36it/s]Extractor Predicting: 135it [01:19,  1.49it/s]Extractor Predicting: 136it [01:20,  1.57it/s]Extractor Predicting: 137it [01:20,  1.66it/s]Extractor Predicting: 138it [01:21,  1.43it/s]Extractor Predicting: 139it [01:22,  1.53it/s]Extractor Predicting: 140it [01:22,  1.58it/s]Extractor Predicting: 141it [01:23,  1.68it/s]Extractor Predicting: 142it [01:23,  1.69it/s]Extractor Predicting: 143it [01:24,  1.63it/s]Extractor Predicting: 144it [01:24,  1.69it/s]Extractor Predicting: 145it [01:25,  1.74it/s]Extractor Predicting: 146it [01:26,  1.71it/s]Extractor Predicting: 147it [01:26,  1.71it/s]Extractor Predicting: 148it [01:27,  1.73it/s]Extractor Predicting: 149it [01:28,  1.41it/s]Extractor Predicting: 150it [01:28,  1.52it/s]Extractor Predicting: 151it [01:29,  1.57it/s]Extractor Predicting: 152it [01:29,  1.67it/s]Extractor Predicting: 153it [01:30,  1.69it/s]Extractor Predicting: 154it [01:31,  1.58it/s]Extractor Predicting: 155it [01:31,  1.66it/s]Extractor Predicting: 156it [01:32,  1.73it/s]Extractor Predicting: 157it [01:32,  1.75it/s]Extractor Predicting: 158it [01:33,  1.80it/s]Extractor Predicting: 159it [01:33,  1.78it/s]Extractor Predicting: 160it [01:34,  1.59it/s]Extractor Predicting: 161it [01:35,  1.63it/s]Extractor Predicting: 162it [01:35,  1.66it/s]Extractor Predicting: 163it [01:36,  1.71it/s]Extractor Predicting: 164it [01:36,  1.71it/s]Extractor Predicting: 165it [01:37,  1.69it/s]Extractor Predicting: 166it [01:38,  1.69it/s]Extractor Predicting: 167it [01:38,  1.69it/s]Extractor Predicting: 168it [01:39,  1.65it/s]Extractor Predicting: 169it [01:39,  1.63it/s]Extractor Predicting: 170it [01:40,  1.66it/s]Extractor Predicting: 171it [01:41,  1.66it/s]Extractor Predicting: 172it [01:41,  1.69it/s]Extractor Predicting: 173it [01:42,  1.60it/s]Extractor Predicting: 174it [01:43,  1.59it/s]Extractor Predicting: 175it [01:43,  1.66it/s]Extractor Predicting: 176it [01:44,  1.65it/s]Extractor Predicting: 177it [01:44,  1.64it/s]Extractor Predicting: 178it [01:46,  1.20it/s]Extractor Predicting: 179it [01:46,  1.29it/s]Extractor Predicting: 180it [01:47,  1.38it/s]Extractor Predicting: 181it [01:48,  1.47it/s]Extractor Predicting: 182it [01:48,  1.45it/s]Extractor Predicting: 183it [01:49,  1.50it/s]Extractor Predicting: 184it [01:49,  1.54it/s]Extractor Predicting: 185it [01:50,  1.61it/s]Extractor Predicting: 186it [01:51,  1.64it/s]Extractor Predicting: 187it [01:51,  1.43it/s]Extractor Predicting: 187it [01:51,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:12,720 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:12,959 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:12,960 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:12,960 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:12,960 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:20:14,794 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:20:14,795 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:20:15,644 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:20:16,902 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:20:16,956 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:21,272 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:21,448 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:21,448 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:21,448 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:20:21,449 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:20:23,293 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:20:23,294 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:20:25,400 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:20:25,865 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:20:25,865 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.46it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.72it/s]Extractor Predicting: 14it [00:08,  1.75it/s]Extractor Predicting: 15it [00:08,  1.80it/s]Extractor Predicting: 16it [00:09,  1.79it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:12,  1.69it/s]Extractor Predicting: 22it [00:13,  1.52it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.70it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.68it/s]Extractor Predicting: 30it [00:18,  1.72it/s]Extractor Predicting: 31it [00:18,  1.70it/s]Extractor Predicting: 32it [00:19,  1.68it/s]Extractor Predicting: 33it [00:19,  1.71it/s]Extractor Predicting: 34it [00:20,  1.75it/s]Extractor Predicting: 35it [00:21,  1.75it/s]Extractor Predicting: 36it [00:21,  1.77it/s]Extractor Predicting: 37it [00:22,  1.75it/s]Extractor Predicting: 38it [00:22,  1.73it/s]Extractor Predicting: 39it [00:23,  1.54it/s]Extractor Predicting: 40it [00:24,  1.60it/s]Extractor Predicting: 41it [00:24,  1.67it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:25,  1.75it/s]Extractor Predicting: 44it [00:26,  1.70it/s]Extractor Predicting: 45it [00:26,  1.75it/s]Extractor Predicting: 46it [00:27,  1.75it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:28,  1.70it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:30,  1.59it/s]Extractor Predicting: 51it [00:30,  1.63it/s]Extractor Predicting: 52it [00:31,  1.66it/s]Extractor Predicting: 53it [00:31,  1.75it/s]Extractor Predicting: 54it [00:32,  1.78it/s]Extractor Predicting: 55it [00:32,  1.80it/s]Extractor Predicting: 56it [00:33,  1.63it/s]Extractor Predicting: 57it [00:34,  1.65it/s]Extractor Predicting: 58it [00:34,  1.71it/s]Extractor Predicting: 59it [00:35,  1.74it/s]Extractor Predicting: 60it [00:35,  1.79it/s]Extractor Predicting: 61it [00:36,  1.77it/s]Extractor Predicting: 62it [00:36,  1.70it/s]Extractor Predicting: 63it [00:37,  1.73it/s]Extractor Predicting: 64it [00:38,  1.74it/s]Extractor Predicting: 65it [00:38,  1.76it/s]Extractor Predicting: 66it [00:39,  1.80it/s]Extractor Predicting: 67it [00:39,  1.82it/s]Extractor Predicting: 68it [00:40,  1.50it/s]Extractor Predicting: 69it [00:41,  1.60it/s]Extractor Predicting: 70it [00:41,  1.60it/s]Extractor Predicting: 71it [00:42,  1.67it/s]Extractor Predicting: 72it [00:42,  1.71it/s]Extractor Predicting: 73it [00:43,  1.62it/s]Extractor Predicting: 74it [00:44,  1.69it/s]Extractor Predicting: 75it [00:44,  1.74it/s]Extractor Predicting: 76it [00:45,  1.72it/s]Extractor Predicting: 77it [00:45,  1.77it/s]Extractor Predicting: 78it [00:46,  1.83it/s]Extractor Predicting: 79it [00:47,  1.46it/s]Extractor Predicting: 80it [00:47,  1.57it/s]Extractor Predicting: 81it [00:48,  1.65it/s]Extractor Predicting: 82it [00:48,  1.70it/s]Extractor Predicting: 83it [00:49,  1.73it/s]Extractor Predicting: 84it [00:50,  1.71it/s]Extractor Predicting: 85it [00:50,  1.66it/s]Extractor Predicting: 86it [00:51,  1.73it/s]Extractor Predicting: 87it [00:51,  1.80it/s]Extractor Predicting: 88it [00:52,  1.86it/s]Extractor Predicting: 89it [00:52,  1.86it/s]Extractor Predicting: 90it [00:53,  1.83it/s]Extractor Predicting: 91it [00:53,  1.71it/s]Extractor Predicting: 92it [00:54,  1.78it/s]Extractor Predicting: 93it [00:55,  1.80it/s]Extractor Predicting: 94it [00:55,  1.85it/s]Extractor Predicting: 95it [00:56,  1.87it/s]Extractor Predicting: 96it [00:56,  1.90it/s]Extractor Predicting: 97it [00:57,  1.62it/s]Extractor Predicting: 98it [00:57,  1.68it/s]Extractor Predicting: 99it [00:58,  1.72it/s]Extractor Predicting: 100it [00:58,  1.79it/s]Extractor Predicting: 101it [00:59,  1.82it/s]Extractor Predicting: 102it [01:00,  1.84it/s]Extractor Predicting: 103it [01:00,  1.64it/s]Extractor Predicting: 104it [01:01,  1.70it/s]Extractor Predicting: 105it [01:01,  1.71it/s]Extractor Predicting: 106it [01:02,  1.73it/s]Extractor Predicting: 107it [01:03,  1.77it/s]Extractor Predicting: 108it [01:03,  1.79it/s]Extractor Predicting: 109it [01:04,  1.62it/s]Extractor Predicting: 110it [01:04,  1.67it/s]Extractor Predicting: 111it [01:05,  1.69it/s]Extractor Predicting: 112it [01:05,  1.75it/s]Extractor Predicting: 113it [01:06,  1.78it/s]Extractor Predicting: 114it [01:07,  1.81it/s]Extractor Predicting: 115it [01:07,  1.74it/s]Extractor Predicting: 116it [01:08,  1.76it/s]Extractor Predicting: 117it [01:08,  1.78it/s]Extractor Predicting: 118it [01:09,  1.79it/s]Extractor Predicting: 119it [01:09,  1.82it/s]Extractor Predicting: 120it [01:10,  1.79it/s]Extractor Predicting: 121it [01:11,  1.77it/s]Extractor Predicting: 122it [01:11,  1.80it/s]Extractor Predicting: 123it [01:12,  1.82it/s]Extractor Predicting: 124it [01:12,  1.84it/s]Extractor Predicting: 125it [01:13,  1.83it/s]Extractor Predicting: 126it [01:14,  1.57it/s]Extractor Predicting: 127it [01:14,  1.60it/s]Extractor Predicting: 128it [01:15,  1.62it/s]Extractor Predicting: 129it [01:15,  1.63it/s]Extractor Predicting: 130it [01:16,  1.71it/s]Extractor Predicting: 131it [01:16,  1.74it/s]Extractor Predicting: 132it [01:17,  1.74it/s]Extractor Predicting: 133it [01:18,  1.73it/s]Extractor Predicting: 134it [01:18,  1.72it/s]Extractor Predicting: 135it [01:19,  1.76it/s]Extractor Predicting: 136it [01:19,  1.78it/s]Extractor Predicting: 137it [01:20,  1.77it/s]Extractor Predicting: 138it [01:20,  1.76it/s]Extractor Predicting: 139it [01:21,  1.67it/s]Extractor Predicting: 140it [01:22,  1.69it/s]Extractor Predicting: 141it [01:22,  1.71it/s]Extractor Predicting: 142it [01:23,  1.73it/s]Extractor Predicting: 143it [01:23,  1.75it/s]Extractor Predicting: 144it [01:24,  1.75it/s]Extractor Predicting: 145it [01:25,  1.52it/s]Extractor Predicting: 146it [01:25,  1.60it/s]Extractor Predicting: 147it [01:26,  1.64it/s]Extractor Predicting: 148it [01:26,  1.70it/s]Extractor Predicting: 149it [01:27,  1.70it/s]Extractor Predicting: 150it [01:28,  1.63it/s]Extractor Predicting: 151it [01:28,  1.66it/s]Extractor Predicting: 152it [01:29,  1.70it/s]Extractor Predicting: 153it [01:29,  1.78it/s]Extractor Predicting: 154it [01:30,  1.80it/s]Extractor Predicting: 155it [01:30,  1.83it/s]Extractor Predicting: 156it [01:31,  1.74it/s]Extractor Predicting: 157it [01:32,  1.77it/s]Extractor Predicting: 158it [01:32,  1.78it/s]Extractor Predicting: 159it [01:33,  1.77it/s]Extractor Predicting: 160it [01:33,  1.77it/s]Extractor Predicting: 161it [01:34,  1.78it/s]Extractor Predicting: 162it [01:35,  1.34it/s]Extractor Predicting: 163it [01:35,  1.46it/s]Extractor Predicting: 164it [01:36,  1.54it/s]Extractor Predicting: 165it [01:37,  1.62it/s]Extractor Predicting: 166it [01:37,  1.67it/s]Extractor Predicting: 167it [01:38,  1.67it/s]Extractor Predicting: 168it [01:38,  1.68it/s]Extractor Predicting: 169it [01:39,  1.71it/s]Extractor Predicting: 170it [01:39,  1.76it/s]Extractor Predicting: 171it [01:40,  1.84it/s]Extractor Predicting: 172it [01:40,  1.84it/s]Extractor Predicting: 173it [01:41,  1.76it/s]Extractor Predicting: 174it [01:42,  1.75it/s]Extractor Predicting: 175it [01:42,  1.70it/s]Extractor Predicting: 176it [01:43,  1.70it/s]Extractor Predicting: 177it [01:43,  1.74it/s]Extractor Predicting: 178it [01:44,  1.73it/s]Extractor Predicting: 179it [01:45,  1.58it/s]Extractor Predicting: 180it [01:45,  1.60it/s]Extractor Predicting: 181it [01:46,  1.64it/s]Extractor Predicting: 182it [01:47,  1.67it/s]Extractor Predicting: 183it [01:47,  1.67it/s]Extractor Predicting: 184it [01:48,  1.42it/s]Extractor Predicting: 185it [01:49,  1.50it/s]Extractor Predicting: 186it [01:49,  1.57it/s]Extractor Predicting: 187it [01:50,  1.59it/s]Extractor Predicting: 188it [01:50,  1.66it/s]Extractor Predicting: 189it [01:51,  1.69it/s]Extractor Predicting: 190it [01:52,  1.46it/s]Extractor Predicting: 191it [01:52,  1.55it/s]Extractor Predicting: 192it [01:53,  1.60it/s]Extractor Predicting: 193it [01:54,  1.64it/s]Extractor Predicting: 194it [01:54,  1.68it/s]Extractor Predicting: 195it [01:55,  1.71it/s]Extractor Predicting: 196it [01:55,  1.77it/s]Extractor Predicting: 197it [01:56,  1.77it/s]Extractor Predicting: 198it [01:56,  1.79it/s]Extractor Predicting: 199it [01:57,  1.80it/s]Extractor Predicting: 200it [01:57,  1.82it/s]Extractor Predicting: 201it [01:58,  1.41it/s]Extractor Predicting: 202it [01:59,  1.51it/s]Extractor Predicting: 203it [02:00,  1.58it/s]Extractor Predicting: 204it [02:00,  1.63it/s]Extractor Predicting: 205it [02:01,  1.68it/s]Extractor Predicting: 206it [02:02,  1.44it/s]Extractor Predicting: 207it [02:02,  1.52it/s]Extractor Predicting: 208it [02:03,  1.63it/s]Extractor Predicting: 209it [02:03,  1.70it/s]Extractor Predicting: 210it [02:04,  1.71it/s]Extractor Predicting: 211it [02:05,  1.43it/s]Extractor Predicting: 212it [02:05,  1.54it/s]Extractor Predicting: 213it [02:06,  1.59it/s]Extractor Predicting: 214it [02:06,  1.62it/s]Extractor Predicting: 215it [02:07,  1.66it/s]Extractor Predicting: 216it [02:08,  1.60it/s]Extractor Predicting: 217it [02:08,  1.68it/s]Extractor Predicting: 218it [02:09,  1.69it/s]Extractor Predicting: 219it [02:09,  1.69it/s]Extractor Predicting: 220it [02:10,  1.76it/s]Extractor Predicting: 221it [02:11,  1.73it/s]Extractor Predicting: 222it [02:11,  1.66it/s]Extractor Predicting: 223it [02:12,  1.69it/s]Extractor Predicting: 224it [02:12,  1.74it/s]Extractor Predicting: 225it [02:13,  1.80it/s]Extractor Predicting: 226it [02:13,  1.81it/s]Extractor Predicting: 227it [02:14,  1.82it/s]Extractor Predicting: 228it [02:14,  1.79it/s]Extractor Predicting: 229it [02:15,  1.78it/s]Extractor Predicting: 230it [02:16,  1.84it/s]Extractor Predicting: 231it [02:16,  1.84it/s]Extractor Predicting: 232it [02:17,  1.84it/s]Extractor Predicting: 233it [02:17,  1.84it/s]Extractor Predicting: 234it [02:18,  1.74it/s]Extractor Predicting: 235it [02:18,  1.74it/s]Extractor Predicting: 236it [02:19,  1.77it/s]Extractor Predicting: 237it [02:20,  1.76it/s]Extractor Predicting: 238it [02:20,  1.78it/s]Extractor Predicting: 239it [02:21,  1.78it/s]Extractor Predicting: 240it [02:21,  1.80it/s]Extractor Predicting: 241it [02:22,  1.84it/s]Extractor Predicting: 242it [02:23,  1.49it/s]Extractor Predicting: 243it [02:23,  1.60it/s]Extractor Predicting: 244it [02:24,  1.66it/s]Extractor Predicting: 245it [02:24,  1.68it/s]Extractor Predicting: 246it [02:25,  1.74it/s]Extractor Predicting: 247it [02:26,  1.63it/s]Extractor Predicting: 248it [02:26,  1.70it/s]Extractor Predicting: 249it [02:27,  1.73it/s]Extractor Predicting: 250it [02:27,  1.82it/s]Extractor Predicting: 251it [02:28,  1.79it/s]Extractor Predicting: 252it [02:28,  1.82it/s]Extractor Predicting: 253it [02:29,  1.57it/s]Extractor Predicting: 254it [02:30,  1.61it/s]Extractor Predicting: 255it [02:30,  1.68it/s]Extractor Predicting: 256it [02:31,  1.72it/s]Extractor Predicting: 257it [02:31,  1.75it/s]Extractor Predicting: 258it [02:32,  1.71it/s]Extractor Predicting: 259it [02:32,  1.75it/s]Extractor Predicting: 260it [02:33,  1.76it/s]Extractor Predicting: 261it [02:34,  1.74it/s]Extractor Predicting: 262it [02:34,  1.77it/s]Extractor Predicting: 263it [02:35,  1.72it/s]Extractor Predicting: 264it [02:35,  1.64it/s]Extractor Predicting: 265it [02:36,  1.67it/s]Extractor Predicting: 266it [02:37,  1.68it/s]Extractor Predicting: 267it [02:37,  1.70it/s]Extractor Predicting: 268it [02:38,  1.71it/s]Extractor Predicting: 269it [02:38,  1.70it/s]Extractor Predicting: 270it [02:39,  1.66it/s]Extractor Predicting: 271it [02:40,  1.70it/s]Extractor Predicting: 272it [02:40,  1.76it/s]Extractor Predicting: 273it [02:41,  1.75it/s]Extractor Predicting: 274it [02:41,  1.74it/s]Extractor Predicting: 275it [02:42,  1.75it/s]Extractor Predicting: 276it [02:42,  1.65it/s]Extractor Predicting: 277it [02:43,  1.68it/s]Extractor Predicting: 278it [02:44,  1.71it/s]Extractor Predicting: 279it [02:44,  1.72it/s]Extractor Predicting: 280it [02:45,  1.74it/s]Extractor Predicting: 281it [02:45,  1.76it/s]Extractor Predicting: 282it [02:46,  1.70it/s]Extractor Predicting: 283it [02:46,  1.72it/s]Extractor Predicting: 284it [02:47,  1.77it/s]Extractor Predicting: 285it [02:48,  1.80it/s]Extractor Predicting: 286it [02:48,  1.80it/s]Extractor Predicting: 287it [02:49,  1.77it/s]Extractor Predicting: 288it [02:49,  1.65it/s]Extractor Predicting: 289it [02:50,  1.56it/s]Extractor Predicting: 290it [02:51,  1.60it/s]Extractor Predicting: 291it [02:51,  1.64it/s]Extractor Predicting: 292it [02:52,  1.69it/s]Extractor Predicting: 293it [02:52,  1.71it/s]Extractor Predicting: 294it [02:53,  1.50it/s]Extractor Predicting: 295it [02:54,  1.58it/s]Extractor Predicting: 296it [02:54,  1.62it/s]Extractor Predicting: 297it [02:55,  1.64it/s]Extractor Predicting: 298it [02:56,  1.67it/s]Extractor Predicting: 299it [02:56,  1.45it/s]Extractor Predicting: 300it [02:57,  1.48it/s]Extractor Predicting: 301it [02:58,  1.49it/s]Extractor Predicting: 302it [02:58,  1.52it/s]Extractor Predicting: 303it [02:59,  1.58it/s]Extractor Predicting: 304it [03:00,  1.28it/s]Extractor Predicting: 305it [03:01,  1.34it/s]Extractor Predicting: 306it [03:01,  1.46it/s]Extractor Predicting: 307it [03:02,  1.55it/s]Extractor Predicting: 308it [03:02,  1.62it/s]Extractor Predicting: 309it [03:03,  1.45it/s]Extractor Predicting: 310it [03:04,  1.52it/s]Extractor Predicting: 311it [03:04,  1.54it/s]Extractor Predicting: 312it [03:05,  1.61it/s]Extractor Predicting: 313it [03:06,  1.66it/s]Extractor Predicting: 314it [03:06,  1.43it/s]Extractor Predicting: 315it [03:07,  1.57it/s]Extractor Predicting: 316it [03:08,  1.64it/s]Extractor Predicting: 317it [03:08,  1.67it/s]Extractor Predicting: 318it [03:09,  1.71it/s]Extractor Predicting: 319it [03:10,  1.49it/s]Extractor Predicting: 320it [03:10,  1.58it/s]Extractor Predicting: 321it [03:11,  1.65it/s]Extractor Predicting: 322it [03:11,  1.73it/s]Extractor Predicting: 323it [03:12,  1.76it/s]Extractor Predicting: 324it [03:12,  1.72it/s]Extractor Predicting: 325it [03:13,  1.73it/s]Extractor Predicting: 326it [03:13,  1.78it/s]Extractor Predicting: 327it [03:14,  1.80it/s]Extractor Predicting: 328it [03:14,  1.79it/s]Extractor Predicting: 329it [03:15,  1.79it/s]Extractor Predicting: 330it [03:16,  1.73it/s]Extractor Predicting: 331it [03:16,  1.73it/s]Extractor Predicting: 332it [03:17,  1.78it/s]Extractor Predicting: 333it [03:17,  1.77it/s]Extractor Predicting: 334it [03:18,  1.78it/s]Extractor Predicting: 335it [03:18,  1.79it/s]Extractor Predicting: 336it [03:19,  1.72it/s]Extractor Predicting: 337it [03:20,  1.74it/s]Extractor Predicting: 338it [03:20,  1.77it/s]Extractor Predicting: 339it [03:21,  1.56it/s]Extractor Predicting: 340it [03:22,  1.63it/s]Extractor Predicting: 341it [03:22,  1.69it/s]Extractor Predicting: 342it [03:23,  1.70it/s]Extractor Predicting: 343it [03:23,  1.72it/s]Extractor Predicting: 344it [03:24,  1.65it/s]Extractor Predicting: 345it [03:24,  1.73it/s]Extractor Predicting: 346it [03:25,  1.80it/s]Extractor Predicting: 347it [03:25,  1.87it/s]Extractor Predicting: 348it [03:26,  1.86it/s]Extractor Predicting: 349it [03:26,  1.89it/s]Extractor Predicting: 350it [03:27,  1.52it/s]Extractor Predicting: 351it [03:28,  1.62it/s]Extractor Predicting: 352it [03:28,  1.69it/s]Extractor Predicting: 353it [03:29,  1.76it/s]Extractor Predicting: 354it [03:29,  1.81it/s]Extractor Predicting: 355it [03:31,  1.41it/s]Extractor Predicting: 356it [03:31,  1.56it/s]Extractor Predicting: 357it [03:32,  1.41it/s]Extractor Predicting: 358it [03:32,  1.52it/s]Extractor Predicting: 359it [03:33,  1.61it/s]Extractor Predicting: 360it [03:34,  1.49it/s]Extractor Predicting: 361it [03:34,  1.59it/s]Extractor Predicting: 362it [03:35,  1.67it/s]Extractor Predicting: 363it [03:35,  1.74it/s]Extractor Predicting: 364it [03:36,  1.83it/s]Extractor Predicting: 365it [03:36,  1.78it/s]Extractor Predicting: 366it [03:37,  1.64it/s]Extractor Predicting: 367it [03:38,  1.63it/s]Extractor Predicting: 368it [03:38,  1.64it/s]Extractor Predicting: 369it [03:39,  1.72it/s]Extractor Predicting: 370it [03:39,  1.70it/s]Extractor Predicting: 371it [03:40,  1.68it/s]Extractor Predicting: 372it [03:41,  1.73it/s]Extractor Predicting: 373it [03:41,  1.76it/s]Extractor Predicting: 374it [03:42,  1.78it/s]Extractor Predicting: 375it [03:42,  1.76it/s]Extractor Predicting: 376it [03:43,  1.76it/s]Extractor Predicting: 377it [03:44,  1.64it/s]Extractor Predicting: 378it [03:44,  1.67it/s]Extractor Predicting: 379it [03:45,  1.69it/s]Extractor Predicting: 380it [03:45,  1.71it/s]Extractor Predicting: 381it [03:46,  1.76it/s]Extractor Predicting: 382it [03:46,  1.80it/s]Extractor Predicting: 383it [03:47,  1.54it/s]Extractor Predicting: 384it [03:48,  1.63it/s]Extractor Predicting: 385it [03:48,  1.66it/s]Extractor Predicting: 386it [03:49,  1.73it/s]Extractor Predicting: 387it [03:49,  1.77it/s]Extractor Predicting: 388it [03:50,  1.77it/s]Extractor Predicting: 389it [03:51,  1.74it/s]Extractor Predicting: 390it [03:51,  1.74it/s]Extractor Predicting: 391it [03:52,  1.77it/s]Extractor Predicting: 392it [03:52,  1.85it/s]Extractor Predicting: 393it [03:53,  1.89it/s]Extractor Predicting: 394it [03:53,  1.90it/s]Extractor Predicting: 395it [03:54,  1.92it/s]Extractor Predicting: 396it [03:54,  1.95it/s]Extractor Predicting: 397it [03:55,  1.82it/s]Extractor Predicting: 398it [03:55,  1.83it/s]Extractor Predicting: 399it [03:56,  1.85it/s]Extractor Predicting: 400it [03:56,  1.83it/s]Extractor Predicting: 401it [03:57,  1.87it/s]Extractor Predicting: 402it [03:57,  1.90it/s]Extractor Predicting: 403it [03:58,  1.66it/s]Extractor Predicting: 404it [03:59,  1.73it/s]Extractor Predicting: 405it [03:59,  1.74it/s]Extractor Predicting: 406it [04:00,  1.80it/s]Extractor Predicting: 407it [04:00,  1.82it/s]Extractor Predicting: 408it [04:01,  1.85it/s]Extractor Predicting: 409it [04:02,  1.56it/s]Extractor Predicting: 410it [04:02,  1.64it/s]Extractor Predicting: 411it [04:03,  1.69it/s]Extractor Predicting: 412it [04:03,  1.73it/s]Extractor Predicting: 413it [04:04,  1.69it/s]Extractor Predicting: 414it [04:05,  1.57it/s]Extractor Predicting: 415it [04:05,  1.57it/s]Extractor Predicting: 416it [04:06,  1.59it/s]Extractor Predicting: 417it [04:07,  1.62it/s]Extractor Predicting: 418it [04:28,  6.77s/it]Extractor Predicting: 419it [04:29,  5.01s/it]Extractor Predicting: 420it [04:29,  3.69s/it]Extractor Predicting: 421it [04:30,  2.76s/it]Extractor Predicting: 422it [04:30,  2.10s/it]Extractor Predicting: 423it [04:31,  1.64s/it]Extractor Predicting: 424it [04:32,  1.48s/it]Extractor Predicting: 425it [04:33,  1.21s/it]Extractor Predicting: 426it [04:33,  1.03s/it]Extractor Predicting: 427it [04:34,  1.12it/s]Extractor Predicting: 428it [04:34,  1.27it/s]Extractor Predicting: 429it [04:35,  1.17it/s]Extractor Predicting: 430it [04:36,  1.31it/s]Extractor Predicting: 431it [04:36,  1.42it/s]Extractor Predicting: 432it [04:37,  1.51it/s]Extractor Predicting: 433it [04:38,  1.57it/s]Extractor Predicting: 434it [04:38,  1.46it/s]Extractor Predicting: 435it [04:39,  1.56it/s]Extractor Predicting: 436it [04:39,  1.65it/s]Extractor Predicting: 437it [04:40,  1.68it/s]Extractor Predicting: 438it [04:41,  1.73it/s]Extractor Predicting: 439it [04:41,  1.74it/s]Extractor Predicting: 440it [04:42,  1.69it/s]Extractor Predicting: 441it [04:42,  1.70it/s]Extractor Predicting: 442it [04:43,  1.73it/s]Extractor Predicting: 443it [04:44,  1.72it/s]Extractor Predicting: 444it [04:44,  1.73it/s]Extractor Predicting: 445it [04:45,  1.72it/s]Extractor Predicting: 446it [04:45,  1.57it/s]Extractor Predicting: 447it [04:46,  1.63it/s]Extractor Predicting: 448it [04:47,  1.65it/s]Extractor Predicting: 449it [04:47,  1.71it/s]Extractor Predicting: 450it [04:48,  1.77it/s]Extractor Predicting: 451it [04:48,  1.76it/s]Extractor Predicting: 452it [04:49,  1.53it/s]Extractor Predicting: 453it [04:50,  1.59it/s]Extractor Predicting: 454it [04:50,  1.64it/s]Extractor Predicting: 455it [04:51,  1.65it/s]Extractor Predicting: 456it [04:51,  1.69it/s]Extractor Predicting: 457it [04:52,  1.59it/s]Extractor Predicting: 458it [04:53,  1.64it/s]Extractor Predicting: 459it [04:53,  1.67it/s]Extractor Predicting: 460it [04:54,  1.68it/s]Extractor Predicting: 461it [04:54,  1.72it/s]Extractor Predicting: 462it [04:55,  1.73it/s]Extractor Predicting: 463it [04:56,  1.61it/s]Extractor Predicting: 464it [04:56,  1.61it/s]Extractor Predicting: 465it [04:57,  1.63it/s]Extractor Predicting: 466it [04:57,  1.67it/s]Extractor Predicting: 467it [04:58,  1.74it/s]Extractor Predicting: 468it [04:59,  1.65it/s]Extractor Predicting: 469it [04:59,  1.74it/s]Extractor Predicting: 470it [05:00,  1.74it/s]Extractor Predicting: 471it [05:00,  1.73it/s]Extractor Predicting: 472it [05:01,  1.75it/s]Extractor Predicting: 473it [05:01,  1.74it/s]Extractor Predicting: 474it [05:02,  1.42it/s]Extractor Predicting: 475it [05:03,  1.50it/s]Extractor Predicting: 476it [05:04,  1.57it/s]Extractor Predicting: 477it [05:04,  1.62it/s]Extractor Predicting: 478it [05:05,  1.65it/s]Extractor Predicting: 479it [05:06,  1.26it/s]Extractor Predicting: 480it [05:07,  1.32it/s]Extractor Predicting: 481it [05:07,  1.39it/s]Extractor Predicting: 482it [05:08,  1.46it/s]Extractor Predicting: 483it [05:09,  1.43it/s]Extractor Predicting: 484it [05:09,  1.46it/s]Extractor Predicting: 485it [05:10,  1.49it/s]Extractor Predicting: 486it [05:10,  1.56it/s]Extractor Predicting: 487it [05:11,  1.61it/s]Extractor Predicting: 488it [05:12,  1.60it/s]Extractor Predicting: 489it [05:12,  1.57it/s]Extractor Predicting: 490it [05:13,  1.56it/s]Extractor Predicting: 491it [05:14,  1.56it/s]Extractor Predicting: 492it [05:14,  1.58it/s]Extractor Predicting: 493it [05:15,  1.59it/s]Extractor Predicting: 494it [05:15,  1.57it/s]Extractor Predicting: 495it [05:16,  1.59it/s]Extractor Predicting: 496it [05:17,  1.60it/s]Extractor Predicting: 497it [05:17,  1.60it/s]Extractor Predicting: 498it [05:18,  1.62it/s]Extractor Predicting: 499it [05:19,  1.45it/s]Extractor Predicting: 500it [05:19,  1.50it/s]Extractor Predicting: 501it [05:20,  1.56it/s]Extractor Predicting: 502it [05:21,  1.55it/s]Extractor Predicting: 503it [05:21,  1.55it/s]Extractor Predicting: 504it [05:23,  1.20it/s]Extractor Predicting: 505it [05:23,  1.29it/s]Extractor Predicting: 506it [05:24,  1.35it/s]Extractor Predicting: 507it [05:24,  1.44it/s]Extractor Predicting: 508it [05:25,  1.45it/s]Extractor Predicting: 508it [05:25,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:28,869 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:28,872 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:28,872 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:28,872 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:28,872 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:26:30,537 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:26:30,538 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:26:31,572 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:26:32,782 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:26:32,782 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:37,281 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:37,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:37,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:37,371 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:26:37,371 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:26:39,057 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:26:39,058 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:26:40,830 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:26:42,084 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:26:42,084 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:04,  1.30it/s]Extractor Predicting: 7it [00:04,  1.39it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.19it/s]Extractor Predicting: 12it [00:08,  1.30it/s]Extractor Predicting: 13it [00:08,  1.41it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.41it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:12,  1.56it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:13,  1.62it/s]Extractor Predicting: 21it [00:13,  1.63it/s]Extractor Predicting: 22it [00:14,  1.63it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.64it/s]Extractor Predicting: 26it [00:17,  1.65it/s]Extractor Predicting: 27it [00:17,  1.65it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.61it/s]Extractor Predicting: 31it [00:20,  1.69it/s]Extractor Predicting: 32it [00:20,  1.75it/s]Extractor Predicting: 33it [00:21,  1.64it/s]Extractor Predicting: 34it [00:21,  1.70it/s]Extractor Predicting: 35it [00:22,  1.72it/s]Extractor Predicting: 36it [00:23,  1.75it/s]Extractor Predicting: 37it [00:23,  1.74it/s]Extractor Predicting: 38it [00:24,  1.78it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:25,  1.63it/s]Extractor Predicting: 41it [00:26,  1.67it/s]Extractor Predicting: 42it [00:26,  1.75it/s]Extractor Predicting: 43it [00:27,  1.63it/s]Extractor Predicting: 44it [00:27,  1.64it/s]Extractor Predicting: 45it [00:28,  1.66it/s]Extractor Predicting: 46it [00:29,  1.71it/s]Extractor Predicting: 47it [00:29,  1.72it/s]Extractor Predicting: 48it [00:30,  1.73it/s]Extractor Predicting: 49it [00:30,  1.74it/s]Extractor Predicting: 50it [00:31,  1.58it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:32,  1.64it/s]Extractor Predicting: 53it [00:33,  1.68it/s]Extractor Predicting: 54it [00:33,  1.64it/s]Extractor Predicting: 55it [00:34,  1.55it/s]Extractor Predicting: 56it [00:35,  1.62it/s]Extractor Predicting: 57it [00:35,  1.66it/s]Extractor Predicting: 58it [00:36,  1.69it/s]Extractor Predicting: 59it [00:36,  1.72it/s]Extractor Predicting: 60it [00:37,  1.71it/s]Extractor Predicting: 61it [00:38,  1.55it/s]Extractor Predicting: 62it [00:38,  1.61it/s]Extractor Predicting: 63it [00:39,  1.66it/s]Extractor Predicting: 64it [00:40,  1.69it/s]Extractor Predicting: 65it [00:40,  1.67it/s]Extractor Predicting: 66it [00:41,  1.65it/s]Extractor Predicting: 67it [00:41,  1.64it/s]Extractor Predicting: 68it [00:42,  1.64it/s]Extractor Predicting: 69it [00:43,  1.62it/s]Extractor Predicting: 70it [00:43,  1.63it/s]Extractor Predicting: 71it [00:44,  1.64it/s]Extractor Predicting: 72it [00:44,  1.65it/s]Extractor Predicting: 73it [00:45,  1.68it/s]Extractor Predicting: 74it [00:46,  1.60it/s]Extractor Predicting: 75it [00:46,  1.82it/s]Extractor Predicting: 75it [00:46,  1.61it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_15_seed_3', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:29, 14.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:28<04:21, 14.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:48<04:48, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:03<04:16, 16.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:19<04:01, 16.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:36<03:51, 16.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:50<03:23, 15.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:05<03:05, 15.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:21<02:51, 15.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:37<02:36, 15.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:52<02:18, 15.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:08<02:05, 15.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:22<01:45, 15.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:37<01:31, 15.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:51<01:12, 14.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:09<01:03, 15.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:22<00:45, 15.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:36<00:29, 14.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:52<00:14, 14.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:08<00:00, 15.39s/it]Generating: 100%|██████████| 20/20 [05:08<00:00, 15.43s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 346, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 500, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.76875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 335, 'raw': 480}
{'target': 600, 'success': 358, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 410, 'raw': 576}
{'target': 600, 'success': 430, 'raw': 608}
{'target': 600, 'success': 454, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7118055555555556, 'errors': {'', "('συῆνος', 'said to be the same as', '', 'They are known as a form of Greek σάρτικρος ( συῆνος ) , τεῖς ( συθροτος ) , τίνρας ( συῆλώηνος ) ; and of the forms σατος τικρος in Latin .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : student .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 112, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 331, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 408, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 503, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7391826923076923, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 258, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 303, 'raw': 416}
{'target': 600, 'success': 327, 'raw': 448}
{'target': 600, 'success': 345, 'raw': 480}
{'target': 600, 'success': 372, 'raw': 512}
{'target': 600, 'success': 397, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 444, 'raw': 608}
{'target': 600, 'success': 466, 'raw': 640}
{'target': 600, 'success': 489, 'raw': 672}
{'target': 600, 'success': 511, 'raw': 704}
{'target': 600, 'success': 531, 'raw': 736}
{'target': 600, 'success': 558, 'raw': 768}
{'target': 600, 'success': 576, 'raw': 800}
{'target': 600, 'success': 598, 'raw': 832}
{'target': 600, 'success': 620, 'raw': 864}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7175925925925926, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8233695652173914, 'errors': {'', "('', 'continent', 'Aegean Sea', 'The island of Aranet , located in the Aegean Sea , is one of two islands that the Greek island of Argos has in common with the Arabian sea .')"}}
['Relation : field of this occupation . Context : The main characters in the series are The New Guy , The New York City Police Detective and the Los Angeles Police Department . Head Entity : Los Angeles Police Department , Tail Entity : police .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 539, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.76625, 'errors': {'', "('Walter L. Cusack', 'field of this occupation', '', 'Walter L. Cusack ( June 23 , 1924 February 21 , 2002 ) was an English cricket er who played six Tests in England .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : Following his graduation in 1957 , he founded the PSA ( now in association with B. C. ) , an academic association for linguistics , linguistics , and other fields , and served as a member of the Princeton University Research Committee . Head Entity : linguistics , Tail Entity : graduate .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 191, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 508, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : field of work .', 'success_rate': 0.76875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : founded by .', 'success_rate': 0.7838541666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n']
['Relation : given name . Context : The name Mödron is a medieval term meaning young man of noble ancestry at the end of the Stone Age ( from a given name , i. e. ) . Head Entity : mödron , Tail Entity : Mödron .\n', 'Relation : given name . Context : James M. Young ( February 14 , 1821 October 19 , 1924 ) was an American poet and writer specializing in the arts . Head Entity : James M. Young , Tail Entity : named name .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8206521739130435, 'errors': {'', 'too many values to unpack (expected 2)', "('Doctor Who', 'given name', '', 'He is best known for his play as a character on Doctor Who , in which he played the name of the villainess in the Doctor Who special , The One and Only Thing .')", "('Joseph L. M. Rennelly', 'given name', '', 'Joseph L. M. Rennelly is a former New York City mayor who served as the 62nd governor of New York City .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8383152173913043, 'errors': {'', "('Never Say Goodbye', 'lyrics by', '', 'After graduating from Berklee College of Music , he started to write a single called Never Say Goodbye that debuted at No. 1 on the Billboard 200 single Hot Hot Song in July 1991 .')", "('Never Let Me Go', 'lyrics by', '', 'She followed this up with her sixth studio album , Never Let Me Go , which is now a cult classic on the Billboard R&B Top 40 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8165760869565217, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 472, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8301630434782609, 'errors': {'', "('Central railway', 'owned by', '', 'It was built between 1941 and 1943 under the direction of the Central railway company ( D.')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9077380952380952, 'errors': {'', "('GMAW', 'performer', '', 'He also performed with the band Pampers on the opening night of WMAW in Los Angeles and the opening night of the ABC drama GMAW in Atlanta .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 128, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 185, 'raw': 288}
{'target': 600, 'success': 205, 'raw': 320}
{'target': 600, 'success': 223, 'raw': 352}
{'target': 600, 'success': 241, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 288, 'raw': 448}
{'target': 600, 'success': 304, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 371, 'raw': 576}
{'target': 600, 'success': 393, 'raw': 608}
{'target': 600, 'success': 417, 'raw': 640}
{'target': 600, 'success': 433, 'raw': 672}
{'target': 600, 'success': 451, 'raw': 704}
{'target': 600, 'success': 471, 'raw': 736}
{'target': 600, 'success': 495, 'raw': 768}
{'target': 600, 'success': 515, 'raw': 800}
{'target': 600, 'success': 540, 'raw': 832}
{'target': 600, 'success': 563, 'raw': 864}
{'target': 600, 'success': 583, 'raw': 896}
{'target': 600, 'success': 601, 'raw': 928}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.6476293103448276, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.8943452380952381, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 565, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : replaces .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/0_ext.jsonl'}}
estimate vocab size: 17944
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18044, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_15_seed_3/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.42s/it]Extractor Estimating: 2it [00:20,  8.72s/it]Extractor Estimating: 3it [00:20,  5.00s/it]Extractor Estimating: 4it [00:21,  3.28s/it]Extractor Estimating: 5it [00:21,  2.31s/it]Extractor Estimating: 6it [00:22,  1.73s/it]Extractor Estimating: 7it [00:22,  1.34s/it]Extractor Estimating: 8it [00:23,  1.10s/it]Extractor Estimating: 9it [00:24,  1.06it/s]Extractor Estimating: 10it [00:24,  1.19it/s]Extractor Estimating: 11it [00:25,  1.32it/s]Extractor Estimating: 12it [00:25,  1.41it/s]Extractor Estimating: 13it [00:26,  1.49it/s]Extractor Estimating: 14it [00:27,  1.59it/s]Extractor Estimating: 15it [00:27,  1.60it/s]Extractor Estimating: 16it [00:28,  1.40it/s]Extractor Estimating: 17it [00:29,  1.51it/s]Extractor Estimating: 18it [00:29,  1.53it/s]Extractor Estimating: 19it [00:30,  1.56it/s]Extractor Estimating: 20it [00:30,  1.60it/s]Extractor Estimating: 21it [00:31,  1.48it/s]Extractor Estimating: 22it [00:32,  1.35it/s]Extractor Estimating: 23it [00:33,  1.42it/s]Extractor Estimating: 24it [00:33,  1.51it/s]Extractor Estimating: 25it [00:34,  1.52it/s]Extractor Estimating: 26it [00:37,  1.35s/it]Extractor Estimating: 27it [00:38,  1.13s/it]Extractor Estimating: 28it [00:39,  1.15s/it]Extractor Estimating: 29it [00:39,  1.03it/s]Extractor Estimating: 30it [00:40,  1.18it/s]Extractor Estimating: 31it [00:40,  1.31it/s]Extractor Estimating: 32it [00:41,  1.44it/s]Extractor Estimating: 33it [00:42,  1.42it/s]Extractor Estimating: 34it [00:42,  1.47it/s]Extractor Estimating: 35it [00:43,  1.54it/s]Extractor Estimating: 36it [00:43,  1.60it/s]Extractor Estimating: 37it [00:44,  1.66it/s]Extractor Estimating: 38it [00:45,  1.61it/s]Extractor Estimating: 39it [00:45,  1.60it/s]Extractor Estimating: 40it [00:46,  1.66it/s]Extractor Estimating: 41it [00:46,  1.73it/s]Extractor Estimating: 42it [00:47,  1.74it/s]Extractor Estimating: 43it [00:48,  1.73it/s]Extractor Estimating: 44it [00:48,  1.68it/s]Extractor Estimating: 45it [00:49,  1.71it/s]Extractor Estimating: 46it [00:49,  1.70it/s]Extractor Estimating: 47it [00:50,  1.66it/s]Extractor Estimating: 48it [00:51,  1.72it/s]Extractor Estimating: 49it [00:51,  1.73it/s]Extractor Estimating: 50it [00:52,  1.41it/s]Extractor Estimating: 51it [00:53,  1.50it/s]Extractor Estimating: 52it [00:53,  1.55it/s]Extractor Estimating: 53it [00:54,  1.57it/s]Extractor Estimating: 54it [00:54,  1.62it/s]Extractor Estimating: 55it [00:56,  1.30it/s]Extractor Estimating: 56it [00:56,  1.34it/s]Extractor Estimating: 57it [00:57,  1.40it/s]Extractor Estimating: 58it [00:58,  1.46it/s]Extractor Estimating: 59it [00:58,  1.33it/s]Extractor Estimating: 60it [00:59,  1.40it/s]Extractor Estimating: 61it [01:00,  1.48it/s]Extractor Estimating: 62it [01:00,  1.54it/s]Extractor Estimating: 63it [01:01,  1.59it/s]Extractor Estimating: 64it [01:02,  1.47it/s]Extractor Estimating: 65it [01:02,  1.52it/s]Extractor Estimating: 66it [01:03,  1.53it/s]Extractor Estimating: 67it [01:04,  1.44it/s]Extractor Estimating: 68it [01:04,  1.49it/s]Extractor Estimating: 69it [01:05,  1.52it/s]Extractor Estimating: 70it [01:06,  1.32it/s]Extractor Estimating: 71it [01:07,  1.35it/s]Extractor Estimating: 72it [01:07,  1.45it/s]Extractor Estimating: 73it [01:08,  1.49it/s]Extractor Estimating: 74it [01:08,  1.54it/s]Extractor Estimating: 75it [01:09,  1.48it/s]Extractor Estimating: 76it [01:10,  1.53it/s]Extractor Estimating: 77it [01:10,  1.57it/s]Extractor Estimating: 78it [01:11,  1.57it/s]Extractor Estimating: 79it [01:12,  1.62it/s]Extractor Estimating: 80it [01:12,  1.54it/s]Extractor Estimating: 81it [01:13,  1.57it/s]Extractor Estimating: 82it [01:13,  1.63it/s]Extractor Estimating: 83it [01:14,  1.66it/s]Extractor Estimating: 84it [01:15,  1.63it/s]Extractor Estimating: 85it [01:15,  1.47it/s]Extractor Estimating: 86it [01:16,  1.51it/s]Extractor Estimating: 87it [01:17,  1.56it/s]Extractor Estimating: 88it [01:17,  1.57it/s]Extractor Estimating: 89it [01:18,  1.60it/s]Extractor Estimating: 90it [01:19,  1.52it/s]Extractor Estimating: 91it [01:19,  1.59it/s]Extractor Estimating: 92it [01:20,  1.57it/s]Extractor Estimating: 93it [01:20,  1.62it/s]Extractor Estimating: 94it [01:21,  1.61it/s]Extractor Estimating: 95it [01:22,  1.53it/s]Extractor Estimating: 96it [01:22,  1.57it/s]Extractor Estimating: 97it [01:23,  1.59it/s]Extractor Estimating: 98it [01:24,  1.60it/s]Extractor Estimating: 99it [01:24,  1.61it/s]Extractor Estimating: 100it [01:25,  1.51it/s]Extractor Estimating: 101it [01:26,  1.56it/s]Extractor Estimating: 102it [01:26,  1.57it/s]Extractor Estimating: 103it [01:27,  1.59it/s]Extractor Estimating: 104it [01:27,  1.57it/s]Extractor Estimating: 105it [01:28,  1.42it/s]Extractor Estimating: 106it [01:29,  1.49it/s]Extractor Estimating: 107it [01:30,  1.53it/s]Extractor Estimating: 108it [01:30,  1.59it/s]Extractor Estimating: 109it [01:31,  1.59it/s]Extractor Estimating: 110it [01:32,  1.36it/s]Extractor Estimating: 111it [01:32,  1.44it/s]Extractor Estimating: 112it [01:33,  1.49it/s]Extractor Estimating: 113it [01:34,  1.47it/s]Extractor Estimating: 114it [01:34,  1.51it/s]Extractor Estimating: 115it [01:35,  1.55it/s]Extractor Estimating: 116it [01:35,  1.59it/s]Extractor Estimating: 117it [01:36,  1.52it/s]Extractor Estimating: 118it [01:37,  1.58it/s]Extractor Estimating: 119it [01:37,  1.58it/s]Extractor Estimating: 120it [01:38,  1.61it/s]Extractor Estimating: 121it [01:39,  1.64it/s]Extractor Estimating: 122it [01:39,  1.55it/s]Extractor Estimating: 123it [01:40,  1.59it/s]Extractor Estimating: 124it [01:40,  1.62it/s]Extractor Estimating: 125it [01:41,  1.63it/s]Extractor Estimating: 126it [01:42,  1.61it/s]Extractor Estimating: 127it [01:42,  1.57it/s]Extractor Estimating: 128it [01:43,  1.60it/s]Extractor Estimating: 129it [01:44,  1.61it/s]Extractor Estimating: 130it [01:44,  1.59it/s]Extractor Estimating: 131it [01:45,  1.58it/s]Extractor Estimating: 132it [01:46,  1.51it/s]Extractor Estimating: 133it [01:46,  1.59it/s]Extractor Estimating: 134it [01:47,  1.62it/s]Extractor Estimating: 135it [01:47,  1.67it/s]Extractor Estimating: 136it [01:48,  1.64it/s]Extractor Estimating: 137it [01:49,  1.55it/s]Extractor Estimating: 138it [01:49,  1.54it/s]Extractor Estimating: 139it [01:50,  1.58it/s]Extractor Estimating: 140it [01:51,  1.60it/s]Extractor Estimating: 141it [01:51,  1.61it/s]Extractor Estimating: 142it [01:52,  1.24it/s]Extractor Estimating: 143it [01:53,  1.32it/s]Extractor Estimating: 144it [01:54,  1.40it/s]Extractor Estimating: 145it [01:54,  1.48it/s]Extractor Estimating: 146it [01:55,  1.20it/s]Extractor Estimating: 147it [01:56,  1.27it/s]Extractor Estimating: 148it [01:57,  1.36it/s]Extractor Estimating: 149it [01:57,  1.42it/s]Extractor Estimating: 150it [01:59,  1.10it/s]Extractor Estimating: 151it [01:59,  1.23it/s]Extractor Estimating: 152it [02:00,  1.31it/s]Extractor Estimating: 153it [02:00,  1.45it/s]Extractor Estimating: 154it [02:01,  1.41it/s]Extractor Estimating: 155it [02:02,  1.49it/s]Extractor Estimating: 156it [02:02,  1.56it/s]Extractor Estimating: 157it [02:03,  1.63it/s]Extractor Estimating: 158it [02:04,  1.26it/s]Extractor Estimating: 159it [02:05,  1.36it/s]Extractor Estimating: 160it [02:05,  1.48it/s]Extractor Estimating: 161it [02:06,  1.56it/s]Extractor Estimating: 162it [02:06,  1.65it/s]Extractor Estimating: 163it [02:08,  1.25it/s]Extractor Estimating: 164it [02:08,  1.38it/s]Extractor Estimating: 165it [02:09,  1.48it/s]Extractor Estimating: 166it [02:09,  1.58it/s]Extractor Estimating: 167it [02:10,  1.65it/s]Extractor Estimating: 168it [02:12,  1.00it/s]Extractor Estimating: 169it [02:12,  1.18it/s]Extractor Estimating: 170it [02:13,  1.29it/s]Extractor Estimating: 171it [02:14,  1.02it/s]Extractor Estimating: 172it [02:15,  1.19it/s]Extractor Estimating: 173it [02:15,  1.33it/s]Extractor Estimating: 174it [02:16,  1.45it/s]Extractor Estimating: 175it [02:17,  1.15it/s]Extractor Estimating: 176it [02:18,  1.27it/s]Extractor Estimating: 177it [02:18,  1.36it/s]Extractor Estimating: 178it [02:19,  1.47it/s]Extractor Estimating: 179it [02:20,  1.31it/s]Extractor Estimating: 180it [02:21,  1.40it/s]Extractor Estimating: 181it [02:21,  1.42it/s]Extractor Estimating: 182it [02:22,  1.53it/s]Extractor Estimating: 183it [02:22,  1.57it/s]Extractor Estimating: 184it [02:24,  1.04it/s]Extractor Estimating: 185it [02:25,  1.16it/s]Extractor Estimating: 186it [02:25,  1.27it/s]Extractor Estimating: 187it [02:26,  1.37it/s]Extractor Estimating: 188it [02:27,  1.08it/s]Extractor Estimating: 189it [02:28,  1.22it/s]Extractor Estimating: 190it [02:28,  1.33it/s]Extractor Estimating: 191it [02:29,  1.43it/s]Extractor Estimating: 192it [02:30,  1.21it/s]Extractor Estimating: 193it [02:31,  1.34it/s]Extractor Estimating: 194it [02:31,  1.41it/s]Extractor Estimating: 195it [02:32,  1.48it/s]Extractor Estimating: 196it [02:33,  1.52it/s]Extractor Estimating: 197it [02:34,  1.22it/s]Extractor Estimating: 198it [02:34,  1.31it/s]Extractor Estimating: 199it [02:35,  1.37it/s]Extractor Estimating: 200it [02:36,  1.49it/s]Extractor Estimating: 201it [02:36,  1.52it/s]Extractor Estimating: 202it [02:37,  1.56it/s]Extractor Estimating: 203it [02:37,  1.55it/s]Extractor Estimating: 204it [02:38,  1.36it/s]Extractor Estimating: 205it [02:39,  1.40it/s]Extractor Estimating: 206it [02:40,  1.43it/s]Extractor Estimating: 207it [02:40,  1.48it/s]Extractor Estimating: 208it [02:41,  1.53it/s]Extractor Estimating: 209it [02:42,  1.27it/s]Extractor Estimating: 210it [02:43,  1.37it/s]Extractor Estimating: 211it [02:43,  1.44it/s]Extractor Estimating: 212it [02:44,  1.48it/s]Extractor Estimating: 213it [02:44,  1.53it/s]Extractor Estimating: 214it [02:45,  1.30it/s]Extractor Estimating: 215it [02:46,  1.35it/s]Extractor Estimating: 216it [02:47,  1.42it/s]Extractor Estimating: 217it [02:47,  1.47it/s]Extractor Estimating: 218it [02:48,  1.56it/s]Extractor Estimating: 219it [02:49,  1.18it/s]Extractor Estimating: 220it [02:50,  1.27it/s]Extractor Estimating: 221it [02:51,  1.39it/s]Extractor Estimating: 222it [02:51,  1.43it/s]Extractor Estimating: 223it [02:52,  1.46it/s]Extractor Estimating: 224it [02:52,  1.48it/s]Extractor Estimating: 225it [02:53,  1.53it/s]Extractor Estimating: 226it [02:54,  1.56it/s]Extractor Estimating: 227it [02:54,  1.58it/s]Extractor Estimating: 228it [02:55,  1.57it/s]Extractor Estimating: 229it [02:56,  1.61it/s]Extractor Estimating: 230it [02:56,  1.46it/s]Extractor Estimating: 231it [02:57,  1.54it/s]Extractor Estimating: 232it [02:58,  1.52it/s]Extractor Estimating: 233it [02:58,  1.41it/s]Extractor Estimating: 234it [02:59,  1.47it/s]Extractor Estimating: 235it [03:00,  1.54it/s]Extractor Estimating: 236it [03:00,  1.60it/s]Extractor Estimating: 237it [03:01,  1.60it/s]Extractor Estimating: 238it [03:02,  1.43it/s]Extractor Estimating: 239it [03:02,  1.50it/s]Extractor Estimating: 240it [03:03,  1.56it/s]Extractor Estimating: 241it [03:03,  1.59it/s]Extractor Estimating: 242it [03:04,  1.59it/s]Extractor Estimating: 243it [03:05,  1.52it/s]Extractor Estimating: 244it [03:05,  1.55it/s]Extractor Estimating: 245it [03:06,  1.58it/s]Extractor Estimating: 246it [03:07,  1.60it/s]Extractor Estimating: 247it [03:07,  1.53it/s]Extractor Estimating: 248it [03:08,  1.59it/s]Extractor Estimating: 249it [03:09,  1.37it/s]Extractor Estimating: 250it [03:09,  1.46it/s]Extractor Estimating: 251it [03:10,  1.54it/s]Extractor Estimating: 252it [03:11,  1.53it/s]Extractor Estimating: 253it [03:11,  1.50it/s]Extractor Estimating: 254it [03:12,  1.40it/s]Extractor Estimating: 255it [03:13,  1.47it/s]Extractor Estimating: 256it [03:13,  1.52it/s]Extractor Estimating: 257it [03:14,  1.56it/s]Extractor Estimating: 258it [03:15,  1.58it/s]Extractor Estimating: 259it [03:15,  1.48it/s]Extractor Estimating: 260it [03:16,  1.49it/s]Extractor Estimating: 261it [03:17,  1.52it/s]Extractor Estimating: 262it [03:17,  1.56it/s]Extractor Estimating: 263it [03:18,  1.59it/s]Extractor Estimating: 264it [03:19,  1.36it/s]Extractor Estimating: 265it [03:20,  1.42it/s]Extractor Estimating: 266it [03:20,  1.45it/s]Extractor Estimating: 267it [03:21,  1.51it/s]Extractor Estimating: 268it [03:21,  1.51it/s]Extractor Estimating: 269it [03:22,  1.46it/s]Extractor Estimating: 270it [03:23,  1.52it/s]Extractor Estimating: 271it [03:23,  1.52it/s]Extractor Estimating: 272it [03:24,  1.56it/s]Extractor Estimating: 273it [03:25,  1.54it/s]Extractor Estimating: 274it [03:25,  1.53it/s]Extractor Estimating: 275it [03:26,  1.50it/s]Extractor Estimating: 276it [03:27,  1.57it/s]Extractor Estimating: 277it [03:27,  1.57it/s]Extractor Estimating: 278it [03:28,  1.54it/s]Extractor Estimating: 279it [03:29,  1.43it/s]Extractor Estimating: 280it [03:29,  1.46it/s]Extractor Estimating: 281it [03:30,  1.47it/s]Extractor Estimating: 282it [03:31,  1.54it/s]Extractor Estimating: 283it [03:31,  1.56it/s]Extractor Estimating: 284it [03:32,  1.32it/s]Extractor Estimating: 285it [03:33,  1.39it/s]Extractor Estimating: 286it [03:34,  1.42it/s]Extractor Estimating: 287it [03:34,  1.48it/s]Extractor Estimating: 288it [03:35,  1.49it/s]Extractor Estimating: 289it [03:36,  1.46it/s]Extractor Estimating: 290it [03:36,  1.39it/s]Extractor Estimating: 291it [03:37,  1.43it/s]Extractor Estimating: 292it [03:38,  1.49it/s]Extractor Estimating: 293it [03:38,  1.49it/s]Extractor Estimating: 294it [03:39,  1.42it/s]Extractor Estimating: 295it [03:40,  1.43it/s]Extractor Estimating: 296it [03:40,  1.47it/s]Extractor Estimating: 297it [03:41,  1.51it/s]Extractor Estimating: 298it [03:42,  1.50it/s]Extractor Estimating: 299it [03:43,  1.18it/s]Extractor Estimating: 300it [03:44,  1.29it/s]Extractor Estimating: 301it [03:44,  1.35it/s]Extractor Estimating: 302it [03:45,  1.43it/s]Extractor Estimating: 303it [03:46,  1.43it/s]Extractor Estimating: 304it [03:46,  1.48it/s]Extractor Estimating: 305it [03:47,  1.51it/s]Extractor Estimating: 306it [03:47,  1.55it/s]Extractor Estimating: 307it [03:48,  1.62it/s]Extractor Estimating: 308it [03:49,  1.57it/s]Extractor Estimating: 309it [03:49,  1.45it/s]Extractor Estimating: 310it [03:50,  1.48it/s]Extractor Estimating: 311it [03:51,  1.48it/s]Extractor Estimating: 312it [03:51,  1.53it/s]Extractor Estimating: 313it [03:52,  1.48it/s]Extractor Estimating: 314it [03:53,  1.45it/s]Extractor Estimating: 315it [03:54,  1.47it/s]Extractor Estimating: 316it [03:54,  1.49it/s]Extractor Estimating: 317it [03:55,  1.54it/s]Extractor Estimating: 318it [03:56,  1.48it/s]Extractor Estimating: 319it [03:56,  1.47it/s]Extractor Estimating: 320it [03:57,  1.55it/s]Extractor Estimating: 321it [03:57,  1.59it/s]Extractor Estimating: 322it [03:58,  1.65it/s]Extractor Estimating: 323it [03:59,  1.50it/s]Extractor Estimating: 324it [03:59,  1.52it/s]Extractor Estimating: 325it [04:00,  1.55it/s]Extractor Estimating: 326it [04:01,  1.60it/s]Extractor Estimating: 327it [04:01,  1.63it/s]Extractor Estimating: 328it [04:02,  1.35it/s]Extractor Estimating: 329it [04:03,  1.43it/s]Extractor Estimating: 330it [04:03,  1.51it/s]Extractor Estimating: 331it [04:04,  1.54it/s]Extractor Estimating: 332it [04:05,  1.56it/s]Extractor Estimating: 333it [04:06,  1.18it/s]Extractor Estimating: 334it [04:06,  1.31it/s]Extractor Estimating: 335it [04:07,  1.41it/s]Extractor Estimating: 336it [04:08,  1.46it/s]Extractor Estimating: 337it [04:08,  1.51it/s]Extractor Estimating: 338it [04:09,  1.54it/s]Extractor Estimating: 339it [04:09,  1.61it/s]Extractor Estimating: 340it [04:10,  1.62it/s]Extractor Estimating: 341it [04:11,  1.63it/s]Extractor Estimating: 342it [04:11,  1.68it/s]Extractor Estimating: 343it [04:12,  1.61it/s]Extractor Estimating: 344it [04:13,  1.62it/s]Extractor Estimating: 345it [04:13,  1.61it/s]Extractor Estimating: 346it [04:14,  1.67it/s]Extractor Estimating: 347it [04:14,  1.67it/s]Extractor Estimating: 348it [04:15,  1.70it/s]Extractor Estimating: 349it [04:15,  1.68it/s]Extractor Estimating: 350it [04:16,  1.70it/s]Extractor Estimating: 351it [04:17,  1.58it/s]Extractor Estimating: 352it [04:17,  1.60it/s]Extractor Estimating: 353it [04:18,  1.59it/s]Extractor Estimating: 354it [04:19,  1.59it/s]Extractor Estimating: 355it [04:19,  1.63it/s]Extractor Estimating: 356it [04:20,  1.49it/s]Extractor Estimating: 357it [04:21,  1.54it/s]Extractor Estimating: 358it [04:21,  1.57it/s]Extractor Estimating: 359it [04:22,  1.54it/s]Extractor Estimating: 360it [04:23,  1.60it/s]Extractor Estimating: 361it [04:23,  1.55it/s]Extractor Estimating: 362it [04:24,  1.60it/s]Extractor Estimating: 363it [04:24,  1.57it/s]Extractor Estimating: 364it [04:25,  1.60it/s]Extractor Estimating: 365it [04:26,  1.62it/s]Extractor Estimating: 366it [04:26,  1.53it/s]Extractor Estimating: 367it [04:27,  1.57it/s]Extractor Estimating: 368it [04:28,  1.65it/s]Extractor Estimating: 369it [04:28,  1.60it/s]Extractor Estimating: 370it [04:29,  1.55it/s]Extractor Estimating: 371it [04:30,  1.53it/s]Extractor Estimating: 372it [04:30,  1.52it/s]Extractor Estimating: 373it [04:31,  1.53it/s]Extractor Estimating: 374it [04:31,  1.57it/s]Extractor Estimating: 375it [04:32,  1.62it/s]Extractor Estimating: 376it [04:33,  1.51it/s]Extractor Estimating: 377it [04:33,  1.53it/s]Extractor Estimating: 378it [04:34,  1.54it/s]Extractor Estimating: 379it [04:35,  1.57it/s]Extractor Estimating: 380it [04:35,  1.54it/s]Extractor Estimating: 381it [04:36,  1.46it/s]Extractor Estimating: 382it [04:37,  1.49it/s]Extractor Estimating: 383it [04:38,  1.32it/s]Extractor Estimating: 384it [04:38,  1.33it/s]Extractor Estimating: 385it [04:39,  1.40it/s]Extractor Estimating: 386it [04:40,  1.46it/s]Extractor Estimating: 387it [04:40,  1.47it/s]Extractor Estimating: 388it [04:41,  1.47it/s]Extractor Estimating: 389it [04:42,  1.48it/s]Extractor Estimating: 390it [04:42,  1.46it/s]Extractor Estimating: 391it [04:43,  1.39it/s]Extractor Estimating: 392it [04:44,  1.43it/s]Extractor Estimating: 393it [04:45,  1.36it/s]Extractor Estimating: 394it [04:45,  1.38it/s]Extractor Estimating: 395it [04:46,  1.44it/s]Extractor Estimating: 396it [04:47,  1.48it/s]Extractor Estimating: 397it [04:47,  1.49it/s]Extractor Estimating: 398it [04:48,  1.46it/s]Extractor Estimating: 399it [04:49,  1.46it/s]Extractor Estimating: 400it [04:49,  1.46it/s]Extractor Estimating: 401it [04:50,  1.46it/s]Extractor Estimating: 402it [04:51,  1.53it/s]Extractor Estimating: 403it [04:52,  1.21it/s]Extractor Estimating: 404it [04:53,  1.30it/s]Extractor Estimating: 405it [04:53,  1.39it/s]Extractor Estimating: 406it [04:54,  1.45it/s]Extractor Estimating: 407it [04:55,  1.25it/s]Extractor Estimating: 408it [04:55,  1.32it/s]Extractor Estimating: 409it [04:56,  1.38it/s]Extractor Estimating: 410it [04:57,  1.46it/s]Extractor Estimating: 411it [04:57,  1.51it/s]Extractor Estimating: 412it [04:58,  1.39it/s]Extractor Estimating: 413it [04:59,  1.44it/s]Extractor Estimating: 414it [04:59,  1.46it/s]Extractor Estimating: 415it [05:00,  1.47it/s]Extractor Estimating: 416it [05:01,  1.52it/s]Extractor Estimating: 417it [05:01,  1.49it/s]Extractor Estimating: 418it [05:02,  1.55it/s]Extractor Estimating: 419it [05:03,  1.58it/s]Extractor Estimating: 420it [05:03,  1.53it/s]Extractor Estimating: 421it [05:04,  1.59it/s]Extractor Estimating: 422it [05:05,  1.52it/s]Extractor Estimating: 423it [05:05,  1.58it/s]Extractor Estimating: 424it [05:06,  1.62it/s]Extractor Estimating: 425it [05:06,  1.64it/s]Extractor Estimating: 426it [05:07,  1.59it/s]Extractor Estimating: 427it [05:08,  1.59it/s]Extractor Estimating: 428it [05:08,  1.61it/s]Extractor Estimating: 429it [05:09,  1.63it/s]Extractor Estimating: 430it [05:10,  1.61it/s]Extractor Estimating: 431it [05:10,  1.65it/s]Extractor Estimating: 432it [05:11,  1.65it/s]Extractor Estimating: 433it [05:11,  1.57it/s]Extractor Estimating: 434it [05:12,  1.55it/s]Extractor Estimating: 435it [05:13,  1.56it/s]Extractor Estimating: 436it [05:13,  1.55it/s]Extractor Estimating: 437it [05:14,  1.57it/s]Extractor Estimating: 438it [05:15,  1.46it/s]Extractor Estimating: 439it [05:15,  1.44it/s]Extractor Estimating: 440it [05:16,  1.51it/s]Extractor Estimating: 441it [05:17,  1.51it/s]Extractor Estimating: 442it [05:17,  1.56it/s]Extractor Estimating: 443it [05:19,  1.17it/s]Extractor Estimating: 444it [05:19,  1.28it/s]Extractor Estimating: 445it [05:20,  1.35it/s]Extractor Estimating: 446it [05:21,  1.38it/s]Extractor Estimating: 447it [05:22,  1.25it/s]Extractor Estimating: 448it [05:22,  1.32it/s]Extractor Estimating: 449it [05:23,  1.42it/s]Extractor Estimating: 450it [05:23,  1.49it/s]Extractor Estimating: 451it [05:24,  1.50it/s]Extractor Estimating: 452it [05:25,  1.15it/s]Extractor Estimating: 453it [05:26,  1.26it/s]Extractor Estimating: 454it [05:27,  1.31it/s]Extractor Estimating: 455it [05:27,  1.38it/s]Extractor Estimating: 456it [05:28,  1.31it/s]Extractor Estimating: 457it [05:29,  1.36it/s]Extractor Estimating: 458it [05:30,  1.40it/s]Extractor Estimating: 459it [05:30,  1.41it/s]Extractor Estimating: 460it [05:31,  1.45it/s]Extractor Estimating: 461it [05:32,  1.38it/s]Extractor Estimating: 462it [05:32,  1.39it/s]Extractor Estimating: 463it [05:33,  1.40it/s]Extractor Estimating: 464it [05:34,  1.46it/s]Extractor Estimating: 465it [05:34,  1.48it/s]Extractor Estimating: 466it [05:35,  1.45it/s]Extractor Estimating: 467it [05:36,  1.52it/s]Extractor Estimating: 468it [05:36,  1.61it/s]Extractor Estimating: 469it [05:37,  1.58it/s]Extractor Estimating: 470it [05:37,  1.62it/s]Extractor Estimating: 471it [05:38,  1.51it/s]Extractor Estimating: 472it [05:39,  1.21it/s]Extractor Estimating: 473it [05:40,  1.16it/s]Extractor Estimating: 474it [05:41,  1.25it/s]Extractor Estimating: 475it [05:42,  1.33it/s]Extractor Estimating: 476it [05:42,  1.34it/s]Extractor Estimating: 477it [05:43,  1.38it/s]Extractor Estimating: 478it [05:44,  1.43it/s]Extractor Estimating: 479it [05:44,  1.50it/s]Extractor Estimating: 480it [05:45,  1.54it/s]Extractor Estimating: 481it [05:46,  1.39it/s]Extractor Estimating: 482it [05:46,  1.44it/s]Extractor Estimating: 483it [05:47,  1.49it/s]Extractor Estimating: 484it [05:48,  1.52it/s]Extractor Estimating: 485it [05:48,  1.54it/s]Extractor Estimating: 486it [05:49,  1.38it/s]Extractor Estimating: 487it [05:50,  1.44it/s]Extractor Estimating: 488it [05:50,  1.51it/s]Extractor Estimating: 489it [05:51,  1.51it/s]Extractor Estimating: 490it [05:52,  1.47it/s]Extractor Estimating: 491it [05:53,  1.31it/s]Extractor Estimating: 492it [05:53,  1.38it/s]Extractor Estimating: 493it [05:54,  1.48it/s]Extractor Estimating: 494it [05:55,  1.52it/s]Extractor Estimating: 495it [05:55,  1.51it/s]Extractor Estimating: 496it [05:56,  1.35it/s]Extractor Estimating: 497it [05:57,  1.43it/s]Extractor Estimating: 498it [05:57,  1.45it/s]Extractor Estimating: 499it [05:58,  1.46it/s]Extractor Estimating: 500it [05:59,  1.19it/s]Extractor Estimating: 500it [06:00,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9787 mean pseudo reward: 0.8963699578883085
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 31774
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31874, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_filtered_large/unseen_15_seed_3/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31874, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.246, loss:1318.7803
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.921, loss:1242.4655
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.936, loss:1141.7536
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.965, loss:1166.9207
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 92, avg_time 0.947, loss:1132.5437
>> valid entity prec:0.5650, rec:0.3109, f1:0.4011
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 192, avg_time 2.485, loss:1117.9400
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 292, avg_time 0.959, loss:1138.9886
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 392, avg_time 0.943, loss:1147.3711
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 84, avg_time 0.988, loss:1108.4303
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 184, avg_time 0.929, loss:1110.7570
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4776, rec:0.3930, f1:0.4312
>> valid relation prec:0.0017, rec:0.0002, f1:0.0004
>> valid relation with NER prec:0.0017, rec:0.0002, f1:0.0004
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 284, avg_time 2.450, loss:1108.1976
g_step 1200, step 384, avg_time 0.936, loss:1111.4529
g_step 1300, step 76, avg_time 0.915, loss:1045.1592
g_step 1400, step 176, avg_time 0.914, loss:1049.7814
g_step 1500, step 276, avg_time 0.922, loss:1092.0997
>> valid entity prec:0.3876, rec:0.2752, f1:0.3219
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 376, avg_time 2.416, loss:1025.6455
g_step 1700, step 68, avg_time 0.920, loss:992.5699
g_step 1800, step 168, avg_time 0.912, loss:1016.9278
g_step 1900, step 268, avg_time 0.918, loss:1006.4454
g_step 2000, step 368, avg_time 0.922, loss:1041.4202
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4533, rec:0.3951, f1:0.4222
>> valid relation prec:0.0013, rec:0.0002, f1:0.0004
>> valid relation with NER prec:0.0013, rec:0.0002, f1:0.0004
g_step 2100, step 60, avg_time 2.423, loss:973.9538
g_step 2200, step 160, avg_time 0.921, loss:956.7148
g_step 2300, step 260, avg_time 0.914, loss:958.8212
g_step 2400, step 360, avg_time 0.923, loss:999.3400
g_step 2500, step 52, avg_time 0.916, loss:918.9428
>> valid entity prec:0.4442, rec:0.3757, f1:0.4071
>> valid relation prec:0.0087, rec:0.0012, f1:0.0022
>> valid relation with NER prec:0.0087, rec:0.0012, f1:0.0022
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 152, avg_time 2.428, loss:926.7198
g_step 2700, step 252, avg_time 0.927, loss:909.9968
g_step 2800, step 352, avg_time 0.924, loss:968.8931
g_step 2900, step 44, avg_time 0.942, loss:930.3270
g_step 3000, step 144, avg_time 0.946, loss:897.1229
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4389, rec:0.4456, f1:0.4422
>> valid relation prec:0.0014, rec:0.0002, f1:0.0004
>> valid relation with NER prec:0.0014, rec:0.0002, f1:0.0004
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 244, avg_time 2.470, loss:919.7503
g_step 3200, step 344, avg_time 0.924, loss:907.2080
g_step 3300, step 36, avg_time 0.918, loss:872.2122
g_step 3400, step 136, avg_time 0.926, loss:857.7241
g_step 3500, step 236, avg_time 0.937, loss:857.5545
>> valid entity prec:0.4251, rec:0.3473, f1:0.3823
>> valid relation prec:0.0036, rec:0.0004, f1:0.0007
>> valid relation with NER prec:0.0036, rec:0.0004, f1:0.0007
g_step 3600, step 336, avg_time 2.423, loss:882.9750
g_step 3700, step 28, avg_time 0.931, loss:848.0699
g_step 3800, step 128, avg_time 0.922, loss:837.3856
g_step 3900, step 228, avg_time 0.936, loss:871.7555
g_step 4000, step 328, avg_time 0.920, loss:876.6351
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4477, rec:0.5226, f1:0.4822
>> valid relation prec:0.0597, rec:0.0103, f1:0.0176
>> valid relation with NER prec:0.0597, rec:0.0103, f1:0.0176
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 20, avg_time 2.444, loss:810.0463
g_step 4200, step 120, avg_time 0.930, loss:822.6208
g_step 4300, step 220, avg_time 0.937, loss:796.9380
g_step 4400, step 320, avg_time 0.922, loss:844.9110
g_step 4500, step 12, avg_time 0.913, loss:800.9703
>> valid entity prec:0.4468, rec:0.4521, f1:0.4494
>> valid relation prec:0.0119, rec:0.0021, f1:0.0035
>> valid relation with NER prec:0.0119, rec:0.0021, f1:0.0035
g_step 4600, step 112, avg_time 2.430, loss:769.4929
g_step 4700, step 212, avg_time 0.931, loss:788.1027
g_step 4800, step 312, avg_time 0.926, loss:807.1606
g_step 4900, step 4, avg_time 0.914, loss:820.4986
g_step 5000, step 104, avg_time 0.916, loss:786.7987
learning rate was adjusted to 0.0008
>> valid entity prec:0.5216, rec:0.4058, f1:0.4565
>> valid relation prec:0.0088, rec:0.0019, f1:0.0031
>> valid relation with NER prec:0.0088, rec:0.0019, f1:0.0031
g_step 5100, step 204, avg_time 2.416, loss:744.1068
g_step 5200, step 304, avg_time 0.916, loss:753.9416
g_step 5300, step 404, avg_time 0.929, loss:777.9330
g_step 5400, step 96, avg_time 0.930, loss:740.4328
g_step 5500, step 196, avg_time 0.924, loss:742.3170
>> valid entity prec:0.5040, rec:0.3675, f1:0.4250
>> valid relation prec:0.0067, rec:0.0010, f1:0.0018
>> valid relation with NER prec:0.0067, rec:0.0010, f1:0.0018
g_step 5600, step 296, avg_time 2.423, loss:741.5232
g_step 5700, step 396, avg_time 0.922, loss:742.3589
g_step 5800, step 88, avg_time 0.918, loss:731.7385
g_step 5900, step 188, avg_time 0.931, loss:708.2721
g_step 6000, step 288, avg_time 0.936, loss:740.1974
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4719, rec:0.4621, f1:0.4670
>> valid relation prec:0.0243, rec:0.0068, f1:0.0106
>> valid relation with NER prec:0.0243, rec:0.0068, f1:0.0106
g_step 6100, step 388, avg_time 2.431, loss:725.6364
g_step 6200, step 80, avg_time 0.918, loss:687.2113
g_step 6300, step 180, avg_time 0.915, loss:680.0349
g_step 6400, step 280, avg_time 0.938, loss:701.7447
g_step 6500, step 380, avg_time 0.923, loss:728.6233
>> valid entity prec:0.4897, rec:0.3649, f1:0.4182
>> valid relation prec:0.0429, rec:0.0078, f1:0.0132
>> valid relation with NER prec:0.0429, rec:0.0078, f1:0.0132
g_step 6600, step 72, avg_time 2.423, loss:658.2376
g_step 6700, step 172, avg_time 0.919, loss:701.2880
g_step 6800, step 272, avg_time 0.928, loss:679.3511
g_step 6900, step 372, avg_time 0.917, loss:660.8684
g_step 7000, step 64, avg_time 0.942, loss:658.1112
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5012, rec:0.3798, f1:0.4321
>> valid relation prec:0.0179, rec:0.0035, f1:0.0059
>> valid relation with NER prec:0.0179, rec:0.0035, f1:0.0059
g_step 7100, step 164, avg_time 2.438, loss:637.4122
g_step 7200, step 264, avg_time 0.925, loss:671.8248
g_step 7300, step 364, avg_time 0.923, loss:652.0776
g_step 7400, step 56, avg_time 0.932, loss:622.9681
g_step 7500, step 156, avg_time 0.921, loss:617.1997
>> valid entity prec:0.4626, rec:0.4286, f1:0.4449
>> valid relation prec:0.0482, rec:0.0105, f1:0.0173
>> valid relation with NER prec:0.0482, rec:0.0105, f1:0.0173
g_step 7600, step 256, avg_time 2.431, loss:616.3095
g_step 7700, step 356, avg_time 0.930, loss:667.5247
g_step 7800, step 48, avg_time 0.930, loss:641.0858
g_step 7900, step 148, avg_time 0.936, loss:633.8910
g_step 8000, step 248, avg_time 0.913, loss:602.8200
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4555, rec:0.3364, f1:0.3870
>> valid relation prec:0.0205, rec:0.0037, f1:0.0063
>> valid relation with NER prec:0.0205, rec:0.0037, f1:0.0063
g_step 8100, step 348, avg_time 2.435, loss:616.3987
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:51:36 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:51:36 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-51-35_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:51:39 - WARNING - datasets.builder -   Using custom data configuration default-fad84666fea8ec0f
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-fad84666fea8ec0f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:51:46,046 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:51:46,081 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:51:46,081 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:51:46,082 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:51:46,186 >> Didn't find file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:46,312 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:46,312 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:46,312 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:46,312 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:46,313 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:51:46,313 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:51:47,445 >> loading weights file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:51:51,345 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:51:51,408 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_15_seed_3/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-fad84666fea8ec0f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 20:51:51 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x147041345ef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:06,  1.48ba/s] 18%|█▊        | 2/11 [00:00<00:03,  2.49ba/s] 27%|██▋       | 3/11 [00:01<00:02,  3.18ba/s] 36%|███▋      | 4/11 [00:01<00:01,  3.65ba/s] 45%|████▌     | 5/11 [00:01<00:01,  3.96ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.17ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.33ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.44ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.51ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.56ba/s]100%|██████████| 11/11 [00:02<00:00,  4.23ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.57ba/s] 40%|████      | 2/5 [00:00<00:01,  2.56ba/s] 60%|██████    | 3/5 [00:01<00:00,  2.65ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.19ba/s]100%|██████████| 5/5 [00:01<00:00,  3.72ba/s]100%|██████████| 5/5 [00:01<00:00,  3.08ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:04,  2.17ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.11ba/s] 36%|███▋      | 4/11 [00:00<00:01,  5.66ba/s] 55%|█████▍    | 6/11 [00:01<00:00,  7.40ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  8.60ba/s] 91%|█████████ | 10/11 [00:01<00:00,  9.40ba/s]100%|██████████| 11/11 [00:01<00:00,  7.84ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:03,  1.32ba/s] 40%|████      | 2/5 [00:00<00:01,  2.24ba/s] 60%|██████    | 3/5 [00:01<00:00,  3.43ba/s]100%|██████████| 5/5 [00:01<00:00,  5.78ba/s]100%|██████████| 5/5 [00:01<00:00,  3.98ba/s]
[INFO|trainer.py:414] 2023-08-28 20:52:04,217 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:52:04,611 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:52:04,611 >>   Num examples = 10018
[INFO|trainer.py:1149] 2023-08-28 20:52:04,611 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:52:04,611 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:52:04,611 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:52:04,611 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:52:04,611 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:02<37:41,  2.88s/it]  0%|          | 2/785 [00:04<26:28,  2.03s/it]  0%|          | 3/785 [00:05<22:12,  1.70s/it]  1%|          | 4/785 [00:06<15:32,  1.19s/it]  1%|          | 5/785 [00:06<14:03,  1.08s/it]  1%|          | 6/785 [00:07<11:25,  1.14it/s]  1%|          | 7/785 [00:08<10:47,  1.20it/s]  1%|          | 8/785 [00:08<08:32,  1.51it/s]  1%|          | 9/785 [00:08<07:12,  1.79it/s]  1%|▏         | 10/785 [00:09<06:08,  2.10it/s]  1%|▏         | 11/785 [00:09<06:01,  2.14it/s]  2%|▏         | 12/785 [00:09<05:34,  2.31it/s]  2%|▏         | 13/785 [00:10<05:01,  2.56it/s]  2%|▏         | 14/785 [00:10<04:37,  2.78it/s]  2%|▏         | 15/785 [00:10<04:21,  2.95it/s]  2%|▏         | 16/785 [00:11<04:09,  3.08it/s]  2%|▏         | 17/785 [00:11<05:31,  2.32it/s]  2%|▏         | 18/785 [00:12<04:58,  2.57it/s]  2%|▏         | 19/785 [00:12<04:35,  2.78it/s]  3%|▎         | 20/785 [00:12<04:27,  2.86it/s]  3%|▎         | 21/785 [00:12<04:13,  3.01it/s]  3%|▎         | 22/785 [00:13<04:04,  3.12it/s]  3%|▎         | 23/785 [00:13<03:57,  3.21it/s]  3%|▎         | 24/785 [00:13<03:52,  3.27it/s]  3%|▎         | 25/785 [00:14<03:49,  3.32it/s]  3%|▎         | 26/785 [00:14<03:46,  3.35it/s]  3%|▎         | 27/785 [00:14<03:44,  3.37it/s]  4%|▎         | 28/785 [00:14<03:43,  3.39it/s]  4%|▎         | 29/785 [00:15<03:42,  3.40it/s]  4%|▍         | 30/785 [00:15<03:41,  3.41it/s]  4%|▍         | 31/785 [00:15<04:09,  3.02it/s]  4%|▍         | 32/785 [00:16<04:00,  3.13it/s]  4%|▍         | 33/785 [00:16<03:53,  3.22it/s]  4%|▍         | 34/785 [00:16<03:49,  3.28it/s]  4%|▍         | 35/785 [00:17<03:45,  3.32it/s]  5%|▍         | 36/785 [00:17<03:43,  3.35it/s]  5%|▍         | 37/785 [00:17<03:41,  3.37it/s]  5%|▍         | 38/785 [00:17<03:40,  3.39it/s]  5%|▍         | 39/785 [00:18<03:39,  3.40it/s]  5%|▌         | 40/785 [00:18<03:38,  3.41it/s]  5%|▌         | 41/785 [00:18<03:52,  3.19it/s]  5%|▌         | 42/785 [00:19<03:47,  3.26it/s]  5%|▌         | 43/785 [00:19<03:44,  3.31it/s]  6%|▌         | 44/785 [00:19<03:41,  3.35it/s]  6%|▌         | 45/785 [00:20<03:39,  3.37it/s]  6%|▌         | 46/785 [00:20<03:38,  3.38it/s]  6%|▌         | 47/785 [00:20<03:37,  3.40it/s]  6%|▌         | 48/785 [00:20<03:36,  3.40it/s]  6%|▌         | 49/785 [00:21<03:35,  3.41it/s]  6%|▋         | 50/785 [00:21<03:34,  3.42it/s]  6%|▋         | 51/785 [00:21<03:34,  3.42it/s]  7%|▋         | 52/785 [00:22<04:15,  2.86it/s]  7%|▋         | 53/785 [00:22<04:03,  3.01it/s]  7%|▋         | 54/785 [00:22<03:53,  3.12it/s]  7%|▋         | 55/785 [00:23<03:47,  3.21it/s]  7%|▋         | 56/785 [00:23<03:42,  3.28it/s]  7%|▋         | 57/785 [00:23<03:39,  3.32it/s]  7%|▋         | 58/785 [00:24<03:37,  3.35it/s]  8%|▊         | 59/785 [00:24<03:35,  3.37it/s]  8%|▊         | 60/785 [00:24<03:34,  3.38it/s]  8%|▊         | 61/785 [00:24<03:33,  3.40it/s]  8%|▊         | 62/785 [00:25<03:56,  3.06it/s]  8%|▊         | 63/785 [00:25<03:48,  3.16it/s]  8%|▊         | 64/785 [00:26<04:25,  2.72it/s]  8%|▊         | 65/785 [00:26<04:08,  2.90it/s]  8%|▊         | 66/785 [00:26<03:56,  3.04it/s]  9%|▊         | 67/785 [00:27<03:48,  3.15it/s]  9%|▊         | 68/785 [00:27<03:42,  3.22it/s]  9%|▉         | 69/785 [00:27<03:38,  3.28it/s]  9%|▉         | 70/785 [00:27<03:35,  3.32it/s]  9%|▉         | 71/785 [00:28<03:32,  3.35it/s]  9%|▉         | 72/785 [00:28<04:15,  2.79it/s]  9%|▉         | 73/785 [00:28<04:01,  2.95it/s]  9%|▉         | 74/785 [00:29<03:50,  3.08it/s] 10%|▉         | 75/785 [00:29<03:43,  3.18it/s] 10%|▉         | 76/785 [00:29<03:38,  3.25it/s] 10%|▉         | 77/785 [00:30<03:34,  3.30it/s] 10%|▉         | 78/785 [00:30<03:31,  3.34it/s] 10%|█         | 79/785 [00:30<03:29,  3.36it/s] 10%|█         | 80/785 [00:31<03:28,  3.38it/s] 10%|█         | 81/785 [00:33<11:23,  1.03it/s] 10%|█         | 82/785 [00:34<09:30,  1.23it/s] 11%|█         | 83/785 [00:34<07:39,  1.53it/s] 11%|█         | 84/785 [00:34<06:23,  1.83it/s] 11%|█         | 85/785 [00:34<05:29,  2.13it/s] 11%|█         | 86/785 [00:35<04:51,  2.40it/s] 11%|█         | 87/785 [00:35<04:24,  2.64it/s] 11%|█         | 88/785 [00:35<04:05,  2.83it/s] 11%|█▏        | 89/785 [00:36<03:52,  2.99it/s] 11%|█▏        | 90/785 [00:36<03:43,  3.11it/s] 12%|█▏        | 91/785 [00:36<03:36,  3.20it/s] 12%|█▏        | 92/785 [00:37<05:54,  1.95it/s] 12%|█▏        | 93/785 [00:37<05:08,  2.24it/s] 12%|█▏        | 94/785 [00:38<04:36,  2.50it/s] 12%|█▏        | 95/785 [00:38<04:13,  2.72it/s] 12%|█▏        | 96/785 [00:38<03:57,  2.90it/s] 12%|█▏        | 97/785 [00:39<03:46,  3.03it/s] 12%|█▏        | 98/785 [00:39<03:38,  3.14it/s] 13%|█▎        | 99/785 [00:39<03:33,  3.22it/s] 13%|█▎        | 100/785 [00:40<03:44,  3.05it/s] 13%|█▎        | 101/785 [00:40<03:36,  3.15it/s] 13%|█▎        | 102/785 [00:40<03:31,  3.23it/s] 13%|█▎        | 103/785 [00:40<03:27,  3.29it/s] 13%|█▎        | 104/785 [00:41<03:24,  3.33it/s] 13%|█▎        | 105/785 [00:41<03:22,  3.35it/s] 14%|█▎        | 106/785 [00:41<03:21,  3.38it/s] 14%|█▎        | 107/785 [00:42<03:19,  3.39it/s] 14%|█▍        | 108/785 [00:42<03:19,  3.40it/s] 14%|█▍        | 109/785 [00:42<03:18,  3.41it/s] 14%|█▍        | 110/785 [00:42<03:17,  3.41it/s] 14%|█▍        | 111/785 [00:43<04:27,  2.52it/s] 14%|█▍        | 112/785 [00:43<04:05,  2.74it/s] 14%|█▍        | 113/785 [00:44<03:50,  2.91it/s] 15%|█▍        | 114/785 [00:44<03:39,  3.05it/s] 15%|█▍        | 115/785 [00:44<03:32,  3.15it/s] 15%|█▍        | 116/785 [00:45<03:27,  3.23it/s] 15%|█▍        | 117/785 [00:45<03:23,  3.29it/s] 15%|█▌        | 118/785 [00:45<03:20,  3.33it/s] 15%|█▌        | 119/785 [00:45<03:18,  3.36it/s] 15%|█▌        | 120/785 [00:46<03:16,  3.38it/s] 15%|█▌        | 121/785 [00:46<03:34,  3.09it/s] 16%|█▌        | 122/785 [00:46<03:28,  3.18it/s] 16%|█▌        | 123/785 [00:47<03:23,  3.25it/s] 16%|█▌        | 124/785 [00:47<03:20,  3.30it/s] 16%|█▌        | 125/785 [00:47<03:17,  3.34it/s] 16%|█▌        | 126/785 [00:48<03:15,  3.36it/s] 16%|█▌        | 127/785 [00:48<03:14,  3.38it/s] 16%|█▋        | 128/785 [00:48<03:13,  3.39it/s] 16%|█▋        | 129/785 [00:48<03:12,  3.41it/s] 17%|█▋        | 130/785 [00:49<03:12,  3.41it/s] 17%|█▋        | 131/785 [00:49<03:40,  2.96it/s] 17%|█▋        | 132/785 [00:49<03:31,  3.09it/s] 17%|█▋        | 133/785 [00:50<03:24,  3.18it/s] 17%|█▋        | 134/785 [00:50<03:20,  3.25it/s] 17%|█▋        | 135/785 [00:50<03:16,  3.30it/s] 17%|█▋        | 136/785 [00:51<03:14,  3.33it/s] 17%|█▋        | 137/785 [00:51<03:12,  3.36it/s] 18%|█▊        | 138/785 [00:51<03:11,  3.38it/s] 18%|█▊        | 139/785 [00:52<03:10,  3.39it/s] 18%|█▊        | 140/785 [00:52<03:09,  3.40it/s] 18%|█▊        | 141/785 [00:52<03:35,  2.98it/s] 18%|█▊        | 142/785 [00:53<03:27,  3.10it/s] 18%|█▊        | 143/785 [00:53<03:21,  3.19it/s] 18%|█▊        | 144/785 [00:53<03:16,  3.26it/s] 18%|█▊        | 145/785 [00:53<03:13,  3.30it/s] 19%|█▊        | 146/785 [00:54<03:11,  3.34it/s] 19%|█▊        | 147/785 [00:54<03:09,  3.36it/s] 19%|█▉        | 148/785 [00:54<03:08,  3.38it/s] 19%|█▉        | 149/785 [00:55<03:07,  3.39it/s] 19%|█▉        | 150/785 [00:55<03:06,  3.40it/s] 19%|█▉        | 151/785 [00:55<03:47,  2.78it/s] 19%|█▉        | 152/785 [00:56<03:34,  2.94it/s] 19%|█▉        | 153/785 [00:56<03:25,  3.07it/s] 20%|█▉        | 154/785 [00:56<03:18,  3.17it/s] 20%|█▉        | 155/785 [00:57<03:14,  3.24it/s] 20%|█▉        | 156/785 [00:57<03:10,  3.30it/s] 20%|██        | 157/785 [00:57<03:33,  2.94it/s][INFO|trainer.py:2140] 2023-08-28 20:53:02,380 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:53:02,380 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 20:53:02,380 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.15it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.73it/s][A
  3%|▎         | 18/608 [00:00<00:12, 48.09it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.16it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.83it/s][A
  5%|▌         | 33/608 [00:00<00:15, 36.97it/s][A
  6%|▋         | 38/608 [00:00<00:14, 39.44it/s][A
  7%|▋         | 43/608 [00:00<00:13, 41.10it/s][A
  8%|▊         | 48/608 [00:01<00:13, 42.53it/s][A
  9%|▊         | 53/608 [00:01<00:31, 17.80it/s][A
 10%|▉         | 58/608 [00:01<00:25, 21.71it/s][A
 10%|█         | 63/608 [00:01<00:21, 25.86it/s][A
 11%|█         | 68/608 [00:02<00:18, 29.76it/s][A
 12%|█▏        | 73/608 [00:02<00:16, 33.28it/s][A
 13%|█▎        | 78/608 [00:02<00:14, 36.27it/s][A
 14%|█▎        | 83/608 [00:02<00:13, 38.68it/s][A
 14%|█▍        | 88/608 [00:02<00:12, 40.59it/s][A
 15%|█▌        | 93/608 [00:02<00:12, 41.68it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 42.62it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 43.33it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 43.95it/s][A
 19%|█▊        | 113/608 [00:03<00:11, 44.54it/s][A
 19%|█▉        | 118/608 [00:03<00:10, 44.89it/s][A
 20%|██        | 123/608 [00:03<00:10, 45.12it/s][A
 21%|██        | 128/608 [00:03<00:10, 45.28it/s][A
 22%|██▏       | 133/608 [00:03<00:10, 45.24it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.17it/s][A
 24%|██▎       | 143/608 [00:03<00:14, 31.58it/s][A
 24%|██▍       | 148/608 [00:04<00:13, 34.87it/s][A
 25%|██▌       | 153/608 [00:04<00:12, 37.54it/s][A
 26%|██▌       | 158/608 [00:04<00:11, 39.68it/s][A
 27%|██▋       | 163/608 [00:04<00:10, 41.36it/s][A
 28%|██▊       | 168/608 [00:04<00:10, 42.62it/s][A
 28%|██▊       | 173/608 [00:04<00:10, 43.45it/s][A
 29%|██▉       | 178/608 [00:04<00:11, 38.47it/s][A
 30%|███       | 183/608 [00:05<00:16, 25.44it/s][A
 31%|███       | 188/608 [00:05<00:14, 29.47it/s][A
 32%|███▏      | 193/608 [00:05<00:12, 33.01it/s][A
 33%|███▎      | 198/608 [00:05<00:11, 36.00it/s][A
 33%|███▎      | 203/608 [00:05<00:10, 38.47it/s][A
 34%|███▍      | 208/608 [00:05<00:09, 40.38it/s][A
 35%|███▌      | 213/608 [00:05<00:09, 41.75it/s][A
 36%|███▌      | 218/608 [00:05<00:09, 42.88it/s][A
 37%|███▋      | 223/608 [00:05<00:08, 43.52it/s][A
 38%|███▊      | 228/608 [00:06<00:08, 43.93it/s][A
 38%|███▊      | 233/608 [00:06<00:08, 44.45it/s][A
 39%|███▉      | 238/608 [00:06<00:08, 44.76it/s][A
 40%|███▉      | 243/608 [00:06<00:08, 45.05it/s][A
 41%|████      | 248/608 [00:06<00:07, 45.18it/s][A
 42%|████▏     | 253/608 [00:06<00:07, 45.26it/s][A
 42%|████▏     | 258/608 [00:06<00:07, 45.25it/s][A
 43%|████▎     | 263/608 [00:06<00:10, 33.46it/s][A
 44%|████▍     | 268/608 [00:07<00:09, 36.42it/s][A
 45%|████▍     | 273/608 [00:07<00:08, 38.80it/s][A
 46%|████▌     | 278/608 [00:07<00:08, 40.66it/s][A
 47%|████▋     | 283/608 [00:07<00:07, 41.97it/s][A
 47%|████▋     | 288/608 [00:07<00:07, 43.11it/s][A
 48%|████▊     | 293/608 [00:07<00:07, 43.83it/s][A
 49%|████▉     | 298/608 [00:07<00:07, 40.98it/s][A
 50%|████▉     | 303/608 [00:07<00:07, 42.77it/s][A
 51%|█████     | 308/608 [00:07<00:06, 43.68it/s][A
 51%|█████▏    | 313/608 [00:08<00:06, 44.26it/s][A
 52%|█████▏    | 318/608 [00:08<00:06, 44.77it/s][A
 53%|█████▎    | 323/608 [00:08<00:06, 44.95it/s][A
 54%|█████▍    | 328/608 [00:08<00:06, 45.27it/s][A
 55%|█████▍    | 333/608 [00:08<00:06, 45.28it/s][A
 56%|█████▌    | 338/608 [00:08<00:05, 45.44it/s][A
 56%|█████▋    | 343/608 [00:08<00:05, 45.03it/s][A
 57%|█████▋    | 348/608 [00:08<00:05, 45.15it/s][A
 58%|█████▊    | 353/608 [00:08<00:05, 45.26it/s][A
 59%|█████▉    | 358/608 [00:09<00:05, 45.40it/s][A
 60%|█████▉    | 363/608 [00:09<00:05, 45.41it/s][A
 61%|██████    | 368/608 [00:09<00:05, 45.54it/s][A
 61%|██████▏   | 373/608 [00:09<00:05, 45.52it/s][A
 62%|██████▏   | 378/608 [00:09<00:05, 45.48it/s][A
 63%|██████▎   | 383/608 [00:09<00:04, 45.35it/s][A
 64%|██████▍   | 388/608 [00:09<00:04, 45.20it/s][A
 65%|██████▍   | 393/608 [00:09<00:04, 43.34it/s][A
 65%|██████▌   | 398/608 [00:09<00:04, 44.15it/s][A
 66%|██████▋   | 403/608 [00:10<00:04, 44.58it/s][A
 67%|██████▋   | 408/608 [00:10<00:05, 39.85it/s][A
 68%|██████▊   | 413/608 [00:10<00:04, 41.43it/s][A
 69%|██████▉   | 418/608 [00:10<00:04, 42.70it/s][A
 70%|██████▉   | 423/608 [00:10<00:04, 43.60it/s][A
 70%|███████   | 428/608 [00:10<00:07, 24.76it/s][A
 71%|███████   | 432/608 [00:11<00:07, 22.99it/s][A
 72%|███████▏  | 437/608 [00:11<00:06, 27.53it/s][A
 73%|███████▎  | 442/608 [00:11<00:05, 31.37it/s][A
 74%|███████▎  | 447/608 [00:11<00:04, 34.68it/s][A
 74%|███████▍  | 452/608 [00:11<00:04, 37.52it/s][A
 75%|███████▌  | 457/608 [00:11<00:03, 39.69it/s][A
 76%|███████▌  | 462/608 [00:11<00:03, 41.31it/s][A
 77%|███████▋  | 467/608 [00:11<00:03, 42.51it/s][A
 78%|███████▊  | 472/608 [00:12<00:03, 43.38it/s][A
 78%|███████▊  | 477/608 [00:12<00:03, 43.55it/s][A
 79%|███████▉  | 482/608 [00:12<00:02, 44.09it/s][A
 80%|████████  | 487/608 [00:12<00:02, 44.55it/s][A
 81%|████████  | 492/608 [00:12<00:02, 44.99it/s][A
 82%|████████▏ | 497/608 [00:12<00:02, 45.17it/s][A
 83%|████████▎ | 502/608 [00:12<00:02, 45.28it/s][A
 83%|████████▎ | 507/608 [00:12<00:02, 45.34it/s][A
 84%|████████▍ | 512/608 [00:12<00:02, 45.44it/s][A
 85%|████████▌ | 517/608 [00:13<00:02, 45.20it/s][A
 86%|████████▌ | 522/608 [00:13<00:03, 26.53it/s][A
 87%|████████▋ | 527/608 [00:13<00:02, 30.34it/s][A
 88%|████████▊ | 532/608 [00:13<00:02, 33.78it/s][A
 88%|████████▊ | 537/608 [00:13<00:01, 36.63it/s][A
 89%|████████▉ | 542/608 [00:13<00:01, 38.96it/s][A
 90%|████████▉ | 547/608 [00:13<00:01, 40.78it/s][A
 91%|█████████ | 552/608 [00:14<00:01, 42.23it/s][A
 92%|█████████▏| 557/608 [00:14<00:01, 43.13it/s][A
 92%|█████████▏| 562/608 [00:14<00:01, 40.98it/s][A
 93%|█████████▎| 567/608 [00:14<00:00, 42.43it/s][A
 94%|█████████▍| 572/608 [00:14<00:00, 43.45it/s][A
 95%|█████████▍| 577/608 [00:14<00:00, 44.05it/s][A
 96%|█████████▌| 582/608 [00:14<00:00, 44.43it/s][A
 97%|█████████▋| 587/608 [00:14<00:00, 44.72it/s][A
 97%|█████████▋| 592/608 [00:14<00:00, 45.01it/s][A
 98%|█████████▊| 597/608 [00:15<00:00, 45.06it/s][A
 99%|█████████▉| 602/608 [00:15<00:00, 44.91it/s][A
100%|█████████▉| 607/608 [00:15<00:00, 44.85it/s][A                                                 
                                                 [A 20%|██        | 157/785 [01:13<03:33,  2.94it/s]
100%|██████████| 608/608 [00:15<00:00, 44.85it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:53:18,729 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-28 20:53:19,886 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:53:29,291 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:53:29,887 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:53:30,907 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:57<3:08:13, 18.01s/it] 20%|██        | 159/785 [01:57<2:12:36, 12.71s/it] 20%|██        | 160/785 [01:57<1:33:34,  8.98s/it] 21%|██        | 161/785 [01:57<1:06:18,  6.38s/it] 21%|██        | 162/785 [01:58<47:14,  4.55s/it]   21%|██        | 163/785 [01:58<33:55,  3.27s/it] 21%|██        | 164/785 [01:58<24:36,  2.38s/it] 21%|██        | 165/785 [01:59<18:06,  1.75s/it] 21%|██        | 166/785 [01:59<13:33,  1.31s/it] 21%|██▏       | 167/785 [01:59<10:22,  1.01s/it] 21%|██▏       | 168/785 [01:59<08:08,  1.26it/s] 22%|██▏       | 169/785 [02:00<06:35,  1.56it/s] 22%|██▏       | 170/785 [02:00<05:36,  1.83it/s] 22%|██▏       | 171/785 [02:00<04:48,  2.13it/s] 22%|██▏       | 172/785 [02:01<04:15,  2.40it/s] 22%|██▏       | 173/785 [02:01<03:51,  2.64it/s] 22%|██▏       | 174/785 [02:01<03:35,  2.84it/s] 22%|██▏       | 175/785 [02:02<03:23,  3.00it/s] 22%|██▏       | 176/785 [02:02<03:15,  3.12it/s] 23%|██▎       | 177/785 [02:02<03:09,  3.21it/s] 23%|██▎       | 178/785 [02:02<03:05,  3.27it/s] 23%|██▎       | 179/785 [02:03<03:02,  3.32it/s] 23%|██▎       | 180/785 [02:03<03:00,  3.35it/s] 23%|██▎       | 181/785 [02:03<03:14,  3.10it/s] 23%|██▎       | 182/785 [02:04<03:08,  3.20it/s] 23%|██▎       | 183/785 [02:04<03:04,  3.27it/s] 23%|██▎       | 184/785 [02:04<03:01,  3.31it/s] 24%|██▎       | 185/785 [02:05<02:59,  3.35it/s] 24%|██▎       | 186/785 [02:05<02:57,  3.37it/s] 24%|██▍       | 187/785 [02:05<02:56,  3.39it/s] 24%|██▍       | 188/785 [02:05<02:55,  3.40it/s] 24%|██▍       | 189/785 [02:06<02:54,  3.41it/s] 24%|██▍       | 190/785 [02:06<02:54,  3.42it/s] 24%|██▍       | 191/785 [02:06<03:24,  2.91it/s] 24%|██▍       | 192/785 [02:07<03:14,  3.05it/s] 25%|██▍       | 193/785 [02:07<03:07,  3.15it/s] 25%|██▍       | 194/785 [02:07<03:02,  3.23it/s] 25%|██▍       | 195/785 [02:08<02:59,  3.29it/s] 25%|██▍       | 196/785 [02:08<02:56,  3.33it/s] 25%|██▌       | 197/785 [02:08<02:55,  3.36it/s] 25%|██▌       | 198/785 [02:08<02:53,  3.38it/s] 25%|██▌       | 199/785 [02:09<02:52,  3.40it/s] 25%|██▌       | 200/785 [02:09<02:56,  3.31it/s] 26%|██▌       | 201/785 [02:09<02:54,  3.35it/s] 26%|██▌       | 202/785 [02:10<02:53,  3.37it/s] 26%|██▌       | 203/785 [02:10<02:51,  3.39it/s] 26%|██▌       | 204/785 [02:10<02:50,  3.40it/s] 26%|██▌       | 205/785 [02:11<02:50,  3.41it/s] 26%|██▌       | 206/785 [02:11<02:49,  3.42it/s] 26%|██▋       | 207/785 [02:11<02:49,  3.42it/s] 26%|██▋       | 208/785 [02:11<02:48,  3.42it/s] 27%|██▋       | 209/785 [02:12<02:48,  3.43it/s] 27%|██▋       | 210/785 [02:12<02:58,  3.21it/s] 27%|██▋       | 211/785 [02:12<02:55,  3.28it/s] 27%|██▋       | 212/785 [02:13<02:52,  3.32it/s] 27%|██▋       | 213/785 [02:13<02:50,  3.35it/s] 27%|██▋       | 214/785 [02:13<02:48,  3.38it/s] 27%|██▋       | 215/785 [02:14<02:47,  3.39it/s] 28%|██▊       | 216/785 [02:14<02:47,  3.41it/s] 28%|██▊       | 217/785 [02:14<02:46,  3.41it/s] 28%|██▊       | 218/785 [02:14<02:45,  3.42it/s] 28%|██▊       | 219/785 [02:15<02:45,  3.42it/s] 28%|██▊       | 220/785 [02:15<02:44,  3.42it/s] 28%|██▊       | 221/785 [02:15<02:54,  3.23it/s] 28%|██▊       | 222/785 [02:16<02:51,  3.28it/s] 28%|██▊       | 223/785 [02:16<02:49,  3.32it/s] 29%|██▊       | 224/785 [02:16<02:47,  3.35it/s] 29%|██▊       | 225/785 [02:17<02:45,  3.37it/s] 29%|██▉       | 226/785 [02:17<02:44,  3.39it/s] 29%|██▉       | 227/785 [02:17<02:44,  3.40it/s] 29%|██▉       | 228/785 [02:17<02:43,  3.41it/s] 29%|██▉       | 229/785 [02:18<02:42,  3.42it/s] 29%|██▉       | 230/785 [02:18<02:42,  3.42it/s] 29%|██▉       | 231/785 [02:18<02:41,  3.42it/s] 30%|██▉       | 232/785 [02:19<03:11,  2.89it/s] 30%|██▉       | 233/785 [02:19<03:01,  3.03it/s] 30%|██▉       | 234/785 [02:19<02:55,  3.14it/s] 30%|██▉       | 235/785 [02:20<02:50,  3.22it/s] 30%|███       | 236/785 [02:20<02:47,  3.28it/s] 30%|███       | 237/785 [02:20<02:45,  3.32it/s] 30%|███       | 238/785 [02:20<02:42,  3.36it/s] 30%|███       | 239/785 [02:21<02:41,  3.38it/s] 31%|███       | 240/785 [02:21<02:40,  3.39it/s] 31%|███       | 241/785 [02:21<02:39,  3.41it/s] 31%|███       | 242/785 [02:22<02:49,  3.21it/s] 31%|███       | 243/785 [02:22<02:45,  3.27it/s] 31%|███       | 244/785 [02:22<02:43,  3.32it/s] 31%|███       | 245/785 [02:23<02:41,  3.35it/s] 31%|███▏      | 246/785 [02:23<02:39,  3.37it/s] 31%|███▏      | 247/785 [02:23<02:38,  3.39it/s] 32%|███▏      | 248/785 [02:23<02:37,  3.40it/s] 32%|███▏      | 249/785 [02:24<02:37,  3.41it/s] 32%|███▏      | 250/785 [02:24<02:36,  3.41it/s] 32%|███▏      | 251/785 [02:24<02:36,  3.42it/s] 32%|███▏      | 252/785 [02:25<02:35,  3.42it/s] 32%|███▏      | 253/785 [02:25<02:57,  2.99it/s] 32%|███▏      | 254/785 [02:25<02:50,  3.11it/s] 32%|███▏      | 255/785 [02:26<02:45,  3.20it/s] 33%|███▎      | 256/785 [02:26<03:02,  2.90it/s] 33%|███▎      | 257/785 [02:26<02:53,  3.04it/s] 33%|███▎      | 258/785 [02:27<02:47,  3.14it/s] 33%|███▎      | 259/785 [02:27<02:43,  3.22it/s] 33%|███▎      | 260/785 [02:27<02:40,  3.28it/s] 33%|███▎      | 261/785 [02:28<02:37,  3.32it/s] 33%|███▎      | 262/785 [02:28<02:35,  3.35it/s] 34%|███▎      | 263/785 [02:28<02:52,  3.02it/s] 34%|███▎      | 264/785 [02:29<02:46,  3.13it/s] 34%|███▍      | 265/785 [02:29<02:41,  3.21it/s] 34%|███▍      | 266/785 [02:29<02:38,  3.28it/s] 34%|███▍      | 267/785 [02:29<02:36,  3.32it/s] 34%|███▍      | 268/785 [02:30<02:34,  3.34it/s] 34%|███▍      | 269/785 [02:30<02:33,  3.37it/s] 34%|███▍      | 270/785 [02:30<02:32,  3.38it/s] 35%|███▍      | 271/785 [02:31<02:31,  3.40it/s] 35%|███▍      | 272/785 [02:31<02:30,  3.40it/s] 35%|███▍      | 273/785 [02:31<02:55,  2.91it/s] 35%|███▍      | 274/785 [02:32<02:47,  3.05it/s] 35%|███▌      | 275/785 [02:32<02:41,  3.15it/s] 35%|███▌      | 276/785 [02:32<02:37,  3.23it/s] 35%|███▌      | 277/785 [02:32<02:34,  3.28it/s] 35%|███▌      | 278/785 [02:33<02:32,  3.32it/s] 36%|███▌      | 279/785 [02:33<02:31,  3.34it/s] 36%|███▌      | 280/785 [02:33<02:30,  3.36it/s] 36%|███▌      | 281/785 [02:34<02:28,  3.38it/s] 36%|███▌      | 282/785 [02:34<02:28,  3.39it/s] 36%|███▌      | 283/785 [02:35<03:15,  2.57it/s] 36%|███▌      | 284/785 [02:35<03:00,  2.77it/s] 36%|███▋      | 285/785 [02:35<02:49,  2.94it/s] 36%|███▋      | 286/785 [02:35<02:42,  3.08it/s] 37%|███▋      | 287/785 [02:36<02:37,  3.17it/s] 37%|███▋      | 288/785 [02:36<02:33,  3.24it/s] 37%|███▋      | 289/785 [02:36<02:30,  3.30it/s] 37%|███▋      | 290/785 [02:37<02:28,  3.33it/s] 37%|███▋      | 291/785 [02:37<02:27,  3.36it/s] 37%|███▋      | 292/785 [02:37<02:25,  3.38it/s] 37%|███▋      | 293/785 [02:38<02:29,  3.29it/s] 37%|███▋      | 294/785 [02:38<02:26,  3.34it/s] 38%|███▊      | 295/785 [02:38<02:24,  3.38it/s] 38%|███▊      | 296/785 [02:38<02:23,  3.41it/s] 38%|███▊      | 297/785 [02:39<02:22,  3.43it/s] 38%|███▊      | 298/785 [02:39<02:21,  3.44it/s] 38%|███▊      | 299/785 [02:39<02:20,  3.45it/s] 38%|███▊      | 300/785 [02:40<02:20,  3.46it/s] 38%|███▊      | 301/785 [02:40<02:48,  2.88it/s] 38%|███▊      | 302/785 [02:40<02:39,  3.03it/s] 39%|███▊      | 303/785 [02:41<02:32,  3.15it/s] 39%|███▊      | 304/785 [02:41<02:28,  3.24it/s] 39%|███▉      | 305/785 [02:41<02:25,  3.31it/s] 39%|███▉      | 306/785 [02:41<02:22,  3.36it/s] 39%|███▉      | 307/785 [02:42<02:20,  3.39it/s] 39%|███▉      | 308/785 [02:42<02:19,  3.42it/s] 39%|███▉      | 309/785 [02:42<02:18,  3.43it/s] 39%|███▉      | 310/785 [02:43<02:17,  3.45it/s] 40%|███▉      | 311/785 [02:43<02:26,  3.23it/s] 40%|███▉      | 312/785 [02:43<02:23,  3.30it/s] 40%|███▉      | 313/785 [02:44<02:20,  3.35it/s] 40%|████      | 314/785 [02:44<02:04,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 20:54:48,838 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:54:48,839 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 20:54:48,839 >>   Batch size = 8
{'eval_loss': 0.917130708694458, 'eval_runtime': 15.3458, 'eval_samples_per_second': 316.959, 'eval_steps_per_second': 39.62, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.10it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.80it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.70it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.16it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.62it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.31it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.67it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.50it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.47it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.60it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.50it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.61it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.67it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.63it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.55it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.45it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.30it/s][A
 15%|█▌        | 93/608 [00:02<00:12, 40.74it/s][A
 16%|█▌        | 98/608 [00:02<00:12, 42.16it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 43.24it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 43.99it/s][A
 19%|█▊        | 113/608 [00:02<00:11, 44.55it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 44.92it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.25it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.20it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 44.83it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.77it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 44.90it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.17it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.39it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.55it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.69it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.72it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.46it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.10it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.01it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.12it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.26it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.45it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.62it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.69it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.61it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.45it/s][A
 37%|███▋      | 223/608 [00:05<00:08, 45.28it/s][A
 38%|███▊      | 228/608 [00:05<00:10, 36.79it/s][A
 38%|███▊      | 233/608 [00:05<00:09, 39.11it/s][A
 39%|███▉      | 238/608 [00:05<00:09, 40.92it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 42.30it/s][A
 41%|████      | 248/608 [00:05<00:08, 43.36it/s][A
 42%|████▏     | 253/608 [00:05<00:08, 44.14it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 44.45it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 44.73it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 44.59it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 44.56it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 44.69it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 44.96it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.30it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.55it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.51it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.57it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.56it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.23it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.05it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.05it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.20it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.33it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.40it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.52it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.53it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.63it/s][A
 59%|█████▉    | 358/608 [00:08<00:05, 45.52it/s][A
 60%|█████▉    | 363/608 [00:08<00:11, 21.46it/s][A
 61%|██████    | 368/608 [00:08<00:09, 25.49it/s][A
 61%|██████▏   | 373/608 [00:08<00:07, 29.44it/s][A
 62%|██████▏   | 378/608 [00:08<00:06, 32.97it/s][A
 63%|██████▎   | 383/608 [00:08<00:06, 35.75it/s][A
 64%|██████▍   | 388/608 [00:09<00:05, 38.52it/s][A
 65%|██████▍   | 393/608 [00:09<00:05, 40.49it/s][A
 65%|██████▌   | 398/608 [00:09<00:05, 41.93it/s][A
 66%|██████▋   | 403/608 [00:09<00:04, 42.59it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 43.15it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 43.94it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 44.44it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 44.83it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.06it/s][A
 71%|███████   | 433/608 [00:10<00:03, 45.28it/s][A
 72%|███████▏  | 438/608 [00:10<00:03, 45.42it/s][A
 73%|███████▎  | 443/608 [00:10<00:03, 45.29it/s][A
 74%|███████▎  | 448/608 [00:10<00:03, 45.08it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.08it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.16it/s][A
 76%|███████▌  | 463/608 [00:11<00:10, 13.80it/s][A
 77%|███████▋  | 467/608 [00:11<00:10, 13.61it/s][A
 78%|███████▊  | 472/608 [00:11<00:07, 17.47it/s][A
 78%|███████▊  | 477/608 [00:12<00:06, 21.61it/s][A
 79%|███████▉  | 482/608 [00:12<00:04, 25.74it/s][A
 80%|████████  | 487/608 [00:12<00:04, 29.68it/s][A
 81%|████████  | 492/608 [00:12<00:03, 33.23it/s][A
 82%|████████▏ | 497/608 [00:12<00:03, 36.24it/s][A
 83%|████████▎ | 502/608 [00:12<00:02, 38.61it/s][A
 83%|████████▎ | 507/608 [00:12<00:02, 40.23it/s][A
 84%|████████▍ | 512/608 [00:12<00:02, 41.63it/s][A
 85%|████████▌ | 517/608 [00:12<00:02, 42.80it/s][A
 86%|████████▌ | 522/608 [00:13<00:01, 43.59it/s][A
 87%|████████▋ | 527/608 [00:13<00:01, 44.16it/s][A
 88%|████████▊ | 532/608 [00:13<00:01, 44.53it/s][A
 88%|████████▊ | 537/608 [00:13<00:01, 44.84it/s][A
 89%|████████▉ | 542/608 [00:13<00:01, 45.13it/s][A
 90%|████████▉ | 547/608 [00:13<00:01, 45.19it/s][A
 91%|█████████ | 552/608 [00:13<00:01, 45.04it/s][A
 92%|█████████▏| 557/608 [00:13<00:01, 45.07it/s][A
 92%|█████████▏| 562/608 [00:13<00:01, 45.23it/s][A
 93%|█████████▎| 567/608 [00:14<00:00, 45.32it/s][A
 94%|█████████▍| 572/608 [00:14<00:00, 45.42it/s][A
 95%|█████████▍| 577/608 [00:14<00:00, 45.51it/s][A
 96%|█████████▌| 582/608 [00:14<00:00, 45.53it/s][A
 97%|█████████▋| 587/608 [00:14<00:00, 45.55it/s][A
 97%|█████████▋| 592/608 [00:14<00:00, 45.42it/s][A
 98%|█████████▊| 597/608 [00:14<00:00, 31.98it/s][A
 99%|█████████▉| 602/608 [00:14<00:00, 35.20it/s][A
100%|█████████▉| 607/608 [00:15<00:00, 37.89it/s][A                                                 
                                                 [A 40%|████      | 314/785 [02:59<02:04,  3.78it/s]
100%|██████████| 608/608 [00:15<00:00, 37.89it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:55:05,113 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 20:55:05,622 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:55:13,670 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:55:14,044 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:55:14,204 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [03:26<1:41:12, 12.92s/it] 40%|████      | 316/785 [03:27<1:11:28,  9.14s/it] 40%|████      | 317/785 [03:27<50:36,  6.49s/it]   41%|████      | 318/785 [03:27<36:01,  4.63s/it] 41%|████      | 319/785 [03:27<25:51,  3.33s/it] 41%|████      | 320/785 [03:28<18:44,  2.42s/it] 41%|████      | 321/785 [03:28<13:45,  1.78s/it] 41%|████      | 322/785 [03:28<10:17,  1.33s/it] 41%|████      | 323/785 [03:29<07:51,  1.02s/it] 41%|████▏     | 324/785 [03:29<06:09,  1.25it/s] 41%|████▏     | 325/785 [03:29<04:58,  1.54it/s] 42%|████▏     | 326/785 [03:29<04:08,  1.84it/s] 42%|████▏     | 327/785 [03:30<03:53,  1.96it/s] 42%|████▏     | 328/785 [03:30<03:23,  2.25it/s] 42%|████▏     | 329/785 [03:30<03:01,  2.51it/s] 42%|████▏     | 330/785 [03:31<02:46,  2.73it/s] 42%|████▏     | 331/785 [03:31<02:36,  2.91it/s] 42%|████▏     | 332/785 [03:31<02:28,  3.04it/s] 42%|████▏     | 333/785 [03:32<02:23,  3.15it/s] 43%|████▎     | 334/785 [03:32<02:19,  3.23it/s] 43%|████▎     | 335/785 [03:32<02:17,  3.28it/s] 43%|████▎     | 336/785 [03:32<02:15,  3.32it/s] 43%|████▎     | 337/785 [03:33<02:28,  3.01it/s] 43%|████▎     | 338/785 [03:33<02:22,  3.13it/s] 43%|████▎     | 339/785 [03:33<02:18,  3.21it/s] 43%|████▎     | 340/785 [03:34<02:16,  3.27it/s] 43%|████▎     | 341/785 [03:34<02:13,  3.32it/s] 44%|████▎     | 342/785 [03:34<02:12,  3.35it/s] 44%|████▎     | 343/785 [03:35<02:11,  3.37it/s] 44%|████▍     | 344/785 [03:35<02:10,  3.38it/s] 44%|████▍     | 345/785 [03:35<02:09,  3.39it/s] 44%|████▍     | 346/785 [03:36<02:08,  3.40it/s] 44%|████▍     | 347/785 [03:36<02:22,  3.07it/s] 44%|████▍     | 348/785 [03:36<02:18,  3.17it/s] 44%|████▍     | 349/785 [03:37<02:14,  3.24it/s] 45%|████▍     | 350/785 [03:37<02:12,  3.29it/s] 45%|████▍     | 351/785 [03:37<02:10,  3.33it/s] 45%|████▍     | 352/785 [03:37<02:08,  3.36it/s] 45%|████▍     | 353/785 [03:38<02:08,  3.37it/s] 45%|████▌     | 354/785 [03:38<02:07,  3.39it/s] 45%|████▌     | 355/785 [03:38<02:06,  3.40it/s] 45%|████▌     | 356/785 [03:39<02:05,  3.41it/s] 45%|████▌     | 357/785 [03:39<02:33,  2.78it/s] 46%|████▌     | 358/785 [03:39<02:24,  2.95it/s] 46%|████▌     | 359/785 [03:40<02:18,  3.08it/s] 46%|████▌     | 360/785 [03:40<02:13,  3.17it/s] 46%|████▌     | 361/785 [03:40<02:10,  3.24it/s] 46%|████▌     | 362/785 [03:41<02:08,  3.29it/s] 46%|████▌     | 363/785 [03:41<02:06,  3.33it/s] 46%|████▋     | 364/785 [03:41<02:05,  3.35it/s] 46%|████▋     | 365/785 [03:41<02:04,  3.38it/s] 47%|████▋     | 366/785 [03:42<02:03,  3.39it/s] 47%|████▋     | 367/785 [03:42<02:02,  3.40it/s] 47%|████▋     | 368/785 [03:42<02:02,  3.41it/s] 47%|████▋     | 369/785 [03:43<02:02,  3.41it/s] 47%|████▋     | 370/785 [03:43<02:01,  3.41it/s] 47%|████▋     | 371/785 [03:43<02:01,  3.42it/s] 47%|████▋     | 372/785 [03:43<02:01,  3.41it/s] 48%|████▊     | 373/785 [03:44<02:00,  3.42it/s] 48%|████▊     | 374/785 [03:44<02:00,  3.42it/s] 48%|████▊     | 375/785 [03:44<02:06,  3.24it/s] 48%|████▊     | 376/785 [03:45<02:04,  3.29it/s] 48%|████▊     | 377/785 [03:45<02:02,  3.33it/s] 48%|████▊     | 378/785 [03:45<02:01,  3.36it/s] 48%|████▊     | 379/785 [03:46<02:00,  3.37it/s] 48%|████▊     | 380/785 [03:46<01:59,  3.39it/s] 49%|████▊     | 381/785 [03:46<01:58,  3.40it/s] 49%|████▊     | 382/785 [03:46<01:58,  3.40it/s] 49%|████▉     | 383/785 [03:47<01:58,  3.41it/s] 49%|████▉     | 384/785 [03:47<01:57,  3.41it/s] 49%|████▉     | 385/785 [03:47<01:57,  3.41it/s] 49%|████▉     | 386/785 [03:48<02:15,  2.94it/s] 49%|████▉     | 387/785 [03:48<02:09,  3.07it/s] 49%|████▉     | 388/785 [03:48<02:05,  3.17it/s] 50%|████▉     | 389/785 [03:49<02:02,  3.24it/s] 50%|████▉     | 390/785 [03:49<02:00,  3.29it/s] 50%|████▉     | 391/785 [03:49<01:58,  3.33it/s] 50%|████▉     | 392/785 [03:50<01:57,  3.35it/s] 50%|█████     | 393/785 [03:50<01:56,  3.37it/s] 50%|█████     | 394/785 [03:50<01:55,  3.39it/s] 50%|█████     | 395/785 [03:50<01:54,  3.39it/s] 50%|█████     | 396/785 [03:51<01:58,  3.29it/s] 51%|█████     | 397/785 [03:51<01:56,  3.33it/s] 51%|█████     | 398/785 [03:51<01:55,  3.36it/s] 51%|█████     | 399/785 [03:52<01:54,  3.37it/s] 51%|█████     | 400/785 [03:52<01:53,  3.38it/s] 51%|█████     | 401/785 [03:52<01:53,  3.40it/s] 51%|█████     | 402/785 [03:52<01:52,  3.40it/s] 51%|█████▏    | 403/785 [03:53<01:52,  3.40it/s] 51%|█████▏    | 404/785 [03:53<01:51,  3.40it/s] 52%|█████▏    | 405/785 [03:53<01:51,  3.41it/s] 52%|█████▏    | 406/785 [03:54<01:51,  3.41it/s] 52%|█████▏    | 407/785 [03:54<01:59,  3.18it/s] 52%|█████▏    | 408/785 [03:54<01:56,  3.24it/s] 52%|█████▏    | 409/785 [03:55<03:34,  1.76it/s] 52%|█████▏    | 410/785 [03:56<03:02,  2.06it/s] 52%|█████▏    | 411/785 [03:56<02:40,  2.33it/s] 52%|█████▏    | 412/785 [03:56<02:24,  2.58it/s] 53%|█████▎    | 413/785 [03:57<02:13,  2.78it/s] 53%|█████▎    | 414/785 [03:57<02:14,  2.76it/s] 53%|█████▎    | 415/785 [03:57<02:06,  2.92it/s] 53%|█████▎    | 416/785 [03:58<02:00,  3.07it/s] 53%|█████▎    | 417/785 [03:58<01:55,  3.18it/s] 53%|█████▎    | 418/785 [03:58<01:52,  3.26it/s] 53%|█████▎    | 419/785 [03:58<01:50,  3.32it/s] 54%|█████▎    | 420/785 [03:59<01:48,  3.37it/s] 54%|█████▎    | 421/785 [03:59<01:47,  3.40it/s] 54%|█████▍    | 422/785 [03:59<01:46,  3.42it/s] 54%|█████▍    | 423/785 [04:00<01:45,  3.43it/s] 54%|█████▍    | 424/785 [04:00<01:44,  3.44it/s] 54%|█████▍    | 425/785 [04:00<02:02,  2.94it/s] 54%|█████▍    | 426/785 [04:01<01:56,  3.09it/s] 54%|█████▍    | 427/785 [04:01<01:52,  3.19it/s] 55%|█████▍    | 428/785 [04:01<01:49,  3.27it/s] 55%|█████▍    | 429/785 [04:02<01:46,  3.33it/s] 55%|█████▍    | 430/785 [04:02<01:45,  3.37it/s] 55%|█████▍    | 431/785 [04:02<01:44,  3.40it/s] 55%|█████▌    | 432/785 [04:02<01:43,  3.42it/s] 55%|█████▌    | 433/785 [04:03<01:42,  3.44it/s] 55%|█████▌    | 434/785 [04:03<01:41,  3.45it/s] 55%|█████▌    | 435/785 [04:03<02:03,  2.83it/s] 56%|█████▌    | 436/785 [04:04<01:56,  3.00it/s] 56%|█████▌    | 437/785 [04:04<01:51,  3.13it/s] 56%|█████▌    | 438/785 [04:04<01:47,  3.22it/s] 56%|█████▌    | 439/785 [04:05<01:45,  3.30it/s] 56%|█████▌    | 440/785 [04:05<01:43,  3.34it/s] 56%|█████▌    | 441/785 [04:05<01:41,  3.38it/s] 56%|█████▋    | 442/785 [04:05<01:40,  3.41it/s] 56%|█████▋    | 443/785 [04:06<01:39,  3.42it/s] 57%|█████▋    | 444/785 [04:06<01:39,  3.44it/s] 57%|█████▋    | 445/785 [04:06<01:48,  3.12it/s] 57%|█████▋    | 446/785 [04:07<01:45,  3.22it/s] 57%|█████▋    | 447/785 [04:07<01:42,  3.29it/s] 57%|█████▋    | 448/785 [04:07<01:40,  3.34it/s] 57%|█████▋    | 449/785 [04:08<01:39,  3.38it/s] 57%|█████▋    | 450/785 [04:08<01:38,  3.41it/s] 57%|█████▋    | 451/785 [04:08<01:37,  3.43it/s] 58%|█████▊    | 452/785 [04:08<01:36,  3.44it/s] 58%|█████▊    | 453/785 [04:09<01:36,  3.45it/s] 58%|█████▊    | 454/785 [04:09<01:35,  3.46it/s] 58%|█████▊    | 455/785 [04:09<01:35,  3.46it/s] 58%|█████▊    | 456/785 [04:10<02:05,  2.62it/s] 58%|█████▊    | 457/785 [04:10<01:55,  2.83it/s] 58%|█████▊    | 458/785 [04:10<01:49,  3.00it/s] 58%|█████▊    | 459/785 [04:11<01:44,  3.12it/s] 59%|█████▊    | 460/785 [04:11<01:40,  3.22it/s] 59%|█████▊    | 461/785 [04:11<01:38,  3.29it/s] 59%|█████▉    | 462/785 [04:12<01:36,  3.34it/s] 59%|█████▉    | 463/785 [04:12<01:56,  2.76it/s] 59%|█████▉    | 464/785 [04:12<01:49,  2.94it/s] 59%|█████▉    | 465/785 [04:13<01:43,  3.08it/s] 59%|█████▉    | 466/785 [04:13<01:40,  3.19it/s] 59%|█████▉    | 467/785 [04:13<01:37,  3.27it/s] 60%|█████▉    | 468/785 [04:14<01:35,  3.33it/s] 60%|█████▉    | 469/785 [04:14<01:33,  3.37it/s] 60%|█████▉    | 470/785 [04:14<01:32,  3.40it/s] 60%|██████    | 471/785 [04:14<01:22,  3.83it/s][INFO|trainer.py:2140] 2023-08-28 20:56:19,480 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:56:19,480 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 20:56:19,480 >>   Batch size = 8
{'eval_loss': 0.9090890288352966, 'eval_runtime': 15.1164, 'eval_samples_per_second': 321.77, 'eval_steps_per_second': 40.221, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.24it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.83it/s][A
  3%|▎         | 18/608 [00:00<00:17, 33.97it/s][A
  4%|▍         | 23/608 [00:00<00:15, 37.36it/s][A
  5%|▍         | 28/608 [00:00<00:14, 39.82it/s][A
  5%|▌         | 33/608 [00:00<00:13, 41.61it/s][A
  6%|▋         | 38/608 [00:00<00:13, 42.80it/s][A
  7%|▋         | 43/608 [00:01<00:12, 43.65it/s][A
  8%|▊         | 48/608 [00:01<00:12, 44.35it/s][A
  9%|▊         | 53/608 [00:01<00:12, 44.68it/s][A
 10%|▉         | 58/608 [00:01<00:12, 44.50it/s][A
 10%|█         | 63/608 [00:01<00:12, 44.66it/s][A
 11%|█         | 68/608 [00:01<00:12, 44.90it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.07it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.34it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.46it/s][A
 14%|█▍        | 88/608 [00:02<00:11, 45.51it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.59it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.38it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 44.98it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.08it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.14it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.31it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.55it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.57it/s][A
 22%|██▏       | 133/608 [00:03<00:10, 45.67it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.62it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.40it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.09it/s][A
 25%|██▌       | 153/608 [00:03<00:12, 36.56it/s][A
 26%|██▌       | 158/608 [00:03<00:11, 39.00it/s][A
 27%|██▋       | 163/608 [00:03<00:10, 40.86it/s][A
 28%|██▊       | 168/608 [00:03<00:10, 42.26it/s][A
 28%|██▊       | 173/608 [00:03<00:10, 43.32it/s][A
 29%|██▉       | 178/608 [00:04<00:09, 43.91it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.57it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.80it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.62it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 44.51it/s][A
 33%|███▎      | 203/608 [00:04<00:09, 44.75it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.12it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.24it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.29it/s][A
 37%|███▋      | 223/608 [00:05<00:08, 45.51it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.62it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.25it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.05it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 44.97it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.10it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.21it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.44it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.55it/s][A
 44%|████▍     | 268/608 [00:06<00:07, 45.63it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.66it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.66it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.32it/s][A
 47%|████▋     | 288/608 [00:06<00:11, 28.61it/s][A
 48%|████▊     | 293/608 [00:06<00:09, 32.24it/s][A
 49%|████▉     | 298/608 [00:06<00:08, 35.38it/s][A
 50%|████▉     | 303/608 [00:07<00:08, 37.98it/s][A
 51%|█████     | 308/608 [00:07<00:07, 40.03it/s][A
 51%|█████▏    | 313/608 [00:07<00:07, 41.61it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 42.74it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 43.59it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 43.79it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 44.05it/s][A
 56%|█████▌    | 338/608 [00:07<00:06, 44.44it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 44.82it/s][A
 57%|█████▋    | 348/608 [00:08<00:05, 45.07it/s][A
 58%|█████▊    | 353/608 [00:08<00:05, 45.23it/s][A
 59%|█████▉    | 358/608 [00:08<00:05, 45.24it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.39it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.35it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.09it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 44.98it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.05it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.18it/s][A
 65%|██████▍   | 393/608 [00:09<00:04, 45.37it/s][A
 65%|██████▌   | 398/608 [00:09<00:04, 45.42it/s][A
 66%|██████▋   | 403/608 [00:09<00:04, 45.56it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.59it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.47it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 40.88it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 42.25it/s][A
 70%|███████   | 428/608 [00:09<00:04, 43.27it/s][A
 71%|███████   | 433/608 [00:09<00:03, 43.95it/s][A
 72%|███████▏  | 438/608 [00:10<00:03, 44.42it/s][A
 73%|███████▎  | 443/608 [00:10<00:03, 44.83it/s][A
 74%|███████▎  | 448/608 [00:10<00:03, 45.14it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.20it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 44.87it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 44.79it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 44.95it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.03it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.34it/s][A
 79%|███████▉  | 483/608 [00:11<00:02, 45.40it/s][A
 80%|████████  | 488/608 [00:11<00:02, 45.57it/s][A
 81%|████████  | 493/608 [00:11<00:02, 45.63it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.45it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.12it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.01it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 44.95it/s][A
 85%|████████▌ | 518/608 [00:11<00:02, 44.13it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 44.56it/s][A
 87%|████████▋ | 528/608 [00:12<00:01, 44.93it/s][A
 88%|████████▊ | 533/608 [00:12<00:01, 45.23it/s][A
 88%|████████▊ | 538/608 [00:12<00:01, 45.42it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.33it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.22it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 32.95it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 35.95it/s][A
 93%|█████████▎| 563/608 [00:12<00:01, 38.42it/s][A
 93%|█████████▎| 568/608 [00:13<00:00, 40.37it/s][A
 94%|█████████▍| 573/608 [00:13<00:00, 41.88it/s][A
 95%|█████████▌| 578/608 [00:13<00:00, 42.99it/s][A
 96%|█████████▌| 583/608 [00:13<00:00, 43.71it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 44.18it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 44.26it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 44.46it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 44.72it/s][A
100%|██████████| 608/608 [00:13<00:00, 44.98it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [04:28<01:22,  3.83it/s]
100%|██████████| 608/608 [00:13<00:00, 44.98it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:56:35,428 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 20:56:36,176 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:56:47,258 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:56:47,765 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:56:47,884 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [05:02<1:15:36, 14.49s/it] 60%|██████    | 473/785 [05:02<53:15, 10.24s/it]   60%|██████    | 474/785 [05:03<37:36,  7.26s/it] 61%|██████    | 475/785 [05:03<26:41,  5.17s/it] 61%|██████    | 476/785 [05:03<19:18,  3.75s/it] 61%|██████    | 477/785 [05:04<13:55,  2.71s/it] 61%|██████    | 478/785 [05:04<10:09,  1.98s/it] 61%|██████    | 479/785 [05:04<07:31,  1.48s/it] 61%|██████    | 480/785 [05:05<05:41,  1.12s/it] 61%|██████▏   | 481/785 [05:05<04:24,  1.15it/s] 61%|██████▏   | 482/785 [05:05<03:30,  1.44it/s] 62%|██████▏   | 483/785 [05:05<02:53,  1.74it/s] 62%|██████▏   | 484/785 [05:06<02:26,  2.05it/s] 62%|██████▏   | 485/785 [05:06<02:08,  2.34it/s] 62%|██████▏   | 486/785 [05:06<02:11,  2.27it/s] 62%|██████▏   | 487/785 [05:07<01:57,  2.53it/s] 62%|██████▏   | 488/785 [05:07<01:47,  2.76it/s] 62%|██████▏   | 489/785 [05:07<01:40,  2.94it/s] 62%|██████▏   | 490/785 [05:08<01:35,  3.08it/s] 63%|██████▎   | 491/785 [05:08<01:32,  3.19it/s] 63%|██████▎   | 492/785 [05:08<01:29,  3.27it/s] 63%|██████▎   | 493/785 [05:08<01:27,  3.33it/s] 63%|██████▎   | 494/785 [05:09<01:26,  3.37it/s] 63%|██████▎   | 495/785 [05:09<01:25,  3.40it/s] 63%|██████▎   | 496/785 [05:10<01:40,  2.88it/s] 63%|██████▎   | 497/785 [05:10<01:34,  3.04it/s] 63%|██████▎   | 498/785 [05:10<01:30,  3.16it/s] 64%|██████▎   | 499/785 [05:10<01:28,  3.24it/s] 64%|██████▎   | 500/785 [05:11<01:26,  3.31it/s]                                                  64%|██████▎   | 500/785 [05:11<01:26,  3.31it/s] 64%|██████▍   | 501/785 [05:11<01:24,  3.36it/s] 64%|██████▍   | 502/785 [05:11<01:23,  3.39it/s] 64%|██████▍   | 503/785 [05:12<01:22,  3.42it/s] 64%|██████▍   | 504/785 [05:12<01:21,  3.43it/s] 64%|██████▍   | 505/785 [05:12<01:21,  3.44it/s] 64%|██████▍   | 506/785 [05:13<01:38,  2.85it/s] 65%|██████▍   | 507/785 [05:13<01:32,  3.01it/s] 65%|██████▍   | 508/785 [05:13<01:28,  3.13it/s] 65%|██████▍   | 509/785 [05:13<01:25,  3.23it/s] 65%|██████▍   | 510/785 [05:14<01:23,  3.30it/s] 65%|██████▌   | 511/785 [05:14<01:21,  3.35it/s] 65%|██████▌   | 512/785 [05:14<01:20,  3.39it/s] 65%|██████▌   | 513/785 [05:15<01:19,  3.41it/s] 65%|██████▌   | 514/785 [05:15<01:18,  3.43it/s] 66%|██████▌   | 515/785 [05:15<01:18,  3.44it/s] 66%|██████▌   | 516/785 [05:16<01:23,  3.24it/s] 66%|██████▌   | 517/785 [05:16<01:21,  3.31it/s] 66%|██████▌   | 518/785 [05:16<01:19,  3.35it/s] 66%|██████▌   | 519/785 [05:16<01:18,  3.39it/s] 66%|██████▌   | 520/785 [05:17<01:17,  3.42it/s] 66%|██████▋   | 521/785 [05:17<01:16,  3.43it/s] 66%|██████▋   | 522/785 [05:17<01:16,  3.45it/s] 67%|██████▋   | 523/785 [05:18<01:15,  3.45it/s] 67%|██████▋   | 524/785 [05:18<01:15,  3.46it/s] 67%|██████▋   | 525/785 [05:18<01:15,  3.47it/s] 67%|██████▋   | 526/785 [05:18<01:14,  3.47it/s] 67%|██████▋   | 527/785 [05:19<01:33,  2.76it/s] 67%|██████▋   | 528/785 [05:19<01:27,  2.94it/s] 67%|██████▋   | 529/785 [05:20<01:23,  3.08it/s] 68%|██████▊   | 530/785 [05:20<01:20,  3.19it/s] 68%|██████▊   | 531/785 [05:20<01:17,  3.27it/s] 68%|██████▊   | 532/785 [05:20<01:16,  3.32it/s] 68%|██████▊   | 533/785 [05:21<01:14,  3.37it/s] 68%|██████▊   | 534/785 [05:21<01:13,  3.40it/s] 68%|██████▊   | 535/785 [05:21<01:13,  3.42it/s] 68%|██████▊   | 536/785 [05:22<01:12,  3.44it/s] 68%|██████▊   | 537/785 [05:22<01:36,  2.57it/s] 69%|██████▊   | 538/785 [05:22<01:28,  2.79it/s] 69%|██████▊   | 539/785 [05:23<01:23,  2.96it/s] 69%|██████▉   | 540/785 [05:23<01:19,  3.10it/s] 69%|██████▉   | 541/785 [05:23<01:16,  3.20it/s] 69%|██████▉   | 542/785 [05:24<01:14,  3.28it/s] 69%|██████▉   | 543/785 [05:24<01:12,  3.33it/s] 69%|██████▉   | 544/785 [05:24<01:11,  3.37it/s] 69%|██████▉   | 545/785 [05:24<01:10,  3.40it/s] 70%|██████▉   | 546/785 [05:25<01:09,  3.42it/s] 70%|██████▉   | 547/785 [05:25<01:22,  2.90it/s] 70%|██████▉   | 548/785 [05:26<01:17,  3.05it/s] 70%|██████▉   | 549/785 [05:26<01:14,  3.16it/s] 70%|███████   | 550/785 [05:26<01:12,  3.25it/s] 70%|███████   | 551/785 [05:26<01:16,  3.06it/s] 70%|███████   | 552/785 [05:27<01:13,  3.17it/s] 70%|███████   | 553/785 [05:27<01:11,  3.26it/s] 71%|███████   | 554/785 [05:27<01:09,  3.32it/s] 71%|███████   | 555/785 [05:28<01:08,  3.36it/s] 71%|███████   | 556/785 [05:28<01:07,  3.40it/s] 71%|███████   | 557/785 [05:28<01:12,  3.15it/s] 71%|███████   | 558/785 [05:29<01:10,  3.24it/s] 71%|███████   | 559/785 [05:29<01:08,  3.30it/s] 71%|███████▏  | 560/785 [05:29<01:07,  3.35it/s] 71%|███████▏  | 561/785 [05:29<01:06,  3.39it/s] 72%|███████▏  | 562/785 [05:30<01:05,  3.41it/s] 72%|███████▏  | 563/785 [05:30<01:04,  3.43it/s] 72%|███████▏  | 564/785 [05:30<01:04,  3.44it/s] 72%|███████▏  | 565/785 [05:31<01:03,  3.45it/s] 72%|███████▏  | 566/785 [05:31<01:03,  3.45it/s] 72%|███████▏  | 567/785 [05:31<01:03,  3.46it/s] 72%|███████▏  | 568/785 [05:32<01:24,  2.57it/s] 72%|███████▏  | 569/785 [05:32<01:17,  2.78it/s] 73%|███████▎  | 570/785 [05:32<01:12,  2.96it/s] 73%|███████▎  | 571/785 [05:33<01:09,  3.10it/s] 73%|███████▎  | 572/785 [05:33<01:06,  3.20it/s] 73%|███████▎  | 573/785 [05:33<01:04,  3.28it/s] 73%|███████▎  | 574/785 [05:34<01:03,  3.33it/s] 73%|███████▎  | 575/785 [05:34<01:23,  2.51it/s] 73%|███████▎  | 576/785 [05:34<01:16,  2.74it/s] 74%|███████▎  | 577/785 [05:35<01:11,  2.92it/s] 74%|███████▎  | 578/785 [05:35<01:07,  3.07it/s] 74%|███████▍  | 579/785 [05:35<01:04,  3.18it/s] 74%|███████▍  | 580/785 [05:36<01:02,  3.26it/s] 74%|███████▍  | 581/785 [05:36<01:01,  3.32it/s] 74%|███████▍  | 582/785 [05:36<01:00,  3.36it/s] 74%|███████▍  | 583/785 [05:36<00:59,  3.39it/s] 74%|███████▍  | 584/785 [05:37<00:58,  3.41it/s] 75%|███████▍  | 585/785 [05:37<01:19,  2.51it/s] 75%|███████▍  | 586/785 [05:38<01:12,  2.74it/s] 75%|███████▍  | 587/785 [05:38<01:07,  2.92it/s] 75%|███████▍  | 588/785 [05:38<01:04,  3.07it/s] 75%|███████▌  | 589/785 [05:39<01:01,  3.18it/s] 75%|███████▌  | 590/785 [05:39<00:59,  3.26it/s] 75%|███████▌  | 591/785 [05:39<00:58,  3.32it/s] 75%|███████▌  | 592/785 [05:39<00:57,  3.36it/s] 76%|███████▌  | 593/785 [05:40<00:56,  3.39it/s] 76%|███████▌  | 594/785 [05:40<00:55,  3.41it/s] 76%|███████▌  | 595/785 [05:40<01:06,  2.88it/s] 76%|███████▌  | 596/785 [05:41<01:02,  3.03it/s] 76%|███████▌  | 597/785 [05:41<00:59,  3.15it/s] 76%|███████▌  | 598/785 [05:41<00:57,  3.24it/s] 76%|███████▋  | 599/785 [05:42<00:56,  3.31it/s] 76%|███████▋  | 600/785 [05:42<00:55,  3.35it/s] 77%|███████▋  | 601/785 [05:42<00:54,  3.39it/s] 77%|███████▋  | 602/785 [05:42<00:53,  3.41it/s] 77%|███████▋  | 603/785 [05:43<00:53,  3.43it/s] 77%|███████▋  | 604/785 [05:43<00:52,  3.44it/s] 77%|███████▋  | 605/785 [05:43<00:55,  3.25it/s] 77%|███████▋  | 606/785 [05:44<00:54,  3.31it/s] 77%|███████▋  | 607/785 [05:44<00:53,  3.35it/s] 77%|███████▋  | 608/785 [05:44<00:52,  3.38it/s] 78%|███████▊  | 609/785 [05:45<00:51,  3.41it/s] 78%|███████▊  | 610/785 [05:45<00:51,  3.43it/s] 78%|███████▊  | 611/785 [05:45<00:50,  3.44it/s] 78%|███████▊  | 612/785 [05:45<00:50,  3.45it/s] 78%|███████▊  | 613/785 [05:46<00:49,  3.46it/s] 78%|███████▊  | 614/785 [05:46<00:49,  3.46it/s] 78%|███████▊  | 615/785 [05:46<00:49,  3.46it/s] 78%|███████▊  | 616/785 [05:47<00:52,  3.21it/s] 79%|███████▊  | 617/785 [05:47<00:51,  3.28it/s] 79%|███████▊  | 618/785 [05:47<00:50,  3.34it/s] 79%|███████▉  | 619/785 [05:48<00:49,  3.37it/s] 79%|███████▉  | 620/785 [05:48<00:48,  3.40it/s] 79%|███████▉  | 621/785 [05:48<00:47,  3.42it/s] 79%|███████▉  | 622/785 [05:48<00:47,  3.44it/s] 79%|███████▉  | 623/785 [05:49<00:46,  3.45it/s] 79%|███████▉  | 624/785 [05:49<00:46,  3.45it/s] 80%|███████▉  | 625/785 [05:49<00:46,  3.46it/s] 80%|███████▉  | 626/785 [05:50<00:45,  3.46it/s] 80%|███████▉  | 627/785 [05:50<00:47,  3.34it/s] 80%|████████  | 628/785 [05:50<00:41,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 20:57:55,174 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:57:55,174 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 20:57:55,174 >>   Batch size = 8
{'eval_loss': 0.9227268695831299, 'eval_runtime': 13.9821, 'eval_samples_per_second': 347.873, 'eval_steps_per_second': 43.484, 'epoch': 3.0}
{'loss': 0.7545, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.65it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.96it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.80it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.07it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.72it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.22it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.81it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.40it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.34it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.44it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.51it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.58it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.66it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.63it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.57it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.41it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.19it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.19it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.14it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.40it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.39it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.61it/s][A
 19%|█▉        | 118/608 [00:03<00:10, 45.68it/s][A
 20%|██        | 123/608 [00:03<00:24, 20.15it/s][A
 21%|██        | 128/608 [00:03<00:19, 24.25it/s][A
 22%|██▏       | 133/608 [00:03<00:16, 28.22it/s][A
 23%|██▎       | 138/608 [00:03<00:14, 31.91it/s][A
 24%|██▎       | 143/608 [00:03<00:13, 35.06it/s][A
 24%|██▍       | 148/608 [00:03<00:12, 37.73it/s][A
 25%|██▌       | 153/608 [00:03<00:11, 39.79it/s][A
 26%|██▌       | 158/608 [00:03<00:10, 41.44it/s][A
 27%|██▋       | 163/608 [00:04<00:10, 42.15it/s][A
 28%|██▊       | 168/608 [00:04<00:10, 43.00it/s][A
 28%|██▊       | 173/608 [00:04<00:09, 43.77it/s][A
 29%|██▉       | 178/608 [00:04<00:09, 44.37it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.72it/s][A
 31%|███       | 188/608 [00:04<00:09, 44.98it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.22it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.37it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.18it/s][A
 34%|███▍      | 208/608 [00:05<00:13, 29.77it/s][A
 35%|███▌      | 214/608 [00:05<00:11, 34.68it/s][A
 36%|███▌      | 219/608 [00:05<00:10, 37.24it/s][A
 37%|███▋      | 224/608 [00:05<00:09, 39.36it/s][A
 38%|███▊      | 229/608 [00:05<00:09, 41.09it/s][A
 38%|███▊      | 234/608 [00:05<00:09, 40.78it/s][A
 39%|███▉      | 239/608 [00:05<00:08, 42.15it/s][A
 40%|████      | 244/608 [00:05<00:08, 43.08it/s][A
 41%|████      | 249/608 [00:06<00:08, 43.74it/s][A
 42%|████▏     | 254/608 [00:06<00:08, 43.97it/s][A
 43%|████▎     | 259/608 [00:06<00:07, 44.43it/s][A
 43%|████▎     | 264/608 [00:06<00:07, 44.80it/s][A
 44%|████▍     | 269/608 [00:06<00:07, 45.13it/s][A
 45%|████▌     | 274/608 [00:06<00:07, 45.04it/s][A
 46%|████▌     | 279/608 [00:06<00:07, 45.15it/s][A
 47%|████▋     | 284/608 [00:06<00:07, 45.07it/s][A
 48%|████▊     | 289/608 [00:06<00:07, 45.33it/s][A
 48%|████▊     | 294/608 [00:07<00:06, 45.19it/s][A
 49%|████▉     | 299/608 [00:07<00:06, 45.12it/s][A
 50%|█████     | 304/608 [00:07<00:06, 45.14it/s][A
 51%|█████     | 309/608 [00:07<00:06, 45.40it/s][A
 52%|█████▏    | 314/608 [00:07<00:06, 45.46it/s][A
 52%|█████▏    | 319/608 [00:07<00:06, 45.44it/s][A
 53%|█████▎    | 324/608 [00:07<00:06, 45.33it/s][A
 54%|█████▍    | 329/608 [00:07<00:06, 45.41it/s][A
 55%|█████▍    | 334/608 [00:07<00:06, 45.37it/s][A
 56%|█████▌    | 339/608 [00:08<00:05, 45.38it/s][A
 57%|█████▋    | 344/608 [00:08<00:05, 45.26it/s][A
 57%|█████▋    | 349/608 [00:08<00:05, 45.29it/s][A
 58%|█████▊    | 354/608 [00:08<00:05, 45.35it/s][A
 59%|█████▉    | 359/608 [00:08<00:05, 45.46it/s][A
 60%|█████▉    | 364/608 [00:08<00:05, 45.52it/s][A
 61%|██████    | 369/608 [00:08<00:05, 45.47it/s][A
 62%|██████▏   | 374/608 [00:08<00:06, 34.46it/s][A
 62%|██████▏   | 379/608 [00:09<00:06, 37.27it/s][A
 63%|██████▎   | 384/608 [00:09<00:05, 39.47it/s][A
 64%|██████▍   | 389/608 [00:09<00:05, 41.19it/s][A
 65%|██████▍   | 394/608 [00:09<00:05, 42.46it/s][A
 66%|██████▌   | 399/608 [00:09<00:04, 43.43it/s][A
 66%|██████▋   | 404/608 [00:09<00:04, 44.10it/s][A
 67%|██████▋   | 409/608 [00:09<00:04, 44.56it/s][A
 68%|██████▊   | 414/608 [00:09<00:04, 44.33it/s][A
 69%|██████▉   | 419/608 [00:09<00:04, 44.53it/s][A
 70%|██████▉   | 424/608 [00:10<00:04, 44.77it/s][A
 71%|███████   | 429/608 [00:10<00:03, 45.07it/s][A
 71%|███████▏  | 434/608 [00:12<00:21,  8.22it/s][A
 72%|███████▏  | 438/608 [00:12<00:18,  9.16it/s][A
 73%|███████▎  | 443/608 [00:12<00:13, 12.25it/s][A
 74%|███████▎  | 448/608 [00:12<00:10, 15.82it/s][A
 75%|███████▍  | 453/608 [00:12<00:07, 19.82it/s][A
 75%|███████▌  | 458/608 [00:12<00:06, 23.98it/s][A
 76%|███████▌  | 463/608 [00:12<00:05, 28.08it/s][A
 77%|███████▋  | 468/608 [00:12<00:04, 31.85it/s][A
 78%|███████▊  | 473/608 [00:12<00:03, 35.14it/s][A
 79%|███████▊  | 478/608 [00:13<00:03, 37.47it/s][A
 79%|███████▉  | 483/608 [00:13<00:03, 39.32it/s][A
 80%|████████  | 488/608 [00:13<00:02, 40.58it/s][A
 81%|████████  | 493/608 [00:13<00:02, 41.75it/s][A
 82%|████████▏ | 498/608 [00:13<00:02, 42.83it/s][A
 83%|████████▎ | 503/608 [00:13<00:02, 43.68it/s][A
 84%|████████▎ | 508/608 [00:13<00:02, 44.32it/s][A
 84%|████████▍ | 513/608 [00:13<00:02, 44.73it/s][A
 85%|████████▌ | 518/608 [00:13<00:01, 45.05it/s][A
 86%|████████▌ | 523/608 [00:14<00:01, 45.32it/s][A
 87%|████████▋ | 528/608 [00:14<00:01, 45.32it/s][A
 88%|████████▊ | 533/608 [00:14<00:01, 45.05it/s][A
 88%|████████▊ | 538/608 [00:14<00:01, 45.00it/s][A
 89%|████████▉ | 543/608 [00:14<00:02, 29.08it/s][A
 90%|█████████ | 548/608 [00:14<00:01, 32.70it/s][A
 91%|█████████ | 553/608 [00:14<00:01, 35.74it/s][A
 92%|█████████▏| 558/608 [00:15<00:01, 38.31it/s][A
 93%|█████████▎| 563/608 [00:15<00:01, 40.28it/s][A
 93%|█████████▎| 568/608 [00:15<00:00, 41.79it/s][A
 94%|█████████▍| 573/608 [00:15<00:00, 42.81it/s][A
 95%|█████████▌| 578/608 [00:15<00:00, 43.67it/s][A
 96%|█████████▌| 583/608 [00:15<00:00, 43.76it/s][A
 97%|█████████▋| 588/608 [00:15<00:00, 44.19it/s][A
 98%|█████████▊| 593/608 [00:15<00:00, 44.55it/s][A
 98%|█████████▊| 598/608 [00:15<00:00, 45.00it/s][A
 99%|█████████▉| 603/608 [00:16<00:00, 45.24it/s][A
100%|██████████| 608/608 [00:16<00:00, 45.34it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [06:06<00:41,  3.78it/s]
100%|██████████| 608/608 [00:16<00:00, 45.34it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:58:11,915 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 20:58:12,661 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:58:23,638 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:58:23,968 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:58:24,051 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [06:42<40:44, 15.67s/it] 80%|████████  | 630/785 [06:42<28:37, 11.08s/it] 80%|████████  | 631/785 [06:42<20:08,  7.84s/it] 81%|████████  | 632/785 [06:43<14:13,  5.58s/it] 81%|████████  | 633/785 [06:43<10:06,  3.99s/it] 81%|████████  | 634/785 [06:43<07:15,  2.88s/it] 81%|████████  | 635/785 [06:44<05:15,  2.10s/it] 81%|████████  | 636/785 [06:44<03:52,  1.56s/it] 81%|████████  | 637/785 [06:44<02:54,  1.18s/it] 81%|████████▏ | 638/785 [06:44<02:14,  1.10it/s] 81%|████████▏ | 639/785 [06:45<01:45,  1.38it/s] 82%|████████▏ | 640/785 [06:45<01:26,  1.68it/s] 82%|████████▏ | 641/785 [06:45<01:13,  1.95it/s] 82%|████████▏ | 642/785 [06:46<01:03,  2.24it/s] 82%|████████▏ | 643/785 [06:46<00:56,  2.51it/s] 82%|████████▏ | 644/785 [06:46<00:51,  2.74it/s] 82%|████████▏ | 645/785 [06:46<00:47,  2.93it/s] 82%|████████▏ | 646/785 [06:47<00:45,  3.08it/s] 82%|████████▏ | 647/785 [06:47<00:43,  3.19it/s] 83%|████████▎ | 648/785 [06:47<00:41,  3.27it/s] 83%|████████▎ | 649/785 [06:48<00:40,  3.33it/s] 83%|████████▎ | 650/785 [06:48<00:40,  3.37it/s] 83%|████████▎ | 651/785 [06:48<00:39,  3.40it/s] 83%|████████▎ | 652/785 [06:48<00:40,  3.25it/s] 83%|████████▎ | 653/785 [06:49<00:39,  3.31it/s] 83%|████████▎ | 654/785 [06:49<00:38,  3.36it/s] 83%|████████▎ | 655/785 [06:49<00:38,  3.39it/s] 84%|████████▎ | 656/785 [06:50<00:37,  3.42it/s] 84%|████████▎ | 657/785 [06:50<00:37,  3.44it/s] 84%|████████▍ | 658/785 [06:50<00:36,  3.45it/s] 84%|████████▍ | 659/785 [06:50<00:36,  3.45it/s] 84%|████████▍ | 660/785 [06:51<00:36,  3.46it/s] 84%|████████▍ | 661/785 [06:51<00:35,  3.46it/s] 84%|████████▍ | 662/785 [06:51<00:35,  3.46it/s] 84%|████████▍ | 663/785 [06:52<00:39,  3.11it/s] 85%|████████▍ | 664/785 [06:52<00:37,  3.21it/s] 85%|████████▍ | 665/785 [06:52<00:36,  3.29it/s] 85%|████████▍ | 666/785 [06:53<00:35,  3.34it/s] 85%|████████▍ | 667/785 [06:53<00:34,  3.38it/s] 85%|████████▌ | 668/785 [06:53<00:34,  3.41it/s] 85%|████████▌ | 669/785 [06:53<00:33,  3.43it/s] 85%|████████▌ | 670/785 [06:54<00:33,  3.44it/s] 85%|████████▌ | 671/785 [06:54<00:33,  3.45it/s] 86%|████████▌ | 672/785 [06:54<00:32,  3.45it/s] 86%|████████▌ | 673/785 [06:55<00:32,  3.45it/s] 86%|████████▌ | 674/785 [06:55<00:36,  3.04it/s] 86%|████████▌ | 675/785 [06:55<00:34,  3.15it/s] 86%|████████▌ | 676/785 [06:56<00:33,  3.24it/s] 86%|████████▌ | 677/785 [06:56<00:32,  3.31it/s] 86%|████████▋ | 678/785 [06:56<00:31,  3.35it/s] 86%|████████▋ | 679/785 [06:57<00:31,  3.39it/s] 87%|████████▋ | 680/785 [06:57<00:30,  3.41it/s] 87%|████████▋ | 681/785 [06:57<00:30,  3.43it/s] 87%|████████▋ | 682/785 [06:57<00:29,  3.44it/s] 87%|████████▋ | 683/785 [06:58<00:29,  3.45it/s] 87%|████████▋ | 684/785 [06:58<00:32,  3.10it/s] 87%|████████▋ | 685/785 [06:58<00:31,  3.21it/s] 87%|████████▋ | 686/785 [06:59<00:30,  3.28it/s] 88%|████████▊ | 687/785 [06:59<00:29,  3.33it/s] 88%|████████▊ | 688/785 [06:59<00:28,  3.37it/s] 88%|████████▊ | 689/785 [06:59<00:28,  3.40it/s] 88%|████████▊ | 690/785 [07:00<00:27,  3.42it/s] 88%|████████▊ | 691/785 [07:00<00:27,  3.44it/s] 88%|████████▊ | 692/785 [07:00<00:27,  3.36it/s] 88%|████████▊ | 693/785 [07:01<00:31,  2.95it/s] 88%|████████▊ | 694/785 [07:01<00:36,  2.51it/s] 89%|████████▊ | 695/785 [07:02<00:51,  1.75it/s] 89%|████████▊ | 696/785 [07:03<00:43,  2.05it/s] 89%|████████▉ | 697/785 [07:03<00:37,  2.34it/s] 89%|████████▉ | 698/785 [07:03<00:33,  2.59it/s] 89%|████████▉ | 699/785 [07:03<00:30,  2.80it/s] 89%|████████▉ | 700/785 [07:04<00:28,  2.98it/s] 89%|████████▉ | 701/785 [07:04<00:27,  3.11it/s] 89%|████████▉ | 702/785 [07:04<00:28,  2.90it/s] 90%|████████▉ | 703/785 [07:05<00:26,  3.05it/s] 90%|████████▉ | 704/785 [07:05<00:25,  3.17it/s] 90%|████████▉ | 705/785 [07:05<00:24,  3.25it/s] 90%|████████▉ | 706/785 [07:06<00:23,  3.31it/s] 90%|█████████ | 707/785 [07:06<00:23,  3.36it/s] 90%|█████████ | 708/785 [07:06<00:22,  3.39it/s] 90%|█████████ | 709/785 [07:07<00:26,  2.87it/s] 90%|█████████ | 710/785 [07:07<00:24,  3.03it/s] 91%|█████████ | 711/785 [07:07<00:23,  3.15it/s] 91%|█████████ | 712/785 [07:08<00:22,  3.24it/s] 91%|█████████ | 713/785 [07:08<00:21,  3.30it/s] 91%|█████████ | 714/785 [07:08<00:21,  3.35it/s] 91%|█████████ | 715/785 [07:08<00:20,  3.39it/s] 91%|█████████ | 716/785 [07:09<00:20,  3.41it/s] 91%|█████████▏| 717/785 [07:09<00:19,  3.43it/s] 91%|█████████▏| 718/785 [07:09<00:19,  3.44it/s] 92%|█████████▏| 719/785 [07:10<00:23,  2.80it/s] 92%|█████████▏| 720/785 [07:10<00:21,  2.97it/s] 92%|█████████▏| 721/785 [07:10<00:20,  3.10it/s] 92%|█████████▏| 722/785 [07:11<00:19,  3.20it/s] 92%|█████████▏| 723/785 [07:11<00:18,  3.28it/s] 92%|█████████▏| 724/785 [07:11<00:18,  3.33it/s] 92%|█████████▏| 725/785 [07:12<00:17,  3.37it/s] 92%|█████████▏| 726/785 [07:12<00:17,  3.40it/s] 93%|█████████▎| 727/785 [07:12<00:16,  3.42it/s] 93%|█████████▎| 728/785 [07:12<00:16,  3.43it/s] 93%|█████████▎| 729/785 [07:13<00:21,  2.60it/s] 93%|█████████▎| 730/785 [07:13<00:19,  2.81it/s] 93%|█████████▎| 731/785 [07:14<00:18,  2.98it/s] 93%|█████████▎| 732/785 [07:14<00:17,  3.11it/s] 93%|█████████▎| 733/785 [07:14<00:16,  3.21it/s] 94%|█████████▎| 734/785 [07:14<00:15,  3.28it/s] 94%|█████████▎| 735/785 [07:15<00:14,  3.34it/s] 94%|█████████▍| 736/785 [07:15<00:14,  3.37it/s] 94%|█████████▍| 737/785 [07:15<00:14,  3.40it/s] 94%|█████████▍| 738/785 [07:16<00:13,  3.42it/s] 94%|█████████▍| 739/785 [07:16<00:14,  3.25it/s] 94%|█████████▍| 740/785 [07:16<00:13,  3.31it/s] 94%|█████████▍| 741/785 [07:16<00:13,  3.36it/s] 95%|█████████▍| 742/785 [07:17<00:12,  3.39it/s] 95%|█████████▍| 743/785 [07:17<00:12,  3.41it/s] 95%|█████████▍| 744/785 [07:17<00:11,  3.43it/s] 95%|█████████▍| 745/785 [07:18<00:11,  3.44it/s] 95%|█████████▌| 746/785 [07:18<00:11,  3.45it/s] 95%|█████████▌| 747/785 [07:18<00:10,  3.46it/s] 95%|█████████▌| 748/785 [07:19<00:10,  3.46it/s] 95%|█████████▌| 749/785 [07:19<00:10,  3.46it/s] 96%|█████████▌| 750/785 [07:19<00:11,  3.15it/s] 96%|█████████▌| 751/785 [07:19<00:10,  3.24it/s] 96%|█████████▌| 752/785 [07:20<00:09,  3.30it/s] 96%|█████████▌| 753/785 [07:20<00:09,  3.35it/s] 96%|█████████▌| 754/785 [07:20<00:09,  3.39it/s] 96%|█████████▌| 755/785 [07:21<00:08,  3.41it/s] 96%|█████████▋| 756/785 [07:21<00:08,  3.43it/s] 96%|█████████▋| 757/785 [07:21<00:08,  3.44it/s] 97%|█████████▋| 758/785 [07:21<00:07,  3.45it/s] 97%|█████████▋| 759/785 [07:22<00:07,  3.46it/s] 97%|█████████▋| 760/785 [07:22<00:07,  3.46it/s] 97%|█████████▋| 761/785 [07:23<00:08,  2.90it/s] 97%|█████████▋| 762/785 [07:23<00:07,  3.05it/s] 97%|█████████▋| 763/785 [07:23<00:06,  3.17it/s] 97%|█████████▋| 764/785 [07:23<00:06,  3.25it/s] 97%|█████████▋| 765/785 [07:24<00:06,  3.32it/s] 98%|█████████▊| 766/785 [07:24<00:05,  3.36it/s] 98%|█████████▊| 767/785 [07:24<00:05,  3.39it/s] 98%|█████████▊| 768/785 [07:25<00:04,  3.42it/s] 98%|█████████▊| 769/785 [07:25<00:04,  3.43it/s] 98%|█████████▊| 770/785 [07:25<00:04,  3.44it/s] 98%|█████████▊| 771/785 [07:26<00:04,  3.05it/s] 98%|█████████▊| 772/785 [07:26<00:04,  3.16it/s] 98%|█████████▊| 773/785 [07:26<00:03,  3.25it/s] 99%|█████████▊| 774/785 [07:26<00:03,  3.31it/s] 99%|█████████▊| 775/785 [07:27<00:02,  3.36it/s] 99%|█████████▉| 776/785 [07:27<00:02,  3.39it/s] 99%|█████████▉| 777/785 [07:27<00:02,  3.42it/s] 99%|█████████▉| 778/785 [07:28<00:02,  3.43it/s] 99%|█████████▉| 779/785 [07:28<00:01,  3.44it/s] 99%|█████████▉| 780/785 [07:28<00:01,  3.45it/s] 99%|█████████▉| 781/785 [07:29<00:01,  3.10it/s]100%|█████████▉| 782/785 [07:29<00:00,  3.20it/s]100%|█████████▉| 783/785 [07:29<00:00,  3.27it/s]100%|█████████▉| 784/785 [07:29<00:00,  3.33it/s]100%|██████████| 785/785 [07:30<00:00,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 20:59:34,710 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:59:34,710 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 20:59:34,710 >>   Batch size = 8
{'eval_loss': 0.9195592999458313, 'eval_runtime': 16.2035, 'eval_samples_per_second': 300.181, 'eval_steps_per_second': 37.523, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.93it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.78it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.85it/s][A
  4%|▍         | 23/608 [00:00<00:12, 47.02it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.46it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.16it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.80it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.49it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.42it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.57it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.29it/s][A
 10%|█         | 63/608 [00:01<00:12, 45.37it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.48it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.52it/s][A
 13%|█▎        | 78/608 [00:01<00:14, 35.45it/s][A
 14%|█▎        | 83/608 [00:01<00:13, 38.03it/s][A
 14%|█▍        | 88/608 [00:02<00:12, 40.01it/s][A
 15%|█▌        | 93/608 [00:02<00:12, 41.62it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 42.80it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 43.62it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 44.24it/s][A
 19%|█▊        | 113/608 [00:02<00:11, 44.69it/s][A
 19%|█▉        | 118/608 [00:02<00:11, 44.43it/s][A
 20%|██        | 123/608 [00:02<00:10, 44.70it/s][A
 21%|██        | 128/608 [00:02<00:10, 44.84it/s][A
 22%|██▏       | 133/608 [00:03<00:10, 45.06it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.23it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.40it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.56it/s][A
 25%|██▌       | 153/608 [00:03<00:09, 45.56it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.48it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.25it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.22it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.23it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.26it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.43it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.51it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.49it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.55it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.40it/s][A
 34%|███▍      | 208/608 [00:05<00:08, 45.29it/s][A
 35%|███▌      | 213/608 [00:05<00:21, 18.70it/s][A
 36%|███▌      | 218/608 [00:05<00:17, 22.74it/s][A
 37%|███▋      | 223/608 [00:05<00:14, 26.78it/s][A
 38%|███▊      | 228/608 [00:05<00:12, 30.61it/s][A
 38%|███▊      | 233/608 [00:05<00:11, 34.00it/s][A
 39%|███▉      | 238/608 [00:05<00:10, 36.80it/s][A
 40%|███▉      | 243/608 [00:05<00:09, 39.09it/s][A
 41%|████      | 248/608 [00:06<00:08, 40.81it/s][A
 42%|████▏     | 253/608 [00:06<00:08, 41.76it/s][A
 42%|████▏     | 258/608 [00:06<00:08, 42.79it/s][A
 43%|████▎     | 263/608 [00:06<00:07, 43.56it/s][A
 44%|████▍     | 268/608 [00:06<00:07, 44.24it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 44.59it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 44.92it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.09it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.25it/s][A
 48%|████▊     | 293/608 [00:07<00:06, 45.16it/s][A
 49%|████▉     | 298/608 [00:07<00:06, 45.02it/s][A
 50%|████▉     | 303/608 [00:07<00:06, 45.08it/s][A
 51%|█████     | 308/608 [00:07<00:06, 45.25it/s][A
 51%|█████▏    | 313/608 [00:07<00:09, 31.69it/s][A
 52%|█████▏    | 318/608 [00:07<00:08, 34.93it/s][A
 53%|█████▎    | 323/608 [00:07<00:07, 37.59it/s][A
 54%|█████▍    | 328/608 [00:08<00:13, 20.41it/s][A
 55%|█████▍    | 333/608 [00:08<00:11, 24.72it/s][A
 56%|█████▌    | 338/608 [00:08<00:09, 28.65it/s][A
 56%|█████▋    | 343/608 [00:08<00:08, 32.28it/s][A
 57%|█████▋    | 348/608 [00:08<00:07, 35.38it/s][A
 58%|█████▊    | 353/608 [00:08<00:06, 37.97it/s][A
 59%|█████▉    | 358/608 [00:09<00:06, 40.07it/s][A
 60%|█████▉    | 363/608 [00:09<00:05, 41.61it/s][A
 61%|██████    | 368/608 [00:09<00:05, 42.59it/s][A
 61%|██████▏   | 373/608 [00:09<00:05, 43.10it/s][A
 62%|██████▏   | 378/608 [00:09<00:05, 43.74it/s][A
 63%|██████▎   | 383/608 [00:09<00:05, 44.27it/s][A
 64%|██████▍   | 388/608 [00:09<00:04, 44.69it/s][A
 65%|██████▍   | 393/608 [00:09<00:04, 44.95it/s][A
 65%|██████▌   | 398/608 [00:09<00:04, 45.14it/s][A
 66%|██████▋   | 403/608 [00:10<00:04, 45.35it/s][A
 67%|██████▋   | 408/608 [00:10<00:04, 45.34it/s][A
 68%|██████▊   | 413/608 [00:10<00:04, 45.23it/s][A
 69%|██████▉   | 418/608 [00:10<00:04, 44.97it/s][A
 70%|██████▉   | 423/608 [00:10<00:05, 32.05it/s][A
 70%|███████   | 428/608 [00:10<00:05, 35.25it/s][A
 71%|███████   | 433/608 [00:10<00:04, 37.88it/s][A
 72%|███████▏  | 438/608 [00:10<00:04, 39.93it/s][A
 73%|███████▎  | 443/608 [00:11<00:03, 41.48it/s][A
 74%|███████▎  | 448/608 [00:11<00:03, 42.70it/s][A
 75%|███████▍  | 453/608 [00:11<00:03, 43.55it/s][A
 75%|███████▌  | 458/608 [00:11<00:03, 44.18it/s][A
 76%|███████▌  | 463/608 [00:11<00:03, 44.10it/s][A
 77%|███████▋  | 468/608 [00:11<00:03, 44.42it/s][A
 78%|███████▊  | 473/608 [00:11<00:03, 44.68it/s][A
 79%|███████▊  | 478/608 [00:11<00:02, 45.06it/s][A
 79%|███████▉  | 483/608 [00:11<00:02, 45.19it/s][A
 80%|████████  | 488/608 [00:12<00:02, 45.32it/s][A
 81%|████████  | 493/608 [00:12<00:02, 45.39it/s][A
 82%|████████▏ | 498/608 [00:12<00:02, 45.46it/s][A
 83%|████████▎ | 503/608 [00:12<00:02, 45.26it/s][A
 84%|████████▎ | 508/608 [00:12<00:02, 45.16it/s][A
 84%|████████▍ | 513/608 [00:12<00:02, 45.05it/s][A
 85%|████████▌ | 518/608 [00:12<00:01, 45.01it/s][A
 86%|████████▌ | 523/608 [00:12<00:01, 45.13it/s][A
 87%|████████▋ | 528/608 [00:12<00:01, 45.44it/s][A
 88%|████████▊ | 533/608 [00:13<00:01, 45.46it/s][A
 88%|████████▊ | 538/608 [00:13<00:01, 45.51it/s][A
 89%|████████▉ | 543/608 [00:13<00:01, 45.54it/s][A
 90%|█████████ | 548/608 [00:13<00:01, 45.42it/s][A
 91%|█████████ | 553/608 [00:13<00:01, 28.53it/s][A
 92%|█████████▏| 558/608 [00:13<00:01, 32.17it/s][A
 93%|█████████▎| 563/608 [00:13<00:01, 35.31it/s][A
 93%|█████████▎| 568/608 [00:14<00:01, 37.96it/s][A
 94%|█████████▍| 573/608 [00:14<00:00, 39.91it/s][A
 95%|█████████▌| 578/608 [00:14<00:00, 41.54it/s][A
 96%|█████████▌| 583/608 [00:14<00:00, 42.70it/s][A
 97%|█████████▋| 588/608 [00:14<00:00, 43.56it/s][A
 98%|█████████▊| 593/608 [00:14<00:00, 43.66it/s][A
 98%|█████████▊| 598/608 [00:14<00:00, 43.98it/s][A
 99%|█████████▉| 603/608 [00:14<00:00, 44.37it/s][A
100%|██████████| 608/608 [00:14<00:00, 44.72it/s][A                                                 
                                                 [A100%|██████████| 785/785 [07:45<00:00,  3.77it/s]
100%|██████████| 608/608 [00:14<00:00, 44.72it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:59:50,177 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 20:59:51,441 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:00:05,358 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:00:06,084 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:00:06,165 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:00:31,451 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:00:31,563 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314 (score: 0.9090890288352966).
                                                 100%|██████████| 785/785 [09:03<00:00,  3.77it/s]100%|██████████| 785/785 [09:03<00:00,  1.44it/s]
[INFO|trainer.py:1894] 2023-08-28 21:01:08,926 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 21:01:09,198 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:01:19,085 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:01:19,454 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:01:20,165 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:01:22,531 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:22,596 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:22,596 >>   train_loss               =     0.7417
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:22,596 >>   train_runtime            = 0:09:03.22
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:22,596 >>   train_samples            =      10018
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:22,596 >>   train_samples_per_second =     92.209
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:22,596 >>   train_steps_per_second   =      1.445
{'eval_loss': 0.9218265414237976, 'eval_runtime': 14.9209, 'eval_samples_per_second': 325.985, 'eval_steps_per_second': 40.748, 'epoch': 5.0}
{'train_runtime': 543.2245, 'train_samples_per_second': 92.209, 'train_steps_per_second': 1.445, 'train_loss': 0.7416631030428941, 'epoch': 5.0}
08/28/2023 21:01:23 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:01:23,572 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:01:23,573 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-28 21:01:23,573 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 56.22it/s]  2%|▏         | 12/608 [00:00<00:11, 50.21it/s]  3%|▎         | 18/608 [00:00<00:12, 48.41it/s]  4%|▍         | 23/608 [00:00<00:12, 47.68it/s]  5%|▍         | 28/608 [00:00<00:12, 47.25it/s]  5%|▌         | 33/608 [00:00<00:12, 46.89it/s]  6%|▋         | 38/608 [00:00<00:12, 46.75it/s]  7%|▋         | 43/608 [00:00<00:12, 46.27it/s]  8%|▊         | 48/608 [00:01<00:12, 45.73it/s]  9%|▊         | 53/608 [00:01<00:12, 45.33it/s] 10%|▉         | 58/608 [00:01<00:12, 45.47it/s] 10%|█         | 63/608 [00:01<00:11, 45.65it/s] 11%|█         | 68/608 [00:01<00:11, 45.91it/s] 12%|█▏        | 73/608 [00:01<00:11, 45.92it/s] 13%|█▎        | 78/608 [00:01<00:11, 46.12it/s] 14%|█▎        | 83/608 [00:01<00:11, 46.06it/s] 14%|█▍        | 88/608 [00:01<00:11, 46.00it/s] 15%|█▌        | 93/608 [00:02<00:11, 45.59it/s] 16%|█▌        | 98/608 [00:02<00:13, 39.12it/s] 17%|█▋        | 103/608 [00:02<00:12, 41.08it/s] 18%|█▊        | 108/608 [00:02<00:11, 42.50it/s] 19%|█▊        | 113/608 [00:02<00:11, 43.52it/s] 19%|█▉        | 118/608 [00:02<00:11, 44.36it/s] 20%|██        | 123/608 [00:02<00:10, 44.93it/s] 21%|██        | 128/608 [00:02<00:10, 45.33it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.42it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.23it/s] 24%|██▎       | 143/608 [00:03<00:10, 44.97it/s] 24%|██▍       | 148/608 [00:03<00:10, 45.03it/s] 25%|██▌       | 153/608 [00:03<00:10, 45.29it/s] 26%|██▌       | 158/608 [00:03<00:09, 45.33it/s] 27%|██▋       | 163/608 [00:03<00:09, 45.71it/s] 28%|██▊       | 168/608 [00:03<00:09, 45.87it/s] 28%|██▊       | 173/608 [00:03<00:09, 46.02it/s] 29%|██▉       | 178/608 [00:03<00:09, 46.05it/s] 30%|███       | 183/608 [00:04<00:09, 45.61it/s] 31%|███       | 188/608 [00:04<00:09, 45.40it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.30it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.28it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.43it/s] 34%|███▍      | 208/608 [00:04<00:08, 45.59it/s] 35%|███▌      | 213/608 [00:04<00:08, 45.66it/s] 36%|███▌      | 218/608 [00:04<00:08, 45.75it/s] 37%|███▋      | 223/608 [00:04<00:08, 45.73it/s] 38%|███▊      | 228/608 [00:05<00:08, 45.89it/s] 38%|███▊      | 233/608 [00:05<00:12, 29.96it/s] 39%|███▉      | 238/608 [00:05<00:11, 33.42it/s] 40%|███▉      | 243/608 [00:05<00:10, 36.37it/s] 41%|████      | 248/608 [00:05<00:09, 38.71it/s] 42%|████▏     | 253/608 [00:05<00:08, 40.68it/s] 42%|████▏     | 258/608 [00:05<00:08, 42.18it/s] 43%|████▎     | 263/608 [00:05<00:07, 43.34it/s] 44%|████▍     | 268/608 [00:06<00:07, 44.14it/s] 45%|████▍     | 273/608 [00:06<00:07, 44.16it/s] 46%|████▌     | 278/608 [00:06<00:07, 44.37it/s] 47%|████▋     | 283/608 [00:06<00:07, 44.60it/s] 47%|████▋     | 288/608 [00:06<00:07, 44.95it/s] 48%|████▊     | 293/608 [00:06<00:06, 45.05it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.21it/s] 50%|████▉     | 303/608 [00:06<00:06, 45.52it/s] 51%|█████     | 308/608 [00:06<00:06, 45.68it/s] 51%|█████▏    | 313/608 [00:07<00:06, 45.43it/s] 52%|█████▏    | 318/608 [00:07<00:06, 45.44it/s] 53%|█████▎    | 323/608 [00:07<00:06, 45.38it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.38it/s] 55%|█████▍    | 333/608 [00:07<00:06, 45.39it/s] 56%|█████▌    | 338/608 [00:07<00:05, 45.46it/s] 56%|█████▋    | 343/608 [00:07<00:05, 45.52it/s] 57%|█████▋    | 348/608 [00:07<00:05, 45.52it/s] 58%|█████▊    | 353/608 [00:07<00:05, 45.57it/s] 59%|█████▉    | 358/608 [00:08<00:05, 45.62it/s] 60%|█████▉    | 363/608 [00:08<00:06, 37.28it/s] 61%|██████    | 368/608 [00:08<00:06, 39.54it/s] 61%|██████▏   | 373/608 [00:08<00:05, 41.31it/s] 62%|██████▏   | 378/608 [00:08<00:05, 42.69it/s] 63%|██████▎   | 383/608 [00:08<00:05, 43.63it/s] 64%|██████▍   | 388/608 [00:08<00:04, 44.32it/s] 65%|██████▍   | 393/608 [00:08<00:04, 44.89it/s] 65%|██████▌   | 398/608 [00:09<00:04, 45.13it/s] 66%|██████▋   | 403/608 [00:09<00:04, 44.91it/s] 67%|██████▋   | 408/608 [00:09<00:04, 44.76it/s] 68%|██████▊   | 413/608 [00:09<00:04, 44.80it/s] 69%|██████▉   | 418/608 [00:09<00:04, 45.05it/s] 70%|██████▉   | 423/608 [00:09<00:04, 45.20it/s] 70%|███████   | 428/608 [00:09<00:03, 45.30it/s] 71%|███████   | 433/608 [00:09<00:03, 45.52it/s] 72%|███████▏  | 438/608 [00:09<00:03, 45.69it/s] 73%|███████▎  | 443/608 [00:10<00:03, 45.80it/s] 74%|███████▎  | 448/608 [00:10<00:03, 45.64it/s] 75%|███████▍  | 453/608 [00:10<00:03, 45.31it/s] 75%|███████▌  | 458/608 [00:10<00:03, 45.30it/s] 76%|███████▌  | 463/608 [00:10<00:03, 45.28it/s] 77%|███████▋  | 468/608 [00:10<00:03, 45.38it/s] 78%|███████▊  | 473/608 [00:10<00:02, 45.44it/s] 79%|███████▊  | 478/608 [00:10<00:02, 45.66it/s] 79%|███████▉  | 483/608 [00:10<00:02, 45.77it/s] 80%|████████  | 488/608 [00:10<00:02, 45.78it/s] 81%|████████  | 493/608 [00:11<00:02, 45.59it/s] 82%|████████▏ | 498/608 [00:11<00:03, 34.72it/s] 83%|████████▎ | 503/608 [00:11<00:02, 37.39it/s] 84%|████████▎ | 508/608 [00:11<00:02, 39.61it/s] 84%|████████▍ | 513/608 [00:11<00:02, 41.36it/s] 85%|████████▌ | 518/608 [00:11<00:02, 42.61it/s] 86%|████████▌ | 523/608 [00:11<00:01, 43.62it/s] 87%|████████▋ | 528/608 [00:11<00:01, 44.35it/s] 88%|████████▊ | 533/608 [00:12<00:01, 44.73it/s] 88%|████████▊ | 538/608 [00:12<00:01, 44.55it/s] 89%|████████▉ | 543/608 [00:12<00:01, 44.51it/s] 90%|█████████ | 548/608 [00:12<00:01, 44.54it/s] 91%|█████████ | 553/608 [00:12<00:01, 44.93it/s] 92%|█████████▏| 558/608 [00:12<00:01, 45.09it/s] 93%|█████████▎| 563/608 [00:12<00:00, 45.42it/s] 93%|█████████▎| 568/608 [00:12<00:00, 45.66it/s] 94%|█████████▍| 573/608 [00:12<00:00, 45.75it/s] 95%|█████████▌| 578/608 [00:13<00:00, 45.65it/s] 96%|█████████▌| 583/608 [00:13<00:00, 45.36it/s] 97%|█████████▋| 588/608 [00:13<00:00, 45.09it/s] 98%|█████████▊| 593/608 [00:13<00:00, 45.01it/s] 98%|█████████▊| 598/608 [00:13<00:00, 45.16it/s] 99%|█████████▉| 603/608 [00:13<00:00, 45.37it/s]100%|██████████| 608/608 [00:13<00:00, 45.52it/s]100%|██████████| 608/608 [00:13<00:00, 44.20it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:01:37,346 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:37,346 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:37,346 >>   eval_loss               =     0.9091
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:37,346 >>   eval_runtime            = 0:00:13.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:37,346 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:37,346 >>   eval_samples_per_second =     353.14
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:37,346 >>   eval_steps_per_second   =     44.143
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:01:37,346 >>   perplexity              =     2.4821
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:08,866 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:09,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:09,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:09,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:09,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:02:11,016 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:02:11,018 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:02:12,155 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:02:13,470 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:02:13,470 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:20,239 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:20,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:20,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:20,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:02:20,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:02:22,129 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:02:22,130 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:02:23,528 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:02:24,268 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:02:24,268 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-471
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-157
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-314
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-785
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/checkpoint-628
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.78it/s]Extractor Predicting: 5it [00:02,  1.80it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.67it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:06,  1.77it/s]Extractor Predicting: 13it [00:07,  1.79it/s]Extractor Predicting: 14it [00:08,  1.76it/s]Extractor Predicting: 15it [00:08,  1.83it/s]Extractor Predicting: 16it [00:09,  1.82it/s]Extractor Predicting: 17it [00:09,  1.62it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.77it/s]Extractor Predicting: 21it [00:12,  1.78it/s]Extractor Predicting: 22it [00:12,  1.77it/s]Extractor Predicting: 23it [00:13,  1.59it/s]Extractor Predicting: 24it [00:14,  1.57it/s]Extractor Predicting: 25it [00:14,  1.55it/s]Extractor Predicting: 26it [00:15,  1.56it/s]Extractor Predicting: 27it [00:15,  1.60it/s]Extractor Predicting: 28it [00:16,  1.51it/s]Extractor Predicting: 29it [00:17,  1.57it/s]Extractor Predicting: 30it [00:17,  1.62it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:19,  1.59it/s]Extractor Predicting: 34it [00:20,  1.62it/s]Extractor Predicting: 35it [00:20,  1.62it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:22,  1.58it/s]Extractor Predicting: 39it [00:23,  1.59it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:24,  1.64it/s]Extractor Predicting: 42it [00:25,  1.64it/s]Extractor Predicting: 43it [00:26,  1.48it/s]Extractor Predicting: 44it [00:26,  1.56it/s]Extractor Predicting: 45it [00:27,  1.58it/s]Extractor Predicting: 46it [00:27,  1.61it/s]Extractor Predicting: 47it [00:28,  1.63it/s]Extractor Predicting: 48it [00:29,  1.62it/s]Extractor Predicting: 49it [00:29,  1.61it/s]Extractor Predicting: 50it [00:30,  1.63it/s]Extractor Predicting: 51it [00:30,  1.62it/s]Extractor Predicting: 52it [00:31,  1.64it/s]Extractor Predicting: 53it [00:32,  1.54it/s]Extractor Predicting: 54it [00:32,  1.62it/s]Extractor Predicting: 55it [00:33,  1.66it/s]Extractor Predicting: 56it [00:33,  1.66it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:35,  1.54it/s]Extractor Predicting: 59it [00:35,  1.58it/s]Extractor Predicting: 60it [00:36,  1.59it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:37,  1.58it/s]Extractor Predicting: 63it [00:38,  1.61it/s]Extractor Predicting: 64it [00:39,  1.64it/s]Extractor Predicting: 65it [00:39,  1.63it/s]Extractor Predicting: 66it [00:40,  1.58it/s]Extractor Predicting: 67it [00:40,  1.62it/s]Extractor Predicting: 68it [00:41,  1.60it/s]Extractor Predicting: 69it [00:42,  1.61it/s]Extractor Predicting: 70it [00:42,  1.65it/s]Extractor Predicting: 71it [00:43,  1.65it/s]Extractor Predicting: 72it [00:43,  1.65it/s]Extractor Predicting: 73it [00:44,  1.68it/s]Extractor Predicting: 74it [00:45,  1.67it/s]Extractor Predicting: 75it [00:45,  1.69it/s]Extractor Predicting: 76it [00:46,  1.68it/s]Extractor Predicting: 77it [00:47,  1.52it/s]Extractor Predicting: 78it [00:47,  1.59it/s]Extractor Predicting: 79it [00:48,  1.62it/s]Extractor Predicting: 80it [00:48,  1.67it/s]Extractor Predicting: 81it [00:49,  1.65it/s]Extractor Predicting: 82it [00:50,  1.45it/s]Extractor Predicting: 83it [00:50,  1.50it/s]Extractor Predicting: 84it [00:51,  1.56it/s]Extractor Predicting: 85it [00:52,  1.58it/s]Extractor Predicting: 86it [00:52,  1.60it/s]Extractor Predicting: 87it [00:53,  1.41it/s]Extractor Predicting: 88it [00:54,  1.47it/s]Extractor Predicting: 89it [00:54,  1.46it/s]Extractor Predicting: 90it [00:55,  1.53it/s]Extractor Predicting: 91it [00:56,  1.59it/s]Extractor Predicting: 92it [00:56,  1.59it/s]Extractor Predicting: 93it [00:57,  1.49it/s]Extractor Predicting: 94it [00:58,  1.53it/s]Extractor Predicting: 95it [00:58,  1.53it/s]Extractor Predicting: 96it [00:59,  1.58it/s]Extractor Predicting: 97it [01:00,  1.53it/s]Extractor Predicting: 98it [01:00,  1.59it/s]Extractor Predicting: 99it [01:01,  1.63it/s]Extractor Predicting: 100it [01:01,  1.63it/s]Extractor Predicting: 101it [01:02,  1.65it/s]Extractor Predicting: 102it [01:03,  1.62it/s]Extractor Predicting: 103it [01:03,  1.67it/s]Extractor Predicting: 104it [01:04,  1.67it/s]Extractor Predicting: 105it [01:04,  1.65it/s]Extractor Predicting: 106it [01:05,  1.66it/s]Extractor Predicting: 107it [01:06,  1.59it/s]Extractor Predicting: 108it [01:06,  1.64it/s]Extractor Predicting: 109it [01:07,  1.62it/s]Extractor Predicting: 110it [01:07,  1.59it/s]Extractor Predicting: 111it [01:08,  1.63it/s]Extractor Predicting: 112it [01:09,  1.64it/s]Extractor Predicting: 113it [01:09,  1.66it/s]Extractor Predicting: 114it [01:10,  1.70it/s]Extractor Predicting: 115it [01:10,  1.76it/s]Extractor Predicting: 116it [01:11,  1.43it/s]Extractor Predicting: 117it [01:12,  1.50it/s]Extractor Predicting: 118it [01:12,  1.58it/s]Extractor Predicting: 119it [01:13,  1.60it/s]Extractor Predicting: 120it [01:14,  1.66it/s]Extractor Predicting: 121it [01:14,  1.61it/s]Extractor Predicting: 122it [01:15,  1.60it/s]Extractor Predicting: 123it [01:16,  1.58it/s]Extractor Predicting: 124it [01:16,  1.56it/s]Extractor Predicting: 125it [01:17,  1.60it/s]Extractor Predicting: 126it [01:17,  1.55it/s]Extractor Predicting: 127it [01:18,  1.58it/s]Extractor Predicting: 128it [01:19,  1.56it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:20,  1.63it/s]Extractor Predicting: 131it [01:21,  1.65it/s]Extractor Predicting: 132it [01:21,  1.61it/s]Extractor Predicting: 133it [01:22,  1.62it/s]Extractor Predicting: 134it [01:22,  1.64it/s]Extractor Predicting: 135it [01:23,  1.68it/s]Extractor Predicting: 136it [01:24,  1.62it/s]Extractor Predicting: 137it [01:24,  1.65it/s]Extractor Predicting: 138it [01:25,  1.65it/s]Extractor Predicting: 139it [01:25,  1.65it/s]Extractor Predicting: 140it [01:26,  1.64it/s]Extractor Predicting: 141it [01:27,  1.62it/s]Extractor Predicting: 142it [01:27,  1.61it/s]Extractor Predicting: 143it [01:28,  1.66it/s]Extractor Predicting: 144it [01:28,  1.67it/s]Extractor Predicting: 145it [01:29,  1.69it/s]Extractor Predicting: 146it [01:30,  1.64it/s]Extractor Predicting: 147it [01:30,  1.50it/s]Extractor Predicting: 148it [01:31,  1.53it/s]Extractor Predicting: 149it [01:32,  1.55it/s]Extractor Predicting: 150it [01:32,  1.60it/s]Extractor Predicting: 151it [01:33,  1.59it/s]Extractor Predicting: 152it [01:34,  1.50it/s]Extractor Predicting: 153it [01:34,  1.53it/s]Extractor Predicting: 154it [01:35,  1.57it/s]Extractor Predicting: 155it [01:35,  1.61it/s]Extractor Predicting: 156it [01:36,  1.66it/s]Extractor Predicting: 157it [01:37,  1.58it/s]Extractor Predicting: 158it [01:37,  1.64it/s]Extractor Predicting: 159it [01:38,  1.63it/s]Extractor Predicting: 160it [01:38,  1.66it/s]Extractor Predicting: 161it [01:39,  1.64it/s]Extractor Predicting: 162it [01:40,  1.63it/s]Extractor Predicting: 163it [01:40,  1.65it/s]Extractor Predicting: 164it [01:41,  1.64it/s]Extractor Predicting: 165it [01:42,  1.61it/s]Extractor Predicting: 166it [01:42,  1.60it/s]Extractor Predicting: 167it [01:43,  1.59it/s]Extractor Predicting: 168it [01:44,  1.58it/s]Extractor Predicting: 169it [01:44,  1.55it/s]Extractor Predicting: 170it [01:45,  1.45it/s]Extractor Predicting: 171it [01:46,  1.48it/s]Extractor Predicting: 172it [01:46,  1.53it/s]Extractor Predicting: 173it [01:47,  1.52it/s]Extractor Predicting: 174it [01:48,  1.51it/s]Extractor Predicting: 175it [01:48,  1.49it/s]Extractor Predicting: 176it [01:49,  1.51it/s]Extractor Predicting: 177it [01:50,  1.51it/s]Extractor Predicting: 178it [01:50,  1.50it/s]Extractor Predicting: 179it [01:51,  1.49it/s]Extractor Predicting: 180it [01:52,  1.48it/s]Extractor Predicting: 181it [01:52,  1.38it/s]Extractor Predicting: 182it [01:53,  1.42it/s]Extractor Predicting: 183it [01:54,  1.46it/s]Extractor Predicting: 184it [01:54,  1.48it/s]Extractor Predicting: 185it [01:55,  1.48it/s]Extractor Predicting: 186it [01:56,  1.51it/s]Extractor Predicting: 187it [01:56,  1.65it/s]Extractor Predicting: 187it [01:56,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:08,881 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:09,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:09,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:09,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:09,119 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:05:10,765 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:05:10,766 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:05:12,223 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:05:13,824 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:05:13,824 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,747 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,863 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,863 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,863 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:05:19,863 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:05:21,523 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:05:21,524 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:05:23,001 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:05:23,433 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:05:23,433 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.7575757575757576,
  "recall": 0.010279605263157895,
  "score": 0.020283975659229212,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.70it/s]Extractor Predicting: 16it [00:09,  1.69it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:11,  1.55it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.49it/s]Extractor Predicting: 24it [00:14,  1.54it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:16,  1.62it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.31it/s]Extractor Predicting: 30it [00:18,  1.41it/s]Extractor Predicting: 31it [00:19,  1.45it/s]Extractor Predicting: 32it [00:20,  1.48it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:21,  1.40it/s]Extractor Predicting: 35it [00:22,  1.46it/s]Extractor Predicting: 36it [00:22,  1.53it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:25,  1.37it/s]Extractor Predicting: 40it [00:25,  1.32it/s]Extractor Predicting: 41it [00:26,  1.43it/s]Extractor Predicting: 42it [00:27,  1.48it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.60it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:30,  1.58it/s]Extractor Predicting: 48it [00:30,  1.59it/s]Extractor Predicting: 49it [00:31,  1.53it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:32,  1.58it/s]Extractor Predicting: 52it [00:33,  1.59it/s]Extractor Predicting: 53it [00:33,  1.65it/s]Extractor Predicting: 54it [00:34,  1.64it/s]Extractor Predicting: 55it [00:35,  1.66it/s]Extractor Predicting: 56it [00:35,  1.67it/s]Extractor Predicting: 57it [00:36,  1.64it/s]Extractor Predicting: 58it [00:36,  1.67it/s]Extractor Predicting: 59it [00:37,  1.64it/s]Extractor Predicting: 60it [00:38,  1.67it/s]Extractor Predicting: 61it [00:38,  1.64it/s]Extractor Predicting: 62it [00:39,  1.65it/s]Extractor Predicting: 63it [00:39,  1.65it/s]Extractor Predicting: 64it [00:40,  1.58it/s]Extractor Predicting: 65it [00:41,  1.60it/s]Extractor Predicting: 66it [00:41,  1.64it/s]Extractor Predicting: 67it [00:42,  1.66it/s]Extractor Predicting: 68it [00:43,  1.57it/s]Extractor Predicting: 69it [00:43,  1.42it/s]Extractor Predicting: 70it [00:44,  1.44it/s]Extractor Predicting: 71it [00:45,  1.51it/s]Extractor Predicting: 72it [00:45,  1.55it/s]Extractor Predicting: 73it [00:46,  1.62it/s]Extractor Predicting: 74it [00:46,  1.65it/s]Extractor Predicting: 75it [00:47,  1.66it/s]Extractor Predicting: 76it [00:48,  1.63it/s]Extractor Predicting: 77it [00:49,  1.46it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:50,  1.58it/s]Extractor Predicting: 80it [00:50,  1.62it/s]Extractor Predicting: 81it [00:51,  1.65it/s]Extractor Predicting: 82it [00:52,  1.56it/s]Extractor Predicting: 83it [00:52,  1.59it/s]Extractor Predicting: 84it [00:53,  1.58it/s]Extractor Predicting: 85it [00:53,  1.64it/s]Extractor Predicting: 86it [00:54,  1.67it/s]Extractor Predicting: 87it [00:55,  1.67it/s]Extractor Predicting: 88it [00:55,  1.72it/s]Extractor Predicting: 89it [00:56,  1.72it/s]Extractor Predicting: 90it [00:56,  1.68it/s]Extractor Predicting: 91it [00:57,  1.73it/s]Extractor Predicting: 92it [00:57,  1.75it/s]Extractor Predicting: 93it [00:58,  1.52it/s]Extractor Predicting: 94it [00:59,  1.60it/s]Extractor Predicting: 95it [00:59,  1.64it/s]Extractor Predicting: 96it [01:00,  1.69it/s]Extractor Predicting: 97it [01:01,  1.69it/s]Extractor Predicting: 98it [01:01,  1.44it/s]Extractor Predicting: 99it [01:02,  1.49it/s]Extractor Predicting: 100it [01:03,  1.57it/s]Extractor Predicting: 101it [01:03,  1.62it/s]Extractor Predicting: 102it [01:04,  1.65it/s]Extractor Predicting: 103it [01:05,  1.46it/s]Extractor Predicting: 104it [01:05,  1.52it/s]Extractor Predicting: 105it [01:06,  1.55it/s]Extractor Predicting: 106it [01:06,  1.58it/s]Extractor Predicting: 107it [01:07,  1.63it/s]Extractor Predicting: 108it [01:08,  1.59it/s]Extractor Predicting: 109it [01:08,  1.59it/s]Extractor Predicting: 110it [01:09,  1.60it/s]Extractor Predicting: 111it [01:10,  1.60it/s]Extractor Predicting: 112it [01:10,  1.64it/s]Extractor Predicting: 113it [01:11,  1.57it/s]Extractor Predicting: 114it [01:11,  1.61it/s]Extractor Predicting: 115it [01:12,  1.63it/s]Extractor Predicting: 116it [01:13,  1.63it/s]Extractor Predicting: 117it [01:13,  1.65it/s]Extractor Predicting: 118it [01:15,  1.16it/s]Extractor Predicting: 119it [01:15,  1.29it/s]Extractor Predicting: 120it [01:16,  1.37it/s]Extractor Predicting: 121it [01:16,  1.46it/s]Extractor Predicting: 122it [01:17,  1.53it/s]Extractor Predicting: 123it [01:18,  1.58it/s]Extractor Predicting: 124it [01:18,  1.62it/s]Extractor Predicting: 125it [01:19,  1.53it/s]Extractor Predicting: 126it [01:20,  1.57it/s]Extractor Predicting: 127it [01:20,  1.61it/s]Extractor Predicting: 128it [01:21,  1.60it/s]Extractor Predicting: 129it [01:21,  1.59it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.61it/s]Extractor Predicting: 132it [01:23,  1.62it/s]Extractor Predicting: 133it [01:24,  1.66it/s]Extractor Predicting: 134it [01:24,  1.65it/s]Extractor Predicting: 135it [01:25,  1.55it/s]Extractor Predicting: 136it [01:26,  1.59it/s]Extractor Predicting: 137it [01:26,  1.60it/s]Extractor Predicting: 138it [01:27,  1.60it/s]Extractor Predicting: 139it [01:28,  1.64it/s]Extractor Predicting: 140it [01:28,  1.50it/s]Extractor Predicting: 141it [01:29,  1.54it/s]Extractor Predicting: 142it [01:30,  1.57it/s]Extractor Predicting: 143it [01:30,  1.60it/s]Extractor Predicting: 144it [01:31,  1.62it/s]Extractor Predicting: 145it [01:31,  1.56it/s]Extractor Predicting: 146it [01:32,  1.58it/s]Extractor Predicting: 147it [01:33,  1.59it/s]Extractor Predicting: 148it [01:33,  1.63it/s]Extractor Predicting: 149it [01:34,  1.62it/s]Extractor Predicting: 150it [01:35,  1.42it/s]Extractor Predicting: 151it [01:35,  1.47it/s]Extractor Predicting: 152it [01:36,  1.53it/s]Extractor Predicting: 153it [01:37,  1.60it/s]Extractor Predicting: 154it [01:37,  1.63it/s]Extractor Predicting: 155it [01:38,  1.64it/s]Extractor Predicting: 156it [01:38,  1.62it/s]Extractor Predicting: 157it [01:39,  1.65it/s]Extractor Predicting: 158it [01:40,  1.65it/s]Extractor Predicting: 159it [01:40,  1.64it/s]Extractor Predicting: 160it [01:41,  1.56it/s]Extractor Predicting: 161it [01:42,  1.59it/s]Extractor Predicting: 162it [01:42,  1.60it/s]Extractor Predicting: 163it [01:43,  1.63it/s]Extractor Predicting: 164it [01:43,  1.63it/s]Extractor Predicting: 165it [01:44,  1.60it/s]Extractor Predicting: 166it [01:45,  1.62it/s]Extractor Predicting: 167it [01:45,  1.64it/s]Extractor Predicting: 168it [01:46,  1.63it/s]Extractor Predicting: 169it [01:46,  1.62it/s]Extractor Predicting: 170it [01:47,  1.46it/s]Extractor Predicting: 171it [01:48,  1.56it/s]Extractor Predicting: 172it [01:48,  1.60it/s]Extractor Predicting: 173it [01:49,  1.61it/s]Extractor Predicting: 174it [01:50,  1.57it/s]Extractor Predicting: 175it [01:50,  1.55it/s]Extractor Predicting: 176it [01:51,  1.56it/s]Extractor Predicting: 177it [01:52,  1.60it/s]Extractor Predicting: 178it [01:52,  1.59it/s]Extractor Predicting: 179it [01:53,  1.46it/s]Extractor Predicting: 180it [01:54,  1.48it/s]Extractor Predicting: 181it [01:54,  1.52it/s]Extractor Predicting: 182it [01:55,  1.54it/s]Extractor Predicting: 183it [01:56,  1.54it/s]Extractor Predicting: 184it [01:56,  1.51it/s]Extractor Predicting: 185it [01:57,  1.54it/s]Extractor Predicting: 186it [01:58,  1.57it/s]Extractor Predicting: 187it [01:58,  1.56it/s]Extractor Predicting: 188it [01:59,  1.60it/s]Extractor Predicting: 189it [01:59,  1.59it/s]Extractor Predicting: 190it [02:00,  1.59it/s]Extractor Predicting: 191it [02:01,  1.61it/s]Extractor Predicting: 192it [02:01,  1.61it/s]Extractor Predicting: 193it [02:02,  1.61it/s]Extractor Predicting: 194it [02:03,  1.48it/s]Extractor Predicting: 195it [02:03,  1.56it/s]Extractor Predicting: 196it [02:04,  1.62it/s]Extractor Predicting: 197it [02:04,  1.62it/s]Extractor Predicting: 198it [02:05,  1.64it/s]Extractor Predicting: 199it [02:06,  1.34it/s]Extractor Predicting: 200it [02:07,  1.44it/s]Extractor Predicting: 201it [02:07,  1.49it/s]Extractor Predicting: 202it [02:08,  1.54it/s]Extractor Predicting: 203it [02:08,  1.57it/s]Extractor Predicting: 204it [02:09,  1.39it/s]Extractor Predicting: 205it [02:10,  1.46it/s]Extractor Predicting: 206it [02:11,  1.54it/s]Extractor Predicting: 207it [02:11,  1.56it/s]Extractor Predicting: 208it [02:12,  1.62it/s]Extractor Predicting: 209it [02:13,  1.40it/s]Extractor Predicting: 210it [02:13,  1.46it/s]Extractor Predicting: 211it [02:14,  1.53it/s]Extractor Predicting: 212it [02:14,  1.58it/s]Extractor Predicting: 213it [02:15,  1.58it/s]Extractor Predicting: 214it [02:16,  1.54it/s]Extractor Predicting: 215it [02:16,  1.57it/s]Extractor Predicting: 216it [02:17,  1.52it/s]Extractor Predicting: 217it [02:18,  1.59it/s]Extractor Predicting: 218it [02:18,  1.59it/s]Extractor Predicting: 219it [02:19,  1.58it/s]Extractor Predicting: 220it [02:19,  1.65it/s]Extractor Predicting: 221it [02:20,  1.58it/s]Extractor Predicting: 222it [02:21,  1.59it/s]Extractor Predicting: 223it [02:21,  1.60it/s]Extractor Predicting: 224it [02:22,  1.64it/s]Extractor Predicting: 225it [02:23,  1.68it/s]Extractor Predicting: 226it [02:23,  1.53it/s]Extractor Predicting: 227it [02:24,  1.58it/s]Extractor Predicting: 228it [02:24,  1.62it/s]Extractor Predicting: 229it [02:25,  1.62it/s]Extractor Predicting: 230it [02:26,  1.67it/s]Extractor Predicting: 231it [02:26,  1.58it/s]Extractor Predicting: 232it [02:27,  1.61it/s]Extractor Predicting: 233it [02:28,  1.63it/s]Extractor Predicting: 234it [02:28,  1.61it/s]Extractor Predicting: 235it [02:29,  1.62it/s]Extractor Predicting: 236it [02:29,  1.60it/s]Extractor Predicting: 237it [02:30,  1.60it/s]Extractor Predicting: 238it [02:31,  1.62it/s]Extractor Predicting: 239it [02:31,  1.62it/s]Extractor Predicting: 240it [02:32,  1.65it/s]Extractor Predicting: 241it [02:33,  1.61it/s]Extractor Predicting: 242it [02:33,  1.61it/s]Extractor Predicting: 243it [02:34,  1.66it/s]Extractor Predicting: 244it [02:34,  1.68it/s]Extractor Predicting: 245it [02:35,  1.66it/s]Extractor Predicting: 246it [02:36,  1.64it/s]Extractor Predicting: 247it [02:36,  1.65it/s]Extractor Predicting: 248it [02:37,  1.68it/s]Extractor Predicting: 249it [02:37,  1.68it/s]Extractor Predicting: 250it [02:38,  1.74it/s]Extractor Predicting: 251it [02:38,  1.70it/s]Extractor Predicting: 252it [02:39,  1.67it/s]Extractor Predicting: 253it [02:40,  1.74it/s]Extractor Predicting: 254it [02:40,  1.70it/s]Extractor Predicting: 255it [02:41,  1.70it/s]Extractor Predicting: 256it [02:41,  1.70it/s]Extractor Predicting: 257it [02:42,  1.70it/s]Extractor Predicting: 258it [02:43,  1.59it/s]Extractor Predicting: 259it [02:43,  1.62it/s]Extractor Predicting: 260it [02:44,  1.64it/s]Extractor Predicting: 261it [02:45,  1.62it/s]Extractor Predicting: 262it [02:45,  1.65it/s]Extractor Predicting: 263it [02:46,  1.22it/s]Extractor Predicting: 264it [02:47,  1.34it/s]Extractor Predicting: 265it [02:48,  1.41it/s]Extractor Predicting: 266it [02:48,  1.36it/s]Extractor Predicting: 267it [02:49,  1.44it/s]Extractor Predicting: 268it [02:50,  1.49it/s]Extractor Predicting: 269it [02:50,  1.50it/s]Extractor Predicting: 270it [02:51,  1.48it/s]Extractor Predicting: 271it [02:52,  1.53it/s]Extractor Predicting: 272it [02:52,  1.60it/s]Extractor Predicting: 273it [02:53,  1.61it/s]Extractor Predicting: 274it [02:53,  1.60it/s]Extractor Predicting: 275it [02:54,  1.55it/s]Extractor Predicting: 276it [02:55,  1.60it/s]Extractor Predicting: 277it [02:55,  1.61it/s]Extractor Predicting: 278it [02:56,  1.63it/s]Extractor Predicting: 279it [02:56,  1.62it/s]Extractor Predicting: 280it [02:57,  1.55it/s]Extractor Predicting: 281it [02:58,  1.58it/s]Extractor Predicting: 282it [02:58,  1.59it/s]Extractor Predicting: 283it [02:59,  1.42it/s]Extractor Predicting: 284it [03:00,  1.50it/s]Extractor Predicting: 285it [03:01,  1.46it/s]Extractor Predicting: 286it [03:01,  1.52it/s]Extractor Predicting: 287it [03:02,  1.54it/s]Extractor Predicting: 288it [03:02,  1.56it/s]Extractor Predicting: 289it [03:03,  1.58it/s]Extractor Predicting: 290it [03:04,  1.50it/s]Extractor Predicting: 291it [03:04,  1.54it/s]Extractor Predicting: 292it [03:05,  1.58it/s]Extractor Predicting: 293it [03:06,  1.59it/s]Extractor Predicting: 294it [03:06,  1.57it/s]Extractor Predicting: 295it [03:07,  1.55it/s]Extractor Predicting: 296it [03:08,  1.57it/s]Extractor Predicting: 297it [03:08,  1.57it/s]Extractor Predicting: 298it [03:09,  1.59it/s]Extractor Predicting: 299it [03:09,  1.59it/s]Extractor Predicting: 300it [03:10,  1.52it/s]Extractor Predicting: 301it [03:11,  1.49it/s]Extractor Predicting: 302it [03:12,  1.49it/s]Extractor Predicting: 303it [03:12,  1.52it/s]Extractor Predicting: 304it [03:13,  1.49it/s]Extractor Predicting: 305it [03:14,  1.43it/s]Extractor Predicting: 306it [03:14,  1.50it/s]Extractor Predicting: 307it [03:15,  1.55it/s]Extractor Predicting: 308it [03:15,  1.58it/s]Extractor Predicting: 309it [03:16,  1.65it/s]Extractor Predicting: 310it [03:17,  1.51it/s]Extractor Predicting: 311it [03:17,  1.51it/s]Extractor Predicting: 312it [03:18,  1.55it/s]Extractor Predicting: 313it [03:19,  1.57it/s]Extractor Predicting: 314it [03:19,  1.58it/s]Extractor Predicting: 315it [03:20,  1.65it/s]Extractor Predicting: 316it [03:20,  1.66it/s]Extractor Predicting: 317it [03:21,  1.64it/s]Extractor Predicting: 318it [03:22,  1.56it/s]Extractor Predicting: 319it [03:22,  1.57it/s]Extractor Predicting: 320it [03:23,  1.60it/s]Extractor Predicting: 321it [03:24,  1.63it/s]Extractor Predicting: 322it [03:24,  1.67it/s]Extractor Predicting: 323it [03:25,  1.59it/s]Extractor Predicting: 324it [03:25,  1.63it/s]Extractor Predicting: 325it [03:26,  1.63it/s]Extractor Predicting: 326it [03:27,  1.66it/s]Extractor Predicting: 327it [03:27,  1.67it/s]Extractor Predicting: 328it [03:28,  1.57it/s]Extractor Predicting: 329it [03:29,  1.59it/s]Extractor Predicting: 330it [03:29,  1.60it/s]Extractor Predicting: 331it [03:30,  1.61it/s]Extractor Predicting: 332it [03:30,  1.65it/s]Extractor Predicting: 333it [03:31,  1.52it/s]Extractor Predicting: 334it [03:32,  1.56it/s]Extractor Predicting: 335it [03:32,  1.59it/s]Extractor Predicting: 336it [03:33,  1.63it/s]Extractor Predicting: 337it [03:33,  1.64it/s]Extractor Predicting: 338it [03:34,  1.57it/s]Extractor Predicting: 339it [03:35,  1.60it/s]Extractor Predicting: 340it [03:35,  1.64it/s]Extractor Predicting: 341it [03:36,  1.65it/s]Extractor Predicting: 342it [03:37,  1.64it/s]Extractor Predicting: 343it [03:37,  1.46it/s]Extractor Predicting: 344it [03:38,  1.54it/s]Extractor Predicting: 345it [03:39,  1.60it/s]Extractor Predicting: 346it [03:39,  1.66it/s]Extractor Predicting: 347it [03:40,  1.72it/s]Extractor Predicting: 348it [03:40,  1.65it/s]Extractor Predicting: 349it [03:41,  1.69it/s]Extractor Predicting: 350it [03:41,  1.69it/s]Extractor Predicting: 351it [03:42,  1.71it/s]Extractor Predicting: 352it [03:43,  1.71it/s]Extractor Predicting: 353it [03:43,  1.73it/s]Extractor Predicting: 354it [03:44,  1.71it/s]Extractor Predicting: 355it [03:44,  1.72it/s]Extractor Predicting: 356it [03:45,  1.75it/s]Extractor Predicting: 357it [03:45,  1.73it/s]Extractor Predicting: 358it [03:46,  1.73it/s]Extractor Predicting: 359it [03:47,  1.72it/s]Extractor Predicting: 360it [03:47,  1.66it/s]Extractor Predicting: 361it [03:48,  1.67it/s]Extractor Predicting: 362it [03:48,  1.70it/s]Extractor Predicting: 363it [03:49,  1.73it/s]Extractor Predicting: 364it [03:50,  1.77it/s]Extractor Predicting: 365it [03:50,  1.71it/s]Extractor Predicting: 366it [03:51,  1.67it/s]Extractor Predicting: 367it [03:51,  1.62it/s]Extractor Predicting: 368it [03:52,  1.51it/s]Extractor Predicting: 369it [03:53,  1.58it/s]Extractor Predicting: 370it [03:53,  1.57it/s]Extractor Predicting: 371it [03:54,  1.60it/s]Extractor Predicting: 372it [03:55,  1.63it/s]Extractor Predicting: 373it [03:56,  1.37it/s]Extractor Predicting: 374it [03:56,  1.45it/s]Extractor Predicting: 375it [03:57,  1.49it/s]Extractor Predicting: 376it [03:57,  1.52it/s]Extractor Predicting: 377it [03:58,  1.54it/s]Extractor Predicting: 378it [03:59,  1.50it/s]Extractor Predicting: 379it [03:59,  1.53it/s]Extractor Predicting: 380it [04:00,  1.56it/s]Extractor Predicting: 381it [04:01,  1.61it/s]Extractor Predicting: 382it [04:01,  1.66it/s]Extractor Predicting: 383it [04:02,  1.60it/s]Extractor Predicting: 384it [04:02,  1.65it/s]Extractor Predicting: 385it [04:03,  1.64it/s]Extractor Predicting: 386it [04:04,  1.68it/s]Extractor Predicting: 387it [04:04,  1.68it/s]Extractor Predicting: 388it [04:05,  1.70it/s]Extractor Predicting: 389it [04:05,  1.66it/s]Extractor Predicting: 390it [04:06,  1.64it/s]Extractor Predicting: 391it [04:07,  1.66it/s]Extractor Predicting: 392it [04:07,  1.73it/s]Extractor Predicting: 393it [04:08,  1.75it/s]Extractor Predicting: 394it [04:08,  1.69it/s]Extractor Predicting: 395it [04:09,  1.73it/s]Extractor Predicting: 396it [04:09,  1.76it/s]Extractor Predicting: 397it [04:10,  1.74it/s]Extractor Predicting: 398it [04:11,  1.73it/s]Extractor Predicting: 399it [04:11,  1.74it/s]Extractor Predicting: 400it [04:12,  1.39it/s]Extractor Predicting: 401it [04:13,  1.49it/s]Extractor Predicting: 402it [04:13,  1.57it/s]Extractor Predicting: 403it [04:14,  1.62it/s]Extractor Predicting: 404it [04:14,  1.66it/s]Extractor Predicting: 405it [04:15,  1.57it/s]Extractor Predicting: 406it [04:16,  1.62it/s]Extractor Predicting: 407it [04:16,  1.65it/s]Extractor Predicting: 408it [04:17,  1.46it/s]Extractor Predicting: 409it [04:18,  1.40it/s]Extractor Predicting: 410it [04:19,  1.36it/s]Extractor Predicting: 411it [04:19,  1.43it/s]Extractor Predicting: 412it [04:20,  1.42it/s]Extractor Predicting: 413it [04:21,  1.44it/s]Extractor Predicting: 414it [04:21,  1.45it/s]Extractor Predicting: 415it [04:22,  1.45it/s]Extractor Predicting: 416it [04:23,  1.48it/s]Extractor Predicting: 417it [04:24,  1.45it/s]Extractor Predicting: 418it [04:24,  1.51it/s]Extractor Predicting: 419it [04:25,  1.53it/s]Extractor Predicting: 420it [04:25,  1.52it/s]Extractor Predicting: 421it [04:26,  1.54it/s]Extractor Predicting: 422it [04:27,  1.53it/s]Extractor Predicting: 423it [04:27,  1.58it/s]Extractor Predicting: 424it [04:28,  1.58it/s]Extractor Predicting: 425it [04:29,  1.57it/s]Extractor Predicting: 426it [04:29,  1.57it/s]Extractor Predicting: 427it [04:30,  1.27it/s]Extractor Predicting: 428it [04:31,  1.36it/s]Extractor Predicting: 429it [04:32,  1.45it/s]Extractor Predicting: 430it [04:32,  1.50it/s]Extractor Predicting: 431it [04:33,  1.54it/s]Extractor Predicting: 432it [04:33,  1.49it/s]Extractor Predicting: 433it [04:34,  1.53it/s]Extractor Predicting: 434it [04:35,  1.55it/s]Extractor Predicting: 435it [04:35,  1.59it/s]Extractor Predicting: 436it [04:36,  1.64it/s]Extractor Predicting: 437it [04:37,  1.60it/s]Extractor Predicting: 438it [04:37,  1.63it/s]Extractor Predicting: 439it [04:38,  1.63it/s]Extractor Predicting: 440it [04:38,  1.63it/s]Extractor Predicting: 441it [04:39,  1.62it/s]Extractor Predicting: 442it [04:40,  1.49it/s]Extractor Predicting: 443it [04:40,  1.51it/s]Extractor Predicting: 444it [04:41,  1.54it/s]Extractor Predicting: 445it [04:42,  1.55it/s]Extractor Predicting: 446it [04:42,  1.57it/s]Extractor Predicting: 447it [04:43,  1.46it/s]Extractor Predicting: 448it [04:44,  1.49it/s]Extractor Predicting: 449it [04:44,  1.56it/s]Extractor Predicting: 450it [04:45,  1.61it/s]Extractor Predicting: 451it [04:45,  1.61it/s]Extractor Predicting: 452it [04:46,  1.44it/s]Extractor Predicting: 453it [04:47,  1.49it/s]Extractor Predicting: 454it [04:48,  1.53it/s]Extractor Predicting: 455it [04:48,  1.53it/s]Extractor Predicting: 456it [04:49,  1.57it/s]Extractor Predicting: 457it [04:50,  1.41it/s]Extractor Predicting: 458it [04:50,  1.48it/s]Extractor Predicting: 459it [04:51,  1.52it/s]Extractor Predicting: 460it [04:52,  1.53it/s]Extractor Predicting: 461it [04:52,  1.57it/s]Extractor Predicting: 462it [04:53,  1.58it/s]Extractor Predicting: 463it [04:53,  1.55it/s]Extractor Predicting: 464it [04:54,  1.48it/s]Extractor Predicting: 465it [04:55,  1.50it/s]Extractor Predicting: 466it [04:55,  1.54it/s]Extractor Predicting: 467it [04:56,  1.60it/s]Extractor Predicting: 468it [04:57,  1.62it/s]Extractor Predicting: 469it [04:57,  1.65it/s]Extractor Predicting: 470it [04:58,  1.63it/s]Extractor Predicting: 471it [04:58,  1.60it/s]Extractor Predicting: 472it [04:59,  1.61it/s]Extractor Predicting: 473it [05:00,  1.60it/s]Extractor Predicting: 474it [05:00,  1.56it/s]Extractor Predicting: 475it [05:01,  1.56it/s]Extractor Predicting: 476it [05:02,  1.59it/s]Extractor Predicting: 477it [05:02,  1.59it/s]Extractor Predicting: 478it [05:03,  1.59it/s]Extractor Predicting: 479it [05:04,  1.48it/s]Extractor Predicting: 480it [05:04,  1.47it/s]Extractor Predicting: 481it [05:05,  1.47it/s]Extractor Predicting: 482it [05:06,  1.48it/s]Extractor Predicting: 483it [05:06,  1.47it/s]Extractor Predicting: 484it [05:07,  1.45it/s]Extractor Predicting: 485it [05:08,  1.46it/s]Extractor Predicting: 486it [05:08,  1.50it/s]Extractor Predicting: 487it [05:09,  1.53it/s]Extractor Predicting: 488it [05:10,  1.51it/s]Extractor Predicting: 489it [05:10,  1.49it/s]Extractor Predicting: 490it [05:11,  1.47it/s]Extractor Predicting: 491it [05:12,  1.48it/s]Extractor Predicting: 492it [05:12,  1.48it/s]Extractor Predicting: 493it [05:13,  1.50it/s]Extractor Predicting: 494it [05:14,  1.44it/s]Extractor Predicting: 495it [05:15,  1.46it/s]Extractor Predicting: 496it [05:15,  1.48it/s]Extractor Predicting: 497it [05:16,  1.49it/s]Extractor Predicting: 498it [05:17,  1.50it/s]Extractor Predicting: 499it [05:17,  1.42it/s]Extractor Predicting: 500it [05:18,  1.46it/s]Extractor Predicting: 501it [05:19,  1.49it/s]Extractor Predicting: 502it [05:19,  1.47it/s]Extractor Predicting: 503it [05:20,  1.47it/s]Extractor Predicting: 504it [05:21,  1.39it/s]Extractor Predicting: 505it [05:22,  1.23it/s]Extractor Predicting: 506it [05:23,  1.28it/s]Extractor Predicting: 507it [05:23,  1.36it/s]Extractor Predicting: 508it [05:24,  1.56it/s]Extractor Predicting: 508it [05:24,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:17,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:17,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:17,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:17,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:17,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:11:19,077 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:11:19,078 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:11:19,520 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:11:20,745 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:11:20,771 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:23,477 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:23,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:23,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:23,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:23,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:11:24,488 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:11:24,561 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:11:25,014 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:11:25,314 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:11:25,314 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.30559345156889495,
  "recall": 0.018389294803382316,
  "score": 0.034691032987455475,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.38it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:10,  1.32it/s]Extractor Predicting: 16it [00:11,  1.37it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.51it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.52it/s]Extractor Predicting: 24it [00:16,  1.56it/s]Extractor Predicting: 25it [00:17,  1.40it/s]Extractor Predicting: 26it [00:17,  1.43it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:20,  1.51it/s]Extractor Predicting: 31it [00:20,  1.57it/s]Extractor Predicting: 32it [00:21,  1.62it/s]Extractor Predicting: 33it [00:22,  1.64it/s]Extractor Predicting: 34it [00:22,  1.65it/s]Extractor Predicting: 35it [00:23,  1.64it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.59it/s]Extractor Predicting: 39it [00:25,  1.61it/s]Extractor Predicting: 40it [00:26,  1.63it/s]Extractor Predicting: 41it [00:27,  1.57it/s]Extractor Predicting: 42it [00:27,  1.63it/s]Extractor Predicting: 43it [00:28,  1.65it/s]Extractor Predicting: 44it [00:28,  1.70it/s]Extractor Predicting: 45it [00:29,  1.68it/s]Extractor Predicting: 46it [00:29,  1.68it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:32,  1.58it/s]Extractor Predicting: 50it [00:32,  1.58it/s]Extractor Predicting: 51it [00:33,  1.58it/s]Extractor Predicting: 52it [00:33,  1.54it/s]Extractor Predicting: 53it [00:34,  1.57it/s]Extractor Predicting: 54it [00:35,  1.54it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:36,  1.61it/s]Extractor Predicting: 57it [00:37,  1.26it/s]Extractor Predicting: 58it [00:38,  1.35it/s]Extractor Predicting: 59it [00:38,  1.43it/s]Extractor Predicting: 60it [00:39,  1.46it/s]Extractor Predicting: 61it [00:40,  1.47it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.55it/s]Extractor Predicting: 64it [00:42,  1.46it/s]Extractor Predicting: 65it [00:42,  1.48it/s]Extractor Predicting: 66it [00:43,  1.47it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:44,  1.50it/s]Extractor Predicting: 69it [00:45,  1.50it/s]Extractor Predicting: 70it [00:46,  1.51it/s]Extractor Predicting: 71it [00:46,  1.47it/s]Extractor Predicting: 72it [00:47,  1.49it/s]Extractor Predicting: 73it [00:48,  1.53it/s]Extractor Predicting: 74it [00:48,  1.49it/s]Extractor Predicting: 75it [00:49,  1.72it/s]Extractor Predicting: 75it [00:49,  1.51it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:12:21,616 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:12:21,618 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:12:21,687 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:12:21,688 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:12:21,740 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:12:47,264 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:12:47,422 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:12:48,457 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:12:48,458 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:12:48,588 >> Didn't find file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:12:48,663 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:12:48,663 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:12:48,663 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:12:48,663 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:12:48,663 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:12:48,663 >> loading file outputs/wrapper/wiki/unseen_15_seed_3/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4731182795698925,
  "recall": 0.011083123425692695,
  "score": 0.02165887275412257,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_3/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:12:49,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:49,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:50,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:50,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:51,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:52,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:52,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:53,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:53,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:54,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:55,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:55,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:56,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:57,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:57,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:58,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:58,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:12:59,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:00,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:00,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:01,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<04:06, 12.95s/it][WARNING|generation_utils.py:914] 2023-08-28 21:13:02,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:02,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:03,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:03,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:04,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:05,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:05,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:06,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:06,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:07,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:08,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:08,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:09,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:09,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:10,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:10,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:11,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:12,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:12,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:13,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:14,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:14,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:15,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:15,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:16,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:27<04:08, 13.81s/it][WARNING|generation_utils.py:914] 2023-08-28 21:13:16,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:17,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:18,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:18,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:19,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:20,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:20,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:21,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:22,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:22,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:23,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:23,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:24,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:25,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:25,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:26,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:27,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:27,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:28,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:29,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:29,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:30,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:30,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:31,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:32,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:43<04:13, 14.90s/it][WARNING|generation_utils.py:914] 2023-08-28 21:13:32,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:33,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:33,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:34,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:35,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:36,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:36,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:37,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:37,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:38,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:39,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:40,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:40,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:41,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:41,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:42,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:43,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:43,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:44,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:45,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:45,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:46,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:57<03:53, 14.61s/it][WARNING|generation_utils.py:914] 2023-08-28 21:13:46,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:47,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:48,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:48,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:49,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:50,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:50,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:51,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:52,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:52,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:53,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:53,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:54,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:54,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:55,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:56,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:56,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:57,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:57,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:58,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:59,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:13:59,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:00,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:01,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:02,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:13<03:44, 14.96s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:02,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:03,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:03,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:04,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:04,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:05,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:06,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:06,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:07,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:07,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:08,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:09,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:09,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:10,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:11,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:11,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:12,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:12,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:13,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:13,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:14,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:15,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:15,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:16,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:27<03:26, 14.77s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:16,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:17,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:18,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:18,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:19,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:20,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:20,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:21,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:21,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:22,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:23,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:23,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:24,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:24,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:25,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:25,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:26,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:27,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:27,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:28,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:28,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:29,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:30,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:41<03:06, 14.37s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:30,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:31,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:31,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:32,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:32,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:33,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:34,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:34,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:35,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:36,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:36,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:37,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:38,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:38,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:39,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:39,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:40,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:41,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:41,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:42,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:42,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:43,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:44,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:44,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:55<02:53, 14.45s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:45,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:45,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:46,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:46,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:47,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:48,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:48,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:49,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:50,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:50,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:51,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:52,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:52,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:53,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:54,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:54,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:55,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:55,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:56,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:57,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:57,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:58,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:09<02:36, 14.25s/it][WARNING|generation_utils.py:914] 2023-08-28 21:14:58,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:14:59,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:00,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:00,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:01,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:02,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:02,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:03,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:04,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:05,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:05,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:06,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:07,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:07,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:08,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:09,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:09,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:10,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:10,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:12,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:12,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:13,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:13,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:25<02:26, 14.66s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:14,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:15,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:15,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:16,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:17,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:17,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:18,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:18,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:19,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:20,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:20,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:21,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:21,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:22,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:23,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:23,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:24,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:24,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:25,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:25,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:26,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:26,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:27,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:28,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:39<02:11, 14.56s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:28,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:29,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:30,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:30,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:31,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:31,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:32,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:33,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:33,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:34,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:34,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:35,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:35,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:36,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:36,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:37,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:38,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:39,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:39,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:40,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:40,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:41,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:52<01:52, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:42,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:42,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:43,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:43,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:44,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:44,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:45,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:46,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:46,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:47,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:48,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:48,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:49,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:49,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:50,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:50,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:51,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:52,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:52,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:53,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:54,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:54,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:55,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:55,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:07<01:39, 14.27s/it][WARNING|generation_utils.py:914] 2023-08-28 21:15:56,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:57,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:57,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:58,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:58,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:15:59,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:00,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:00,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:01,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:01,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:02,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:02,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:03,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:04,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:04,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:05,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:05,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:06,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:07,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:07,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:08,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:08,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:09,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:21<01:24, 14.15s/it][WARNING|generation_utils.py:914] 2023-08-28 21:16:10,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:10,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:11,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:12,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:12,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:13,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:13,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:14,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:14,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:15,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:16,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:16,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:17,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:17,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:18,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:18,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:19,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:20,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:20,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:21,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:21,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:33<01:07, 13.56s/it][WARNING|generation_utils.py:914] 2023-08-28 21:16:22,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:23,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:23,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:24,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:25,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:25,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:26,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:26,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:27,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:28,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:28,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:29,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:29,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:30,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:31,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:31,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:32,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:32,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:33,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:34,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:34,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:35,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:36,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:36,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:37,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:38,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:49<00:57, 14.29s/it][WARNING|generation_utils.py:914] 2023-08-28 21:16:38,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:39,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:39,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:40,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:41,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:41,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:42,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:42,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:43,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:44,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:44,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:45,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:45,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:46,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:47,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:47,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:48,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:48,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:49,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:50,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:50,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:01<00:41, 13.77s/it][WARNING|generation_utils.py:914] 2023-08-28 21:16:51,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:51,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:52,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:52,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:53,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:54,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:54,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:55,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:55,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:56,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:56,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:57,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:57,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:58,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:59,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:16:59,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:00,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:01,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:01,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:02,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:02,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:03,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:15<00:27, 13.64s/it][WARNING|generation_utils.py:914] 2023-08-28 21:17:04,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:05,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:05,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:06,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:07,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:08,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:08,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:09,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:09,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:10,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:11,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:12,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:12,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:13,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:14,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:14,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:15,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:15,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:16,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:17,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:18,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:18,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:19,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:30<00:14, 14.11s/it][WARNING|generation_utils.py:914] 2023-08-28 21:17:19,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:20,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:21,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:21,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:22,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:22,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:23,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:24,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:24,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:25,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:26,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:26,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:27,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:28,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:28,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:29,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:29,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:31,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:32,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:32,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:33,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:34,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:34,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:45<00:00, 14.53s/it]Generating: 100%|██████████| 20/20 [04:45<00:00, 14.30s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:17:53,024 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:17:53,174 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:17:53,174 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:17:53,174 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:17:53,174 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:17:54,573 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:17:54,574 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:17:55,448 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:17:56,835 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:17:57,409 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:18:01,186 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:18:01,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:18:01,365 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:18:01,365 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:18:01,365 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:18:02,142 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:18:02,144 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:18:03,317 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:18:04,284 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:18:04,467 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9151785714285714, 'errors': {''}}
['Relation : product or material produced . Context : Later in the year ( 1840 ) , the company purchased the French railway station of Port de Leipzig , in which it made railway furniture , and a number of other pieces , including locomotive s and locomotive parts . Head Entity : trains , Tail Entity : locomotive .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 137, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 419, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 490, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 613, 'raw': 800}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.76625, 'errors': {'', "('Renault R8', 'product or material produced', '', 'It is the latest installment in the Renault R8 class , the only generation of the R8 variant that has since been produced worldwide .')", "('footwear packaging', 'product or material produced', '', 'Most manufacturers of clothing and footwear produce hand sanitizer s as part of their footwear packaging and may also use microfibre cloth .')", "('video game console', 'product or material produced', '', 'It was used in the manufacture of television , film , and other media products , including television sets and video game console s .')", "('painters', 'product or material produced', '', 'In the late 1960s , he studied with a group of German painters Emil Schmelvig ( d.')"}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 623, 'raw': 800}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.77875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : student .', 'success_rate': 0.8664772727272727, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 364, 'raw': 480}
{'target': 600, 'success': 385, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 537, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : winner .', 'success_rate': 0.76875, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 606, 'raw': 768}
{'prompt': 'Relation : conflict .', 'success_rate': 0.7890625, 'errors': {''}}
['Relation : continent . Context : The Cimarron ( also known as the Puy Puy Mountains ) are a series of mountain ranges located in the mountains of the Andes , including Mexico , Colombia , Peru and Central America . Head Entity : Chile , Tail Entity : Mexico .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : continent .', 'success_rate': 0.8383152173913043, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 565, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.8098958333333334, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8192934782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 583, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : given name .', 'success_rate': 0.7955729166666666, 'errors': {'', "('2015 Miss Universe', 'given name', '', 'She won the inaugural Miss Universe s 2015 competition at the 2015 Miss Universe Brazil in Rio de Janeiro on March 8 , 2015 .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('One Life to Live', 'given name', '', 'She was born in London , England , and educated at the Royal Academy where she earned a BAFTA nomination for her performance in the film One Life to Live and a Lifetime Achievement Award for her performance in Lonesome Dove .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.8522727272727273, 'errors': {'', "('Never Let Me Go', 'lyrics by', '', 'She followed this up with her sixth studio album , Never Let Me Go , which is now a cult classic on the Billboard R&B Top 40 .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : movement .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : owned by . Context : Later in 2008 , the county became a part of a deal between D.W. Griffiths and MGM to build Las Vegas for MGM in May 2009 , and a portion of Bodegas was added following the 2010 purchase of the MGM Las Vegas Resort and Casino in Las Vegas , Nevada . Head Entity : MGM Casino , Tail Entity : Bodegas .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8519021739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : performer .', 'success_rate': 0.9166666666666666, 'errors': {'', "('Born Like A Virgin', 'performer', '', 'The band debuted on the Billboard chart with hits Like A Mountain Home and Born Like A Virgin , both from 1999 to 2003 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 280, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 352, 'raw': 480}
{'target': 600, 'success': 376, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 473, 'raw': 640}
{'target': 600, 'success': 499, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 571, 'raw': 768}
{'target': 600, 'success': 596, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.7451923076923077, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8565340909090909, 'errors': {'', "('Finding Dory', 'publisher', '', 'In 2009 , it was released opposite the animated feature film of the same name , Finding Dory , and was the first live action version of B.')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : record label .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8179347826086957, 'errors': {'', "('Kevin Hart', 'replaces', '', 'The first season saw the arrival of Kevin Hart , and the return of the most sought after young power forward in the league , the Portland Trail Blazers .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/1_ext.jsonl'}}
estimate vocab size: 15253
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15353, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.68it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.58it/s]Extractor Estimating: 5it [00:03,  1.50it/s]Extractor Estimating: 6it [00:04,  1.17it/s]Extractor Estimating: 7it [00:05,  1.27it/s]Extractor Estimating: 8it [00:05,  1.37it/s]Extractor Estimating: 9it [00:06,  1.47it/s]Extractor Estimating: 10it [00:07,  1.30it/s]Extractor Estimating: 11it [00:07,  1.37it/s]Extractor Estimating: 12it [00:08,  1.45it/s]Extractor Estimating: 13it [00:09,  1.52it/s]Extractor Estimating: 14it [00:09,  1.54it/s]Extractor Estimating: 15it [00:10,  1.39it/s]Extractor Estimating: 16it [00:11,  1.47it/s]Extractor Estimating: 17it [00:11,  1.51it/s]Extractor Estimating: 18it [00:12,  1.55it/s]Extractor Estimating: 19it [00:13,  1.59it/s]Extractor Estimating: 20it [00:14,  1.18it/s]Extractor Estimating: 21it [00:14,  1.30it/s]Extractor Estimating: 22it [00:15,  1.36it/s]Extractor Estimating: 23it [00:16,  1.44it/s]Extractor Estimating: 24it [00:16,  1.51it/s]Extractor Estimating: 25it [00:17,  1.55it/s]Extractor Estimating: 26it [00:18,  1.54it/s]Extractor Estimating: 27it [00:18,  1.40it/s]Extractor Estimating: 28it [00:19,  1.51it/s]Extractor Estimating: 29it [00:19,  1.62it/s]Extractor Estimating: 30it [00:20,  1.67it/s]Extractor Estimating: 31it [00:21,  1.72it/s]Extractor Estimating: 32it [00:21,  1.58it/s]Extractor Estimating: 33it [00:22,  1.66it/s]Extractor Estimating: 34it [00:22,  1.70it/s]Extractor Estimating: 35it [00:23,  1.70it/s]Extractor Estimating: 36it [00:24,  1.70it/s]Extractor Estimating: 37it [00:25,  1.42it/s]Extractor Estimating: 38it [00:25,  1.49it/s]Extractor Estimating: 39it [00:26,  1.59it/s]Extractor Estimating: 40it [00:26,  1.66it/s]Extractor Estimating: 41it [00:27,  1.69it/s]Extractor Estimating: 42it [00:28,  1.57it/s]Extractor Estimating: 43it [00:28,  1.62it/s]Extractor Estimating: 44it [00:29,  1.71it/s]Extractor Estimating: 45it [00:29,  1.71it/s]Extractor Estimating: 46it [00:30,  1.73it/s]Extractor Estimating: 47it [00:30,  1.71it/s]Extractor Estimating: 48it [00:32,  1.33it/s]Extractor Estimating: 49it [00:32,  1.45it/s]Extractor Estimating: 50it [00:33,  1.55it/s]Extractor Estimating: 51it [00:33,  1.60it/s]Extractor Estimating: 52it [00:34,  1.60it/s]Extractor Estimating: 53it [00:35,  1.25it/s]Extractor Estimating: 54it [00:36,  1.36it/s]Extractor Estimating: 55it [00:36,  1.43it/s]Extractor Estimating: 56it [00:37,  1.52it/s]Extractor Estimating: 57it [00:37,  1.55it/s]Extractor Estimating: 58it [00:38,  1.40it/s]Extractor Estimating: 59it [00:39,  1.47it/s]Extractor Estimating: 60it [00:39,  1.56it/s]Extractor Estimating: 61it [00:40,  1.63it/s]Extractor Estimating: 62it [00:41,  1.62it/s]Extractor Estimating: 63it [00:42,  1.35it/s]Extractor Estimating: 64it [00:42,  1.45it/s]Extractor Estimating: 65it [00:43,  1.50it/s]Extractor Estimating: 66it [00:43,  1.55it/s]Extractor Estimating: 67it [00:44,  1.64it/s]Extractor Estimating: 68it [00:47,  1.28s/it]Extractor Estimating: 69it [00:47,  1.07s/it]Extractor Estimating: 70it [00:48,  1.08it/s]Extractor Estimating: 71it [00:49,  1.08it/s]Extractor Estimating: 72it [00:49,  1.21it/s]Extractor Estimating: 73it [00:50,  1.33it/s]Extractor Estimating: 74it [00:51,  1.43it/s]Extractor Estimating: 75it [00:51,  1.46it/s]Extractor Estimating: 76it [00:52,  1.39it/s]Extractor Estimating: 77it [00:53,  1.49it/s]Extractor Estimating: 78it [00:53,  1.52it/s]Extractor Estimating: 79it [00:54,  1.56it/s]Extractor Estimating: 80it [00:54,  1.59it/s]Extractor Estimating: 81it [00:55,  1.42it/s]Extractor Estimating: 82it [00:56,  1.48it/s]Extractor Estimating: 83it [00:57,  1.54it/s]Extractor Estimating: 84it [00:57,  1.56it/s]Extractor Estimating: 85it [00:58,  1.59it/s]Extractor Estimating: 86it [00:58,  1.59it/s]Extractor Estimating: 87it [00:59,  1.46it/s]Extractor Estimating: 88it [01:00,  1.50it/s]Extractor Estimating: 89it [01:00,  1.53it/s]Extractor Estimating: 90it [01:01,  1.58it/s]Extractor Estimating: 91it [01:02,  1.29it/s]Extractor Estimating: 92it [01:03,  1.39it/s]Extractor Estimating: 93it [01:03,  1.44it/s]Extractor Estimating: 94it [01:04,  1.50it/s]Extractor Estimating: 95it [01:05,  1.57it/s]Extractor Estimating: 96it [01:05,  1.52it/s]Extractor Estimating: 97it [01:06,  1.56it/s]Extractor Estimating: 98it [01:06,  1.62it/s]Extractor Estimating: 99it [01:07,  1.59it/s]Extractor Estimating: 100it [01:08,  1.54it/s]Extractor Estimating: 101it [01:08,  1.47it/s]Extractor Estimating: 102it [01:09,  1.55it/s]Extractor Estimating: 103it [01:10,  1.61it/s]Extractor Estimating: 104it [01:10,  1.65it/s]Extractor Estimating: 105it [01:11,  1.65it/s]Extractor Estimating: 106it [01:12,  1.34it/s]Extractor Estimating: 107it [01:12,  1.41it/s]Extractor Estimating: 108it [01:13,  1.45it/s]Extractor Estimating: 109it [01:14,  1.52it/s]Extractor Estimating: 110it [01:14,  1.58it/s]Extractor Estimating: 111it [01:15,  1.41it/s]Extractor Estimating: 112it [01:16,  1.43it/s]Extractor Estimating: 113it [01:16,  1.50it/s]Extractor Estimating: 114it [01:17,  1.53it/s]Extractor Estimating: 115it [01:18,  1.59it/s]Extractor Estimating: 116it [01:18,  1.58it/s]Extractor Estimating: 117it [01:19,  1.60it/s]Extractor Estimating: 118it [01:20,  1.56it/s]Extractor Estimating: 119it [01:20,  1.61it/s]Extractor Estimating: 120it [01:21,  1.67it/s]Extractor Estimating: 121it [01:21,  1.67it/s]Extractor Estimating: 122it [01:22,  1.68it/s]Extractor Estimating: 123it [01:22,  1.66it/s]Extractor Estimating: 124it [01:23,  1.64it/s]Extractor Estimating: 125it [01:24,  1.60it/s]Extractor Estimating: 126it [01:24,  1.61it/s]Extractor Estimating: 127it [01:25,  1.60it/s]Extractor Estimating: 128it [01:26,  1.60it/s]Extractor Estimating: 129it [01:27,  1.40it/s]Extractor Estimating: 130it [01:27,  1.52it/s]Extractor Estimating: 131it [01:28,  1.56it/s]Extractor Estimating: 132it [01:28,  1.60it/s]Extractor Estimating: 133it [01:29,  1.68it/s]Extractor Estimating: 134it [01:30,  1.55it/s]Extractor Estimating: 135it [01:30,  1.60it/s]Extractor Estimating: 136it [01:31,  1.63it/s]Extractor Estimating: 137it [01:31,  1.64it/s]Extractor Estimating: 138it [01:32,  1.68it/s]Extractor Estimating: 139it [01:33,  1.41it/s]Extractor Estimating: 140it [01:33,  1.51it/s]Extractor Estimating: 141it [01:34,  1.55it/s]Extractor Estimating: 142it [01:35,  1.60it/s]Extractor Estimating: 143it [01:35,  1.59it/s]Extractor Estimating: 144it [01:36,  1.62it/s]Extractor Estimating: 145it [01:36,  1.65it/s]Extractor Estimating: 146it [01:37,  1.63it/s]Extractor Estimating: 147it [01:38,  1.63it/s]Extractor Estimating: 148it [01:38,  1.62it/s]Extractor Estimating: 149it [01:39,  1.36it/s]Extractor Estimating: 150it [01:40,  1.42it/s]Extractor Estimating: 151it [01:41,  1.52it/s]Extractor Estimating: 152it [01:41,  1.61it/s]Extractor Estimating: 153it [01:42,  1.63it/s]Extractor Estimating: 154it [01:42,  1.49it/s]Extractor Estimating: 155it [01:43,  1.60it/s]Extractor Estimating: 156it [01:43,  1.70it/s]Extractor Estimating: 157it [01:44,  1.72it/s]Extractor Estimating: 158it [01:45,  1.77it/s]Extractor Estimating: 159it [01:45,  1.82it/s]Extractor Estimating: 160it [01:46,  1.53it/s]Extractor Estimating: 161it [01:47,  1.61it/s]Extractor Estimating: 162it [01:48,  1.32it/s]Extractor Estimating: 163it [01:48,  1.42it/s]Extractor Estimating: 164it [01:49,  1.51it/s]Extractor Estimating: 165it [01:49,  1.63it/s]Extractor Estimating: 166it [01:50,  1.71it/s]Extractor Estimating: 167it [01:51,  1.34it/s]Extractor Estimating: 168it [01:51,  1.45it/s]Extractor Estimating: 169it [01:52,  1.51it/s]Extractor Estimating: 170it [01:53,  1.43it/s]Extractor Estimating: 171it [01:53,  1.48it/s]Extractor Estimating: 172it [01:54,  1.57it/s]Extractor Estimating: 173it [01:55,  1.63it/s]Extractor Estimating: 174it [01:55,  1.72it/s]Extractor Estimating: 175it [01:56,  1.78it/s]Extractor Estimating: 176it [01:56,  1.73it/s]Extractor Estimating: 177it [01:57,  1.35it/s]Extractor Estimating: 178it [01:58,  1.45it/s]Extractor Estimating: 179it [01:58,  1.50it/s]Extractor Estimating: 180it [01:59,  1.55it/s]Extractor Estimating: 181it [02:00,  1.54it/s]Extractor Estimating: 182it [02:01,  1.25it/s]Extractor Estimating: 183it [02:01,  1.36it/s]Extractor Estimating: 184it [02:02,  1.43it/s]Extractor Estimating: 185it [02:03,  1.46it/s]Extractor Estimating: 186it [02:04,  1.30it/s]Extractor Estimating: 187it [02:04,  1.37it/s]Extractor Estimating: 188it [02:05,  1.45it/s]Extractor Estimating: 189it [02:06,  1.48it/s]Extractor Estimating: 190it [02:06,  1.53it/s]Extractor Estimating: 191it [02:07,  1.43it/s]Extractor Estimating: 192it [02:08,  1.49it/s]Extractor Estimating: 193it [02:08,  1.55it/s]Extractor Estimating: 194it [02:09,  1.60it/s]Extractor Estimating: 195it [02:09,  1.60it/s]Extractor Estimating: 196it [02:10,  1.47it/s]Extractor Estimating: 197it [02:11,  1.54it/s]Extractor Estimating: 198it [02:11,  1.54it/s]Extractor Estimating: 199it [02:12,  1.61it/s]Extractor Estimating: 200it [02:13,  1.62it/s]Extractor Estimating: 201it [02:13,  1.62it/s]Extractor Estimating: 202it [02:14,  1.67it/s]Extractor Estimating: 203it [02:14,  1.63it/s]Extractor Estimating: 204it [02:15,  1.59it/s]Extractor Estimating: 205it [02:16,  1.61it/s]Extractor Estimating: 206it [02:16,  1.49it/s]Extractor Estimating: 207it [02:17,  1.50it/s]Extractor Estimating: 208it [02:18,  1.51it/s]Extractor Estimating: 209it [02:18,  1.56it/s]Extractor Estimating: 210it [02:19,  1.58it/s]Extractor Estimating: 211it [02:20,  1.60it/s]Extractor Estimating: 212it [02:20,  1.49it/s]Extractor Estimating: 213it [02:21,  1.53it/s]Extractor Estimating: 214it [02:22,  1.50it/s]Extractor Estimating: 215it [02:22,  1.58it/s]Extractor Estimating: 216it [02:23,  1.55it/s]Extractor Estimating: 217it [02:24,  1.57it/s]Extractor Estimating: 218it [02:24,  1.50it/s]Extractor Estimating: 219it [02:25,  1.55it/s]Extractor Estimating: 220it [02:25,  1.60it/s]Extractor Estimating: 221it [02:26,  1.64it/s]Extractor Estimating: 222it [02:27,  1.65it/s]Extractor Estimating: 223it [02:27,  1.63it/s]Extractor Estimating: 224it [02:28,  1.61it/s]Extractor Estimating: 225it [02:28,  1.63it/s]Extractor Estimating: 226it [02:29,  1.61it/s]Extractor Estimating: 227it [02:30,  1.64it/s]Extractor Estimating: 228it [02:30,  1.58it/s]Extractor Estimating: 229it [02:31,  1.57it/s]Extractor Estimating: 230it [02:32,  1.61it/s]Extractor Estimating: 231it [02:32,  1.59it/s]Extractor Estimating: 232it [02:33,  1.63it/s]Extractor Estimating: 233it [02:34,  1.49it/s]Extractor Estimating: 234it [02:34,  1.53it/s]Extractor Estimating: 235it [02:35,  1.53it/s]Extractor Estimating: 236it [02:36,  1.56it/s]Extractor Estimating: 237it [02:36,  1.57it/s]Extractor Estimating: 238it [02:37,  1.49it/s]Extractor Estimating: 239it [02:37,  1.55it/s]Extractor Estimating: 240it [02:38,  1.52it/s]Extractor Estimating: 241it [02:39,  1.60it/s]Extractor Estimating: 242it [02:39,  1.62it/s]Extractor Estimating: 243it [02:40,  1.44it/s]Extractor Estimating: 244it [02:41,  1.52it/s]Extractor Estimating: 245it [02:41,  1.55it/s]Extractor Estimating: 246it [02:42,  1.58it/s]Extractor Estimating: 247it [02:43,  1.63it/s]Extractor Estimating: 248it [02:43,  1.45it/s]Extractor Estimating: 249it [02:44,  1.51it/s]Extractor Estimating: 250it [02:45,  1.54it/s]Extractor Estimating: 251it [02:45,  1.57it/s]Extractor Estimating: 252it [02:46,  1.58it/s]Extractor Estimating: 253it [02:47,  1.45it/s]Extractor Estimating: 254it [02:47,  1.42it/s]Extractor Estimating: 255it [02:48,  1.50it/s]Extractor Estimating: 256it [02:49,  1.11it/s]Extractor Estimating: 257it [02:50,  1.24it/s]Extractor Estimating: 258it [02:51,  1.34it/s]Extractor Estimating: 259it [02:51,  1.36it/s]Extractor Estimating: 260it [02:52,  1.38it/s]Extractor Estimating: 261it [02:53,  1.44it/s]Extractor Estimating: 262it [02:53,  1.44it/s]Extractor Estimating: 263it [02:54,  1.47it/s]Extractor Estimating: 264it [02:55,  1.48it/s]Extractor Estimating: 265it [02:55,  1.46it/s]Extractor Estimating: 266it [02:56,  1.52it/s]Extractor Estimating: 267it [02:57,  1.60it/s]Extractor Estimating: 268it [02:57,  1.59it/s]Extractor Estimating: 269it [02:58,  1.55it/s]Extractor Estimating: 270it [02:59,  1.33it/s]Extractor Estimating: 271it [02:59,  1.40it/s]Extractor Estimating: 272it [03:00,  1.46it/s]Extractor Estimating: 273it [03:01,  1.53it/s]Extractor Estimating: 274it [03:01,  1.56it/s]Extractor Estimating: 275it [03:03,  1.20it/s]Extractor Estimating: 276it [03:03,  1.27it/s]Extractor Estimating: 277it [03:04,  1.36it/s]Extractor Estimating: 278it [03:05,  1.39it/s]Extractor Estimating: 279it [03:05,  1.28it/s]Extractor Estimating: 280it [03:06,  1.35it/s]Extractor Estimating: 281it [03:07,  1.41it/s]Extractor Estimating: 282it [03:07,  1.49it/s]Extractor Estimating: 283it [03:08,  1.51it/s]Extractor Estimating: 284it [03:09,  1.33it/s]Extractor Estimating: 285it [03:10,  1.43it/s]Extractor Estimating: 286it [03:10,  1.48it/s]Extractor Estimating: 287it [03:11,  1.50it/s]Extractor Estimating: 288it [03:11,  1.53it/s]Extractor Estimating: 289it [03:13,  1.20it/s]Extractor Estimating: 290it [03:13,  1.31it/s]Extractor Estimating: 291it [03:14,  1.42it/s]Extractor Estimating: 292it [03:15,  1.44it/s]Extractor Estimating: 293it [03:16,  1.17it/s]Extractor Estimating: 294it [03:16,  1.26it/s]Extractor Estimating: 295it [03:17,  1.32it/s]Extractor Estimating: 296it [03:18,  1.38it/s]Extractor Estimating: 297it [03:19,  1.22it/s]Extractor Estimating: 298it [03:19,  1.29it/s]Extractor Estimating: 299it [03:20,  1.40it/s]Extractor Estimating: 300it [03:21,  1.46it/s]Extractor Estimating: 301it [03:21,  1.55it/s]Extractor Estimating: 302it [03:22,  1.51it/s]Extractor Estimating: 303it [03:22,  1.55it/s]Extractor Estimating: 304it [03:23,  1.58it/s]Extractor Estimating: 305it [03:24,  1.59it/s]Extractor Estimating: 306it [03:24,  1.47it/s]Extractor Estimating: 307it [03:25,  1.54it/s]Extractor Estimating: 308it [03:26,  1.58it/s]Extractor Estimating: 309it [03:26,  1.62it/s]Extractor Estimating: 310it [03:27,  1.64it/s]Extractor Estimating: 311it [03:27,  1.63it/s]Extractor Estimating: 312it [03:28,  1.65it/s]Extractor Estimating: 313it [03:29,  1.65it/s]Extractor Estimating: 314it [03:29,  1.67it/s]Extractor Estimating: 315it [03:30,  1.47it/s]Extractor Estimating: 316it [03:31,  1.26it/s]Extractor Estimating: 317it [03:32,  1.36it/s]Extractor Estimating: 318it [03:32,  1.44it/s]Extractor Estimating: 319it [03:33,  1.51it/s]Extractor Estimating: 320it [03:34,  1.54it/s]Extractor Estimating: 321it [03:34,  1.43it/s]Extractor Estimating: 322it [03:35,  1.49it/s]Extractor Estimating: 323it [03:36,  1.51it/s]Extractor Estimating: 324it [03:36,  1.58it/s]Extractor Estimating: 325it [03:37,  1.60it/s]Extractor Estimating: 326it [03:37,  1.65it/s]Extractor Estimating: 327it [03:38,  1.67it/s]Extractor Estimating: 328it [03:39,  1.70it/s]Extractor Estimating: 329it [03:39,  1.70it/s]Extractor Estimating: 330it [03:40,  1.69it/s]Extractor Estimating: 331it [03:40,  1.69it/s]Extractor Estimating: 332it [03:41,  1.49it/s]Extractor Estimating: 333it [03:42,  1.54it/s]Extractor Estimating: 334it [03:42,  1.60it/s]Extractor Estimating: 335it [03:43,  1.62it/s]Extractor Estimating: 336it [03:43,  1.67it/s]Extractor Estimating: 337it [03:44,  1.52it/s]Extractor Estimating: 338it [03:45,  1.57it/s]Extractor Estimating: 339it [03:45,  1.60it/s]Extractor Estimating: 340it [03:46,  1.60it/s]Extractor Estimating: 341it [03:47,  1.62it/s]Extractor Estimating: 342it [03:48,  1.41it/s]Extractor Estimating: 343it [03:48,  1.49it/s]Extractor Estimating: 344it [03:49,  1.54it/s]Extractor Estimating: 345it [03:49,  1.62it/s]Extractor Estimating: 346it [03:50,  1.67it/s]Extractor Estimating: 347it [03:51,  1.35it/s]Extractor Estimating: 348it [03:52,  1.45it/s]Extractor Estimating: 349it [03:52,  1.54it/s]Extractor Estimating: 350it [03:53,  1.56it/s]Extractor Estimating: 351it [03:53,  1.57it/s]Extractor Estimating: 352it [03:54,  1.50it/s]Extractor Estimating: 353it [03:55,  1.54it/s]Extractor Estimating: 354it [03:55,  1.56it/s]Extractor Estimating: 355it [03:56,  1.58it/s]Extractor Estimating: 356it [03:56,  1.63it/s]Extractor Estimating: 357it [03:57,  1.69it/s]Extractor Estimating: 358it [03:58,  1.72it/s]Extractor Estimating: 359it [03:58,  1.59it/s]Extractor Estimating: 360it [03:59,  1.57it/s]Extractor Estimating: 361it [04:00,  1.57it/s]Extractor Estimating: 362it [04:00,  1.58it/s]Extractor Estimating: 363it [04:01,  1.58it/s]Extractor Estimating: 364it [04:02,  1.54it/s]Extractor Estimating: 365it [04:02,  1.58it/s]Extractor Estimating: 366it [04:03,  1.62it/s]Extractor Estimating: 367it [04:03,  1.63it/s]Extractor Estimating: 368it [04:04,  1.64it/s]Extractor Estimating: 369it [04:05,  1.55it/s]Extractor Estimating: 370it [04:05,  1.58it/s]Extractor Estimating: 371it [04:06,  1.55it/s]Extractor Estimating: 372it [04:07,  1.53it/s]Extractor Estimating: 373it [04:07,  1.56it/s]Extractor Estimating: 374it [04:08,  1.54it/s]Extractor Estimating: 375it [04:09,  1.54it/s]Extractor Estimating: 376it [04:09,  1.54it/s]Extractor Estimating: 377it [04:10,  1.53it/s]Extractor Estimating: 378it [04:10,  1.56it/s]Extractor Estimating: 379it [04:11,  1.49it/s]Extractor Estimating: 380it [04:12,  1.54it/s]Extractor Estimating: 381it [04:12,  1.59it/s]Extractor Estimating: 382it [04:13,  1.61it/s]Extractor Estimating: 383it [04:14,  1.58it/s]Extractor Estimating: 384it [04:14,  1.48it/s]Extractor Estimating: 385it [04:15,  1.49it/s]Extractor Estimating: 386it [04:16,  1.53it/s]Extractor Estimating: 387it [04:17,  1.40it/s]Extractor Estimating: 388it [04:17,  1.47it/s]Extractor Estimating: 389it [04:18,  1.52it/s]Extractor Estimating: 390it [04:18,  1.54it/s]Extractor Estimating: 391it [04:19,  1.54it/s]Extractor Estimating: 392it [04:20,  1.57it/s]Extractor Estimating: 393it [04:20,  1.57it/s]Extractor Estimating: 394it [04:21,  1.48it/s]Extractor Estimating: 395it [04:22,  1.52it/s]Extractor Estimating: 396it [04:22,  1.54it/s]Extractor Estimating: 397it [04:23,  1.56it/s]Extractor Estimating: 398it [04:24,  1.55it/s]Extractor Estimating: 399it [04:25,  1.29it/s]Extractor Estimating: 400it [04:25,  1.34it/s]Extractor Estimating: 401it [04:26,  1.42it/s]Extractor Estimating: 402it [04:27,  1.48it/s]Extractor Estimating: 403it [04:27,  1.53it/s]Extractor Estimating: 404it [04:28,  1.57it/s]Extractor Estimating: 405it [04:28,  1.59it/s]Extractor Estimating: 406it [04:29,  1.52it/s]Extractor Estimating: 407it [04:30,  1.58it/s]Extractor Estimating: 408it [04:30,  1.60it/s]Extractor Estimating: 409it [04:31,  1.63it/s]Extractor Estimating: 410it [04:32,  1.54it/s]Extractor Estimating: 411it [04:32,  1.58it/s]Extractor Estimating: 412it [04:33,  1.60it/s]Extractor Estimating: 413it [04:33,  1.62it/s]Extractor Estimating: 414it [04:34,  1.62it/s]Extractor Estimating: 415it [04:35,  1.60it/s]Extractor Estimating: 416it [04:36,  1.43it/s]Extractor Estimating: 417it [04:36,  1.49it/s]Extractor Estimating: 418it [04:37,  1.53it/s]Extractor Estimating: 419it [04:37,  1.56it/s]Extractor Estimating: 420it [04:38,  1.61it/s]Extractor Estimating: 421it [04:39,  1.64it/s]Extractor Estimating: 422it [04:39,  1.57it/s]Extractor Estimating: 423it [04:40,  1.57it/s]Extractor Estimating: 424it [04:40,  1.60it/s]Extractor Estimating: 425it [04:41,  1.62it/s]Extractor Estimating: 426it [04:42,  1.69it/s]Extractor Estimating: 427it [04:42,  1.62it/s]Extractor Estimating: 428it [04:43,  1.61it/s]Extractor Estimating: 429it [04:44,  1.55it/s]Extractor Estimating: 430it [04:44,  1.60it/s]Extractor Estimating: 431it [04:45,  1.62it/s]Extractor Estimating: 432it [04:46,  1.43it/s]Extractor Estimating: 433it [04:46,  1.50it/s]Extractor Estimating: 434it [04:47,  1.55it/s]Extractor Estimating: 435it [04:47,  1.57it/s]Extractor Estimating: 436it [04:48,  1.60it/s]Extractor Estimating: 437it [04:49,  1.36it/s]Extractor Estimating: 438it [04:50,  1.43it/s]Extractor Estimating: 439it [04:50,  1.47it/s]Extractor Estimating: 440it [04:51,  1.52it/s]Extractor Estimating: 441it [04:52,  1.53it/s]Extractor Estimating: 442it [04:53,  1.32it/s]Extractor Estimating: 443it [04:53,  1.40it/s]Extractor Estimating: 444it [04:54,  1.43it/s]Extractor Estimating: 445it [04:54,  1.49it/s]Extractor Estimating: 446it [04:55,  1.53it/s]Extractor Estimating: 447it [04:56,  1.36it/s]Extractor Estimating: 448it [04:57,  1.44it/s]Extractor Estimating: 449it [04:57,  1.50it/s]Extractor Estimating: 450it [04:58,  1.50it/s]Extractor Estimating: 451it [04:59,  1.47it/s]Extractor Estimating: 452it [04:59,  1.38it/s]Extractor Estimating: 453it [05:00,  1.42it/s]Extractor Estimating: 454it [05:01,  1.44it/s]Extractor Estimating: 455it [05:01,  1.51it/s]Extractor Estimating: 456it [05:02,  1.53it/s]Extractor Estimating: 457it [05:03,  1.39it/s]Extractor Estimating: 458it [05:03,  1.42it/s]Extractor Estimating: 459it [05:04,  1.43it/s]Extractor Estimating: 460it [05:05,  1.51it/s]Extractor Estimating: 461it [05:05,  1.52it/s]Extractor Estimating: 462it [05:06,  1.50it/s]Extractor Estimating: 463it [05:07,  1.28it/s]Extractor Estimating: 464it [05:08,  1.36it/s]Extractor Estimating: 465it [05:08,  1.45it/s]Extractor Estimating: 466it [05:09,  1.34it/s]Extractor Estimating: 467it [05:10,  1.34it/s]Extractor Estimating: 468it [05:11,  1.39it/s]Extractor Estimating: 469it [05:11,  1.42it/s]Extractor Estimating: 470it [05:12,  1.45it/s]Extractor Estimating: 471it [05:13,  1.50it/s]Extractor Estimating: 472it [05:13,  1.51it/s]Extractor Estimating: 473it [05:14,  1.55it/s]Extractor Estimating: 474it [05:14,  1.58it/s]Extractor Estimating: 475it [05:15,  1.57it/s]Extractor Estimating: 476it [05:16,  1.56it/s]Extractor Estimating: 477it [05:16,  1.55it/s]Extractor Estimating: 478it [05:17,  1.55it/s]Extractor Estimating: 479it [05:18,  1.49it/s]Extractor Estimating: 480it [05:18,  1.51it/s]Extractor Estimating: 481it [05:19,  1.59it/s]Extractor Estimating: 482it [05:20,  1.44it/s]Extractor Estimating: 483it [05:20,  1.50it/s]Extractor Estimating: 484it [05:21,  1.55it/s]Extractor Estimating: 485it [05:22,  1.56it/s]Extractor Estimating: 486it [05:22,  1.56it/s]Extractor Estimating: 487it [05:23,  1.43it/s]Extractor Estimating: 488it [05:24,  1.46it/s]Extractor Estimating: 489it [05:24,  1.50it/s]Extractor Estimating: 490it [05:25,  1.56it/s]Extractor Estimating: 491it [05:26,  1.58it/s]Extractor Estimating: 492it [05:26,  1.54it/s]Extractor Estimating: 493it [05:27,  1.59it/s]Extractor Estimating: 494it [05:27,  1.63it/s]Extractor Estimating: 495it [05:28,  1.62it/s]Extractor Estimating: 496it [05:29,  1.60it/s]Extractor Estimating: 497it [05:29,  1.54it/s]Extractor Estimating: 498it [05:30,  1.57it/s]Extractor Estimating: 499it [05:31,  1.62it/s]Extractor Estimating: 500it [05:31,  1.71it/s]Extractor Estimating: 500it [05:31,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:42,272 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:42,469 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:42,469 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:42,469 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:42,469 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:24:43,457 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:24:43,459 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:24:44,101 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:24:45,188 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:24:45,188 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:48,845 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:49,261 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:49,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:49,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:24:49,262 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:24:50,104 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:24:50,105 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:24:50,769 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:24:50,945 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:24:50,945 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:17:51,442 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:17:51,475 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9921 mean pseudo reward: 0.9284647276203354
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 29441
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 29541, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=29541, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.938, loss:866.9562
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.937, loss:793.4686
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.931, loss:796.1882
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.930, loss:794.4277
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 86, avg_time 0.949, loss:778.0593
>> valid entity prec:0.5100, rec:0.3620, f1:0.4234
>> valid relation prec:0.0666, rec:0.0087, f1:0.0153
>> valid relation with NER prec:0.0666, rec:0.0087, f1:0.0153
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 186, avg_time 2.478, loss:776.7864
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 286, avg_time 0.935, loss:818.5063
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 386, avg_time 0.954, loss:834.7857
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 72, avg_time 0.939, loss:788.6522
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 172, avg_time 0.944, loss:820.9539
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4924, rec:0.3834, f1:0.4311
>> valid relation prec:0.0648, rec:0.0109, f1:0.0187
>> valid relation with NER prec:0.0648, rec:0.0109, f1:0.0187
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 272, avg_time 2.508, loss:798.6501
g_step 1200, step 372, avg_time 0.939, loss:841.3904
g_step 1300, step 58, avg_time 0.952, loss:792.3846
g_step 1400, step 158, avg_time 0.962, loss:789.2287
g_step 1500, step 258, avg_time 0.947, loss:771.6557
>> valid entity prec:0.4756, rec:0.3382, f1:0.3953
>> valid relation prec:0.1006, rec:0.0136, f1:0.0240
>> valid relation with NER prec:0.1006, rec:0.0136, f1:0.0240
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 358, avg_time 2.437, loss:803.0445
g_step 1700, step 44, avg_time 0.939, loss:761.8214
g_step 1800, step 144, avg_time 0.951, loss:736.3928
g_step 1900, step 244, avg_time 0.945, loss:778.9757
g_step 2000, step 344, avg_time 0.946, loss:799.0833
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5234, rec:0.3087, f1:0.3883
>> valid relation prec:0.0716, rec:0.0062, f1:0.0114
>> valid relation with NER prec:0.0716, rec:0.0062, f1:0.0114
g_step 2100, step 30, avg_time 2.435, loss:737.8661
g_step 2200, step 130, avg_time 0.943, loss:716.4557
g_step 2300, step 230, avg_time 0.935, loss:744.3957
g_step 2400, step 330, avg_time 0.947, loss:742.6533
g_step 2500, step 16, avg_time 0.951, loss:725.9931
>> valid entity prec:0.5011, rec:0.3840, f1:0.4348
>> valid relation prec:0.0608, rec:0.0140, f1:0.0228
>> valid relation with NER prec:0.0608, rec:0.0140, f1:0.0228
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 116, avg_time 2.479, loss:697.8496
g_step 2700, step 216, avg_time 0.939, loss:720.3620
g_step 2800, step 316, avg_time 0.938, loss:691.8489
g_step 2900, step 2, avg_time 0.950, loss:751.0391
g_step 3000, step 102, avg_time 0.947, loss:667.5811
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5280, rec:0.3694, f1:0.4347
>> valid relation prec:0.0798, rec:0.0154, f1:0.0259
>> valid relation with NER prec:0.0798, rec:0.0154, f1:0.0259
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 202, avg_time 2.478, loss:682.0645
g_step 3200, step 302, avg_time 0.942, loss:705.9445
g_step 3300, step 402, avg_time 0.942, loss:701.2934
g_step 3400, step 88, avg_time 0.941, loss:643.4080
g_step 3500, step 188, avg_time 0.956, loss:658.2611
>> valid entity prec:0.5090, rec:0.4301, f1:0.4662
>> valid relation prec:0.0865, rec:0.0196, f1:0.0319
>> valid relation with NER prec:0.0865, rec:0.0196, f1:0.0319
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 288, avg_time 2.507, loss:666.3584
g_step 3700, step 388, avg_time 0.955, loss:704.6974
g_step 3800, step 74, avg_time 0.942, loss:644.3182
g_step 3900, step 174, avg_time 0.956, loss:646.5147
g_step 4000, step 274, avg_time 0.949, loss:645.8371
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4841, rec:0.3923, f1:0.4334
>> valid relation prec:0.0671, rec:0.0181, f1:0.0285
>> valid relation with NER prec:0.0671, rec:0.0181, f1:0.0285
g_step 4100, step 374, avg_time 2.466, loss:647.6768
g_step 4200, step 60, avg_time 0.953, loss:624.2600
g_step 4300, step 160, avg_time 0.953, loss:620.2321
g_step 4400, step 260, avg_time 0.951, loss:607.4337
g_step 4500, step 360, avg_time 0.952, loss:635.3174
>> valid entity prec:0.4586, rec:0.3819, f1:0.4167
>> valid relation prec:0.0221, rec:0.0047, f1:0.0078
>> valid relation with NER prec:0.0221, rec:0.0047, f1:0.0078
g_step 4600, step 46, avg_time 2.477, loss:632.2661
g_step 4700, step 146, avg_time 0.959, loss:595.6601
g_step 4800, step 246, avg_time 0.957, loss:603.0813
g_step 4900, step 346, avg_time 0.958, loss:621.6731
g_step 5000, step 32, avg_time 0.949, loss:608.5257
learning rate was adjusted to 0.0008
>> valid entity prec:0.4717, rec:0.4279, f1:0.4487
>> valid relation prec:0.0560, rec:0.0146, f1:0.0232
>> valid relation with NER prec:0.0560, rec:0.0146, f1:0.0232
g_step 5100, step 132, avg_time 2.462, loss:572.0914
g_step 5200, step 232, avg_time 0.953, loss:612.1032
g_step 5300, step 332, avg_time 0.961, loss:574.4026
g_step 5400, step 18, avg_time 0.947, loss:582.0444
g_step 5500, step 118, avg_time 0.952, loss:540.9948
>> valid entity prec:0.4808, rec:0.3334, f1:0.3937
>> valid relation prec:0.0405, rec:0.0084, f1:0.0140
>> valid relation with NER prec:0.0405, rec:0.0084, f1:0.0140
g_step 5600, step 218, avg_time 2.469, loss:562.3732
g_step 5700, step 318, avg_time 0.960, loss:579.0194
g_step 5800, step 4, avg_time 0.950, loss:594.9334
g_step 5900, step 104, avg_time 0.966, loss:551.1118
g_step 6000, step 204, avg_time 0.971, loss:539.6741
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4292, rec:0.4807, f1:0.4535
>> valid relation prec:0.0643, rec:0.0171, f1:0.0270
>> valid relation with NER prec:0.0643, rec:0.0171, f1:0.0270
g_step 6100, step 304, avg_time 2.514, loss:566.3729
g_step 6200, step 404, avg_time 0.963, loss:571.4605
g_step 6300, step 90, avg_time 0.965, loss:519.1407
g_step 6400, step 190, avg_time 0.974, loss:534.7323
g_step 6500, step 290, avg_time 0.986, loss:530.2016
>> valid entity prec:0.4866, rec:0.3956, f1:0.4364
>> valid relation prec:0.1203, rec:0.0264, f1:0.0433
>> valid relation with NER prec:0.1203, rec:0.0264, f1:0.0433
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6600, step 390, avg_time 2.517, loss:555.2315
g_step 6700, step 76, avg_time 0.986, loss:508.7292
g_step 6800, step 176, avg_time 0.972, loss:513.1660
g_step 6900, step 276, avg_time 0.975, loss:525.2650
g_step 7000, step 376, avg_time 0.967, loss:529.1357
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4719, rec:0.3855, f1:0.4243
>> valid relation prec:0.0530, rec:0.0152, f1:0.0237
>> valid relation with NER prec:0.0530, rec:0.0152, f1:0.0237
g_step 7100, step 62, avg_time 2.513, loss:498.1721
g_step 7200, step 162, avg_time 0.966, loss:494.4862
g_step 7300, step 262, avg_time 0.976, loss:497.2517
g_step 7400, step 362, avg_time 0.982, loss:514.2291
g_step 7500, step 48, avg_time 0.982, loss:493.3573
>> valid entity prec:0.4529, rec:0.4451, f1:0.4490
>> valid relation prec:0.0680, rec:0.0216, f1:0.0328
>> valid relation with NER prec:0.0680, rec:0.0216, f1:0.0328
g_step 7600, step 148, avg_time 2.526, loss:483.2988
g_step 7700, step 248, avg_time 0.980, loss:479.8717
g_step 7800, step 348, avg_time 0.980, loss:513.0719
g_step 7900, step 34, avg_time 0.982, loss:483.2074
g_step 8000, step 134, avg_time 0.975, loss:458.2128
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4585, rec:0.3413, f1:0.3913
>> valid relation prec:0.0618, rec:0.0144, f1:0.0234
>> valid relation with NER prec:0.0618, rec:0.0144, f1:0.0234
g_step 8100, step 234, avg_time 2.506, loss:474.2349
g_step 8200, step 334, avg_time 0.977, loss:487.9566
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:17:51 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:17:51 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-17-51_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:17:52 - WARNING - datasets.builder -   Using custom data configuration default-c022da2a9522920d
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c022da2a9522920d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:17:54,319 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:17:54,321 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:17:54,321 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:17:54,322 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:17:54,439 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:17:54,490 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:17:54,516 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:17:54,516 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:17:54,516 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:17:54,516 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:17:54,516 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:17:54,884 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:17:58,032 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:17:58,063 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c022da2a9522920d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.84ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.73ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.09ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.29ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.41ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.48ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.54ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.56ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.58ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.59ba/s]100%|██████████| 11/11 [00:02<00:00,  4.78ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.30ba/s] 40%|████      | 2/5 [00:00<00:00,  3.25ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.60ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.89ba/s]100%|██████████| 5/5 [00:01<00:00,  4.28ba/s]100%|██████████| 5/5 [00:01<00:00,  3.81ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  5.39ba/s] 27%|██▋       | 3/11 [00:00<00:00,  8.38ba/s] 45%|████▌     | 5/11 [00:00<00:00,  9.32ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.77ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.12ba/s]100%|██████████| 11/11 [00:01<00:00, 12.14ba/s]100%|██████████| 11/11 [00:01<00:00, 10.47ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.94ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.46ba/s]100%|██████████| 5/5 [00:00<00:00, 10.39ba/s]100%|██████████| 5/5 [00:00<00:00,  9.93ba/s]
[INFO|trainer.py:414] 2023-08-29 00:18:04,308 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:18:04,377 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:18:04,377 >>   Num examples = 10045
[INFO|trainer.py:1149] 2023-08-29 00:18:04,377 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:18:04,377 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:18:04,377 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:18:04,377 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:18:04,377 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:56,  3.31it/s]  0%|          | 2/785 [00:00<03:49,  3.41it/s]  0%|          | 3/785 [00:00<04:26,  2.94it/s]  1%|          | 4/785 [00:01<04:09,  3.13it/s]  1%|          | 5/785 [00:01<04:00,  3.24it/s]  1%|          | 6/785 [00:01<03:55,  3.31it/s]  1%|          | 7/785 [00:02<03:51,  3.36it/s]  1%|          | 8/785 [00:02<03:48,  3.40it/s]  1%|          | 9/785 [00:02<03:46,  3.42it/s]  1%|▏         | 10/785 [00:03<03:45,  3.44it/s]  1%|▏         | 11/785 [00:03<03:47,  3.40it/s]  2%|▏         | 12/785 [00:03<03:45,  3.42it/s]  2%|▏         | 13/785 [00:03<03:44,  3.44it/s]  2%|▏         | 14/785 [00:04<03:43,  3.45it/s]  2%|▏         | 15/785 [00:04<03:42,  3.46it/s]  2%|▏         | 16/785 [00:04<03:41,  3.46it/s]  2%|▏         | 17/785 [00:05<03:41,  3.47it/s]  2%|▏         | 18/785 [00:05<03:41,  3.47it/s]  2%|▏         | 19/785 [00:05<03:40,  3.47it/s]  3%|▎         | 20/785 [00:05<03:40,  3.47it/s]  3%|▎         | 21/785 [00:06<03:39,  3.48it/s]  3%|▎         | 22/785 [00:06<03:50,  3.32it/s]  3%|▎         | 23/785 [00:06<03:46,  3.37it/s]  3%|▎         | 24/785 [00:07<03:43,  3.40it/s]  3%|▎         | 25/785 [00:07<03:42,  3.42it/s]  3%|▎         | 26/785 [00:07<03:40,  3.44it/s]  3%|▎         | 27/785 [00:07<03:39,  3.45it/s]  4%|▎         | 28/785 [00:08<03:38,  3.46it/s]  4%|▎         | 29/785 [00:08<03:38,  3.46it/s]  4%|▍         | 30/785 [00:08<03:37,  3.47it/s]  4%|▍         | 31/785 [00:09<03:37,  3.47it/s]  4%|▍         | 32/785 [00:09<03:37,  3.47it/s]  4%|▍         | 33/785 [00:09<03:45,  3.33it/s]  4%|▍         | 34/785 [00:10<03:42,  3.37it/s]  4%|▍         | 35/785 [00:10<03:40,  3.40it/s]  5%|▍         | 36/785 [00:10<03:39,  3.42it/s]  5%|▍         | 37/785 [00:10<03:37,  3.43it/s]  5%|▍         | 38/785 [00:11<03:36,  3.45it/s]  5%|▍         | 39/785 [00:11<03:35,  3.46it/s]  5%|▌         | 40/785 [00:11<03:35,  3.46it/s]  5%|▌         | 41/785 [00:12<03:34,  3.47it/s]  5%|▌         | 42/785 [00:12<03:34,  3.47it/s]  5%|▌         | 43/785 [00:12<03:33,  3.47it/s]  6%|▌         | 44/785 [00:12<03:39,  3.37it/s]  6%|▌         | 45/785 [00:13<03:37,  3.40it/s]  6%|▌         | 46/785 [00:13<03:35,  3.42it/s]  6%|▌         | 47/785 [00:13<03:34,  3.44it/s]  6%|▌         | 48/785 [00:14<03:33,  3.45it/s]  6%|▌         | 49/785 [00:14<03:32,  3.46it/s]  6%|▋         | 50/785 [00:14<03:32,  3.47it/s]  6%|▋         | 51/785 [00:14<03:31,  3.47it/s]  7%|▋         | 52/785 [00:15<03:31,  3.47it/s]  7%|▋         | 53/785 [00:15<03:30,  3.47it/s]  7%|▋         | 54/785 [00:15<03:30,  3.47it/s]  7%|▋         | 55/785 [00:16<03:40,  3.31it/s]  7%|▋         | 56/785 [00:16<03:36,  3.36it/s]  7%|▋         | 57/785 [00:16<03:34,  3.39it/s]  7%|▋         | 58/785 [00:16<03:32,  3.42it/s]  8%|▊         | 59/785 [00:17<04:34,  2.64it/s]  8%|▊         | 60/785 [00:17<04:14,  2.85it/s]  8%|▊         | 61/785 [00:18<04:00,  3.01it/s]  8%|▊         | 62/785 [00:18<03:50,  3.13it/s]  8%|▊         | 63/785 [00:18<03:43,  3.23it/s]  8%|▊         | 64/785 [00:19<03:38,  3.30it/s]  8%|▊         | 65/785 [00:19<03:41,  3.26it/s]  8%|▊         | 66/785 [00:19<03:43,  3.21it/s]  9%|▊         | 67/785 [00:19<03:38,  3.29it/s]  9%|▊         | 68/785 [00:20<03:34,  3.34it/s]  9%|▉         | 69/785 [00:20<03:31,  3.38it/s]  9%|▉         | 70/785 [00:20<03:29,  3.41it/s]  9%|▉         | 71/785 [00:21<03:28,  3.43it/s]  9%|▉         | 72/785 [00:21<03:27,  3.44it/s]  9%|▉         | 73/785 [00:21<03:26,  3.45it/s]  9%|▉         | 74/785 [00:21<03:25,  3.46it/s] 10%|▉         | 75/785 [00:22<03:25,  3.46it/s] 10%|▉         | 76/785 [00:22<03:24,  3.46it/s] 10%|▉         | 77/785 [00:22<03:24,  3.47it/s] 10%|▉         | 78/785 [00:23<03:23,  3.47it/s] 10%|█         | 79/785 [00:23<03:29,  3.37it/s] 10%|█         | 80/785 [00:23<03:27,  3.40it/s] 10%|█         | 81/785 [00:23<03:25,  3.42it/s] 10%|█         | 82/785 [00:24<03:24,  3.44it/s] 11%|█         | 83/785 [00:24<03:23,  3.45it/s] 11%|█         | 84/785 [00:24<03:22,  3.46it/s] 11%|█         | 85/785 [00:25<03:22,  3.46it/s] 11%|█         | 86/785 [00:25<03:21,  3.46it/s] 11%|█         | 87/785 [00:25<03:21,  3.47it/s] 11%|█         | 88/785 [00:26<03:21,  3.47it/s] 11%|█▏        | 89/785 [00:26<03:20,  3.47it/s] 11%|█▏        | 90/785 [00:26<03:25,  3.39it/s] 12%|█▏        | 91/785 [00:26<03:23,  3.41it/s] 12%|█▏        | 92/785 [00:27<03:22,  3.43it/s] 12%|█▏        | 93/785 [00:27<03:21,  3.44it/s] 12%|█▏        | 94/785 [00:27<03:20,  3.45it/s] 12%|█▏        | 95/785 [00:28<03:19,  3.46it/s] 12%|█▏        | 96/785 [00:28<03:19,  3.46it/s] 12%|█▏        | 97/785 [00:28<03:18,  3.46it/s] 12%|█▏        | 98/785 [00:28<03:18,  3.47it/s] 13%|█▎        | 99/785 [00:29<03:17,  3.47it/s] 13%|█▎        | 100/785 [00:29<03:17,  3.47it/s] 13%|█▎        | 101/785 [00:29<03:23,  3.36it/s] 13%|█▎        | 102/785 [00:30<03:21,  3.39it/s] 13%|█▎        | 103/785 [00:30<03:20,  3.41it/s] 13%|█▎        | 104/785 [00:30<03:18,  3.43it/s] 13%|█▎        | 105/785 [00:30<03:17,  3.44it/s] 14%|█▎        | 106/785 [00:31<03:17,  3.45it/s] 14%|█▎        | 107/785 [00:31<03:16,  3.45it/s] 14%|█▍        | 108/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 109/785 [00:32<03:15,  3.46it/s] 14%|█▍        | 110/785 [00:32<03:14,  3.46it/s] 14%|█▍        | 111/785 [00:32<03:14,  3.47it/s] 14%|█▍        | 112/785 [00:33<03:17,  3.40it/s] 14%|█▍        | 113/785 [00:33<03:16,  3.42it/s] 15%|█▍        | 114/785 [00:33<03:15,  3.44it/s] 15%|█▍        | 115/785 [00:33<03:14,  3.45it/s] 15%|█▍        | 116/785 [00:34<03:13,  3.45it/s] 15%|█▍        | 117/785 [00:34<03:13,  3.46it/s] 15%|█▌        | 118/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 119/785 [00:35<03:12,  3.46it/s] 15%|█▌        | 120/785 [00:35<03:12,  3.46it/s] 15%|█▌        | 121/785 [00:35<03:11,  3.47it/s] 16%|█▌        | 122/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 123/785 [00:36<03:16,  3.37it/s] 16%|█▌        | 124/785 [00:36<03:14,  3.40it/s] 16%|█▌        | 125/785 [00:36<03:13,  3.42it/s] 16%|█▌        | 126/785 [00:37<03:11,  3.43it/s] 16%|█▌        | 127/785 [00:37<03:11,  3.44it/s] 16%|█▋        | 128/785 [00:37<03:10,  3.45it/s] 16%|█▋        | 129/785 [00:37<03:09,  3.45it/s] 17%|█▋        | 130/785 [00:38<03:09,  3.46it/s] 17%|█▋        | 131/785 [00:38<03:09,  3.46it/s] 17%|█▋        | 132/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 133/785 [00:39<03:08,  3.46it/s] 17%|█▋        | 134/785 [00:39<03:10,  3.41it/s] 17%|█▋        | 135/785 [00:39<03:09,  3.43it/s] 17%|█▋        | 136/785 [00:39<03:08,  3.44it/s] 17%|█▋        | 137/785 [00:40<03:07,  3.45it/s] 18%|█▊        | 138/785 [00:40<03:07,  3.45it/s] 18%|█▊        | 139/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 140/785 [00:41<03:06,  3.46it/s] 18%|█▊        | 141/785 [00:41<03:06,  3.46it/s] 18%|█▊        | 142/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 143/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 144/785 [00:42<03:04,  3.47it/s] 18%|█▊        | 145/785 [00:42<03:07,  3.41it/s] 19%|█▊        | 146/785 [00:42<03:06,  3.43it/s] 19%|█▊        | 147/785 [00:43<03:05,  3.44it/s] 19%|█▉        | 148/785 [00:43<03:05,  3.44it/s] 19%|█▉        | 149/785 [00:43<03:04,  3.45it/s] 19%|█▉        | 150/785 [00:44<03:03,  3.45it/s] 19%|█▉        | 151/785 [00:44<03:03,  3.46it/s] 19%|█▉        | 152/785 [00:44<03:03,  3.46it/s] 19%|█▉        | 153/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 154/785 [00:45<03:02,  3.46it/s] 20%|█▉        | 155/785 [00:45<03:01,  3.46it/s] 20%|█▉        | 156/785 [00:45<03:06,  3.38it/s] 20%|██        | 157/785 [00:46<03:02,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 00:18:50,438 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:18:50,438 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 00:18:50,438 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.76it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.77it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.76it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.73it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.25it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.86it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.64it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.37it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.43it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.47it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.54it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.62it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.55it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.51it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.38it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.31it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.21it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.14it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.32it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.44it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.55it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.46it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.12it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.20it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.21it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.08it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.15it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.28it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.31it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.38it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.44it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.49it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.47it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 45.32it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 45.24it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.23it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.27it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.37it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.37it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.44it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.41it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.41it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.30it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.27it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.29it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.40it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.35it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.42it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.31it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.42it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.40it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.26it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.20it/s][A
 45%|████▍     | 273/608 [00:05<00:07, 45.28it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.26it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.29it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.34it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.40it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.39it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.37it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.34it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.35it/s][A
 52%|█████▏    | 318/608 [00:06<00:06, 45.25it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.28it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.34it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.39it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.34it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.36it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.40it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.32it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 44.37it/s][A
 60%|█████▉    | 363/608 [00:07<00:05, 44.62it/s][A
 61%|██████    | 368/608 [00:08<00:05, 44.85it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.05it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.18it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.23it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.29it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.27it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.08it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.15it/s][A
 67%|██████▋   | 408/608 [00:08<00:04, 45.22it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.26it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.34it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.48it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.44it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.44it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.33it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.23it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.18it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 45.23it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.31it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.29it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.43it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.44it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.47it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.37it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.25it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.25it/s][A
 82%|████████▏ | 498/608 [00:10<00:02, 45.28it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.20it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.30it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.43it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.41it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.37it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.32it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.19it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.24it/s][A
 89%|████████▉ | 543/608 [00:11<00:01, 45.25it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.29it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.22it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.41it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.39it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.46it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.38it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.33it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.25it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 45.32it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.18it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.21it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.36it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.47it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.47it/s][A 20%|██        | 157/785 [00:59<03:02,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:19:03,939 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-29 00:19:03,989 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:19:05,859 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:19:05,890 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:19:05,912 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:06<1:07:19,  6.44s/it] 20%|██        | 159/785 [01:07<48:07,  4.61s/it]   20%|██        | 160/785 [01:07<34:32,  3.32s/it] 21%|██        | 161/785 [01:07<25:03,  2.41s/it] 21%|██        | 162/785 [01:08<18:25,  1.77s/it] 21%|██        | 163/785 [01:08<13:47,  1.33s/it] 21%|██        | 164/785 [01:08<10:32,  1.02s/it] 21%|██        | 165/785 [01:08<08:16,  1.25it/s] 21%|██        | 166/785 [01:09<06:41,  1.54it/s] 21%|██▏       | 167/785 [01:09<05:34,  1.85it/s] 21%|██▏       | 168/785 [01:09<04:48,  2.14it/s] 22%|██▏       | 169/785 [01:10<04:15,  2.41it/s] 22%|██▏       | 170/785 [01:10<04:05,  2.51it/s] 22%|██▏       | 171/785 [01:10<03:45,  2.72it/s] 22%|██▏       | 172/785 [01:11<03:31,  2.89it/s] 22%|██▏       | 173/785 [01:11<03:21,  3.03it/s] 22%|██▏       | 174/785 [01:11<03:14,  3.14it/s] 22%|██▏       | 175/785 [01:11<03:09,  3.22it/s] 22%|██▏       | 176/785 [01:12<03:06,  3.27it/s] 23%|██▎       | 177/785 [01:12<03:03,  3.31it/s] 23%|██▎       | 178/785 [01:12<03:01,  3.34it/s] 23%|██▎       | 179/785 [01:13<03:00,  3.36it/s] 23%|██▎       | 180/785 [01:13<02:59,  3.37it/s] 23%|██▎       | 181/785 [01:13<02:58,  3.38it/s] 23%|██▎       | 182/785 [01:14<02:57,  3.39it/s] 23%|██▎       | 183/785 [01:14<02:57,  3.40it/s] 23%|██▎       | 184/785 [01:14<02:56,  3.40it/s] 24%|██▎       | 185/785 [01:14<02:56,  3.40it/s] 24%|██▎       | 186/785 [01:15<02:55,  3.41it/s] 24%|██▍       | 187/785 [01:15<02:55,  3.41it/s] 24%|██▍       | 188/785 [01:15<02:54,  3.41it/s] 24%|██▍       | 189/785 [01:16<02:54,  3.41it/s] 24%|██▍       | 190/785 [01:16<02:54,  3.41it/s] 24%|██▍       | 191/785 [01:16<02:54,  3.41it/s] 24%|██▍       | 192/785 [01:16<02:53,  3.41it/s] 25%|██▍       | 193/785 [01:17<02:53,  3.41it/s] 25%|██▍       | 194/785 [01:17<03:00,  3.27it/s] 25%|██▍       | 195/785 [01:17<02:58,  3.31it/s] 25%|██▍       | 196/785 [01:18<02:56,  3.34it/s] 25%|██▌       | 197/785 [01:18<02:54,  3.36it/s] 25%|██▌       | 198/785 [01:18<02:54,  3.37it/s] 25%|██▌       | 199/785 [01:19<02:53,  3.38it/s] 25%|██▌       | 200/785 [01:19<02:52,  3.39it/s] 26%|██▌       | 201/785 [01:19<02:53,  3.37it/s] 26%|██▌       | 202/785 [01:19<02:52,  3.37it/s] 26%|██▌       | 203/785 [01:20<02:52,  3.38it/s] 26%|██▌       | 204/785 [01:20<02:51,  3.39it/s] 26%|██▌       | 205/785 [01:20<02:50,  3.39it/s] 26%|██▌       | 206/785 [01:21<02:50,  3.40it/s] 26%|██▋       | 207/785 [01:21<02:49,  3.40it/s] 26%|██▋       | 208/785 [01:21<02:49,  3.41it/s] 27%|██▋       | 209/785 [01:21<02:48,  3.41it/s] 27%|██▋       | 210/785 [01:22<02:48,  3.41it/s] 27%|██▋       | 211/785 [01:22<02:48,  3.41it/s] 27%|██▋       | 212/785 [01:22<02:47,  3.41it/s] 27%|██▋       | 213/785 [01:23<02:47,  3.41it/s] 27%|██▋       | 214/785 [01:23<02:47,  3.41it/s] 27%|██▋       | 215/785 [01:23<02:47,  3.40it/s] 28%|██▊       | 216/785 [01:24<02:47,  3.39it/s] 28%|██▊       | 217/785 [01:24<02:47,  3.40it/s] 28%|██▊       | 218/785 [01:24<02:46,  3.40it/s] 28%|██▊       | 219/785 [01:24<02:46,  3.39it/s] 28%|██▊       | 220/785 [01:25<02:46,  3.39it/s] 28%|██▊       | 221/785 [01:25<02:45,  3.40it/s] 28%|██▊       | 222/785 [01:25<02:45,  3.40it/s] 28%|██▊       | 223/785 [01:26<02:45,  3.40it/s] 29%|██▊       | 224/785 [01:26<02:44,  3.40it/s] 29%|██▊       | 225/785 [01:26<02:44,  3.41it/s] 29%|██▉       | 226/785 [01:26<02:44,  3.41it/s] 29%|██▉       | 227/785 [01:27<02:43,  3.41it/s] 29%|██▉       | 228/785 [01:27<02:43,  3.40it/s] 29%|██▉       | 229/785 [01:27<02:43,  3.41it/s] 29%|██▉       | 230/785 [01:28<02:44,  3.38it/s] 29%|██▉       | 231/785 [01:28<02:43,  3.39it/s] 30%|██▉       | 232/785 [01:28<02:42,  3.40it/s] 30%|██▉       | 233/785 [01:29<02:42,  3.40it/s] 30%|██▉       | 234/785 [01:29<02:41,  3.40it/s] 30%|██▉       | 235/785 [01:29<02:41,  3.41it/s] 30%|███       | 236/785 [01:29<02:41,  3.40it/s] 30%|███       | 237/785 [01:30<02:40,  3.40it/s] 30%|███       | 238/785 [01:30<02:40,  3.41it/s] 30%|███       | 239/785 [01:30<02:40,  3.41it/s] 31%|███       | 240/785 [01:31<02:39,  3.41it/s] 31%|███       | 241/785 [01:31<02:41,  3.37it/s] 31%|███       | 242/785 [01:31<02:40,  3.39it/s] 31%|███       | 243/785 [01:31<02:39,  3.39it/s] 31%|███       | 244/785 [01:32<02:39,  3.40it/s] 31%|███       | 245/785 [01:32<02:38,  3.40it/s] 31%|███▏      | 246/785 [01:32<02:38,  3.41it/s] 31%|███▏      | 247/785 [01:33<02:37,  3.41it/s] 32%|███▏      | 248/785 [01:33<02:37,  3.41it/s] 32%|███▏      | 249/785 [01:33<02:37,  3.41it/s] 32%|███▏      | 250/785 [01:34<02:36,  3.41it/s] 32%|███▏      | 251/785 [01:34<02:36,  3.41it/s] 32%|███▏      | 252/785 [01:34<02:39,  3.34it/s] 32%|███▏      | 253/785 [01:34<02:38,  3.36it/s] 32%|███▏      | 254/785 [01:35<02:37,  3.38it/s] 32%|███▏      | 255/785 [01:35<02:36,  3.38it/s] 33%|███▎      | 256/785 [01:35<02:35,  3.39it/s] 33%|███▎      | 257/785 [01:36<02:35,  3.40it/s] 33%|███▎      | 258/785 [01:36<02:34,  3.40it/s] 33%|███▎      | 259/785 [01:36<02:34,  3.40it/s] 33%|███▎      | 260/785 [01:36<02:34,  3.41it/s] 33%|███▎      | 261/785 [01:37<02:33,  3.41it/s] 33%|███▎      | 262/785 [01:37<02:33,  3.41it/s] 34%|███▎      | 263/785 [01:37<02:38,  3.30it/s] 34%|███▎      | 264/785 [01:38<02:36,  3.33it/s] 34%|███▍      | 265/785 [01:38<02:34,  3.36it/s] 34%|███▍      | 266/785 [01:38<02:33,  3.37it/s] 34%|███▍      | 267/785 [01:39<02:33,  3.38it/s] 34%|███▍      | 268/785 [01:39<02:32,  3.39it/s] 34%|███▍      | 269/785 [01:39<02:31,  3.40it/s] 34%|███▍      | 270/785 [01:39<02:31,  3.40it/s] 35%|███▍      | 271/785 [01:40<02:30,  3.40it/s] 35%|███▍      | 272/785 [01:40<02:30,  3.40it/s] 35%|███▍      | 273/785 [01:40<02:30,  3.40it/s] 35%|███▍      | 274/785 [01:41<02:35,  3.29it/s] 35%|███▌      | 275/785 [01:41<02:33,  3.33it/s] 35%|███▌      | 276/785 [01:41<02:31,  3.35it/s] 35%|███▌      | 277/785 [01:42<02:30,  3.37it/s] 35%|███▌      | 278/785 [01:42<02:29,  3.38it/s] 36%|███▌      | 279/785 [01:42<02:29,  3.39it/s] 36%|███▌      | 280/785 [01:42<02:28,  3.39it/s] 36%|███▌      | 281/785 [01:43<02:28,  3.40it/s] 36%|███▌      | 282/785 [01:43<02:27,  3.40it/s] 36%|███▌      | 283/785 [01:43<02:27,  3.40it/s] 36%|███▌      | 284/785 [01:44<02:27,  3.40it/s] 36%|███▋      | 285/785 [01:44<02:31,  3.31it/s] 36%|███▋      | 286/785 [01:44<02:29,  3.34it/s] 37%|███▋      | 287/785 [01:44<02:28,  3.36it/s] 37%|███▋      | 288/785 [01:45<02:27,  3.37it/s] 37%|███▋      | 289/785 [01:45<02:26,  3.38it/s] 37%|███▋      | 290/785 [01:45<02:25,  3.41it/s] 37%|███▋      | 291/785 [01:46<02:24,  3.42it/s] 37%|███▋      | 292/785 [01:46<02:23,  3.43it/s] 37%|███▋      | 293/785 [01:46<02:23,  3.44it/s] 37%|███▋      | 294/785 [01:47<02:22,  3.44it/s] 38%|███▊      | 295/785 [01:47<02:22,  3.45it/s] 38%|███▊      | 296/785 [01:47<02:26,  3.34it/s] 38%|███▊      | 297/785 [01:47<02:24,  3.38it/s] 38%|███▊      | 298/785 [01:48<02:23,  3.40it/s] 38%|███▊      | 299/785 [01:48<02:22,  3.42it/s] 38%|███▊      | 300/785 [01:48<02:21,  3.43it/s] 38%|███▊      | 301/785 [01:49<02:20,  3.44it/s] 38%|███▊      | 302/785 [01:49<02:20,  3.44it/s] 39%|███▊      | 303/785 [01:49<02:19,  3.45it/s] 39%|███▊      | 304/785 [01:49<02:19,  3.45it/s] 39%|███▉      | 305/785 [01:50<02:19,  3.45it/s] 39%|███▉      | 306/785 [01:50<02:18,  3.45it/s] 39%|███▉      | 307/785 [01:50<02:23,  3.33it/s] 39%|███▉      | 308/785 [01:51<02:21,  3.37it/s] 39%|███▉      | 309/785 [01:51<02:20,  3.40it/s] 39%|███▉      | 310/785 [01:51<02:19,  3.41it/s] 40%|███▉      | 311/785 [01:52<02:18,  3.43it/s] 40%|███▉      | 312/785 [01:52<02:17,  3.43it/s] 40%|███▉      | 313/785 [01:52<02:17,  3.44it/s] 40%|████      | 314/785 [01:52<02:15,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 00:19:57,255 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:19:57,255 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 00:19:57,255 >>   Batch size = 8
{'eval_loss': 0.9167154431343079, 'eval_runtime': 13.4193, 'eval_samples_per_second': 362.463, 'eval_steps_per_second': 45.308, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.63it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.28it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.54it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.68it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.09it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.65it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.46it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.09it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.18it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.34it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.39it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.40it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.35it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.29it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.13it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.04it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.93it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 45.04it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.24it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.32it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.35it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 43.38it/s][A
 19%|█▉        | 117/608 [00:02<00:11, 43.96it/s][A
 20%|██        | 122/608 [00:02<00:10, 44.22it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.40it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.62it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.82it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.08it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.18it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.07it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 45.03it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.13it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.04it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.04it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.92it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 45.13it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.31it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.31it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.25it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.17it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.15it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.10it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.97it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.06it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.22it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.23it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.18it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.23it/s][A
 41%|████      | 247/608 [00:05<00:08, 43.56it/s][A
 41%|████▏     | 252/608 [00:05<00:08, 44.06it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.37it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.52it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.77it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.85it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.06it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.97it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.83it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 45.00it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.17it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.13it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.16it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.08it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 45.03it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.02it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.98it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.89it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 44.96it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.21it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.20it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.20it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.16it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 45.12it/s][A
 60%|██████    | 367/608 [00:08<00:05, 45.18it/s][A
 61%|██████    | 372/608 [00:08<00:05, 45.02it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.03it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.24it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.67it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.91it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 44.91it/s][A
 66%|██████▌   | 402/608 [00:08<00:05, 37.54it/s][A
 67%|██████▋   | 407/608 [00:09<00:05, 39.89it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 41.53it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 42.76it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 43.52it/s][A
 70%|███████   | 427/608 [00:09<00:04, 44.24it/s][A
 71%|███████   | 432/608 [00:09<00:03, 44.63it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.01it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 44.80it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 44.57it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.44it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.52it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.70it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.07it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.17it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.32it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.44it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.17it/s][A
 81%|████████  | 492/608 [00:10<00:02, 44.97it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.82it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 44.80it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.90it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.13it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.29it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.30it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.37it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.26it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.02it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.85it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.86it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.86it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.07it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.20it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.35it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.40it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.14it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.10it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 44.92it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.72it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.80it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.97it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.06it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.06it/s][A 40%|████      | 314/785 [02:06<02:15,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:20:11,016 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-29 00:20:11,167 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:20:15,392 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:20:15,673 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:20:15,837 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:23<1:13:39,  9.40s/it] 40%|████      | 316/785 [02:23<52:08,  6.67s/it]   40%|████      | 317/785 [02:24<37:06,  4.76s/it] 41%|████      | 318/785 [02:24<26:35,  3.42s/it] 41%|████      | 319/785 [02:24<19:15,  2.48s/it] 41%|████      | 320/785 [02:25<14:07,  1.82s/it] 41%|████      | 321/785 [02:25<10:32,  1.36s/it] 41%|████      | 322/785 [02:25<08:02,  1.04s/it] 41%|████      | 323/785 [02:25<06:17,  1.22it/s] 41%|████▏     | 324/785 [02:26<05:04,  1.52it/s] 41%|████▏     | 325/785 [02:26<04:16,  1.79it/s] 42%|████▏     | 326/785 [02:26<03:39,  2.09it/s] 42%|████▏     | 327/785 [02:27<03:12,  2.38it/s] 42%|████▏     | 328/785 [02:27<02:54,  2.62it/s] 42%|████▏     | 329/785 [02:27<02:41,  2.83it/s] 42%|████▏     | 330/785 [02:27<02:32,  2.99it/s] 42%|████▏     | 331/785 [02:28<02:25,  3.12it/s] 42%|████▏     | 332/785 [02:28<02:20,  3.22it/s] 42%|████▏     | 333/785 [02:28<02:17,  3.29it/s] 43%|████▎     | 334/785 [02:29<02:15,  3.34it/s] 43%|████▎     | 335/785 [02:29<02:13,  3.38it/s] 43%|████▎     | 336/785 [02:29<02:14,  3.34it/s] 43%|████▎     | 337/785 [02:29<02:12,  3.38it/s] 43%|████▎     | 338/785 [02:30<02:11,  3.41it/s] 43%|████▎     | 339/785 [02:30<02:10,  3.42it/s] 43%|████▎     | 340/785 [02:30<02:09,  3.44it/s] 43%|████▎     | 341/785 [02:31<02:09,  3.44it/s] 44%|████▎     | 342/785 [02:31<02:08,  3.45it/s] 44%|████▎     | 343/785 [02:31<02:08,  3.45it/s] 44%|████▍     | 344/785 [02:31<02:07,  3.46it/s] 44%|████▍     | 345/785 [02:32<02:07,  3.46it/s] 44%|████▍     | 346/785 [02:32<02:06,  3.46it/s] 44%|████▍     | 347/785 [02:32<02:11,  3.34it/s] 44%|████▍     | 348/785 [02:33<02:09,  3.37it/s] 44%|████▍     | 349/785 [02:33<02:08,  3.40it/s] 45%|████▍     | 350/785 [02:33<02:07,  3.42it/s] 45%|████▍     | 351/785 [02:34<02:06,  3.43it/s] 45%|████▍     | 352/785 [02:34<02:05,  3.44it/s] 45%|████▍     | 353/785 [02:34<02:05,  3.45it/s] 45%|████▌     | 354/785 [02:34<02:04,  3.45it/s] 45%|████▌     | 355/785 [02:35<02:04,  3.46it/s] 45%|████▌     | 356/785 [02:35<02:04,  3.46it/s] 45%|████▌     | 357/785 [02:35<02:03,  3.46it/s] 46%|████▌     | 358/785 [02:36<02:07,  3.34it/s] 46%|████▌     | 359/785 [02:36<02:06,  3.38it/s] 46%|████▌     | 360/785 [02:36<02:04,  3.40it/s] 46%|████▌     | 361/785 [02:36<02:03,  3.42it/s] 46%|████▌     | 362/785 [02:37<02:03,  3.44it/s] 46%|████▌     | 363/785 [02:37<02:02,  3.44it/s] 46%|████▋     | 364/785 [02:37<02:02,  3.45it/s] 46%|████▋     | 365/785 [02:38<02:01,  3.45it/s] 47%|████▋     | 366/785 [02:38<02:01,  3.46it/s] 47%|████▋     | 367/785 [02:38<02:00,  3.46it/s] 47%|████▋     | 368/785 [02:38<02:00,  3.46it/s] 47%|████▋     | 369/785 [02:39<02:02,  3.38it/s] 47%|████▋     | 370/785 [02:39<02:01,  3.41it/s] 47%|████▋     | 371/785 [02:39<02:00,  3.42it/s] 47%|████▋     | 372/785 [02:40<02:00,  3.44it/s] 48%|████▊     | 373/785 [02:40<01:59,  3.44it/s] 48%|████▊     | 374/785 [02:40<01:59,  3.45it/s] 48%|████▊     | 375/785 [02:41<01:58,  3.45it/s] 48%|████▊     | 376/785 [02:41<01:58,  3.45it/s] 48%|████▊     | 377/785 [02:41<01:57,  3.46it/s] 48%|████▊     | 378/785 [02:41<01:57,  3.46it/s] 48%|████▊     | 379/785 [02:42<01:57,  3.46it/s] 48%|████▊     | 380/785 [02:42<01:59,  3.39it/s] 49%|████▊     | 381/785 [02:42<01:58,  3.42it/s] 49%|████▊     | 382/785 [02:43<01:57,  3.43it/s] 49%|████▉     | 383/785 [02:43<01:56,  3.44it/s] 49%|████▉     | 384/785 [02:43<01:56,  3.45it/s] 49%|████▉     | 385/785 [02:43<01:55,  3.45it/s] 49%|████▉     | 386/785 [02:44<01:55,  3.45it/s] 49%|████▉     | 387/785 [02:44<01:55,  3.46it/s] 49%|████▉     | 388/785 [02:44<01:54,  3.46it/s] 50%|████▉     | 389/785 [02:45<01:54,  3.46it/s] 50%|████▉     | 390/785 [02:45<01:54,  3.46it/s] 50%|████▉     | 391/785 [02:45<01:55,  3.41it/s] 50%|████▉     | 392/785 [02:45<01:54,  3.42it/s] 50%|█████     | 393/785 [02:46<01:54,  3.44it/s] 50%|█████     | 394/785 [02:46<01:53,  3.44it/s] 50%|█████     | 395/785 [02:46<01:53,  3.45it/s] 50%|█████     | 396/785 [02:47<01:52,  3.45it/s] 51%|█████     | 397/785 [02:47<01:52,  3.45it/s] 51%|█████     | 398/785 [02:47<01:51,  3.46it/s] 51%|█████     | 399/785 [02:48<01:51,  3.46it/s] 51%|█████     | 400/785 [02:48<01:51,  3.46it/s] 51%|█████     | 401/785 [02:48<01:50,  3.46it/s] 51%|█████     | 402/785 [02:48<01:52,  3.40it/s] 51%|█████▏    | 403/785 [02:49<01:51,  3.42it/s] 51%|█████▏    | 404/785 [02:49<01:51,  3.43it/s] 52%|█████▏    | 405/785 [02:49<01:50,  3.44it/s] 52%|█████▏    | 406/785 [02:50<01:49,  3.45it/s] 52%|█████▏    | 407/785 [02:50<01:49,  3.45it/s] 52%|█████▏    | 408/785 [02:50<01:49,  3.45it/s] 52%|█████▏    | 409/785 [02:50<01:48,  3.45it/s] 52%|█████▏    | 410/785 [02:51<01:48,  3.45it/s] 52%|█████▏    | 411/785 [02:51<01:48,  3.46it/s] 52%|█████▏    | 412/785 [02:51<01:47,  3.46it/s] 53%|█████▎    | 413/785 [02:52<01:50,  3.36it/s] 53%|█████▎    | 414/785 [02:52<01:49,  3.39it/s] 53%|█████▎    | 415/785 [02:52<01:48,  3.41it/s] 53%|█████▎    | 416/785 [02:52<01:47,  3.43it/s] 53%|█████▎    | 417/785 [02:53<01:47,  3.44it/s] 53%|█████▎    | 418/785 [02:53<01:46,  3.44it/s] 53%|█████▎    | 419/785 [02:53<01:46,  3.45it/s] 54%|█████▎    | 420/785 [02:54<01:45,  3.45it/s] 54%|█████▎    | 421/785 [02:54<01:45,  3.45it/s] 54%|█████▍    | 422/785 [02:54<01:45,  3.45it/s] 54%|█████▍    | 423/785 [02:54<01:44,  3.46it/s] 54%|█████▍    | 424/785 [02:55<01:44,  3.46it/s] 54%|█████▍    | 425/785 [02:55<01:44,  3.45it/s] 54%|█████▍    | 426/785 [02:55<01:43,  3.46it/s] 54%|█████▍    | 427/785 [02:56<01:43,  3.45it/s] 55%|█████▍    | 428/785 [02:56<01:43,  3.46it/s] 55%|█████▍    | 429/785 [02:56<01:43,  3.45it/s] 55%|█████▍    | 430/785 [02:57<01:46,  3.34it/s] 55%|█████▍    | 431/785 [02:57<01:44,  3.37it/s] 55%|█████▌    | 432/785 [02:57<01:43,  3.40it/s] 55%|█████▌    | 433/785 [02:57<01:43,  3.42it/s] 55%|█████▌    | 434/785 [02:58<01:42,  3.43it/s] 55%|█████▌    | 435/785 [02:58<01:41,  3.44it/s] 56%|█████▌    | 436/785 [02:58<01:41,  3.44it/s] 56%|█████▌    | 437/785 [02:59<01:40,  3.45it/s] 56%|█████▌    | 438/785 [02:59<01:40,  3.45it/s] 56%|█████▌    | 439/785 [02:59<01:40,  3.45it/s] 56%|█████▌    | 440/785 [02:59<01:39,  3.45it/s] 56%|█████▌    | 441/785 [03:00<01:46,  3.23it/s] 56%|█████▋    | 442/785 [03:00<01:43,  3.30it/s] 56%|█████▋    | 443/785 [03:00<01:42,  3.35it/s] 57%|█████▋    | 444/785 [03:01<01:41,  3.38it/s] 57%|█████▋    | 445/785 [03:01<01:39,  3.40it/s] 57%|█████▋    | 446/785 [03:01<01:39,  3.42it/s] 57%|█████▋    | 447/785 [03:02<01:53,  2.97it/s] 57%|█████▋    | 448/785 [03:02<01:48,  3.11it/s] 57%|█████▋    | 449/785 [03:02<01:44,  3.20it/s] 57%|█████▋    | 450/785 [03:03<01:42,  3.27it/s] 57%|█████▋    | 451/785 [03:03<01:43,  3.23it/s] 58%|█████▊    | 452/785 [03:03<01:41,  3.29it/s] 58%|█████▊    | 453/785 [03:03<01:39,  3.34it/s] 58%|█████▊    | 454/785 [03:04<01:38,  3.37it/s] 58%|█████▊    | 455/785 [03:04<01:37,  3.40it/s] 58%|█████▊    | 456/785 [03:04<01:36,  3.41it/s] 58%|█████▊    | 457/785 [03:05<01:35,  3.43it/s] 58%|█████▊    | 458/785 [03:05<01:35,  3.43it/s] 58%|█████▊    | 459/785 [03:05<01:34,  3.44it/s] 59%|█████▊    | 460/785 [03:05<01:34,  3.44it/s] 59%|█████▊    | 461/785 [03:06<01:33,  3.45it/s] 59%|█████▉    | 462/785 [03:06<01:35,  3.36it/s] 59%|█████▉    | 463/785 [03:06<01:34,  3.39it/s] 59%|█████▉    | 464/785 [03:07<01:34,  3.40it/s] 59%|█████▉    | 465/785 [03:07<01:33,  3.42it/s] 59%|█████▉    | 466/785 [03:07<01:33,  3.43it/s] 59%|█████▉    | 467/785 [03:08<01:32,  3.44it/s] 60%|█████▉    | 468/785 [03:08<01:32,  3.44it/s] 60%|█████▉    | 469/785 [03:08<01:31,  3.45it/s] 60%|█████▉    | 470/785 [03:08<01:31,  3.45it/s] 60%|██████    | 471/785 [03:09<01:30,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 00:21:13,560 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:21:13,560 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 00:21:13,560 >>   Batch size = 8
{'eval_loss': 0.93179851770401, 'eval_runtime': 13.5728, 'eval_samples_per_second': 358.365, 'eval_steps_per_second': 44.796, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.49it/s][A
  2%|▏         | 12/608 [00:00<00:13, 43.98it/s][A
  3%|▎         | 17/608 [00:00<00:13, 44.34it/s][A
  4%|▎         | 22/608 [00:00<00:13, 44.47it/s][A
  4%|▍         | 27/608 [00:00<00:13, 44.37it/s][A
  5%|▌         | 32/608 [00:00<00:12, 44.48it/s][A
  6%|▌         | 37/608 [00:00<00:12, 44.55it/s][A
  7%|▋         | 42/608 [00:00<00:12, 44.72it/s][A
  8%|▊         | 47/608 [00:01<00:12, 44.77it/s][A
  9%|▊         | 52/608 [00:01<00:12, 44.78it/s][A
  9%|▉         | 57/608 [00:01<00:12, 44.90it/s][A
 10%|█         | 62/608 [00:01<00:12, 44.82it/s][A
 11%|█         | 67/608 [00:01<00:12, 44.95it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 44.88it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 44.88it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.89it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.86it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.90it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 44.97it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 44.92it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.95it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 44.90it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 44.92it/s][A
 20%|██        | 122/608 [00:02<00:10, 44.91it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.89it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.92it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.92it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.88it/s][A
 24%|██▍       | 147/608 [00:03<00:11, 41.66it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 42.78it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 43.48it/s][A
 27%|██▋       | 162/608 [00:03<00:10, 43.97it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 44.23it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 44.36it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.44it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.63it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.45it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 44.62it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 44.75it/s][A
 33%|███▎      | 202/608 [00:04<00:09, 44.92it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 44.98it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 44.93it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.87it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.94it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.68it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 44.61it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 44.76it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.77it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.96it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.98it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.89it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.94it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.89it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.80it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.74it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 41.97it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 42.94it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 43.69it/s][A
 49%|████▉     | 297/608 [00:06<00:07, 44.20it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 44.41it/s][A
 50%|█████     | 307/608 [00:06<00:06, 44.55it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 44.68it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.59it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.26it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.41it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.55it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 44.76it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 44.86it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.05it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.10it/s][A
 59%|█████▊    | 357/608 [00:08<00:05, 45.02it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.75it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.61it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.54it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.61it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.66it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.80it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.96it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.00it/s][A
 66%|██████▌   | 402/608 [00:09<00:04, 45.03it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.74it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.59it/s][A
 69%|██████▊   | 417/608 [00:09<00:05, 38.18it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 40.11it/s][A
 70%|███████   | 427/608 [00:09<00:04, 41.55it/s][A
 71%|███████   | 432/608 [00:09<00:04, 42.62it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 43.47it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 43.98it/s][A
 74%|███████▎  | 447/608 [00:10<00:03, 44.33it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.53it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.18it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 41.28it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 43.16it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 43.90it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 44.36it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 44.60it/s][A
 80%|████████  | 487/608 [00:10<00:02, 44.81it/s][A
 81%|████████  | 492/608 [00:11<00:02, 44.93it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.63it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 44.48it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.16it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.38it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.56it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.80it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.01it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.11it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 45.02it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.95it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.60it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 40.71it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 39.27it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 40.89it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 42.17it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 43.08it/s][A
 95%|█████████▍| 577/608 [00:13<00:00, 43.71it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 26.08it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 30.45it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 33.73it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 36.54it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 38.83it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 40.59it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 40.59it/s][A 60%|██████    | 471/785 [03:23<01:30,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:21:27,884 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-29 00:21:28,155 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:21:34,275 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:21:34,365 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:21:34,408 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:37<44:38,  8.56s/it] 60%|██████    | 473/785 [03:37<31:38,  6.08s/it] 60%|██████    | 474/785 [03:37<22:31,  4.35s/it] 61%|██████    | 475/785 [03:37<16:10,  3.13s/it] 61%|██████    | 476/785 [03:38<11:44,  2.28s/it] 61%|██████    | 477/785 [03:38<08:38,  1.68s/it] 61%|██████    | 478/785 [03:38<06:28,  1.27s/it] 61%|██████    | 479/785 [03:39<04:58,  1.03it/s] 61%|██████    | 480/785 [03:39<03:54,  1.30it/s] 61%|██████▏   | 481/785 [03:39<03:10,  1.60it/s] 61%|██████▏   | 482/785 [03:39<02:39,  1.90it/s] 62%|██████▏   | 483/785 [03:40<02:17,  2.19it/s] 62%|██████▏   | 484/785 [03:40<02:04,  2.41it/s] 62%|██████▏   | 485/785 [03:40<01:53,  2.65it/s] 62%|██████▏   | 486/785 [03:41<01:45,  2.84it/s] 62%|██████▏   | 487/785 [03:41<01:39,  2.99it/s] 62%|██████▏   | 488/785 [03:41<01:35,  3.11it/s] 62%|██████▏   | 489/785 [03:42<01:32,  3.19it/s] 62%|██████▏   | 490/785 [03:42<01:30,  3.26it/s] 63%|██████▎   | 491/785 [03:42<01:28,  3.30it/s] 63%|██████▎   | 492/785 [03:42<01:27,  3.34it/s] 63%|██████▎   | 493/785 [03:43<01:26,  3.36it/s] 63%|██████▎   | 494/785 [03:43<01:26,  3.38it/s] 63%|██████▎   | 495/785 [03:43<01:27,  3.32it/s] 63%|██████▎   | 496/785 [03:44<01:26,  3.35it/s] 63%|██████▎   | 497/785 [03:44<01:25,  3.37it/s] 63%|██████▎   | 498/785 [03:44<01:24,  3.38it/s] 64%|██████▎   | 499/785 [03:45<01:24,  3.39it/s] 64%|██████▎   | 500/785 [03:45<01:23,  3.40it/s]                                                  64%|██████▎   | 500/785 [03:45<01:23,  3.40it/s] 64%|██████▍   | 501/785 [03:45<01:23,  3.40it/s] 64%|██████▍   | 502/785 [03:45<01:23,  3.40it/s] 64%|██████▍   | 503/785 [03:46<01:22,  3.41it/s] 64%|██████▍   | 504/785 [03:46<01:22,  3.41it/s] 64%|██████▍   | 505/785 [03:46<01:22,  3.41it/s] 64%|██████▍   | 506/785 [03:47<01:23,  3.32it/s] 65%|██████▍   | 507/785 [03:47<01:23,  3.35it/s] 65%|██████▍   | 508/785 [03:47<01:22,  3.36it/s] 65%|██████▍   | 509/785 [03:47<01:21,  3.38it/s] 65%|██████▍   | 510/785 [03:48<01:21,  3.39it/s] 65%|██████▌   | 511/785 [03:48<01:20,  3.40it/s] 65%|██████▌   | 512/785 [03:48<01:20,  3.40it/s] 65%|██████▌   | 513/785 [03:49<01:19,  3.40it/s] 65%|██████▌   | 514/785 [03:49<01:19,  3.41it/s] 66%|██████▌   | 515/785 [03:49<01:19,  3.41it/s] 66%|██████▌   | 516/785 [03:50<01:18,  3.41it/s] 66%|██████▌   | 517/785 [03:50<01:20,  3.31it/s] 66%|██████▌   | 518/785 [03:50<01:19,  3.34it/s] 66%|██████▌   | 519/785 [03:50<01:19,  3.36it/s] 66%|██████▌   | 520/785 [03:51<01:18,  3.38it/s] 66%|██████▋   | 521/785 [03:51<01:17,  3.38it/s] 66%|██████▋   | 522/785 [03:51<01:17,  3.39it/s] 67%|██████▋   | 523/785 [03:52<01:17,  3.40it/s] 67%|██████▋   | 524/785 [03:52<01:16,  3.40it/s] 67%|██████▋   | 525/785 [03:52<01:16,  3.41it/s] 67%|██████▋   | 526/785 [03:52<01:15,  3.41it/s] 67%|██████▋   | 527/785 [03:53<01:15,  3.41it/s] 67%|██████▋   | 528/785 [03:53<01:16,  3.37it/s] 67%|██████▋   | 529/785 [03:53<01:15,  3.38it/s] 68%|██████▊   | 530/785 [03:54<01:15,  3.39it/s] 68%|██████▊   | 531/785 [03:54<01:14,  3.40it/s] 68%|██████▊   | 532/785 [03:54<01:14,  3.40it/s] 68%|██████▊   | 533/785 [03:55<01:13,  3.41it/s] 68%|██████▊   | 534/785 [03:55<01:13,  3.41it/s] 68%|██████▊   | 535/785 [03:55<01:13,  3.41it/s] 68%|██████▊   | 536/785 [03:55<01:13,  3.41it/s] 68%|██████▊   | 537/785 [03:56<01:12,  3.41it/s] 69%|██████▊   | 538/785 [03:56<01:12,  3.41it/s] 69%|██████▊   | 539/785 [03:56<01:12,  3.41it/s] 69%|██████▉   | 540/785 [03:57<01:11,  3.41it/s] 69%|██████▉   | 541/785 [03:57<01:11,  3.41it/s] 69%|██████▉   | 542/785 [03:57<01:11,  3.41it/s] 69%|██████▉   | 543/785 [03:57<01:10,  3.41it/s] 69%|██████▉   | 544/785 [03:58<01:10,  3.41it/s] 69%|██████▉   | 545/785 [03:58<01:14,  3.23it/s] 70%|██████▉   | 546/785 [03:58<01:12,  3.28it/s] 70%|██████▉   | 547/785 [03:59<01:11,  3.32it/s] 70%|██████▉   | 548/785 [03:59<01:10,  3.35it/s] 70%|██████▉   | 549/785 [03:59<01:10,  3.37it/s] 70%|███████   | 550/785 [04:00<01:09,  3.38it/s] 70%|███████   | 551/785 [04:00<01:09,  3.39it/s] 70%|███████   | 552/785 [04:00<01:08,  3.39it/s] 70%|███████   | 553/785 [04:00<01:08,  3.40it/s] 71%|███████   | 554/785 [04:01<01:08,  3.40it/s] 71%|███████   | 555/785 [04:01<01:07,  3.40it/s] 71%|███████   | 556/785 [04:01<01:08,  3.33it/s] 71%|███████   | 557/785 [04:02<01:12,  3.14it/s] 71%|███████   | 558/785 [04:02<01:10,  3.21it/s] 71%|███████   | 559/785 [04:02<01:09,  3.27it/s] 71%|███████▏  | 560/785 [04:03<01:08,  3.31it/s] 71%|███████▏  | 561/785 [04:03<01:07,  3.34it/s] 72%|███████▏  | 562/785 [04:03<01:06,  3.36it/s] 72%|███████▏  | 563/785 [04:03<01:05,  3.37it/s] 72%|███████▏  | 564/785 [04:04<01:05,  3.38it/s] 72%|███████▏  | 565/785 [04:04<01:04,  3.39it/s] 72%|███████▏  | 566/785 [04:04<01:06,  3.29it/s] 72%|███████▏  | 567/785 [04:05<01:05,  3.32it/s] 72%|███████▏  | 568/785 [04:05<01:04,  3.35it/s] 72%|███████▏  | 569/785 [04:05<01:04,  3.37it/s] 73%|███████▎  | 570/785 [04:06<01:03,  3.38it/s] 73%|███████▎  | 571/785 [04:06<01:03,  3.39it/s] 73%|███████▎  | 572/785 [04:06<01:02,  3.39it/s] 73%|███████▎  | 573/785 [04:06<01:02,  3.39it/s] 73%|███████▎  | 574/785 [04:07<01:02,  3.40it/s] 73%|███████▎  | 575/785 [04:07<01:01,  3.40it/s] 73%|███████▎  | 576/785 [04:07<01:01,  3.40it/s] 74%|███████▎  | 577/785 [04:08<01:02,  3.32it/s] 74%|███████▎  | 578/785 [04:08<01:01,  3.35it/s] 74%|███████▍  | 579/785 [04:08<01:01,  3.36it/s] 74%|███████▍  | 580/785 [04:09<01:00,  3.38it/s] 74%|███████▍  | 581/785 [04:09<01:00,  3.38it/s] 74%|███████▍  | 582/785 [04:09<00:59,  3.39it/s] 74%|███████▍  | 583/785 [04:09<00:59,  3.39it/s] 74%|███████▍  | 584/785 [04:10<00:59,  3.40it/s] 75%|███████▍  | 585/785 [04:10<00:58,  3.40it/s] 75%|███████▍  | 586/785 [04:10<00:58,  3.40it/s] 75%|███████▍  | 587/785 [04:11<00:58,  3.40it/s] 75%|███████▍  | 588/785 [04:11<01:00,  3.24it/s] 75%|███████▌  | 589/785 [04:11<00:59,  3.29it/s] 75%|███████▌  | 590/785 [04:12<00:58,  3.32it/s] 75%|███████▌  | 591/785 [04:12<00:57,  3.35it/s] 75%|███████▌  | 592/785 [04:12<00:57,  3.37it/s] 76%|███████▌  | 593/785 [04:12<00:56,  3.38it/s] 76%|███████▌  | 594/785 [04:13<00:56,  3.39it/s] 76%|███████▌  | 595/785 [04:13<00:56,  3.39it/s] 76%|███████▌  | 596/785 [04:13<00:55,  3.39it/s] 76%|███████▌  | 597/785 [04:14<00:55,  3.40it/s] 76%|███████▌  | 598/785 [04:14<00:55,  3.40it/s] 76%|███████▋  | 599/785 [04:14<00:58,  3.17it/s] 76%|███████▋  | 600/785 [04:15<00:57,  3.23it/s] 77%|███████▋  | 601/785 [04:15<00:56,  3.28it/s] 77%|███████▋  | 602/785 [04:15<00:55,  3.32it/s] 77%|███████▋  | 603/785 [04:15<00:54,  3.34it/s] 77%|███████▋  | 604/785 [04:16<00:53,  3.36it/s] 77%|███████▋  | 605/785 [04:16<00:53,  3.38it/s] 77%|███████▋  | 606/785 [04:16<00:52,  3.39it/s] 77%|███████▋  | 607/785 [04:17<00:52,  3.39it/s] 77%|███████▋  | 608/785 [04:17<00:52,  3.39it/s] 78%|███████▊  | 609/785 [04:17<00:56,  3.10it/s] 78%|███████▊  | 610/785 [04:18<00:55,  3.18it/s] 78%|███████▊  | 611/785 [04:18<00:53,  3.24it/s] 78%|███████▊  | 612/785 [04:18<00:52,  3.29it/s] 78%|███████▊  | 613/785 [04:18<00:51,  3.32it/s] 78%|███████▊  | 614/785 [04:19<00:51,  3.34it/s] 78%|███████▊  | 615/785 [04:19<00:50,  3.36it/s] 78%|███████▊  | 616/785 [04:19<00:50,  3.38it/s] 79%|███████▊  | 617/785 [04:20<00:49,  3.38it/s] 79%|███████▊  | 618/785 [04:20<00:49,  3.39it/s] 79%|███████▉  | 619/785 [04:20<00:55,  2.97it/s] 79%|███████▉  | 620/785 [04:21<00:53,  3.09it/s] 79%|███████▉  | 621/785 [04:21<00:51,  3.18it/s] 79%|███████▉  | 622/785 [04:21<00:56,  2.90it/s] 79%|███████▉  | 623/785 [04:22<00:53,  3.03it/s] 79%|███████▉  | 624/785 [04:23<01:23,  1.94it/s] 80%|███████▉  | 625/785 [04:23<01:11,  2.22it/s] 80%|███████▉  | 626/785 [04:23<01:04,  2.48it/s] 80%|███████▉  | 627/785 [04:24<01:03,  2.48it/s] 80%|████████  | 628/785 [04:24<00:57,  2.72it/s][INFO|trainer.py:2140] 2023-08-29 00:22:28,749 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:22:28,749 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 00:22:28,749 >>   Batch size = 8
{'eval_loss': 0.9393065571784973, 'eval_runtime': 14.0111, 'eval_samples_per_second': 347.153, 'eval_steps_per_second': 43.394, 'epoch': 3.0}
{'loss': 0.6764, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.19it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.19it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.20it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.05it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.55it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.12it/s][A
  6%|▌         | 37/608 [00:00<00:12, 44.86it/s][A
  7%|▋         | 42/608 [00:00<00:12, 44.81it/s][A
  8%|▊         | 47/608 [00:01<00:12, 44.81it/s][A
  9%|▊         | 52/608 [00:01<00:12, 44.93it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.01it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.15it/s][A
 11%|█         | 67/608 [00:01<00:12, 45.01it/s][A
 12%|█▏        | 72/608 [00:01<00:12, 43.40it/s][A
 13%|█▎        | 77/608 [00:01<00:12, 43.87it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.09it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.10it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.28it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 44.54it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 44.73it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.91it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 44.65it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 44.82it/s][A
 20%|██        | 122/608 [00:02<00:10, 44.76it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.54it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.68it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.74it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.78it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.91it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 44.98it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 45.00it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 44.96it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 44.89it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 44.80it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.69it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.65it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.77it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 44.90it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 44.96it/s][A
 33%|███▎      | 202/608 [00:04<00:09, 44.87it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 44.57it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 44.64it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.68it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.75it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.68it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 44.68it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 44.68it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.68it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.80it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.90it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.88it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.85it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.91it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.91it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.82it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.80it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.74it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 44.77it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 44.88it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 44.86it/s][A
 50%|█████     | 307/608 [00:06<00:06, 44.94it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 44.97it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.82it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.86it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.92it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.86it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 44.92it/s][A
 56%|█████▋    | 342/608 [00:07<00:06, 44.08it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 44.35it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 44.46it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 44.45it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.59it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.68it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.78it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.76it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.74it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.90it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.96it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 44.94it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.73it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.87it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.84it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.83it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 44.78it/s][A
 70%|███████   | 427/608 [00:09<00:04, 44.77it/s][A
 71%|███████   | 432/608 [00:09<00:03, 44.82it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 44.89it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 44.96it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 44.90it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.94it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.93it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.81it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 44.78it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 44.76it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 44.08it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 44.33it/s][A
 80%|████████  | 487/608 [00:10<00:02, 44.47it/s][A
 81%|████████  | 492/608 [00:10<00:02, 44.57it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.75it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 44.84it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.68it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.31it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.85it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.77it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.89it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 42.45it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 43.28it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 43.87it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.16it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.36it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 44.43it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 44.46it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 44.60it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 44.46it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 44.58it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 44.73it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 44.95it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.03it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.95it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.87it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 44.84it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.84it/s][A 80%|████████  | 628/785 [04:37<00:57,  2.72it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:22:42,552 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-29 00:22:42,752 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:22:45,954 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:22:46,090 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:22:46,193 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:48<19:50,  7.63s/it] 80%|████████  | 630/785 [04:49<14:03,  5.44s/it] 80%|████████  | 631/785 [04:49<09:59,  3.90s/it] 81%|████████  | 632/785 [04:49<07:10,  2.81s/it] 81%|████████  | 633/785 [04:50<05:12,  2.06s/it] 81%|████████  | 634/785 [04:50<03:50,  1.53s/it] 81%|████████  | 635/785 [04:50<02:53,  1.16s/it] 81%|████████  | 636/785 [04:51<02:13,  1.11it/s] 81%|████████  | 637/785 [04:51<01:46,  1.39it/s] 81%|████████▏ | 638/785 [04:51<01:26,  1.70it/s] 81%|████████▏ | 639/785 [04:51<01:12,  2.00it/s] 82%|████████▏ | 640/785 [04:52<01:03,  2.29it/s] 82%|████████▏ | 641/785 [04:52<00:58,  2.47it/s] 82%|████████▏ | 642/785 [04:52<00:52,  2.70it/s] 82%|████████▏ | 643/785 [04:53<00:49,  2.89it/s] 82%|████████▏ | 644/785 [04:53<00:46,  3.04it/s] 82%|████████▏ | 645/785 [04:53<00:44,  3.16it/s] 82%|████████▏ | 646/785 [04:53<00:42,  3.24it/s] 82%|████████▏ | 647/785 [04:54<00:41,  3.30it/s] 83%|████████▎ | 648/785 [04:54<00:40,  3.35it/s] 83%|████████▎ | 649/785 [04:54<00:40,  3.38it/s] 83%|████████▎ | 650/785 [04:55<00:39,  3.40it/s] 83%|████████▎ | 651/785 [04:55<00:39,  3.42it/s] 83%|████████▎ | 652/785 [04:55<00:39,  3.36it/s] 83%|████████▎ | 653/785 [04:56<00:38,  3.39it/s] 83%|████████▎ | 654/785 [04:56<00:38,  3.41it/s] 83%|████████▎ | 655/785 [04:56<00:37,  3.43it/s] 84%|████████▎ | 656/785 [04:56<00:37,  3.44it/s] 84%|████████▎ | 657/785 [04:57<00:37,  3.45it/s] 84%|████████▍ | 658/785 [04:57<00:36,  3.45it/s] 84%|████████▍ | 659/785 [04:57<00:36,  3.45it/s] 84%|████████▍ | 660/785 [04:58<00:36,  3.46it/s] 84%|████████▍ | 661/785 [04:58<00:35,  3.46it/s] 84%|████████▍ | 662/785 [04:58<00:35,  3.46it/s] 84%|████████▍ | 663/785 [04:58<00:36,  3.35it/s] 85%|████████▍ | 664/785 [04:59<00:35,  3.39it/s] 85%|████████▍ | 665/785 [04:59<00:35,  3.41it/s] 85%|████████▍ | 666/785 [04:59<00:34,  3.42it/s] 85%|████████▍ | 667/785 [05:00<00:34,  3.44it/s] 85%|████████▌ | 668/785 [05:00<00:33,  3.44it/s] 85%|████████▌ | 669/785 [05:00<00:33,  3.45it/s] 85%|████████▌ | 670/785 [05:00<00:33,  3.45it/s] 85%|████████▌ | 671/785 [05:01<00:32,  3.45it/s] 86%|████████▌ | 672/785 [05:01<00:32,  3.46it/s] 86%|████████▌ | 673/785 [05:01<00:32,  3.46it/s] 86%|████████▌ | 674/785 [05:02<01:01,  1.81it/s] 86%|████████▌ | 675/785 [05:03<00:52,  2.11it/s] 86%|████████▌ | 676/785 [05:03<00:45,  2.39it/s] 86%|████████▌ | 677/785 [05:03<00:41,  2.63it/s] 86%|████████▋ | 678/785 [05:04<00:37,  2.83it/s] 86%|████████▋ | 679/785 [05:04<00:35,  3.00it/s] 87%|████████▋ | 680/785 [05:04<00:33,  3.12it/s] 87%|████████▋ | 681/785 [05:05<00:32,  3.22it/s] 87%|████████▋ | 682/785 [05:05<00:32,  3.14it/s] 87%|████████▋ | 683/785 [05:05<00:31,  3.23it/s] 87%|████████▋ | 684/785 [05:05<00:30,  3.30it/s] 87%|████████▋ | 685/785 [05:06<00:29,  3.34it/s] 87%|████████▋ | 686/785 [05:06<00:29,  3.38it/s] 88%|████████▊ | 687/785 [05:06<00:28,  3.40it/s] 88%|████████▊ | 688/785 [05:07<00:30,  3.22it/s] 88%|████████▊ | 689/785 [05:07<00:29,  3.29it/s] 88%|████████▊ | 690/785 [05:07<00:28,  3.34it/s] 88%|████████▊ | 691/785 [05:08<00:27,  3.37it/s] 88%|████████▊ | 692/785 [05:08<00:27,  3.40it/s] 88%|████████▊ | 693/785 [05:08<00:26,  3.42it/s] 88%|████████▊ | 694/785 [05:08<00:26,  3.43it/s] 89%|████████▊ | 695/785 [05:09<00:26,  3.44it/s] 89%|████████▊ | 696/785 [05:09<00:25,  3.45it/s] 89%|████████▉ | 697/785 [05:09<00:25,  3.45it/s] 89%|████████▉ | 698/785 [05:10<00:25,  3.45it/s] 89%|████████▉ | 699/785 [05:10<00:25,  3.40it/s] 89%|████████▉ | 700/785 [05:10<00:24,  3.42it/s] 89%|████████▉ | 701/785 [05:10<00:24,  3.43it/s] 89%|████████▉ | 702/785 [05:11<00:24,  3.44it/s] 90%|████████▉ | 703/785 [05:11<00:23,  3.45it/s] 90%|████████▉ | 704/785 [05:11<00:23,  3.45it/s] 90%|████████▉ | 705/785 [05:12<00:23,  3.45it/s] 90%|████████▉ | 706/785 [05:12<00:22,  3.46it/s] 90%|█████████ | 707/785 [05:12<00:22,  3.46it/s] 90%|█████████ | 708/785 [05:12<00:22,  3.46it/s] 90%|█████████ | 709/785 [05:13<00:21,  3.46it/s] 90%|█████████ | 710/785 [05:13<00:22,  3.26it/s] 91%|█████████ | 711/785 [05:13<00:22,  3.32it/s] 91%|█████████ | 712/785 [05:14<00:21,  3.36it/s] 91%|█████████ | 713/785 [05:14<00:21,  3.39it/s] 91%|█████████ | 714/785 [05:14<00:20,  3.41it/s] 91%|█████████ | 715/785 [05:15<00:20,  3.43it/s] 91%|█████████ | 716/785 [05:15<00:20,  3.43it/s] 91%|█████████▏| 717/785 [05:15<00:19,  3.44it/s] 91%|█████████▏| 718/785 [05:15<00:19,  3.45it/s] 92%|█████████▏| 719/785 [05:16<00:19,  3.45it/s] 92%|█████████▏| 720/785 [05:16<00:18,  3.45it/s] 92%|█████████▏| 721/785 [05:16<00:19,  3.28it/s] 92%|█████████▏| 722/785 [05:17<00:18,  3.33it/s] 92%|█████████▏| 723/785 [05:17<00:18,  3.37it/s] 92%|█████████▏| 724/785 [05:17<00:17,  3.40it/s] 92%|█████████▏| 725/785 [05:17<00:17,  3.42it/s] 92%|█████████▏| 726/785 [05:18<00:17,  3.43it/s] 93%|█████████▎| 727/785 [05:18<00:16,  3.44it/s] 93%|█████████▎| 728/785 [05:18<00:16,  3.44it/s] 93%|█████████▎| 729/785 [05:19<00:16,  3.45it/s] 93%|█████████▎| 730/785 [05:19<00:15,  3.45it/s] 93%|█████████▎| 731/785 [05:19<00:16,  3.29it/s] 93%|█████████▎| 732/785 [05:20<00:16,  3.23it/s] 93%|█████████▎| 733/785 [05:20<00:15,  3.30it/s] 94%|█████████▎| 734/785 [05:20<00:15,  3.35it/s] 94%|█████████▎| 735/785 [05:20<00:14,  3.38it/s] 94%|█████████▍| 736/785 [05:21<00:14,  3.40it/s] 94%|█████████▍| 737/785 [05:21<00:14,  3.42it/s] 94%|█████████▍| 738/785 [05:21<00:14,  3.25it/s] 94%|█████████▍| 739/785 [05:22<00:13,  3.31it/s] 94%|█████████▍| 740/785 [05:22<00:14,  3.20it/s] 94%|█████████▍| 741/785 [05:22<00:15,  2.92it/s] 95%|█████████▍| 742/785 [05:23<00:15,  2.71it/s] 95%|█████████▍| 743/785 [05:23<00:14,  2.90it/s] 95%|█████████▍| 744/785 [05:23<00:13,  3.05it/s] 95%|█████████▍| 745/785 [05:24<00:12,  3.16it/s] 95%|█████████▌| 746/785 [05:24<00:12,  3.24it/s] 95%|█████████▌| 747/785 [05:24<00:11,  3.30it/s] 95%|█████████▌| 748/785 [05:25<00:11,  3.35it/s] 95%|█████████▌| 749/785 [05:25<00:10,  3.38it/s] 96%|█████████▌| 750/785 [05:25<00:10,  3.41it/s] 96%|█████████▌| 751/785 [05:25<00:09,  3.42it/s] 96%|█████████▌| 752/785 [05:26<00:10,  3.18it/s] 96%|█████████▌| 753/785 [05:26<00:09,  3.26it/s] 96%|█████████▌| 754/785 [05:26<00:09,  3.32it/s] 96%|█████████▌| 755/785 [05:27<00:08,  3.36it/s] 96%|█████████▋| 756/785 [05:27<00:08,  3.39it/s] 96%|█████████▋| 757/785 [05:27<00:08,  3.41it/s] 97%|█████████▋| 758/785 [05:28<00:07,  3.42it/s] 97%|█████████▋| 759/785 [05:28<00:07,  3.44it/s] 97%|█████████▋| 760/785 [05:28<00:07,  3.44it/s] 97%|█████████▋| 761/785 [05:28<00:06,  3.45it/s] 97%|█████████▋| 762/785 [05:29<00:06,  3.45it/s] 97%|█████████▋| 763/785 [05:29<00:06,  3.34it/s] 97%|█████████▋| 764/785 [05:29<00:06,  3.38it/s] 97%|█████████▋| 765/785 [05:30<00:05,  3.40it/s] 98%|█████████▊| 766/785 [05:30<00:05,  3.42it/s] 98%|█████████▊| 767/785 [05:30<00:05,  3.43it/s] 98%|█████████▊| 768/785 [05:30<00:04,  3.44it/s] 98%|█████████▊| 769/785 [05:31<00:04,  3.45it/s] 98%|█████████▊| 770/785 [05:31<00:04,  3.45it/s] 98%|█████████▊| 771/785 [05:31<00:04,  3.46it/s] 98%|█████████▊| 772/785 [05:32<00:03,  3.46it/s] 98%|█████████▊| 773/785 [05:32<00:03,  3.46it/s] 99%|█████████▊| 774/785 [05:32<00:03,  3.39it/s] 99%|█████████▊| 775/785 [05:32<00:02,  3.41it/s] 99%|█████████▉| 776/785 [05:33<00:02,  3.43it/s] 99%|█████████▉| 777/785 [05:33<00:02,  3.44it/s] 99%|█████████▉| 778/785 [05:33<00:02,  3.45it/s] 99%|█████████▉| 779/785 [05:34<00:01,  3.45it/s] 99%|█████████▉| 780/785 [05:34<00:01,  3.45it/s] 99%|█████████▉| 781/785 [05:34<00:01,  3.45it/s]100%|█████████▉| 782/785 [05:35<00:00,  3.46it/s]100%|█████████▉| 783/785 [05:35<00:00,  3.46it/s]100%|█████████▉| 784/785 [05:35<00:00,  3.46it/s]100%|██████████| 785/785 [05:35<00:00,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 00:23:40,290 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:23:40,290 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 00:23:40,290 >>   Batch size = 8
{'eval_loss': 0.9462314248085022, 'eval_runtime': 13.6098, 'eval_samples_per_second': 357.39, 'eval_steps_per_second': 44.674, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.54it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.84it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.76it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.92it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.31it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.42it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.04it/s][A
  7%|▋         | 43/608 [00:00<00:12, 44.98it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.12it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.25it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.36it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.45it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.60it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.36it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 44.43it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 44.43it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 44.55it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 44.64it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 44.98it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.21it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.38it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.42it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.37it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.19it/s][A
 21%|██        | 128/608 [00:02<00:10, 44.98it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 44.92it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 44.93it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.16it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.21it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.31it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.33it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.29it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.14it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 44.99it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.84it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.95it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.06it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.24it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.35it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.31it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.19it/s][A
 35%|███▌      | 213/608 [00:04<00:09, 43.49it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 43.91it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 44.07it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 44.42it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 44.63it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 44.89it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.12it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.16it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.00it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.09it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.03it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 44.79it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 44.87it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.06it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.16it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.26it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.25it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.26it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.18it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.17it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.03it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.06it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.22it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.26it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.21it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.27it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.15it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 44.01it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 44.05it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 44.51it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 44.81it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.01it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 44.99it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.15it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.19it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 44.97it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.02it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 44.95it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.10it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.21it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.22it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.25it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.31it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.15it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.10it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.04it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 44.98it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.13it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.17it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.19it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.31it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.33it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.15it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.11it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 43.27it/s][A
 80%|████████  | 488/608 [00:10<00:02, 43.85it/s][A
 81%|████████  | 493/608 [00:10<00:02, 44.33it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 44.53it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 44.72it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 44.99it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.13it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.10it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 44.74it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 44.83it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.07it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.11it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.23it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.19it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.27it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.31it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.14it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 44.92it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 44.90it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 44.92it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.03it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.17it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.29it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.40it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.38it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.19it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.19it/s][A100%|██████████| 785/785 [05:49<00:00,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:23:54,085 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-29 00:23:54,292 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:23:57,926 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:23:58,184 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:23:58,320 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:24:05,351 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:24:05,374 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157 (score: 0.9167154431343079).
                                                 100%|██████████| 785/785 [06:09<00:00,  3.39it/s]100%|██████████| 785/785 [06:09<00:00,  2.12it/s]
[INFO|trainer.py:1894] 2023-08-29 00:24:14,350 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 00:24:14,434 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:24:17,055 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:24:17,294 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:24:17,422 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:24:18,134 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:18,134 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:18,134 >>   train_loss               =     0.6651
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:18,134 >>   train_runtime            = 0:06:09.95
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:18,134 >>   train_samples            =      10045
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:18,134 >>   train_samples_per_second =     135.76
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:18,134 >>   train_steps_per_second   =      2.122
{'eval_loss': 0.9501121044158936, 'eval_runtime': 13.509, 'eval_samples_per_second': 360.057, 'eval_steps_per_second': 45.007, 'epoch': 5.0}
{'train_runtime': 369.9555, 'train_samples_per_second': 135.76, 'train_steps_per_second': 2.122, 'train_loss': 0.6650743229374004, 'epoch': 5.0}
08/29/2023 00:24:18 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:24:18,596 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:24:18,596 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 00:24:18,596 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 56.42it/s]  2%|▏         | 12/608 [00:00<00:11, 49.97it/s]  3%|▎         | 18/608 [00:00<00:12, 48.05it/s]  4%|▍         | 23/608 [00:00<00:12, 47.37it/s]  5%|▍         | 28/608 [00:00<00:12, 46.93it/s]  5%|▌         | 33/608 [00:00<00:12, 46.60it/s]  6%|▋         | 38/608 [00:00<00:12, 46.40it/s]  7%|▋         | 43/608 [00:00<00:12, 45.82it/s]  8%|▊         | 48/608 [00:01<00:12, 45.29it/s]  9%|▊         | 53/608 [00:01<00:12, 45.03it/s] 10%|▉         | 58/608 [00:01<00:12, 45.15it/s] 10%|█         | 63/608 [00:01<00:12, 45.30it/s] 11%|█         | 68/608 [00:01<00:11, 45.50it/s] 12%|█▏        | 73/608 [00:01<00:12, 41.92it/s] 13%|█▎        | 78/608 [00:01<00:12, 43.01it/s] 14%|█▎        | 83/608 [00:01<00:11, 43.93it/s] 14%|█▍        | 88/608 [00:01<00:11, 44.53it/s] 15%|█▌        | 93/608 [00:02<00:11, 44.69it/s] 16%|█▌        | 98/608 [00:02<00:11, 44.66it/s] 17%|█▋        | 103/608 [00:02<00:11, 44.91it/s] 18%|█▊        | 108/608 [00:02<00:11, 45.13it/s] 19%|█▊        | 113/608 [00:02<00:11, 44.99it/s] 19%|█▉        | 118/608 [00:02<00:10, 45.15it/s] 20%|██        | 123/608 [00:02<00:10, 45.39it/s] 21%|██        | 128/608 [00:02<00:10, 45.38it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.44it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.47it/s] 24%|██▎       | 143/608 [00:03<00:10, 45.53it/s] 24%|██▍       | 148/608 [00:03<00:10, 45.38it/s] 25%|██▌       | 153/608 [00:03<00:10, 45.33it/s] 26%|██▌       | 158/608 [00:03<00:09, 45.27it/s] 27%|██▋       | 163/608 [00:03<00:09, 45.34it/s] 28%|██▊       | 168/608 [00:03<00:09, 45.33it/s] 28%|██▊       | 173/608 [00:03<00:09, 45.34it/s] 29%|██▉       | 178/608 [00:03<00:09, 45.37it/s] 30%|███       | 183/608 [00:04<00:09, 45.47it/s] 31%|███       | 188/608 [00:04<00:09, 45.45it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.53it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.37it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.37it/s] 34%|███▍      | 208/608 [00:04<00:09, 42.39it/s] 35%|███▌      | 213/608 [00:04<00:09, 43.39it/s] 36%|███▌      | 218/608 [00:04<00:08, 44.07it/s] 37%|███▋      | 223/608 [00:04<00:08, 44.59it/s] 38%|███▊      | 228/608 [00:05<00:08, 44.80it/s] 38%|███▊      | 233/608 [00:05<00:08, 45.01it/s] 39%|███▉      | 238/608 [00:05<00:08, 45.09it/s] 40%|███▉      | 243/608 [00:05<00:08, 45.15it/s] 41%|████      | 248/608 [00:05<00:08, 45.00it/s] 42%|████▏     | 253/608 [00:05<00:08, 42.28it/s] 42%|████▏     | 258/608 [00:05<00:08, 43.21it/s] 43%|████▎     | 263/608 [00:05<00:07, 44.01it/s] 44%|████▍     | 268/608 [00:05<00:07, 44.54it/s] 45%|████▍     | 273/608 [00:06<00:07, 44.88it/s] 46%|████▌     | 278/608 [00:06<00:07, 45.07it/s] 47%|████▋     | 283/608 [00:06<00:07, 45.21it/s] 47%|████▋     | 288/608 [00:06<00:07, 45.42it/s] 48%|████▊     | 293/608 [00:06<00:06, 45.04it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.10it/s] 50%|████▉     | 303/608 [00:06<00:06, 45.14it/s] 51%|█████     | 308/608 [00:06<00:06, 45.37it/s] 51%|█████▏    | 313/608 [00:06<00:06, 45.45it/s] 52%|█████▏    | 318/608 [00:07<00:06, 45.50it/s] 53%|█████▎    | 323/608 [00:07<00:06, 45.53it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.57it/s] 55%|█████▍    | 333/608 [00:07<00:06, 45.36it/s] 56%|█████▌    | 338/608 [00:07<00:05, 45.27it/s] 56%|█████▋    | 343/608 [00:07<00:07, 34.67it/s] 57%|█████▋    | 348/608 [00:07<00:06, 37.45it/s] 58%|█████▊    | 353/608 [00:07<00:06, 39.62it/s] 59%|█████▉    | 358/608 [00:08<00:06, 41.24it/s] 60%|█████▉    | 363/608 [00:08<00:05, 42.54it/s] 61%|██████    | 368/608 [00:08<00:06, 37.46it/s] 61%|██████▏   | 373/608 [00:08<00:05, 39.79it/s] 62%|██████▏   | 378/608 [00:08<00:08, 28.36it/s] 63%|██████▎   | 383/608 [00:08<00:06, 32.56it/s] 64%|██████▍   | 388/608 [00:08<00:06, 35.71it/s] 65%|██████▍   | 393/608 [00:09<00:06, 33.88it/s] 65%|██████▌   | 398/608 [00:09<00:05, 37.16it/s] 66%|██████▋   | 403/608 [00:09<00:05, 39.45it/s] 67%|██████▋   | 408/608 [00:09<00:04, 41.18it/s] 68%|██████▊   | 413/608 [00:09<00:04, 42.49it/s] 69%|██████▉   | 418/608 [00:09<00:04, 43.30it/s] 70%|██████▉   | 423/608 [00:09<00:04, 43.86it/s] 70%|███████   | 428/608 [00:09<00:04, 44.40it/s] 71%|███████   | 433/608 [00:09<00:03, 44.66it/s] 72%|███████▏  | 438/608 [00:10<00:03, 44.47it/s] 73%|███████▎  | 443/608 [00:10<00:03, 44.68it/s] 74%|███████▎  | 448/608 [00:10<00:03, 45.00it/s] 75%|███████▍  | 453/608 [00:10<00:03, 45.23it/s] 75%|███████▌  | 458/608 [00:10<00:03, 45.47it/s] 76%|███████▌  | 463/608 [00:10<00:03, 41.56it/s] 77%|███████▋  | 468/608 [00:10<00:03, 42.76it/s] 78%|███████▊  | 473/608 [00:10<00:03, 43.63it/s] 79%|███████▊  | 478/608 [00:10<00:02, 44.15it/s] 79%|███████▉  | 483/608 [00:11<00:02, 44.39it/s] 80%|████████  | 488/608 [00:11<00:02, 44.59it/s] 81%|████████  | 493/608 [00:11<00:02, 44.88it/s] 82%|████████▏ | 498/608 [00:11<00:02, 45.06it/s] 83%|████████▎ | 503/608 [00:11<00:02, 44.91it/s] 84%|████████▎ | 508/608 [00:11<00:02, 45.02it/s] 84%|████████▍ | 513/608 [00:11<00:02, 45.20it/s] 85%|████████▌ | 518/608 [00:11<00:01, 45.34it/s] 86%|████████▌ | 523/608 [00:11<00:01, 45.39it/s] 87%|████████▋ | 528/608 [00:12<00:01, 45.31it/s] 88%|████████▊ | 533/608 [00:12<00:01, 45.34it/s] 88%|████████▊ | 538/608 [00:12<00:01, 45.29it/s] 89%|████████▉ | 543/608 [00:12<00:01, 45.37it/s] 90%|█████████ | 548/608 [00:12<00:01, 45.31it/s] 91%|█████████ | 553/608 [00:12<00:01, 45.32it/s] 92%|█████████▏| 558/608 [00:12<00:01, 45.31it/s] 93%|█████████▎| 563/608 [00:12<00:00, 45.38it/s] 93%|█████████▎| 568/608 [00:12<00:00, 45.29it/s] 94%|█████████▍| 573/608 [00:13<00:00, 45.31it/s] 95%|█████████▌| 578/608 [00:13<00:00, 45.14it/s] 96%|█████████▌| 583/608 [00:13<00:00, 45.09it/s] 97%|█████████▋| 588/608 [00:13<00:00, 45.11it/s] 98%|█████████▊| 593/608 [00:13<00:00, 45.12it/s] 98%|█████████▊| 598/608 [00:13<00:00, 39.63it/s] 99%|█████████▉| 603/608 [00:13<00:00, 41.32it/s]100%|██████████| 608/608 [00:13<00:00, 42.55it/s]100%|██████████| 608/608 [00:13<00:00, 43.65it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:24:32,541 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:32,542 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:32,542 >>   eval_loss               =     0.9167
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:32,542 >>   eval_runtime            = 0:00:13.94
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:32,542 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:32,542 >>   eval_samples_per_second =    348.785
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:32,542 >>   eval_steps_per_second   =     43.598
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:24:32,542 >>   perplexity              =     2.5011
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:44,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:44,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:44,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:44,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:44,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:24:44,715 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:24:44,716 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:24:45,403 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:24:46,426 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:24:46,458 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:48,698 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:48,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:48,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:48,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:24:48,700 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:24:49,041 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:24:49,042 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:24:49,745 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:24:49,930 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:24:49,930 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-314
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-628
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-785
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-471
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/checkpoint-157
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.71it/s]Extractor Predicting: 6it [00:03,  1.76it/s]Extractor Predicting: 7it [00:04,  1.74it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:05,  1.78it/s]Extractor Predicting: 11it [00:06,  1.77it/s]Extractor Predicting: 12it [00:07,  1.79it/s]Extractor Predicting: 13it [00:07,  1.80it/s]Extractor Predicting: 14it [00:08,  1.75it/s]Extractor Predicting: 15it [00:08,  1.82it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.78it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:11,  1.73it/s]Extractor Predicting: 20it [00:11,  1.79it/s]Extractor Predicting: 21it [00:12,  1.79it/s]Extractor Predicting: 22it [00:12,  1.77it/s]Extractor Predicting: 23it [00:13,  1.69it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:14,  1.56it/s]Extractor Predicting: 26it [00:15,  1.57it/s]Extractor Predicting: 27it [00:15,  1.60it/s]Extractor Predicting: 28it [00:16,  1.60it/s]Extractor Predicting: 29it [00:17,  1.60it/s]Extractor Predicting: 30it [00:17,  1.64it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:18,  1.68it/s]Extractor Predicting: 33it [00:19,  1.68it/s]Extractor Predicting: 34it [00:20,  1.68it/s]Extractor Predicting: 35it [00:20,  1.63it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:21,  1.63it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:23,  1.64it/s]Extractor Predicting: 41it [00:24,  1.64it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:25,  1.62it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:26,  1.64it/s]Extractor Predicting: 46it [00:27,  1.61it/s]Extractor Predicting: 47it [00:28,  1.62it/s]Extractor Predicting: 48it [00:28,  1.63it/s]Extractor Predicting: 49it [00:29,  1.62it/s]Extractor Predicting: 50it [00:29,  1.63it/s]Extractor Predicting: 51it [00:30,  1.60it/s]Extractor Predicting: 52it [00:31,  1.61it/s]Extractor Predicting: 53it [00:31,  1.60it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:32,  1.69it/s]Extractor Predicting: 56it [00:33,  1.63it/s]Extractor Predicting: 57it [00:34,  1.63it/s]Extractor Predicting: 58it [00:34,  1.62it/s]Extractor Predicting: 59it [00:35,  1.63it/s]Extractor Predicting: 60it [00:36,  1.63it/s]Extractor Predicting: 61it [00:36,  1.51it/s]Extractor Predicting: 62it [00:37,  1.55it/s]Extractor Predicting: 63it [00:38,  1.58it/s]Extractor Predicting: 64it [00:38,  1.61it/s]Extractor Predicting: 65it [00:39,  1.61it/s]Extractor Predicting: 66it [00:39,  1.56it/s]Extractor Predicting: 67it [00:40,  1.60it/s]Extractor Predicting: 68it [00:41,  1.58it/s]Extractor Predicting: 69it [00:41,  1.59it/s]Extractor Predicting: 70it [00:42,  1.62it/s]Extractor Predicting: 71it [00:42,  1.66it/s]Extractor Predicting: 72it [00:43,  1.66it/s]Extractor Predicting: 73it [00:44,  1.68it/s]Extractor Predicting: 74it [00:45,  1.49it/s]Extractor Predicting: 75it [00:45,  1.55it/s]Extractor Predicting: 76it [00:46,  1.57it/s]Extractor Predicting: 77it [00:46,  1.58it/s]Extractor Predicting: 78it [00:47,  1.62it/s]Extractor Predicting: 79it [00:48,  1.64it/s]Extractor Predicting: 80it [00:48,  1.67it/s]Extractor Predicting: 81it [00:49,  1.65it/s]Extractor Predicting: 82it [00:49,  1.62it/s]Extractor Predicting: 83it [00:50,  1.61it/s]Extractor Predicting: 84it [00:51,  1.64it/s]Extractor Predicting: 85it [00:51,  1.63it/s]Extractor Predicting: 86it [00:52,  1.62it/s]Extractor Predicting: 87it [00:52,  1.64it/s]Extractor Predicting: 88it [00:53,  1.63it/s]Extractor Predicting: 89it [00:54,  1.56it/s]Extractor Predicting: 90it [00:54,  1.61it/s]Extractor Predicting: 91it [00:55,  1.61it/s]Extractor Predicting: 92it [00:56,  1.63it/s]Extractor Predicting: 93it [00:56,  1.66it/s]Extractor Predicting: 94it [00:57,  1.64it/s]Extractor Predicting: 95it [00:57,  1.60it/s]Extractor Predicting: 96it [00:58,  1.60it/s]Extractor Predicting: 97it [00:59,  1.61it/s]Extractor Predicting: 98it [00:59,  1.64it/s]Extractor Predicting: 99it [01:00,  1.66it/s]Extractor Predicting: 100it [01:00,  1.65it/s]Extractor Predicting: 101it [01:01,  1.64it/s]Extractor Predicting: 102it [01:02,  1.66it/s]Extractor Predicting: 103it [01:02,  1.69it/s]Extractor Predicting: 104it [01:03,  1.67it/s]Extractor Predicting: 105it [01:03,  1.65it/s]Extractor Predicting: 106it [01:04,  1.64it/s]Extractor Predicting: 107it [01:05,  1.65it/s]Extractor Predicting: 108it [01:05,  1.68it/s]Extractor Predicting: 109it [01:06,  1.64it/s]Extractor Predicting: 110it [01:06,  1.61it/s]Extractor Predicting: 111it [01:07,  1.61it/s]Extractor Predicting: 112it [01:08,  1.62it/s]Extractor Predicting: 113it [01:08,  1.64it/s]Extractor Predicting: 114it [01:09,  1.68it/s]Extractor Predicting: 115it [01:09,  1.73it/s]Extractor Predicting: 116it [01:10,  1.73it/s]Extractor Predicting: 117it [01:11,  1.69it/s]Extractor Predicting: 118it [01:11,  1.72it/s]Extractor Predicting: 119it [01:12,  1.70it/s]Extractor Predicting: 120it [01:12,  1.72it/s]Extractor Predicting: 121it [01:13,  1.69it/s]Extractor Predicting: 122it [01:14,  1.65it/s]Extractor Predicting: 123it [01:14,  1.58it/s]Extractor Predicting: 124it [01:15,  1.56it/s]Extractor Predicting: 125it [01:16,  1.59it/s]Extractor Predicting: 126it [01:16,  1.62it/s]Extractor Predicting: 127it [01:17,  1.63it/s]Extractor Predicting: 128it [01:17,  1.55it/s]Extractor Predicting: 129it [01:18,  1.59it/s]Extractor Predicting: 130it [01:19,  1.61it/s]Extractor Predicting: 131it [01:19,  1.67it/s]Extractor Predicting: 132it [01:20,  1.62it/s]Extractor Predicting: 133it [01:21,  1.56it/s]Extractor Predicting: 134it [01:21,  1.60it/s]Extractor Predicting: 135it [01:22,  1.64it/s]Extractor Predicting: 136it [01:22,  1.65it/s]Extractor Predicting: 137it [01:23,  1.67it/s]Extractor Predicting: 138it [01:24,  1.65it/s]Extractor Predicting: 139it [01:24,  1.65it/s]Extractor Predicting: 140it [01:25,  1.63it/s]Extractor Predicting: 141it [01:25,  1.67it/s]Extractor Predicting: 142it [01:26,  1.64it/s]Extractor Predicting: 143it [01:27,  1.67it/s]Extractor Predicting: 144it [01:27,  1.68it/s]Extractor Predicting: 145it [01:28,  1.68it/s]Extractor Predicting: 146it [01:28,  1.63it/s]Extractor Predicting: 147it [01:29,  1.56it/s]Extractor Predicting: 148it [01:30,  1.58it/s]Extractor Predicting: 149it [01:30,  1.57it/s]Extractor Predicting: 150it [01:31,  1.60it/s]Extractor Predicting: 151it [01:32,  1.59it/s]Extractor Predicting: 152it [01:32,  1.61it/s]Extractor Predicting: 153it [01:33,  1.59it/s]Extractor Predicting: 154it [01:33,  1.60it/s]Extractor Predicting: 155it [01:34,  1.63it/s]Extractor Predicting: 156it [01:35,  1.66it/s]Extractor Predicting: 157it [01:35,  1.62it/s]Extractor Predicting: 158it [01:36,  1.65it/s]Extractor Predicting: 159it [01:36,  1.63it/s]Extractor Predicting: 160it [01:37,  1.65it/s]Extractor Predicting: 161it [01:38,  1.62it/s]Extractor Predicting: 162it [01:38,  1.58it/s]Extractor Predicting: 163it [01:39,  1.60it/s]Extractor Predicting: 164it [01:40,  1.61it/s]Extractor Predicting: 165it [01:40,  1.64it/s]Extractor Predicting: 166it [01:41,  1.61it/s]Extractor Predicting: 167it [01:42,  1.44it/s]Extractor Predicting: 168it [01:42,  1.47it/s]Extractor Predicting: 169it [01:43,  1.46it/s]Extractor Predicting: 170it [01:44,  1.50it/s]Extractor Predicting: 171it [01:44,  1.51it/s]Extractor Predicting: 172it [01:45,  1.54it/s]Extractor Predicting: 173it [01:46,  1.52it/s]Extractor Predicting: 174it [01:46,  1.50it/s]Extractor Predicting: 175it [01:47,  1.55it/s]Extractor Predicting: 176it [01:48,  1.54it/s]Extractor Predicting: 177it [01:48,  1.50it/s]Extractor Predicting: 178it [01:49,  1.48it/s]Extractor Predicting: 179it [01:50,  1.47it/s]Extractor Predicting: 180it [01:50,  1.48it/s]Extractor Predicting: 181it [01:51,  1.51it/s]Extractor Predicting: 182it [01:52,  1.49it/s]Extractor Predicting: 183it [01:52,  1.50it/s]Extractor Predicting: 184it [01:53,  1.51it/s]Extractor Predicting: 185it [01:54,  1.54it/s]Extractor Predicting: 186it [01:54,  1.55it/s]Extractor Predicting: 187it [01:55,  1.66it/s]Extractor Predicting: 187it [01:55,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:00,635 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:00,653 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:00,654 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:00,654 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:00,654 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:27:01,464 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:27:01,466 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:27:02,129 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:27:03,263 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:27:03,263 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:06,302 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:06,314 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:06,314 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:06,314 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:06,314 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:27:07,097 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:27:07,099 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:27:07,710 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:27:07,955 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:27:07,955 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5843373493975904,
  "recall": 0.019942434210526317,
  "score": 0.03856858846918489,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:07,  1.61it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.45it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:14,  1.54it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:21,  1.65it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.64it/s]Extractor Predicting: 37it [00:22,  1.61it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:25,  1.65it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:26,  1.66it/s]Extractor Predicting: 44it [00:27,  1.64it/s]Extractor Predicting: 45it [00:27,  1.64it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:29,  1.57it/s]Extractor Predicting: 49it [00:30,  1.55it/s]Extractor Predicting: 50it [00:31,  1.56it/s]Extractor Predicting: 51it [00:31,  1.56it/s]Extractor Predicting: 52it [00:32,  1.57it/s]Extractor Predicting: 53it [00:32,  1.64it/s]Extractor Predicting: 54it [00:33,  1.64it/s]Extractor Predicting: 55it [00:34,  1.64it/s]Extractor Predicting: 56it [00:34,  1.65it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:35,  1.65it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:37,  1.66it/s]Extractor Predicting: 61it [00:37,  1.63it/s]Extractor Predicting: 62it [00:38,  1.64it/s]Extractor Predicting: 63it [00:39,  1.64it/s]Extractor Predicting: 64it [00:39,  1.63it/s]Extractor Predicting: 65it [00:40,  1.64it/s]Extractor Predicting: 66it [00:40,  1.66it/s]Extractor Predicting: 67it [00:41,  1.66it/s]Extractor Predicting: 68it [00:42,  1.57it/s]Extractor Predicting: 69it [00:42,  1.60it/s]Extractor Predicting: 70it [00:43,  1.55it/s]Extractor Predicting: 71it [00:44,  1.59it/s]Extractor Predicting: 72it [00:44,  1.60it/s]Extractor Predicting: 73it [00:45,  1.66it/s]Extractor Predicting: 74it [00:45,  1.66it/s]Extractor Predicting: 75it [00:46,  1.66it/s]Extractor Predicting: 76it [00:47,  1.62it/s]Extractor Predicting: 77it [00:47,  1.64it/s]Extractor Predicting: 78it [00:48,  1.69it/s]Extractor Predicting: 79it [00:48,  1.65it/s]Extractor Predicting: 80it [00:49,  1.67it/s]Extractor Predicting: 81it [00:50,  1.67it/s]Extractor Predicting: 82it [00:50,  1.67it/s]Extractor Predicting: 83it [00:51,  1.67it/s]Extractor Predicting: 84it [00:51,  1.60it/s]Extractor Predicting: 85it [00:52,  1.64it/s]Extractor Predicting: 86it [00:53,  1.67it/s]Extractor Predicting: 87it [00:53,  1.71it/s]Extractor Predicting: 88it [00:54,  1.74it/s]Extractor Predicting: 89it [00:54,  1.72it/s]Extractor Predicting: 90it [00:55,  1.66it/s]Extractor Predicting: 91it [00:55,  1.71it/s]Extractor Predicting: 92it [00:56,  1.73it/s]Extractor Predicting: 93it [00:57,  1.71it/s]Extractor Predicting: 94it [00:57,  1.73it/s]Extractor Predicting: 95it [00:58,  1.73it/s]Extractor Predicting: 96it [00:58,  1.73it/s]Extractor Predicting: 97it [00:59,  1.71it/s]Extractor Predicting: 98it [01:00,  1.69it/s]Extractor Predicting: 99it [01:00,  1.68it/s]Extractor Predicting: 100it [01:01,  1.71it/s]Extractor Predicting: 101it [01:01,  1.70it/s]Extractor Predicting: 102it [01:02,  1.69it/s]Extractor Predicting: 103it [01:02,  1.66it/s]Extractor Predicting: 104it [01:03,  1.67it/s]Extractor Predicting: 105it [01:04,  1.64it/s]Extractor Predicting: 106it [01:04,  1.64it/s]Extractor Predicting: 107it [01:05,  1.64it/s]Extractor Predicting: 108it [01:06,  1.65it/s]Extractor Predicting: 109it [01:06,  1.62it/s]Extractor Predicting: 110it [01:07,  1.62it/s]Extractor Predicting: 111it [01:07,  1.61it/s]Extractor Predicting: 112it [01:08,  1.64it/s]Extractor Predicting: 113it [01:09,  1.64it/s]Extractor Predicting: 114it [01:09,  1.66it/s]Extractor Predicting: 115it [01:10,  1.64it/s]Extractor Predicting: 116it [01:10,  1.64it/s]Extractor Predicting: 117it [01:11,  1.64it/s]Extractor Predicting: 118it [01:12,  1.64it/s]Extractor Predicting: 119it [01:12,  1.66it/s]Extractor Predicting: 120it [01:13,  1.63it/s]Extractor Predicting: 121it [01:14,  1.63it/s]Extractor Predicting: 122it [01:14,  1.64it/s]Extractor Predicting: 123it [01:15,  1.65it/s]Extractor Predicting: 124it [01:15,  1.66it/s]Extractor Predicting: 125it [01:16,  1.65it/s]Extractor Predicting: 126it [01:17,  1.61it/s]Extractor Predicting: 127it [01:17,  1.64it/s]Extractor Predicting: 128it [01:18,  1.61it/s]Extractor Predicting: 129it [01:18,  1.59it/s]Extractor Predicting: 130it [01:19,  1.63it/s]Extractor Predicting: 131it [01:20,  1.59it/s]Extractor Predicting: 132it [01:20,  1.60it/s]Extractor Predicting: 133it [01:21,  1.64it/s]Extractor Predicting: 134it [01:22,  1.62it/s]Extractor Predicting: 135it [01:22,  1.64it/s]Extractor Predicting: 136it [01:23,  1.62it/s]Extractor Predicting: 137it [01:23,  1.61it/s]Extractor Predicting: 138it [01:24,  1.40it/s]Extractor Predicting: 139it [01:25,  1.47it/s]Extractor Predicting: 140it [01:26,  1.51it/s]Extractor Predicting: 141it [01:26,  1.52it/s]Extractor Predicting: 142it [01:27,  1.54it/s]Extractor Predicting: 143it [01:27,  1.57it/s]Extractor Predicting: 144it [01:28,  1.59it/s]Extractor Predicting: 145it [01:29,  1.58it/s]Extractor Predicting: 146it [01:29,  1.57it/s]Extractor Predicting: 147it [01:30,  1.58it/s]Extractor Predicting: 148it [01:31,  1.61it/s]Extractor Predicting: 149it [01:31,  1.61it/s]Extractor Predicting: 150it [01:32,  1.57it/s]Extractor Predicting: 151it [01:32,  1.55it/s]Extractor Predicting: 152it [01:33,  1.58it/s]Extractor Predicting: 153it [01:34,  1.63it/s]Extractor Predicting: 154it [01:34,  1.65it/s]Extractor Predicting: 155it [01:35,  1.67it/s]Extractor Predicting: 156it [01:35,  1.61it/s]Extractor Predicting: 157it [01:36,  1.63it/s]Extractor Predicting: 158it [01:37,  1.63it/s]Extractor Predicting: 159it [01:37,  1.62it/s]Extractor Predicting: 160it [01:38,  1.62it/s]Extractor Predicting: 161it [01:39,  1.62it/s]Extractor Predicting: 162it [01:39,  1.61it/s]Extractor Predicting: 163it [01:40,  1.63it/s]Extractor Predicting: 164it [01:40,  1.63it/s]Extractor Predicting: 165it [01:41,  1.64it/s]Extractor Predicting: 166it [01:42,  1.64it/s]Extractor Predicting: 167it [01:42,  1.65it/s]Extractor Predicting: 168it [01:43,  1.63it/s]Extractor Predicting: 169it [01:43,  1.62it/s]Extractor Predicting: 170it [01:44,  1.64it/s]Extractor Predicting: 171it [01:45,  1.70it/s]Extractor Predicting: 172it [01:45,  1.69it/s]Extractor Predicting: 173it [01:46,  1.67it/s]Extractor Predicting: 174it [01:46,  1.64it/s]Extractor Predicting: 175it [01:47,  1.59it/s]Extractor Predicting: 176it [01:48,  1.57it/s]Extractor Predicting: 177it [01:48,  1.60it/s]Extractor Predicting: 178it [01:49,  1.59it/s]Extractor Predicting: 179it [01:50,  1.59it/s]Extractor Predicting: 180it [01:50,  1.57it/s]Extractor Predicting: 181it [01:51,  1.55it/s]Extractor Predicting: 182it [01:52,  1.56it/s]Extractor Predicting: 183it [01:52,  1.55it/s]Extractor Predicting: 184it [01:53,  1.55it/s]Extractor Predicting: 185it [01:54,  1.56it/s]Extractor Predicting: 186it [01:54,  1.57it/s]Extractor Predicting: 187it [01:55,  1.55it/s]Extractor Predicting: 188it [01:55,  1.58it/s]Extractor Predicting: 189it [01:56,  1.59it/s]Extractor Predicting: 190it [01:57,  1.58it/s]Extractor Predicting: 191it [01:57,  1.58it/s]Extractor Predicting: 192it [01:58,  1.58it/s]Extractor Predicting: 193it [01:59,  1.58it/s]Extractor Predicting: 194it [01:59,  1.59it/s]Extractor Predicting: 195it [02:00,  1.63it/s]Extractor Predicting: 196it [02:00,  1.65it/s]Extractor Predicting: 197it [02:01,  1.64it/s]Extractor Predicting: 198it [02:02,  1.64it/s]Extractor Predicting: 199it [02:02,  1.64it/s]Extractor Predicting: 200it [02:03,  1.66it/s]Extractor Predicting: 201it [02:03,  1.63it/s]Extractor Predicting: 202it [02:04,  1.62it/s]Extractor Predicting: 203it [02:05,  1.62it/s]Extractor Predicting: 204it [02:05,  1.62it/s]Extractor Predicting: 205it [02:06,  1.62it/s]Extractor Predicting: 206it [02:07,  1.63it/s]Extractor Predicting: 207it [02:07,  1.62it/s]Extractor Predicting: 208it [02:08,  1.66it/s]Extractor Predicting: 209it [02:08,  1.68it/s]Extractor Predicting: 210it [02:09,  1.65it/s]Extractor Predicting: 211it [02:10,  1.65it/s]Extractor Predicting: 212it [02:10,  1.66it/s]Extractor Predicting: 213it [02:11,  1.63it/s]Extractor Predicting: 214it [02:11,  1.55it/s]Extractor Predicting: 215it [02:12,  1.57it/s]Extractor Predicting: 216it [02:13,  1.58it/s]Extractor Predicting: 217it [02:13,  1.63it/s]Extractor Predicting: 218it [02:14,  1.62it/s]Extractor Predicting: 219it [02:15,  1.56it/s]Extractor Predicting: 220it [02:15,  1.62it/s]Extractor Predicting: 221it [02:16,  1.59it/s]Extractor Predicting: 222it [02:16,  1.60it/s]Extractor Predicting: 223it [02:17,  1.61it/s]Extractor Predicting: 224it [02:18,  1.61it/s]Extractor Predicting: 225it [02:18,  1.65it/s]Extractor Predicting: 226it [02:19,  1.66it/s]Extractor Predicting: 227it [02:19,  1.68it/s]Extractor Predicting: 228it [02:20,  1.69it/s]Extractor Predicting: 229it [02:21,  1.67it/s]Extractor Predicting: 230it [02:21,  1.66it/s]Extractor Predicting: 231it [02:22,  1.66it/s]Extractor Predicting: 232it [02:22,  1.66it/s]Extractor Predicting: 233it [02:23,  1.66it/s]Extractor Predicting: 234it [02:24,  1.63it/s]Extractor Predicting: 235it [02:24,  1.57it/s]Extractor Predicting: 236it [02:25,  1.60it/s]Extractor Predicting: 237it [02:26,  1.59it/s]Extractor Predicting: 238it [02:26,  1.62it/s]Extractor Predicting: 239it [02:27,  1.62it/s]Extractor Predicting: 240it [02:27,  1.63it/s]Extractor Predicting: 241it [02:28,  1.67it/s]Extractor Predicting: 242it [02:29,  1.65it/s]Extractor Predicting: 243it [02:29,  1.69it/s]Extractor Predicting: 244it [02:30,  1.69it/s]Extractor Predicting: 245it [02:30,  1.66it/s]Extractor Predicting: 246it [02:31,  1.66it/s]Extractor Predicting: 247it [02:32,  1.65it/s]Extractor Predicting: 248it [02:32,  1.68it/s]Extractor Predicting: 249it [02:33,  1.67it/s]Extractor Predicting: 250it [02:34,  1.51it/s]Extractor Predicting: 251it [02:34,  1.52it/s]Extractor Predicting: 252it [02:35,  1.58it/s]Extractor Predicting: 253it [02:35,  1.66it/s]Extractor Predicting: 254it [02:36,  1.64it/s]Extractor Predicting: 255it [02:37,  1.66it/s]Extractor Predicting: 256it [02:37,  1.66it/s]Extractor Predicting: 257it [02:38,  1.64it/s]Extractor Predicting: 258it [02:38,  1.66it/s]Extractor Predicting: 259it [02:39,  1.66it/s]Extractor Predicting: 260it [02:40,  1.66it/s]Extractor Predicting: 261it [02:40,  1.63it/s]Extractor Predicting: 262it [02:41,  1.63it/s]Extractor Predicting: 263it [02:41,  1.58it/s]Extractor Predicting: 264it [02:42,  1.59it/s]Extractor Predicting: 265it [02:43,  1.59it/s]Extractor Predicting: 266it [02:43,  1.58it/s]Extractor Predicting: 267it [02:44,  1.59it/s]Extractor Predicting: 268it [02:45,  1.60it/s]Extractor Predicting: 269it [02:45,  1.54it/s]Extractor Predicting: 270it [02:46,  1.57it/s]Extractor Predicting: 271it [02:47,  1.59it/s]Extractor Predicting: 272it [02:47,  1.64it/s]Extractor Predicting: 273it [02:48,  1.62it/s]Extractor Predicting: 274it [02:48,  1.59it/s]Extractor Predicting: 275it [02:49,  1.59it/s]Extractor Predicting: 276it [02:50,  1.63it/s]Extractor Predicting: 277it [02:50,  1.62it/s]Extractor Predicting: 278it [02:51,  1.63it/s]Extractor Predicting: 279it [02:51,  1.60it/s]Extractor Predicting: 280it [02:52,  1.61it/s]Extractor Predicting: 281it [02:53,  1.61it/s]Extractor Predicting: 282it [02:53,  1.62it/s]Extractor Predicting: 283it [02:54,  1.62it/s]Extractor Predicting: 284it [02:55,  1.62it/s]Extractor Predicting: 285it [02:55,  1.64it/s]Extractor Predicting: 286it [02:56,  1.64it/s]Extractor Predicting: 287it [02:56,  1.60it/s]Extractor Predicting: 288it [02:57,  1.61it/s]Extractor Predicting: 289it [02:58,  1.59it/s]Extractor Predicting: 290it [02:58,  1.58it/s]Extractor Predicting: 291it [02:59,  1.59it/s]Extractor Predicting: 292it [03:00,  1.61it/s]Extractor Predicting: 293it [03:00,  1.60it/s]Extractor Predicting: 294it [03:01,  1.56it/s]Extractor Predicting: 295it [03:01,  1.58it/s]Extractor Predicting: 296it [03:02,  1.59it/s]Extractor Predicting: 297it [03:03,  1.58it/s]Extractor Predicting: 298it [03:03,  1.59it/s]Extractor Predicting: 299it [03:04,  1.55it/s]Extractor Predicting: 300it [03:05,  1.52it/s]Extractor Predicting: 301it [03:05,  1.48it/s]Extractor Predicting: 302it [03:06,  1.47it/s]Extractor Predicting: 303it [03:07,  1.51it/s]Extractor Predicting: 304it [03:08,  1.45it/s]Extractor Predicting: 305it [03:08,  1.43it/s]Extractor Predicting: 306it [03:09,  1.50it/s]Extractor Predicting: 307it [03:09,  1.54it/s]Extractor Predicting: 308it [03:10,  1.57it/s]Extractor Predicting: 309it [03:11,  1.62it/s]Extractor Predicting: 310it [03:11,  1.61it/s]Extractor Predicting: 311it [03:12,  1.57it/s]Extractor Predicting: 312it [03:13,  1.59it/s]Extractor Predicting: 313it [03:13,  1.60it/s]Extractor Predicting: 314it [03:14,  1.59it/s]Extractor Predicting: 315it [03:14,  1.66it/s]Extractor Predicting: 316it [03:15,  1.67it/s]Extractor Predicting: 317it [03:16,  1.64it/s]Extractor Predicting: 318it [03:16,  1.60it/s]Extractor Predicting: 319it [03:17,  1.59it/s]Extractor Predicting: 320it [03:17,  1.62it/s]Extractor Predicting: 321it [03:18,  1.63it/s]Extractor Predicting: 322it [03:19,  1.67it/s]Extractor Predicting: 323it [03:19,  1.64it/s]Extractor Predicting: 324it [03:20,  1.66it/s]Extractor Predicting: 325it [03:20,  1.65it/s]Extractor Predicting: 326it [03:21,  1.66it/s]Extractor Predicting: 327it [03:22,  1.67it/s]Extractor Predicting: 328it [03:22,  1.58it/s]Extractor Predicting: 329it [03:23,  1.59it/s]Extractor Predicting: 330it [03:24,  1.59it/s]Extractor Predicting: 331it [03:24,  1.60it/s]Extractor Predicting: 332it [03:25,  1.63it/s]Extractor Predicting: 333it [03:25,  1.56it/s]Extractor Predicting: 334it [03:26,  1.58it/s]Extractor Predicting: 335it [03:27,  1.60it/s]Extractor Predicting: 336it [03:27,  1.63it/s]Extractor Predicting: 337it [03:28,  1.64it/s]Extractor Predicting: 338it [03:29,  1.62it/s]Extractor Predicting: 339it [03:29,  1.63it/s]Extractor Predicting: 340it [03:30,  1.65it/s]Extractor Predicting: 341it [03:30,  1.66it/s]Extractor Predicting: 342it [03:31,  1.64it/s]Extractor Predicting: 343it [03:32,  1.61it/s]Extractor Predicting: 344it [03:32,  1.67it/s]Extractor Predicting: 345it [03:33,  1.69it/s]Extractor Predicting: 346it [03:33,  1.71it/s]Extractor Predicting: 347it [03:34,  1.76it/s]Extractor Predicting: 348it [03:34,  1.73it/s]Extractor Predicting: 349it [03:35,  1.71it/s]Extractor Predicting: 350it [03:36,  1.71it/s]Extractor Predicting: 351it [03:36,  1.72it/s]Extractor Predicting: 352it [03:37,  1.71it/s]Extractor Predicting: 353it [03:37,  1.74it/s]Extractor Predicting: 354it [03:38,  1.73it/s]Extractor Predicting: 355it [03:38,  1.72it/s]Extractor Predicting: 356it [03:39,  1.75it/s]Extractor Predicting: 357it [03:40,  1.73it/s]Extractor Predicting: 358it [03:40,  1.72it/s]Extractor Predicting: 359it [03:41,  1.71it/s]Extractor Predicting: 360it [03:41,  1.70it/s]Extractor Predicting: 361it [03:42,  1.68it/s]Extractor Predicting: 362it [03:43,  1.70it/s]Extractor Predicting: 363it [03:43,  1.72it/s]Extractor Predicting: 364it [03:44,  1.76it/s]Extractor Predicting: 365it [03:44,  1.70it/s]Extractor Predicting: 366it [03:45,  1.66it/s]Extractor Predicting: 367it [03:46,  1.61it/s]Extractor Predicting: 368it [03:46,  1.59it/s]Extractor Predicting: 369it [03:47,  1.62it/s]Extractor Predicting: 370it [03:48,  1.59it/s]Extractor Predicting: 371it [03:48,  1.62it/s]Extractor Predicting: 372it [03:49,  1.64it/s]Extractor Predicting: 373it [03:49,  1.65it/s]Extractor Predicting: 374it [03:50,  1.63it/s]Extractor Predicting: 375it [03:51,  1.62it/s]Extractor Predicting: 376it [03:51,  1.61it/s]Extractor Predicting: 377it [03:52,  1.60it/s]Extractor Predicting: 378it [03:52,  1.60it/s]Extractor Predicting: 379it [03:53,  1.59it/s]Extractor Predicting: 380it [03:54,  1.60it/s]Extractor Predicting: 381it [03:54,  1.63it/s]Extractor Predicting: 382it [03:55,  1.66it/s]Extractor Predicting: 383it [03:55,  1.66it/s]Extractor Predicting: 384it [03:56,  1.66it/s]Extractor Predicting: 385it [03:57,  1.65it/s]Extractor Predicting: 386it [03:57,  1.67it/s]Extractor Predicting: 387it [03:58,  1.67it/s]Extractor Predicting: 388it [03:58,  1.71it/s]Extractor Predicting: 389it [03:59,  1.66it/s]Extractor Predicting: 390it [04:00,  1.62it/s]Extractor Predicting: 391it [04:01,  1.44it/s]Extractor Predicting: 392it [04:01,  1.54it/s]Extractor Predicting: 393it [04:02,  1.61it/s]Extractor Predicting: 394it [04:02,  1.64it/s]Extractor Predicting: 395it [04:03,  1.67it/s]Extractor Predicting: 396it [04:03,  1.71it/s]Extractor Predicting: 397it [04:04,  1.69it/s]Extractor Predicting: 398it [04:05,  1.69it/s]Extractor Predicting: 399it [04:05,  1.70it/s]Extractor Predicting: 400it [04:06,  1.67it/s]Extractor Predicting: 401it [04:06,  1.69it/s]Extractor Predicting: 402it [04:07,  1.71it/s]Extractor Predicting: 403it [04:08,  1.72it/s]Extractor Predicting: 404it [04:08,  1.72it/s]Extractor Predicting: 405it [04:09,  1.68it/s]Extractor Predicting: 406it [04:09,  1.69it/s]Extractor Predicting: 407it [04:10,  1.69it/s]Extractor Predicting: 408it [04:10,  1.70it/s]Extractor Predicting: 409it [04:11,  1.53it/s]Extractor Predicting: 410it [04:12,  1.57it/s]Extractor Predicting: 411it [04:12,  1.59it/s]Extractor Predicting: 412it [04:13,  1.60it/s]Extractor Predicting: 413it [04:14,  1.54it/s]Extractor Predicting: 414it [04:14,  1.52it/s]Extractor Predicting: 415it [04:15,  1.49it/s]Extractor Predicting: 416it [04:16,  1.50it/s]Extractor Predicting: 417it [04:16,  1.52it/s]Extractor Predicting: 418it [04:17,  1.55it/s]Extractor Predicting: 419it [04:18,  1.52it/s]Extractor Predicting: 420it [04:18,  1.51it/s]Extractor Predicting: 421it [04:19,  1.52it/s]Extractor Predicting: 422it [04:20,  1.53it/s]Extractor Predicting: 423it [04:20,  1.57it/s]Extractor Predicting: 424it [04:21,  1.51it/s]Extractor Predicting: 425it [04:22,  1.52it/s]Extractor Predicting: 426it [04:22,  1.52it/s]Extractor Predicting: 427it [04:23,  1.54it/s]Extractor Predicting: 428it [04:24,  1.56it/s]Extractor Predicting: 429it [04:24,  1.51it/s]Extractor Predicting: 430it [04:25,  1.54it/s]Extractor Predicting: 431it [04:26,  1.56it/s]Extractor Predicting: 432it [04:26,  1.56it/s]Extractor Predicting: 433it [04:27,  1.57it/s]Extractor Predicting: 434it [04:27,  1.57it/s]Extractor Predicting: 435it [04:28,  1.59it/s]Extractor Predicting: 436it [04:29,  1.63it/s]Extractor Predicting: 437it [04:29,  1.62it/s]Extractor Predicting: 438it [04:30,  1.63it/s]Extractor Predicting: 439it [04:31,  1.61it/s]Extractor Predicting: 440it [04:31,  1.61it/s]Extractor Predicting: 441it [04:32,  1.59it/s]Extractor Predicting: 442it [04:32,  1.60it/s]Extractor Predicting: 443it [04:33,  1.58it/s]Extractor Predicting: 444it [04:34,  1.58it/s]Extractor Predicting: 445it [04:34,  1.56it/s]Extractor Predicting: 446it [04:35,  1.57it/s]Extractor Predicting: 447it [04:36,  1.58it/s]Extractor Predicting: 448it [04:36,  1.57it/s]Extractor Predicting: 449it [04:37,  1.59it/s]Extractor Predicting: 450it [04:37,  1.62it/s]Extractor Predicting: 451it [04:38,  1.61it/s]Extractor Predicting: 452it [04:39,  1.59it/s]Extractor Predicting: 453it [04:39,  1.59it/s]Extractor Predicting: 454it [04:40,  1.57it/s]Extractor Predicting: 455it [04:41,  1.55it/s]Extractor Predicting: 456it [04:41,  1.58it/s]Extractor Predicting: 457it [04:42,  1.55it/s]Extractor Predicting: 458it [04:43,  1.57it/s]Extractor Predicting: 459it [04:43,  1.55it/s]Extractor Predicting: 460it [04:44,  1.55it/s]Extractor Predicting: 461it [04:45,  1.58it/s]Extractor Predicting: 462it [04:45,  1.58it/s]Extractor Predicting: 463it [04:46,  1.55it/s]Extractor Predicting: 464it [04:46,  1.52it/s]Extractor Predicting: 465it [04:47,  1.52it/s]Extractor Predicting: 466it [04:48,  1.55it/s]Extractor Predicting: 467it [04:48,  1.59it/s]Extractor Predicting: 468it [04:49,  1.60it/s]Extractor Predicting: 469it [04:50,  1.66it/s]Extractor Predicting: 470it [04:50,  1.63it/s]Extractor Predicting: 471it [04:51,  1.60it/s]Extractor Predicting: 472it [04:51,  1.60it/s]Extractor Predicting: 473it [04:52,  1.58it/s]Extractor Predicting: 474it [04:53,  1.57it/s]Extractor Predicting: 475it [04:53,  1.57it/s]Extractor Predicting: 476it [04:54,  1.59it/s]Extractor Predicting: 477it [04:55,  1.58it/s]Extractor Predicting: 478it [04:55,  1.58it/s]Extractor Predicting: 479it [04:56,  1.53it/s]Extractor Predicting: 480it [04:57,  1.49it/s]Extractor Predicting: 481it [04:57,  1.48it/s]Extractor Predicting: 482it [04:58,  1.48it/s]Extractor Predicting: 483it [04:59,  1.46it/s]Extractor Predicting: 484it [04:59,  1.45it/s]Extractor Predicting: 485it [05:00,  1.46it/s]Extractor Predicting: 486it [05:01,  1.49it/s]Extractor Predicting: 487it [05:01,  1.51it/s]Extractor Predicting: 488it [05:02,  1.49it/s]Extractor Predicting: 489it [05:03,  1.52it/s]Extractor Predicting: 490it [05:03,  1.48it/s]Extractor Predicting: 491it [05:04,  1.48it/s]Extractor Predicting: 492it [05:05,  1.46it/s]Extractor Predicting: 493it [05:05,  1.48it/s]Extractor Predicting: 494it [05:06,  1.48it/s]Extractor Predicting: 495it [05:07,  1.48it/s]Extractor Predicting: 496it [05:08,  1.48it/s]Extractor Predicting: 497it [05:08,  1.47it/s]Extractor Predicting: 498it [05:09,  1.49it/s]Extractor Predicting: 499it [05:10,  1.45it/s]Extractor Predicting: 500it [05:10,  1.48it/s]Extractor Predicting: 501it [05:11,  1.49it/s]Extractor Predicting: 502it [05:12,  1.46it/s]Extractor Predicting: 503it [05:12,  1.45it/s]Extractor Predicting: 504it [05:13,  1.27it/s]Extractor Predicting: 505it [05:14,  1.31it/s]Extractor Predicting: 506it [05:15,  1.32it/s]Extractor Predicting: 507it [05:15,  1.38it/s]Extractor Predicting: 508it [05:16,  1.56it/s]Extractor Predicting: 508it [05:16,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:42,699 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:42,733 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:42,734 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:42,734 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:42,734 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:32:43,550 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:32:43,551 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:32:44,146 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:32:45,298 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:32:45,298 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:48,439 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:48,457 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:48,457 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:48,457 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:32:48,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:32:49,241 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:32:49,243 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:32:49,876 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:32:50,071 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:32:50,071 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.212298682284041,
  "recall": 0.02380756916509318,
  "score": 0.042813907138111756,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:17,  1.54it/s]Extractor Predicting: 29it [00:18,  1.51it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:20,  1.63it/s]Extractor Predicting: 33it [00:21,  1.65it/s]Extractor Predicting: 34it [00:21,  1.62it/s]Extractor Predicting: 35it [00:22,  1.62it/s]Extractor Predicting: 36it [00:22,  1.64it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:24,  1.63it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.63it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:26,  1.66it/s]Extractor Predicting: 43it [00:27,  1.66it/s]Extractor Predicting: 44it [00:27,  1.68it/s]Extractor Predicting: 45it [00:28,  1.65it/s]Extractor Predicting: 46it [00:28,  1.65it/s]Extractor Predicting: 47it [00:29,  1.65it/s]Extractor Predicting: 48it [00:30,  1.64it/s]Extractor Predicting: 49it [00:30,  1.61it/s]Extractor Predicting: 50it [00:31,  1.60it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:32,  1.59it/s]Extractor Predicting: 53it [00:33,  1.60it/s]Extractor Predicting: 54it [00:34,  1.53it/s]Extractor Predicting: 55it [00:34,  1.56it/s]Extractor Predicting: 56it [00:35,  1.59it/s]Extractor Predicting: 57it [00:35,  1.59it/s]Extractor Predicting: 58it [00:36,  1.60it/s]Extractor Predicting: 59it [00:37,  1.61it/s]Extractor Predicting: 60it [00:37,  1.59it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:39,  1.61it/s]Extractor Predicting: 63it [00:39,  1.62it/s]Extractor Predicting: 64it [00:40,  1.62it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:41,  1.48it/s]Extractor Predicting: 67it [00:42,  1.48it/s]Extractor Predicting: 68it [00:43,  1.46it/s]Extractor Predicting: 69it [00:43,  1.46it/s]Extractor Predicting: 70it [00:44,  1.47it/s]Extractor Predicting: 71it [00:45,  1.48it/s]Extractor Predicting: 72it [00:45,  1.50it/s]Extractor Predicting: 73it [00:46,  1.50it/s]Extractor Predicting: 74it [00:47,  1.47it/s]Extractor Predicting: 75it [00:47,  1.67it/s]Extractor Predicting: 75it [00:47,  1.58it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:33:41,252 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:33:41,307 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:33:41,421 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:33:41,422 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:33:41,507 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:33:53,298 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:33:53,326 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:33:53,457 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:33:53,457 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_3/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:33:53,541 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:53,597 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:53,597 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:53,597 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:53,598 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:53,598 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:53,598 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.1984732824427481,
  "recall": 0.00654911838790932,
  "score": 0.012679834186783713,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:33:53,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:54,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:55,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:55,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:56,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:56,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:57,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:57,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:58,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:59,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:59,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:00,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:00,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:01,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:02,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:02,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:03,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:03,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:04,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:04,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:11<03:38, 11.52s/it][WARNING|generation_utils.py:914] 2023-08-29 00:34:05,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:06,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:06,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:07,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:07,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:08,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:08,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:09,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:09,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:10,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:10,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:11,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:12,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:12,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:13,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:13,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:14,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:14,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:15,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:16,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:16,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:17,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:17,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:24<03:41, 12.28s/it][WARNING|generation_utils.py:914] 2023-08-29 00:34:18,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:19,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:19,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:20,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:21,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:21,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:22,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:22,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:23,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:24,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:24,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:25,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:26,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:27,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:27,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:28,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:28,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:29,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:30,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:30,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:31,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:32,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:33,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:39<03:52, 13.69s/it][WARNING|generation_utils.py:914] 2023-08-29 00:34:33,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:34,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:34,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:35,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:36,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:36,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:37,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:38,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:38,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:39,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:40,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:40,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:41,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:41,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:42,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:43,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:43,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:44,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:44,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:45,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:46,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:52<03:35, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-29 00:34:46,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:47,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:47,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:48,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:49,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:49,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:50,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:50,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:51,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:52,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:52,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:53,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:53,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:54,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:54,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:55,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:56,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:56,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:57,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:57,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:58,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:58,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:59,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:00,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:06<03:23, 13.58s/it][WARNING|generation_utils.py:914] 2023-08-29 00:35:00,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:01,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:01,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:02,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:03,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:03,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:04,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:05,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:05,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:06,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:06,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:07,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:08,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:08,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:09,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:09,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:10,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:11,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:11,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:12,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:13,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:13,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:14,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:20<03:13, 13.79s/it][WARNING|generation_utils.py:914] 2023-08-29 00:35:14,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:15,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:16,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:16,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:17,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:17,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:18,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:19,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:19,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:20,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:20,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:21,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:22,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:22,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:23,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:23,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:24,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:24,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:25,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:25,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:26,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:26,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:33<02:54, 13.40s/it][WARNING|generation_utils.py:914] 2023-08-29 00:35:27,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:28,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:28,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:29,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:29,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:30,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:31,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:31,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:32,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:33,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:33,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:34,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:34,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:35,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:36,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:36,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:37,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:38,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:38,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:39,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:39,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:46<02:40, 13.34s/it][WARNING|generation_utils.py:914] 2023-08-29 00:35:40,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:41,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:41,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:42,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:43,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:43,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:44,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:44,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:45,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:46,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:46,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:47,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:48,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:49,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:49,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:50,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:51,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:51,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:52,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:53,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:53,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:00<02:28, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-29 00:35:54,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:55,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:55,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:56,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:56,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:57,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:57,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:58,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:59,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:59,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:00,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:00,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:01,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:02,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:02,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:03,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:03,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:04,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:05,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:05,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:06,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:07,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:13<02:14, 13.46s/it][WARNING|generation_utils.py:914] 2023-08-29 00:36:07,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:08,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:08,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:09,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:10,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:10,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:11,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:12,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:12,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:13,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:14,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:14,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:15,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:15,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:16,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:17,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:17,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:18,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:18,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:19,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:20,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:21,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:21,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:28<02:04, 13.83s/it][WARNING|generation_utils.py:914] 2023-08-29 00:36:22,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:23,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:23,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:24,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:24,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:25,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:26,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:26,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:27,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:28,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:28,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:29,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:29,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:30,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:31,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:31,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:32,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:32,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:33,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:34,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:34,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:41<01:48, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-29 00:36:35,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:36,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:36,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:37,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:37,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:38,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:38,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:39,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:40,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:40,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:41,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:41,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:42,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:43,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:43,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:44,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:44,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:45,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:46,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:46,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:47,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:47,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:48,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:55<01:35, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-29 00:36:49,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:49,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:50,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:50,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:51,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:52,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:52,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:53,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:54,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:54,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:55,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:55,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:56,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:57,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:57,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:58,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:58,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:59,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:59,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:00,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:01,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:01,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:08<01:20, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-29 00:37:02,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:02,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:03,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:04,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:04,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:05,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:05,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:06,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:06,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:07,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:08,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:08,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:09,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:09,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:10,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:11,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:11,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:12,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:12,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:13,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:20<01:04, 12.93s/it][WARNING|generation_utils.py:914] 2023-08-29 00:37:13,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:14,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:15,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:15,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:16,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:17,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:17,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:18,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:19,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:19,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:20,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:21,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:21,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:22,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:23,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:23,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:24,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:25,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:25,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:26,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:27,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:27,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:28,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:35<00:54, 13.56s/it][WARNING|generation_utils.py:914] 2023-08-29 00:37:29,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:29,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:30,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:30,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:31,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:31,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:32,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:32,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:33,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:33,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:34,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:35,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:35,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:36,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:36,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:37,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:37,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:38,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:39,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:39,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:40,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:47<00:39, 13.17s/it][WARNING|generation_utils.py:914] 2023-08-29 00:37:41,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:41,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:42,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:43,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:43,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:44,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:44,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:45,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:45,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:46,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:47,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:47,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:48,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:48,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:49,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:49,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:50,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:51,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:51,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:52,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:52,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:53,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:00<00:26, 13.06s/it][WARNING|generation_utils.py:914] 2023-08-29 00:37:54,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:54,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:55,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:55,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:56,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:57,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:57,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:58,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:58,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:59,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:00,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:00,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:01,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:01,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:02,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:03,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:03,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:04,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:04,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:05,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:06,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:06,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:13<00:13, 13.11s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:07,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:07,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:08,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:09,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:09,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:10,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:11,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:11,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:12,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:13,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:13,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:14,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:14,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:15,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:16,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:16,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:17,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:18,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:18,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:19,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:20,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:20,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:21,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:27<00:00, 13.56s/it]Generating: 100%|██████████| 20/20 [04:27<00:00, 13.40s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:30,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:31,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:31,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:31,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:31,005 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:38:31,855 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:38:31,856 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:38:32,516 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:38:33,739 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:38:33,739 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:36,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:36,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:36,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:36,831 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:36,832 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:38:37,683 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:38:37,684 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:38:38,389 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:38:38,715 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:38:38,715 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : student .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 419, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : winner .', 'success_rate': 0.796875, 'errors': {'', "('Albert Pujols', 'winner', '', 'After losing his first two games to the Red Sox , Bobroes manager Bob Melvin was able to improve the team with his acquisition of Albert Pujols .')", "('New York Yankees', 'winner', '', 'In a close round series against the New York Yankees , he would lose to a Yankee starter , who was out for the season .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : conflict .', 'success_rate': 0.8233695652173914, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : continent .', 'success_rate': 0.8721590909090909, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : founded by .', 'success_rate': 0.8707386363636364, 'errors': {'', "('local newspaper', 'founded by', '', 'In 1866 he founded a local newspaper which ran for governor and was a major player in the union .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8315217391304348, 'errors': {'', "('Lieutenant Governor of New York', 'given name', '', 'He was born in York in 1775 and served as Lieutenant Governor of New York , and served as Governor of Pennsylvania from 1778 until 1786 .')", 'not enough values to unpack (expected 2, got 1)', "('first Australian', 'given name', '', 'In 2004 , he was chosen twice as the first Australian to make the jump to the finals and in 2006 as one of six New South Wales n players to take part in the 2002 2004 World Cup .')", 'too many values to unpack (expected 2)', "('St. Josephs Academy', 'given name', '', 'He attended St. Joseph s Academy from 1961 to 1960 , graduating from Columbia University and the Graduate Center of Music and Drama in 1967 .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9122023809523809, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : movement .', 'success_rate': 0.8301630434782609, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8735795454545454, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9515625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.8383152173913043, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : producer .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : record label .', 'success_rate': 0.8877840909090909, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/2_ext.jsonl'}}
estimate vocab size: 13797
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13897, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.64it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:01,  1.58it/s]Extractor Estimating: 4it [00:02,  1.60it/s]Extractor Estimating: 5it [00:03,  1.60it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.58it/s]Extractor Estimating: 8it [00:05,  1.61it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.63it/s]Extractor Estimating: 11it [00:06,  1.55it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.61it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.61it/s]Extractor Estimating: 16it [00:10,  1.56it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.59it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:12,  1.64it/s]Extractor Estimating: 21it [00:13,  1.63it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:14,  1.61it/s]Extractor Estimating: 25it [00:15,  1.64it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:16,  1.64it/s]Extractor Estimating: 28it [00:17,  1.66it/s]Extractor Estimating: 29it [00:17,  1.68it/s]Extractor Estimating: 30it [00:18,  1.77it/s]Extractor Estimating: 31it [00:19,  1.78it/s]Extractor Estimating: 32it [00:19,  1.75it/s]Extractor Estimating: 33it [00:20,  1.70it/s]Extractor Estimating: 34it [00:20,  1.73it/s]Extractor Estimating: 35it [00:21,  1.74it/s]Extractor Estimating: 36it [00:21,  1.72it/s]Extractor Estimating: 37it [00:22,  1.73it/s]Extractor Estimating: 38it [00:23,  1.74it/s]Extractor Estimating: 39it [00:23,  1.77it/s]Extractor Estimating: 40it [00:24,  1.75it/s]Extractor Estimating: 41it [00:24,  1.71it/s]Extractor Estimating: 42it [00:25,  1.71it/s]Extractor Estimating: 43it [00:26,  1.66it/s]Extractor Estimating: 44it [00:26,  1.73it/s]Extractor Estimating: 45it [00:27,  1.71it/s]Extractor Estimating: 46it [00:27,  1.72it/s]Extractor Estimating: 47it [00:28,  1.75it/s]Extractor Estimating: 48it [00:28,  1.74it/s]Extractor Estimating: 49it [00:29,  1.74it/s]Extractor Estimating: 50it [00:30,  1.77it/s]Extractor Estimating: 51it [00:30,  1.69it/s]Extractor Estimating: 52it [00:31,  1.69it/s]Extractor Estimating: 53it [00:31,  1.60it/s]Extractor Estimating: 54it [00:32,  1.64it/s]Extractor Estimating: 55it [00:33,  1.65it/s]Extractor Estimating: 56it [00:33,  1.63it/s]Extractor Estimating: 57it [00:34,  1.63it/s]Extractor Estimating: 58it [00:34,  1.68it/s]Extractor Estimating: 59it [00:35,  1.67it/s]Extractor Estimating: 60it [00:36,  1.66it/s]Extractor Estimating: 61it [00:36,  1.65it/s]Extractor Estimating: 62it [00:37,  1.68it/s]Extractor Estimating: 63it [00:37,  1.64it/s]Extractor Estimating: 64it [00:38,  1.57it/s]Extractor Estimating: 65it [00:39,  1.59it/s]Extractor Estimating: 66it [00:39,  1.61it/s]Extractor Estimating: 67it [00:40,  1.66it/s]Extractor Estimating: 68it [00:41,  1.69it/s]Extractor Estimating: 69it [00:41,  1.64it/s]Extractor Estimating: 70it [00:42,  1.61it/s]Extractor Estimating: 71it [00:42,  1.61it/s]Extractor Estimating: 72it [00:43,  1.64it/s]Extractor Estimating: 73it [00:44,  1.58it/s]Extractor Estimating: 74it [00:44,  1.58it/s]Extractor Estimating: 75it [00:45,  1.59it/s]Extractor Estimating: 76it [00:46,  1.64it/s]Extractor Estimating: 77it [00:46,  1.62it/s]Extractor Estimating: 78it [00:47,  1.62it/s]Extractor Estimating: 79it [00:47,  1.64it/s]Extractor Estimating: 80it [00:48,  1.64it/s]Extractor Estimating: 81it [00:49,  1.65it/s]Extractor Estimating: 82it [00:49,  1.65it/s]Extractor Estimating: 83it [00:50,  1.64it/s]Extractor Estimating: 84it [00:50,  1.61it/s]Extractor Estimating: 85it [00:51,  1.63it/s]Extractor Estimating: 86it [00:52,  1.63it/s]Extractor Estimating: 87it [00:52,  1.59it/s]Extractor Estimating: 88it [00:53,  1.59it/s]Extractor Estimating: 89it [00:54,  1.57it/s]Extractor Estimating: 90it [00:54,  1.57it/s]Extractor Estimating: 91it [00:55,  1.57it/s]Extractor Estimating: 92it [00:56,  1.50it/s]Extractor Estimating: 93it [00:56,  1.55it/s]Extractor Estimating: 94it [00:57,  1.59it/s]Extractor Estimating: 95it [00:57,  1.64it/s]Extractor Estimating: 96it [00:58,  1.61it/s]Extractor Estimating: 97it [00:59,  1.59it/s]Extractor Estimating: 98it [00:59,  1.62it/s]Extractor Estimating: 99it [01:00,  1.62it/s]Extractor Estimating: 100it [01:00,  1.66it/s]Extractor Estimating: 101it [01:01,  1.65it/s]Extractor Estimating: 102it [01:02,  1.61it/s]Extractor Estimating: 103it [01:02,  1.62it/s]Extractor Estimating: 104it [01:03,  1.51it/s]Extractor Estimating: 105it [01:04,  1.58it/s]Extractor Estimating: 106it [01:04,  1.59it/s]Extractor Estimating: 107it [01:05,  1.52it/s]Extractor Estimating: 108it [01:06,  1.53it/s]Extractor Estimating: 109it [01:06,  1.60it/s]Extractor Estimating: 110it [01:07,  1.62it/s]Extractor Estimating: 111it [01:07,  1.69it/s]Extractor Estimating: 112it [01:08,  1.69it/s]Extractor Estimating: 113it [01:08,  1.71it/s]Extractor Estimating: 114it [01:09,  1.65it/s]Extractor Estimating: 115it [01:10,  1.70it/s]Extractor Estimating: 116it [01:10,  1.70it/s]Extractor Estimating: 117it [01:11,  1.72it/s]Extractor Estimating: 118it [01:11,  1.65it/s]Extractor Estimating: 119it [01:12,  1.66it/s]Extractor Estimating: 120it [01:13,  1.66it/s]Extractor Estimating: 121it [01:13,  1.67it/s]Extractor Estimating: 122it [01:14,  1.66it/s]Extractor Estimating: 123it [01:15,  1.63it/s]Extractor Estimating: 124it [01:15,  1.68it/s]Extractor Estimating: 125it [01:16,  1.69it/s]Extractor Estimating: 126it [01:16,  1.70it/s]Extractor Estimating: 127it [01:17,  1.68it/s]Extractor Estimating: 128it [01:17,  1.67it/s]Extractor Estimating: 129it [01:18,  1.63it/s]Extractor Estimating: 130it [01:19,  1.61it/s]Extractor Estimating: 131it [01:19,  1.63it/s]Extractor Estimating: 132it [01:20,  1.66it/s]Extractor Estimating: 133it [01:21,  1.67it/s]Extractor Estimating: 134it [01:21,  1.64it/s]Extractor Estimating: 135it [01:22,  1.64it/s]Extractor Estimating: 136it [01:22,  1.65it/s]Extractor Estimating: 137it [01:23,  1.70it/s]Extractor Estimating: 138it [01:24,  1.68it/s]Extractor Estimating: 139it [01:24,  1.65it/s]Extractor Estimating: 140it [01:25,  1.62it/s]Extractor Estimating: 141it [01:25,  1.66it/s]Extractor Estimating: 142it [01:26,  1.67it/s]Extractor Estimating: 143it [01:27,  1.69it/s]Extractor Estimating: 144it [01:27,  1.64it/s]Extractor Estimating: 145it [01:28,  1.65it/s]Extractor Estimating: 146it [01:28,  1.70it/s]Extractor Estimating: 147it [01:29,  1.66it/s]Extractor Estimating: 148it [01:30,  1.61it/s]Extractor Estimating: 149it [01:30,  1.66it/s]Extractor Estimating: 150it [01:31,  1.69it/s]Extractor Estimating: 151it [01:31,  1.77it/s]Extractor Estimating: 152it [01:32,  1.71it/s]Extractor Estimating: 153it [01:32,  1.74it/s]Extractor Estimating: 154it [01:33,  1.74it/s]Extractor Estimating: 155it [01:33,  1.82it/s]Extractor Estimating: 156it [01:34,  1.83it/s]Extractor Estimating: 157it [01:35,  1.89it/s]Extractor Estimating: 158it [01:35,  1.89it/s]Extractor Estimating: 159it [01:36,  1.87it/s]Extractor Estimating: 160it [01:36,  1.87it/s]Extractor Estimating: 161it [01:37,  1.95it/s]Extractor Estimating: 162it [01:37,  1.90it/s]Extractor Estimating: 163it [01:38,  1.86it/s]Extractor Estimating: 164it [01:38,  1.88it/s]Extractor Estimating: 165it [01:39,  1.92it/s]Extractor Estimating: 166it [01:39,  1.79it/s]Extractor Estimating: 167it [01:40,  1.86it/s]Extractor Estimating: 168it [01:40,  1.98it/s]Extractor Estimating: 169it [01:41,  1.92it/s]Extractor Estimating: 170it [01:41,  1.96it/s]Extractor Estimating: 171it [01:42,  1.97it/s]Extractor Estimating: 172it [01:42,  1.93it/s]Extractor Estimating: 173it [01:43,  1.92it/s]Extractor Estimating: 174it [01:43,  1.92it/s]Extractor Estimating: 175it [01:44,  1.87it/s]Extractor Estimating: 176it [01:45,  1.76it/s]Extractor Estimating: 177it [01:45,  1.65it/s]Extractor Estimating: 178it [01:46,  1.60it/s]Extractor Estimating: 179it [01:47,  1.58it/s]Extractor Estimating: 180it [01:47,  1.60it/s]Extractor Estimating: 181it [01:48,  1.57it/s]Extractor Estimating: 182it [01:49,  1.51it/s]Extractor Estimating: 183it [01:49,  1.51it/s]Extractor Estimating: 184it [01:50,  1.52it/s]Extractor Estimating: 185it [01:51,  1.57it/s]Extractor Estimating: 186it [01:51,  1.55it/s]Extractor Estimating: 187it [01:52,  1.58it/s]Extractor Estimating: 188it [01:53,  1.54it/s]Extractor Estimating: 189it [01:53,  1.57it/s]Extractor Estimating: 190it [01:54,  1.55it/s]Extractor Estimating: 191it [01:54,  1.55it/s]Extractor Estimating: 192it [01:55,  1.59it/s]Extractor Estimating: 193it [01:56,  1.39it/s]Extractor Estimating: 194it [01:57,  1.42it/s]Extractor Estimating: 195it [01:57,  1.45it/s]Extractor Estimating: 196it [01:58,  1.46it/s]Extractor Estimating: 197it [01:59,  1.48it/s]Extractor Estimating: 198it [01:59,  1.53it/s]Extractor Estimating: 199it [02:00,  1.57it/s]Extractor Estimating: 200it [02:00,  1.58it/s]Extractor Estimating: 201it [02:01,  1.59it/s]Extractor Estimating: 202it [02:02,  1.62it/s]Extractor Estimating: 203it [02:02,  1.67it/s]Extractor Estimating: 204it [02:03,  1.65it/s]Extractor Estimating: 205it [02:03,  1.59it/s]Extractor Estimating: 206it [02:04,  1.63it/s]Extractor Estimating: 207it [02:05,  1.66it/s]Extractor Estimating: 208it [02:05,  1.70it/s]Extractor Estimating: 209it [02:06,  1.71it/s]Extractor Estimating: 210it [02:06,  1.68it/s]Extractor Estimating: 211it [02:07,  1.63it/s]Extractor Estimating: 212it [02:08,  1.60it/s]Extractor Estimating: 213it [02:08,  1.62it/s]Extractor Estimating: 214it [02:09,  1.60it/s]Extractor Estimating: 215it [02:10,  1.59it/s]Extractor Estimating: 216it [02:10,  1.56it/s]Extractor Estimating: 217it [02:11,  1.60it/s]Extractor Estimating: 218it [02:11,  1.62it/s]Extractor Estimating: 219it [02:12,  1.61it/s]Extractor Estimating: 220it [02:13,  1.63it/s]Extractor Estimating: 221it [02:13,  1.62it/s]Extractor Estimating: 222it [02:14,  1.60it/s]Extractor Estimating: 223it [02:15,  1.59it/s]Extractor Estimating: 224it [02:15,  1.64it/s]Extractor Estimating: 225it [02:16,  1.60it/s]Extractor Estimating: 226it [02:16,  1.55it/s]Extractor Estimating: 227it [02:17,  1.57it/s]Extractor Estimating: 228it [02:18,  1.63it/s]Extractor Estimating: 229it [02:18,  1.66it/s]Extractor Estimating: 230it [02:19,  1.65it/s]Extractor Estimating: 231it [02:19,  1.64it/s]Extractor Estimating: 232it [02:20,  1.60it/s]Extractor Estimating: 233it [02:21,  1.62it/s]Extractor Estimating: 234it [02:21,  1.63it/s]Extractor Estimating: 235it [02:22,  1.63it/s]Extractor Estimating: 236it [02:23,  1.62it/s]Extractor Estimating: 237it [02:23,  1.68it/s]Extractor Estimating: 238it [02:24,  1.67it/s]Extractor Estimating: 239it [02:24,  1.69it/s]Extractor Estimating: 240it [02:25,  1.71it/s]Extractor Estimating: 241it [02:26,  1.68it/s]Extractor Estimating: 242it [02:26,  1.66it/s]Extractor Estimating: 243it [02:27,  1.69it/s]Extractor Estimating: 244it [02:27,  1.71it/s]Extractor Estimating: 245it [02:28,  1.69it/s]Extractor Estimating: 246it [02:28,  1.70it/s]Extractor Estimating: 247it [02:29,  1.67it/s]Extractor Estimating: 248it [02:30,  1.65it/s]Extractor Estimating: 249it [02:30,  1.66it/s]Extractor Estimating: 250it [02:31,  1.66it/s]Extractor Estimating: 251it [02:32,  1.65it/s]Extractor Estimating: 252it [02:32,  1.65it/s]Extractor Estimating: 253it [02:33,  1.64it/s]Extractor Estimating: 254it [02:33,  1.59it/s]Extractor Estimating: 255it [02:34,  1.60it/s]Extractor Estimating: 256it [02:35,  1.65it/s]Extractor Estimating: 257it [02:35,  1.61it/s]Extractor Estimating: 258it [02:36,  1.58it/s]Extractor Estimating: 259it [02:37,  1.58it/s]Extractor Estimating: 260it [02:37,  1.56it/s]Extractor Estimating: 261it [02:38,  1.52it/s]Extractor Estimating: 262it [02:38,  1.57it/s]Extractor Estimating: 263it [02:39,  1.58it/s]Extractor Estimating: 264it [02:40,  1.60it/s]Extractor Estimating: 265it [02:40,  1.51it/s]Extractor Estimating: 266it [02:41,  1.54it/s]Extractor Estimating: 267it [02:42,  1.42it/s]Extractor Estimating: 268it [02:42,  1.49it/s]Extractor Estimating: 269it [02:43,  1.51it/s]Extractor Estimating: 270it [02:44,  1.57it/s]Extractor Estimating: 271it [02:44,  1.59it/s]Extractor Estimating: 272it [02:45,  1.62it/s]Extractor Estimating: 273it [02:46,  1.59it/s]Extractor Estimating: 274it [02:46,  1.61it/s]Extractor Estimating: 275it [02:47,  1.57it/s]Extractor Estimating: 276it [02:47,  1.57it/s]Extractor Estimating: 277it [02:48,  1.54it/s]Extractor Estimating: 278it [02:49,  1.58it/s]Extractor Estimating: 279it [02:49,  1.59it/s]Extractor Estimating: 280it [02:50,  1.55it/s]Extractor Estimating: 281it [02:51,  1.53it/s]Extractor Estimating: 282it [02:51,  1.51it/s]Extractor Estimating: 283it [02:52,  1.51it/s]Extractor Estimating: 284it [02:53,  1.51it/s]Extractor Estimating: 285it [02:53,  1.48it/s]Extractor Estimating: 286it [02:54,  1.49it/s]Extractor Estimating: 287it [02:55,  1.52it/s]Extractor Estimating: 288it [02:55,  1.54it/s]Extractor Estimating: 289it [02:56,  1.56it/s]Extractor Estimating: 290it [02:57,  1.57it/s]Extractor Estimating: 291it [02:57,  1.56it/s]Extractor Estimating: 292it [02:58,  1.55it/s]Extractor Estimating: 293it [02:59,  1.57it/s]Extractor Estimating: 294it [02:59,  1.59it/s]Extractor Estimating: 295it [03:00,  1.52it/s]Extractor Estimating: 296it [03:01,  1.51it/s]Extractor Estimating: 297it [03:01,  1.54it/s]Extractor Estimating: 298it [03:02,  1.53it/s]Extractor Estimating: 299it [03:02,  1.57it/s]Extractor Estimating: 300it [03:03,  1.56it/s]Extractor Estimating: 301it [03:04,  1.59it/s]Extractor Estimating: 302it [03:04,  1.60it/s]Extractor Estimating: 303it [03:05,  1.63it/s]Extractor Estimating: 304it [03:06,  1.62it/s]Extractor Estimating: 305it [03:06,  1.63it/s]Extractor Estimating: 306it [03:07,  1.66it/s]Extractor Estimating: 307it [03:07,  1.67it/s]Extractor Estimating: 308it [03:08,  1.69it/s]Extractor Estimating: 309it [03:08,  1.67it/s]Extractor Estimating: 310it [03:09,  1.66it/s]Extractor Estimating: 311it [03:10,  1.69it/s]Extractor Estimating: 312it [03:10,  1.70it/s]Extractor Estimating: 313it [03:11,  1.70it/s]Extractor Estimating: 314it [03:11,  1.67it/s]Extractor Estimating: 315it [03:12,  1.66it/s]Extractor Estimating: 316it [03:13,  1.64it/s]Extractor Estimating: 317it [03:13,  1.68it/s]Extractor Estimating: 318it [03:14,  1.65it/s]Extractor Estimating: 319it [03:15,  1.63it/s]Extractor Estimating: 320it [03:15,  1.58it/s]Extractor Estimating: 321it [03:16,  1.63it/s]Extractor Estimating: 322it [03:16,  1.67it/s]Extractor Estimating: 323it [03:17,  1.65it/s]Extractor Estimating: 324it [03:18,  1.64it/s]Extractor Estimating: 325it [03:18,  1.60it/s]Extractor Estimating: 326it [03:19,  1.69it/s]Extractor Estimating: 327it [03:19,  1.74it/s]Extractor Estimating: 328it [03:20,  1.73it/s]Extractor Estimating: 329it [03:20,  1.72it/s]Extractor Estimating: 330it [03:21,  1.70it/s]Extractor Estimating: 331it [03:22,  1.65it/s]Extractor Estimating: 332it [03:22,  1.62it/s]Extractor Estimating: 333it [03:23,  1.65it/s]Extractor Estimating: 334it [03:23,  1.69it/s]Extractor Estimating: 335it [03:24,  1.57it/s]Extractor Estimating: 336it [03:25,  1.60it/s]Extractor Estimating: 337it [03:25,  1.66it/s]Extractor Estimating: 338it [03:26,  1.63it/s]Extractor Estimating: 339it [03:27,  1.64it/s]Extractor Estimating: 340it [03:27,  1.68it/s]Extractor Estimating: 341it [03:28,  1.71it/s]Extractor Estimating: 342it [03:28,  1.68it/s]Extractor Estimating: 343it [03:29,  1.73it/s]Extractor Estimating: 344it [03:30,  1.55it/s]Extractor Estimating: 345it [03:30,  1.56it/s]Extractor Estimating: 346it [03:31,  1.59it/s]Extractor Estimating: 347it [03:31,  1.64it/s]Extractor Estimating: 348it [03:32,  1.64it/s]Extractor Estimating: 349it [03:33,  1.70it/s]Extractor Estimating: 350it [03:33,  1.70it/s]Extractor Estimating: 351it [03:34,  1.72it/s]Extractor Estimating: 352it [03:34,  1.66it/s]Extractor Estimating: 353it [03:35,  1.68it/s]Extractor Estimating: 354it [03:36,  1.68it/s]Extractor Estimating: 355it [03:36,  1.67it/s]Extractor Estimating: 356it [03:37,  1.66it/s]Extractor Estimating: 357it [03:37,  1.65it/s]Extractor Estimating: 358it [03:38,  1.64it/s]Extractor Estimating: 359it [03:39,  1.63it/s]Extractor Estimating: 360it [03:39,  1.63it/s]Extractor Estimating: 361it [03:40,  1.62it/s]Extractor Estimating: 362it [03:41,  1.65it/s]Extractor Estimating: 363it [03:41,  1.63it/s]Extractor Estimating: 364it [03:42,  1.59it/s]Extractor Estimating: 365it [03:42,  1.62it/s]Extractor Estimating: 366it [03:43,  1.58it/s]Extractor Estimating: 367it [03:44,  1.59it/s]Extractor Estimating: 368it [03:44,  1.62it/s]Extractor Estimating: 369it [03:45,  1.56it/s]Extractor Estimating: 370it [03:46,  1.59it/s]Extractor Estimating: 371it [03:46,  1.66it/s]Extractor Estimating: 372it [03:47,  1.69it/s]Extractor Estimating: 373it [03:47,  1.75it/s]Extractor Estimating: 374it [03:48,  1.74it/s]Extractor Estimating: 375it [03:48,  1.71it/s]Extractor Estimating: 376it [03:49,  1.66it/s]Extractor Estimating: 377it [03:50,  1.62it/s]Extractor Estimating: 378it [03:50,  1.61it/s]Extractor Estimating: 379it [03:51,  1.62it/s]Extractor Estimating: 380it [03:52,  1.61it/s]Extractor Estimating: 381it [03:52,  1.61it/s]Extractor Estimating: 382it [03:53,  1.67it/s]Extractor Estimating: 383it [03:53,  1.63it/s]Extractor Estimating: 384it [03:54,  1.59it/s]Extractor Estimating: 385it [03:55,  1.55it/s]Extractor Estimating: 386it [03:55,  1.54it/s]Extractor Estimating: 387it [03:56,  1.55it/s]Extractor Estimating: 388it [03:57,  1.62it/s]Extractor Estimating: 389it [03:57,  1.64it/s]Extractor Estimating: 390it [03:58,  1.60it/s]Extractor Estimating: 391it [03:58,  1.63it/s]Extractor Estimating: 392it [03:59,  1.61it/s]Extractor Estimating: 393it [04:00,  1.64it/s]Extractor Estimating: 394it [04:00,  1.66it/s]Extractor Estimating: 395it [04:01,  1.59it/s]Extractor Estimating: 396it [04:02,  1.60it/s]Extractor Estimating: 397it [04:02,  1.60it/s]Extractor Estimating: 398it [04:03,  1.55it/s]Extractor Estimating: 399it [04:04,  1.52it/s]Extractor Estimating: 400it [04:04,  1.57it/s]Extractor Estimating: 401it [04:05,  1.59it/s]Extractor Estimating: 402it [04:05,  1.60it/s]Extractor Estimating: 403it [04:06,  1.65it/s]Extractor Estimating: 404it [04:07,  1.60it/s]Extractor Estimating: 405it [04:07,  1.65it/s]Extractor Estimating: 406it [04:08,  1.71it/s]Extractor Estimating: 407it [04:08,  1.71it/s]Extractor Estimating: 408it [04:09,  1.72it/s]Extractor Estimating: 409it [04:09,  1.72it/s]Extractor Estimating: 410it [04:10,  1.70it/s]Extractor Estimating: 411it [04:11,  1.71it/s]Extractor Estimating: 412it [04:11,  1.75it/s]Extractor Estimating: 413it [04:12,  1.74it/s]Extractor Estimating: 414it [04:12,  1.70it/s]Extractor Estimating: 415it [04:13,  1.71it/s]Extractor Estimating: 416it [04:14,  1.63it/s]Extractor Estimating: 417it [04:14,  1.62it/s]Extractor Estimating: 418it [04:15,  1.62it/s]Extractor Estimating: 419it [04:15,  1.63it/s]Extractor Estimating: 420it [04:16,  1.62it/s]Extractor Estimating: 421it [04:17,  1.58it/s]Extractor Estimating: 422it [04:17,  1.61it/s]Extractor Estimating: 423it [04:18,  1.63it/s]Extractor Estimating: 424it [04:19,  1.65it/s]Extractor Estimating: 425it [04:19,  1.66it/s]Extractor Estimating: 426it [04:20,  1.60it/s]Extractor Estimating: 427it [04:21,  1.39it/s]Extractor Estimating: 428it [04:21,  1.42it/s]Extractor Estimating: 429it [04:22,  1.49it/s]Extractor Estimating: 430it [04:23,  1.52it/s]Extractor Estimating: 431it [04:23,  1.52it/s]Extractor Estimating: 432it [04:24,  1.52it/s]Extractor Estimating: 433it [04:24,  1.60it/s]Extractor Estimating: 434it [04:25,  1.60it/s]Extractor Estimating: 435it [04:26,  1.62it/s]Extractor Estimating: 436it [04:26,  1.59it/s]Extractor Estimating: 437it [04:27,  1.64it/s]Extractor Estimating: 438it [04:28,  1.65it/s]Extractor Estimating: 439it [04:28,  1.65it/s]Extractor Estimating: 440it [04:29,  1.68it/s]Extractor Estimating: 441it [04:29,  1.67it/s]Extractor Estimating: 442it [04:30,  1.64it/s]Extractor Estimating: 443it [04:31,  1.65it/s]Extractor Estimating: 444it [04:31,  1.66it/s]Extractor Estimating: 445it [04:32,  1.68it/s]Extractor Estimating: 446it [04:32,  1.70it/s]Extractor Estimating: 447it [04:33,  1.64it/s]Extractor Estimating: 448it [04:34,  1.56it/s]Extractor Estimating: 449it [04:34,  1.58it/s]Extractor Estimating: 450it [04:35,  1.58it/s]Extractor Estimating: 451it [04:36,  1.56it/s]Extractor Estimating: 452it [04:36,  1.57it/s]Extractor Estimating: 453it [04:37,  1.49it/s]Extractor Estimating: 454it [04:38,  1.48it/s]Extractor Estimating: 455it [04:38,  1.48it/s]Extractor Estimating: 456it [04:39,  1.51it/s]Extractor Estimating: 457it [04:40,  1.46it/s]Extractor Estimating: 458it [04:40,  1.47it/s]Extractor Estimating: 459it [04:41,  1.48it/s]Extractor Estimating: 460it [04:42,  1.51it/s]Extractor Estimating: 461it [04:42,  1.56it/s]Extractor Estimating: 462it [04:43,  1.59it/s]Extractor Estimating: 463it [04:44,  1.50it/s]Extractor Estimating: 464it [04:44,  1.52it/s]Extractor Estimating: 465it [04:45,  1.55it/s]Extractor Estimating: 466it [04:45,  1.55it/s]Extractor Estimating: 467it [04:46,  1.52it/s]Extractor Estimating: 468it [04:47,  1.50it/s]Extractor Estimating: 469it [04:48,  1.51it/s]Extractor Estimating: 470it [04:48,  1.58it/s]Extractor Estimating: 471it [04:49,  1.57it/s]Extractor Estimating: 472it [04:49,  1.58it/s]Extractor Estimating: 473it [04:50,  1.48it/s]Extractor Estimating: 474it [04:51,  1.52it/s]Extractor Estimating: 475it [04:51,  1.52it/s]Extractor Estimating: 476it [04:52,  1.53it/s]Extractor Estimating: 477it [04:53,  1.60it/s]Extractor Estimating: 478it [04:53,  1.54it/s]Extractor Estimating: 479it [04:54,  1.61it/s]Extractor Estimating: 480it [04:55,  1.59it/s]Extractor Estimating: 481it [04:55,  1.62it/s]Extractor Estimating: 482it [04:56,  1.58it/s]Extractor Estimating: 483it [04:56,  1.58it/s]Extractor Estimating: 484it [04:57,  1.55it/s]Extractor Estimating: 485it [04:58,  1.61it/s]Extractor Estimating: 486it [04:58,  1.55it/s]Extractor Estimating: 487it [04:59,  1.56it/s]Extractor Estimating: 488it [05:00,  1.60it/s]Extractor Estimating: 489it [05:00,  1.61it/s]Extractor Estimating: 490it [05:01,  1.66it/s]Extractor Estimating: 491it [05:01,  1.69it/s]Extractor Estimating: 492it [05:02,  1.66it/s]Extractor Estimating: 493it [05:03,  1.62it/s]Extractor Estimating: 494it [05:03,  1.62it/s]Extractor Estimating: 495it [05:04,  1.58it/s]Extractor Estimating: 496it [05:05,  1.56it/s]Extractor Estimating: 497it [05:05,  1.56it/s]Extractor Estimating: 498it [05:06,  1.51it/s]Extractor Estimating: 499it [05:06,  1.55it/s]Extractor Estimating: 500it [05:07,  1.78it/s]Extractor Estimating: 500it [05:07,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:15,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:15,118 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:15,118 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:15,118 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:15,118 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:44:15,612 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:44:15,613 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:44:15,919 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:44:17,083 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:44:17,083 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:19,067 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:19,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:19,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:19,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:19,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:44:19,547 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:44:19,549 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:44:20,376 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:44:20,611 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:44:20,612 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 03:41:14,539 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 03:41:14,703 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9964 mean pseudo reward: 0.9484341508678134
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 26713
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26813, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26813, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.992, loss:738.7801
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.995, loss:672.0205
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.003, loss:723.5567
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.000, loss:688.5121
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.997, loss:664.7075
>> valid entity prec:0.5261, rec:0.4175, f1:0.4655
>> valid relation prec:0.1181, rec:0.0237, f1:0.0395
>> valid relation with NER prec:0.1181, rec:0.0237, f1:0.0395
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.561, loss:668.8365
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.980, loss:714.0113
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.003, loss:722.2715
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.975, loss:664.9640
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.989, loss:695.9307
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4801, rec:0.4710, f1:0.4755
>> valid relation prec:0.0864, rec:0.0235, f1:0.0369
>> valid relation with NER prec:0.0864, rec:0.0235, f1:0.0369
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 268, avg_time 2.560, loss:713.4225
g_step 1200, step 368, avg_time 0.999, loss:722.1983
g_step 1300, step 52, avg_time 0.997, loss:708.6912
g_step 1400, step 152, avg_time 0.986, loss:662.4477
g_step 1500, step 252, avg_time 0.992, loss:690.9625
>> valid entity prec:0.4861, rec:0.3779, f1:0.4252
>> valid relation prec:0.0952, rec:0.0138, f1:0.0241
>> valid relation with NER prec:0.0952, rec:0.0138, f1:0.0241
g_step 1600, step 352, avg_time 2.539, loss:686.2292
g_step 1700, step 36, avg_time 0.986, loss:678.8520
g_step 1800, step 136, avg_time 0.987, loss:670.2415
g_step 1900, step 236, avg_time 0.995, loss:648.2627
g_step 2000, step 336, avg_time 0.984, loss:675.0613
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4901, rec:0.4178, f1:0.4511
>> valid relation prec:0.0542, rec:0.0134, f1:0.0215
>> valid relation with NER prec:0.0542, rec:0.0134, f1:0.0215
g_step 2100, step 20, avg_time 2.525, loss:638.6757
g_step 2200, step 120, avg_time 0.989, loss:609.9016
g_step 2300, step 220, avg_time 0.996, loss:643.7001
g_step 2400, step 320, avg_time 0.973, loss:643.7510
g_step 2500, step 4, avg_time 0.982, loss:633.1142
>> valid entity prec:0.4755, rec:0.4050, f1:0.4374
>> valid relation prec:0.0872, rec:0.0167, f1:0.0280
>> valid relation with NER prec:0.0872, rec:0.0167, f1:0.0280
g_step 2600, step 104, avg_time 2.555, loss:599.7340
g_step 2700, step 204, avg_time 0.982, loss:607.7716
g_step 2800, step 304, avg_time 0.961, loss:619.5920
g_step 2900, step 404, avg_time 0.980, loss:632.1565
g_step 3000, step 88, avg_time 0.982, loss:575.4229
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5415, rec:0.3903, f1:0.4536
>> valid relation prec:0.1031, rec:0.0204, f1:0.0340
>> valid relation with NER prec:0.1031, rec:0.0204, f1:0.0340
g_step 3100, step 188, avg_time 2.519, loss:575.7864
g_step 3200, step 288, avg_time 0.982, loss:601.7311
g_step 3300, step 388, avg_time 0.978, loss:614.3621
g_step 3400, step 72, avg_time 0.969, loss:553.1700
g_step 3500, step 172, avg_time 0.984, loss:576.3095
>> valid entity prec:0.5147, rec:0.3505, f1:0.4170
>> valid relation prec:0.0868, rec:0.0146, f1:0.0250
>> valid relation with NER prec:0.0868, rec:0.0146, f1:0.0250
g_step 3600, step 272, avg_time 2.546, loss:575.0125
g_step 3700, step 372, avg_time 0.988, loss:588.6475
g_step 3800, step 56, avg_time 0.974, loss:550.0549
g_step 3900, step 156, avg_time 0.984, loss:546.0548
g_step 4000, step 256, avg_time 0.972, loss:564.1122
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5245, rec:0.4358, f1:0.4761
>> valid relation prec:0.1136, rec:0.0303, f1:0.0478
>> valid relation with NER prec:0.1136, rec:0.0303, f1:0.0478
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 356, avg_time 2.525, loss:590.1659
g_step 4200, step 40, avg_time 0.959, loss:524.2028
g_step 4300, step 140, avg_time 0.970, loss:517.1871
g_step 4400, step 240, avg_time 0.980, loss:545.5453
g_step 4500, step 340, avg_time 0.983, loss:560.3206
>> valid entity prec:0.4770, rec:0.4102, f1:0.4411
>> valid relation prec:0.1050, rec:0.0262, f1:0.0419
>> valid relation with NER prec:0.1050, rec:0.0262, f1:0.0419
g_step 4600, step 24, avg_time 2.498, loss:540.1686
g_step 4700, step 124, avg_time 0.966, loss:516.2790
g_step 4800, step 224, avg_time 0.974, loss:513.7130
g_step 4900, step 324, avg_time 0.974, loss:531.6117
g_step 5000, step 8, avg_time 0.971, loss:533.9660
learning rate was adjusted to 0.0008
>> valid entity prec:0.4571, rec:0.4626, f1:0.4599
>> valid relation prec:0.1018, rec:0.0284, f1:0.0444
>> valid relation with NER prec:0.1018, rec:0.0284, f1:0.0444
g_step 5100, step 108, avg_time 2.514, loss:488.3156
g_step 5200, step 208, avg_time 0.966, loss:480.6130
g_step 5300, step 308, avg_time 0.975, loss:521.4696
g_step 5400, step 408, avg_time 0.975, loss:530.9597
g_step 5500, step 92, avg_time 0.984, loss:481.0236
>> valid entity prec:0.4822, rec:0.4040, f1:0.4396
>> valid relation prec:0.1072, rec:0.0264, f1:0.0423
>> valid relation with NER prec:0.1072, rec:0.0264, f1:0.0423
g_step 5600, step 192, avg_time 2.496, loss:481.4760
g_step 5700, step 292, avg_time 0.965, loss:496.2789
g_step 5800, step 392, avg_time 0.976, loss:508.4670
g_step 5900, step 76, avg_time 0.962, loss:467.5702
g_step 6000, step 176, avg_time 0.977, loss:467.2689
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4628, rec:0.3789, f1:0.4167
>> valid relation prec:0.0871, rec:0.0204, f1:0.0330
>> valid relation with NER prec:0.0871, rec:0.0204, f1:0.0330
g_step 6100, step 276, avg_time 2.494, loss:483.3300
g_step 6200, step 376, avg_time 0.975, loss:476.0203
g_step 6300, step 60, avg_time 0.965, loss:443.9327
g_step 6400, step 160, avg_time 0.975, loss:449.4857
g_step 6500, step 260, avg_time 0.976, loss:461.5134
>> valid entity prec:0.4846, rec:0.4073, f1:0.4426
>> valid relation prec:0.0828, rec:0.0200, f1:0.0322
>> valid relation with NER prec:0.0828, rec:0.0200, f1:0.0322
g_step 6600, step 360, avg_time 2.505, loss:474.3282
g_step 6700, step 44, avg_time 0.959, loss:456.9788
g_step 6800, step 144, avg_time 0.966, loss:434.1012
g_step 6900, step 244, avg_time 0.968, loss:431.8141
g_step 7000, step 344, avg_time 0.963, loss:459.7950
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4631, rec:0.3827, f1:0.4191
>> valid relation prec:0.0639, rec:0.0167, f1:0.0265
>> valid relation with NER prec:0.0639, rec:0.0167, f1:0.0265
g_step 7100, step 28, avg_time 2.492, loss:471.8674
g_step 7200, step 128, avg_time 0.965, loss:406.1958
g_step 7300, step 228, avg_time 0.978, loss:421.5838
g_step 7400, step 328, avg_time 0.974, loss:433.8252
g_step 7500, step 12, avg_time 0.951, loss:458.0309
>> valid entity prec:0.4687, rec:0.4007, f1:0.4320
>> valid relation prec:0.0983, rec:0.0262, f1:0.0413
>> valid relation with NER prec:0.0983, rec:0.0262, f1:0.0413
g_step 7600, step 112, avg_time 2.503, loss:415.1855
g_step 7700, step 212, avg_time 0.967, loss:420.5528
g_step 7800, step 312, avg_time 0.955, loss:439.0884
g_step 7900, step 412, avg_time 0.975, loss:430.4184
g_step 8000, step 96, avg_time 0.979, loss:391.6604
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4501, rec:0.4239, f1:0.4366
>> valid relation prec:0.0624, rec:0.0214, f1:0.0319
>> valid relation with NER prec:0.0624, rec:0.0214, f1:0.0319
g_step 8100, step 196, avg_time 2.509, loss:422.4833
g_step 8200, step 296, avg_time 0.973, loss:410.1497
g_step 8300, step 396, avg_time 0.973, loss:422.7690
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 03:41:14 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 03:41:14 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_03-41-14_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 03:41:15 - WARNING - datasets.builder -   Using custom data configuration default-bc900908e07d8614
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-bc900908e07d8614/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 03:41:17,244 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:41:17,245 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:41:17,246 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:41:17,247 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:41:17,309 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:41:17,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:41:17,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:41:17,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:41:17,357 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:41:17,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:41:17,357 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 03:41:17,656 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:41:20,770 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 03:41:20,808 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-bc900908e07d8614/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:04,  2.07ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.11ba/s] 27%|██▋       | 3/11 [00:00<00:02,  3.69ba/s] 36%|███▋      | 4/11 [00:01<00:01,  4.03ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.23ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.38ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.49ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.56ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.58ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.61ba/s]100%|██████████| 11/11 [00:02<00:00,  4.57ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.15ba/s] 40%|████      | 2/5 [00:00<00:00,  3.83ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.97ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.16ba/s]100%|██████████| 5/5 [00:01<00:00,  3.68ba/s]100%|██████████| 5/5 [00:01<00:00,  3.76ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  6.96ba/s] 27%|██▋       | 3/11 [00:00<00:00,  9.46ba/s] 45%|████▌     | 5/11 [00:00<00:00, 10.01ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 10.40ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.57ba/s]100%|██████████| 11/11 [00:00<00:00, 11.26ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.16ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.23ba/s]100%|██████████| 5/5 [00:00<00:00, 10.43ba/s]100%|██████████| 5/5 [00:00<00:00,  9.81ba/s]
[INFO|trainer.py:414] 2023-08-29 03:41:27,028 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 03:41:27,127 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 03:41:27,127 >>   Num examples = 10015
[INFO|trainer.py:1149] 2023-08-29 03:41:27,127 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 03:41:27,128 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 03:41:27,128 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 03:41:27,128 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 03:41:27,128 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:56,  3.29it/s]  0%|          | 2/780 [00:00<03:49,  3.39it/s]  0%|          | 3/780 [00:00<03:46,  3.43it/s]  1%|          | 4/780 [00:01<03:45,  3.44it/s]  1%|          | 5/780 [00:01<03:44,  3.45it/s]  1%|          | 6/780 [00:01<03:43,  3.46it/s]  1%|          | 7/780 [00:02<03:43,  3.47it/s]  1%|          | 8/780 [00:02<03:42,  3.47it/s]  1%|          | 9/780 [00:02<03:42,  3.47it/s]  1%|▏         | 10/780 [00:02<03:41,  3.47it/s]  1%|▏         | 11/780 [00:03<03:41,  3.48it/s]  2%|▏         | 12/780 [00:03<03:46,  3.39it/s]  2%|▏         | 13/780 [00:03<03:44,  3.42it/s]  2%|▏         | 14/780 [00:04<03:43,  3.43it/s]  2%|▏         | 15/780 [00:04<03:42,  3.45it/s]  2%|▏         | 16/780 [00:04<03:40,  3.46it/s]  2%|▏         | 17/780 [00:04<03:40,  3.46it/s]  2%|▏         | 18/780 [00:05<03:39,  3.47it/s]  2%|▏         | 19/780 [00:05<03:39,  3.47it/s]  3%|▎         | 20/780 [00:05<03:38,  3.47it/s]  3%|▎         | 21/780 [00:06<03:38,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.47it/s]  3%|▎         | 23/780 [00:06<03:45,  3.36it/s]  3%|▎         | 24/780 [00:06<03:42,  3.39it/s]  3%|▎         | 25/780 [00:07<03:41,  3.41it/s]  3%|▎         | 26/780 [00:07<03:39,  3.43it/s]  3%|▎         | 27/780 [00:07<03:39,  3.43it/s]  4%|▎         | 28/780 [00:08<03:39,  3.42it/s]  4%|▎         | 29/780 [00:08<03:39,  3.42it/s]  4%|▍         | 30/780 [00:08<03:39,  3.42it/s]  4%|▍         | 31/780 [00:09<03:38,  3.42it/s]  4%|▍         | 32/780 [00:09<03:38,  3.42it/s]  4%|▍         | 33/780 [00:09<03:38,  3.42it/s]  4%|▍         | 34/780 [00:09<03:45,  3.31it/s]  4%|▍         | 35/780 [00:10<03:42,  3.35it/s]  5%|▍         | 36/780 [00:10<03:41,  3.37it/s]  5%|▍         | 37/780 [00:10<03:39,  3.38it/s]  5%|▍         | 38/780 [00:11<03:38,  3.39it/s]  5%|▌         | 39/780 [00:11<03:37,  3.40it/s]  5%|▌         | 40/780 [00:11<03:36,  3.41it/s]  5%|▌         | 41/780 [00:11<03:35,  3.43it/s]  5%|▌         | 42/780 [00:12<03:34,  3.44it/s]  6%|▌         | 43/780 [00:12<03:33,  3.45it/s]  6%|▌         | 44/780 [00:12<03:33,  3.45it/s]  6%|▌         | 45/780 [00:13<03:36,  3.40it/s]  6%|▌         | 46/780 [00:13<03:34,  3.42it/s]  6%|▌         | 47/780 [00:13<03:33,  3.43it/s]  6%|▌         | 48/780 [00:14<03:32,  3.44it/s]  6%|▋         | 49/780 [00:14<03:31,  3.45it/s]  6%|▋         | 50/780 [00:14<03:31,  3.46it/s]  7%|▋         | 51/780 [00:14<03:30,  3.46it/s]  7%|▋         | 52/780 [00:15<03:30,  3.46it/s]  7%|▋         | 53/780 [00:15<03:30,  3.46it/s]  7%|▋         | 54/780 [00:15<03:29,  3.46it/s]  7%|▋         | 55/780 [00:16<03:29,  3.47it/s]  7%|▋         | 56/780 [00:16<03:33,  3.40it/s]  7%|▋         | 57/780 [00:16<03:31,  3.42it/s]  7%|▋         | 58/780 [00:16<03:30,  3.43it/s]  8%|▊         | 59/780 [00:17<03:29,  3.44it/s]  8%|▊         | 60/780 [00:17<03:28,  3.45it/s]  8%|▊         | 61/780 [00:17<03:28,  3.45it/s]  8%|▊         | 62/780 [00:18<03:36,  3.32it/s]  8%|▊         | 63/780 [00:18<03:33,  3.36it/s]  8%|▊         | 64/780 [00:18<03:31,  3.39it/s]  8%|▊         | 65/780 [00:18<03:29,  3.41it/s]  8%|▊         | 66/780 [00:19<03:28,  3.43it/s]  9%|▊         | 67/780 [00:19<03:27,  3.44it/s]  9%|▊         | 68/780 [00:19<03:26,  3.45it/s]  9%|▉         | 69/780 [00:20<03:26,  3.45it/s]  9%|▉         | 70/780 [00:20<03:25,  3.45it/s]  9%|▉         | 71/780 [00:20<03:25,  3.45it/s]  9%|▉         | 72/780 [00:20<03:24,  3.46it/s]  9%|▉         | 73/780 [00:21<03:28,  3.39it/s]  9%|▉         | 74/780 [00:21<03:27,  3.41it/s] 10%|▉         | 75/780 [00:21<03:25,  3.43it/s] 10%|▉         | 76/780 [00:22<03:24,  3.44it/s] 10%|▉         | 77/780 [00:22<03:23,  3.45it/s] 10%|█         | 78/780 [00:22<03:23,  3.45it/s] 10%|█         | 79/780 [00:23<03:22,  3.46it/s] 10%|█         | 80/780 [00:23<03:22,  3.46it/s] 10%|█         | 81/780 [00:23<03:21,  3.46it/s] 11%|█         | 82/780 [00:23<03:21,  3.46it/s] 11%|█         | 83/780 [00:24<03:21,  3.46it/s] 11%|█         | 84/780 [00:24<03:23,  3.42it/s] 11%|█         | 85/780 [00:24<03:22,  3.43it/s] 11%|█         | 86/780 [00:25<03:21,  3.44it/s] 11%|█         | 87/780 [00:25<03:20,  3.45it/s] 11%|█▏        | 88/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 89/780 [00:25<03:19,  3.46it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 91/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 92/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 93/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 94/780 [00:27<03:17,  3.47it/s] 12%|█▏        | 95/780 [00:27<03:21,  3.39it/s] 12%|█▏        | 96/780 [00:27<03:20,  3.42it/s] 12%|█▏        | 97/780 [00:28<03:19,  3.43it/s] 13%|█▎        | 98/780 [00:28<03:18,  3.44it/s] 13%|█▎        | 99/780 [00:28<03:17,  3.45it/s] 13%|█▎        | 100/780 [00:29<03:16,  3.45it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.46it/s] 13%|█▎        | 102/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 103/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 104/780 [00:30<03:15,  3.46it/s] 13%|█▎        | 105/780 [00:30<03:15,  3.46it/s] 14%|█▎        | 106/780 [00:30<03:21,  3.34it/s] 14%|█▎        | 107/780 [00:31<03:18,  3.38it/s] 14%|█▍        | 108/780 [00:31<03:17,  3.41it/s] 14%|█▍        | 109/780 [00:31<03:15,  3.42it/s] 14%|█▍        | 110/780 [00:32<03:14,  3.44it/s] 14%|█▍        | 111/780 [00:32<03:14,  3.45it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.45it/s] 14%|█▍        | 113/780 [00:32<03:13,  3.45it/s] 15%|█▍        | 114/780 [00:33<03:12,  3.46it/s] 15%|█▍        | 115/780 [00:33<03:12,  3.46it/s] 15%|█▍        | 116/780 [00:33<03:11,  3.46it/s] 15%|█▌        | 117/780 [00:34<03:19,  3.33it/s] 15%|█▌        | 118/780 [00:34<03:16,  3.37it/s] 15%|█▌        | 119/780 [00:34<03:14,  3.40it/s] 15%|█▌        | 120/780 [00:34<03:13,  3.42it/s] 16%|█▌        | 121/780 [00:35<03:12,  3.43it/s] 16%|█▌        | 122/780 [00:35<03:11,  3.44it/s] 16%|█▌        | 123/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 124/780 [00:36<03:09,  3.45it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.46it/s] 16%|█▌        | 126/780 [00:36<03:09,  3.46it/s] 16%|█▋        | 127/780 [00:36<03:08,  3.46it/s] 16%|█▋        | 128/780 [00:37<03:26,  3.15it/s] 17%|█▋        | 129/780 [00:37<03:21,  3.24it/s] 17%|█▋        | 130/780 [00:37<03:16,  3.30it/s] 17%|█▋        | 131/780 [00:38<03:13,  3.35it/s] 17%|█▋        | 132/780 [00:38<03:11,  3.38it/s] 17%|█▋        | 133/780 [00:38<03:10,  3.41it/s] 17%|█▋        | 134/780 [00:39<03:08,  3.42it/s] 17%|█▋        | 135/780 [00:39<03:07,  3.43it/s] 17%|█▋        | 136/780 [00:39<03:07,  3.44it/s] 18%|█▊        | 137/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 138/780 [00:40<03:06,  3.45it/s] 18%|█▊        | 139/780 [00:40<03:28,  3.08it/s] 18%|█▊        | 140/780 [00:40<03:20,  3.18it/s] 18%|█▊        | 141/780 [00:41<03:15,  3.26it/s] 18%|█▊        | 142/780 [00:41<03:12,  3.32it/s] 18%|█▊        | 143/780 [00:41<03:09,  3.36it/s] 18%|█▊        | 144/780 [00:42<03:07,  3.39it/s] 19%|█▊        | 145/780 [00:42<03:06,  3.41it/s] 19%|█▊        | 146/780 [00:42<03:04,  3.43it/s] 19%|█▉        | 147/780 [00:42<03:04,  3.43it/s] 19%|█▉        | 148/780 [00:43<03:03,  3.44it/s] 19%|█▉        | 149/780 [00:43<03:08,  3.35it/s] 19%|█▉        | 150/780 [00:43<03:06,  3.38it/s] 19%|█▉        | 151/780 [00:44<03:04,  3.40it/s] 19%|█▉        | 152/780 [00:44<03:03,  3.42it/s] 20%|█▉        | 153/780 [00:44<03:02,  3.43it/s] 20%|█▉        | 154/780 [00:45<03:02,  3.44it/s] 20%|█▉        | 155/780 [00:45<03:08,  3.31it/s] 20%|██        | 156/780 [00:45<03:05,  3.36it/s][INFO|trainer.py:2140] 2023-08-29 03:42:12,917 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:42:12,917 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 03:42:12,917 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.33it/s][A
  2%|▏         | 12/608 [00:00<00:11, 49.84it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.94it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.82it/s][A
  5%|▍         | 28/608 [00:00<00:12, 45.20it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.32it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.19it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.20it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.31it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.47it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.57it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.71it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.42it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.37it/s][A
 13%|█▎        | 78/608 [00:01<00:19, 26.84it/s][A
 14%|█▎        | 83/608 [00:02<00:17, 30.70it/s][A
 14%|█▍        | 88/608 [00:02<00:15, 34.07it/s][A
 15%|█▌        | 93/608 [00:02<00:13, 36.91it/s][A
 16%|█▌        | 98/608 [00:02<00:13, 39.17it/s][A
 17%|█▋        | 103/608 [00:02<00:12, 40.95it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 42.33it/s][A
 19%|█▊        | 113/608 [00:02<00:11, 43.26it/s][A
 19%|█▉        | 118/608 [00:02<00:11, 43.64it/s][A
 20%|██        | 123/608 [00:02<00:11, 43.97it/s][A
 21%|██        | 128/608 [00:03<00:10, 44.49it/s][A
 22%|██▏       | 133/608 [00:03<00:10, 44.87it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.12it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.15it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.41it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.34it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.46it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.26it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.09it/s][A
 28%|██▊       | 173/608 [00:04<00:09, 45.15it/s][A
 29%|██▉       | 178/608 [00:04<00:09, 45.26it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.42it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.58it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.61it/s][A
 33%|███▎      | 198/608 [00:04<00:08, 45.65it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.54it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.35it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.22it/s][A
 36%|███▌      | 218/608 [00:05<00:08, 45.24it/s][A
 37%|███▋      | 223/608 [00:05<00:08, 45.30it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.45it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.41it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.53it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.52it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.34it/s][A
 42%|████▏     | 253/608 [00:05<00:08, 43.34it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 43.96it/s][A
 43%|████▎     | 263/608 [00:06<00:07, 44.45it/s][A
 44%|████▍     | 268/608 [00:06<00:07, 44.82it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.06it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.28it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.40it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.39it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.17it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.07it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.20it/s][A
 51%|█████     | 308/608 [00:07<00:06, 45.31it/s][A
 51%|█████▏    | 313/608 [00:07<00:06, 45.38it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.55it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.60it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.54it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.57it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.32it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.22it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.28it/s][A
 58%|█████▊    | 353/608 [00:08<00:05, 45.35it/s][A
 59%|█████▉    | 358/608 [00:08<00:05, 45.33it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.41it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.56it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.61it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.54it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.38it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.19it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 43.33it/s][A
 65%|██████▌   | 398/608 [00:09<00:04, 44.09it/s][A
 66%|██████▋   | 403/608 [00:09<00:04, 44.54it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 44.82it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.07it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.31it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.30it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.32it/s][A
 71%|███████   | 433/608 [00:09<00:03, 44.96it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.06it/s][A
 73%|███████▎  | 443/608 [00:10<00:03, 45.31it/s][A
 74%|███████▎  | 448/608 [00:10<00:03, 45.41it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.50it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.51it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.55it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.60it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.40it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.07it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.20it/s][A
 80%|████████  | 488/608 [00:11<00:02, 45.26it/s][A
 81%|████████  | 493/608 [00:11<00:02, 45.45it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.58it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.59it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.55it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.46it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.41it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 44.87it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.00it/s][A
 88%|████████▊ | 533/608 [00:12<00:01, 43.72it/s][A
 88%|████████▊ | 538/608 [00:12<00:01, 44.48it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 44.86it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.20it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.23it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.25it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.20it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.16it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 44.95it/s][A
 95%|█████████▌| 578/608 [00:13<00:00, 45.14it/s][A
 96%|█████████▌| 583/608 [00:13<00:00, 45.26it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.48it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.54it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.68it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.52it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.46it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.46it/s][A 20%|██        | 156/780 [00:59<03:05,  3.36it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:42:26,859 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 03:42:27,114 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:42:29,818 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:42:29,968 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:42:30,055 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:08<1:14:16,  7.15s/it] 20%|██        | 158/780 [01:09<52:52,  5.10s/it]   20%|██        | 159/780 [01:09<37:51,  3.66s/it] 21%|██        | 160/780 [01:09<27:22,  2.65s/it] 21%|██        | 161/780 [01:09<20:01,  1.94s/it] 21%|██        | 162/780 [01:10<14:54,  1.45s/it] 21%|██        | 163/780 [01:10<11:19,  1.10s/it] 21%|██        | 164/780 [01:10<08:48,  1.17it/s] 21%|██        | 165/780 [01:11<07:03,  1.45it/s] 21%|██▏       | 166/780 [01:11<05:49,  1.75it/s] 21%|██▏       | 167/780 [01:11<04:58,  2.05it/s] 22%|██▏       | 168/780 [01:12<04:22,  2.33it/s] 22%|██▏       | 169/780 [01:12<04:02,  2.52it/s] 22%|██▏       | 170/780 [01:12<03:43,  2.73it/s] 22%|██▏       | 171/780 [01:12<03:29,  2.91it/s] 22%|██▏       | 172/780 [01:13<03:19,  3.04it/s] 22%|██▏       | 173/780 [01:13<03:12,  3.15it/s] 22%|██▏       | 174/780 [01:13<03:07,  3.22it/s] 22%|██▏       | 175/780 [01:14<03:04,  3.28it/s] 23%|██▎       | 176/780 [01:14<03:01,  3.32it/s] 23%|██▎       | 177/780 [01:14<03:00,  3.35it/s] 23%|██▎       | 178/780 [01:14<02:58,  3.37it/s] 23%|██▎       | 179/780 [01:15<02:57,  3.38it/s] 23%|██▎       | 180/780 [01:15<03:02,  3.29it/s] 23%|██▎       | 181/780 [01:15<02:59,  3.33it/s] 23%|██▎       | 182/780 [01:16<02:58,  3.35it/s] 23%|██▎       | 183/780 [01:16<02:57,  3.37it/s] 24%|██▎       | 184/780 [01:16<02:56,  3.38it/s] 24%|██▎       | 185/780 [01:17<02:55,  3.39it/s] 24%|██▍       | 186/780 [01:17<02:55,  3.39it/s] 24%|██▍       | 187/780 [01:17<02:54,  3.40it/s] 24%|██▍       | 188/780 [01:17<02:53,  3.40it/s] 24%|██▍       | 189/780 [01:18<02:53,  3.41it/s] 24%|██▍       | 190/780 [01:18<02:53,  3.41it/s] 24%|██▍       | 191/780 [01:18<02:56,  3.33it/s] 25%|██▍       | 192/780 [01:19<02:55,  3.36it/s] 25%|██▍       | 193/780 [01:19<02:53,  3.37it/s] 25%|██▍       | 194/780 [01:19<02:53,  3.38it/s] 25%|██▌       | 195/780 [01:20<02:52,  3.39it/s] 25%|██▌       | 196/780 [01:20<02:51,  3.40it/s] 25%|██▌       | 197/780 [01:20<02:51,  3.39it/s] 25%|██▌       | 198/780 [01:20<02:51,  3.40it/s] 26%|██▌       | 199/780 [01:21<02:50,  3.40it/s] 26%|██▌       | 200/780 [01:21<02:50,  3.41it/s] 26%|██▌       | 201/780 [01:21<02:49,  3.41it/s] 26%|██▌       | 202/780 [01:22<02:49,  3.41it/s] 26%|██▌       | 203/780 [01:22<02:49,  3.41it/s] 26%|██▌       | 204/780 [01:22<02:52,  3.33it/s] 26%|██▋       | 205/780 [01:22<02:51,  3.35it/s] 26%|██▋       | 206/780 [01:23<02:50,  3.37it/s] 27%|██▋       | 207/780 [01:23<02:49,  3.38it/s] 27%|██▋       | 208/780 [01:23<02:48,  3.39it/s] 27%|██▋       | 209/780 [01:24<02:48,  3.39it/s] 27%|██▋       | 210/780 [01:24<02:47,  3.39it/s] 27%|██▋       | 211/780 [01:24<02:47,  3.40it/s] 27%|██▋       | 212/780 [01:25<02:46,  3.41it/s] 27%|██▋       | 213/780 [01:25<02:46,  3.41it/s] 27%|██▋       | 214/780 [01:25<02:46,  3.41it/s] 28%|██▊       | 215/780 [01:25<02:50,  3.32it/s] 28%|██▊       | 216/780 [01:26<02:48,  3.35it/s] 28%|██▊       | 217/780 [01:26<02:47,  3.36it/s] 28%|██▊       | 218/780 [01:26<02:46,  3.38it/s] 28%|██▊       | 219/780 [01:27<02:45,  3.39it/s] 28%|██▊       | 220/780 [01:27<02:45,  3.39it/s] 28%|██▊       | 221/780 [01:27<02:44,  3.40it/s] 28%|██▊       | 222/780 [01:27<02:43,  3.40it/s] 29%|██▊       | 223/780 [01:28<02:43,  3.41it/s] 29%|██▊       | 224/780 [01:28<02:43,  3.41it/s] 29%|██▉       | 225/780 [01:28<02:42,  3.41it/s] 29%|██▉       | 226/780 [01:29<02:46,  3.33it/s] 29%|██▉       | 227/780 [01:29<02:44,  3.36it/s] 29%|██▉       | 228/780 [01:29<02:43,  3.38it/s] 29%|██▉       | 229/780 [01:30<02:42,  3.38it/s] 29%|██▉       | 230/780 [01:30<02:42,  3.39it/s] 30%|██▉       | 231/780 [01:30<02:41,  3.40it/s] 30%|██▉       | 232/780 [01:30<02:41,  3.40it/s] 30%|██▉       | 233/780 [01:31<02:40,  3.40it/s] 30%|███       | 234/780 [01:31<02:40,  3.41it/s] 30%|███       | 235/780 [01:31<02:39,  3.41it/s] 30%|███       | 236/780 [01:32<02:39,  3.41it/s] 30%|███       | 237/780 [01:32<02:47,  3.23it/s] 31%|███       | 238/780 [01:32<02:45,  3.28it/s] 31%|███       | 239/780 [01:33<02:42,  3.32it/s] 31%|███       | 240/780 [01:33<02:41,  3.35it/s] 31%|███       | 241/780 [01:33<02:40,  3.37it/s] 31%|███       | 242/780 [01:33<02:39,  3.38it/s] 31%|███       | 243/780 [01:34<02:38,  3.39it/s] 31%|███▏      | 244/780 [01:34<02:37,  3.40it/s] 31%|███▏      | 245/780 [01:34<02:37,  3.40it/s] 32%|███▏      | 246/780 [01:35<02:36,  3.40it/s] 32%|███▏      | 247/780 [01:35<02:36,  3.41it/s] 32%|███▏      | 248/780 [01:35<02:42,  3.27it/s] 32%|███▏      | 249/780 [01:36<02:40,  3.31it/s] 32%|███▏      | 250/780 [01:36<02:38,  3.34it/s] 32%|███▏      | 251/780 [01:36<02:37,  3.36it/s] 32%|███▏      | 252/780 [01:36<02:36,  3.38it/s] 32%|███▏      | 253/780 [01:37<02:35,  3.39it/s] 33%|███▎      | 254/780 [01:37<02:35,  3.39it/s] 33%|███▎      | 255/780 [01:37<02:34,  3.40it/s] 33%|███▎      | 256/780 [01:38<02:33,  3.40it/s] 33%|███▎      | 257/780 [01:38<02:33,  3.41it/s] 33%|███▎      | 258/780 [01:38<02:33,  3.41it/s] 33%|███▎      | 259/780 [01:39<02:47,  3.12it/s] 33%|███▎      | 260/780 [01:39<02:42,  3.20it/s] 33%|███▎      | 261/780 [01:39<02:39,  3.26it/s] 34%|███▎      | 262/780 [01:39<02:36,  3.31it/s] 34%|███▎      | 263/780 [01:40<02:34,  3.34it/s] 34%|███▍      | 264/780 [01:40<02:33,  3.36it/s] 34%|███▍      | 265/780 [01:40<02:32,  3.38it/s] 34%|███▍      | 266/780 [01:41<02:31,  3.38it/s] 34%|███▍      | 267/780 [01:41<02:31,  3.39it/s] 34%|███▍      | 268/780 [01:41<02:30,  3.40it/s] 34%|███▍      | 269/780 [01:42<02:44,  3.10it/s] 35%|███▍      | 270/780 [01:42<02:39,  3.19it/s] 35%|███▍      | 271/780 [01:42<02:36,  3.25it/s] 35%|███▍      | 272/780 [01:42<02:34,  3.30it/s] 35%|███▌      | 273/780 [01:43<02:32,  3.33it/s] 35%|███▌      | 274/780 [01:43<02:30,  3.35it/s] 35%|███▌      | 275/780 [01:43<02:29,  3.37it/s] 35%|███▌      | 276/780 [01:44<02:29,  3.38it/s] 36%|███▌      | 277/780 [01:44<02:28,  3.38it/s] 36%|███▌      | 278/780 [01:44<02:28,  3.39it/s] 36%|███▌      | 279/780 [01:45<02:32,  3.29it/s] 36%|███▌      | 280/780 [01:45<02:34,  3.23it/s] 36%|███▌      | 281/780 [01:45<02:31,  3.29it/s] 36%|███▌      | 282/780 [01:45<02:29,  3.34it/s] 36%|███▋      | 283/780 [01:46<02:27,  3.37it/s] 36%|███▋      | 284/780 [01:46<02:26,  3.39it/s] 37%|███▋      | 285/780 [01:46<02:25,  3.41it/s] 37%|███▋      | 286/780 [01:47<02:24,  3.43it/s] 37%|███▋      | 287/780 [01:47<02:23,  3.44it/s] 37%|███▋      | 288/780 [01:47<02:51,  2.87it/s] 37%|███▋      | 289/780 [01:48<02:46,  2.94it/s] 37%|███▋      | 290/780 [01:48<02:39,  3.08it/s] 37%|███▋      | 291/780 [01:48<02:33,  3.18it/s] 37%|███▋      | 292/780 [01:49<02:29,  3.26it/s] 38%|███▊      | 293/780 [01:49<02:26,  3.32it/s] 38%|███▊      | 294/780 [01:49<02:24,  3.36it/s] 38%|███▊      | 295/780 [01:49<02:23,  3.39it/s] 38%|███▊      | 296/780 [01:50<02:26,  3.30it/s] 38%|███▊      | 297/780 [01:50<02:24,  3.35it/s] 38%|███▊      | 298/780 [01:50<02:22,  3.38it/s] 38%|███▊      | 299/780 [01:51<02:21,  3.40it/s] 38%|███▊      | 300/780 [01:51<02:20,  3.42it/s] 39%|███▊      | 301/780 [01:51<02:19,  3.43it/s] 39%|███▊      | 302/780 [01:51<02:19,  3.44it/s] 39%|███▉      | 303/780 [01:52<02:18,  3.44it/s] 39%|███▉      | 304/780 [01:52<02:18,  3.45it/s] 39%|███▉      | 305/780 [01:52<02:17,  3.45it/s] 39%|███▉      | 306/780 [01:53<02:17,  3.45it/s] 39%|███▉      | 307/780 [01:53<02:19,  3.38it/s] 39%|███▉      | 308/780 [01:53<02:18,  3.41it/s] 40%|███▉      | 309/780 [01:54<02:17,  3.42it/s] 40%|███▉      | 310/780 [01:54<02:17,  3.43it/s] 40%|███▉      | 311/780 [01:54<02:16,  3.44it/s] 40%|████      | 312/780 [01:54<02:15,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 03:43:22,169 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:43:22,170 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 03:43:22,170 >>   Batch size = 8
{'eval_loss': 0.9352977871894836, 'eval_runtime': 13.6861, 'eval_samples_per_second': 355.398, 'eval_steps_per_second': 44.425, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.66it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.37it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.63it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.82it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.28it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.80it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.57it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.37it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.30it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.43it/s][A
  9%|▉         | 57/608 [00:01<00:12, 44.09it/s][A
 10%|█         | 62/608 [00:01<00:12, 44.60it/s][A
 11%|█         | 67/608 [00:01<00:12, 44.92it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.02it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.00it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.99it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.08it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 45.07it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.18it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.25it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.37it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.54it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.37it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.29it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.21it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 45.12it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 45.13it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.18it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.38it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.44it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.56it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.38it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.38it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.21it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 45.27it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 45.30it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.24it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.31it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 44.29it/s][A
 33%|███▎      | 202/608 [00:04<00:09, 44.79it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 44.90it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.00it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.05it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.13it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.09it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.17it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.03it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.22it/s][A
 41%|████      | 247/608 [00:05<00:07, 45.36it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.44it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 45.43it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.38it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.27it/s][A
 45%|████▍     | 272/608 [00:05<00:07, 45.22it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.26it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.16it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.21it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.33it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.47it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.49it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.45it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.30it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 43.13it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.01it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.42it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 43.83it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 44.43it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 44.82it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.01it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.11it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.06it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.77it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.99it/s][A
 61%|██████    | 372/608 [00:08<00:05, 45.11it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.02it/s][A
 63%|██████▎   | 382/608 [00:08<00:04, 45.27it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.50it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 45.60it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.55it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 45.29it/s][A
 67%|██████▋   | 407/608 [00:08<00:04, 45.11it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.12it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.06it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.28it/s][A
 70%|███████   | 427/608 [00:09<00:03, 45.33it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.52it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.55it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.48it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.36it/s][A
 74%|███████▍  | 452/608 [00:09<00:03, 45.25it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 45.23it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.16it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.22it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 43.83it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 44.44it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 44.88it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.06it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.12it/s][A
 82%|████████▏ | 497/608 [00:10<00:02, 45.00it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.00it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.15it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.05it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.14it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.37it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.45it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.48it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.52it/s][A
 89%|████████▉ | 542/608 [00:11<00:01, 45.31it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.11it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.06it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 44.95it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.17it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.22it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.32it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.43it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.49it/s][A
 97%|█████████▋| 587/608 [00:12<00:00, 45.40it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.33it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 45.28it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 45.10it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.26it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.26it/s][A 40%|████      | 312/780 [02:08<02:15,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:43:35,919 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 03:43:36,083 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:43:38,509 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:43:38,595 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:43:38,641 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:17<55:04,  7.08s/it] 40%|████      | 314/780 [02:18<39:12,  5.05s/it] 40%|████      | 315/780 [02:18<28:03,  3.62s/it] 41%|████      | 316/780 [02:18<20:16,  2.62s/it] 41%|████      | 317/780 [02:19<14:50,  1.92s/it] 41%|████      | 318/780 [02:19<11:02,  1.43s/it] 41%|████      | 319/780 [02:19<08:23,  1.09s/it] 41%|████      | 320/780 [02:19<06:31,  1.17it/s] 41%|████      | 321/780 [02:20<05:14,  1.46it/s] 41%|████▏     | 322/780 [02:20<04:19,  1.76it/s] 41%|████▏     | 323/780 [02:20<03:41,  2.06it/s] 42%|████▏     | 324/780 [02:21<03:14,  2.34it/s] 42%|████▏     | 325/780 [02:21<02:56,  2.58it/s] 42%|████▏     | 326/780 [02:21<02:42,  2.79it/s] 42%|████▏     | 327/780 [02:21<02:33,  2.95it/s] 42%|████▏     | 328/780 [02:22<02:27,  3.07it/s] 42%|████▏     | 329/780 [02:22<02:22,  3.17it/s] 42%|████▏     | 330/780 [02:22<02:19,  3.24it/s] 42%|████▏     | 331/780 [02:23<02:16,  3.29it/s] 43%|████▎     | 332/780 [02:23<02:14,  3.33it/s] 43%|████▎     | 333/780 [02:23<02:13,  3.34it/s] 43%|████▎     | 334/780 [02:24<02:16,  3.27it/s] 43%|████▎     | 335/780 [02:24<02:14,  3.31it/s] 43%|████▎     | 336/780 [02:24<02:12,  3.34it/s] 43%|████▎     | 337/780 [02:24<02:11,  3.36it/s] 43%|████▎     | 338/780 [02:25<02:10,  3.38it/s] 43%|████▎     | 339/780 [02:25<02:10,  3.39it/s] 44%|████▎     | 340/780 [02:25<02:09,  3.40it/s] 44%|████▎     | 341/780 [02:26<02:09,  3.40it/s] 44%|████▍     | 342/780 [02:26<02:08,  3.41it/s] 44%|████▍     | 343/780 [02:26<02:08,  3.41it/s] 44%|████▍     | 344/780 [02:26<02:08,  3.40it/s] 44%|████▍     | 345/780 [02:27<02:09,  3.35it/s] 44%|████▍     | 346/780 [02:27<02:08,  3.37it/s] 44%|████▍     | 347/780 [02:27<02:07,  3.38it/s] 45%|████▍     | 348/780 [02:28<02:07,  3.39it/s] 45%|████▍     | 349/780 [02:28<02:06,  3.40it/s] 45%|████▍     | 350/780 [02:28<02:06,  3.40it/s] 45%|████▌     | 351/780 [02:29<02:05,  3.41it/s] 45%|████▌     | 352/780 [02:29<02:05,  3.41it/s] 45%|████▌     | 353/780 [02:29<02:05,  3.41it/s] 45%|████▌     | 354/780 [02:29<02:04,  3.41it/s] 46%|████▌     | 355/780 [02:30<02:04,  3.41it/s] 46%|████▌     | 356/780 [02:30<02:08,  3.30it/s] 46%|████▌     | 357/780 [02:30<02:07,  3.33it/s] 46%|████▌     | 358/780 [02:31<02:05,  3.36it/s] 46%|████▌     | 359/780 [02:31<02:04,  3.37it/s] 46%|████▌     | 360/780 [02:31<02:04,  3.38it/s] 46%|████▋     | 361/780 [02:31<02:03,  3.39it/s] 46%|████▋     | 362/780 [02:32<02:03,  3.40it/s] 47%|████▋     | 363/780 [02:32<02:02,  3.40it/s] 47%|████▋     | 364/780 [02:32<02:02,  3.40it/s] 47%|████▋     | 365/780 [02:33<02:01,  3.40it/s] 47%|████▋     | 366/780 [02:33<02:01,  3.40it/s] 47%|████▋     | 367/780 [02:33<02:06,  3.25it/s] 47%|████▋     | 368/780 [02:34<02:04,  3.30it/s] 47%|████▋     | 369/780 [02:34<02:03,  3.33it/s] 47%|████▋     | 370/780 [02:34<02:02,  3.36it/s] 48%|████▊     | 371/780 [02:34<02:01,  3.37it/s] 48%|████▊     | 372/780 [02:35<02:00,  3.38it/s] 48%|████▊     | 373/780 [02:35<02:00,  3.39it/s] 48%|████▊     | 374/780 [02:35<01:59,  3.40it/s] 48%|████▊     | 375/780 [02:36<01:59,  3.40it/s] 48%|████▊     | 376/780 [02:36<01:58,  3.40it/s] 48%|████▊     | 377/780 [02:36<01:58,  3.41it/s] 48%|████▊     | 378/780 [02:37<02:02,  3.29it/s] 49%|████▊     | 379/780 [02:37<02:00,  3.32it/s] 49%|████▊     | 380/780 [02:37<01:59,  3.35it/s] 49%|████▉     | 381/780 [02:37<01:58,  3.37it/s] 49%|████▉     | 382/780 [02:38<01:57,  3.38it/s] 49%|████▉     | 383/780 [02:38<01:57,  3.39it/s] 49%|████▉     | 384/780 [02:38<01:56,  3.39it/s] 49%|████▉     | 385/780 [02:39<01:56,  3.40it/s] 49%|████▉     | 386/780 [02:39<01:55,  3.40it/s] 50%|████▉     | 387/780 [02:39<01:55,  3.41it/s] 50%|████▉     | 388/780 [02:39<01:55,  3.41it/s] 50%|████▉     | 389/780 [02:40<02:01,  3.23it/s] 50%|█████     | 390/780 [02:40<01:58,  3.28it/s] 50%|█████     | 391/780 [02:40<01:57,  3.32it/s] 50%|█████     | 392/780 [02:41<01:55,  3.35it/s] 50%|█████     | 393/780 [02:41<01:54,  3.37it/s] 51%|█████     | 394/780 [02:41<01:54,  3.38it/s] 51%|█████     | 395/780 [02:42<01:53,  3.39it/s] 51%|█████     | 396/780 [02:42<01:53,  3.39it/s] 51%|█████     | 397/780 [02:42<01:52,  3.40it/s] 51%|█████     | 398/780 [02:42<01:52,  3.40it/s] 51%|█████     | 399/780 [02:43<01:51,  3.40it/s] 51%|█████▏    | 400/780 [02:43<01:53,  3.36it/s] 51%|█████▏    | 401/780 [02:43<01:52,  3.37it/s] 52%|█████▏    | 402/780 [02:44<01:51,  3.38it/s] 52%|█████▏    | 403/780 [02:44<01:51,  3.39it/s] 52%|█████▏    | 404/780 [02:44<01:50,  3.39it/s] 52%|█████▏    | 405/780 [02:45<01:50,  3.40it/s] 52%|█████▏    | 406/780 [02:45<01:49,  3.40it/s] 52%|█████▏    | 407/780 [02:45<01:49,  3.40it/s] 52%|█████▏    | 408/780 [02:45<01:49,  3.41it/s] 52%|█████▏    | 409/780 [02:46<01:48,  3.41it/s] 53%|█████▎    | 410/780 [02:46<01:48,  3.41it/s] 53%|█████▎    | 411/780 [02:46<01:49,  3.37it/s] 53%|█████▎    | 412/780 [02:47<01:48,  3.38it/s] 53%|█████▎    | 413/780 [02:47<01:48,  3.39it/s] 53%|█████▎    | 414/780 [02:47<02:00,  3.04it/s] 53%|█████▎    | 415/780 [02:48<01:56,  3.15it/s] 53%|█████▎    | 416/780 [02:48<01:52,  3.22it/s] 53%|█████▎    | 417/780 [02:48<01:50,  3.27it/s] 54%|█████▎    | 418/780 [02:48<01:49,  3.31it/s] 54%|█████▎    | 419/780 [02:49<01:48,  3.34it/s] 54%|█████▍    | 420/780 [02:49<01:47,  3.36it/s] 54%|█████▍    | 421/780 [02:49<01:49,  3.28it/s] 54%|█████▍    | 422/780 [02:50<01:47,  3.32it/s] 54%|█████▍    | 423/780 [02:50<01:46,  3.35it/s] 54%|█████▍    | 424/780 [02:50<01:45,  3.36it/s] 54%|█████▍    | 425/780 [02:51<01:45,  3.38it/s] 55%|█████▍    | 426/780 [02:51<01:44,  3.39it/s] 55%|█████▍    | 427/780 [02:51<01:43,  3.39it/s] 55%|█████▍    | 428/780 [02:51<01:43,  3.40it/s] 55%|█████▌    | 429/780 [02:52<01:43,  3.40it/s] 55%|█████▌    | 430/780 [02:52<01:42,  3.41it/s] 55%|█████▌    | 431/780 [02:52<01:42,  3.41it/s] 55%|█████▌    | 432/780 [02:53<01:42,  3.41it/s] 56%|█████▌    | 433/780 [02:53<01:41,  3.41it/s] 56%|█████▌    | 434/780 [02:53<01:41,  3.41it/s] 56%|█████▌    | 435/780 [02:53<01:41,  3.41it/s] 56%|█████▌    | 436/780 [02:54<01:41,  3.41it/s] 56%|█████▌    | 437/780 [02:54<01:40,  3.41it/s] 56%|█████▌    | 438/780 [02:54<01:43,  3.29it/s] 56%|█████▋    | 439/780 [02:55<01:42,  3.33it/s] 56%|█████▋    | 440/780 [02:55<01:41,  3.35it/s] 57%|█████▋    | 441/780 [02:55<01:40,  3.37it/s] 57%|█████▋    | 442/780 [02:56<01:40,  3.38it/s] 57%|█████▋    | 443/780 [02:56<01:39,  3.39it/s] 57%|█████▋    | 444/780 [02:56<01:38,  3.40it/s] 57%|█████▋    | 445/780 [02:56<01:38,  3.40it/s] 57%|█████▋    | 446/780 [02:57<01:38,  3.41it/s] 57%|█████▋    | 447/780 [02:57<01:37,  3.41it/s] 57%|█████▋    | 448/780 [02:57<01:37,  3.41it/s] 58%|█████▊    | 449/780 [02:58<01:39,  3.33it/s] 58%|█████▊    | 450/780 [02:58<01:38,  3.35it/s] 58%|█████▊    | 451/780 [02:58<01:37,  3.37it/s] 58%|█████▊    | 452/780 [02:59<01:37,  3.38it/s] 58%|█████▊    | 453/780 [02:59<01:36,  3.39it/s] 58%|█████▊    | 454/780 [02:59<01:36,  3.40it/s] 58%|█████▊    | 455/780 [02:59<01:35,  3.40it/s] 58%|█████▊    | 456/780 [03:00<01:35,  3.41it/s] 59%|█████▊    | 457/780 [03:00<01:34,  3.41it/s] 59%|█████▊    | 458/780 [03:00<01:34,  3.41it/s] 59%|█████▉    | 459/780 [03:01<01:34,  3.41it/s] 59%|█████▉    | 460/780 [03:01<01:37,  3.30it/s] 59%|█████▉    | 461/780 [03:01<01:35,  3.33it/s] 59%|█████▉    | 462/780 [03:02<01:34,  3.35it/s] 59%|█████▉    | 463/780 [03:02<01:34,  3.37it/s] 59%|█████▉    | 464/780 [03:02<01:33,  3.38it/s] 60%|█████▉    | 465/780 [03:02<01:32,  3.39it/s] 60%|█████▉    | 466/780 [03:03<01:32,  3.40it/s] 60%|█████▉    | 467/780 [03:03<01:32,  3.40it/s] 60%|██████    | 468/780 [03:03<01:31,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 03:44:31,028 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:44:31,028 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 03:44:31,028 >>   Batch size = 8
{'eval_loss': 0.9509889483451843, 'eval_runtime': 13.4909, 'eval_samples_per_second': 360.54, 'eval_steps_per_second': 45.068, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.76it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.52it/s][A
  3%|▎         | 18/608 [00:00<00:12, 45.74it/s][A
  4%|▍         | 23/608 [00:00<00:12, 45.76it/s][A
  5%|▍         | 28/608 [00:00<00:12, 45.46it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.17it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.11it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.07it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.29it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.47it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.46it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.48it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.44it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.33it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.21it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.07it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.09it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.24it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.43it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.34it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.51it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.44it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.29it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.21it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.06it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.00it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.12it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.30it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.36it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 42.12it/s][A
 26%|██▌       | 158/608 [00:03<00:10, 43.18it/s][A
 27%|██▋       | 163/608 [00:03<00:10, 43.91it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 44.28it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 44.49it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.62it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.84it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.09it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 44.87it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.08it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.25it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.44it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.33it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.26it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.18it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.29it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.03it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.12it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.24it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.39it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.38it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.41it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.29it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.27it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.22it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.22it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.20it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 44.95it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.23it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.20it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.15it/s][A
 51%|█████     | 308/608 [00:06<00:06, 44.94it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 45.05it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.05it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.14it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.05it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.17it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.28it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.43it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.41it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.13it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.15it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.22it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.16it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.23it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.23it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.27it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.41it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.35it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.32it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.30it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.18it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.17it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.05it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.28it/s][A
 70%|███████   | 428/608 [00:09<00:04, 43.86it/s][A
 71%|███████   | 433/608 [00:09<00:03, 44.49it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 44.78it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 44.94it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.03it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 45.00it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.07it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.15it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.09it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.07it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.31it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.44it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.47it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.34it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.36it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.34it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.24it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.14it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.08it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.24it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.40it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.28it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 45.19it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.16it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.08it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.21it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.16it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.21it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 42.75it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 43.56it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 44.14it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 44.70it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 44.97it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.14it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.19it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.23it/s][A
100%|██████████| 608/608 [00:13<00:00, 44.97it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.97it/s][A 60%|██████    | 468/780 [03:17<01:31,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:44:44,830 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 03:44:45,200 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:44:48,400 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:44:48,619 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:44:48,738 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:29<40:42,  7.85s/it] 60%|██████    | 470/780 [03:29<28:52,  5.59s/it] 60%|██████    | 471/780 [03:29<20:35,  4.00s/it] 61%|██████    | 472/780 [03:30<14:49,  2.89s/it] 61%|██████    | 473/780 [03:30<10:47,  2.11s/it] 61%|██████    | 474/780 [03:30<07:58,  1.56s/it] 61%|██████    | 475/780 [03:31<06:00,  1.18s/it] 61%|██████    | 476/780 [03:31<04:38,  1.09it/s] 61%|██████    | 477/780 [03:31<03:40,  1.37it/s] 61%|██████▏   | 478/780 [03:31<03:00,  1.67it/s] 61%|██████▏   | 479/780 [03:32<02:32,  1.98it/s] 62%|██████▏   | 480/780 [03:32<02:12,  2.26it/s] 62%|██████▏   | 481/780 [03:32<02:02,  2.43it/s] 62%|██████▏   | 482/780 [03:33<01:51,  2.67it/s] 62%|██████▏   | 483/780 [03:33<01:44,  2.86it/s] 62%|██████▏   | 484/780 [03:33<01:38,  3.00it/s] 62%|██████▏   | 485/780 [03:33<01:34,  3.12it/s] 62%|██████▏   | 486/780 [03:34<01:31,  3.20it/s] 62%|██████▏   | 487/780 [03:34<01:29,  3.27it/s] 63%|██████▎   | 488/780 [03:34<01:28,  3.31it/s] 63%|██████▎   | 489/780 [03:35<01:27,  3.34it/s] 63%|██████▎   | 490/780 [03:35<01:26,  3.36it/s] 63%|██████▎   | 491/780 [03:35<01:25,  3.38it/s] 63%|██████▎   | 492/780 [03:36<01:29,  3.21it/s] 63%|██████▎   | 493/780 [03:36<01:27,  3.27it/s] 63%|██████▎   | 494/780 [03:36<01:26,  3.31it/s] 63%|██████▎   | 495/780 [03:36<01:25,  3.34it/s] 64%|██████▎   | 496/780 [03:37<01:24,  3.37it/s] 64%|██████▎   | 497/780 [03:37<01:23,  3.38it/s] 64%|██████▍   | 498/780 [03:37<01:23,  3.39it/s] 64%|██████▍   | 499/780 [03:38<01:22,  3.40it/s] 64%|██████▍   | 500/780 [03:38<01:22,  3.40it/s]                                                  64%|██████▍   | 500/780 [03:38<01:22,  3.40it/s] 64%|██████▍   | 501/780 [03:38<01:21,  3.41it/s] 64%|██████▍   | 502/780 [03:39<01:21,  3.41it/s] 64%|██████▍   | 503/780 [03:39<01:24,  3.29it/s] 65%|██████▍   | 504/780 [03:39<01:22,  3.33it/s] 65%|██████▍   | 505/780 [03:39<01:21,  3.36it/s] 65%|██████▍   | 506/780 [03:40<01:21,  3.38it/s] 65%|██████▌   | 507/780 [03:40<01:20,  3.39it/s] 65%|██████▌   | 508/780 [03:40<01:20,  3.40it/s] 65%|██████▌   | 509/780 [03:41<01:19,  3.41it/s] 65%|██████▌   | 510/780 [03:41<01:19,  3.41it/s] 66%|██████▌   | 511/780 [03:41<01:18,  3.41it/s] 66%|██████▌   | 512/780 [03:41<01:18,  3.42it/s] 66%|██████▌   | 513/780 [03:42<01:18,  3.42it/s] 66%|██████▌   | 514/780 [03:42<01:24,  3.16it/s] 66%|██████▌   | 515/780 [03:42<01:22,  3.23it/s] 66%|██████▌   | 516/780 [03:43<01:20,  3.28it/s] 66%|██████▋   | 517/780 [03:43<01:19,  3.32it/s] 66%|██████▋   | 518/780 [03:43<01:18,  3.35it/s] 67%|██████▋   | 519/780 [03:44<01:17,  3.37it/s] 67%|██████▋   | 520/780 [03:44<01:16,  3.39it/s] 67%|██████▋   | 521/780 [03:44<01:16,  3.40it/s] 67%|██████▋   | 522/780 [03:44<01:15,  3.40it/s] 67%|██████▋   | 523/780 [03:45<01:15,  3.41it/s] 67%|██████▋   | 524/780 [03:45<01:27,  2.93it/s] 67%|██████▋   | 525/780 [03:46<01:23,  3.06it/s] 67%|██████▋   | 526/780 [03:46<01:20,  3.16it/s] 68%|██████▊   | 527/780 [03:46<01:18,  3.23it/s] 68%|██████▊   | 528/780 [03:46<01:16,  3.28it/s] 68%|██████▊   | 529/780 [03:47<01:15,  3.32it/s] 68%|██████▊   | 530/780 [03:47<01:14,  3.35it/s] 68%|██████▊   | 531/780 [03:48<01:34,  2.63it/s] 68%|██████▊   | 532/780 [03:48<01:27,  2.82it/s] 68%|██████▊   | 533/780 [03:48<01:24,  2.92it/s] 68%|██████▊   | 534/780 [03:48<01:20,  3.06it/s] 69%|██████▊   | 535/780 [03:49<01:17,  3.15it/s] 69%|██████▊   | 536/780 [03:49<01:15,  3.23it/s] 69%|██████▉   | 537/780 [03:49<01:13,  3.28it/s] 69%|██████▉   | 538/780 [03:50<01:12,  3.32it/s] 69%|██████▉   | 539/780 [03:50<01:11,  3.35it/s] 69%|██████▉   | 540/780 [03:50<01:11,  3.37it/s] 69%|██████▉   | 541/780 [03:51<01:10,  3.38it/s] 69%|██████▉   | 542/780 [03:51<01:10,  3.39it/s] 70%|██████▉   | 543/780 [03:51<01:09,  3.40it/s] 70%|██████▉   | 544/780 [03:51<01:11,  3.30it/s] 70%|██████▉   | 545/780 [03:52<01:10,  3.33it/s] 70%|███████   | 546/780 [03:52<01:09,  3.36it/s] 70%|███████   | 547/780 [03:52<01:09,  3.38it/s] 70%|███████   | 548/780 [03:53<01:08,  3.39it/s] 70%|███████   | 549/780 [03:53<01:08,  3.39it/s] 71%|███████   | 550/780 [03:53<01:07,  3.40it/s] 71%|███████   | 551/780 [03:53<01:07,  3.40it/s] 71%|███████   | 552/780 [03:54<01:06,  3.41it/s] 71%|███████   | 553/780 [03:54<01:06,  3.41it/s] 71%|███████   | 554/780 [03:54<01:06,  3.41it/s] 71%|███████   | 555/780 [03:55<01:06,  3.41it/s] 71%|███████▏  | 556/780 [03:55<01:05,  3.41it/s] 71%|███████▏  | 557/780 [03:55<01:05,  3.41it/s] 72%|███████▏  | 558/780 [03:56<01:05,  3.41it/s] 72%|███████▏  | 559/780 [03:56<01:05,  3.36it/s] 72%|███████▏  | 560/780 [03:56<01:05,  3.37it/s] 72%|███████▏  | 561/780 [03:56<01:04,  3.38it/s] 72%|███████▏  | 562/780 [03:57<01:04,  3.39it/s] 72%|███████▏  | 563/780 [03:57<01:03,  3.40it/s] 72%|███████▏  | 564/780 [03:57<01:03,  3.40it/s] 72%|███████▏  | 565/780 [03:58<01:03,  3.41it/s] 73%|███████▎  | 566/780 [03:58<01:02,  3.41it/s] 73%|███████▎  | 567/780 [03:58<01:02,  3.41it/s] 73%|███████▎  | 568/780 [03:58<01:02,  3.41it/s] 73%|███████▎  | 569/780 [03:59<01:01,  3.41it/s] 73%|███████▎  | 570/780 [03:59<01:03,  3.32it/s] 73%|███████▎  | 571/780 [03:59<01:02,  3.35it/s] 73%|███████▎  | 572/780 [04:00<01:01,  3.37it/s] 73%|███████▎  | 573/780 [04:00<01:01,  3.38it/s] 74%|███████▎  | 574/780 [04:00<01:00,  3.39it/s] 74%|███████▎  | 575/780 [04:01<01:00,  3.40it/s] 74%|███████▍  | 576/780 [04:01<00:59,  3.40it/s] 74%|███████▍  | 577/780 [04:01<00:59,  3.41it/s] 74%|███████▍  | 578/780 [04:01<00:59,  3.41it/s] 74%|███████▍  | 579/780 [04:02<00:58,  3.41it/s] 74%|███████▍  | 580/780 [04:02<00:58,  3.41it/s] 74%|███████▍  | 581/780 [04:02<00:59,  3.33it/s] 75%|███████▍  | 582/780 [04:03<00:59,  3.36it/s] 75%|███████▍  | 583/780 [04:03<00:58,  3.37it/s] 75%|███████▍  | 584/780 [04:03<00:57,  3.38it/s] 75%|███████▌  | 585/780 [04:03<00:57,  3.40it/s] 75%|███████▌  | 586/780 [04:04<00:57,  3.40it/s] 75%|███████▌  | 587/780 [04:04<00:56,  3.40it/s] 75%|███████▌  | 588/780 [04:04<00:56,  3.41it/s] 76%|███████▌  | 589/780 [04:05<00:56,  3.41it/s] 76%|███████▌  | 590/780 [04:05<00:55,  3.41it/s] 76%|███████▌  | 591/780 [04:05<00:55,  3.41it/s] 76%|███████▌  | 592/780 [04:06<00:58,  3.22it/s] 76%|███████▌  | 593/780 [04:06<00:57,  3.27it/s] 76%|███████▌  | 594/780 [04:06<00:56,  3.32it/s] 76%|███████▋  | 595/780 [04:06<00:55,  3.34it/s] 76%|███████▋  | 596/780 [04:07<00:54,  3.36it/s] 77%|███████▋  | 597/780 [04:07<00:54,  3.38it/s] 77%|███████▋  | 598/780 [04:07<00:53,  3.39it/s] 77%|███████▋  | 599/780 [04:08<00:53,  3.40it/s] 77%|███████▋  | 600/780 [04:08<00:52,  3.40it/s] 77%|███████▋  | 601/780 [04:08<00:52,  3.40it/s] 77%|███████▋  | 602/780 [04:09<00:52,  3.40it/s] 77%|███████▋  | 603/780 [04:09<00:53,  3.32it/s] 77%|███████▋  | 604/780 [04:09<00:52,  3.35it/s] 78%|███████▊  | 605/780 [04:09<00:52,  3.36it/s] 78%|███████▊  | 606/780 [04:10<00:51,  3.38it/s] 78%|███████▊  | 607/780 [04:10<00:51,  3.39it/s] 78%|███████▊  | 608/780 [04:10<00:50,  3.40it/s] 78%|███████▊  | 609/780 [04:11<00:50,  3.40it/s] 78%|███████▊  | 610/780 [04:11<00:49,  3.41it/s] 78%|███████▊  | 611/780 [04:11<00:49,  3.41it/s] 78%|███████▊  | 612/780 [04:11<00:49,  3.41it/s] 79%|███████▊  | 613/780 [04:12<00:48,  3.41it/s] 79%|███████▊  | 614/780 [04:12<00:50,  3.26it/s] 79%|███████▉  | 615/780 [04:12<00:49,  3.30it/s] 79%|███████▉  | 616/780 [04:13<00:49,  3.34it/s] 79%|███████▉  | 617/780 [04:13<00:48,  3.36it/s] 79%|███████▉  | 618/780 [04:13<00:47,  3.38it/s] 79%|███████▉  | 619/780 [04:14<00:47,  3.39it/s] 79%|███████▉  | 620/780 [04:14<00:47,  3.39it/s] 80%|███████▉  | 621/780 [04:14<00:46,  3.41it/s] 80%|███████▉  | 622/780 [04:14<00:46,  3.42it/s] 80%|███████▉  | 623/780 [04:15<00:45,  3.44it/s] 80%|████████  | 624/780 [04:15<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 03:45:42,820 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:45:42,820 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 03:45:42,820 >>   Batch size = 8
{'eval_loss': 0.962094783782959, 'eval_runtime': 13.5038, 'eval_samples_per_second': 360.194, 'eval_steps_per_second': 45.024, 'epoch': 3.0}
{'loss': 0.6087, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.76it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.52it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.86it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.99it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.43it/s][A
  5%|▌         | 33/608 [00:00<00:12, 46.10it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.62it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.23it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.28it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.39it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.54it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.63it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.61it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.52it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.42it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.23it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.06it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.06it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.15it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 45.33it/s][A
 18%|█▊        | 108/608 [00:02<00:10, 45.50it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.57it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.53it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.45it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.25it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.00it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 43.64it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 44.23it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 44.65it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.04it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.18it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.28it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.08it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 44.96it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.83it/s][A
 30%|███       | 183/608 [00:04<00:09, 44.94it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.11it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.24it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.47it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.54it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.52it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.32it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.17it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.02it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 44.90it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.19it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.39it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.47it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.58it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.57it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.49it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.13it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 44.98it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.10it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 44.23it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 44.76it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.06it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.23it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.24it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.25it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.10it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 44.89it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 44.95it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 45.01it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.24it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.32it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.46it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.57it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.48it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.19it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.08it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 45.05it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.16it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.17it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.28it/s][A
 63%|██████▎   | 383/608 [00:08<00:04, 45.40it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 45.53it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 45.49it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 45.36it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 45.13it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 45.13it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.21it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.24it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.29it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.39it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.50it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.47it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.34it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.28it/s][A
 75%|███████▍  | 453/608 [00:09<00:03, 45.16it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.15it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.28it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.33it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.40it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.50it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.49it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.35it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.19it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 42.69it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 43.56it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 44.05it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 44.53it/s][A
 85%|████████▌ | 518/608 [00:11<00:02, 44.81it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 45.05it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 45.18it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 45.09it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 44.71it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 44.86it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.09it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.25it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.33it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.49it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.50it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.45it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.26it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 44.91it/s][A
 97%|█████████▋| 588/608 [00:12<00:00, 45.08it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.13it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.25it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.40it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.49it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.49it/s][A 80%|████████  | 624/780 [04:29<00:45,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:45:56,457 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 03:45:56,642 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:45:59,392 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:45:59,552 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:45:59,631 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:41<20:28,  7.93s/it] 80%|████████  | 626/780 [04:41<14:30,  5.66s/it] 80%|████████  | 627/780 [04:41<10:19,  4.05s/it] 81%|████████  | 628/780 [04:42<07:23,  2.92s/it] 81%|████████  | 629/780 [04:42<05:21,  2.13s/it] 81%|████████  | 630/780 [04:42<03:57,  1.58s/it] 81%|████████  | 631/780 [04:43<02:57,  1.19s/it] 81%|████████  | 632/780 [04:43<02:16,  1.08it/s] 81%|████████  | 633/780 [04:43<01:47,  1.36it/s] 81%|████████▏ | 634/780 [04:43<01:27,  1.66it/s] 81%|████████▏ | 635/780 [04:44<01:13,  1.96it/s] 82%|████████▏ | 636/780 [04:44<01:03,  2.25it/s] 82%|████████▏ | 637/780 [04:44<00:59,  2.41it/s] 82%|████████▏ | 638/780 [04:45<00:53,  2.65it/s] 82%|████████▏ | 639/780 [04:45<00:49,  2.84it/s] 82%|████████▏ | 640/780 [04:45<00:46,  2.98it/s] 82%|████████▏ | 641/780 [04:46<00:44,  3.10it/s] 82%|████████▏ | 642/780 [04:46<00:43,  3.19it/s] 82%|████████▏ | 643/780 [04:46<00:42,  3.26it/s] 83%|████████▎ | 644/780 [04:46<00:41,  3.30it/s] 83%|████████▎ | 645/780 [04:47<00:40,  3.34it/s] 83%|████████▎ | 646/780 [04:47<00:39,  3.36it/s] 83%|████████▎ | 647/780 [04:48<00:57,  2.33it/s] 83%|████████▎ | 648/780 [04:48<00:53,  2.47it/s] 83%|████████▎ | 649/780 [04:48<00:48,  2.69it/s] 83%|████████▎ | 650/780 [04:49<00:45,  2.88it/s] 83%|████████▎ | 651/780 [04:49<00:42,  3.02it/s] 84%|████████▎ | 652/780 [04:49<00:40,  3.13it/s] 84%|████████▎ | 653/780 [04:50<00:39,  3.21it/s] 84%|████████▍ | 654/780 [04:50<00:38,  3.27it/s] 84%|████████▍ | 655/780 [04:50<00:37,  3.31it/s] 84%|████████▍ | 656/780 [04:50<00:37,  3.34it/s] 84%|████████▍ | 657/780 [04:51<00:36,  3.36it/s] 84%|████████▍ | 658/780 [04:51<00:36,  3.38it/s] 84%|████████▍ | 659/780 [04:51<00:36,  3.31it/s] 85%|████████▍ | 660/780 [04:52<00:35,  3.34it/s] 85%|████████▍ | 661/780 [04:52<00:35,  3.36it/s] 85%|████████▍ | 662/780 [04:52<00:34,  3.38it/s] 85%|████████▌ | 663/780 [04:53<00:34,  3.39it/s] 85%|████████▌ | 664/780 [04:53<00:34,  3.40it/s] 85%|████████▌ | 665/780 [04:53<00:33,  3.40it/s] 85%|████████▌ | 666/780 [04:53<00:33,  3.40it/s] 86%|████████▌ | 667/780 [04:54<00:33,  3.41it/s] 86%|████████▌ | 668/780 [04:54<00:32,  3.41it/s] 86%|████████▌ | 669/780 [04:54<00:32,  3.41it/s] 86%|████████▌ | 670/780 [04:55<00:32,  3.42it/s] 86%|████████▌ | 671/780 [04:55<00:31,  3.41it/s] 86%|████████▌ | 672/780 [04:55<00:31,  3.41it/s] 86%|████████▋ | 673/780 [04:55<00:31,  3.42it/s] 86%|████████▋ | 674/780 [04:56<00:31,  3.42it/s] 87%|████████▋ | 675/780 [04:56<00:30,  3.41it/s] 87%|████████▋ | 676/780 [04:56<00:30,  3.42it/s] 87%|████████▋ | 677/780 [04:57<00:30,  3.41it/s] 87%|████████▋ | 678/780 [04:57<00:29,  3.41it/s] 87%|████████▋ | 679/780 [04:57<00:30,  3.33it/s] 87%|████████▋ | 680/780 [04:58<00:29,  3.35it/s] 87%|████████▋ | 681/780 [04:58<00:29,  3.37it/s] 87%|████████▋ | 682/780 [04:58<00:28,  3.39it/s] 88%|████████▊ | 683/780 [04:58<00:28,  3.40it/s] 88%|████████▊ | 684/780 [04:59<00:28,  3.40it/s] 88%|████████▊ | 685/780 [04:59<00:27,  3.41it/s] 88%|████████▊ | 686/780 [04:59<00:27,  3.41it/s] 88%|████████▊ | 687/780 [05:00<00:27,  3.41it/s] 88%|████████▊ | 688/780 [05:00<00:26,  3.41it/s] 88%|████████▊ | 689/780 [05:00<00:26,  3.41it/s] 88%|████████▊ | 690/780 [05:01<00:27,  3.22it/s] 89%|████████▊ | 691/780 [05:01<00:27,  3.28it/s] 89%|████████▊ | 692/780 [05:01<00:26,  3.32it/s] 89%|████████▉ | 693/780 [05:01<00:25,  3.35it/s] 89%|████████▉ | 694/780 [05:02<00:25,  3.37it/s] 89%|████████▉ | 695/780 [05:02<00:25,  3.38it/s] 89%|████████▉ | 696/780 [05:02<00:24,  3.39it/s] 89%|████████▉ | 697/780 [05:03<00:24,  3.39it/s] 89%|████████▉ | 698/780 [05:03<00:24,  3.40it/s] 90%|████████▉ | 699/780 [05:03<00:23,  3.40it/s] 90%|████████▉ | 700/780 [05:03<00:23,  3.40it/s] 90%|████████▉ | 701/780 [05:04<00:23,  3.29it/s] 90%|█████████ | 702/780 [05:04<00:23,  3.33it/s] 90%|█████████ | 703/780 [05:04<00:22,  3.35it/s] 90%|█████████ | 704/780 [05:05<00:22,  3.37it/s] 90%|█████████ | 705/780 [05:05<00:22,  3.38it/s] 91%|█████████ | 706/780 [05:05<00:21,  3.39it/s] 91%|█████████ | 707/780 [05:06<00:21,  3.40it/s] 91%|█████████ | 708/780 [05:06<00:21,  3.40it/s] 91%|█████████ | 709/780 [05:06<00:20,  3.40it/s] 91%|█████████ | 710/780 [05:06<00:20,  3.41it/s] 91%|█████████ | 711/780 [05:07<00:20,  3.41it/s] 91%|█████████▏| 712/780 [05:07<00:20,  3.32it/s] 91%|█████████▏| 713/780 [05:07<00:20,  3.35it/s] 92%|█████████▏| 714/780 [05:08<00:19,  3.37it/s] 92%|█████████▏| 715/780 [05:08<00:19,  3.38it/s] 92%|█████████▏| 716/780 [05:08<00:18,  3.39it/s] 92%|█████████▏| 717/780 [05:09<00:18,  3.40it/s] 92%|█████████▏| 718/780 [05:09<00:18,  3.40it/s] 92%|█████████▏| 719/780 [05:09<00:17,  3.40it/s] 92%|█████████▏| 720/780 [05:09<00:17,  3.40it/s] 92%|█████████▏| 721/780 [05:10<00:17,  3.40it/s] 93%|█████████▎| 722/780 [05:10<00:17,  3.40it/s] 93%|█████████▎| 723/780 [05:10<00:16,  3.37it/s] 93%|█████████▎| 724/780 [05:11<00:16,  3.39it/s] 93%|█████████▎| 725/780 [05:11<00:16,  3.41it/s] 93%|█████████▎| 726/780 [05:11<00:15,  3.43it/s] 93%|█████████▎| 727/780 [05:11<00:15,  3.44it/s] 93%|█████████▎| 728/780 [05:12<00:15,  3.45it/s] 93%|█████████▎| 729/780 [05:12<00:14,  3.45it/s] 94%|█████████▎| 730/780 [05:12<00:14,  3.46it/s] 94%|█████████▎| 731/780 [05:13<00:14,  3.46it/s] 94%|█████████▍| 732/780 [05:13<00:13,  3.46it/s] 94%|█████████▍| 733/780 [05:13<00:13,  3.46it/s] 94%|█████████▍| 734/780 [05:13<00:13,  3.39it/s] 94%|█████████▍| 735/780 [05:14<00:13,  3.42it/s] 94%|█████████▍| 736/780 [05:14<00:12,  3.43it/s] 94%|█████████▍| 737/780 [05:14<00:12,  3.44it/s] 95%|█████████▍| 738/780 [05:15<00:12,  3.44it/s] 95%|█████████▍| 739/780 [05:15<00:11,  3.45it/s] 95%|█████████▍| 740/780 [05:15<00:11,  3.45it/s] 95%|█████████▌| 741/780 [05:16<00:11,  3.45it/s] 95%|█████████▌| 742/780 [05:16<00:10,  3.46it/s] 95%|█████████▌| 743/780 [05:16<00:10,  3.46it/s] 95%|█████████▌| 744/780 [05:16<00:10,  3.46it/s] 96%|█████████▌| 745/780 [05:17<00:10,  3.39it/s] 96%|█████████▌| 746/780 [05:17<00:09,  3.41it/s] 96%|█████████▌| 747/780 [05:17<00:09,  3.43it/s] 96%|█████████▌| 748/780 [05:18<00:09,  3.44it/s] 96%|█████████▌| 749/780 [05:18<00:09,  3.44it/s] 96%|█████████▌| 750/780 [05:18<00:08,  3.45it/s] 96%|█████████▋| 751/780 [05:18<00:08,  3.45it/s] 96%|█████████▋| 752/780 [05:19<00:08,  3.45it/s] 97%|█████████▋| 753/780 [05:19<00:07,  3.46it/s] 97%|█████████▋| 754/780 [05:19<00:07,  3.45it/s] 97%|█████████▋| 755/780 [05:20<00:07,  3.46it/s] 97%|█████████▋| 756/780 [05:20<00:07,  3.32it/s] 97%|█████████▋| 757/780 [05:20<00:06,  3.36it/s] 97%|█████████▋| 758/780 [05:20<00:06,  3.39it/s] 97%|█████████▋| 759/780 [05:21<00:06,  3.41it/s] 97%|█████████▋| 760/780 [05:21<00:05,  3.42it/s] 98%|█████████▊| 761/780 [05:21<00:05,  3.43it/s] 98%|█████████▊| 762/780 [05:22<00:05,  3.45it/s] 98%|█████████▊| 763/780 [05:22<00:04,  3.45it/s] 98%|█████████▊| 764/780 [05:22<00:04,  3.45it/s] 98%|█████████▊| 765/780 [05:22<00:04,  3.45it/s] 98%|█████████▊| 766/780 [05:23<00:04,  3.46it/s] 98%|█████████▊| 767/780 [05:23<00:03,  3.38it/s] 98%|█████████▊| 768/780 [05:23<00:03,  3.40it/s] 99%|█████████▊| 769/780 [05:24<00:03,  3.42it/s] 99%|█████████▊| 770/780 [05:24<00:02,  3.43it/s] 99%|█████████▉| 771/780 [05:24<00:02,  3.44it/s] 99%|█████████▉| 772/780 [05:25<00:02,  3.45it/s] 99%|█████████▉| 773/780 [05:25<00:02,  3.45it/s] 99%|█████████▉| 774/780 [05:25<00:01,  3.45it/s] 99%|█████████▉| 775/780 [05:25<00:01,  3.46it/s] 99%|█████████▉| 776/780 [05:26<00:01,  3.45it/s]100%|█████████▉| 777/780 [05:26<00:00,  3.46it/s]100%|█████████▉| 778/780 [05:26<00:00,  3.46it/s]100%|█████████▉| 779/780 [05:27<00:00,  3.46it/s]100%|██████████| 780/780 [05:27<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 03:46:54,489 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:46:54,489 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 03:46:54,489 >>   Batch size = 8
{'eval_loss': 0.9708976745605469, 'eval_runtime': 13.4603, 'eval_samples_per_second': 361.359, 'eval_steps_per_second': 45.17, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.70it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.31it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.75it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.86it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.27it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.89it/s][A
  6%|▌         | 37/608 [00:00<00:13, 43.35it/s][A
  7%|▋         | 42/608 [00:00<00:12, 43.91it/s][A
  8%|▊         | 47/608 [00:01<00:12, 44.46it/s][A
  9%|▊         | 52/608 [00:01<00:12, 44.89it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.05it/s][A
 10%|█         | 62/608 [00:01<00:12, 44.98it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.20it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.13it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 44.90it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.97it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.09it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 45.23it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.32it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.19it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.41it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.51it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.34it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.15it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.03it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 45.05it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 45.06it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.35it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.42it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.58it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.49it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.39it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.18it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 44.87it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.85it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 45.04it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.28it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.39it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.40it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.51it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.43it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.13it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.98it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.11it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.27it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.35it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.49it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.46it/s][A
 41%|████      | 247/608 [00:05<00:07, 45.53it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.32it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 45.16it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.08it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.17it/s][A
 45%|████▍     | 272/608 [00:05<00:07, 45.30it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.42it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.50it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.57it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.51it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.26it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.09it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.01it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 44.41it/s][A
 52%|█████▏    | 317/608 [00:06<00:06, 44.81it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.10it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.26it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.34it/s][A
 55%|█████▌    | 337/608 [00:07<00:05, 45.38it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.21it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.14it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 44.94it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.04it/s][A
 60%|█████▉    | 362/608 [00:07<00:05, 45.14it/s][A
 60%|██████    | 367/608 [00:08<00:05, 45.28it/s][A
 61%|██████    | 372/608 [00:08<00:05, 45.32it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.56it/s][A
 63%|██████▎   | 382/608 [00:08<00:04, 45.60it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.44it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 45.27it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.09it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 45.13it/s][A
 67%|██████▋   | 407/608 [00:08<00:04, 45.20it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.32it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.43it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.57it/s][A
 70%|███████   | 427/608 [00:09<00:03, 45.56it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.36it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.25it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.09it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.12it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 40.99it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 42.36it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 43.33it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 44.10it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 44.62it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 44.82it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.00it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.08it/s][A
 81%|████████  | 492/608 [00:10<00:02, 44.70it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.75it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.01it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.30it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.47it/s][A
 85%|████████▌ | 517/608 [00:11<00:01, 45.55it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.36it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.38it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.13it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 44.82it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.88it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.02it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.20it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.20it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.30it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.49it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.56it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.27it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.10it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 42.15it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 43.17it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 43.97it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.52it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 44.77it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.77it/s][A100%|██████████| 780/780 [05:40<00:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:47:08,265 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 03:47:08,583 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:47:13,720 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:47:13,903 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:47:14,002 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 03:47:24,422 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 03:47:24,423 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156 (score: 0.9352977871894836).
                                                 100%|██████████| 780/780 [06:06<00:00,  3.46it/s]100%|██████████| 780/780 [06:06<00:00,  2.13it/s]
[INFO|trainer.py:1894] 2023-08-29 03:47:33,626 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 03:47:33,823 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:47:37,179 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:47:37,282 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:47:37,331 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:47:37,767 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:37,767 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:37,767 >>   train_loss               =     0.5982
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:37,767 >>   train_runtime            = 0:06:06.43
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:37,767 >>   train_samples            =      10015
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:37,767 >>   train_samples_per_second =    136.655
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:37,767 >>   train_steps_per_second   =      2.129
{'eval_loss': 0.9745804667472839, 'eval_runtime': 13.5062, 'eval_samples_per_second': 360.131, 'eval_steps_per_second': 45.016, 'epoch': 5.0}
{'train_runtime': 366.4344, 'train_samples_per_second': 136.655, 'train_steps_per_second': 2.129, 'train_loss': 0.5981738750751202, 'epoch': 5.0}
08/29/2023 03:47:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 03:47:38,009 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:47:38,009 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 03:47:38,009 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 56.48it/s]  2%|▏         | 12/608 [00:00<00:11, 50.13it/s]  3%|▎         | 18/608 [00:00<00:12, 48.23it/s]  4%|▍         | 23/608 [00:00<00:12, 47.47it/s]  5%|▍         | 28/608 [00:00<00:12, 47.05it/s]  5%|▌         | 33/608 [00:00<00:12, 46.77it/s]  6%|▋         | 38/608 [00:00<00:12, 46.60it/s]  7%|▋         | 43/608 [00:00<00:12, 46.07it/s]  8%|▊         | 48/608 [00:01<00:12, 45.61it/s]  9%|▊         | 53/608 [00:01<00:12, 45.42it/s] 10%|▉         | 58/608 [00:01<00:12, 45.45it/s] 10%|█         | 63/608 [00:01<00:11, 45.50it/s] 11%|█         | 68/608 [00:01<00:11, 45.74it/s] 12%|█▏        | 73/608 [00:01<00:11, 45.92it/s] 13%|█▎        | 78/608 [00:01<00:11, 45.78it/s] 14%|█▎        | 83/608 [00:01<00:11, 45.96it/s] 14%|█▍        | 88/608 [00:01<00:11, 45.63it/s] 15%|█▌        | 93/608 [00:02<00:11, 45.50it/s] 16%|█▌        | 98/608 [00:02<00:11, 45.29it/s] 17%|█▋        | 103/608 [00:02<00:11, 45.32it/s] 18%|█▊        | 108/608 [00:02<00:11, 44.08it/s] 19%|█▊        | 113/608 [00:02<00:11, 44.68it/s] 19%|█▉        | 118/608 [00:02<00:10, 45.15it/s] 20%|██        | 123/608 [00:02<00:10, 45.44it/s] 21%|██        | 128/608 [00:02<00:10, 45.62it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.56it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.44it/s] 24%|██▎       | 143/608 [00:03<00:10, 45.25it/s] 24%|██▍       | 148/608 [00:03<00:10, 45.13it/s] 25%|██▌       | 153/608 [00:03<00:10, 45.23it/s] 26%|██▌       | 158/608 [00:03<00:09, 45.16it/s] 27%|██▋       | 163/608 [00:03<00:09, 45.48it/s] 28%|██▊       | 168/608 [00:03<00:09, 45.66it/s] 28%|██▊       | 173/608 [00:03<00:09, 45.70it/s] 29%|██▉       | 178/608 [00:03<00:09, 45.79it/s] 30%|███       | 183/608 [00:03<00:09, 45.78it/s] 31%|███       | 188/608 [00:04<00:09, 45.69it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.54it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.39it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.42it/s] 34%|███▍      | 208/608 [00:04<00:08, 45.50it/s] 35%|███▌      | 213/608 [00:04<00:08, 45.48it/s] 36%|███▌      | 218/608 [00:04<00:08, 45.65it/s] 37%|███▋      | 223/608 [00:04<00:08, 45.73it/s] 38%|███▊      | 228/608 [00:04<00:08, 45.72it/s] 38%|███▊      | 233/608 [00:05<00:08, 45.72it/s] 39%|███▉      | 238/608 [00:05<00:08, 45.54it/s] 40%|███▉      | 243/608 [00:05<00:08, 45.51it/s] 41%|████      | 248/608 [00:05<00:08, 43.71it/s] 42%|████▏     | 253/608 [00:05<00:07, 44.46it/s] 42%|████▏     | 258/608 [00:05<00:07, 44.73it/s] 43%|████▎     | 263/608 [00:05<00:07, 45.10it/s] 44%|████▍     | 268/608 [00:05<00:07, 45.32it/s] 45%|████▍     | 273/608 [00:05<00:07, 45.49it/s] 46%|████▌     | 278/608 [00:06<00:07, 45.50it/s] 47%|████▋     | 283/608 [00:06<00:07, 45.45it/s] 47%|████▋     | 288/608 [00:06<00:07, 45.10it/s] 48%|████▊     | 293/608 [00:06<00:06, 45.20it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.29it/s] 50%|████▉     | 303/608 [00:06<00:06, 45.49it/s] 51%|█████     | 308/608 [00:06<00:06, 45.65it/s] 51%|█████▏    | 313/608 [00:06<00:06, 45.71it/s] 52%|█████▏    | 318/608 [00:06<00:06, 45.79it/s] 53%|█████▎    | 323/608 [00:07<00:06, 45.64it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.63it/s] 55%|█████▍    | 333/608 [00:07<00:06, 45.33it/s] 56%|█████▌    | 338/608 [00:07<00:05, 45.26it/s] 56%|█████▋    | 343/608 [00:07<00:05, 45.43it/s] 57%|█████▋    | 348/608 [00:07<00:05, 45.58it/s] 58%|█████▊    | 353/608 [00:07<00:05, 45.56it/s] 59%|█████▉    | 358/608 [00:07<00:05, 45.64it/s] 60%|█████▉    | 363/608 [00:07<00:05, 45.64it/s] 61%|██████    | 368/608 [00:08<00:05, 45.52it/s] 61%|██████▏   | 373/608 [00:08<00:05, 45.44it/s] 62%|██████▏   | 378/608 [00:08<00:05, 45.20it/s] 63%|██████▎   | 383/608 [00:08<00:04, 45.17it/s] 64%|██████▍   | 388/608 [00:08<00:05, 43.26it/s] 65%|██████▍   | 393/608 [00:08<00:04, 44.10it/s] 65%|██████▌   | 398/608 [00:08<00:04, 44.70it/s] 66%|██████▋   | 403/608 [00:08<00:04, 45.13it/s] 67%|██████▋   | 408/608 [00:08<00:04, 45.38it/s] 68%|██████▊   | 413/608 [00:09<00:04, 45.43it/s] 69%|██████▉   | 418/608 [00:09<00:04, 45.23it/s] 70%|██████▉   | 423/608 [00:09<00:04, 45.20it/s] 70%|███████   | 428/608 [00:09<00:03, 45.05it/s] 71%|███████   | 433/608 [00:09<00:03, 45.14it/s] 72%|███████▏  | 438/608 [00:09<00:03, 45.39it/s] 73%|███████▎  | 443/608 [00:09<00:03, 45.55it/s] 74%|███████▎  | 448/608 [00:09<00:03, 45.75it/s] 75%|███████▍  | 453/608 [00:09<00:03, 45.90it/s] 75%|███████▌  | 458/608 [00:10<00:03, 45.78it/s] 76%|███████▌  | 463/608 [00:10<00:03, 45.51it/s] 77%|███████▋  | 468/608 [00:10<00:03, 45.35it/s] 78%|███████▊  | 473/608 [00:10<00:02, 45.15it/s] 79%|███████▊  | 478/608 [00:10<00:02, 45.18it/s] 79%|███████▉  | 483/608 [00:10<00:02, 45.20it/s] 80%|████████  | 488/608 [00:10<00:02, 45.44it/s] 81%|████████  | 493/608 [00:10<00:02, 45.56it/s] 82%|████████▏ | 498/608 [00:10<00:02, 45.69it/s] 83%|████████▎ | 503/608 [00:11<00:02, 45.74it/s] 84%|████████▎ | 508/608 [00:11<00:02, 45.63it/s] 84%|████████▍ | 513/608 [00:11<00:02, 45.44it/s] 85%|████████▌ | 518/608 [00:11<00:01, 45.23it/s] 86%|████████▌ | 523/608 [00:11<00:01, 45.16it/s] 87%|████████▋ | 528/608 [00:11<00:01, 43.95it/s] 88%|████████▊ | 533/608 [00:11<00:01, 44.55it/s] 88%|████████▊ | 538/608 [00:11<00:01, 44.96it/s] 89%|████████▉ | 543/608 [00:11<00:01, 45.24it/s] 90%|█████████ | 548/608 [00:12<00:01, 45.43it/s] 91%|█████████ | 553/608 [00:12<00:01, 45.46it/s] 92%|█████████▏| 558/608 [00:12<00:01, 45.27it/s] 93%|█████████▎| 563/608 [00:12<00:00, 45.20it/s] 93%|█████████▎| 568/608 [00:12<00:00, 45.08it/s] 94%|█████████▍| 573/608 [00:12<00:00, 45.16it/s] 95%|█████████▌| 578/608 [00:12<00:00, 45.33it/s] 96%|█████████▌| 583/608 [00:12<00:00, 45.38it/s] 97%|█████████▋| 588/608 [00:12<00:00, 45.48it/s] 98%|█████████▊| 593/608 [00:13<00:00, 45.69it/s] 98%|█████████▊| 598/608 [00:13<00:00, 45.64it/s] 99%|█████████▉| 603/608 [00:13<00:00, 45.55it/s]100%|██████████| 608/608 [00:13<00:00, 45.35it/s]100%|██████████| 608/608 [00:13<00:00, 45.41it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:47:51,415 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:51,415 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:51,415 >>   eval_loss               =     0.9353
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:51,415 >>   eval_runtime            = 0:00:13.40
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:51,415 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:51,415 >>   eval_samples_per_second =    362.838
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:51,415 >>   eval_steps_per_second   =     45.355
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:47:51,415 >>   perplexity              =      2.548
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:01,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:01,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:01,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:01,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:01,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:48:02,232 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:48:02,233 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:48:02,548 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:48:03,697 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:48:03,697 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:06,512 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:06,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:06,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:06,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:06,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:48:07,125 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:48:07,126 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:48:07,890 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:48:08,177 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:48:08,177 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.74it/s]Extractor Predicting: 5it [00:02,  1.76it/s]Extractor Predicting: 6it [00:03,  1.81it/s]Extractor Predicting: 7it [00:04,  1.72it/s]Extractor Predicting: 8it [00:04,  1.68it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.78it/s]Extractor Predicting: 11it [00:06,  1.77it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.80it/s]Extractor Predicting: 14it [00:08,  1.76it/s]Extractor Predicting: 15it [00:08,  1.83it/s]Extractor Predicting: 16it [00:09,  1.81it/s]Extractor Predicting: 17it [00:09,  1.78it/s]Extractor Predicting: 18it [00:10,  1.78it/s]Extractor Predicting: 19it [00:10,  1.76it/s]Extractor Predicting: 20it [00:11,  1.81it/s]Extractor Predicting: 21it [00:12,  1.69it/s]Extractor Predicting: 22it [00:12,  1.68it/s]Extractor Predicting: 23it [00:13,  1.64it/s]Extractor Predicting: 24it [00:13,  1.60it/s]Extractor Predicting: 25it [00:14,  1.56it/s]Extractor Predicting: 26it [00:15,  1.57it/s]Extractor Predicting: 27it [00:15,  1.57it/s]Extractor Predicting: 28it [00:16,  1.59it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:17,  1.65it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:18,  1.68it/s]Extractor Predicting: 33it [00:19,  1.69it/s]Extractor Predicting: 34it [00:20,  1.69it/s]Extractor Predicting: 35it [00:20,  1.65it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:21,  1.65it/s]Extractor Predicting: 38it [00:22,  1.64it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:23,  1.64it/s]Extractor Predicting: 41it [00:24,  1.66it/s]Extractor Predicting: 42it [00:24,  1.64it/s]Extractor Predicting: 43it [00:25,  1.61it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:26,  1.64it/s]Extractor Predicting: 46it [00:27,  1.65it/s]Extractor Predicting: 47it [00:27,  1.67it/s]Extractor Predicting: 48it [00:28,  1.65it/s]Extractor Predicting: 49it [00:29,  1.64it/s]Extractor Predicting: 50it [00:29,  1.65it/s]Extractor Predicting: 51it [00:30,  1.63it/s]Extractor Predicting: 52it [00:31,  1.63it/s]Extractor Predicting: 53it [00:31,  1.61it/s]Extractor Predicting: 54it [00:32,  1.68it/s]Extractor Predicting: 55it [00:32,  1.71it/s]Extractor Predicting: 56it [00:33,  1.70it/s]Extractor Predicting: 57it [00:33,  1.68it/s]Extractor Predicting: 58it [00:34,  1.66it/s]Extractor Predicting: 59it [00:35,  1.65it/s]Extractor Predicting: 60it [00:35,  1.64it/s]Extractor Predicting: 61it [00:36,  1.59it/s]Extractor Predicting: 62it [00:37,  1.61it/s]Extractor Predicting: 63it [00:37,  1.62it/s]Extractor Predicting: 64it [00:38,  1.63it/s]Extractor Predicting: 65it [00:38,  1.63it/s]Extractor Predicting: 66it [00:39,  1.65it/s]Extractor Predicting: 67it [00:40,  1.66it/s]Extractor Predicting: 68it [00:40,  1.63it/s]Extractor Predicting: 69it [00:41,  1.63it/s]Extractor Predicting: 70it [00:41,  1.66it/s]Extractor Predicting: 71it [00:42,  1.70it/s]Extractor Predicting: 72it [00:43,  1.69it/s]Extractor Predicting: 73it [00:43,  1.69it/s]Extractor Predicting: 74it [00:44,  1.67it/s]Extractor Predicting: 75it [00:44,  1.69it/s]Extractor Predicting: 76it [00:45,  1.68it/s]Extractor Predicting: 77it [00:46,  1.67it/s]Extractor Predicting: 78it [00:46,  1.70it/s]Extractor Predicting: 79it [00:47,  1.68it/s]Extractor Predicting: 80it [00:47,  1.71it/s]Extractor Predicting: 81it [00:48,  1.68it/s]Extractor Predicting: 82it [00:49,  1.66it/s]Extractor Predicting: 83it [00:49,  1.65it/s]Extractor Predicting: 84it [00:50,  1.67it/s]Extractor Predicting: 85it [00:50,  1.66it/s]Extractor Predicting: 86it [00:51,  1.65it/s]Extractor Predicting: 87it [00:52,  1.67it/s]Extractor Predicting: 88it [00:52,  1.66it/s]Extractor Predicting: 89it [00:53,  1.55it/s]Extractor Predicting: 90it [00:53,  1.61it/s]Extractor Predicting: 91it [00:54,  1.64it/s]Extractor Predicting: 92it [00:55,  1.66it/s]Extractor Predicting: 93it [00:55,  1.69it/s]Extractor Predicting: 94it [00:56,  1.62it/s]Extractor Predicting: 95it [00:57,  1.59it/s]Extractor Predicting: 96it [00:57,  1.61it/s]Extractor Predicting: 97it [00:58,  1.63it/s]Extractor Predicting: 98it [00:59,  1.50it/s]Extractor Predicting: 99it [00:59,  1.50it/s]Extractor Predicting: 100it [01:00,  1.53it/s]Extractor Predicting: 101it [01:00,  1.57it/s]Extractor Predicting: 102it [01:01,  1.61it/s]Extractor Predicting: 103it [01:02,  1.65it/s]Extractor Predicting: 104it [01:02,  1.63it/s]Extractor Predicting: 105it [01:03,  1.62it/s]Extractor Predicting: 106it [01:03,  1.63it/s]Extractor Predicting: 107it [01:04,  1.64it/s]Extractor Predicting: 108it [01:05,  1.68it/s]Extractor Predicting: 109it [01:05,  1.56it/s]Extractor Predicting: 110it [01:06,  1.54it/s]Extractor Predicting: 111it [01:07,  1.59it/s]Extractor Predicting: 112it [01:07,  1.60it/s]Extractor Predicting: 113it [01:08,  1.63it/s]Extractor Predicting: 114it [01:08,  1.65it/s]Extractor Predicting: 115it [01:09,  1.71it/s]Extractor Predicting: 116it [01:09,  1.72it/s]Extractor Predicting: 117it [01:10,  1.71it/s]Extractor Predicting: 118it [01:11,  1.73it/s]Extractor Predicting: 119it [01:11,  1.70it/s]Extractor Predicting: 120it [01:12,  1.72it/s]Extractor Predicting: 121it [01:12,  1.69it/s]Extractor Predicting: 122it [01:13,  1.65it/s]Extractor Predicting: 123it [01:14,  1.60it/s]Extractor Predicting: 124it [01:14,  1.57it/s]Extractor Predicting: 125it [01:15,  1.60it/s]Extractor Predicting: 126it [01:16,  1.63it/s]Extractor Predicting: 127it [01:16,  1.63it/s]Extractor Predicting: 128it [01:17,  1.56it/s]Extractor Predicting: 129it [01:17,  1.60it/s]Extractor Predicting: 130it [01:18,  1.63it/s]Extractor Predicting: 131it [01:19,  1.67it/s]Extractor Predicting: 132it [01:19,  1.63it/s]Extractor Predicting: 133it [01:20,  1.61it/s]Extractor Predicting: 134it [01:21,  1.63it/s]Extractor Predicting: 135it [01:21,  1.66it/s]Extractor Predicting: 136it [01:22,  1.67it/s]Extractor Predicting: 137it [01:22,  1.68it/s]Extractor Predicting: 138it [01:23,  1.66it/s]Extractor Predicting: 139it [01:24,  1.63it/s]Extractor Predicting: 140it [01:24,  1.62it/s]Extractor Predicting: 141it [01:25,  1.67it/s]Extractor Predicting: 142it [01:25,  1.64it/s]Extractor Predicting: 143it [01:26,  1.69it/s]Extractor Predicting: 144it [01:27,  1.68it/s]Extractor Predicting: 145it [01:27,  1.69it/s]Extractor Predicting: 146it [01:28,  1.63it/s]Extractor Predicting: 147it [01:28,  1.62it/s]Extractor Predicting: 148it [01:29,  1.63it/s]Extractor Predicting: 149it [01:30,  1.61it/s]Extractor Predicting: 150it [01:30,  1.63it/s]Extractor Predicting: 151it [01:31,  1.62it/s]Extractor Predicting: 152it [01:31,  1.67it/s]Extractor Predicting: 153it [01:32,  1.65it/s]Extractor Predicting: 154it [01:33,  1.63it/s]Extractor Predicting: 155it [01:33,  1.66it/s]Extractor Predicting: 156it [01:34,  1.69it/s]Extractor Predicting: 157it [01:34,  1.69it/s]Extractor Predicting: 158it [01:35,  1.71it/s]Extractor Predicting: 159it [01:36,  1.69it/s]Extractor Predicting: 160it [01:36,  1.68it/s]Extractor Predicting: 161it [01:37,  1.65it/s]Extractor Predicting: 162it [01:37,  1.64it/s]Extractor Predicting: 163it [01:38,  1.65it/s]Extractor Predicting: 164it [01:39,  1.65it/s]Extractor Predicting: 165it [01:39,  1.64it/s]Extractor Predicting: 166it [01:40,  1.62it/s]Extractor Predicting: 167it [01:41,  1.61it/s]Extractor Predicting: 168it [01:41,  1.59it/s]Extractor Predicting: 169it [01:42,  1.56it/s]Extractor Predicting: 170it [01:42,  1.58it/s]Extractor Predicting: 171it [01:43,  1.57it/s]Extractor Predicting: 172it [01:44,  1.60it/s]Extractor Predicting: 173it [01:44,  1.56it/s]Extractor Predicting: 174it [01:45,  1.54it/s]Extractor Predicting: 175it [01:46,  1.59it/s]Extractor Predicting: 176it [01:46,  1.57it/s]Extractor Predicting: 177it [01:47,  1.55it/s]Extractor Predicting: 178it [01:48,  1.51it/s]Extractor Predicting: 179it [01:48,  1.50it/s]Extractor Predicting: 180it [01:49,  1.50it/s]Extractor Predicting: 181it [01:50,  1.53it/s]Extractor Predicting: 182it [01:50,  1.53it/s]Extractor Predicting: 183it [01:51,  1.53it/s]Extractor Predicting: 184it [01:52,  1.53it/s]Extractor Predicting: 185it [01:52,  1.56it/s]Extractor Predicting: 186it [01:53,  1.56it/s]Extractor Predicting: 187it [01:53,  1.68it/s]Extractor Predicting: 187it [01:53,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:22,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:22,134 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:22,134 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:22,134 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:22,134 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:50:22,906 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:50:22,907 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:50:23,540 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:50:24,645 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:50:24,645 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,607 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,610 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,610 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,610 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:50:27,610 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:50:28,398 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:50:28,399 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:50:29,030 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:50:29,248 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:50:29,249 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6436363636363637,
  "recall": 0.03638980263157895,
  "score": 0.06888499708114419,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.67it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.61it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.62it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:15,  1.64it/s]Extractor Predicting: 27it [00:16,  1.62it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.57it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:22,  1.62it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.67it/s]Extractor Predicting: 42it [00:25,  1.65it/s]Extractor Predicting: 43it [00:26,  1.68it/s]Extractor Predicting: 44it [00:26,  1.63it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:28,  1.65it/s]Extractor Predicting: 47it [00:28,  1.61it/s]Extractor Predicting: 48it [00:29,  1.45it/s]Extractor Predicting: 49it [00:30,  1.45it/s]Extractor Predicting: 50it [00:30,  1.50it/s]Extractor Predicting: 51it [00:31,  1.52it/s]Extractor Predicting: 52it [00:32,  1.55it/s]Extractor Predicting: 53it [00:32,  1.61it/s]Extractor Predicting: 54it [00:33,  1.62it/s]Extractor Predicting: 55it [00:34,  1.64it/s]Extractor Predicting: 56it [00:34,  1.65it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:35,  1.65it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:37,  1.66it/s]Extractor Predicting: 61it [00:37,  1.63it/s]Extractor Predicting: 62it [00:38,  1.64it/s]Extractor Predicting: 63it [00:38,  1.64it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:40,  1.62it/s]Extractor Predicting: 66it [00:40,  1.65it/s]Extractor Predicting: 67it [00:41,  1.67it/s]Extractor Predicting: 68it [00:42,  1.57it/s]Extractor Predicting: 69it [00:42,  1.58it/s]Extractor Predicting: 70it [00:43,  1.55it/s]Extractor Predicting: 71it [00:43,  1.60it/s]Extractor Predicting: 72it [00:44,  1.61it/s]Extractor Predicting: 73it [00:45,  1.67it/s]Extractor Predicting: 74it [00:45,  1.67it/s]Extractor Predicting: 75it [00:46,  1.68it/s]Extractor Predicting: 76it [00:46,  1.65it/s]Extractor Predicting: 77it [00:47,  1.67it/s]Extractor Predicting: 78it [00:48,  1.72it/s]Extractor Predicting: 79it [00:48,  1.70it/s]Extractor Predicting: 80it [00:49,  1.69it/s]Extractor Predicting: 81it [00:49,  1.69it/s]Extractor Predicting: 82it [00:50,  1.69it/s]Extractor Predicting: 83it [00:50,  1.69it/s]Extractor Predicting: 84it [00:51,  1.65it/s]Extractor Predicting: 85it [00:52,  1.67it/s]Extractor Predicting: 86it [00:52,  1.70it/s]Extractor Predicting: 87it [00:53,  1.73it/s]Extractor Predicting: 88it [00:53,  1.77it/s]Extractor Predicting: 89it [00:54,  1.75it/s]Extractor Predicting: 90it [00:55,  1.69it/s]Extractor Predicting: 91it [00:55,  1.74it/s]Extractor Predicting: 92it [00:56,  1.77it/s]Extractor Predicting: 93it [00:56,  1.74it/s]Extractor Predicting: 94it [00:57,  1.75it/s]Extractor Predicting: 95it [00:57,  1.74it/s]Extractor Predicting: 96it [00:58,  1.76it/s]Extractor Predicting: 97it [00:59,  1.73it/s]Extractor Predicting: 98it [00:59,  1.72it/s]Extractor Predicting: 99it [01:00,  1.70it/s]Extractor Predicting: 100it [01:00,  1.73it/s]Extractor Predicting: 101it [01:01,  1.71it/s]Extractor Predicting: 102it [01:01,  1.71it/s]Extractor Predicting: 103it [01:02,  1.69it/s]Extractor Predicting: 104it [01:03,  1.69it/s]Extractor Predicting: 105it [01:03,  1.66it/s]Extractor Predicting: 106it [01:04,  1.64it/s]Extractor Predicting: 107it [01:05,  1.67it/s]Extractor Predicting: 108it [01:05,  1.67it/s]Extractor Predicting: 109it [01:06,  1.64it/s]Extractor Predicting: 110it [01:06,  1.64it/s]Extractor Predicting: 111it [01:07,  1.61it/s]Extractor Predicting: 112it [01:08,  1.65it/s]Extractor Predicting: 113it [01:08,  1.66it/s]Extractor Predicting: 114it [01:09,  1.67it/s]Extractor Predicting: 115it [01:09,  1.66it/s]Extractor Predicting: 116it [01:10,  1.64it/s]Extractor Predicting: 117it [01:11,  1.64it/s]Extractor Predicting: 118it [01:11,  1.65it/s]Extractor Predicting: 119it [01:12,  1.67it/s]Extractor Predicting: 120it [01:12,  1.65it/s]Extractor Predicting: 121it [01:13,  1.65it/s]Extractor Predicting: 122it [01:14,  1.67it/s]Extractor Predicting: 123it [01:14,  1.68it/s]Extractor Predicting: 124it [01:15,  1.69it/s]Extractor Predicting: 125it [01:15,  1.67it/s]Extractor Predicting: 126it [01:16,  1.66it/s]Extractor Predicting: 127it [01:17,  1.67it/s]Extractor Predicting: 128it [01:17,  1.64it/s]Extractor Predicting: 129it [01:18,  1.61it/s]Extractor Predicting: 130it [01:18,  1.66it/s]Extractor Predicting: 131it [01:19,  1.66it/s]Extractor Predicting: 132it [01:20,  1.61it/s]Extractor Predicting: 133it [01:20,  1.65it/s]Extractor Predicting: 134it [01:21,  1.63it/s]Extractor Predicting: 135it [01:21,  1.65it/s]Extractor Predicting: 136it [01:22,  1.66it/s]Extractor Predicting: 137it [01:23,  1.63it/s]Extractor Predicting: 138it [01:23,  1.62it/s]Extractor Predicting: 139it [01:24,  1.65it/s]Extractor Predicting: 140it [01:25,  1.64it/s]Extractor Predicting: 141it [01:25,  1.63it/s]Extractor Predicting: 142it [01:26,  1.63it/s]Extractor Predicting: 143it [01:26,  1.64it/s]Extractor Predicting: 144it [01:27,  1.65it/s]Extractor Predicting: 145it [01:28,  1.63it/s]Extractor Predicting: 146it [01:28,  1.62it/s]Extractor Predicting: 147it [01:29,  1.62it/s]Extractor Predicting: 148it [01:29,  1.64it/s]Extractor Predicting: 149it [01:30,  1.63it/s]Extractor Predicting: 150it [01:31,  1.60it/s]Extractor Predicting: 151it [01:31,  1.58it/s]Extractor Predicting: 152it [01:32,  1.61it/s]Extractor Predicting: 153it [01:33,  1.66it/s]Extractor Predicting: 154it [01:33,  1.67it/s]Extractor Predicting: 155it [01:34,  1.69it/s]Extractor Predicting: 156it [01:34,  1.66it/s]Extractor Predicting: 157it [01:35,  1.43it/s]Extractor Predicting: 158it [01:36,  1.49it/s]Extractor Predicting: 159it [01:37,  1.50it/s]Extractor Predicting: 160it [01:37,  1.54it/s]Extractor Predicting: 161it [01:38,  1.57it/s]Extractor Predicting: 162it [01:38,  1.55it/s]Extractor Predicting: 163it [01:39,  1.59it/s]Extractor Predicting: 164it [01:40,  1.60it/s]Extractor Predicting: 165it [01:40,  1.63it/s]Extractor Predicting: 166it [01:41,  1.64it/s]Extractor Predicting: 167it [01:41,  1.63it/s]Extractor Predicting: 168it [01:42,  1.61it/s]Extractor Predicting: 169it [01:43,  1.61it/s]Extractor Predicting: 170it [01:43,  1.65it/s]Extractor Predicting: 171it [01:44,  1.71it/s]Extractor Predicting: 172it [01:44,  1.69it/s]Extractor Predicting: 173it [01:45,  1.62it/s]Extractor Predicting: 174it [01:46,  1.61it/s]Extractor Predicting: 175it [01:46,  1.57it/s]Extractor Predicting: 176it [01:47,  1.57it/s]Extractor Predicting: 177it [01:48,  1.60it/s]Extractor Predicting: 178it [01:48,  1.56it/s]Extractor Predicting: 179it [01:49,  1.58it/s]Extractor Predicting: 180it [01:50,  1.56it/s]Extractor Predicting: 181it [01:50,  1.57it/s]Extractor Predicting: 182it [01:51,  1.57it/s]Extractor Predicting: 183it [01:51,  1.56it/s]Extractor Predicting: 184it [01:52,  1.56it/s]Extractor Predicting: 185it [01:53,  1.57it/s]Extractor Predicting: 186it [01:53,  1.59it/s]Extractor Predicting: 187it [01:54,  1.57it/s]Extractor Predicting: 188it [01:55,  1.59it/s]Extractor Predicting: 189it [01:55,  1.60it/s]Extractor Predicting: 190it [01:56,  1.59it/s]Extractor Predicting: 191it [01:56,  1.61it/s]Extractor Predicting: 192it [01:57,  1.61it/s]Extractor Predicting: 193it [01:58,  1.60it/s]Extractor Predicting: 194it [01:58,  1.62it/s]Extractor Predicting: 195it [01:59,  1.64it/s]Extractor Predicting: 196it [01:59,  1.68it/s]Extractor Predicting: 197it [02:00,  1.66it/s]Extractor Predicting: 198it [02:01,  1.67it/s]Extractor Predicting: 199it [02:01,  1.67it/s]Extractor Predicting: 200it [02:02,  1.69it/s]Extractor Predicting: 201it [02:03,  1.65it/s]Extractor Predicting: 202it [02:03,  1.65it/s]Extractor Predicting: 203it [02:04,  1.64it/s]Extractor Predicting: 204it [02:04,  1.64it/s]Extractor Predicting: 205it [02:05,  1.64it/s]Extractor Predicting: 206it [02:06,  1.65it/s]Extractor Predicting: 207it [02:06,  1.64it/s]Extractor Predicting: 208it [02:07,  1.68it/s]Extractor Predicting: 209it [02:07,  1.70it/s]Extractor Predicting: 210it [02:08,  1.67it/s]Extractor Predicting: 211it [02:09,  1.68it/s]Extractor Predicting: 212it [02:09,  1.68it/s]Extractor Predicting: 213it [02:10,  1.64it/s]Extractor Predicting: 214it [02:10,  1.62it/s]Extractor Predicting: 215it [02:11,  1.62it/s]Extractor Predicting: 216it [02:12,  1.63it/s]Extractor Predicting: 217it [02:12,  1.65it/s]Extractor Predicting: 218it [02:13,  1.63it/s]Extractor Predicting: 219it [02:13,  1.61it/s]Extractor Predicting: 220it [02:14,  1.67it/s]Extractor Predicting: 221it [02:15,  1.62it/s]Extractor Predicting: 222it [02:15,  1.59it/s]Extractor Predicting: 223it [02:16,  1.60it/s]Extractor Predicting: 224it [02:17,  1.63it/s]Extractor Predicting: 225it [02:17,  1.67it/s]Extractor Predicting: 226it [02:18,  1.67it/s]Extractor Predicting: 227it [02:18,  1.66it/s]Extractor Predicting: 228it [02:19,  1.67it/s]Extractor Predicting: 229it [02:20,  1.66it/s]Extractor Predicting: 230it [02:20,  1.70it/s]Extractor Predicting: 231it [02:21,  1.69it/s]Extractor Predicting: 232it [02:21,  1.69it/s]Extractor Predicting: 233it [02:22,  1.66it/s]Extractor Predicting: 234it [02:23,  1.63it/s]Extractor Predicting: 235it [02:23,  1.62it/s]Extractor Predicting: 236it [02:24,  1.64it/s]Extractor Predicting: 237it [02:24,  1.62it/s]Extractor Predicting: 238it [02:25,  1.62it/s]Extractor Predicting: 239it [02:26,  1.61it/s]Extractor Predicting: 240it [02:26,  1.63it/s]Extractor Predicting: 241it [02:27,  1.67it/s]Extractor Predicting: 242it [02:27,  1.65it/s]Extractor Predicting: 243it [02:28,  1.69it/s]Extractor Predicting: 244it [02:29,  1.69it/s]Extractor Predicting: 245it [02:29,  1.65it/s]Extractor Predicting: 246it [02:30,  1.68it/s]Extractor Predicting: 247it [02:30,  1.67it/s]Extractor Predicting: 248it [02:31,  1.69it/s]Extractor Predicting: 249it [02:32,  1.69it/s]Extractor Predicting: 250it [02:32,  1.74it/s]Extractor Predicting: 251it [02:33,  1.68it/s]Extractor Predicting: 252it [02:33,  1.70it/s]Extractor Predicting: 253it [02:34,  1.76it/s]Extractor Predicting: 254it [02:34,  1.70it/s]Extractor Predicting: 255it [02:35,  1.70it/s]Extractor Predicting: 256it [02:36,  1.69it/s]Extractor Predicting: 257it [02:36,  1.66it/s]Extractor Predicting: 258it [02:37,  1.68it/s]Extractor Predicting: 259it [02:37,  1.68it/s]Extractor Predicting: 260it [02:38,  1.67it/s]Extractor Predicting: 261it [02:39,  1.65it/s]Extractor Predicting: 262it [02:39,  1.62it/s]Extractor Predicting: 263it [02:40,  1.58it/s]Extractor Predicting: 264it [02:41,  1.63it/s]Extractor Predicting: 265it [02:41,  1.63it/s]Extractor Predicting: 266it [02:42,  1.61it/s]Extractor Predicting: 267it [02:42,  1.60it/s]Extractor Predicting: 268it [02:43,  1.60it/s]Extractor Predicting: 269it [02:44,  1.58it/s]Extractor Predicting: 270it [02:44,  1.60it/s]Extractor Predicting: 271it [02:45,  1.41it/s]Extractor Predicting: 272it [02:46,  1.45it/s]Extractor Predicting: 273it [02:46,  1.49it/s]Extractor Predicting: 274it [02:47,  1.51it/s]Extractor Predicting: 275it [02:48,  1.54it/s]Extractor Predicting: 276it [02:48,  1.58it/s]Extractor Predicting: 277it [02:49,  1.55it/s]Extractor Predicting: 278it [02:50,  1.57it/s]Extractor Predicting: 279it [02:50,  1.57it/s]Extractor Predicting: 280it [02:51,  1.58it/s]Extractor Predicting: 281it [02:52,  1.59it/s]Extractor Predicting: 282it [02:52,  1.59it/s]Extractor Predicting: 283it [02:53,  1.59it/s]Extractor Predicting: 284it [02:53,  1.62it/s]Extractor Predicting: 285it [02:54,  1.64it/s]Extractor Predicting: 286it [02:55,  1.64it/s]Extractor Predicting: 287it [02:55,  1.60it/s]Extractor Predicting: 288it [02:56,  1.61it/s]Extractor Predicting: 289it [02:56,  1.60it/s]Extractor Predicting: 290it [02:57,  1.58it/s]Extractor Predicting: 291it [02:58,  1.59it/s]Extractor Predicting: 292it [02:58,  1.62it/s]Extractor Predicting: 293it [02:59,  1.60it/s]Extractor Predicting: 294it [03:00,  1.57it/s]Extractor Predicting: 295it [03:00,  1.58it/s]Extractor Predicting: 296it [03:01,  1.59it/s]Extractor Predicting: 297it [03:02,  1.58it/s]Extractor Predicting: 298it [03:02,  1.59it/s]Extractor Predicting: 299it [03:03,  1.58it/s]Extractor Predicting: 300it [03:04,  1.52it/s]Extractor Predicting: 301it [03:04,  1.48it/s]Extractor Predicting: 302it [03:05,  1.47it/s]Extractor Predicting: 303it [03:06,  1.51it/s]Extractor Predicting: 304it [03:06,  1.47it/s]Extractor Predicting: 305it [03:07,  1.43it/s]Extractor Predicting: 306it [03:08,  1.50it/s]Extractor Predicting: 307it [03:08,  1.55it/s]Extractor Predicting: 308it [03:09,  1.58it/s]Extractor Predicting: 309it [03:09,  1.64it/s]Extractor Predicting: 310it [03:10,  1.61it/s]Extractor Predicting: 311it [03:11,  1.56it/s]Extractor Predicting: 312it [03:11,  1.58it/s]Extractor Predicting: 313it [03:12,  1.59it/s]Extractor Predicting: 314it [03:13,  1.58it/s]Extractor Predicting: 315it [03:13,  1.63it/s]Extractor Predicting: 316it [03:14,  1.64it/s]Extractor Predicting: 317it [03:14,  1.62it/s]Extractor Predicting: 318it [03:15,  1.62it/s]Extractor Predicting: 319it [03:16,  1.61it/s]Extractor Predicting: 320it [03:16,  1.62it/s]Extractor Predicting: 321it [03:17,  1.64it/s]Extractor Predicting: 322it [03:17,  1.67it/s]Extractor Predicting: 323it [03:18,  1.67it/s]Extractor Predicting: 324it [03:19,  1.68it/s]Extractor Predicting: 325it [03:19,  1.67it/s]Extractor Predicting: 326it [03:20,  1.66it/s]Extractor Predicting: 327it [03:20,  1.67it/s]Extractor Predicting: 328it [03:21,  1.66it/s]Extractor Predicting: 329it [03:22,  1.65it/s]Extractor Predicting: 330it [03:22,  1.64it/s]Extractor Predicting: 331it [03:23,  1.62it/s]Extractor Predicting: 332it [03:23,  1.65it/s]Extractor Predicting: 333it [03:24,  1.64it/s]Extractor Predicting: 334it [03:25,  1.64it/s]Extractor Predicting: 335it [03:25,  1.64it/s]Extractor Predicting: 336it [03:26,  1.66it/s]Extractor Predicting: 337it [03:26,  1.65it/s]Extractor Predicting: 338it [03:27,  1.66it/s]Extractor Predicting: 339it [03:28,  1.66it/s]Extractor Predicting: 340it [03:28,  1.67it/s]Extractor Predicting: 341it [03:29,  1.68it/s]Extractor Predicting: 342it [03:29,  1.65it/s]Extractor Predicting: 343it [03:30,  1.65it/s]Extractor Predicting: 344it [03:31,  1.69it/s]Extractor Predicting: 345it [03:31,  1.71it/s]Extractor Predicting: 346it [03:32,  1.74it/s]Extractor Predicting: 347it [03:32,  1.78it/s]Extractor Predicting: 348it [03:33,  1.74it/s]Extractor Predicting: 349it [03:33,  1.76it/s]Extractor Predicting: 350it [03:34,  1.73it/s]Extractor Predicting: 351it [03:35,  1.73it/s]Extractor Predicting: 352it [03:35,  1.72it/s]Extractor Predicting: 353it [03:36,  1.74it/s]Extractor Predicting: 354it [03:36,  1.74it/s]Extractor Predicting: 355it [03:37,  1.74it/s]Extractor Predicting: 356it [03:37,  1.76it/s]Extractor Predicting: 357it [03:38,  1.74it/s]Extractor Predicting: 358it [03:39,  1.73it/s]Extractor Predicting: 359it [03:39,  1.73it/s]Extractor Predicting: 360it [03:40,  1.72it/s]Extractor Predicting: 361it [03:40,  1.72it/s]Extractor Predicting: 362it [03:41,  1.72it/s]Extractor Predicting: 363it [03:42,  1.74it/s]Extractor Predicting: 364it [03:42,  1.78it/s]Extractor Predicting: 365it [03:43,  1.72it/s]Extractor Predicting: 366it [03:43,  1.68it/s]Extractor Predicting: 367it [03:44,  1.63it/s]Extractor Predicting: 368it [03:45,  1.59it/s]Extractor Predicting: 369it [03:45,  1.64it/s]Extractor Predicting: 370it [03:46,  1.61it/s]Extractor Predicting: 371it [03:46,  1.63it/s]Extractor Predicting: 372it [03:47,  1.65it/s]Extractor Predicting: 373it [03:48,  1.63it/s]Extractor Predicting: 374it [03:48,  1.65it/s]Extractor Predicting: 375it [03:49,  1.63it/s]Extractor Predicting: 376it [03:50,  1.63it/s]Extractor Predicting: 377it [03:50,  1.61it/s]Extractor Predicting: 378it [03:51,  1.61it/s]Extractor Predicting: 379it [03:51,  1.61it/s]Extractor Predicting: 380it [03:52,  1.62it/s]Extractor Predicting: 381it [03:53,  1.65it/s]Extractor Predicting: 382it [03:53,  1.68it/s]Extractor Predicting: 383it [03:54,  1.67it/s]Extractor Predicting: 384it [03:54,  1.69it/s]Extractor Predicting: 385it [03:55,  1.67it/s]Extractor Predicting: 386it [03:56,  1.69it/s]Extractor Predicting: 387it [03:56,  1.69it/s]Extractor Predicting: 388it [03:57,  1.49it/s]Extractor Predicting: 389it [03:58,  1.49it/s]Extractor Predicting: 390it [03:58,  1.51it/s]Extractor Predicting: 391it [03:59,  1.56it/s]Extractor Predicting: 392it [03:59,  1.64it/s]Extractor Predicting: 393it [04:00,  1.68it/s]Extractor Predicting: 394it [04:01,  1.69it/s]Extractor Predicting: 395it [04:01,  1.72it/s]Extractor Predicting: 396it [04:02,  1.73it/s]Extractor Predicting: 397it [04:02,  1.70it/s]Extractor Predicting: 398it [04:03,  1.69it/s]Extractor Predicting: 399it [04:03,  1.70it/s]Extractor Predicting: 400it [04:04,  1.68it/s]Extractor Predicting: 401it [04:05,  1.70it/s]Extractor Predicting: 402it [04:05,  1.70it/s]Extractor Predicting: 403it [04:06,  1.70it/s]Extractor Predicting: 404it [04:06,  1.71it/s]Extractor Predicting: 405it [04:07,  1.68it/s]Extractor Predicting: 406it [04:08,  1.69it/s]Extractor Predicting: 407it [04:08,  1.70it/s]Extractor Predicting: 408it [04:09,  1.70it/s]Extractor Predicting: 409it [04:10,  1.53it/s]Extractor Predicting: 410it [04:10,  1.57it/s]Extractor Predicting: 411it [04:11,  1.59it/s]Extractor Predicting: 412it [04:11,  1.61it/s]Extractor Predicting: 413it [04:12,  1.55it/s]Extractor Predicting: 414it [04:13,  1.53it/s]Extractor Predicting: 415it [04:13,  1.50it/s]Extractor Predicting: 416it [04:14,  1.51it/s]Extractor Predicting: 417it [04:15,  1.53it/s]Extractor Predicting: 418it [04:15,  1.55it/s]Extractor Predicting: 419it [04:16,  1.56it/s]Extractor Predicting: 420it [04:17,  1.54it/s]Extractor Predicting: 421it [04:17,  1.54it/s]Extractor Predicting: 422it [04:18,  1.56it/s]Extractor Predicting: 423it [04:19,  1.57it/s]Extractor Predicting: 424it [04:19,  1.56it/s]Extractor Predicting: 425it [04:20,  1.55it/s]Extractor Predicting: 426it [04:21,  1.54it/s]Extractor Predicting: 427it [04:21,  1.56it/s]Extractor Predicting: 428it [04:22,  1.57it/s]Extractor Predicting: 429it [04:22,  1.59it/s]Extractor Predicting: 430it [04:23,  1.60it/s]Extractor Predicting: 431it [04:24,  1.61it/s]Extractor Predicting: 432it [04:24,  1.60it/s]Extractor Predicting: 433it [04:25,  1.58it/s]Extractor Predicting: 434it [04:26,  1.58it/s]Extractor Predicting: 435it [04:26,  1.61it/s]Extractor Predicting: 436it [04:27,  1.64it/s]Extractor Predicting: 437it [04:27,  1.63it/s]Extractor Predicting: 438it [04:28,  1.62it/s]Extractor Predicting: 439it [04:29,  1.62it/s]Extractor Predicting: 440it [04:29,  1.62it/s]Extractor Predicting: 441it [04:30,  1.61it/s]Extractor Predicting: 442it [04:30,  1.61it/s]Extractor Predicting: 443it [04:31,  1.59it/s]Extractor Predicting: 444it [04:32,  1.59it/s]Extractor Predicting: 445it [04:32,  1.56it/s]Extractor Predicting: 446it [04:33,  1.57it/s]Extractor Predicting: 447it [04:34,  1.58it/s]Extractor Predicting: 448it [04:34,  1.57it/s]Extractor Predicting: 449it [04:35,  1.60it/s]Extractor Predicting: 450it [04:36,  1.62it/s]Extractor Predicting: 451it [04:36,  1.60it/s]Extractor Predicting: 452it [04:37,  1.59it/s]Extractor Predicting: 453it [04:37,  1.59it/s]Extractor Predicting: 454it [04:38,  1.60it/s]Extractor Predicting: 455it [04:39,  1.55it/s]Extractor Predicting: 456it [04:39,  1.57it/s]Extractor Predicting: 457it [04:40,  1.55it/s]Extractor Predicting: 458it [04:41,  1.58it/s]Extractor Predicting: 459it [04:41,  1.58it/s]Extractor Predicting: 460it [04:42,  1.55it/s]Extractor Predicting: 461it [04:43,  1.58it/s]Extractor Predicting: 462it [04:43,  1.59it/s]Extractor Predicting: 463it [04:44,  1.56it/s]Extractor Predicting: 464it [04:45,  1.53it/s]Extractor Predicting: 465it [04:45,  1.51it/s]Extractor Predicting: 466it [04:46,  1.54it/s]Extractor Predicting: 467it [04:46,  1.60it/s]Extractor Predicting: 468it [04:47,  1.62it/s]Extractor Predicting: 469it [04:48,  1.67it/s]Extractor Predicting: 470it [04:48,  1.60it/s]Extractor Predicting: 471it [04:49,  1.58it/s]Extractor Predicting: 472it [04:49,  1.60it/s]Extractor Predicting: 473it [04:50,  1.59it/s]Extractor Predicting: 474it [04:51,  1.58it/s]Extractor Predicting: 475it [04:51,  1.51it/s]Extractor Predicting: 476it [04:52,  1.55it/s]Extractor Predicting: 477it [04:53,  1.57it/s]Extractor Predicting: 478it [04:53,  1.58it/s]Extractor Predicting: 479it [04:54,  1.53it/s]Extractor Predicting: 480it [04:55,  1.44it/s]Extractor Predicting: 481it [04:56,  1.45it/s]Extractor Predicting: 482it [04:56,  1.47it/s]Extractor Predicting: 483it [04:57,  1.45it/s]Extractor Predicting: 484it [04:58,  1.44it/s]Extractor Predicting: 485it [04:58,  1.45it/s]Extractor Predicting: 486it [04:59,  1.49it/s]Extractor Predicting: 487it [05:00,  1.52it/s]Extractor Predicting: 488it [05:00,  1.50it/s]Extractor Predicting: 489it [05:01,  1.53it/s]Extractor Predicting: 490it [05:02,  1.49it/s]Extractor Predicting: 491it [05:02,  1.49it/s]Extractor Predicting: 492it [05:03,  1.47it/s]Extractor Predicting: 493it [05:04,  1.49it/s]Extractor Predicting: 494it [05:04,  1.50it/s]Extractor Predicting: 495it [05:05,  1.49it/s]Extractor Predicting: 496it [05:06,  1.50it/s]Extractor Predicting: 497it [05:06,  1.49it/s]Extractor Predicting: 498it [05:07,  1.50it/s]Extractor Predicting: 499it [05:08,  1.47it/s]Extractor Predicting: 500it [05:08,  1.49it/s]Extractor Predicting: 501it [05:09,  1.31it/s]Extractor Predicting: 502it [05:10,  1.33it/s]Extractor Predicting: 503it [05:11,  1.35it/s]Extractor Predicting: 504it [05:11,  1.36it/s]Extractor Predicting: 505it [05:12,  1.38it/s]Extractor Predicting: 506it [05:13,  1.38it/s]Extractor Predicting: 507it [05:13,  1.41it/s]Extractor Predicting: 508it [05:14,  1.58it/s]Extractor Predicting: 508it [05:14,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:56,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:56,800 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:56,800 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:56,800 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:55:56,800 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:55:57,113 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:55:57,114 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:55:57,400 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:55:58,467 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:55:58,491 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:56:01,211 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:56:01,214 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:56:01,214 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:56:01,214 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:56:01,214 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:56:01,550 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:56:01,577 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:56:02,266 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:56:02,439 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:56:02,440 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.20710059171597633,
  "recall": 0.02873327313028487,
  "score": 0.050464998918607167,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.59it/s]Extractor Predicting: 26it [00:16,  1.58it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:21,  1.61it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:22,  1.63it/s]Extractor Predicting: 36it [00:22,  1.65it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:24,  1.64it/s]Extractor Predicting: 39it [00:24,  1.64it/s]Extractor Predicting: 40it [00:25,  1.64it/s]Extractor Predicting: 41it [00:26,  1.64it/s]Extractor Predicting: 42it [00:26,  1.68it/s]Extractor Predicting: 43it [00:27,  1.67it/s]Extractor Predicting: 44it [00:27,  1.72it/s]Extractor Predicting: 45it [00:28,  1.68it/s]Extractor Predicting: 46it [00:28,  1.66it/s]Extractor Predicting: 47it [00:29,  1.66it/s]Extractor Predicting: 48it [00:30,  1.65it/s]Extractor Predicting: 49it [00:30,  1.64it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.60it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:33,  1.62it/s]Extractor Predicting: 54it [00:33,  1.57it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:35,  1.51it/s]Extractor Predicting: 57it [00:35,  1.54it/s]Extractor Predicting: 58it [00:36,  1.57it/s]Extractor Predicting: 59it [00:37,  1.47it/s]Extractor Predicting: 60it [00:37,  1.50it/s]Extractor Predicting: 61it [00:38,  1.52it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:41,  1.57it/s]Extractor Predicting: 66it [00:41,  1.57it/s]Extractor Predicting: 67it [00:42,  1.54it/s]Extractor Predicting: 68it [00:43,  1.53it/s]Extractor Predicting: 69it [00:43,  1.51it/s]Extractor Predicting: 70it [00:44,  1.52it/s]Extractor Predicting: 71it [00:45,  1.50it/s]Extractor Predicting: 72it [00:45,  1.51it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:47,  1.50it/s]Extractor Predicting: 75it [00:47,  1.69it/s]Extractor Predicting: 75it [00:47,  1.58it/s]
[INFO|configuration_utils.py:515] 2023-08-29 03:56:52,456 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:56:52,458 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:56:52,512 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:56:52,513 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 03:56:52,523 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:57:01,143 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 03:57:01,152 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 03:57:01,274 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:57:01,275 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:57:01,344 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:57:01,392 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:57:01,392 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:57:01,392 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:57:01,392 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:57:01,392 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:57:01,392 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3971631205673759,
  "recall": 0.014105793450881612,
  "score": 0.027243979567015325,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 03:57:01,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:02,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:02,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:03,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:03,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:04,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:05,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:05,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:06,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:06,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:07,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:07,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:08,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:08,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:09,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:09,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:10,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:11,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:11,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:12,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:11<03:29, 11.01s/it][WARNING|generation_utils.py:914] 2023-08-29 03:57:12,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:13,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:13,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:14,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:14,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:15,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:15,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:16,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:16,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:17,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:17,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:18,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:19,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:19,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:20,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:20,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:21,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:21,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:22,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:23,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:23,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:24,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:24,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:23<03:34, 11.93s/it][WARNING|generation_utils.py:914] 2023-08-29 03:57:25,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:26,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:26,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:27,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:27,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:28,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:28,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:29,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:30,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:31,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:31,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:32,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:33,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:33,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:34,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:35,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:35,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:36,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:36,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:37,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:37,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:38,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:39,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:38<03:44, 13.18s/it][WARNING|generation_utils.py:914] 2023-08-29 03:57:39,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:40,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:41,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:42,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:42,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:43,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:43,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:44,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:45,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:45,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:46,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:46,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:47,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:48,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:48,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:49,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:49,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:50,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:51,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:52,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:52,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:51<03:31, 13.21s/it][WARNING|generation_utils.py:914] 2023-08-29 03:57:53,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:53,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:54,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:54,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:55,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:55,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:56,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:57,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:57,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:58,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:58,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:59,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:57:59,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:00,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:00,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:01,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:01,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:02,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:02,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:03,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:03,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:04,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:05,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:04<03:14, 12.95s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:05,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:06,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:06,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:07,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:08,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:08,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:09,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:09,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:10,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:10,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:11,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:11,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:12,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:13,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:13,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:14,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:14,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:15,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:16,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:16,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:17,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:17,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:16<02:59, 12.85s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:18,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:18,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:19,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:19,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:20,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:21,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:21,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:22,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:22,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:23,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:23,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:24,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:24,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:25,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:26,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:26,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:27,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:27,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:28,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:28,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:29,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:28<02:40, 12.38s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:29,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:30,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:30,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:31,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:32,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:32,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:33,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:34,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:34,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:35,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:36,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:36,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:37,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:38,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:38,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:39,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:39,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:40,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:41,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:41,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:42,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:41<02:31, 12.65s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:43,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:43,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:44,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:45,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:45,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:46,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:47,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:47,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:48,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:49,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:49,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:50,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:51,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:51,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:52,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:53,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:54,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:54,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:55,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:56,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:56,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:55<02:24, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-29 03:58:57,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:57,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:58,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:58,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:58:59,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:00,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:00,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:01,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:01,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:02,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:03,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:03,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:04,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:04,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:05,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:05,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:06,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:07,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:07,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:08,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:08,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:07<02:08, 12.81s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:09,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:10,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:10,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:11,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:11,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:12,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:12,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:13,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:14,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:14,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:15,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:15,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:16,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:17,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:17,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:18,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:18,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:19,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:20,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:20,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:21,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:22,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:22,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:21<01:58, 13.19s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:23,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:23,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:24,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:25,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:25,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:26,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:26,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:27,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:27,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:28,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:29,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:29,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:30,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:30,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:31,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:32,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:32,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:33,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:33,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:34,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:33<01:41, 12.65s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:34,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:35,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:35,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:36,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:37,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:37,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:38,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:38,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:39,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:39,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:40,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:41,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:41,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:42,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:42,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:43,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:43,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:44,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:45,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:45,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:46,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:46,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:45<01:28, 12.65s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:47,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:47,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:48,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:49,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:49,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:50,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:50,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:51,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:51,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:52,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:52,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:53,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:53,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:54,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:54,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:55,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:55,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:56,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:56,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:57,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:58,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:56<01:13, 12.19s/it][WARNING|generation_utils.py:914] 2023-08-29 03:59:58,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:59,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:59:59,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:00,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:00,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:01,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:01,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:02,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:02,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:03,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:03,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:04,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:05,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:05,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:06,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:06,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:07,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:07,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:08,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:08,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:07<00:59, 11.82s/it][WARNING|generation_utils.py:914] 2023-08-29 04:00:09,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:10,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:10,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:11,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:12,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:12,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:13,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:14,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:14,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:15,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:16,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:16,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:17,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:18,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:18,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:19,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:20,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:20,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:21,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:22,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:22,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:21<00:49, 12.45s/it][WARNING|generation_utils.py:914] 2023-08-29 04:00:23,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:24,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:24,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:25,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:25,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:26,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:26,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:27,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:28,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:28,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:29,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:29,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:30,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:30,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:31,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:31,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:32,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:32,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:33,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:34,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:32<00:36, 12.05s/it][WARNING|generation_utils.py:914] 2023-08-29 04:00:34,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:35,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:35,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:36,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:36,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:37,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:37,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:38,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:38,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:39,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:39,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:40,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:41,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:41,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:42,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:42,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:43,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:43,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:44,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:44,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:45,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:44<00:23, 11.79s/it][WARNING|generation_utils.py:914] 2023-08-29 04:00:45,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:46,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:47,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:47,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:48,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:48,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:49,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:50,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:50,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:51,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:51,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:52,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:53,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:53,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:54,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:54,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:55,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:56,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:56,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:57,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:56<00:11, 11.87s/it][WARNING|generation_utils.py:914] 2023-08-29 04:00:57,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:58,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:59,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:00:59,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:00,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:00,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:01,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:02,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:02,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:03,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:04,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:04,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:05,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:05,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:06,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:06,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:07,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:08,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:08,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:09,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:09,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 04:01:10,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:09<00:00, 12.24s/it]Generating: 100%|██████████| 20/20 [04:09<00:00, 12.46s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:19,450 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:19,480 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:19,480 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:19,480 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:19,480 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:01:20,087 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:01:20,088 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:01:20,683 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:01:21,754 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:01:21,755 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:25,020 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:25,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:25,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:25,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:01:25,052 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:01:25,973 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:01:25,974 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:01:26,627 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:01:26,911 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:01:26,911 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country of citizenship . Context : Edward C. Ehrlich ( born July 8 , 1954 ) is a Republican Party politician in Kansas from Anniston , Arkansas . Head Entity : Edward C. Ehrlich , Tail Entity : American .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9703125, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.842391304347826, 'errors': {''}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Vespasian', 'said to be the same as', '', 'The same deity was worshipped by the Romans ( e. g. the god Vespasian , Vian and the god Apollo , who was worshipped as a god ) .')", "('', 'said to be the same as', 'Russian language', 'He is one of three groups of people who speak the Russian language fluently ( Рувстудев Рупоцет ) , with one being the first to speak the Russian language fluently .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : student .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : winner .', 'success_rate': 0.8315217391304348, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : conflict .', 'success_rate': 0.875, 'errors': {''}}
['Relation : continent . Context : The Cretaceous ( κ Cr ) , a marine age of cephalopodian fossils , is a marine age marine in the Western Atlantic , extending for approximately 25 million years . Head Entity : Western Atlantic , Tail Entity : Atlantic Ocean .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : continent .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.9166666666666666, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : founded by .', 'success_rate': 0.9032738095238095, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : given name . Context : Later in life he studied at the Conservatory of Fine Arts at Loyola Marymount in Los Angeles , where he also made his first appearance as a guest singer for the California Bad Bloods , The Rolling Stones and Motown . Head Entity : The Rolling Stones , Tail Entity : the Los Angeles .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8355978260869565, 'errors': {'', "('Soviet Union', 'given name', '', 'In 1992 ( in honor of the 60th anniversary of the establishment of the Union of Soviet Socialist Republics ) he competed in the womens javelin throw event for the Soviet Union .')", "('contestant', 'given name', '', 'He became a contestant on the reality television show The Celebrity Apprentice , appearing as two different contestants who were contestants in the same show .')", "('Florida House', 'given name', '', 'In the 1950s , he served as a Democratic member of the Florida House representing the 11th District , which is divided between the two districts .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : movement .', 'success_rate': 0.8579545454545454, 'errors': {'', "('Conservative MP for the City of London', 'movement', '', 'Born in 1885 , he was educated at the University of the South , Durham , and was the Conservative MP for the City of London .')", "('International Organization of the Tattoo Decorators', 'movement', '', 'The International Organization of the Tattoo Decorators is an international convention and trade association based in Tsinghua , China .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9765625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9330357142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.94375, 'errors': {''}}
['Relation : replaces . Context : Later in the year ( 1143 ) , he founded the Church of Jerusalem to serve as the official interpreter of the Old Testament . Head Entity : King , Tail Entity : Church of Jerusalem .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8551136363636364, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/3_ext.jsonl'}}
estimate vocab size: 12324
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12424, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.57it/s]Extractor Estimating: 2it [00:01,  1.52it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.61it/s]Extractor Estimating: 5it [00:03,  1.65it/s]Extractor Estimating: 6it [00:03,  1.54it/s]Extractor Estimating: 7it [00:04,  1.58it/s]Extractor Estimating: 8it [00:05,  1.60it/s]Extractor Estimating: 9it [00:05,  1.53it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:06,  1.63it/s]Extractor Estimating: 12it [00:07,  1.65it/s]Extractor Estimating: 13it [00:08,  1.63it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.66it/s]Extractor Estimating: 16it [00:09,  1.69it/s]Extractor Estimating: 17it [00:10,  1.69it/s]Extractor Estimating: 18it [00:11,  1.71it/s]Extractor Estimating: 19it [00:11,  1.70it/s]Extractor Estimating: 20it [00:12,  1.67it/s]Extractor Estimating: 21it [00:12,  1.65it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:14,  1.66it/s]Extractor Estimating: 25it [00:15,  1.66it/s]Extractor Estimating: 26it [00:15,  1.72it/s]Extractor Estimating: 27it [00:16,  1.72it/s]Extractor Estimating: 28it [00:16,  1.75it/s]Extractor Estimating: 29it [00:17,  1.75it/s]Extractor Estimating: 30it [00:18,  1.72it/s]Extractor Estimating: 31it [00:18,  1.75it/s]Extractor Estimating: 32it [00:19,  1.78it/s]Extractor Estimating: 33it [00:19,  1.80it/s]Extractor Estimating: 34it [00:20,  1.79it/s]Extractor Estimating: 35it [00:20,  1.79it/s]Extractor Estimating: 36it [00:21,  1.81it/s]Extractor Estimating: 37it [00:22,  1.79it/s]Extractor Estimating: 38it [00:22,  1.77it/s]Extractor Estimating: 39it [00:23,  1.80it/s]Extractor Estimating: 40it [00:23,  1.79it/s]Extractor Estimating: 41it [00:24,  1.82it/s]Extractor Estimating: 42it [00:24,  1.83it/s]Extractor Estimating: 43it [00:25,  1.78it/s]Extractor Estimating: 44it [00:25,  1.77it/s]Extractor Estimating: 45it [00:26,  1.81it/s]Extractor Estimating: 46it [00:27,  1.78it/s]Extractor Estimating: 47it [00:27,  1.76it/s]Extractor Estimating: 48it [00:28,  1.71it/s]Extractor Estimating: 49it [00:28,  1.73it/s]Extractor Estimating: 50it [00:29,  1.75it/s]Extractor Estimating: 51it [00:29,  1.76it/s]Extractor Estimating: 52it [00:30,  1.73it/s]Extractor Estimating: 53it [00:31,  1.75it/s]Extractor Estimating: 54it [00:31,  1.81it/s]Extractor Estimating: 55it [00:32,  1.79it/s]Extractor Estimating: 56it [00:32,  1.79it/s]Extractor Estimating: 57it [00:33,  1.75it/s]Extractor Estimating: 58it [00:33,  1.76it/s]Extractor Estimating: 59it [00:34,  1.78it/s]Extractor Estimating: 60it [00:34,  1.78it/s]Extractor Estimating: 61it [00:35,  1.78it/s]Extractor Estimating: 62it [00:36,  1.77it/s]Extractor Estimating: 63it [00:36,  1.71it/s]Extractor Estimating: 64it [00:37,  1.71it/s]Extractor Estimating: 65it [00:37,  1.73it/s]Extractor Estimating: 66it [00:38,  1.81it/s]Extractor Estimating: 67it [00:39,  1.67it/s]Extractor Estimating: 68it [00:39,  1.64it/s]Extractor Estimating: 69it [00:40,  1.67it/s]Extractor Estimating: 70it [00:40,  1.70it/s]Extractor Estimating: 71it [00:41,  1.78it/s]Extractor Estimating: 72it [00:41,  1.81it/s]Extractor Estimating: 73it [00:42,  1.77it/s]Extractor Estimating: 74it [00:43,  1.70it/s]Extractor Estimating: 75it [00:43,  1.74it/s]Extractor Estimating: 76it [00:44,  1.66it/s]Extractor Estimating: 77it [00:44,  1.64it/s]Extractor Estimating: 78it [00:45,  1.66it/s]Extractor Estimating: 79it [00:46,  1.61it/s]Extractor Estimating: 80it [00:46,  1.61it/s]Extractor Estimating: 81it [00:47,  1.63it/s]Extractor Estimating: 82it [00:48,  1.66it/s]Extractor Estimating: 83it [00:48,  1.68it/s]Extractor Estimating: 84it [00:49,  1.64it/s]Extractor Estimating: 85it [00:49,  1.63it/s]Extractor Estimating: 86it [00:50,  1.63it/s]Extractor Estimating: 87it [00:51,  1.65it/s]Extractor Estimating: 88it [00:51,  1.69it/s]Extractor Estimating: 89it [00:52,  1.67it/s]Extractor Estimating: 90it [00:52,  1.67it/s]Extractor Estimating: 91it [00:53,  1.51it/s]Extractor Estimating: 92it [00:54,  1.58it/s]Extractor Estimating: 93it [00:54,  1.63it/s]Extractor Estimating: 94it [00:55,  1.60it/s]Extractor Estimating: 95it [00:56,  1.61it/s]Extractor Estimating: 96it [00:56,  1.61it/s]Extractor Estimating: 97it [00:57,  1.62it/s]Extractor Estimating: 98it [00:57,  1.63it/s]Extractor Estimating: 99it [00:58,  1.53it/s]Extractor Estimating: 100it [00:59,  1.58it/s]Extractor Estimating: 101it [00:59,  1.61it/s]Extractor Estimating: 102it [01:00,  1.68it/s]Extractor Estimating: 103it [01:00,  1.67it/s]Extractor Estimating: 104it [01:01,  1.66it/s]Extractor Estimating: 105it [01:02,  1.68it/s]Extractor Estimating: 106it [01:02,  1.73it/s]Extractor Estimating: 107it [01:03,  1.74it/s]Extractor Estimating: 108it [01:03,  1.66it/s]Extractor Estimating: 109it [01:04,  1.68it/s]Extractor Estimating: 110it [01:05,  1.75it/s]Extractor Estimating: 111it [01:05,  1.72it/s]Extractor Estimating: 112it [01:06,  1.74it/s]Extractor Estimating: 113it [01:06,  1.75it/s]Extractor Estimating: 114it [01:07,  1.72it/s]Extractor Estimating: 115it [01:07,  1.76it/s]Extractor Estimating: 116it [01:08,  1.80it/s]Extractor Estimating: 117it [01:08,  1.77it/s]Extractor Estimating: 118it [01:09,  1.83it/s]Extractor Estimating: 119it [01:10,  1.87it/s]Extractor Estimating: 120it [01:10,  1.83it/s]Extractor Estimating: 121it [01:11,  1.75it/s]Extractor Estimating: 122it [01:11,  1.77it/s]Extractor Estimating: 123it [01:12,  1.73it/s]Extractor Estimating: 124it [01:12,  1.75it/s]Extractor Estimating: 125it [01:13,  1.73it/s]Extractor Estimating: 126it [01:14,  1.76it/s]Extractor Estimating: 127it [01:14,  1.77it/s]Extractor Estimating: 128it [01:15,  1.77it/s]Extractor Estimating: 129it [01:15,  1.74it/s]Extractor Estimating: 130it [01:16,  1.74it/s]Extractor Estimating: 131it [01:16,  1.73it/s]Extractor Estimating: 132it [01:17,  1.75it/s]Extractor Estimating: 133it [01:18,  1.75it/s]Extractor Estimating: 134it [01:18,  1.77it/s]Extractor Estimating: 135it [01:19,  1.78it/s]Extractor Estimating: 136it [01:19,  1.79it/s]Extractor Estimating: 137it [01:20,  1.76it/s]Extractor Estimating: 138it [01:20,  1.72it/s]Extractor Estimating: 139it [01:21,  1.73it/s]Extractor Estimating: 140it [01:22,  1.75it/s]Extractor Estimating: 141it [01:22,  1.76it/s]Extractor Estimating: 142it [01:23,  1.75it/s]Extractor Estimating: 143it [01:23,  1.67it/s]Extractor Estimating: 144it [01:24,  1.69it/s]Extractor Estimating: 145it [01:24,  1.74it/s]Extractor Estimating: 146it [01:25,  1.74it/s]Extractor Estimating: 147it [01:26,  1.81it/s]Extractor Estimating: 148it [01:26,  1.79it/s]Extractor Estimating: 149it [01:27,  1.78it/s]Extractor Estimating: 150it [01:27,  1.71it/s]Extractor Estimating: 151it [01:28,  1.77it/s]Extractor Estimating: 152it [01:28,  1.85it/s]Extractor Estimating: 153it [01:29,  2.00it/s]Extractor Estimating: 154it [01:29,  2.03it/s]Extractor Estimating: 155it [01:30,  2.06it/s]Extractor Estimating: 156it [01:30,  2.01it/s]Extractor Estimating: 157it [01:31,  2.00it/s]Extractor Estimating: 158it [01:31,  1.99it/s]Extractor Estimating: 159it [01:32,  1.99it/s]Extractor Estimating: 160it [01:32,  2.02it/s]Extractor Estimating: 161it [01:33,  1.99it/s]Extractor Estimating: 162it [01:33,  2.04it/s]Extractor Estimating: 163it [01:34,  2.02it/s]Extractor Estimating: 164it [01:34,  2.10it/s]Extractor Estimating: 165it [01:35,  2.06it/s]Extractor Estimating: 166it [01:35,  2.12it/s]Extractor Estimating: 167it [01:36,  2.05it/s]Extractor Estimating: 168it [01:36,  2.00it/s]Extractor Estimating: 169it [01:37,  1.91it/s]Extractor Estimating: 170it [01:37,  1.95it/s]Extractor Estimating: 171it [01:38,  1.99it/s]Extractor Estimating: 172it [01:38,  1.97it/s]Extractor Estimating: 173it [01:39,  2.00it/s]Extractor Estimating: 174it [01:39,  2.05it/s]Extractor Estimating: 175it [01:40,  2.01it/s]Extractor Estimating: 176it [01:40,  1.83it/s]Extractor Estimating: 177it [01:41,  1.75it/s]Extractor Estimating: 178it [01:42,  1.72it/s]Extractor Estimating: 179it [01:42,  1.72it/s]Extractor Estimating: 180it [01:43,  1.64it/s]Extractor Estimating: 181it [01:43,  1.63it/s]Extractor Estimating: 182it [01:44,  1.60it/s]Extractor Estimating: 183it [01:45,  1.60it/s]Extractor Estimating: 184it [01:45,  1.64it/s]Extractor Estimating: 185it [01:46,  1.65it/s]Extractor Estimating: 186it [01:46,  1.66it/s]Extractor Estimating: 187it [01:47,  1.62it/s]Extractor Estimating: 188it [01:48,  1.57it/s]Extractor Estimating: 189it [01:48,  1.57it/s]Extractor Estimating: 190it [01:49,  1.38it/s]Extractor Estimating: 191it [01:50,  1.47it/s]Extractor Estimating: 192it [01:51,  1.47it/s]Extractor Estimating: 193it [01:51,  1.46it/s]Extractor Estimating: 194it [01:52,  1.50it/s]Extractor Estimating: 195it [01:53,  1.51it/s]Extractor Estimating: 196it [01:53,  1.51it/s]Extractor Estimating: 197it [01:54,  1.53it/s]Extractor Estimating: 198it [01:54,  1.57it/s]Extractor Estimating: 199it [01:55,  1.59it/s]Extractor Estimating: 200it [01:56,  1.61it/s]Extractor Estimating: 201it [01:56,  1.67it/s]Extractor Estimating: 202it [01:57,  1.69it/s]Extractor Estimating: 203it [01:57,  1.67it/s]Extractor Estimating: 204it [01:58,  1.69it/s]Extractor Estimating: 205it [01:59,  1.68it/s]Extractor Estimating: 206it [01:59,  1.67it/s]Extractor Estimating: 207it [02:00,  1.70it/s]Extractor Estimating: 208it [02:00,  1.68it/s]Extractor Estimating: 209it [02:01,  1.67it/s]Extractor Estimating: 210it [02:02,  1.68it/s]Extractor Estimating: 211it [02:02,  1.72it/s]Extractor Estimating: 212it [02:03,  1.65it/s]Extractor Estimating: 213it [02:03,  1.69it/s]Extractor Estimating: 214it [02:04,  1.69it/s]Extractor Estimating: 215it [02:05,  1.54it/s]Extractor Estimating: 216it [02:05,  1.57it/s]Extractor Estimating: 217it [02:06,  1.54it/s]Extractor Estimating: 218it [02:07,  1.60it/s]Extractor Estimating: 219it [02:07,  1.64it/s]Extractor Estimating: 220it [02:08,  1.66it/s]Extractor Estimating: 221it [02:08,  1.72it/s]Extractor Estimating: 222it [02:09,  1.70it/s]Extractor Estimating: 223it [02:09,  1.67it/s]Extractor Estimating: 224it [02:10,  1.64it/s]Extractor Estimating: 225it [02:11,  1.69it/s]Extractor Estimating: 226it [02:11,  1.67it/s]Extractor Estimating: 227it [02:12,  1.66it/s]Extractor Estimating: 228it [02:12,  1.70it/s]Extractor Estimating: 229it [02:13,  1.74it/s]Extractor Estimating: 230it [02:14,  1.74it/s]Extractor Estimating: 231it [02:14,  1.73it/s]Extractor Estimating: 232it [02:15,  1.77it/s]Extractor Estimating: 233it [02:15,  1.79it/s]Extractor Estimating: 234it [02:16,  1.80it/s]Extractor Estimating: 235it [02:16,  1.78it/s]Extractor Estimating: 236it [02:17,  1.77it/s]Extractor Estimating: 237it [02:18,  1.77it/s]Extractor Estimating: 238it [02:18,  1.68it/s]Extractor Estimating: 239it [02:19,  1.72it/s]Extractor Estimating: 240it [02:19,  1.73it/s]Extractor Estimating: 241it [02:20,  1.74it/s]Extractor Estimating: 242it [02:20,  1.77it/s]Extractor Estimating: 243it [02:21,  1.76it/s]Extractor Estimating: 244it [02:22,  1.73it/s]Extractor Estimating: 245it [02:22,  1.74it/s]Extractor Estimating: 246it [02:23,  1.77it/s]Extractor Estimating: 247it [02:23,  1.72it/s]Extractor Estimating: 248it [02:24,  1.69it/s]Extractor Estimating: 249it [02:24,  1.73it/s]Extractor Estimating: 250it [02:25,  1.67it/s]Extractor Estimating: 251it [02:26,  1.66it/s]Extractor Estimating: 252it [02:26,  1.65it/s]Extractor Estimating: 253it [02:27,  1.63it/s]Extractor Estimating: 254it [02:28,  1.66it/s]Extractor Estimating: 255it [02:28,  1.62it/s]Extractor Estimating: 256it [02:29,  1.61it/s]Extractor Estimating: 257it [02:29,  1.65it/s]Extractor Estimating: 258it [02:30,  1.63it/s]Extractor Estimating: 259it [02:31,  1.53it/s]Extractor Estimating: 260it [02:31,  1.57it/s]Extractor Estimating: 261it [02:32,  1.44it/s]Extractor Estimating: 262it [02:33,  1.50it/s]Extractor Estimating: 263it [02:33,  1.53it/s]Extractor Estimating: 264it [02:34,  1.59it/s]Extractor Estimating: 265it [02:35,  1.57it/s]Extractor Estimating: 266it [02:35,  1.56it/s]Extractor Estimating: 267it [02:36,  1.60it/s]Extractor Estimating: 268it [02:37,  1.59it/s]Extractor Estimating: 269it [02:37,  1.59it/s]Extractor Estimating: 270it [02:38,  1.58it/s]Extractor Estimating: 271it [02:38,  1.55it/s]Extractor Estimating: 272it [02:39,  1.55it/s]Extractor Estimating: 273it [02:40,  1.49it/s]Extractor Estimating: 274it [02:40,  1.51it/s]Extractor Estimating: 275it [02:41,  1.53it/s]Extractor Estimating: 276it [02:42,  1.58it/s]Extractor Estimating: 277it [02:42,  1.58it/s]Extractor Estimating: 278it [02:43,  1.56it/s]Extractor Estimating: 279it [02:44,  1.61it/s]Extractor Estimating: 280it [02:44,  1.64it/s]Extractor Estimating: 281it [02:45,  1.68it/s]Extractor Estimating: 282it [02:45,  1.67it/s]Extractor Estimating: 283it [02:46,  1.65it/s]Extractor Estimating: 284it [02:47,  1.60it/s]Extractor Estimating: 285it [02:47,  1.55it/s]Extractor Estimating: 286it [02:48,  1.58it/s]Extractor Estimating: 287it [02:49,  1.56it/s]Extractor Estimating: 288it [02:49,  1.55it/s]Extractor Estimating: 289it [02:50,  1.56it/s]Extractor Estimating: 290it [02:50,  1.58it/s]Extractor Estimating: 291it [02:51,  1.56it/s]Extractor Estimating: 292it [02:52,  1.54it/s]Extractor Estimating: 293it [02:52,  1.55it/s]Extractor Estimating: 294it [02:53,  1.53it/s]Extractor Estimating: 295it [02:54,  1.53it/s]Extractor Estimating: 296it [02:54,  1.54it/s]Extractor Estimating: 297it [02:55,  1.55it/s]Extractor Estimating: 298it [02:56,  1.57it/s]Extractor Estimating: 299it [02:56,  1.59it/s]Extractor Estimating: 300it [02:57,  1.61it/s]Extractor Estimating: 301it [02:57,  1.67it/s]Extractor Estimating: 302it [02:58,  1.69it/s]Extractor Estimating: 303it [02:59,  1.75it/s]Extractor Estimating: 304it [02:59,  1.78it/s]Extractor Estimating: 305it [03:00,  1.72it/s]Extractor Estimating: 306it [03:00,  1.74it/s]Extractor Estimating: 307it [03:01,  1.82it/s]Extractor Estimating: 308it [03:01,  1.71it/s]Extractor Estimating: 309it [03:02,  1.70it/s]Extractor Estimating: 310it [03:03,  1.69it/s]Extractor Estimating: 311it [03:03,  1.69it/s]Extractor Estimating: 312it [03:04,  1.70it/s]Extractor Estimating: 313it [03:04,  1.66it/s]Extractor Estimating: 314it [03:05,  1.65it/s]Extractor Estimating: 315it [03:06,  1.68it/s]Extractor Estimating: 316it [03:06,  1.55it/s]Extractor Estimating: 317it [03:07,  1.65it/s]Extractor Estimating: 318it [03:07,  1.69it/s]Extractor Estimating: 319it [03:08,  1.73it/s]Extractor Estimating: 320it [03:09,  1.73it/s]Extractor Estimating: 321it [03:09,  1.72it/s]Extractor Estimating: 322it [03:10,  1.65it/s]Extractor Estimating: 323it [03:10,  1.66it/s]Extractor Estimating: 324it [03:11,  1.69it/s]Extractor Estimating: 325it [03:12,  1.67it/s]Extractor Estimating: 326it [03:12,  1.72it/s]Extractor Estimating: 327it [03:13,  1.76it/s]Extractor Estimating: 328it [03:13,  1.80it/s]Extractor Estimating: 329it [03:14,  1.79it/s]Extractor Estimating: 330it [03:14,  1.80it/s]Extractor Estimating: 331it [03:15,  1.82it/s]Extractor Estimating: 332it [03:16,  1.63it/s]Extractor Estimating: 333it [03:16,  1.71it/s]Extractor Estimating: 334it [03:17,  1.72it/s]Extractor Estimating: 335it [03:17,  1.77it/s]Extractor Estimating: 336it [03:18,  1.83it/s]Extractor Estimating: 337it [03:18,  1.87it/s]Extractor Estimating: 338it [03:19,  1.87it/s]Extractor Estimating: 339it [03:19,  1.83it/s]Extractor Estimating: 340it [03:20,  1.79it/s]Extractor Estimating: 341it [03:20,  1.78it/s]Extractor Estimating: 342it [03:21,  1.72it/s]Extractor Estimating: 343it [03:22,  1.74it/s]Extractor Estimating: 344it [03:22,  1.81it/s]Extractor Estimating: 345it [03:23,  1.81it/s]Extractor Estimating: 346it [03:23,  1.84it/s]Extractor Estimating: 347it [03:24,  1.79it/s]Extractor Estimating: 348it [03:24,  1.80it/s]Extractor Estimating: 349it [03:25,  1.83it/s]Extractor Estimating: 350it [03:25,  1.84it/s]Extractor Estimating: 351it [03:26,  1.85it/s]Extractor Estimating: 352it [03:27,  1.83it/s]Extractor Estimating: 353it [03:27,  1.71it/s]Extractor Estimating: 354it [03:28,  1.74it/s]Extractor Estimating: 355it [03:28,  1.72it/s]Extractor Estimating: 356it [03:29,  1.73it/s]Extractor Estimating: 357it [03:30,  1.70it/s]Extractor Estimating: 358it [03:30,  1.70it/s]Extractor Estimating: 359it [03:31,  1.72it/s]Extractor Estimating: 360it [03:31,  1.73it/s]Extractor Estimating: 361it [03:32,  1.75it/s]Extractor Estimating: 362it [03:32,  1.75it/s]Extractor Estimating: 363it [03:33,  1.75it/s]Extractor Estimating: 364it [03:34,  1.74it/s]Extractor Estimating: 365it [03:34,  1.72it/s]Extractor Estimating: 366it [03:35,  1.73it/s]Extractor Estimating: 367it [03:35,  1.77it/s]Extractor Estimating: 368it [03:36,  1.77it/s]Extractor Estimating: 369it [03:36,  1.72it/s]Extractor Estimating: 370it [03:37,  1.72it/s]Extractor Estimating: 371it [03:38,  1.72it/s]Extractor Estimating: 372it [03:38,  1.72it/s]Extractor Estimating: 373it [03:39,  1.71it/s]Extractor Estimating: 374it [03:39,  1.70it/s]Extractor Estimating: 375it [03:40,  1.65it/s]Extractor Estimating: 376it [03:41,  1.66it/s]Extractor Estimating: 377it [03:41,  1.65it/s]Extractor Estimating: 378it [03:42,  1.66it/s]Extractor Estimating: 379it [03:42,  1.66it/s]Extractor Estimating: 380it [03:43,  1.63it/s]Extractor Estimating: 381it [03:44,  1.67it/s]Extractor Estimating: 382it [03:44,  1.66it/s]Extractor Estimating: 383it [03:45,  1.65it/s]Extractor Estimating: 384it [03:46,  1.62it/s]Extractor Estimating: 385it [03:46,  1.59it/s]Extractor Estimating: 386it [03:47,  1.54it/s]Extractor Estimating: 387it [03:47,  1.57it/s]Extractor Estimating: 388it [03:48,  1.54it/s]Extractor Estimating: 389it [03:49,  1.56it/s]Extractor Estimating: 390it [03:49,  1.59it/s]Extractor Estimating: 391it [03:50,  1.63it/s]Extractor Estimating: 392it [03:51,  1.63it/s]Extractor Estimating: 393it [03:51,  1.68it/s]Extractor Estimating: 394it [03:52,  1.72it/s]Extractor Estimating: 395it [03:52,  1.70it/s]Extractor Estimating: 396it [03:53,  1.69it/s]Extractor Estimating: 397it [03:54,  1.62it/s]Extractor Estimating: 398it [03:54,  1.61it/s]Extractor Estimating: 399it [03:55,  1.64it/s]Extractor Estimating: 400it [03:55,  1.68it/s]Extractor Estimating: 401it [03:56,  1.67it/s]Extractor Estimating: 402it [03:57,  1.70it/s]Extractor Estimating: 403it [03:57,  1.67it/s]Extractor Estimating: 404it [03:58,  1.70it/s]Extractor Estimating: 405it [03:58,  1.72it/s]Extractor Estimating: 406it [03:59,  1.73it/s]Extractor Estimating: 407it [03:59,  1.66it/s]Extractor Estimating: 408it [04:00,  1.68it/s]Extractor Estimating: 409it [04:01,  1.71it/s]Extractor Estimating: 410it [04:01,  1.78it/s]Extractor Estimating: 411it [04:02,  1.81it/s]Extractor Estimating: 412it [04:02,  1.80it/s]Extractor Estimating: 413it [04:03,  1.72it/s]Extractor Estimating: 414it [04:03,  1.75it/s]Extractor Estimating: 415it [04:04,  1.73it/s]Extractor Estimating: 416it [04:05,  1.71it/s]Extractor Estimating: 417it [04:05,  1.73it/s]Extractor Estimating: 418it [04:06,  1.77it/s]Extractor Estimating: 419it [04:06,  1.74it/s]Extractor Estimating: 420it [04:07,  1.78it/s]Extractor Estimating: 421it [04:07,  1.78it/s]Extractor Estimating: 422it [04:08,  1.76it/s]Extractor Estimating: 423it [04:09,  1.73it/s]Extractor Estimating: 424it [04:09,  1.78it/s]Extractor Estimating: 425it [04:10,  1.76it/s]Extractor Estimating: 426it [04:10,  1.77it/s]Extractor Estimating: 427it [04:11,  1.75it/s]Extractor Estimating: 428it [04:11,  1.76it/s]Extractor Estimating: 429it [04:12,  1.73it/s]Extractor Estimating: 430it [04:13,  1.72it/s]Extractor Estimating: 431it [04:13,  1.74it/s]Extractor Estimating: 432it [04:14,  1.72it/s]Extractor Estimating: 433it [04:14,  1.70it/s]Extractor Estimating: 434it [04:15,  1.72it/s]Extractor Estimating: 435it [04:15,  1.75it/s]Extractor Estimating: 436it [04:16,  1.78it/s]Extractor Estimating: 437it [04:17,  1.79it/s]Extractor Estimating: 438it [04:17,  1.74it/s]Extractor Estimating: 439it [04:18,  1.72it/s]Extractor Estimating: 440it [04:18,  1.72it/s]Extractor Estimating: 441it [04:19,  1.75it/s]Extractor Estimating: 442it [04:19,  1.78it/s]Extractor Estimating: 443it [04:20,  1.73it/s]Extractor Estimating: 444it [04:21,  1.59it/s]Extractor Estimating: 445it [04:21,  1.62it/s]Extractor Estimating: 446it [04:22,  1.65it/s]Extractor Estimating: 447it [04:23,  1.66it/s]Extractor Estimating: 448it [04:23,  1.63it/s]Extractor Estimating: 449it [04:24,  1.66it/s]Extractor Estimating: 450it [04:24,  1.69it/s]Extractor Estimating: 451it [04:25,  1.54it/s]Extractor Estimating: 452it [04:26,  1.54it/s]Extractor Estimating: 453it [04:26,  1.49it/s]Extractor Estimating: 454it [04:27,  1.46it/s]Extractor Estimating: 455it [04:28,  1.47it/s]Extractor Estimating: 456it [04:29,  1.45it/s]Extractor Estimating: 457it [04:29,  1.43it/s]Extractor Estimating: 458it [04:30,  1.49it/s]Extractor Estimating: 459it [04:31,  1.46it/s]Extractor Estimating: 460it [04:31,  1.55it/s]Extractor Estimating: 461it [04:32,  1.53it/s]Extractor Estimating: 462it [04:33,  1.54it/s]Extractor Estimating: 463it [04:33,  1.54it/s]Extractor Estimating: 464it [04:34,  1.57it/s]Extractor Estimating: 465it [04:34,  1.52it/s]Extractor Estimating: 466it [04:35,  1.54it/s]Extractor Estimating: 467it [04:36,  1.55it/s]Extractor Estimating: 468it [04:36,  1.52it/s]Extractor Estimating: 469it [04:37,  1.57it/s]Extractor Estimating: 470it [04:38,  1.55it/s]Extractor Estimating: 471it [04:38,  1.52it/s]Extractor Estimating: 472it [04:39,  1.51it/s]Extractor Estimating: 473it [04:40,  1.55it/s]Extractor Estimating: 474it [04:40,  1.58it/s]Extractor Estimating: 475it [04:41,  1.55it/s]Extractor Estimating: 476it [04:42,  1.54it/s]Extractor Estimating: 477it [04:42,  1.58it/s]Extractor Estimating: 478it [04:43,  1.62it/s]Extractor Estimating: 479it [04:43,  1.63it/s]Extractor Estimating: 480it [04:44,  1.61it/s]Extractor Estimating: 481it [04:45,  1.64it/s]Extractor Estimating: 482it [04:45,  1.67it/s]Extractor Estimating: 483it [04:46,  1.68it/s]Extractor Estimating: 484it [04:46,  1.70it/s]Extractor Estimating: 485it [04:47,  1.68it/s]Extractor Estimating: 486it [04:48,  1.63it/s]Extractor Estimating: 487it [04:48,  1.64it/s]Extractor Estimating: 488it [04:49,  1.67it/s]Extractor Estimating: 489it [04:49,  1.71it/s]Extractor Estimating: 490it [04:50,  1.64it/s]Extractor Estimating: 491it [04:51,  1.69it/s]Extractor Estimating: 492it [04:51,  1.69it/s]Extractor Estimating: 493it [04:52,  1.66it/s]Extractor Estimating: 494it [04:52,  1.70it/s]Extractor Estimating: 495it [04:53,  1.74it/s]Extractor Estimating: 496it [04:53,  1.76it/s]Extractor Estimating: 497it [04:54,  1.74it/s]Extractor Estimating: 498it [04:55,  1.74it/s]Extractor Estimating: 499it [04:55,  1.65it/s]Extractor Estimating: 500it [04:56,  1.72it/s]Extractor Estimating: 500it [04:56,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:46,271 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:46,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:46,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:46,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:46,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:06:47,039 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:06:47,040 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:06:47,628 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:06:48,713 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:06:48,713 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:51,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:51,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:51,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:51,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:06:51,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:06:52,448 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:06:52,449 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:06:53,142 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:06:53,312 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:06:53,312 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 07:03:18,805 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 07:03:18,985 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9983 mean pseudo reward: 0.9471722514828125
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 23537
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23637, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23637, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.994, loss:664.3523
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.978, loss:675.1860
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.970, loss:655.6620
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.962, loss:655.9821
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.974, loss:603.1170
>> valid entity prec:0.4777, rec:0.4742, f1:0.4760
>> valid relation prec:0.1060, rec:0.0272, f1:0.0433
>> valid relation with NER prec:0.1060, rec:0.0272, f1:0.0433
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.567, loss:643.1306
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.983, loss:648.4713
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 0.982, loss:668.9033
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.979, loss:615.9375
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.994, loss:638.7835
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4873, rec:0.3624, f1:0.4157
>> valid relation prec:0.0693, rec:0.0130, f1:0.0219
>> valid relation with NER prec:0.0693, rec:0.0130, f1:0.0219
g_step 1100, step 268, avg_time 2.508, loss:668.3560
g_step 1200, step 368, avg_time 0.962, loss:643.7895
g_step 1300, step 52, avg_time 0.961, loss:617.7739
g_step 1400, step 152, avg_time 0.968, loss:619.4294
g_step 1500, step 252, avg_time 0.972, loss:615.7946
>> valid entity prec:0.5081, rec:0.3277, f1:0.3984
>> valid relation prec:0.1187, rec:0.0169, f1:0.0296
>> valid relation with NER prec:0.1187, rec:0.0169, f1:0.0296
g_step 1600, step 352, avg_time 2.504, loss:617.5156
g_step 1700, step 36, avg_time 0.981, loss:619.1998
g_step 1800, step 136, avg_time 0.974, loss:583.9420
g_step 1900, step 236, avg_time 0.975, loss:599.9920
g_step 2000, step 336, avg_time 0.956, loss:599.1481
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4411, rec:0.3879, f1:0.4128
>> valid relation prec:0.0516, rec:0.0113, f1:0.0186
>> valid relation with NER prec:0.0516, rec:0.0113, f1:0.0186
g_step 2100, step 20, avg_time 2.505, loss:587.2329
g_step 2200, step 120, avg_time 0.972, loss:547.7644
g_step 2300, step 220, avg_time 0.963, loss:544.8066
g_step 2400, step 320, avg_time 0.971, loss:626.9945
g_step 2500, step 4, avg_time 0.975, loss:582.8155
>> valid entity prec:0.4991, rec:0.3986, f1:0.4433
>> valid relation prec:0.0598, rec:0.0107, f1:0.0182
>> valid relation with NER prec:0.0598, rec:0.0107, f1:0.0182
g_step 2600, step 104, avg_time 2.498, loss:533.8409
g_step 2700, step 204, avg_time 0.976, loss:552.3706
g_step 2800, step 304, avg_time 0.980, loss:570.5113
g_step 2900, step 404, avg_time 0.964, loss:568.3687
g_step 3000, step 88, avg_time 0.976, loss:524.6798
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5586, rec:0.3429, f1:0.4249
>> valid relation prec:0.1750, rec:0.0216, f1:0.0385
>> valid relation with NER prec:0.1750, rec:0.0216, f1:0.0385
g_step 3100, step 188, avg_time 2.499, loss:504.7862
g_step 3200, step 288, avg_time 0.976, loss:542.8549
g_step 3300, step 388, avg_time 0.964, loss:561.4034
g_step 3400, step 72, avg_time 0.959, loss:488.7163
g_step 3500, step 172, avg_time 0.964, loss:516.4479
>> valid entity prec:0.5047, rec:0.3304, f1:0.3994
>> valid relation prec:0.0919, rec:0.0150, f1:0.0258
>> valid relation with NER prec:0.0919, rec:0.0150, f1:0.0258
g_step 3600, step 272, avg_time 2.516, loss:520.7327
g_step 3700, step 372, avg_time 0.989, loss:530.3946
g_step 3800, step 56, avg_time 0.974, loss:488.1978
g_step 3900, step 156, avg_time 0.980, loss:495.6911
g_step 4000, step 256, avg_time 0.968, loss:490.3042
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5105, rec:0.3920, f1:0.4435
>> valid relation prec:0.1615, rec:0.0319, f1:0.0533
>> valid relation with NER prec:0.1615, rec:0.0319, f1:0.0533
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 356, avg_time 2.532, loss:508.7057
g_step 4200, step 40, avg_time 0.970, loss:503.7129
g_step 4300, step 140, avg_time 0.980, loss:483.7615
g_step 4400, step 240, avg_time 0.977, loss:460.3170
g_step 4500, step 340, avg_time 0.967, loss:482.8536
>> valid entity prec:0.5060, rec:0.4403, f1:0.4709
>> valid relation prec:0.1194, rec:0.0235, f1:0.0392
>> valid relation with NER prec:0.1194, rec:0.0235, f1:0.0392
g_step 4600, step 24, avg_time 2.520, loss:501.6998
g_step 4700, step 124, avg_time 0.975, loss:453.1101
g_step 4800, step 224, avg_time 0.984, loss:478.5479
g_step 4900, step 324, avg_time 0.976, loss:476.1094
g_step 5000, step 8, avg_time 0.992, loss:475.6502
learning rate was adjusted to 0.0008
>> valid entity prec:0.5200, rec:0.3895, f1:0.4454
>> valid relation prec:0.0962, rec:0.0167, f1:0.0284
>> valid relation with NER prec:0.0962, rec:0.0167, f1:0.0284
g_step 5100, step 108, avg_time 2.535, loss:426.8088
g_step 5200, step 208, avg_time 0.977, loss:456.5532
g_step 5300, step 308, avg_time 0.975, loss:464.1254
g_step 5400, step 408, avg_time 0.970, loss:455.3542
g_step 5500, step 92, avg_time 0.973, loss:426.7864
>> valid entity prec:0.5057, rec:0.3741, f1:0.4301
>> valid relation prec:0.0723, rec:0.0150, f1:0.0249
>> valid relation with NER prec:0.0723, rec:0.0150, f1:0.0249
g_step 5600, step 192, avg_time 2.508, loss:427.3320
g_step 5700, step 292, avg_time 0.967, loss:452.6489
g_step 5800, step 392, avg_time 0.978, loss:462.0660
g_step 5900, step 76, avg_time 0.989, loss:423.8208
g_step 6000, step 176, avg_time 0.964, loss:430.0773
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4911, rec:0.3683, f1:0.4209
>> valid relation prec:0.0845, rec:0.0173, f1:0.0287
>> valid relation with NER prec:0.0845, rec:0.0173, f1:0.0287
g_step 6100, step 276, avg_time 2.484, loss:429.4159
g_step 6200, step 376, avg_time 0.961, loss:438.8177
g_step 6300, step 60, avg_time 0.976, loss:418.5957
g_step 6400, step 160, avg_time 0.974, loss:410.7878
g_step 6500, step 260, avg_time 0.965, loss:427.2982
>> valid entity prec:0.4980, rec:0.3896, f1:0.4372
>> valid relation prec:0.0715, rec:0.0154, f1:0.0254
>> valid relation with NER prec:0.0715, rec:0.0154, f1:0.0254
g_step 6600, step 360, avg_time 2.522, loss:412.4656
g_step 6700, step 44, avg_time 0.969, loss:419.0298
g_step 6800, step 144, avg_time 0.994, loss:407.3824
g_step 6900, step 244, avg_time 0.985, loss:412.3556
g_step 7000, step 344, avg_time 0.981, loss:397.8322
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4863, rec:0.3654, f1:0.4173
>> valid relation prec:0.0856, rec:0.0187, f1:0.0308
>> valid relation with NER prec:0.0856, rec:0.0187, f1:0.0308
g_step 7100, step 28, avg_time 2.504, loss:405.1168
g_step 7200, step 128, avg_time 0.990, loss:366.2196
g_step 7300, step 228, avg_time 0.979, loss:394.6919
g_step 7400, step 328, avg_time 0.965, loss:392.7522
g_step 7500, step 12, avg_time 0.964, loss:400.4201
>> valid entity prec:0.5190, rec:0.3255, f1:0.4001
>> valid relation prec:0.1015, rec:0.0183, f1:0.0311
>> valid relation with NER prec:0.1015, rec:0.0183, f1:0.0311
g_step 7600, step 112, avg_time 2.524, loss:367.1936
g_step 7700, step 212, avg_time 0.987, loss:378.4938
g_step 7800, step 312, avg_time 0.981, loss:372.7096
g_step 7900, step 412, avg_time 0.992, loss:389.8698
g_step 8000, step 96, avg_time 1.003, loss:360.7981
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4765, rec:0.3834, f1:0.4249
>> valid relation prec:0.0698, rec:0.0179, f1:0.0285
>> valid relation with NER prec:0.0698, rec:0.0179, f1:0.0285
g_step 8100, step 196, avg_time 2.520, loss:353.5826
g_step 8200, step 296, avg_time 0.981, loss:374.8012
g_step 8300, step 396, avg_time 0.970, loss:382.9646
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 07:03:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 07:03:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_07-03-18_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 07:03:20 - WARNING - datasets.builder -   Using custom data configuration default-8549185516e39b3f
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8549185516e39b3f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 07:03:21,593 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:03:21,595 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:03:21,595 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:03:21,596 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:03:21,675 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:03:21,708 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:03:21,709 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:03:21,709 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:03:21,709 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:03:21,709 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:03:21,709 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 07:03:22,033 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:03:25,164 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 07:03:25,181 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8549185516e39b3f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:03,  2.90ba/s] 20%|██        | 2/10 [00:00<00:02,  3.80ba/s] 30%|███       | 3/10 [00:00<00:01,  4.23ba/s] 40%|████      | 4/10 [00:00<00:01,  4.48ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.58ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.66ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.71ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.73ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.74ba/s]100%|██████████| 10/10 [00:02<00:00,  4.77ba/s]100%|██████████| 10/10 [00:02<00:00,  4.53ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.45ba/s] 40%|████      | 2/5 [00:00<00:00,  4.02ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.24ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.21ba/s]100%|██████████| 5/5 [00:01<00:00,  4.52ba/s]100%|██████████| 5/5 [00:01<00:00,  4.30ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  6.66ba/s] 30%|███       | 3/10 [00:00<00:00,  9.07ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.73ba/s] 70%|███████   | 7/10 [00:00<00:00, 10.05ba/s] 90%|█████████ | 9/10 [00:00<00:00, 10.26ba/s]100%|██████████| 10/10 [00:01<00:00,  9.94ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.57ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.32ba/s]100%|██████████| 5/5 [00:00<00:00, 10.28ba/s]100%|██████████| 5/5 [00:00<00:00,  9.78ba/s]
[INFO|trainer.py:414] 2023-08-29 07:03:31,154 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 07:03:31,236 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 07:03:31,236 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-29 07:03:31,236 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 07:03:31,236 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 07:03:31,236 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 07:03:31,236 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 07:03:31,236 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:01<13:39,  1.05s/it]  0%|          | 2/780 [00:01<09:59,  1.30it/s]  0%|          | 3/780 [00:02<07:53,  1.64it/s]  1%|          | 4/780 [00:02<06:46,  1.91it/s]  1%|          | 5/780 [00:02<05:45,  2.24it/s]  1%|          | 6/780 [00:03<05:11,  2.49it/s]  1%|          | 7/780 [00:03<04:46,  2.69it/s]  1%|          | 8/780 [00:03<04:42,  2.73it/s]  1%|          | 9/780 [00:04<04:24,  2.91it/s]  1%|▏         | 10/780 [00:04<04:19,  2.97it/s]  1%|▏         | 11/780 [00:04<04:21,  2.94it/s]  2%|▏         | 12/780 [00:04<04:10,  3.07it/s]  2%|▏         | 13/780 [00:05<04:02,  3.17it/s]  2%|▏         | 14/780 [00:05<03:56,  3.24it/s]  2%|▏         | 15/780 [00:05<03:52,  3.29it/s]  2%|▏         | 16/780 [00:06<03:49,  3.33it/s]  2%|▏         | 17/780 [00:06<03:47,  3.35it/s]  2%|▏         | 18/780 [00:06<03:46,  3.37it/s]  2%|▏         | 19/780 [00:07<03:44,  3.38it/s]  3%|▎         | 20/780 [00:07<03:44,  3.39it/s]  3%|▎         | 21/780 [00:07<03:43,  3.40it/s]  3%|▎         | 22/780 [00:07<03:54,  3.24it/s]  3%|▎         | 23/780 [00:08<03:50,  3.29it/s]  3%|▎         | 24/780 [00:08<03:47,  3.32it/s]  3%|▎         | 25/780 [00:08<03:45,  3.35it/s]  3%|▎         | 26/780 [00:09<03:43,  3.37it/s]  3%|▎         | 27/780 [00:09<03:42,  3.38it/s]  4%|▎         | 28/780 [00:09<03:41,  3.39it/s]  4%|▎         | 29/780 [00:10<03:40,  3.40it/s]  4%|▍         | 30/780 [00:10<03:40,  3.40it/s]  4%|▍         | 31/780 [00:10<03:39,  3.41it/s]  4%|▍         | 32/780 [00:10<03:39,  3.41it/s]  4%|▍         | 33/780 [00:11<03:47,  3.29it/s]  4%|▍         | 34/780 [00:11<03:44,  3.33it/s]  4%|▍         | 35/780 [00:11<03:42,  3.35it/s]  5%|▍         | 36/780 [00:12<03:41,  3.37it/s]  5%|▍         | 37/780 [00:12<03:39,  3.38it/s]  5%|▍         | 38/780 [00:12<03:38,  3.39it/s]  5%|▌         | 39/780 [00:12<03:37,  3.40it/s]  5%|▌         | 40/780 [00:13<03:37,  3.41it/s]  5%|▌         | 41/780 [00:13<03:36,  3.41it/s]  5%|▌         | 42/780 [00:13<03:36,  3.41it/s]  6%|▌         | 43/780 [00:14<03:36,  3.41it/s]  6%|▌         | 44/780 [00:14<03:45,  3.26it/s]  6%|▌         | 45/780 [00:14<03:42,  3.30it/s]  6%|▌         | 46/780 [00:15<03:40,  3.34it/s]  6%|▌         | 47/780 [00:15<03:38,  3.36it/s]  6%|▌         | 48/780 [00:15<03:36,  3.38it/s]  6%|▋         | 49/780 [00:15<03:35,  3.39it/s]  6%|▋         | 50/780 [00:16<03:34,  3.40it/s]  7%|▋         | 51/780 [00:16<03:34,  3.40it/s]  7%|▋         | 52/780 [00:16<03:33,  3.41it/s]  7%|▋         | 53/780 [00:17<03:33,  3.41it/s]  7%|▋         | 54/780 [00:17<03:32,  3.41it/s]  7%|▋         | 55/780 [00:17<03:50,  3.15it/s]  7%|▋         | 56/780 [00:18<03:44,  3.22it/s]  7%|▋         | 57/780 [00:18<04:12,  2.86it/s]  7%|▋         | 58/780 [00:18<04:00,  3.01it/s]  8%|▊         | 59/780 [00:19<03:50,  3.12it/s]  8%|▊         | 60/780 [00:19<03:45,  3.20it/s]  8%|▊         | 61/780 [00:19<03:40,  3.26it/s]  8%|▊         | 62/780 [00:19<03:37,  3.31it/s]  8%|▊         | 63/780 [00:20<03:34,  3.34it/s]  8%|▊         | 64/780 [00:20<03:33,  3.36it/s]  8%|▊         | 65/780 [00:20<03:31,  3.38it/s]  8%|▊         | 66/780 [00:21<03:30,  3.39it/s]  9%|▊         | 67/780 [00:21<03:36,  3.30it/s]  9%|▊         | 68/780 [00:21<03:33,  3.33it/s]  9%|▉         | 69/780 [00:22<03:31,  3.35it/s]  9%|▉         | 70/780 [00:22<03:30,  3.37it/s]  9%|▉         | 71/780 [00:22<03:29,  3.38it/s]  9%|▉         | 72/780 [00:22<03:28,  3.39it/s]  9%|▉         | 73/780 [00:23<03:28,  3.40it/s]  9%|▉         | 74/780 [00:23<03:27,  3.40it/s] 10%|▉         | 75/780 [00:23<03:27,  3.40it/s] 10%|▉         | 76/780 [00:24<03:26,  3.41it/s] 10%|▉         | 77/780 [00:24<03:26,  3.40it/s] 10%|█         | 78/780 [00:24<03:31,  3.31it/s] 10%|█         | 79/780 [00:25<03:29,  3.34it/s] 10%|█         | 80/780 [00:25<03:28,  3.36it/s] 10%|█         | 81/780 [00:25<03:27,  3.38it/s] 11%|█         | 82/780 [00:25<03:26,  3.39it/s] 11%|█         | 83/780 [00:26<03:25,  3.39it/s] 11%|█         | 84/780 [00:26<03:24,  3.40it/s] 11%|█         | 85/780 [00:26<03:23,  3.41it/s] 11%|█         | 86/780 [00:27<03:22,  3.42it/s] 11%|█         | 87/780 [00:27<03:21,  3.44it/s] 11%|█▏        | 88/780 [00:27<03:21,  3.44it/s] 11%|█▏        | 89/780 [00:27<03:25,  3.37it/s] 12%|█▏        | 90/780 [00:28<03:23,  3.39it/s] 12%|█▏        | 91/780 [00:28<03:21,  3.41it/s] 12%|█▏        | 92/780 [00:28<03:20,  3.43it/s] 12%|█▏        | 93/780 [00:29<03:19,  3.44it/s] 12%|█▏        | 94/780 [00:29<03:19,  3.44it/s] 12%|█▏        | 95/780 [00:29<03:18,  3.45it/s] 12%|█▏        | 96/780 [00:29<03:18,  3.45it/s] 12%|█▏        | 97/780 [00:30<03:17,  3.45it/s] 13%|█▎        | 98/780 [00:30<03:17,  3.46it/s] 13%|█▎        | 99/780 [00:30<03:17,  3.45it/s] 13%|█▎        | 100/780 [00:31<03:19,  3.42it/s] 13%|█▎        | 101/780 [00:31<03:17,  3.43it/s] 13%|█▎        | 102/780 [00:31<03:17,  3.44it/s] 13%|█▎        | 103/780 [00:32<03:17,  3.43it/s] 13%|█▎        | 104/780 [00:32<03:17,  3.43it/s] 13%|█▎        | 105/780 [00:32<03:17,  3.42it/s] 14%|█▎        | 106/780 [00:32<03:17,  3.41it/s] 14%|█▎        | 107/780 [00:33<03:17,  3.41it/s] 14%|█▍        | 108/780 [00:33<03:16,  3.41it/s] 14%|█▍        | 109/780 [00:33<03:16,  3.41it/s] 14%|█▍        | 110/780 [00:34<03:16,  3.41it/s] 14%|█▍        | 111/780 [00:34<03:24,  3.28it/s] 14%|█▍        | 112/780 [00:34<03:21,  3.31it/s] 14%|█▍        | 113/780 [00:35<03:19,  3.34it/s] 15%|█▍        | 114/780 [00:35<03:18,  3.36it/s] 15%|█▍        | 115/780 [00:35<03:17,  3.37it/s] 15%|█▍        | 116/780 [00:35<03:16,  3.38it/s] 15%|█▌        | 117/780 [00:36<03:15,  3.39it/s] 15%|█▌        | 118/780 [00:36<03:14,  3.40it/s] 15%|█▌        | 119/780 [00:36<03:14,  3.40it/s] 15%|█▌        | 120/780 [00:37<03:13,  3.40it/s] 16%|█▌        | 121/780 [00:37<03:13,  3.40it/s] 16%|█▌        | 122/780 [00:37<03:17,  3.34it/s] 16%|█▌        | 123/780 [00:37<03:15,  3.36it/s] 16%|█▌        | 124/780 [00:38<03:14,  3.37it/s] 16%|█▌        | 125/780 [00:38<03:13,  3.39it/s] 16%|█▌        | 126/780 [00:38<03:12,  3.39it/s] 16%|█▋        | 127/780 [00:39<03:12,  3.39it/s] 16%|█▋        | 128/780 [00:39<03:11,  3.40it/s] 17%|█▋        | 129/780 [00:39<03:11,  3.40it/s] 17%|█▋        | 130/780 [00:40<03:11,  3.40it/s] 17%|█▋        | 131/780 [00:40<03:10,  3.40it/s] 17%|█▋        | 132/780 [00:40<03:10,  3.40it/s] 17%|█▋        | 133/780 [00:40<03:15,  3.31it/s] 17%|█▋        | 134/780 [00:41<03:13,  3.33it/s] 17%|█▋        | 135/780 [00:41<03:12,  3.35it/s] 17%|█▋        | 136/780 [00:41<03:11,  3.37it/s] 18%|█▊        | 137/780 [00:42<03:10,  3.38it/s] 18%|█▊        | 138/780 [00:42<03:09,  3.39it/s] 18%|█▊        | 139/780 [00:42<03:08,  3.39it/s] 18%|█▊        | 140/780 [00:42<03:08,  3.40it/s] 18%|█▊        | 141/780 [00:43<03:07,  3.40it/s] 18%|█▊        | 142/780 [00:43<03:07,  3.40it/s] 18%|█▊        | 143/780 [00:43<03:07,  3.41it/s] 18%|█▊        | 144/780 [00:44<03:11,  3.32it/s] 19%|█▊        | 145/780 [00:44<03:10,  3.34it/s] 19%|█▊        | 146/780 [00:44<03:08,  3.36it/s] 19%|█▉        | 147/780 [00:45<03:07,  3.37it/s] 19%|█▉        | 148/780 [00:45<03:06,  3.38it/s] 19%|█▉        | 149/780 [00:45<03:11,  3.30it/s] 19%|█▉        | 150/780 [00:45<03:09,  3.33it/s] 19%|█▉        | 151/780 [00:46<03:07,  3.35it/s] 19%|█▉        | 152/780 [00:46<03:06,  3.37it/s] 20%|█▉        | 153/780 [00:46<03:05,  3.38it/s] 20%|█▉        | 154/780 [00:47<03:04,  3.39it/s] 20%|█▉        | 155/780 [00:47<03:06,  3.36it/s] 20%|██        | 156/780 [00:47<03:44,  2.78it/s][INFO|trainer.py:2140] 2023-08-29 07:04:19,227 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:04:19,227 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 07:04:19,227 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.49it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.38it/s][A
  3%|▎         | 17/608 [00:00<00:12, 48.01it/s][A
  4%|▎         | 22/608 [00:00<00:12, 47.15it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.64it/s][A
  5%|▌         | 32/608 [00:00<00:12, 46.25it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.91it/s][A
  7%|▋         | 42/608 [00:00<00:12, 43.63it/s][A
  8%|▊         | 47/608 [00:01<00:12, 44.23it/s][A
  9%|▊         | 52/608 [00:01<00:12, 44.64it/s][A
  9%|▉         | 57/608 [00:01<00:12, 44.87it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.04it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.13it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.17it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.44it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.04it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.13it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 45.25it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.33it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.43it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.49it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.38it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.36it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.43it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.15it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 45.28it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 45.39it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.33it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.43it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.38it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.49it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.33it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.23it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.18it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 43.98it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.45it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.79it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.01it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.25it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.31it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.33it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.26it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.14it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.17it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.17it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.33it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.43it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.52it/s][A
 41%|████      | 247/608 [00:05<00:07, 45.36it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.42it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 45.37it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.25it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.20it/s][A
 45%|████▍     | 272/608 [00:05<00:07, 45.21it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.09it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.38it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.44it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.43it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.40it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.36it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.31it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.20it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 43.34it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.12it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.65it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.87it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 45.09it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 44.99it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.05it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 44.94it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 44.75it/s][A
 60%|█████▉    | 362/608 [00:07<00:05, 44.90it/s][A
 60%|██████    | 367/608 [00:08<00:05, 45.10it/s][A
 61%|██████    | 372/608 [00:08<00:05, 45.44it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.50it/s][A
 63%|██████▎   | 382/608 [00:08<00:04, 45.54it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.35it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 45.37it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.12it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 45.12it/s][A
 67%|██████▋   | 407/608 [00:08<00:04, 45.02it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.23it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.38it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.51it/s][A
 70%|███████   | 427/608 [00:09<00:03, 45.57it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.53it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.45it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.14it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 44.96it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.91it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.31it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.79it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.15it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.30it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.27it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.26it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.14it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.06it/s][A
 82%|████████▏ | 497/608 [00:10<00:02, 44.89it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.16it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.30it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.40it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.41it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.48it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.44it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 45.26it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.12it/s][A
 89%|████████▉ | 542/608 [00:11<00:01, 45.18it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.11it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.14it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.34it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.52it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.34it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.42it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.29it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.11it/s][A
 97%|█████████▋| 587/608 [00:12<00:00, 45.17it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.21it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 43.96it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.58it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 44.79it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.79it/s][A 20%|██        | 156/780 [01:01<03:44,  2.78it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:04:33,030 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 07:04:33,233 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:04:36,072 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:04:36,169 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:04:36,227 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:11<1:16:49,  7.40s/it] 20%|██        | 158/780 [01:12<54:49,  5.29s/it]   20%|██        | 159/780 [01:12<39:13,  3.79s/it] 21%|██        | 160/780 [01:12<28:19,  2.74s/it] 21%|██        | 161/780 [01:13<20:41,  2.01s/it] 21%|██        | 162/780 [01:13<15:22,  1.49s/it] 21%|██        | 163/780 [01:13<11:38,  1.13s/it] 21%|██        | 164/780 [01:13<09:02,  1.14it/s] 21%|██        | 165/780 [01:14<07:13,  1.42it/s] 21%|██▏       | 166/780 [01:14<05:56,  1.72it/s] 21%|██▏       | 167/780 [01:14<05:02,  2.02it/s] 22%|██▏       | 168/780 [01:15<04:25,  2.30it/s] 22%|██▏       | 169/780 [01:15<04:06,  2.47it/s] 22%|██▏       | 170/780 [01:15<03:46,  2.70it/s] 22%|██▏       | 171/780 [01:15<03:31,  2.88it/s] 22%|██▏       | 172/780 [01:16<03:21,  3.02it/s] 22%|██▏       | 173/780 [01:16<03:14,  3.13it/s] 22%|██▏       | 174/780 [01:16<03:08,  3.21it/s] 22%|██▏       | 175/780 [01:17<03:05,  3.27it/s] 23%|██▎       | 176/780 [01:17<03:02,  3.31it/s] 23%|██▎       | 177/780 [01:17<03:00,  3.34it/s] 23%|██▎       | 178/780 [01:18<02:59,  3.36it/s] 23%|██▎       | 179/780 [01:18<02:57,  3.38it/s] 23%|██▎       | 180/780 [01:18<03:05,  3.24it/s] 23%|██▎       | 181/780 [01:18<03:02,  3.29it/s] 23%|██▎       | 182/780 [01:19<02:59,  3.33it/s] 23%|██▎       | 183/780 [01:19<02:58,  3.35it/s] 24%|██▎       | 184/780 [01:19<03:17,  3.02it/s] 24%|██▎       | 185/780 [01:20<03:10,  3.13it/s] 24%|██▍       | 186/780 [01:20<03:05,  3.21it/s] 24%|██▍       | 187/780 [01:20<03:00,  3.29it/s] 24%|██▍       | 188/780 [01:21<02:57,  3.33it/s] 24%|██▍       | 189/780 [01:21<02:55,  3.38it/s] 24%|██▍       | 190/780 [01:21<02:53,  3.40it/s] 24%|██▍       | 191/780 [01:21<02:52,  3.42it/s] 25%|██▍       | 192/780 [01:22<02:51,  3.43it/s] 25%|██▍       | 193/780 [01:22<02:50,  3.44it/s] 25%|██▍       | 194/780 [01:22<03:05,  3.16it/s] 25%|██▌       | 195/780 [01:23<03:00,  3.24it/s] 25%|██▌       | 196/780 [01:23<02:56,  3.31it/s] 25%|██▌       | 197/780 [01:23<02:53,  3.35it/s] 25%|██▌       | 198/780 [01:24<02:52,  3.38it/s] 26%|██▌       | 199/780 [01:24<02:50,  3.40it/s] 26%|██▌       | 200/780 [01:24<02:49,  3.42it/s] 26%|██▌       | 201/780 [01:24<02:48,  3.43it/s] 26%|██▌       | 202/780 [01:25<02:48,  3.44it/s] 26%|██▌       | 203/780 [01:25<02:47,  3.44it/s] 26%|██▌       | 204/780 [01:25<02:47,  3.44it/s] 26%|██▋       | 205/780 [01:26<02:50,  3.38it/s] 26%|██▋       | 206/780 [01:26<02:48,  3.40it/s] 27%|██▋       | 207/780 [01:26<02:47,  3.42it/s] 27%|██▋       | 208/780 [01:27<02:46,  3.43it/s] 27%|██▋       | 209/780 [01:27<02:46,  3.44it/s] 27%|██▋       | 210/780 [01:27<02:45,  3.44it/s] 27%|██▋       | 211/780 [01:27<02:45,  3.44it/s] 27%|██▋       | 212/780 [01:28<02:44,  3.44it/s] 27%|██▋       | 213/780 [01:28<02:44,  3.45it/s] 27%|██▋       | 214/780 [01:28<02:44,  3.45it/s] 28%|██▊       | 215/780 [01:29<02:43,  3.45it/s] 28%|██▊       | 216/780 [01:29<02:46,  3.38it/s] 28%|██▊       | 217/780 [01:29<02:45,  3.40it/s] 28%|██▊       | 218/780 [01:29<02:44,  3.41it/s] 28%|██▊       | 219/780 [01:30<02:43,  3.42it/s] 28%|██▊       | 220/780 [01:30<02:43,  3.44it/s] 28%|██▊       | 221/780 [01:30<02:42,  3.44it/s] 28%|██▊       | 222/780 [01:31<02:42,  3.44it/s] 29%|██▊       | 223/780 [01:31<02:41,  3.44it/s] 29%|██▊       | 224/780 [01:31<02:41,  3.45it/s] 29%|██▉       | 225/780 [01:31<02:41,  3.45it/s] 29%|██▉       | 226/780 [01:32<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:32<02:45,  3.35it/s] 29%|██▉       | 228/780 [01:32<02:43,  3.38it/s] 29%|██▉       | 229/780 [01:33<02:42,  3.40it/s] 29%|██▉       | 230/780 [01:33<02:41,  3.41it/s] 30%|██▉       | 231/780 [01:33<02:40,  3.43it/s] 30%|██▉       | 232/780 [01:34<02:39,  3.43it/s] 30%|██▉       | 233/780 [01:34<02:39,  3.44it/s] 30%|███       | 234/780 [01:34<02:38,  3.44it/s] 30%|███       | 235/780 [01:34<02:38,  3.44it/s] 30%|███       | 236/780 [01:35<02:37,  3.44it/s] 30%|███       | 237/780 [01:35<02:37,  3.45it/s] 31%|███       | 238/780 [01:35<02:42,  3.34it/s] 31%|███       | 239/780 [01:36<02:40,  3.38it/s] 31%|███       | 240/780 [01:36<02:38,  3.40it/s] 31%|███       | 241/780 [01:36<02:38,  3.40it/s] 31%|███       | 242/780 [01:36<02:37,  3.42it/s] 31%|███       | 243/780 [01:37<02:36,  3.43it/s] 31%|███▏      | 244/780 [01:37<02:36,  3.43it/s] 31%|███▏      | 245/780 [01:37<02:35,  3.43it/s] 32%|███▏      | 246/780 [01:38<02:35,  3.44it/s] 32%|███▏      | 247/780 [01:38<02:34,  3.44it/s] 32%|███▏      | 248/780 [01:38<02:34,  3.45it/s] 32%|███▏      | 249/780 [01:38<02:37,  3.37it/s] 32%|███▏      | 250/780 [01:39<02:35,  3.40it/s] 32%|███▏      | 251/780 [01:39<02:34,  3.41it/s] 32%|███▏      | 252/780 [01:39<02:34,  3.42it/s] 32%|███▏      | 253/780 [01:40<02:33,  3.43it/s] 33%|███▎      | 254/780 [01:40<02:33,  3.44it/s] 33%|███▎      | 255/780 [01:40<02:32,  3.44it/s] 33%|███▎      | 256/780 [01:41<02:32,  3.45it/s] 33%|███▎      | 257/780 [01:41<02:31,  3.44it/s] 33%|███▎      | 258/780 [01:41<02:31,  3.45it/s] 33%|███▎      | 259/780 [01:41<02:31,  3.45it/s] 33%|███▎      | 260/780 [01:42<02:36,  3.33it/s] 33%|███▎      | 261/780 [01:42<02:34,  3.37it/s] 34%|███▎      | 262/780 [01:42<02:32,  3.39it/s] 34%|███▎      | 263/780 [01:43<02:31,  3.41it/s] 34%|███▍      | 264/780 [01:43<02:30,  3.42it/s] 34%|███▍      | 265/780 [01:43<02:30,  3.43it/s] 34%|███▍      | 266/780 [01:43<02:29,  3.44it/s] 34%|███▍      | 267/780 [01:44<02:28,  3.44it/s] 34%|███▍      | 268/780 [01:44<02:28,  3.44it/s] 34%|███▍      | 269/780 [01:44<02:28,  3.45it/s] 35%|███▍      | 270/780 [01:45<02:27,  3.45it/s] 35%|███▍      | 271/780 [01:45<02:32,  3.35it/s] 35%|███▍      | 272/780 [01:45<02:34,  3.29it/s] 35%|███▌      | 273/780 [01:46<02:31,  3.34it/s] 35%|███▌      | 274/780 [01:46<02:29,  3.37it/s] 35%|███▌      | 275/780 [01:46<02:28,  3.39it/s] 35%|███▌      | 276/780 [01:46<02:27,  3.41it/s] 36%|███▌      | 277/780 [01:47<02:27,  3.42it/s] 36%|███▌      | 278/780 [01:47<02:26,  3.43it/s] 36%|███▌      | 279/780 [01:47<02:57,  2.82it/s] 36%|███▌      | 280/780 [01:48<02:47,  2.99it/s] 36%|███▌      | 281/780 [01:48<02:43,  3.05it/s] 36%|███▌      | 282/780 [01:48<02:37,  3.16it/s] 36%|███▋      | 283/780 [01:49<02:33,  3.24it/s] 36%|███▋      | 284/780 [01:49<02:30,  3.30it/s] 37%|███▋      | 285/780 [01:49<02:27,  3.35it/s] 37%|███▋      | 286/780 [01:50<02:26,  3.38it/s] 37%|███▋      | 287/780 [01:50<02:25,  3.40it/s] 37%|███▋      | 288/780 [01:50<02:24,  3.41it/s] 37%|███▋      | 289/780 [01:50<02:23,  3.42it/s] 37%|███▋      | 290/780 [01:51<02:22,  3.43it/s] 37%|███▋      | 291/780 [01:51<02:22,  3.44it/s] 37%|███▋      | 292/780 [01:51<02:21,  3.44it/s] 38%|███▊      | 293/780 [01:52<02:21,  3.44it/s] 38%|███▊      | 294/780 [01:52<02:21,  3.44it/s] 38%|███▊      | 295/780 [01:52<02:20,  3.44it/s] 38%|███▊      | 296/780 [01:52<02:20,  3.45it/s] 38%|███▊      | 297/780 [01:53<02:20,  3.44it/s] 38%|███▊      | 298/780 [01:53<02:19,  3.45it/s] 38%|███▊      | 299/780 [01:53<02:21,  3.39it/s] 38%|███▊      | 300/780 [01:54<02:20,  3.41it/s] 39%|███▊      | 301/780 [01:54<02:20,  3.42it/s] 39%|███▊      | 302/780 [01:54<02:19,  3.43it/s] 39%|███▉      | 303/780 [01:54<02:18,  3.43it/s] 39%|███▉      | 304/780 [01:55<02:18,  3.44it/s] 39%|███▉      | 305/780 [01:55<02:17,  3.44it/s] 39%|███▉      | 306/780 [01:55<02:17,  3.44it/s] 39%|███▉      | 307/780 [01:56<02:17,  3.45it/s] 39%|███▉      | 308/780 [01:56<02:16,  3.45it/s] 40%|███▉      | 309/780 [01:56<02:16,  3.45it/s] 40%|███▉      | 310/780 [01:57<02:18,  3.40it/s] 40%|███▉      | 311/780 [01:57<02:17,  3.42it/s] 40%|████      | 312/780 [01:57<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 07:05:28,879 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:05:28,880 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 07:05:28,880 >>   Batch size = 8
{'eval_loss': 0.9668400287628174, 'eval_runtime': 13.4956, 'eval_samples_per_second': 360.415, 'eval_steps_per_second': 45.052, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 55.94it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.16it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.42it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.50it/s][A
  4%|▍         | 27/608 [00:00<00:12, 46.01it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.58it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.28it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.04it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.16it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.23it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.37it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.42it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.35it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.36it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.23it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.04it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.95it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.91it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 45.18it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 43.71it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.31it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 44.65it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 44.83it/s][A
 20%|██        | 122/608 [00:02<00:10, 44.91it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.72it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.72it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.90it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.78it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.04it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.19it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.39it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.35it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.19it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.09it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 45.10it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 45.02it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.92it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.06it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.14it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.32it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.34it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.26it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.20it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 45.07it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.97it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 44.99it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 44.51it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.75it/s][A
 41%|████      | 247/608 [00:05<00:08, 45.04it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.06it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 45.17it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.09it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.05it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.00it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.84it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.66it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.22it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.33it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.34it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.34it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.21it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.08it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.98it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.94it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.02it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.15it/s][A
 55%|█████▌    | 337/608 [00:07<00:05, 45.18it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.33it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.17it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.08it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 45.13it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.89it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.98it/s][A
 61%|██████    | 372/608 [00:08<00:05, 45.00it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 42.09it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 43.18it/s][A
 64%|██████▎   | 387/608 [00:08<00:05, 43.95it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.49it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 44.69it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.80it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.61it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.84it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.68it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 44.79it/s][A
 70%|███████   | 427/608 [00:09<00:04, 45.06it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.28it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.40it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.36it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.11it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 45.06it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.92it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.67it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 44.73it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 44.90it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.21it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.31it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.28it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.25it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 45.13it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.11it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.84it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 43.79it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.27it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.62it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.91it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 44.97it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 45.08it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.99it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.93it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.72it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 44.78it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.03it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.16it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.20it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.16it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 44.80it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.17it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.97it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.88it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.89it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.07it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.07it/s][A 40%|████      | 312/780 [02:11<02:16,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:05:42,894 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 07:05:43,215 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:05:47,338 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:05:47,638 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:05:47,773 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:26<1:09:48,  8.97s/it] 40%|████      | 314/780 [02:27<49:33,  6.38s/it]   40%|████      | 315/780 [02:27<35:18,  4.56s/it] 41%|████      | 316/780 [02:27<25:20,  3.28s/it] 41%|████      | 317/780 [02:28<18:22,  2.38s/it] 41%|████      | 318/780 [02:28<13:31,  1.76s/it] 41%|████      | 319/780 [02:28<10:07,  1.32s/it] 41%|████      | 320/780 [02:28<07:44,  1.01s/it] 41%|████      | 321/780 [02:29<06:04,  1.26it/s] 41%|████▏     | 322/780 [02:29<04:55,  1.55it/s] 41%|████▏     | 323/780 [02:29<04:06,  1.85it/s] 42%|████▏     | 324/780 [02:30<03:32,  2.15it/s] 42%|████▏     | 325/780 [02:30<03:10,  2.39it/s] 42%|████▏     | 326/780 [02:30<02:53,  2.62it/s] 42%|████▏     | 327/780 [02:30<02:40,  2.82it/s] 42%|████▏     | 328/780 [02:31<02:32,  2.97it/s] 42%|████▏     | 329/780 [02:31<02:26,  3.09it/s] 42%|████▏     | 330/780 [02:31<02:21,  3.18it/s] 42%|████▏     | 331/780 [02:32<02:18,  3.24it/s] 43%|████▎     | 332/780 [02:32<02:16,  3.29it/s] 43%|████▎     | 333/780 [02:32<02:14,  3.32it/s] 43%|████▎     | 334/780 [02:33<02:13,  3.34it/s] 43%|████▎     | 335/780 [02:33<02:12,  3.36it/s] 43%|████▎     | 336/780 [02:33<02:14,  3.31it/s] 43%|████▎     | 337/780 [02:33<02:12,  3.34it/s] 43%|████▎     | 338/780 [02:34<02:11,  3.36it/s] 43%|████▎     | 339/780 [02:34<02:10,  3.37it/s] 44%|████▎     | 340/780 [02:34<02:10,  3.38it/s] 44%|████▎     | 341/780 [02:35<02:09,  3.38it/s] 44%|████▍     | 342/780 [02:35<02:09,  3.39it/s] 44%|████▍     | 343/780 [02:35<02:08,  3.39it/s] 44%|████▍     | 344/780 [02:36<02:08,  3.40it/s] 44%|████▍     | 345/780 [02:36<02:08,  3.40it/s] 44%|████▍     | 346/780 [02:36<02:07,  3.40it/s] 44%|████▍     | 347/780 [02:36<02:12,  3.27it/s] 45%|████▍     | 348/780 [02:37<02:10,  3.31it/s] 45%|████▍     | 349/780 [02:37<02:09,  3.34it/s] 45%|████▍     | 350/780 [02:37<02:08,  3.35it/s] 45%|████▌     | 351/780 [02:38<02:07,  3.37it/s] 45%|████▌     | 352/780 [02:38<02:06,  3.38it/s] 45%|████▌     | 353/780 [02:38<02:06,  3.38it/s] 45%|████▌     | 354/780 [02:38<02:05,  3.39it/s] 46%|████▌     | 355/780 [02:39<02:05,  3.39it/s] 46%|████▌     | 356/780 [02:39<02:04,  3.39it/s] 46%|████▌     | 357/780 [02:39<02:04,  3.39it/s] 46%|████▌     | 358/780 [02:40<02:07,  3.31it/s] 46%|████▌     | 359/780 [02:40<02:06,  3.34it/s] 46%|████▌     | 360/780 [02:40<02:05,  3.36it/s] 46%|████▋     | 361/780 [02:41<02:04,  3.38it/s] 46%|████▋     | 362/780 [02:41<02:03,  3.38it/s] 47%|████▋     | 363/780 [02:41<02:02,  3.39it/s] 47%|████▋     | 364/780 [02:41<02:02,  3.40it/s] 47%|████▋     | 365/780 [02:42<02:02,  3.40it/s] 47%|████▋     | 366/780 [02:42<02:01,  3.40it/s] 47%|████▋     | 367/780 [02:42<02:01,  3.40it/s] 47%|████▋     | 368/780 [02:43<02:00,  3.41it/s] 47%|████▋     | 369/780 [02:43<02:03,  3.33it/s] 47%|████▋     | 370/780 [02:43<02:02,  3.35it/s] 48%|████▊     | 371/780 [02:44<02:01,  3.37it/s] 48%|████▊     | 372/780 [02:44<02:00,  3.38it/s] 48%|████▊     | 373/780 [02:44<02:00,  3.39it/s] 48%|████▊     | 374/780 [02:44<01:59,  3.39it/s] 48%|████▊     | 375/780 [02:45<01:59,  3.40it/s] 48%|████▊     | 376/780 [02:45<01:58,  3.40it/s] 48%|████▊     | 377/780 [02:45<02:00,  3.34it/s] 48%|████▊     | 378/780 [02:46<01:59,  3.36it/s] 49%|████▊     | 379/780 [02:46<01:58,  3.38it/s] 49%|████▊     | 380/780 [02:46<01:59,  3.35it/s] 49%|████▉     | 381/780 [02:46<01:58,  3.37it/s] 49%|████▉     | 382/780 [02:47<01:57,  3.38it/s] 49%|████▉     | 383/780 [02:47<01:57,  3.39it/s] 49%|████▉     | 384/780 [02:48<02:16,  2.89it/s] 49%|████▉     | 385/780 [02:48<02:10,  3.03it/s] 49%|████▉     | 386/780 [02:48<02:05,  3.13it/s] 50%|████▉     | 387/780 [02:48<02:02,  3.21it/s] 50%|████▉     | 388/780 [02:49<02:00,  3.27it/s] 50%|████▉     | 389/780 [02:49<01:58,  3.31it/s] 50%|█████     | 390/780 [02:49<01:59,  3.27it/s] 50%|█████     | 391/780 [02:50<01:57,  3.31it/s] 50%|█████     | 392/780 [02:50<01:56,  3.34it/s] 50%|█████     | 393/780 [02:50<01:55,  3.36it/s] 51%|█████     | 394/780 [02:51<01:54,  3.37it/s] 51%|█████     | 395/780 [02:51<01:53,  3.38it/s] 51%|█████     | 396/780 [02:51<01:53,  3.39it/s] 51%|█████     | 397/780 [02:51<01:52,  3.39it/s] 51%|█████     | 398/780 [02:52<01:52,  3.40it/s] 51%|█████     | 399/780 [02:52<01:52,  3.40it/s] 51%|█████▏    | 400/780 [02:52<01:51,  3.40it/s] 51%|█████▏    | 401/780 [02:53<01:51,  3.40it/s] 52%|█████▏    | 402/780 [02:53<01:51,  3.40it/s] 52%|█████▏    | 403/780 [02:53<01:50,  3.40it/s] 52%|█████▏    | 404/780 [02:53<01:50,  3.40it/s] 52%|█████▏    | 405/780 [02:54<01:50,  3.40it/s] 52%|█████▏    | 406/780 [02:54<01:49,  3.40it/s] 52%|█████▏    | 407/780 [02:54<01:49,  3.40it/s] 52%|█████▏    | 408/780 [02:55<01:51,  3.35it/s] 52%|█████▏    | 409/780 [02:55<01:50,  3.36it/s] 53%|█████▎    | 410/780 [02:55<01:49,  3.38it/s] 53%|█████▎    | 411/780 [02:56<01:48,  3.39it/s] 53%|█████▎    | 412/780 [02:56<01:48,  3.39it/s] 53%|█████▎    | 413/780 [02:56<01:48,  3.40it/s] 53%|█████▎    | 414/780 [02:56<01:47,  3.40it/s] 53%|█████▎    | 415/780 [02:57<01:47,  3.40it/s] 53%|█████▎    | 416/780 [02:57<01:47,  3.40it/s] 53%|█████▎    | 417/780 [02:57<01:46,  3.40it/s] 54%|█████▎    | 418/780 [02:58<01:46,  3.40it/s] 54%|█████▎    | 419/780 [02:58<01:48,  3.33it/s] 54%|█████▍    | 420/780 [02:58<01:47,  3.35it/s] 54%|█████▍    | 421/780 [02:58<01:46,  3.37it/s] 54%|█████▍    | 422/780 [02:59<01:46,  3.37it/s] 54%|█████▍    | 423/780 [02:59<01:45,  3.38it/s] 54%|█████▍    | 424/780 [02:59<01:45,  3.39it/s] 54%|█████▍    | 425/780 [03:00<01:44,  3.39it/s] 55%|█████▍    | 426/780 [03:00<01:44,  3.40it/s] 55%|█████▍    | 427/780 [03:00<01:43,  3.40it/s] 55%|█████▍    | 428/780 [03:01<01:43,  3.40it/s] 55%|█████▌    | 429/780 [03:01<01:43,  3.40it/s] 55%|█████▌    | 430/780 [03:01<01:45,  3.32it/s] 55%|█████▌    | 431/780 [03:01<01:44,  3.34it/s] 55%|█████▌    | 432/780 [03:02<01:43,  3.36it/s] 56%|█████▌    | 433/780 [03:02<01:43,  3.37it/s] 56%|█████▌    | 434/780 [03:02<01:42,  3.38it/s] 56%|█████▌    | 435/780 [03:03<01:41,  3.39it/s] 56%|█████▌    | 436/780 [03:03<01:41,  3.39it/s] 56%|█████▌    | 437/780 [03:03<01:41,  3.40it/s] 56%|█████▌    | 438/780 [03:03<01:40,  3.40it/s] 56%|█████▋    | 439/780 [03:04<01:40,  3.40it/s] 56%|█████▋    | 440/780 [03:04<01:40,  3.40it/s] 57%|█████▋    | 441/780 [03:04<01:42,  3.31it/s] 57%|█████▋    | 442/780 [03:05<01:41,  3.34it/s] 57%|█████▋    | 443/780 [03:05<01:40,  3.36it/s] 57%|█████▋    | 444/780 [03:05<01:39,  3.37it/s] 57%|█████▋    | 445/780 [03:06<01:39,  3.37it/s] 57%|█████▋    | 446/780 [03:06<01:38,  3.38it/s] 57%|█████▋    | 447/780 [03:06<01:38,  3.38it/s] 57%|█████▋    | 448/780 [03:06<01:37,  3.39it/s] 58%|█████▊    | 449/780 [03:07<01:37,  3.39it/s] 58%|█████▊    | 450/780 [03:07<01:37,  3.39it/s] 58%|█████▊    | 451/780 [03:07<01:36,  3.40it/s] 58%|█████▊    | 452/780 [03:08<01:38,  3.35it/s] 58%|█████▊    | 453/780 [03:08<01:37,  3.36it/s] 58%|█████▊    | 454/780 [03:08<01:36,  3.37it/s] 58%|█████▊    | 455/780 [03:09<01:36,  3.38it/s] 58%|█████▊    | 456/780 [03:09<01:35,  3.39it/s] 59%|█████▊    | 457/780 [03:09<01:35,  3.39it/s] 59%|█████▊    | 458/780 [03:09<01:34,  3.39it/s] 59%|█████▉    | 459/780 [03:10<01:34,  3.39it/s] 59%|█████▉    | 460/780 [03:10<01:34,  3.40it/s] 59%|█████▉    | 461/780 [03:10<01:33,  3.40it/s] 59%|█████▉    | 462/780 [03:11<01:33,  3.40it/s] 59%|█████▉    | 463/780 [03:11<01:36,  3.30it/s] 59%|█████▉    | 464/780 [03:11<01:35,  3.32it/s] 60%|█████▉    | 465/780 [03:12<01:34,  3.35it/s] 60%|█████▉    | 466/780 [03:12<01:33,  3.36it/s] 60%|█████▉    | 467/780 [03:12<01:32,  3.38it/s] 60%|██████    | 468/780 [03:12<01:32,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 07:06:44,177 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:06:44,177 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 07:06:44,177 >>   Batch size = 8
{'eval_loss': 0.9738293886184692, 'eval_runtime': 13.5556, 'eval_samples_per_second': 358.819, 'eval_steps_per_second': 44.852, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.44it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.17it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.61it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.43it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.75it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.45it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.14it/s][A
  7%|▋         | 42/608 [00:00<00:12, 44.96it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.01it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.13it/s][A
  9%|▉         | 57/608 [00:01<00:12, 42.84it/s][A
 10%|█         | 62/608 [00:01<00:12, 43.62it/s][A
 11%|█         | 67/608 [00:01<00:12, 44.11it/s][A
 12%|█▏        | 72/608 [00:01<00:12, 44.54it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 44.72it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.71it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.70it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.73it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 44.72it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 44.77it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.00it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.10it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.21it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.10it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.05it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.93it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.88it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.76it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.75it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 44.97it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.17it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.18it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.01it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 44.96it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 45.02it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.93it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.94it/s][A
 32%|███▏      | 192/608 [00:04<00:10, 40.82it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 42.19it/s][A
 33%|███▎      | 202/608 [00:04<00:09, 43.16it/s][A
 34%|███▍      | 207/608 [00:04<00:09, 43.83it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 44.32it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.69it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.69it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.67it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 44.42it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 44.34it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.71it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.71it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.97it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 45.15it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.28it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.20it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.98it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.82it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.63it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.88it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 45.01it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.02it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.09it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.13it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.14it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.92it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.83it/s][A
 54%|█████▍    | 327/608 [00:07<00:07, 39.62it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 41.27it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 42.39it/s][A
 56%|█████▋    | 342/608 [00:07<00:06, 43.33it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 43.94it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 44.42it/s][A
 59%|█████▊    | 357/608 [00:08<00:05, 44.69it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.80it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.38it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.35it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.36it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.70it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.97it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 45.13it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.27it/s][A
 66%|██████▌   | 402/608 [00:09<00:04, 45.35it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 45.09it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.84it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.75it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 44.70it/s][A
 70%|███████   | 427/608 [00:09<00:05, 35.85it/s][A
 71%|███████   | 432/608 [00:09<00:04, 38.28it/s][A
 72%|███████▏  | 437/608 [00:09<00:04, 40.14it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 41.69it/s][A
 74%|███████▎  | 447/608 [00:10<00:03, 42.70it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 43.57it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.11it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.39it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 44.17it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 43.86it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 44.24it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 44.44it/s][A
 80%|████████  | 487/608 [00:10<00:02, 44.93it/s][A
 81%|████████  | 492/608 [00:11<00:02, 45.09it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 45.20it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.16it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.98it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.86it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.66it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.62it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.86it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 44.90it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 45.11it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 45.17it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.33it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.10it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 41.97it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 42.97it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 43.43it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 43.96it/s][A
 95%|█████████▍| 577/608 [00:13<00:00, 44.29it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 44.69it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 44.96it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.90it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.58it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.63it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 44.79it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 44.79it/s][A 60%|██████    | 468/780 [03:26<01:32,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:06:58,100 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 07:06:58,215 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:07:05,679 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:07:05,772 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:07:05,832 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:40<44:36,  8.60s/it] 60%|██████    | 470/780 [03:41<31:36,  6.12s/it] 60%|██████    | 471/780 [03:41<22:30,  4.37s/it] 61%|██████    | 472/780 [03:41<16:09,  3.15s/it] 61%|██████    | 473/780 [03:42<11:43,  2.29s/it] 61%|██████    | 474/780 [03:42<08:37,  1.69s/it] 61%|██████    | 475/780 [03:42<06:28,  1.27s/it] 61%|██████    | 476/780 [03:42<04:57,  1.02it/s] 61%|██████    | 477/780 [03:43<03:54,  1.29it/s] 61%|██████▏   | 478/780 [03:43<03:10,  1.59it/s] 61%|██████▏   | 479/780 [03:43<02:39,  1.89it/s] 62%|██████▏   | 480/780 [03:44<02:17,  2.18it/s] 62%|██████▏   | 481/780 [03:44<02:04,  2.41it/s] 62%|██████▏   | 482/780 [03:44<01:52,  2.64it/s] 62%|██████▏   | 483/780 [03:45<01:44,  2.83it/s] 62%|██████▏   | 484/780 [03:45<01:39,  2.98it/s] 62%|██████▏   | 485/780 [03:45<01:35,  3.10it/s] 62%|██████▏   | 486/780 [03:45<01:34,  3.10it/s] 62%|██████▏   | 487/780 [03:46<01:31,  3.19it/s] 63%|██████▎   | 488/780 [03:46<01:29,  3.25it/s] 63%|██████▎   | 489/780 [03:46<01:28,  3.29it/s] 63%|██████▎   | 490/780 [03:47<01:27,  3.33it/s] 63%|██████▎   | 491/780 [03:47<01:26,  3.35it/s] 63%|██████▎   | 492/780 [03:47<01:27,  3.30it/s] 63%|██████▎   | 493/780 [03:48<01:44,  2.75it/s] 63%|██████▎   | 494/780 [03:48<01:38,  2.91it/s] 63%|██████▎   | 495/780 [03:48<01:33,  3.05it/s] 64%|██████▎   | 496/780 [03:49<01:30,  3.15it/s] 64%|██████▎   | 497/780 [03:49<01:27,  3.22it/s] 64%|██████▍   | 498/780 [03:49<01:26,  3.27it/s] 64%|██████▍   | 499/780 [03:49<01:24,  3.31it/s] 64%|██████▍   | 500/780 [03:50<01:23,  3.34it/s]                                                  64%|██████▍   | 500/780 [03:50<01:23,  3.34it/s] 64%|██████▍   | 501/780 [03:50<01:23,  3.35it/s] 64%|██████▍   | 502/780 [03:50<01:22,  3.35it/s] 64%|██████▍   | 503/780 [03:51<01:22,  3.37it/s] 65%|██████▍   | 504/780 [03:51<01:21,  3.38it/s] 65%|██████▍   | 505/780 [03:51<01:21,  3.38it/s] 65%|██████▍   | 506/780 [03:52<01:20,  3.39it/s] 65%|██████▌   | 507/780 [03:52<01:20,  3.39it/s] 65%|██████▌   | 508/780 [03:52<01:20,  3.39it/s] 65%|██████▌   | 509/780 [03:52<01:19,  3.40it/s] 65%|██████▌   | 510/780 [03:53<01:19,  3.40it/s] 66%|██████▌   | 511/780 [03:53<01:19,  3.40it/s] 66%|██████▌   | 512/780 [03:53<01:18,  3.40it/s] 66%|██████▌   | 513/780 [03:54<01:20,  3.31it/s] 66%|██████▌   | 514/780 [03:54<01:19,  3.34it/s] 66%|██████▌   | 515/780 [03:54<01:18,  3.36it/s] 66%|██████▌   | 516/780 [03:55<01:18,  3.37it/s] 66%|██████▋   | 517/780 [03:55<01:17,  3.38it/s] 66%|██████▋   | 518/780 [03:55<01:17,  3.39it/s] 67%|██████▋   | 519/780 [03:55<01:16,  3.39it/s] 67%|██████▋   | 520/780 [03:56<01:16,  3.39it/s] 67%|██████▋   | 521/780 [03:56<01:16,  3.40it/s] 67%|██████▋   | 522/780 [03:56<01:15,  3.40it/s] 67%|██████▋   | 523/780 [03:57<01:15,  3.40it/s] 67%|██████▋   | 524/780 [03:57<01:16,  3.36it/s] 67%|██████▋   | 525/780 [03:57<01:15,  3.37it/s] 67%|██████▋   | 526/780 [03:57<01:15,  3.38it/s] 68%|██████▊   | 527/780 [03:58<01:14,  3.39it/s] 68%|██████▊   | 528/780 [03:58<01:14,  3.39it/s] 68%|██████▊   | 529/780 [03:58<01:13,  3.40it/s] 68%|██████▊   | 530/780 [03:59<01:13,  3.40it/s] 68%|██████▊   | 531/780 [03:59<01:13,  3.40it/s] 68%|██████▊   | 532/780 [03:59<01:12,  3.40it/s] 68%|██████▊   | 533/780 [04:00<01:12,  3.40it/s] 68%|██████▊   | 534/780 [04:00<01:12,  3.40it/s] 69%|██████▊   | 535/780 [04:00<01:12,  3.40it/s] 69%|██████▊   | 536/780 [04:00<01:11,  3.40it/s] 69%|██████▉   | 537/780 [04:01<01:11,  3.40it/s] 69%|██████▉   | 538/780 [04:01<01:11,  3.40it/s] 69%|██████▉   | 539/780 [04:01<01:12,  3.31it/s] 69%|██████▉   | 540/780 [04:02<01:11,  3.34it/s] 69%|██████▉   | 541/780 [04:02<01:11,  3.36it/s] 69%|██████▉   | 542/780 [04:02<01:10,  3.37it/s] 70%|██████▉   | 543/780 [04:03<01:10,  3.38it/s] 70%|██████▉   | 544/780 [04:03<01:09,  3.39it/s] 70%|██████▉   | 545/780 [04:03<01:09,  3.39it/s] 70%|███████   | 546/780 [04:03<01:08,  3.39it/s] 70%|███████   | 547/780 [04:04<01:08,  3.40it/s] 70%|███████   | 548/780 [04:04<01:08,  3.40it/s] 70%|███████   | 549/780 [04:04<01:07,  3.40it/s] 71%|███████   | 550/780 [04:05<01:09,  3.32it/s] 71%|███████   | 551/780 [04:05<01:08,  3.35it/s] 71%|███████   | 552/780 [04:05<01:07,  3.36it/s] 71%|███████   | 553/780 [04:05<01:07,  3.37it/s] 71%|███████   | 554/780 [04:06<01:06,  3.39it/s] 71%|███████   | 555/780 [04:06<01:06,  3.39it/s] 71%|███████▏  | 556/780 [04:06<01:06,  3.39it/s] 71%|███████▏  | 557/780 [04:07<01:05,  3.39it/s] 72%|███████▏  | 558/780 [04:07<01:05,  3.39it/s] 72%|███████▏  | 559/780 [04:07<01:05,  3.39it/s] 72%|███████▏  | 560/780 [04:08<01:04,  3.39it/s] 72%|███████▏  | 561/780 [04:08<01:05,  3.36it/s] 72%|███████▏  | 562/780 [04:08<01:04,  3.37it/s] 72%|███████▏  | 563/780 [04:08<01:04,  3.38it/s] 72%|███████▏  | 564/780 [04:09<01:03,  3.39it/s] 72%|███████▏  | 565/780 [04:09<01:03,  3.39it/s] 73%|███████▎  | 566/780 [04:09<01:03,  3.39it/s] 73%|███████▎  | 567/780 [04:10<01:02,  3.40it/s] 73%|███████▎  | 568/780 [04:10<01:02,  3.40it/s] 73%|███████▎  | 569/780 [04:10<01:02,  3.40it/s] 73%|███████▎  | 570/780 [04:10<01:01,  3.40it/s] 73%|███████▎  | 571/780 [04:11<01:01,  3.40it/s] 73%|███████▎  | 572/780 [04:11<01:02,  3.33it/s] 73%|███████▎  | 573/780 [04:11<01:01,  3.35it/s] 74%|███████▎  | 574/780 [04:12<01:01,  3.36it/s] 74%|███████▎  | 575/780 [04:12<01:00,  3.37it/s] 74%|███████▍  | 576/780 [04:12<01:00,  3.38it/s] 74%|███████▍  | 577/780 [04:13<00:59,  3.39it/s] 74%|███████▍  | 578/780 [04:13<00:59,  3.39it/s] 74%|███████▍  | 579/780 [04:13<00:59,  3.39it/s] 74%|███████▍  | 580/780 [04:13<00:58,  3.39it/s] 74%|███████▍  | 581/780 [04:14<00:58,  3.39it/s] 75%|███████▍  | 582/780 [04:14<00:58,  3.40it/s] 75%|███████▍  | 583/780 [04:14<01:02,  3.17it/s] 75%|███████▍  | 584/780 [04:15<01:00,  3.24it/s] 75%|███████▌  | 585/780 [04:15<00:59,  3.28it/s] 75%|███████▌  | 586/780 [04:15<00:58,  3.32it/s] 75%|███████▌  | 587/780 [04:16<00:57,  3.34it/s] 75%|███████▌  | 588/780 [04:16<00:57,  3.36it/s] 76%|███████▌  | 589/780 [04:16<00:56,  3.37it/s] 76%|███████▌  | 590/780 [04:16<00:56,  3.38it/s] 76%|███████▌  | 591/780 [04:17<00:55,  3.39it/s] 76%|███████▌  | 592/780 [04:17<00:55,  3.39it/s] 76%|███████▌  | 593/780 [04:17<00:56,  3.28it/s] 76%|███████▌  | 594/780 [04:18<00:56,  3.32it/s] 76%|███████▋  | 595/780 [04:18<00:55,  3.34it/s] 76%|███████▋  | 596/780 [04:18<00:54,  3.36it/s] 77%|███████▋  | 597/780 [04:19<00:54,  3.37it/s] 77%|███████▋  | 598/780 [04:19<00:53,  3.38it/s] 77%|███████▋  | 599/780 [04:19<00:53,  3.38it/s] 77%|███████▋  | 600/780 [04:19<00:53,  3.39it/s] 77%|███████▋  | 601/780 [04:20<00:52,  3.40it/s] 77%|███████▋  | 602/780 [04:20<00:52,  3.40it/s] 77%|███████▋  | 603/780 [04:20<00:52,  3.40it/s] 77%|███████▋  | 604/780 [04:21<00:55,  3.20it/s] 78%|███████▊  | 605/780 [04:21<00:53,  3.25it/s] 78%|███████▊  | 606/780 [04:21<00:52,  3.30it/s] 78%|███████▊  | 607/780 [04:22<00:51,  3.33it/s] 78%|███████▊  | 608/780 [04:22<00:51,  3.35it/s] 78%|███████▊  | 609/780 [04:22<00:50,  3.36it/s] 78%|███████▊  | 610/780 [04:22<00:50,  3.38it/s] 78%|███████▊  | 611/780 [04:23<00:49,  3.38it/s] 78%|███████▊  | 612/780 [04:23<00:49,  3.39it/s] 79%|███████▊  | 613/780 [04:23<00:49,  3.39it/s] 79%|███████▊  | 614/780 [04:24<00:52,  3.13it/s] 79%|███████▉  | 615/780 [04:24<00:51,  3.21it/s] 79%|███████▉  | 616/780 [04:24<00:50,  3.26it/s] 79%|███████▉  | 617/780 [04:25<00:49,  3.31it/s] 79%|███████▉  | 618/780 [04:25<00:48,  3.33it/s] 79%|███████▉  | 619/780 [04:25<00:48,  3.35it/s] 79%|███████▉  | 620/780 [04:25<00:47,  3.37it/s] 80%|███████▉  | 621/780 [04:26<00:47,  3.38it/s] 80%|███████▉  | 622/780 [04:26<00:46,  3.38it/s] 80%|███████▉  | 623/780 [04:26<00:46,  3.38it/s] 80%|████████  | 624/780 [04:27<00:47,  3.32it/s][INFO|trainer.py:2140] 2023-08-29 07:07:58,436 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:07:58,436 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 07:07:58,436 >>   Batch size = 8
{'eval_loss': 0.9880287647247314, 'eval_runtime': 13.7571, 'eval_samples_per_second': 353.563, 'eval_steps_per_second': 44.195, 'epoch': 3.0}
{'loss': 0.5227, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 55.99it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.06it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.20it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.66it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.95it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.38it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.11it/s][A
  7%|▋         | 42/608 [00:00<00:12, 44.89it/s][A
  8%|▊         | 47/608 [00:01<00:12, 44.96it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.04it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.18it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.31it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.33it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.20it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.06it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.87it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.72it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.94it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 44.99it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.05it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 45.20it/s][A
 18%|█▊        | 112/608 [00:02<00:10, 45.28it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.20it/s][A
 20%|██        | 122/608 [00:02<00:10, 44.99it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.86it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.81it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.89it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.91it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.06it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.13it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.32it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.21it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 44.97it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 44.93it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.83it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.90it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.89it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.01it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.16it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.27it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.16it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.07it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.98it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.84it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.92it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 43.79it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 44.38it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.61it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.80it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.89it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.90it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.89it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.80it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.72it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.83it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.99it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.07it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 45.08it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.33it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.19it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.08it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 44.96it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.93it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.85it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.93it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.88it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 45.04it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.17it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.18it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.11it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 44.94it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.81it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.47it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.69it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.89it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.99it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.18it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 45.14it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.08it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.94it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.94it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.95it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.99it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.14it/s][A
 70%|███████   | 427/608 [00:09<00:04, 45.02it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.21it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.17it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.19it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.06it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.99it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.99it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.02it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.09it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.14it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.11it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.18it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.19it/s][A
 81%|████████  | 492/608 [00:10<00:02, 45.12it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.95it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 44.94it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 43.57it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.19it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.40it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.70it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.71it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 44.87it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 44.92it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.90it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.79it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.86it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.01it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.13it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.21it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.24it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.20it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 45.16it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.03it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.75it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.90it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 44.89it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.15it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.15it/s][A 80%|████████  | 624/780 [04:40<00:47,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:08:12,210 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 07:08:12,423 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:08:14,993 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:08:15,103 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:08:15,160 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:49<17:48,  6.89s/it] 80%|████████  | 626/780 [04:49<12:37,  4.92s/it] 80%|████████  | 627/780 [04:50<09:00,  3.53s/it] 81%|████████  | 628/780 [04:50<06:28,  2.56s/it] 81%|████████  | 629/780 [04:50<04:43,  1.88s/it] 81%|████████  | 630/780 [04:50<03:30,  1.40s/it] 81%|████████  | 631/780 [04:51<02:39,  1.07s/it] 81%|████████  | 632/780 [04:51<02:03,  1.19it/s] 81%|████████  | 633/780 [04:51<01:39,  1.48it/s] 81%|████████▏ | 634/780 [04:52<01:21,  1.79it/s] 81%|████████▏ | 635/780 [04:52<01:09,  2.08it/s] 82%|████████▏ | 636/780 [04:52<01:00,  2.36it/s] 82%|████████▏ | 637/780 [04:52<00:55,  2.59it/s] 82%|████████▏ | 638/780 [04:53<00:50,  2.79it/s] 82%|████████▏ | 639/780 [04:53<00:47,  2.95it/s] 82%|████████▏ | 640/780 [04:53<00:45,  3.08it/s] 82%|████████▏ | 641/780 [04:54<00:43,  3.17it/s] 82%|████████▏ | 642/780 [04:54<00:42,  3.24it/s] 82%|████████▏ | 643/780 [04:54<00:41,  3.29it/s] 83%|████████▎ | 644/780 [04:55<00:40,  3.32it/s] 83%|████████▎ | 645/780 [04:55<00:40,  3.35it/s] 83%|████████▎ | 646/780 [04:55<00:39,  3.37it/s] 83%|████████▎ | 647/780 [04:55<00:39,  3.38it/s] 83%|████████▎ | 648/780 [04:56<00:39,  3.35it/s] 83%|████████▎ | 649/780 [04:56<00:38,  3.37it/s] 83%|████████▎ | 650/780 [04:56<00:38,  3.38it/s] 83%|████████▎ | 651/780 [04:57<00:38,  3.39it/s] 84%|████████▎ | 652/780 [04:57<00:37,  3.40it/s] 84%|████████▎ | 653/780 [04:57<00:37,  3.40it/s] 84%|████████▍ | 654/780 [04:57<00:37,  3.40it/s] 84%|████████▍ | 655/780 [04:58<00:36,  3.41it/s] 84%|████████▍ | 656/780 [04:58<00:36,  3.41it/s] 84%|████████▍ | 657/780 [04:58<00:36,  3.41it/s] 84%|████████▍ | 658/780 [04:59<00:35,  3.41it/s] 84%|████████▍ | 659/780 [04:59<00:35,  3.39it/s] 85%|████████▍ | 660/780 [04:59<00:35,  3.39it/s] 85%|████████▍ | 661/780 [05:00<00:35,  3.40it/s] 85%|████████▍ | 662/780 [05:00<00:34,  3.40it/s] 85%|████████▌ | 663/780 [05:00<00:34,  3.41it/s] 85%|████████▌ | 664/780 [05:00<00:34,  3.41it/s] 85%|████████▌ | 665/780 [05:01<00:33,  3.41it/s] 85%|████████▌ | 666/780 [05:01<00:33,  3.41it/s] 86%|████████▌ | 667/780 [05:01<00:33,  3.41it/s] 86%|████████▌ | 668/780 [05:02<00:32,  3.41it/s] 86%|████████▌ | 669/780 [05:02<00:32,  3.41it/s] 86%|████████▌ | 670/780 [05:02<00:32,  3.41it/s] 86%|████████▌ | 671/780 [05:02<00:31,  3.41it/s] 86%|████████▌ | 672/780 [05:03<00:32,  3.35it/s] 86%|████████▋ | 673/780 [05:03<00:31,  3.36it/s] 86%|████████▋ | 674/780 [05:03<00:31,  3.38it/s] 87%|████████▋ | 675/780 [05:04<00:31,  3.38it/s] 87%|████████▋ | 676/780 [05:04<00:30,  3.39it/s] 87%|████████▋ | 677/780 [05:04<00:30,  3.40it/s] 87%|████████▋ | 678/780 [05:05<00:29,  3.40it/s] 87%|████████▋ | 679/780 [05:05<00:29,  3.40it/s] 87%|████████▋ | 680/780 [05:05<00:29,  3.41it/s] 87%|████████▋ | 681/780 [05:05<00:29,  3.41it/s] 87%|████████▋ | 682/780 [05:06<00:28,  3.41it/s] 88%|████████▊ | 683/780 [05:06<00:28,  3.39it/s] 88%|████████▊ | 684/780 [05:06<00:28,  3.40it/s] 88%|████████▊ | 685/780 [05:07<00:27,  3.40it/s] 88%|████████▊ | 686/780 [05:07<00:27,  3.40it/s] 88%|████████▊ | 687/780 [05:07<00:27,  3.41it/s] 88%|████████▊ | 688/780 [05:07<00:26,  3.41it/s] 88%|████████▊ | 689/780 [05:08<00:26,  3.41it/s] 88%|████████▊ | 690/780 [05:08<00:26,  3.41it/s] 89%|████████▊ | 691/780 [05:08<00:26,  3.41it/s] 89%|████████▊ | 692/780 [05:09<00:25,  3.41it/s] 89%|████████▉ | 693/780 [05:09<00:25,  3.41it/s] 89%|████████▉ | 694/780 [05:09<00:25,  3.40it/s] 89%|████████▉ | 695/780 [05:10<00:24,  3.41it/s] 89%|████████▉ | 696/780 [05:10<00:24,  3.41it/s] 89%|████████▉ | 697/780 [05:10<00:24,  3.42it/s] 89%|████████▉ | 698/780 [05:10<00:23,  3.43it/s] 90%|████████▉ | 699/780 [05:11<00:23,  3.44it/s] 90%|████████▉ | 700/780 [05:11<00:23,  3.45it/s] 90%|████████▉ | 701/780 [05:11<00:22,  3.45it/s] 90%|█████████ | 702/780 [05:12<00:22,  3.46it/s] 90%|█████████ | 703/780 [05:12<00:22,  3.46it/s] 90%|█████████ | 704/780 [05:12<00:21,  3.46it/s] 90%|█████████ | 705/780 [05:12<00:21,  3.45it/s] 91%|█████████ | 706/780 [05:13<00:21,  3.46it/s] 91%|█████████ | 707/780 [05:13<00:21,  3.46it/s] 91%|█████████ | 708/780 [05:13<00:20,  3.46it/s] 91%|█████████ | 709/780 [05:14<00:20,  3.46it/s] 91%|█████████ | 710/780 [05:14<00:20,  3.46it/s] 91%|█████████ | 711/780 [05:14<00:19,  3.46it/s] 91%|█████████▏| 712/780 [05:14<00:19,  3.46it/s] 91%|█████████▏| 713/780 [05:15<00:19,  3.47it/s] 92%|█████████▏| 714/780 [05:15<00:19,  3.46it/s] 92%|█████████▏| 715/780 [05:15<00:18,  3.46it/s] 92%|█████████▏| 716/780 [05:16<00:19,  3.21it/s] 92%|█████████▏| 717/780 [05:16<00:19,  3.28it/s] 92%|█████████▏| 718/780 [05:16<00:18,  3.33it/s] 92%|█████████▏| 719/780 [05:17<00:18,  3.37it/s] 92%|█████████▏| 720/780 [05:17<00:17,  3.40it/s] 92%|█████████▏| 721/780 [05:17<00:17,  3.41it/s] 93%|█████████▎| 722/780 [05:17<00:16,  3.43it/s] 93%|█████████▎| 723/780 [05:18<00:16,  3.44it/s] 93%|█████████▎| 724/780 [05:18<00:16,  3.44it/s] 93%|█████████▎| 725/780 [05:18<00:15,  3.45it/s] 93%|█████████▎| 726/780 [05:19<00:15,  3.45it/s] 93%|█████████▎| 727/780 [05:19<00:16,  3.30it/s] 93%|█████████▎| 728/780 [05:19<00:15,  3.35it/s] 93%|█████████▎| 729/780 [05:19<00:15,  3.38it/s] 94%|█████████▎| 730/780 [05:20<00:14,  3.41it/s] 94%|█████████▎| 731/780 [05:20<00:14,  3.42it/s] 94%|█████████▍| 732/780 [05:20<00:13,  3.43it/s] 94%|█████████▍| 733/780 [05:21<00:13,  3.44it/s] 94%|█████████▍| 734/780 [05:21<00:13,  3.45it/s] 94%|█████████▍| 735/780 [05:21<00:13,  3.45it/s] 94%|█████████▍| 736/780 [05:21<00:12,  3.46it/s] 94%|█████████▍| 737/780 [05:22<00:12,  3.46it/s] 95%|█████████▍| 738/780 [05:22<00:12,  3.45it/s] 95%|█████████▍| 739/780 [05:22<00:11,  3.45it/s] 95%|█████████▍| 740/780 [05:23<00:11,  3.45it/s] 95%|█████████▌| 741/780 [05:23<00:11,  3.45it/s] 95%|█████████▌| 742/780 [05:23<00:10,  3.46it/s] 95%|█████████▌| 743/780 [05:24<00:10,  3.46it/s] 95%|█████████▌| 744/780 [05:24<00:10,  3.46it/s] 96%|█████████▌| 745/780 [05:24<00:10,  3.45it/s] 96%|█████████▌| 746/780 [05:24<00:09,  3.46it/s] 96%|█████████▌| 747/780 [05:25<00:09,  3.46it/s] 96%|█████████▌| 748/780 [05:25<00:09,  3.46it/s] 96%|█████████▌| 749/780 [05:25<00:08,  3.45it/s] 96%|█████████▌| 750/780 [05:26<00:08,  3.45it/s] 96%|█████████▋| 751/780 [05:26<00:08,  3.45it/s] 96%|█████████▋| 752/780 [05:26<00:08,  3.46it/s] 97%|█████████▋| 753/780 [05:26<00:07,  3.46it/s] 97%|█████████▋| 754/780 [05:27<00:07,  3.46it/s] 97%|█████████▋| 755/780 [05:27<00:07,  3.46it/s] 97%|█████████▋| 756/780 [05:27<00:06,  3.46it/s] 97%|█████████▋| 757/780 [05:28<00:06,  3.46it/s] 97%|█████████▋| 758/780 [05:28<00:06,  3.46it/s] 97%|█████████▋| 759/780 [05:28<00:06,  3.46it/s] 97%|█████████▋| 760/780 [05:28<00:05,  3.45it/s] 98%|█████████▊| 761/780 [05:29<00:05,  3.45it/s] 98%|█████████▊| 762/780 [05:29<00:05,  3.45it/s] 98%|█████████▊| 763/780 [05:29<00:04,  3.46it/s] 98%|█████████▊| 764/780 [05:30<00:04,  3.46it/s] 98%|█████████▊| 765/780 [05:30<00:04,  3.46it/s] 98%|█████████▊| 766/780 [05:30<00:04,  3.46it/s] 98%|█████████▊| 767/780 [05:30<00:03,  3.46it/s] 98%|█████████▊| 768/780 [05:31<00:03,  3.46it/s] 99%|█████████▊| 769/780 [05:31<00:03,  3.46it/s] 99%|█████████▊| 770/780 [05:31<00:02,  3.46it/s] 99%|█████████▉| 771/780 [05:32<00:02,  3.46it/s] 99%|█████████▉| 772/780 [05:32<00:02,  3.46it/s] 99%|█████████▉| 773/780 [05:32<00:02,  3.46it/s] 99%|█████████▉| 774/780 [05:32<00:01,  3.46it/s] 99%|█████████▉| 775/780 [05:33<00:01,  3.46it/s] 99%|█████████▉| 776/780 [05:33<00:01,  3.46it/s]100%|█████████▉| 777/780 [05:33<00:00,  3.45it/s]100%|█████████▉| 778/780 [05:34<00:00,  3.38it/s]100%|█████████▉| 779/780 [05:34<00:00,  3.40it/s]100%|██████████| 780/780 [05:34<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 07:09:05,966 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:09:05,966 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 07:09:05,966 >>   Batch size = 8
{'eval_loss': 0.9933059811592102, 'eval_runtime': 13.5506, 'eval_samples_per_second': 358.952, 'eval_steps_per_second': 44.869, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.77it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.53it/s][A
  3%|▎         | 18/608 [00:00<00:12, 47.47it/s][A
  4%|▍         | 23/608 [00:00<00:12, 46.60it/s][A
  5%|▍         | 28/608 [00:00<00:12, 46.07it/s][A
  5%|▌         | 33/608 [00:00<00:12, 45.57it/s][A
  6%|▋         | 38/608 [00:00<00:12, 45.48it/s][A
  7%|▋         | 43/608 [00:00<00:12, 45.30it/s][A
  8%|▊         | 48/608 [00:01<00:12, 45.21it/s][A
  9%|▊         | 53/608 [00:01<00:12, 45.20it/s][A
 10%|▉         | 58/608 [00:01<00:12, 45.40it/s][A
 10%|█         | 63/608 [00:01<00:11, 45.43it/s][A
 11%|█         | 68/608 [00:01<00:11, 45.40it/s][A
 12%|█▏        | 73/608 [00:01<00:11, 45.27it/s][A
 13%|█▎        | 78/608 [00:01<00:11, 45.20it/s][A
 14%|█▎        | 83/608 [00:01<00:11, 45.10it/s][A
 14%|█▍        | 88/608 [00:01<00:11, 45.14it/s][A
 15%|█▌        | 93/608 [00:02<00:11, 45.13it/s][A
 16%|█▌        | 98/608 [00:02<00:11, 45.22it/s][A
 17%|█▋        | 103/608 [00:02<00:11, 44.94it/s][A
 18%|█▊        | 108/608 [00:02<00:11, 45.14it/s][A
 19%|█▊        | 113/608 [00:02<00:10, 45.23it/s][A
 19%|█▉        | 118/608 [00:02<00:10, 45.11it/s][A
 20%|██        | 123/608 [00:02<00:10, 45.08it/s][A
 21%|██        | 128/608 [00:02<00:10, 45.08it/s][A
 22%|██▏       | 133/608 [00:02<00:10, 45.11it/s][A
 23%|██▎       | 138/608 [00:03<00:10, 45.10it/s][A
 24%|██▎       | 143/608 [00:03<00:10, 45.20it/s][A
 24%|██▍       | 148/608 [00:03<00:10, 45.11it/s][A
 25%|██▌       | 153/608 [00:03<00:10, 45.17it/s][A
 26%|██▌       | 158/608 [00:03<00:09, 45.12it/s][A
 27%|██▋       | 163/608 [00:03<00:09, 45.09it/s][A
 28%|██▊       | 168/608 [00:03<00:09, 45.15it/s][A
 28%|██▊       | 173/608 [00:03<00:09, 44.71it/s][A
 29%|██▉       | 178/608 [00:03<00:09, 44.91it/s][A
 30%|███       | 183/608 [00:04<00:09, 45.05it/s][A
 31%|███       | 188/608 [00:04<00:09, 45.06it/s][A
 32%|███▏      | 193/608 [00:04<00:09, 45.15it/s][A
 33%|███▎      | 198/608 [00:04<00:09, 45.26it/s][A
 33%|███▎      | 203/608 [00:04<00:08, 45.19it/s][A
 34%|███▍      | 208/608 [00:04<00:08, 45.20it/s][A
 35%|███▌      | 213/608 [00:04<00:08, 45.14it/s][A
 36%|███▌      | 218/608 [00:04<00:08, 45.02it/s][A
 37%|███▋      | 223/608 [00:04<00:08, 45.12it/s][A
 38%|███▊      | 228/608 [00:05<00:08, 45.00it/s][A
 38%|███▊      | 233/608 [00:05<00:08, 45.08it/s][A
 39%|███▉      | 238/608 [00:05<00:08, 45.21it/s][A
 40%|███▉      | 243/608 [00:05<00:08, 45.25it/s][A
 41%|████      | 248/608 [00:05<00:07, 45.09it/s][A
 42%|████▏     | 253/608 [00:05<00:07, 45.14it/s][A
 42%|████▏     | 258/608 [00:05<00:07, 45.19it/s][A
 43%|████▎     | 263/608 [00:05<00:07, 45.14it/s][A
 44%|████▍     | 268/608 [00:05<00:07, 45.00it/s][A
 45%|████▍     | 273/608 [00:06<00:07, 45.09it/s][A
 46%|████▌     | 278/608 [00:06<00:07, 45.01it/s][A
 47%|████▋     | 283/608 [00:06<00:07, 45.24it/s][A
 47%|████▋     | 288/608 [00:06<00:07, 45.23it/s][A
 48%|████▊     | 293/608 [00:06<00:06, 45.18it/s][A
 49%|████▉     | 298/608 [00:06<00:06, 45.19it/s][A
 50%|████▉     | 303/608 [00:06<00:06, 45.20it/s][A
 51%|█████     | 308/608 [00:06<00:06, 45.18it/s][A
 51%|█████▏    | 313/608 [00:06<00:06, 44.94it/s][A
 52%|█████▏    | 318/608 [00:07<00:06, 45.05it/s][A
 53%|█████▎    | 323/608 [00:07<00:06, 44.98it/s][A
 54%|█████▍    | 328/608 [00:07<00:06, 45.06it/s][A
 55%|█████▍    | 333/608 [00:07<00:06, 45.17it/s][A
 56%|█████▌    | 338/608 [00:07<00:05, 45.23it/s][A
 56%|█████▋    | 343/608 [00:07<00:05, 45.27it/s][A
 57%|█████▋    | 348/608 [00:07<00:05, 45.37it/s][A
 58%|█████▊    | 353/608 [00:07<00:05, 45.14it/s][A
 59%|█████▉    | 358/608 [00:07<00:05, 45.03it/s][A
 60%|█████▉    | 363/608 [00:08<00:05, 44.99it/s][A
 61%|██████    | 368/608 [00:08<00:05, 45.12it/s][A
 61%|██████▏   | 373/608 [00:08<00:05, 45.26it/s][A
 62%|██████▏   | 378/608 [00:08<00:05, 45.24it/s][A
 63%|██████▎   | 383/608 [00:08<00:05, 44.38it/s][A
 64%|██████▍   | 388/608 [00:08<00:04, 44.78it/s][A
 65%|██████▍   | 393/608 [00:08<00:04, 44.92it/s][A
 65%|██████▌   | 398/608 [00:08<00:04, 44.87it/s][A
 66%|██████▋   | 403/608 [00:08<00:04, 44.70it/s][A
 67%|██████▋   | 408/608 [00:09<00:04, 44.86it/s][A
 68%|██████▊   | 413/608 [00:09<00:04, 45.01it/s][A
 69%|██████▉   | 418/608 [00:09<00:04, 45.14it/s][A
 70%|██████▉   | 423/608 [00:09<00:04, 45.07it/s][A
 70%|███████   | 428/608 [00:09<00:03, 45.13it/s][A
 71%|███████   | 433/608 [00:09<00:03, 45.24it/s][A
 72%|███████▏  | 438/608 [00:09<00:03, 45.32it/s][A
 73%|███████▎  | 443/608 [00:09<00:03, 45.11it/s][A
 74%|███████▎  | 448/608 [00:09<00:03, 45.14it/s][A
 75%|███████▍  | 453/608 [00:10<00:03, 44.96it/s][A
 75%|███████▌  | 458/608 [00:10<00:03, 45.12it/s][A
 76%|███████▌  | 463/608 [00:10<00:03, 45.20it/s][A
 77%|███████▋  | 468/608 [00:10<00:03, 45.16it/s][A
 78%|███████▊  | 473/608 [00:10<00:02, 45.23it/s][A
 79%|███████▊  | 478/608 [00:10<00:02, 45.27it/s][A
 79%|███████▉  | 483/608 [00:10<00:02, 45.27it/s][A
 80%|████████  | 488/608 [00:10<00:02, 45.11it/s][A
 81%|████████  | 493/608 [00:10<00:02, 45.18it/s][A
 82%|████████▏ | 498/608 [00:11<00:02, 45.13it/s][A
 83%|████████▎ | 503/608 [00:11<00:02, 45.18it/s][A
 84%|████████▎ | 508/608 [00:11<00:02, 45.21it/s][A
 84%|████████▍ | 513/608 [00:11<00:02, 45.23it/s][A
 85%|████████▌ | 518/608 [00:11<00:01, 45.27it/s][A
 86%|████████▌ | 523/608 [00:11<00:01, 44.72it/s][A
 87%|████████▋ | 528/608 [00:11<00:01, 44.85it/s][A
 88%|████████▊ | 533/608 [00:11<00:01, 44.82it/s][A
 88%|████████▊ | 538/608 [00:11<00:01, 44.89it/s][A
 89%|████████▉ | 543/608 [00:12<00:01, 45.00it/s][A
 90%|█████████ | 548/608 [00:12<00:01, 45.09it/s][A
 91%|█████████ | 553/608 [00:12<00:01, 45.12it/s][A
 92%|█████████▏| 558/608 [00:12<00:01, 45.09it/s][A
 93%|█████████▎| 563/608 [00:12<00:00, 45.07it/s][A
 93%|█████████▎| 568/608 [00:12<00:00, 45.17it/s][A
 94%|█████████▍| 573/608 [00:12<00:00, 45.18it/s][A
 95%|█████████▌| 578/608 [00:12<00:00, 45.14it/s][A
 96%|█████████▌| 583/608 [00:12<00:00, 45.25it/s][A
 97%|█████████▋| 588/608 [00:13<00:00, 45.18it/s][A
 98%|█████████▊| 593/608 [00:13<00:00, 45.23it/s][A
 98%|█████████▊| 598/608 [00:13<00:00, 45.14it/s][A
 99%|█████████▉| 603/608 [00:13<00:00, 45.15it/s][A
100%|██████████| 608/608 [00:13<00:00, 45.17it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.17it/s][A100%|██████████| 780/780 [05:48<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:09:19,556 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 07:09:19,656 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:09:22,014 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:09:22,172 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:09:22,233 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 07:09:27,799 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 07:09:27,816 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156 (score: 0.9668400287628174).
                                                 100%|██████████| 780/780 [06:03<00:00,  3.42it/s]100%|██████████| 780/780 [06:03<00:00,  2.15it/s]
[INFO|trainer.py:1894] 2023-08-29 07:09:34,850 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 07:09:34,944 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:09:37,475 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:09:37,588 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:09:37,644 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:09:38,087 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:38,088 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:38,088 >>   train_loss               =     0.5133
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:38,088 >>   train_runtime            = 0:06:03.60
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:38,088 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:38,088 >>   train_samples_per_second =      137.5
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:38,088 >>   train_steps_per_second   =      2.145
{'eval_loss': 1.0001124143600464, 'eval_runtime': 13.4766, 'eval_samples_per_second': 360.921, 'eval_steps_per_second': 45.115, 'epoch': 5.0}
{'train_runtime': 363.601, 'train_samples_per_second': 137.5, 'train_steps_per_second': 2.145, 'train_loss': 0.5133418743426983, 'epoch': 5.0}
08/29/2023 07:09:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 07:09:38,377 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:09:38,377 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 07:09:38,377 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 56.21it/s]  2%|▏         | 12/608 [00:00<00:11, 49.73it/s]  3%|▎         | 18/608 [00:00<00:12, 47.95it/s]  4%|▍         | 23/608 [00:00<00:12, 47.36it/s]  5%|▍         | 28/608 [00:00<00:12, 46.69it/s]  5%|▌         | 33/608 [00:00<00:12, 46.37it/s]  6%|▋         | 38/608 [00:00<00:12, 46.26it/s]  7%|▋         | 43/608 [00:00<00:12, 45.84it/s]  8%|▊         | 48/608 [00:01<00:12, 45.26it/s]  9%|▊         | 53/608 [00:01<00:12, 45.03it/s] 10%|▉         | 58/608 [00:01<00:12, 45.19it/s] 10%|█         | 63/608 [00:01<00:12, 45.16it/s] 11%|█         | 68/608 [00:01<00:11, 45.31it/s] 12%|█▏        | 73/608 [00:01<00:11, 45.41it/s] 13%|█▎        | 78/608 [00:01<00:11, 45.61it/s] 14%|█▎        | 83/608 [00:01<00:11, 45.80it/s] 14%|█▍        | 88/608 [00:01<00:11, 45.53it/s] 15%|█▌        | 93/608 [00:02<00:11, 45.22it/s] 16%|█▌        | 98/608 [00:02<00:11, 45.03it/s] 17%|█▋        | 103/608 [00:02<00:11, 44.71it/s] 18%|█▊        | 108/608 [00:02<00:11, 44.89it/s] 19%|█▊        | 113/608 [00:02<00:10, 45.11it/s] 19%|█▉        | 118/608 [00:02<00:10, 45.24it/s] 20%|██        | 123/608 [00:02<00:10, 45.50it/s] 21%|██        | 128/608 [00:02<00:10, 45.62it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.50it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.22it/s] 24%|██▎       | 143/608 [00:03<00:10, 45.12it/s] 24%|██▍       | 148/608 [00:03<00:10, 44.98it/s] 25%|██▌       | 153/608 [00:03<00:10, 45.22it/s] 26%|██▌       | 158/608 [00:03<00:09, 45.31it/s] 27%|██▋       | 163/608 [00:03<00:09, 45.33it/s] 28%|██▊       | 168/608 [00:03<00:09, 45.47it/s] 28%|██▊       | 173/608 [00:03<00:09, 45.57it/s] 29%|██▉       | 178/608 [00:03<00:09, 45.54it/s] 30%|███       | 183/608 [00:04<00:09, 45.35it/s] 31%|███       | 188/608 [00:04<00:09, 45.18it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.18it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.19it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.31it/s] 34%|███▍      | 208/608 [00:04<00:08, 45.28it/s] 35%|███▌      | 213/608 [00:04<00:08, 45.37it/s] 36%|███▌      | 218/608 [00:04<00:08, 45.48it/s] 37%|███▋      | 223/608 [00:04<00:08, 45.46it/s] 38%|███▊      | 228/608 [00:05<00:08, 45.39it/s] 38%|███▊      | 233/608 [00:05<00:08, 45.26it/s] 39%|███▉      | 238/608 [00:05<00:08, 45.15it/s] 40%|███▉      | 243/608 [00:05<00:08, 43.95it/s] 41%|████      | 248/608 [00:05<00:08, 44.50it/s] 42%|████▏     | 253/608 [00:05<00:07, 44.85it/s] 42%|████▏     | 258/608 [00:05<00:07, 45.04it/s] 43%|████▎     | 263/608 [00:05<00:07, 45.14it/s] 44%|████▍     | 268/608 [00:05<00:07, 45.11it/s] 45%|████▍     | 273/608 [00:06<00:07, 45.03it/s] 46%|████▌     | 278/608 [00:06<00:07, 45.00it/s] 47%|████▋     | 283/608 [00:06<00:07, 44.97it/s] 47%|████▋     | 288/608 [00:06<00:07, 45.08it/s] 48%|████▊     | 293/608 [00:06<00:06, 45.19it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.27it/s] 50%|████▉     | 303/608 [00:06<00:06, 45.14it/s] 51%|█████     | 308/608 [00:06<00:06, 45.57it/s] 51%|█████▏    | 313/608 [00:06<00:06, 45.57it/s] 52%|█████▏    | 318/608 [00:06<00:06, 45.45it/s] 53%|█████▎    | 323/608 [00:07<00:06, 45.26it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.20it/s] 55%|█████▍    | 333/608 [00:07<00:06, 45.08it/s] 56%|█████▌    | 338/608 [00:07<00:05, 45.19it/s] 56%|█████▋    | 343/608 [00:07<00:05, 45.41it/s] 57%|█████▋    | 348/608 [00:07<00:05, 45.41it/s] 58%|█████▊    | 353/608 [00:07<00:05, 45.48it/s] 59%|█████▉    | 358/608 [00:07<00:05, 45.46it/s] 60%|█████▉    | 363/608 [00:07<00:05, 45.41it/s] 61%|██████    | 368/608 [00:08<00:05, 45.24it/s] 61%|██████▏   | 373/608 [00:08<00:05, 45.17it/s] 62%|██████▏   | 378/608 [00:08<00:05, 45.13it/s] 63%|██████▎   | 383/608 [00:08<00:05, 43.41it/s] 64%|██████▍   | 388/608 [00:08<00:05, 43.99it/s] 65%|██████▍   | 393/608 [00:08<00:04, 44.60it/s] 65%|██████▌   | 398/608 [00:08<00:04, 44.91it/s] 66%|██████▋   | 403/608 [00:08<00:04, 45.05it/s] 67%|██████▋   | 408/608 [00:08<00:04, 45.15it/s] 68%|██████▊   | 413/608 [00:09<00:04, 45.06it/s] 69%|██████▉   | 418/608 [00:09<00:04, 44.99it/s] 70%|██████▉   | 423/608 [00:09<00:04, 44.94it/s] 70%|███████   | 428/608 [00:09<00:04, 45.00it/s] 71%|███████   | 433/608 [00:09<00:03, 45.22it/s] 72%|███████▏  | 438/608 [00:09<00:03, 45.31it/s] 73%|███████▎  | 443/608 [00:09<00:03, 45.40it/s] 74%|███████▎  | 448/608 [00:09<00:03, 45.53it/s] 75%|███████▍  | 453/608 [00:09<00:03, 45.43it/s] 75%|███████▌  | 458/608 [00:10<00:03, 45.34it/s] 76%|███████▌  | 463/608 [00:10<00:03, 45.17it/s] 77%|███████▋  | 468/608 [00:10<00:03, 45.15it/s] 78%|███████▊  | 473/608 [00:10<00:02, 45.18it/s] 79%|███████▊  | 478/608 [00:10<00:02, 45.28it/s] 79%|███████▉  | 483/608 [00:10<00:02, 45.31it/s] 80%|████████  | 488/608 [00:10<00:02, 45.50it/s] 81%|████████  | 493/608 [00:10<00:02, 45.54it/s] 82%|████████▏ | 498/608 [00:10<00:02, 45.47it/s] 83%|████████▎ | 503/608 [00:11<00:02, 45.38it/s] 84%|████████▎ | 508/608 [00:11<00:02, 45.16it/s] 84%|████████▍ | 513/608 [00:11<00:02, 45.18it/s] 85%|████████▌ | 518/608 [00:11<00:01, 45.20it/s] 86%|████████▌ | 523/608 [00:11<00:01, 42.94it/s] 87%|████████▋ | 528/608 [00:11<00:01, 43.77it/s] 88%|████████▊ | 533/608 [00:11<00:01, 44.46it/s] 88%|████████▊ | 538/608 [00:11<00:01, 44.77it/s] 89%|████████▉ | 543/608 [00:11<00:01, 44.78it/s] 90%|█████████ | 548/608 [00:12<00:01, 44.89it/s] 91%|█████████ | 553/608 [00:12<00:01, 44.81it/s] 92%|█████████▏| 558/608 [00:12<00:01, 44.78it/s] 93%|█████████▎| 563/608 [00:12<00:01, 44.76it/s] 93%|█████████▎| 568/608 [00:12<00:00, 44.86it/s] 94%|█████████▍| 573/608 [00:12<00:00, 44.93it/s] 95%|█████████▌| 578/608 [00:12<00:00, 45.30it/s] 96%|█████████▌| 583/608 [00:12<00:00, 45.49it/s] 97%|█████████▋| 588/608 [00:12<00:00, 45.38it/s] 98%|█████████▊| 593/608 [00:13<00:00, 45.29it/s] 98%|█████████▊| 598/608 [00:13<00:00, 45.15it/s] 99%|█████████▉| 603/608 [00:13<00:00, 45.12it/s]100%|██████████| 608/608 [00:13<00:00, 45.03it/s]100%|██████████| 608/608 [00:13<00:00, 45.20it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:09:51,849 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:51,849 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:51,849 >>   eval_loss               =     0.9668
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:51,849 >>   eval_runtime            = 0:00:13.47
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:51,849 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:51,849 >>   eval_samples_per_second =    361.046
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:51,849 >>   eval_steps_per_second   =     45.131
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:09:51,849 >>   perplexity              =     2.6296
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:06,493 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:06,518 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:06,518 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:06,518 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:06,518 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:10:06,976 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:10:06,977 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:10:07,583 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:10:08,660 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:10:08,660 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:11,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:11,679 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:11,679 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:11,679 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:10:11,679 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:10:12,443 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:10:12,444 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:10:13,053 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:10:13,253 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:10:13,253 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.74it/s]Extractor Predicting: 5it [00:02,  1.75it/s]Extractor Predicting: 6it [00:03,  1.79it/s]Extractor Predicting: 7it [00:04,  1.74it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.76it/s]Extractor Predicting: 11it [00:06,  1.75it/s]Extractor Predicting: 12it [00:06,  1.79it/s]Extractor Predicting: 13it [00:07,  1.78it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.79it/s]Extractor Predicting: 16it [00:09,  1.78it/s]Extractor Predicting: 17it [00:09,  1.76it/s]Extractor Predicting: 18it [00:10,  1.75it/s]Extractor Predicting: 19it [00:10,  1.71it/s]Extractor Predicting: 20it [00:11,  1.77it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:12,  1.65it/s]Extractor Predicting: 23it [00:13,  1.61it/s]Extractor Predicting: 24it [00:14,  1.56it/s]Extractor Predicting: 25it [00:14,  1.52it/s]Extractor Predicting: 26it [00:15,  1.53it/s]Extractor Predicting: 27it [00:16,  1.56it/s]Extractor Predicting: 28it [00:16,  1.56it/s]Extractor Predicting: 29it [00:17,  1.58it/s]Extractor Predicting: 30it [00:17,  1.61it/s]Extractor Predicting: 31it [00:18,  1.63it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:19,  1.66it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:20,  1.63it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:22,  1.62it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.61it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:24,  1.63it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:25,  1.60it/s]Extractor Predicting: 44it [00:26,  1.65it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:27,  1.62it/s]Extractor Predicting: 47it [00:28,  1.63it/s]Extractor Predicting: 48it [00:28,  1.64it/s]Extractor Predicting: 49it [00:29,  1.62it/s]Extractor Predicting: 50it [00:30,  1.63it/s]Extractor Predicting: 51it [00:30,  1.59it/s]Extractor Predicting: 52it [00:31,  1.60it/s]Extractor Predicting: 53it [00:32,  1.59it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:33,  1.68it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:35,  1.63it/s]Extractor Predicting: 59it [00:35,  1.64it/s]Extractor Predicting: 60it [00:36,  1.63it/s]Extractor Predicting: 61it [00:36,  1.56it/s]Extractor Predicting: 62it [00:37,  1.58it/s]Extractor Predicting: 63it [00:38,  1.59it/s]Extractor Predicting: 64it [00:38,  1.62it/s]Extractor Predicting: 65it [00:39,  1.62it/s]Extractor Predicting: 66it [00:40,  1.62it/s]Extractor Predicting: 67it [00:40,  1.64it/s]Extractor Predicting: 68it [00:41,  1.60it/s]Extractor Predicting: 69it [00:41,  1.61it/s]Extractor Predicting: 70it [00:42,  1.64it/s]Extractor Predicting: 71it [00:43,  1.68it/s]Extractor Predicting: 72it [00:43,  1.67it/s]Extractor Predicting: 73it [00:44,  1.69it/s]Extractor Predicting: 74it [00:44,  1.67it/s]Extractor Predicting: 75it [00:45,  1.68it/s]Extractor Predicting: 76it [00:46,  1.67it/s]Extractor Predicting: 77it [00:46,  1.65it/s]Extractor Predicting: 78it [00:47,  1.68it/s]Extractor Predicting: 79it [00:47,  1.68it/s]Extractor Predicting: 80it [00:48,  1.70it/s]Extractor Predicting: 81it [00:49,  1.67it/s]Extractor Predicting: 82it [00:49,  1.65it/s]Extractor Predicting: 83it [00:50,  1.64it/s]Extractor Predicting: 84it [00:50,  1.66it/s]Extractor Predicting: 85it [00:51,  1.65it/s]Extractor Predicting: 86it [00:52,  1.63it/s]Extractor Predicting: 87it [00:52,  1.65it/s]Extractor Predicting: 88it [00:53,  1.64it/s]Extractor Predicting: 89it [00:54,  1.57it/s]Extractor Predicting: 90it [00:54,  1.62it/s]Extractor Predicting: 91it [00:55,  1.64it/s]Extractor Predicting: 92it [00:55,  1.65it/s]Extractor Predicting: 93it [00:56,  1.51it/s]Extractor Predicting: 94it [00:57,  1.54it/s]Extractor Predicting: 95it [00:57,  1.52it/s]Extractor Predicting: 96it [00:58,  1.54it/s]Extractor Predicting: 97it [00:59,  1.57it/s]Extractor Predicting: 98it [00:59,  1.60it/s]Extractor Predicting: 99it [01:00,  1.63it/s]Extractor Predicting: 100it [01:00,  1.62it/s]Extractor Predicting: 101it [01:01,  1.62it/s]Extractor Predicting: 102it [01:02,  1.64it/s]Extractor Predicting: 103it [01:02,  1.67it/s]Extractor Predicting: 104it [01:03,  1.66it/s]Extractor Predicting: 105it [01:03,  1.63it/s]Extractor Predicting: 106it [01:04,  1.63it/s]Extractor Predicting: 107it [01:05,  1.63it/s]Extractor Predicting: 108it [01:05,  1.66it/s]Extractor Predicting: 109it [01:06,  1.61it/s]Extractor Predicting: 110it [01:07,  1.58it/s]Extractor Predicting: 111it [01:07,  1.59it/s]Extractor Predicting: 112it [01:08,  1.60it/s]Extractor Predicting: 113it [01:08,  1.62it/s]Extractor Predicting: 114it [01:09,  1.65it/s]Extractor Predicting: 115it [01:10,  1.70it/s]Extractor Predicting: 116it [01:10,  1.71it/s]Extractor Predicting: 117it [01:11,  1.68it/s]Extractor Predicting: 118it [01:11,  1.71it/s]Extractor Predicting: 119it [01:12,  1.69it/s]Extractor Predicting: 120it [01:12,  1.72it/s]Extractor Predicting: 121it [01:13,  1.68it/s]Extractor Predicting: 122it [01:14,  1.63it/s]Extractor Predicting: 123it [01:14,  1.58it/s]Extractor Predicting: 124it [01:15,  1.55it/s]Extractor Predicting: 125it [01:16,  1.57it/s]Extractor Predicting: 126it [01:16,  1.60it/s]Extractor Predicting: 127it [01:17,  1.61it/s]Extractor Predicting: 128it [01:18,  1.55it/s]Extractor Predicting: 129it [01:18,  1.58it/s]Extractor Predicting: 130it [01:19,  1.60it/s]Extractor Predicting: 131it [01:19,  1.66it/s]Extractor Predicting: 132it [01:20,  1.61it/s]Extractor Predicting: 133it [01:21,  1.60it/s]Extractor Predicting: 134it [01:21,  1.62it/s]Extractor Predicting: 135it [01:22,  1.65it/s]Extractor Predicting: 136it [01:22,  1.64it/s]Extractor Predicting: 137it [01:23,  1.65it/s]Extractor Predicting: 138it [01:24,  1.63it/s]Extractor Predicting: 139it [01:24,  1.64it/s]Extractor Predicting: 140it [01:25,  1.62it/s]Extractor Predicting: 141it [01:26,  1.65it/s]Extractor Predicting: 142it [01:26,  1.62it/s]Extractor Predicting: 143it [01:27,  1.67it/s]Extractor Predicting: 144it [01:27,  1.68it/s]Extractor Predicting: 145it [01:28,  1.68it/s]Extractor Predicting: 146it [01:29,  1.62it/s]Extractor Predicting: 147it [01:29,  1.59it/s]Extractor Predicting: 148it [01:30,  1.60it/s]Extractor Predicting: 149it [01:30,  1.60it/s]Extractor Predicting: 150it [01:31,  1.62it/s]Extractor Predicting: 151it [01:32,  1.61it/s]Extractor Predicting: 152it [01:32,  1.64it/s]Extractor Predicting: 153it [01:33,  1.62it/s]Extractor Predicting: 154it [01:34,  1.62it/s]Extractor Predicting: 155it [01:34,  1.64it/s]Extractor Predicting: 156it [01:35,  1.67it/s]Extractor Predicting: 157it [01:35,  1.67it/s]Extractor Predicting: 158it [01:36,  1.66it/s]Extractor Predicting: 159it [01:37,  1.64it/s]Extractor Predicting: 160it [01:37,  1.66it/s]Extractor Predicting: 161it [01:38,  1.63it/s]Extractor Predicting: 162it [01:38,  1.61it/s]Extractor Predicting: 163it [01:39,  1.61it/s]Extractor Predicting: 164it [01:40,  1.61it/s]Extractor Predicting: 165it [01:40,  1.64it/s]Extractor Predicting: 166it [01:41,  1.61it/s]Extractor Predicting: 167it [01:42,  1.60it/s]Extractor Predicting: 168it [01:42,  1.54it/s]Extractor Predicting: 169it [01:43,  1.52it/s]Extractor Predicting: 170it [01:44,  1.55it/s]Extractor Predicting: 171it [01:44,  1.54it/s]Extractor Predicting: 172it [01:45,  1.57it/s]Extractor Predicting: 173it [01:46,  1.50it/s]Extractor Predicting: 174it [01:46,  1.49it/s]Extractor Predicting: 175it [01:47,  1.55it/s]Extractor Predicting: 176it [01:47,  1.54it/s]Extractor Predicting: 177it [01:48,  1.52it/s]Extractor Predicting: 178it [01:49,  1.50it/s]Extractor Predicting: 179it [01:50,  1.48it/s]Extractor Predicting: 180it [01:50,  1.49it/s]Extractor Predicting: 181it [01:51,  1.51it/s]Extractor Predicting: 182it [01:51,  1.51it/s]Extractor Predicting: 183it [01:52,  1.37it/s]Extractor Predicting: 184it [01:53,  1.39it/s]Extractor Predicting: 185it [01:54,  1.45it/s]Extractor Predicting: 186it [01:54,  1.48it/s]Extractor Predicting: 187it [01:55,  1.59it/s]Extractor Predicting: 187it [01:55,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:22,744 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:22,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:22,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:22,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:22,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:12:23,529 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:12:23,530 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:12:24,132 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:12:25,231 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:12:25,231 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:28,271 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:28,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:28,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:28,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:12:28,294 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:12:29,025 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:12:29,026 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:12:29,627 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:12:29,824 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:12:29,824 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5809128630705395,
  "recall": 0.028782894736842105,
  "score": 0.05484818805093046,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.61it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.54it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:22,  1.64it/s]Extractor Predicting: 37it [00:23,  1.60it/s]Extractor Predicting: 38it [00:23,  1.58it/s]Extractor Predicting: 39it [00:24,  1.62it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:25,  1.65it/s]Extractor Predicting: 42it [00:26,  1.61it/s]Extractor Predicting: 43it [00:26,  1.65it/s]Extractor Predicting: 44it [00:27,  1.63it/s]Extractor Predicting: 45it [00:27,  1.66it/s]Extractor Predicting: 46it [00:28,  1.65it/s]Extractor Predicting: 47it [00:29,  1.54it/s]Extractor Predicting: 48it [00:29,  1.56it/s]Extractor Predicting: 49it [00:30,  1.55it/s]Extractor Predicting: 50it [00:31,  1.58it/s]Extractor Predicting: 51it [00:31,  1.58it/s]Extractor Predicting: 52it [00:32,  1.57it/s]Extractor Predicting: 53it [00:32,  1.63it/s]Extractor Predicting: 54it [00:33,  1.64it/s]Extractor Predicting: 55it [00:34,  1.65it/s]Extractor Predicting: 56it [00:34,  1.66it/s]Extractor Predicting: 57it [00:35,  1.63it/s]Extractor Predicting: 58it [00:35,  1.65it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:37,  1.67it/s]Extractor Predicting: 61it [00:37,  1.61it/s]Extractor Predicting: 62it [00:38,  1.62it/s]Extractor Predicting: 63it [00:39,  1.63it/s]Extractor Predicting: 64it [00:39,  1.46it/s]Extractor Predicting: 65it [00:40,  1.51it/s]Extractor Predicting: 66it [00:41,  1.55it/s]Extractor Predicting: 67it [00:41,  1.59it/s]Extractor Predicting: 68it [00:42,  1.51it/s]Extractor Predicting: 69it [00:43,  1.56it/s]Extractor Predicting: 70it [00:43,  1.53it/s]Extractor Predicting: 71it [00:44,  1.56it/s]Extractor Predicting: 72it [00:44,  1.57it/s]Extractor Predicting: 73it [00:45,  1.63it/s]Extractor Predicting: 74it [00:46,  1.64it/s]Extractor Predicting: 75it [00:46,  1.65it/s]Extractor Predicting: 76it [00:47,  1.61it/s]Extractor Predicting: 77it [00:47,  1.62it/s]Extractor Predicting: 78it [00:48,  1.67it/s]Extractor Predicting: 79it [00:49,  1.65it/s]Extractor Predicting: 80it [00:49,  1.67it/s]Extractor Predicting: 81it [00:50,  1.68it/s]Extractor Predicting: 82it [00:50,  1.67it/s]Extractor Predicting: 83it [00:51,  1.66it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:52,  1.66it/s]Extractor Predicting: 86it [00:53,  1.68it/s]Extractor Predicting: 87it [00:53,  1.72it/s]Extractor Predicting: 88it [00:54,  1.75it/s]Extractor Predicting: 89it [00:55,  1.71it/s]Extractor Predicting: 90it [00:55,  1.67it/s]Extractor Predicting: 91it [00:56,  1.72it/s]Extractor Predicting: 92it [00:56,  1.74it/s]Extractor Predicting: 93it [00:57,  1.72it/s]Extractor Predicting: 94it [00:57,  1.72it/s]Extractor Predicting: 95it [00:58,  1.71it/s]Extractor Predicting: 96it [00:59,  1.73it/s]Extractor Predicting: 97it [00:59,  1.71it/s]Extractor Predicting: 98it [01:00,  1.69it/s]Extractor Predicting: 99it [01:00,  1.67it/s]Extractor Predicting: 100it [01:01,  1.71it/s]Extractor Predicting: 101it [01:02,  1.68it/s]Extractor Predicting: 102it [01:02,  1.69it/s]Extractor Predicting: 103it [01:03,  1.66it/s]Extractor Predicting: 104it [01:03,  1.67it/s]Extractor Predicting: 105it [01:04,  1.65it/s]Extractor Predicting: 106it [01:05,  1.62it/s]Extractor Predicting: 107it [01:05,  1.64it/s]Extractor Predicting: 108it [01:06,  1.65it/s]Extractor Predicting: 109it [01:06,  1.62it/s]Extractor Predicting: 110it [01:07,  1.62it/s]Extractor Predicting: 111it [01:08,  1.61it/s]Extractor Predicting: 112it [01:08,  1.63it/s]Extractor Predicting: 113it [01:09,  1.65it/s]Extractor Predicting: 114it [01:10,  1.66it/s]Extractor Predicting: 115it [01:10,  1.64it/s]Extractor Predicting: 116it [01:11,  1.64it/s]Extractor Predicting: 117it [01:11,  1.63it/s]Extractor Predicting: 118it [01:12,  1.63it/s]Extractor Predicting: 119it [01:13,  1.65it/s]Extractor Predicting: 120it [01:13,  1.63it/s]Extractor Predicting: 121it [01:14,  1.65it/s]Extractor Predicting: 122it [01:14,  1.65it/s]Extractor Predicting: 123it [01:15,  1.66it/s]Extractor Predicting: 124it [01:16,  1.67it/s]Extractor Predicting: 125it [01:16,  1.65it/s]Extractor Predicting: 126it [01:17,  1.65it/s]Extractor Predicting: 127it [01:17,  1.65it/s]Extractor Predicting: 128it [01:18,  1.62it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:19,  1.65it/s]Extractor Predicting: 131it [01:20,  1.65it/s]Extractor Predicting: 132it [01:21,  1.62it/s]Extractor Predicting: 133it [01:21,  1.65it/s]Extractor Predicting: 134it [01:22,  1.64it/s]Extractor Predicting: 135it [01:22,  1.65it/s]Extractor Predicting: 136it [01:23,  1.66it/s]Extractor Predicting: 137it [01:24,  1.61it/s]Extractor Predicting: 138it [01:24,  1.61it/s]Extractor Predicting: 139it [01:25,  1.64it/s]Extractor Predicting: 140it [01:25,  1.63it/s]Extractor Predicting: 141it [01:26,  1.63it/s]Extractor Predicting: 142it [01:27,  1.60it/s]Extractor Predicting: 143it [01:27,  1.61it/s]Extractor Predicting: 144it [01:28,  1.62it/s]Extractor Predicting: 145it [01:29,  1.61it/s]Extractor Predicting: 146it [01:29,  1.62it/s]Extractor Predicting: 147it [01:30,  1.56it/s]Extractor Predicting: 148it [01:30,  1.60it/s]Extractor Predicting: 149it [01:31,  1.60it/s]Extractor Predicting: 150it [01:32,  1.57it/s]Extractor Predicting: 151it [01:32,  1.58it/s]Extractor Predicting: 152it [01:33,  1.56it/s]Extractor Predicting: 153it [01:34,  1.62it/s]Extractor Predicting: 154it [01:34,  1.64it/s]Extractor Predicting: 155it [01:35,  1.67it/s]Extractor Predicting: 156it [01:35,  1.64it/s]Extractor Predicting: 157it [01:36,  1.65it/s]Extractor Predicting: 158it [01:37,  1.64it/s]Extractor Predicting: 159it [01:37,  1.63it/s]Extractor Predicting: 160it [01:38,  1.62it/s]Extractor Predicting: 161it [01:38,  1.63it/s]Extractor Predicting: 162it [01:39,  1.62it/s]Extractor Predicting: 163it [01:40,  1.64it/s]Extractor Predicting: 164it [01:40,  1.63it/s]Extractor Predicting: 165it [01:41,  1.65it/s]Extractor Predicting: 166it [01:41,  1.65it/s]Extractor Predicting: 167it [01:42,  1.65it/s]Extractor Predicting: 168it [01:43,  1.63it/s]Extractor Predicting: 169it [01:43,  1.62it/s]Extractor Predicting: 170it [01:44,  1.66it/s]Extractor Predicting: 171it [01:44,  1.71it/s]Extractor Predicting: 172it [01:45,  1.70it/s]Extractor Predicting: 173it [01:46,  1.65it/s]Extractor Predicting: 174it [01:46,  1.62it/s]Extractor Predicting: 175it [01:47,  1.38it/s]Extractor Predicting: 176it [01:48,  1.42it/s]Extractor Predicting: 177it [01:49,  1.48it/s]Extractor Predicting: 178it [01:49,  1.48it/s]Extractor Predicting: 179it [01:50,  1.51it/s]Extractor Predicting: 180it [01:51,  1.51it/s]Extractor Predicting: 181it [01:51,  1.52it/s]Extractor Predicting: 182it [01:52,  1.54it/s]Extractor Predicting: 183it [01:52,  1.52it/s]Extractor Predicting: 184it [01:53,  1.52it/s]Extractor Predicting: 185it [01:54,  1.53it/s]Extractor Predicting: 186it [01:54,  1.55it/s]Extractor Predicting: 187it [01:55,  1.54it/s]Extractor Predicting: 188it [01:56,  1.55it/s]Extractor Predicting: 189it [01:56,  1.57it/s]Extractor Predicting: 190it [01:57,  1.56it/s]Extractor Predicting: 191it [01:58,  1.58it/s]Extractor Predicting: 192it [01:58,  1.58it/s]Extractor Predicting: 193it [01:59,  1.57it/s]Extractor Predicting: 194it [01:59,  1.59it/s]Extractor Predicting: 195it [02:00,  1.64it/s]Extractor Predicting: 196it [02:01,  1.66it/s]Extractor Predicting: 197it [02:01,  1.65it/s]Extractor Predicting: 198it [02:02,  1.64it/s]Extractor Predicting: 199it [02:02,  1.64it/s]Extractor Predicting: 200it [02:03,  1.65it/s]Extractor Predicting: 201it [02:04,  1.64it/s]Extractor Predicting: 202it [02:04,  1.64it/s]Extractor Predicting: 203it [02:05,  1.63it/s]Extractor Predicting: 204it [02:06,  1.63it/s]Extractor Predicting: 205it [02:06,  1.63it/s]Extractor Predicting: 206it [02:07,  1.65it/s]Extractor Predicting: 207it [02:07,  1.63it/s]Extractor Predicting: 208it [02:08,  1.67it/s]Extractor Predicting: 209it [02:09,  1.68it/s]Extractor Predicting: 210it [02:09,  1.66it/s]Extractor Predicting: 211it [02:10,  1.66it/s]Extractor Predicting: 212it [02:10,  1.67it/s]Extractor Predicting: 213it [02:11,  1.64it/s]Extractor Predicting: 214it [02:12,  1.61it/s]Extractor Predicting: 215it [02:12,  1.61it/s]Extractor Predicting: 216it [02:13,  1.61it/s]Extractor Predicting: 217it [02:13,  1.65it/s]Extractor Predicting: 218it [02:14,  1.62it/s]Extractor Predicting: 219it [02:15,  1.61it/s]Extractor Predicting: 220it [02:15,  1.66it/s]Extractor Predicting: 221it [02:16,  1.60it/s]Extractor Predicting: 222it [02:17,  1.60it/s]Extractor Predicting: 223it [02:17,  1.60it/s]Extractor Predicting: 224it [02:18,  1.64it/s]Extractor Predicting: 225it [02:18,  1.67it/s]Extractor Predicting: 226it [02:19,  1.66it/s]Extractor Predicting: 227it [02:20,  1.67it/s]Extractor Predicting: 228it [02:20,  1.68it/s]Extractor Predicting: 229it [02:21,  1.66it/s]Extractor Predicting: 230it [02:21,  1.70it/s]Extractor Predicting: 231it [02:22,  1.69it/s]Extractor Predicting: 232it [02:23,  1.67it/s]Extractor Predicting: 233it [02:23,  1.67it/s]Extractor Predicting: 234it [02:24,  1.63it/s]Extractor Predicting: 235it [02:24,  1.63it/s]Extractor Predicting: 236it [02:25,  1.64it/s]Extractor Predicting: 237it [02:26,  1.60it/s]Extractor Predicting: 238it [02:26,  1.62it/s]Extractor Predicting: 239it [02:27,  1.62it/s]Extractor Predicting: 240it [02:27,  1.65it/s]Extractor Predicting: 241it [02:28,  1.68it/s]Extractor Predicting: 242it [02:29,  1.63it/s]Extractor Predicting: 243it [02:29,  1.68it/s]Extractor Predicting: 244it [02:30,  1.69it/s]Extractor Predicting: 245it [02:30,  1.66it/s]Extractor Predicting: 246it [02:31,  1.67it/s]Extractor Predicting: 247it [02:32,  1.65it/s]Extractor Predicting: 248it [02:32,  1.67it/s]Extractor Predicting: 249it [02:33,  1.67it/s]Extractor Predicting: 250it [02:33,  1.71it/s]Extractor Predicting: 251it [02:34,  1.67it/s]Extractor Predicting: 252it [02:35,  1.69it/s]Extractor Predicting: 253it [02:35,  1.73it/s]Extractor Predicting: 254it [02:36,  1.68it/s]Extractor Predicting: 255it [02:36,  1.68it/s]Extractor Predicting: 256it [02:37,  1.68it/s]Extractor Predicting: 257it [02:38,  1.67it/s]Extractor Predicting: 258it [02:38,  1.68it/s]Extractor Predicting: 259it [02:39,  1.68it/s]Extractor Predicting: 260it [02:39,  1.67it/s]Extractor Predicting: 261it [02:40,  1.65it/s]Extractor Predicting: 262it [02:41,  1.65it/s]Extractor Predicting: 263it [02:41,  1.59it/s]Extractor Predicting: 264it [02:42,  1.63it/s]Extractor Predicting: 265it [02:42,  1.63it/s]Extractor Predicting: 266it [02:43,  1.61it/s]Extractor Predicting: 267it [02:44,  1.59it/s]Extractor Predicting: 268it [02:44,  1.60it/s]Extractor Predicting: 269it [02:45,  1.58it/s]Extractor Predicting: 270it [02:46,  1.60it/s]Extractor Predicting: 271it [02:46,  1.61it/s]Extractor Predicting: 272it [02:47,  1.64it/s]Extractor Predicting: 273it [02:47,  1.63it/s]Extractor Predicting: 274it [02:48,  1.61it/s]Extractor Predicting: 275it [02:49,  1.61it/s]Extractor Predicting: 276it [02:49,  1.64it/s]Extractor Predicting: 277it [02:50,  1.61it/s]Extractor Predicting: 278it [02:51,  1.62it/s]Extractor Predicting: 279it [02:51,  1.61it/s]Extractor Predicting: 280it [02:52,  1.62it/s]Extractor Predicting: 281it [02:52,  1.63it/s]Extractor Predicting: 282it [02:53,  1.61it/s]Extractor Predicting: 283it [02:54,  1.62it/s]Extractor Predicting: 284it [02:54,  1.64it/s]Extractor Predicting: 285it [02:55,  1.66it/s]Extractor Predicting: 286it [02:55,  1.65it/s]Extractor Predicting: 287it [02:56,  1.62it/s]Extractor Predicting: 288it [02:57,  1.62it/s]Extractor Predicting: 289it [02:57,  1.60it/s]Extractor Predicting: 290it [02:58,  1.59it/s]Extractor Predicting: 291it [02:59,  1.60it/s]Extractor Predicting: 292it [02:59,  1.59it/s]Extractor Predicting: 293it [03:00,  1.59it/s]Extractor Predicting: 294it [03:01,  1.56it/s]Extractor Predicting: 295it [03:01,  1.58it/s]Extractor Predicting: 296it [03:02,  1.59it/s]Extractor Predicting: 297it [03:02,  1.57it/s]Extractor Predicting: 298it [03:03,  1.57it/s]Extractor Predicting: 299it [03:04,  1.57it/s]Extractor Predicting: 300it [03:04,  1.53it/s]Extractor Predicting: 301it [03:05,  1.48it/s]Extractor Predicting: 302it [03:06,  1.29it/s]Extractor Predicting: 303it [03:07,  1.36it/s]Extractor Predicting: 304it [03:08,  1.36it/s]Extractor Predicting: 305it [03:08,  1.37it/s]Extractor Predicting: 306it [03:09,  1.44it/s]Extractor Predicting: 307it [03:09,  1.50it/s]Extractor Predicting: 308it [03:10,  1.54it/s]Extractor Predicting: 309it [03:11,  1.60it/s]Extractor Predicting: 310it [03:11,  1.58it/s]Extractor Predicting: 311it [03:12,  1.54it/s]Extractor Predicting: 312it [03:13,  1.57it/s]Extractor Predicting: 313it [03:13,  1.58it/s]Extractor Predicting: 314it [03:14,  1.57it/s]Extractor Predicting: 315it [03:14,  1.63it/s]Extractor Predicting: 316it [03:15,  1.65it/s]Extractor Predicting: 317it [03:16,  1.63it/s]Extractor Predicting: 318it [03:16,  1.63it/s]Extractor Predicting: 319it [03:17,  1.61it/s]Extractor Predicting: 320it [03:17,  1.62it/s]Extractor Predicting: 321it [03:18,  1.64it/s]Extractor Predicting: 322it [03:19,  1.67it/s]Extractor Predicting: 323it [03:19,  1.67it/s]Extractor Predicting: 324it [03:20,  1.68it/s]Extractor Predicting: 325it [03:20,  1.66it/s]Extractor Predicting: 326it [03:21,  1.67it/s]Extractor Predicting: 327it [03:22,  1.66it/s]Extractor Predicting: 328it [03:22,  1.65it/s]Extractor Predicting: 329it [03:23,  1.64it/s]Extractor Predicting: 330it [03:24,  1.63it/s]Extractor Predicting: 331it [03:24,  1.59it/s]Extractor Predicting: 332it [03:25,  1.62it/s]Extractor Predicting: 333it [03:25,  1.61it/s]Extractor Predicting: 334it [03:26,  1.62it/s]Extractor Predicting: 335it [03:27,  1.62it/s]Extractor Predicting: 336it [03:27,  1.61it/s]Extractor Predicting: 337it [03:28,  1.62it/s]Extractor Predicting: 338it [03:28,  1.64it/s]Extractor Predicting: 339it [03:29,  1.64it/s]Extractor Predicting: 340it [03:30,  1.66it/s]Extractor Predicting: 341it [03:30,  1.64it/s]Extractor Predicting: 342it [03:31,  1.63it/s]Extractor Predicting: 343it [03:32,  1.62it/s]Extractor Predicting: 344it [03:32,  1.67it/s]Extractor Predicting: 345it [03:33,  1.70it/s]Extractor Predicting: 346it [03:33,  1.73it/s]Extractor Predicting: 347it [03:34,  1.73it/s]Extractor Predicting: 348it [03:34,  1.71it/s]Extractor Predicting: 349it [03:35,  1.73it/s]Extractor Predicting: 350it [03:36,  1.72it/s]Extractor Predicting: 351it [03:36,  1.72it/s]Extractor Predicting: 352it [03:37,  1.72it/s]Extractor Predicting: 353it [03:37,  1.69it/s]Extractor Predicting: 354it [03:38,  1.70it/s]Extractor Predicting: 355it [03:38,  1.71it/s]Extractor Predicting: 356it [03:39,  1.74it/s]Extractor Predicting: 357it [03:40,  1.72it/s]Extractor Predicting: 358it [03:40,  1.71it/s]Extractor Predicting: 359it [03:41,  1.71it/s]Extractor Predicting: 360it [03:41,  1.70it/s]Extractor Predicting: 361it [03:42,  1.69it/s]Extractor Predicting: 362it [03:43,  1.70it/s]Extractor Predicting: 363it [03:43,  1.73it/s]Extractor Predicting: 364it [03:44,  1.76it/s]Extractor Predicting: 365it [03:44,  1.70it/s]Extractor Predicting: 366it [03:45,  1.66it/s]Extractor Predicting: 367it [03:46,  1.60it/s]Extractor Predicting: 368it [03:46,  1.58it/s]Extractor Predicting: 369it [03:47,  1.64it/s]Extractor Predicting: 370it [03:47,  1.60it/s]Extractor Predicting: 371it [03:48,  1.63it/s]Extractor Predicting: 372it [03:49,  1.63it/s]Extractor Predicting: 373it [03:49,  1.65it/s]Extractor Predicting: 374it [03:50,  1.65it/s]Extractor Predicting: 375it [03:50,  1.63it/s]Extractor Predicting: 376it [03:51,  1.62it/s]Extractor Predicting: 377it [03:52,  1.60it/s]Extractor Predicting: 378it [03:52,  1.60it/s]Extractor Predicting: 379it [03:53,  1.60it/s]Extractor Predicting: 380it [03:54,  1.61it/s]Extractor Predicting: 381it [03:54,  1.64it/s]Extractor Predicting: 382it [03:55,  1.65it/s]Extractor Predicting: 383it [03:55,  1.66it/s]Extractor Predicting: 384it [03:56,  1.68it/s]Extractor Predicting: 385it [03:57,  1.66it/s]Extractor Predicting: 386it [03:57,  1.68it/s]Extractor Predicting: 387it [03:58,  1.68it/s]Extractor Predicting: 388it [03:58,  1.70it/s]Extractor Predicting: 389it [03:59,  1.65it/s]Extractor Predicting: 390it [04:00,  1.63it/s]Extractor Predicting: 391it [04:00,  1.64it/s]Extractor Predicting: 392it [04:01,  1.71it/s]Extractor Predicting: 393it [04:01,  1.73it/s]Extractor Predicting: 394it [04:02,  1.71it/s]Extractor Predicting: 395it [04:02,  1.73it/s]Extractor Predicting: 396it [04:03,  1.76it/s]Extractor Predicting: 397it [04:04,  1.73it/s]Extractor Predicting: 398it [04:04,  1.72it/s]Extractor Predicting: 399it [04:05,  1.72it/s]Extractor Predicting: 400it [04:05,  1.68it/s]Extractor Predicting: 401it [04:06,  1.70it/s]Extractor Predicting: 402it [04:07,  1.73it/s]Extractor Predicting: 403it [04:07,  1.72it/s]Extractor Predicting: 404it [04:08,  1.73it/s]Extractor Predicting: 405it [04:08,  1.69it/s]Extractor Predicting: 406it [04:09,  1.69it/s]Extractor Predicting: 407it [04:09,  1.70it/s]Extractor Predicting: 408it [04:10,  1.71it/s]Extractor Predicting: 409it [04:11,  1.55it/s]Extractor Predicting: 410it [04:11,  1.58it/s]Extractor Predicting: 411it [04:12,  1.58it/s]Extractor Predicting: 412it [04:13,  1.60it/s]Extractor Predicting: 413it [04:13,  1.56it/s]Extractor Predicting: 414it [04:14,  1.53it/s]Extractor Predicting: 415it [04:15,  1.49it/s]Extractor Predicting: 416it [04:15,  1.50it/s]Extractor Predicting: 417it [04:16,  1.51it/s]Extractor Predicting: 418it [04:17,  1.54it/s]Extractor Predicting: 419it [04:17,  1.56it/s]Extractor Predicting: 420it [04:18,  1.54it/s]Extractor Predicting: 421it [04:19,  1.54it/s]Extractor Predicting: 422it [04:19,  1.53it/s]Extractor Predicting: 423it [04:20,  1.57it/s]Extractor Predicting: 424it [04:21,  1.56it/s]Extractor Predicting: 425it [04:21,  1.55it/s]Extractor Predicting: 426it [04:22,  1.54it/s]Extractor Predicting: 427it [04:23,  1.36it/s]Extractor Predicting: 428it [04:23,  1.42it/s]Extractor Predicting: 429it [04:24,  1.48it/s]Extractor Predicting: 430it [04:25,  1.52it/s]Extractor Predicting: 431it [04:25,  1.55it/s]Extractor Predicting: 432it [04:26,  1.53it/s]Extractor Predicting: 433it [04:27,  1.54it/s]Extractor Predicting: 434it [04:27,  1.55it/s]Extractor Predicting: 435it [04:28,  1.58it/s]Extractor Predicting: 436it [04:28,  1.62it/s]Extractor Predicting: 437it [04:29,  1.58it/s]Extractor Predicting: 438it [04:30,  1.60it/s]Extractor Predicting: 439it [04:30,  1.60it/s]Extractor Predicting: 440it [04:31,  1.60it/s]Extractor Predicting: 441it [04:32,  1.59it/s]Extractor Predicting: 442it [04:32,  1.58it/s]Extractor Predicting: 443it [04:33,  1.56it/s]Extractor Predicting: 444it [04:34,  1.57it/s]Extractor Predicting: 445it [04:34,  1.56it/s]Extractor Predicting: 446it [04:35,  1.57it/s]Extractor Predicting: 447it [04:35,  1.55it/s]Extractor Predicting: 448it [04:36,  1.55it/s]Extractor Predicting: 449it [04:37,  1.59it/s]Extractor Predicting: 450it [04:37,  1.62it/s]Extractor Predicting: 451it [04:38,  1.60it/s]Extractor Predicting: 452it [04:39,  1.58it/s]Extractor Predicting: 453it [04:39,  1.59it/s]Extractor Predicting: 454it [04:40,  1.59it/s]Extractor Predicting: 455it [04:40,  1.57it/s]Extractor Predicting: 456it [04:41,  1.59it/s]Extractor Predicting: 457it [04:42,  1.56it/s]Extractor Predicting: 458it [04:42,  1.58it/s]Extractor Predicting: 459it [04:43,  1.56it/s]Extractor Predicting: 460it [04:44,  1.56it/s]Extractor Predicting: 461it [04:44,  1.58it/s]Extractor Predicting: 462it [04:45,  1.59it/s]Extractor Predicting: 463it [04:46,  1.55it/s]Extractor Predicting: 464it [04:46,  1.52it/s]Extractor Predicting: 465it [04:47,  1.53it/s]Extractor Predicting: 466it [04:48,  1.55it/s]Extractor Predicting: 467it [04:48,  1.61it/s]Extractor Predicting: 468it [04:49,  1.63it/s]Extractor Predicting: 469it [04:49,  1.66it/s]Extractor Predicting: 470it [04:50,  1.64it/s]Extractor Predicting: 471it [04:51,  1.60it/s]Extractor Predicting: 472it [04:51,  1.61it/s]Extractor Predicting: 473it [04:52,  1.60it/s]Extractor Predicting: 474it [04:52,  1.57it/s]Extractor Predicting: 475it [04:53,  1.57it/s]Extractor Predicting: 476it [04:54,  1.59it/s]Extractor Predicting: 477it [04:54,  1.59it/s]Extractor Predicting: 478it [04:55,  1.59it/s]Extractor Predicting: 479it [04:56,  1.52it/s]Extractor Predicting: 480it [04:56,  1.49it/s]Extractor Predicting: 481it [04:57,  1.48it/s]Extractor Predicting: 482it [04:58,  1.49it/s]Extractor Predicting: 483it [04:58,  1.47it/s]Extractor Predicting: 484it [04:59,  1.45it/s]Extractor Predicting: 485it [05:00,  1.46it/s]Extractor Predicting: 486it [05:00,  1.49it/s]Extractor Predicting: 487it [05:01,  1.52it/s]Extractor Predicting: 488it [05:02,  1.50it/s]Extractor Predicting: 489it [05:02,  1.50it/s]Extractor Predicting: 490it [05:03,  1.47it/s]Extractor Predicting: 491it [05:04,  1.47it/s]Extractor Predicting: 492it [05:05,  1.47it/s]Extractor Predicting: 493it [05:05,  1.49it/s]Extractor Predicting: 494it [05:06,  1.46it/s]Extractor Predicting: 495it [05:07,  1.47it/s]Extractor Predicting: 496it [05:07,  1.47it/s]Extractor Predicting: 497it [05:08,  1.48it/s]Extractor Predicting: 498it [05:09,  1.49it/s]Extractor Predicting: 499it [05:09,  1.45it/s]Extractor Predicting: 500it [05:10,  1.47it/s]Extractor Predicting: 501it [05:11,  1.49it/s]Extractor Predicting: 502it [05:11,  1.47it/s]Extractor Predicting: 503it [05:12,  1.46it/s]Extractor Predicting: 504it [05:13,  1.42it/s]Extractor Predicting: 505it [05:13,  1.42it/s]Extractor Predicting: 506it [05:14,  1.40it/s]Extractor Predicting: 507it [05:15,  1.44it/s]Extractor Predicting: 508it [05:15,  1.60it/s]Extractor Predicting: 508it [05:15,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:59,343 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:59,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:59,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:59,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:17:59,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:18:00,302 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:18:00,303 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:18:00,926 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:18:02,083 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:18:02,083 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:18:04,915 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:18:04,962 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:18:04,962 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:18:04,962 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:18:04,962 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:18:05,527 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:18:05,529 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:18:05,889 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:18:06,148 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:18:06,148 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.21264059989287626,
  "recall": 0.032591741236351696,
  "score": 0.05652050113895216,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:18,  1.44it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:21,  1.61it/s]Extractor Predicting: 34it [00:22,  1.61it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:23,  1.62it/s]Extractor Predicting: 37it [00:23,  1.60it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:25,  1.60it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:26,  1.61it/s]Extractor Predicting: 42it [00:26,  1.65it/s]Extractor Predicting: 43it [00:27,  1.64it/s]Extractor Predicting: 44it [00:28,  1.68it/s]Extractor Predicting: 45it [00:28,  1.65it/s]Extractor Predicting: 46it [00:29,  1.66it/s]Extractor Predicting: 47it [00:29,  1.65it/s]Extractor Predicting: 48it [00:30,  1.63it/s]Extractor Predicting: 49it [00:31,  1.61it/s]Extractor Predicting: 50it [00:31,  1.59it/s]Extractor Predicting: 51it [00:32,  1.58it/s]Extractor Predicting: 52it [00:33,  1.58it/s]Extractor Predicting: 53it [00:33,  1.59it/s]Extractor Predicting: 54it [00:34,  1.53it/s]Extractor Predicting: 55it [00:35,  1.56it/s]Extractor Predicting: 56it [00:35,  1.58it/s]Extractor Predicting: 57it [00:36,  1.59it/s]Extractor Predicting: 58it [00:36,  1.60it/s]Extractor Predicting: 59it [00:37,  1.60it/s]Extractor Predicting: 60it [00:38,  1.58it/s]Extractor Predicting: 61it [00:38,  1.59it/s]Extractor Predicting: 62it [00:39,  1.61it/s]Extractor Predicting: 63it [00:40,  1.61it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:41,  1.57it/s]Extractor Predicting: 66it [00:41,  1.59it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:43,  1.54it/s]Extractor Predicting: 69it [00:44,  1.49it/s]Extractor Predicting: 70it [00:44,  1.50it/s]Extractor Predicting: 71it [00:45,  1.50it/s]Extractor Predicting: 72it [00:46,  1.51it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:47,  1.49it/s]Extractor Predicting: 75it [00:47,  1.67it/s]Extractor Predicting: 75it [00:47,  1.57it/s]
[INFO|configuration_utils.py:515] 2023-08-29 07:18:59,024 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:18:59,044 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:18:59,124 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:18:59,125 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 07:18:59,161 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:19:13,042 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 07:19:13,069 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 07:19:13,199 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:19:13,200 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:19:13,284 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:13,322 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:13,322 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:13,322 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:13,322 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:13,322 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:19:13,322 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.36904761904761907,
  "recall": 0.023425692695214106,
  "score": 0.04405495026054002,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 07:19:13,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:14,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:14,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:15,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:16,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:16,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:17,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:17,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:18,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:19,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:19,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:20,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:20,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:21,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:21,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:22,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:23,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:23,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:24,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:24,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:11<03:41, 11.66s/it][WARNING|generation_utils.py:914] 2023-08-29 07:19:25,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:25,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:26,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:26,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:27,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:27,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:28,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:28,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:29,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:30,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:30,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:31,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:31,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:32,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:32,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:33,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:33,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:34,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:34,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:35,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:36,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:36,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:23<03:31, 11.73s/it][WARNING|generation_utils.py:914] 2023-08-29 07:19:37,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:37,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:38,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:39,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:39,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:40,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:41,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:41,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:42,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:43,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:43,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:44,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:45,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:45,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:46,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:47,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:47,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:48,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:48,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:49,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:50,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:36<03:32, 12.52s/it][WARNING|generation_utils.py:914] 2023-08-29 07:19:50,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:51,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:51,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:52,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:53,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:53,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:54,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:55,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:55,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:56,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:57,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:57,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:58,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:59,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:19:59,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:00,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:01,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:01,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:02,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:03,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:50<03:25, 12.82s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:03,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:04,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:05,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:05,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:06,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:06,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:07,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:07,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:08,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:08,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:09,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:09,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:10,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:10,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:11,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:12,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:12,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:13,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:13,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:14,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:14,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:01<03:05, 12.36s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:15,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:15,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:16,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:17,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:17,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:18,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:18,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:19,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:19,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:20,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:21,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:21,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:22,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:22,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:23,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:23,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:24,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:24,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:25,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:26,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:26,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:13<02:51, 12.28s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:27,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:28,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:28,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:29,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:29,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:30,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:30,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:30,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:31,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:32,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:32,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:33,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:33,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:34,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:34,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:34,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:35,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:35,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:36,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:36,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:37,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:24<02:31, 11.65s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:37,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:38,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:39,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:39,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:40,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:41,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:41,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:42,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:42,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:43,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:44,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:44,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:45,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:45,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:46,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:47,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:47,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:48,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:48,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:49,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:36<02:21, 11.80s/it][WARNING|generation_utils.py:914] 2023-08-29 07:20:50,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:50,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:51,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:51,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:52,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:53,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:53,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:54,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:55,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:55,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:56,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:57,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:57,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:58,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:59,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:20:59,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:00,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:00,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:01,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:02,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:49<02:13, 12.18s/it][WARNING|generation_utils.py:914] 2023-08-29 07:21:03,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:03,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:04,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:04,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:05,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:05,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:06,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:07,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:07,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:08,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:08,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:09,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:10,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:10,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:11,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:11,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:12,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:13,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:13,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:14,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:14,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:01<02:01, 12.19s/it][WARNING|generation_utils.py:914] 2023-08-29 07:21:15,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:15,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:16,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:17,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:17,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:18,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:18,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:19,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:20,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:20,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:21,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:21,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:22,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:23,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:23,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:24,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:24,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:25,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:26,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:26,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:27,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:28,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:28,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:15<01:55, 12.79s/it][WARNING|generation_utils.py:914] 2023-08-29 07:21:29,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:29,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:30,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:31,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:31,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:32,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:32,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:33,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:34,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:34,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:35,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:35,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:36,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:37,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:37,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:38,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:38,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:39,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:40,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:40,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:27<01:40, 12.51s/it][WARNING|generation_utils.py:914] 2023-08-29 07:21:41,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:41,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:42,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:42,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:43,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:44,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:44,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:45,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:45,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:46,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:46,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:47,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:47,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:48,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:49,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:49,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:50,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:50,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:51,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:51,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:52,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:39<01:25, 12.22s/it][WARNING|generation_utils.py:914] 2023-08-29 07:21:52,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:53,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:53,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:54,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:54,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:55,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:55,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:56,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:56,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:57,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:57,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:58,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:58,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:59,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:21:59,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:00,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:00,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:01,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:01,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:02,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:03,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:49<01:10, 11.79s/it][WARNING|generation_utils.py:914] 2023-08-29 07:22:03,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:04,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:04,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:05,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:05,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:06,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:06,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:07,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:07,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:08,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:09,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:09,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:10,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:10,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:11,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:11,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:12,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:12,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:13,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:13,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:00<00:57, 11.52s/it][WARNING|generation_utils.py:914] 2023-08-29 07:22:14,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:15,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:15,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:16,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:17,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:17,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:18,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:18,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:19,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:20,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:20,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:21,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:22,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:22,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:23,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:24,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:24,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:25,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:25,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:26,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:27,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:14<00:48, 12.07s/it][WARNING|generation_utils.py:914] 2023-08-29 07:22:27,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:28,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:28,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:29,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:29,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:30,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:31,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:31,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:32,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:32,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:33,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:33,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:34,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:34,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:35,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:35,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:36,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:37,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:37,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:38,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:24<00:35, 11.70s/it][WARNING|generation_utils.py:914] 2023-08-29 07:22:38,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:39,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:39,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:40,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:40,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:41,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:41,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:42,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:42,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:43,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:43,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:44,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:45,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:45,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:46,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:46,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:47,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:47,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:48,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:48,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:35<00:22, 11.35s/it][WARNING|generation_utils.py:914] 2023-08-29 07:22:49,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:49,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:50,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:50,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:51,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:51,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:52,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:53,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:53,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:54,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:54,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:55,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:56,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:56,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:57,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:57,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:58,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:58,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:22:59,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:00,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:47<00:11, 11.39s/it][WARNING|generation_utils.py:914] 2023-08-29 07:23:00,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:01,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:01,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:02,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:02,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:03,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:03,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:04,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:05,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:05,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:06,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:06,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:07,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:07,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:08,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:09,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:09,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:10,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:11,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:11,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:12,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:23:12,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [03:59<00:00, 11.87s/it]Generating: 100%|██████████| 20/20 [03:59<00:00, 12.00s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:22,303 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:22,326 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:22,327 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:22,327 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:22,327 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:23:23,124 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:23:23,125 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:23:23,842 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:23:24,957 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:23:24,957 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:27,999 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:28,024 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:28,025 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:28,025 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:23:28,025 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:23:28,764 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:23:28,766 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:23:29,356 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:23:29,574 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:23:29,574 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country of citizenship . Context : Edward C. Ehrlich ( born July 21 , 1954 ) is a Republican Party politician in Kansas from Anniston , Arkansas . Head Entity : Edward C. Ehrlich , Tail Entity : American .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 409, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 472, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 535, 'raw': 544}
{'target': 600, 'success': 567, 'raw': 576}
{'target': 600, 'success': 597, 'raw': 608}
{'target': 600, 'success': 629, 'raw': 640}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.9828125, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : product or material produced .', 'success_rate': 0.8721590909090909, 'errors': {'', "('Nabisco', 'product or material produced', '', 'In May 2011 , Vectra announced it would purchase Nabisco s .')", "('Toyota', 'product or material produced', '', 'He has spent the last 20 years working in the automotive industry with companies such as Toyota , Honda , and Fiat Suzuki .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : student .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : conflict .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : continent .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : field of this occupation .', 'success_rate': 0.9484375, 'errors': {''}}
['Relation : field of work . Context : Later in 1853 he came to study under John G. Haughey at Harvard University studying under John D. Rockefeller at Columbia University in the 1960s and 1970s . Head Entity : John R. W. Waughey , Tail Entity : Harvard University .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : founded by .', 'success_rate': 0.9092261904761905, 'errors': {''}}
['Relation : given name . Context : Later in life he studied at the Conservatory of Fine Arts at Loyola Marymount in Los Angeles , where he also made his first appearance as a guest singer for the California jazz band The Ritz . Head Entity : California , Tail Entity : John .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : given name .', 'success_rate': 0.8369565217391305, 'errors': {'', 'too many values to unpack (expected 2)', "('House of Representatives', 'given name', '', 'He was a member of both the House of Representatives and the Senate .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : movement .', 'success_rate': 0.8943452380952381, 'errors': {''}}
['Relation : owned by . Context : Later in 2008 , the club became a part of a deal that saw it buy a controlling stake in the New York Giants from owner Ted Turner for a reported $ 12 million . Head Entity : New York Giants , Tail Entity : New York City .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9315476190476191, 'errors': {'', "('Chicago Cubs', 'owned by', '', 'He played one season with the Chicago Cubs , in which he was traded for Bobby Evans , Ron Darling , and the Cleveland Indians .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 223, 'raw': 224}
{'target': 600, 'success': 255, 'raw': 256}
{'target': 600, 'success': 286, 'raw': 288}
{'target': 600, 'success': 318, 'raw': 320}
{'target': 600, 'success': 349, 'raw': 352}
{'target': 600, 'success': 381, 'raw': 384}
{'target': 600, 'success': 412, 'raw': 416}
{'target': 600, 'success': 442, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 505, 'raw': 512}
{'target': 600, 'success': 537, 'raw': 544}
{'target': 600, 'success': 567, 'raw': 576}
{'target': 600, 'success': 599, 'raw': 608}
{'target': 600, 'success': 631, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9859375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : place of birth .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 377, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 439, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : producer .', 'success_rate': 0.9765625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : publisher . Context : Later in 2008 , he appeared in The New Yorker magazine s All My Children entitled All My Sons and had a brief appearance as a role in the HBO television anthology series Game of Thrones . Head Entity : All My Sons and Tail Entity : HBO .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9515625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.946875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : replaces .', 'success_rate': 0.8835227272727273, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/4_ext.jsonl'}}
estimate vocab size: 10401
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10501, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.57it/s]Extractor Estimating: 2it [00:01,  1.50it/s]Extractor Estimating: 3it [00:01,  1.52it/s]Extractor Estimating: 4it [00:02,  1.59it/s]Extractor Estimating: 5it [00:03,  1.68it/s]Extractor Estimating: 6it [00:03,  1.66it/s]Extractor Estimating: 7it [00:04,  1.65it/s]Extractor Estimating: 8it [00:04,  1.60it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.52it/s]Extractor Estimating: 12it [00:07,  1.54it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:08,  1.58it/s]Extractor Estimating: 15it [00:09,  1.58it/s]Extractor Estimating: 16it [00:10,  1.60it/s]Extractor Estimating: 17it [00:10,  1.55it/s]Extractor Estimating: 18it [00:11,  1.60it/s]Extractor Estimating: 19it [00:12,  1.60it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:13,  1.63it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:15,  1.60it/s]Extractor Estimating: 25it [00:15,  1.57it/s]Extractor Estimating: 26it [00:16,  1.62it/s]Extractor Estimating: 27it [00:16,  1.63it/s]Extractor Estimating: 28it [00:17,  1.67it/s]Extractor Estimating: 29it [00:18,  1.74it/s]Extractor Estimating: 30it [00:18,  1.79it/s]Extractor Estimating: 31it [00:19,  1.84it/s]Extractor Estimating: 32it [00:19,  1.88it/s]Extractor Estimating: 33it [00:20,  1.87it/s]Extractor Estimating: 34it [00:20,  1.85it/s]Extractor Estimating: 35it [00:21,  1.87it/s]Extractor Estimating: 36it [00:21,  1.88it/s]Extractor Estimating: 37it [00:22,  1.88it/s]Extractor Estimating: 38it [00:22,  1.79it/s]Extractor Estimating: 39it [00:23,  1.81it/s]Extractor Estimating: 40it [00:23,  1.80it/s]Extractor Estimating: 41it [00:24,  1.84it/s]Extractor Estimating: 42it [00:24,  1.85it/s]Extractor Estimating: 43it [00:25,  1.87it/s]Extractor Estimating: 44it [00:26,  1.80it/s]Extractor Estimating: 45it [00:26,  1.77it/s]Extractor Estimating: 46it [00:27,  1.77it/s]Extractor Estimating: 47it [00:27,  1.81it/s]Extractor Estimating: 48it [00:28,  1.81it/s]Extractor Estimating: 49it [00:28,  1.79it/s]Extractor Estimating: 50it [00:29,  1.81it/s]Extractor Estimating: 51it [00:30,  1.75it/s]Extractor Estimating: 52it [00:30,  1.73it/s]Extractor Estimating: 53it [00:31,  1.74it/s]Extractor Estimating: 54it [00:31,  1.77it/s]Extractor Estimating: 55it [00:32,  1.83it/s]Extractor Estimating: 56it [00:32,  1.84it/s]Extractor Estimating: 57it [00:33,  1.84it/s]Extractor Estimating: 58it [00:33,  1.81it/s]Extractor Estimating: 59it [00:34,  1.83it/s]Extractor Estimating: 60it [00:34,  1.88it/s]Extractor Estimating: 61it [00:35,  1.80it/s]Extractor Estimating: 62it [00:36,  1.75it/s]Extractor Estimating: 63it [00:36,  1.74it/s]Extractor Estimating: 64it [00:37,  1.78it/s]Extractor Estimating: 65it [00:37,  1.80it/s]Extractor Estimating: 66it [00:38,  1.77it/s]Extractor Estimating: 67it [00:38,  1.80it/s]Extractor Estimating: 68it [00:39,  1.85it/s]Extractor Estimating: 69it [00:40,  1.81it/s]Extractor Estimating: 70it [00:40,  1.86it/s]Extractor Estimating: 71it [00:41,  1.88it/s]Extractor Estimating: 72it [00:41,  1.85it/s]Extractor Estimating: 73it [00:42,  1.81it/s]Extractor Estimating: 74it [00:42,  1.83it/s]Extractor Estimating: 75it [00:43,  1.86it/s]Extractor Estimating: 76it [00:43,  1.82it/s]Extractor Estimating: 77it [00:44,  1.78it/s]Extractor Estimating: 78it [00:45,  1.75it/s]Extractor Estimating: 79it [00:45,  1.75it/s]Extractor Estimating: 80it [00:46,  1.72it/s]Extractor Estimating: 81it [00:46,  1.68it/s]Extractor Estimating: 82it [00:47,  1.70it/s]Extractor Estimating: 83it [00:48,  1.68it/s]Extractor Estimating: 84it [00:48,  1.64it/s]Extractor Estimating: 85it [00:49,  1.63it/s]Extractor Estimating: 86it [00:49,  1.64it/s]Extractor Estimating: 87it [00:50,  1.52it/s]Extractor Estimating: 88it [00:51,  1.54it/s]Extractor Estimating: 89it [00:51,  1.56it/s]Extractor Estimating: 90it [00:52,  1.55it/s]Extractor Estimating: 91it [00:53,  1.57it/s]Extractor Estimating: 92it [00:53,  1.59it/s]Extractor Estimating: 93it [00:54,  1.59it/s]Extractor Estimating: 94it [00:55,  1.59it/s]Extractor Estimating: 95it [00:55,  1.61it/s]Extractor Estimating: 96it [00:56,  1.64it/s]Extractor Estimating: 97it [00:56,  1.68it/s]Extractor Estimating: 98it [00:57,  1.64it/s]Extractor Estimating: 99it [00:58,  1.64it/s]Extractor Estimating: 100it [00:58,  1.57it/s]Extractor Estimating: 101it [00:59,  1.61it/s]Extractor Estimating: 102it [00:59,  1.66it/s]Extractor Estimating: 103it [01:00,  1.70it/s]Extractor Estimating: 104it [01:00,  1.73it/s]Extractor Estimating: 105it [01:01,  1.75it/s]Extractor Estimating: 106it [01:02,  1.76it/s]Extractor Estimating: 107it [01:02,  1.74it/s]Extractor Estimating: 108it [01:03,  1.72it/s]Extractor Estimating: 109it [01:03,  1.76it/s]Extractor Estimating: 110it [01:04,  1.76it/s]Extractor Estimating: 111it [01:04,  1.75it/s]Extractor Estimating: 112it [01:05,  1.68it/s]Extractor Estimating: 113it [01:06,  1.67it/s]Extractor Estimating: 114it [01:06,  1.69it/s]Extractor Estimating: 115it [01:07,  1.68it/s]Extractor Estimating: 116it [01:07,  1.71it/s]Extractor Estimating: 117it [01:08,  1.68it/s]Extractor Estimating: 118it [01:09,  1.70it/s]Extractor Estimating: 119it [01:09,  1.74it/s]Extractor Estimating: 120it [01:10,  1.61it/s]Extractor Estimating: 121it [01:11,  1.61it/s]Extractor Estimating: 122it [01:11,  1.67it/s]Extractor Estimating: 123it [01:12,  1.63it/s]Extractor Estimating: 124it [01:12,  1.61it/s]Extractor Estimating: 125it [01:13,  1.65it/s]Extractor Estimating: 126it [01:14,  1.63it/s]Extractor Estimating: 127it [01:14,  1.66it/s]Extractor Estimating: 128it [01:15,  1.70it/s]Extractor Estimating: 129it [01:15,  1.71it/s]Extractor Estimating: 130it [01:16,  1.68it/s]Extractor Estimating: 131it [01:17,  1.66it/s]Extractor Estimating: 132it [01:17,  1.68it/s]Extractor Estimating: 133it [01:18,  1.70it/s]Extractor Estimating: 134it [01:18,  1.69it/s]Extractor Estimating: 135it [01:19,  1.71it/s]Extractor Estimating: 136it [01:19,  1.68it/s]Extractor Estimating: 137it [01:20,  1.71it/s]Extractor Estimating: 138it [01:21,  1.72it/s]Extractor Estimating: 139it [01:21,  1.71it/s]Extractor Estimating: 140it [01:22,  1.69it/s]Extractor Estimating: 141it [01:22,  1.70it/s]Extractor Estimating: 142it [01:23,  1.73it/s]Extractor Estimating: 143it [01:24,  1.70it/s]Extractor Estimating: 144it [01:24,  1.76it/s]Extractor Estimating: 145it [01:25,  1.76it/s]Extractor Estimating: 146it [01:25,  1.78it/s]Extractor Estimating: 147it [01:26,  1.75it/s]Extractor Estimating: 148it [01:26,  1.70it/s]Extractor Estimating: 149it [01:27,  1.72it/s]Extractor Estimating: 150it [01:28,  1.71it/s]Extractor Estimating: 151it [01:28,  1.79it/s]Extractor Estimating: 152it [01:29,  1.86it/s]Extractor Estimating: 153it [01:29,  1.92it/s]Extractor Estimating: 154it [01:30,  1.94it/s]Extractor Estimating: 155it [01:30,  1.97it/s]Extractor Estimating: 156it [01:30,  2.09it/s]Extractor Estimating: 157it [01:31,  2.15it/s]Extractor Estimating: 158it [01:31,  2.17it/s]Extractor Estimating: 159it [01:32,  2.13it/s]Extractor Estimating: 160it [01:32,  2.20it/s]Extractor Estimating: 161it [01:33,  2.10it/s]Extractor Estimating: 162it [01:33,  2.12it/s]Extractor Estimating: 163it [01:34,  2.05it/s]Extractor Estimating: 164it [01:34,  2.08it/s]Extractor Estimating: 165it [01:35,  2.05it/s]Extractor Estimating: 166it [01:35,  2.05it/s]Extractor Estimating: 167it [01:36,  2.10it/s]Extractor Estimating: 168it [01:36,  2.14it/s]Extractor Estimating: 169it [01:37,  2.15it/s]Extractor Estimating: 170it [01:37,  2.11it/s]Extractor Estimating: 171it [01:38,  2.05it/s]Extractor Estimating: 172it [01:38,  1.96it/s]Extractor Estimating: 173it [01:39,  2.00it/s]Extractor Estimating: 174it [01:39,  1.95it/s]Extractor Estimating: 175it [01:40,  2.06it/s]Extractor Estimating: 176it [01:40,  1.89it/s]Extractor Estimating: 177it [01:41,  1.76it/s]Extractor Estimating: 178it [01:42,  1.69it/s]Extractor Estimating: 179it [01:42,  1.46it/s]Extractor Estimating: 180it [01:43,  1.47it/s]Extractor Estimating: 181it [01:44,  1.45it/s]Extractor Estimating: 182it [01:44,  1.50it/s]Extractor Estimating: 183it [01:45,  1.54it/s]Extractor Estimating: 184it [01:46,  1.54it/s]Extractor Estimating: 185it [01:46,  1.47it/s]Extractor Estimating: 186it [01:47,  1.51it/s]Extractor Estimating: 187it [01:48,  1.53it/s]Extractor Estimating: 188it [01:48,  1.56it/s]Extractor Estimating: 189it [01:49,  1.58it/s]Extractor Estimating: 190it [01:50,  1.53it/s]Extractor Estimating: 191it [01:50,  1.50it/s]Extractor Estimating: 192it [01:51,  1.50it/s]Extractor Estimating: 193it [01:52,  1.51it/s]Extractor Estimating: 194it [01:52,  1.53it/s]Extractor Estimating: 195it [01:53,  1.50it/s]Extractor Estimating: 196it [01:54,  1.50it/s]Extractor Estimating: 197it [01:54,  1.50it/s]Extractor Estimating: 198it [01:55,  1.49it/s]Extractor Estimating: 199it [01:56,  1.53it/s]Extractor Estimating: 200it [01:56,  1.52it/s]Extractor Estimating: 201it [01:57,  1.56it/s]Extractor Estimating: 202it [01:57,  1.62it/s]Extractor Estimating: 203it [01:58,  1.66it/s]Extractor Estimating: 204it [01:59,  1.68it/s]Extractor Estimating: 205it [01:59,  1.71it/s]Extractor Estimating: 206it [02:00,  1.72it/s]Extractor Estimating: 207it [02:00,  1.72it/s]Extractor Estimating: 208it [02:01,  1.71it/s]Extractor Estimating: 209it [02:01,  1.72it/s]Extractor Estimating: 210it [02:02,  1.68it/s]Extractor Estimating: 211it [02:03,  1.71it/s]Extractor Estimating: 212it [02:03,  1.62it/s]Extractor Estimating: 213it [02:04,  1.64it/s]Extractor Estimating: 214it [02:05,  1.63it/s]Extractor Estimating: 215it [02:05,  1.66it/s]Extractor Estimating: 216it [02:06,  1.68it/s]Extractor Estimating: 217it [02:06,  1.68it/s]Extractor Estimating: 218it [02:07,  1.68it/s]Extractor Estimating: 219it [02:08,  1.66it/s]Extractor Estimating: 220it [02:08,  1.69it/s]Extractor Estimating: 221it [02:09,  1.68it/s]Extractor Estimating: 222it [02:09,  1.68it/s]Extractor Estimating: 223it [02:10,  1.61it/s]Extractor Estimating: 224it [02:10,  1.67it/s]Extractor Estimating: 225it [02:11,  1.58it/s]Extractor Estimating: 226it [02:12,  1.67it/s]Extractor Estimating: 227it [02:12,  1.68it/s]Extractor Estimating: 228it [02:13,  1.62it/s]Extractor Estimating: 229it [02:14,  1.66it/s]Extractor Estimating: 230it [02:14,  1.71it/s]Extractor Estimating: 231it [02:15,  1.75it/s]Extractor Estimating: 232it [02:15,  1.78it/s]Extractor Estimating: 233it [02:16,  1.81it/s]Extractor Estimating: 234it [02:16,  1.77it/s]Extractor Estimating: 235it [02:17,  1.74it/s]Extractor Estimating: 236it [02:17,  1.74it/s]Extractor Estimating: 237it [02:18,  1.75it/s]Extractor Estimating: 238it [02:19,  1.72it/s]Extractor Estimating: 239it [02:19,  1.76it/s]Extractor Estimating: 240it [02:20,  1.75it/s]Extractor Estimating: 241it [02:20,  1.72it/s]Extractor Estimating: 242it [02:21,  1.73it/s]Extractor Estimating: 243it [02:21,  1.78it/s]Extractor Estimating: 244it [02:22,  1.80it/s]Extractor Estimating: 245it [02:23,  1.78it/s]Extractor Estimating: 246it [02:23,  1.74it/s]Extractor Estimating: 247it [02:24,  1.73it/s]Extractor Estimating: 248it [02:24,  1.72it/s]Extractor Estimating: 249it [02:25,  1.72it/s]Extractor Estimating: 250it [02:25,  1.74it/s]Extractor Estimating: 251it [02:26,  1.71it/s]Extractor Estimating: 252it [02:27,  1.65it/s]Extractor Estimating: 253it [02:27,  1.60it/s]Extractor Estimating: 254it [02:28,  1.56it/s]Extractor Estimating: 255it [02:29,  1.59it/s]Extractor Estimating: 256it [02:30,  1.45it/s]Extractor Estimating: 257it [02:30,  1.45it/s]Extractor Estimating: 258it [02:31,  1.50it/s]Extractor Estimating: 259it [02:32,  1.50it/s]Extractor Estimating: 260it [02:32,  1.55it/s]Extractor Estimating: 261it [02:33,  1.58it/s]Extractor Estimating: 262it [02:33,  1.59it/s]Extractor Estimating: 263it [02:34,  1.61it/s]Extractor Estimating: 264it [02:35,  1.59it/s]Extractor Estimating: 265it [02:35,  1.54it/s]Extractor Estimating: 266it [02:36,  1.60it/s]Extractor Estimating: 267it [02:37,  1.57it/s]Extractor Estimating: 268it [02:37,  1.56it/s]Extractor Estimating: 269it [02:38,  1.53it/s]Extractor Estimating: 270it [02:38,  1.57it/s]Extractor Estimating: 271it [02:39,  1.59it/s]Extractor Estimating: 272it [02:40,  1.52it/s]Extractor Estimating: 273it [02:40,  1.55it/s]Extractor Estimating: 274it [02:41,  1.56it/s]Extractor Estimating: 275it [02:42,  1.57it/s]Extractor Estimating: 276it [02:42,  1.56it/s]Extractor Estimating: 277it [02:43,  1.58it/s]Extractor Estimating: 278it [02:44,  1.55it/s]Extractor Estimating: 279it [02:44,  1.58it/s]Extractor Estimating: 280it [02:45,  1.58it/s]Extractor Estimating: 281it [02:45,  1.59it/s]Extractor Estimating: 282it [02:46,  1.58it/s]Extractor Estimating: 283it [02:47,  1.59it/s]Extractor Estimating: 284it [02:47,  1.61it/s]Extractor Estimating: 285it [02:48,  1.57it/s]Extractor Estimating: 286it [02:49,  1.57it/s]Extractor Estimating: 287it [02:49,  1.55it/s]Extractor Estimating: 288it [02:50,  1.59it/s]Extractor Estimating: 289it [02:50,  1.62it/s]Extractor Estimating: 290it [02:51,  1.62it/s]Extractor Estimating: 291it [02:52,  1.65it/s]Extractor Estimating: 292it [02:52,  1.64it/s]Extractor Estimating: 293it [02:53,  1.64it/s]Extractor Estimating: 294it [02:54,  1.59it/s]Extractor Estimating: 295it [02:54,  1.64it/s]Extractor Estimating: 296it [02:55,  1.66it/s]Extractor Estimating: 297it [02:55,  1.65it/s]Extractor Estimating: 298it [02:56,  1.65it/s]Extractor Estimating: 299it [02:57,  1.61it/s]Extractor Estimating: 300it [02:57,  1.60it/s]Extractor Estimating: 301it [02:58,  1.66it/s]Extractor Estimating: 302it [02:58,  1.70it/s]Extractor Estimating: 303it [02:59,  1.69it/s]Extractor Estimating: 304it [03:00,  1.69it/s]Extractor Estimating: 305it [03:00,  1.70it/s]Extractor Estimating: 306it [03:01,  1.73it/s]Extractor Estimating: 307it [03:01,  1.68it/s]Extractor Estimating: 308it [03:02,  1.71it/s]Extractor Estimating: 309it [03:02,  1.72it/s]Extractor Estimating: 310it [03:03,  1.76it/s]Extractor Estimating: 311it [03:04,  1.74it/s]Extractor Estimating: 312it [03:04,  1.73it/s]Extractor Estimating: 313it [03:05,  1.76it/s]Extractor Estimating: 314it [03:05,  1.78it/s]Extractor Estimating: 315it [03:06,  1.78it/s]Extractor Estimating: 316it [03:06,  1.83it/s]Extractor Estimating: 317it [03:07,  1.79it/s]Extractor Estimating: 318it [03:07,  1.75it/s]Extractor Estimating: 319it [03:08,  1.76it/s]Extractor Estimating: 320it [03:09,  1.77it/s]Extractor Estimating: 321it [03:09,  1.71it/s]Extractor Estimating: 322it [03:10,  1.72it/s]Extractor Estimating: 323it [03:10,  1.76it/s]Extractor Estimating: 324it [03:11,  1.77it/s]Extractor Estimating: 325it [03:12,  1.69it/s]Extractor Estimating: 326it [03:12,  1.74it/s]Extractor Estimating: 327it [03:13,  1.76it/s]Extractor Estimating: 328it [03:13,  1.80it/s]Extractor Estimating: 329it [03:14,  1.81it/s]Extractor Estimating: 330it [03:14,  1.81it/s]Extractor Estimating: 331it [03:15,  1.84it/s]Extractor Estimating: 332it [03:15,  1.88it/s]Extractor Estimating: 333it [03:16,  1.78it/s]Extractor Estimating: 334it [03:16,  1.82it/s]Extractor Estimating: 335it [03:17,  1.81it/s]Extractor Estimating: 336it [03:18,  1.80it/s]Extractor Estimating: 337it [03:18,  1.77it/s]Extractor Estimating: 338it [03:19,  1.81it/s]Extractor Estimating: 339it [03:19,  1.82it/s]Extractor Estimating: 340it [03:20,  1.85it/s]Extractor Estimating: 341it [03:21,  1.62it/s]Extractor Estimating: 342it [03:21,  1.69it/s]Extractor Estimating: 343it [03:22,  1.76it/s]Extractor Estimating: 344it [03:22,  1.77it/s]Extractor Estimating: 345it [03:23,  1.76it/s]Extractor Estimating: 346it [03:23,  1.75it/s]Extractor Estimating: 347it [03:24,  1.81it/s]Extractor Estimating: 348it [03:24,  1.81it/s]Extractor Estimating: 349it [03:25,  1.78it/s]Extractor Estimating: 350it [03:26,  1.76it/s]Extractor Estimating: 351it [03:26,  1.74it/s]Extractor Estimating: 352it [03:27,  1.71it/s]Extractor Estimating: 353it [03:27,  1.67it/s]Extractor Estimating: 354it [03:28,  1.64it/s]Extractor Estimating: 355it [03:29,  1.61it/s]Extractor Estimating: 356it [03:29,  1.68it/s]Extractor Estimating: 357it [03:30,  1.68it/s]Extractor Estimating: 358it [03:30,  1.67it/s]Extractor Estimating: 359it [03:31,  1.61it/s]Extractor Estimating: 360it [03:32,  1.67it/s]Extractor Estimating: 361it [03:32,  1.73it/s]Extractor Estimating: 362it [03:33,  1.72it/s]Extractor Estimating: 363it [03:33,  1.71it/s]Extractor Estimating: 364it [03:34,  1.70it/s]Extractor Estimating: 365it [03:34,  1.71it/s]Extractor Estimating: 366it [03:35,  1.73it/s]Extractor Estimating: 367it [03:36,  1.72it/s]Extractor Estimating: 368it [03:36,  1.78it/s]Extractor Estimating: 369it [03:37,  1.78it/s]Extractor Estimating: 370it [03:37,  1.75it/s]Extractor Estimating: 371it [03:38,  1.71it/s]Extractor Estimating: 372it [03:39,  1.70it/s]Extractor Estimating: 373it [03:39,  1.73it/s]Extractor Estimating: 374it [03:40,  1.71it/s]Extractor Estimating: 375it [03:40,  1.69it/s]Extractor Estimating: 376it [03:41,  1.65it/s]Extractor Estimating: 377it [03:42,  1.65it/s]Extractor Estimating: 378it [03:42,  1.58it/s]Extractor Estimating: 379it [03:43,  1.61it/s]Extractor Estimating: 380it [03:43,  1.65it/s]Extractor Estimating: 381it [03:44,  1.70it/s]Extractor Estimating: 382it [03:45,  1.71it/s]Extractor Estimating: 383it [03:45,  1.67it/s]Extractor Estimating: 384it [03:46,  1.65it/s]Extractor Estimating: 385it [03:46,  1.68it/s]Extractor Estimating: 386it [03:47,  1.67it/s]Extractor Estimating: 387it [03:48,  1.69it/s]Extractor Estimating: 388it [03:48,  1.66it/s]Extractor Estimating: 389it [03:49,  1.62it/s]Extractor Estimating: 390it [03:49,  1.60it/s]Extractor Estimating: 391it [03:50,  1.65it/s]Extractor Estimating: 392it [03:51,  1.66it/s]Extractor Estimating: 393it [03:51,  1.65it/s]Extractor Estimating: 394it [03:52,  1.57it/s]Extractor Estimating: 395it [03:53,  1.56it/s]Extractor Estimating: 396it [03:53,  1.63it/s]Extractor Estimating: 397it [03:54,  1.64it/s]Extractor Estimating: 398it [03:54,  1.67it/s]Extractor Estimating: 399it [03:55,  1.68it/s]Extractor Estimating: 400it [03:55,  1.69it/s]Extractor Estimating: 401it [03:56,  1.71it/s]Extractor Estimating: 402it [03:57,  1.73it/s]Extractor Estimating: 403it [03:57,  1.73it/s]Extractor Estimating: 404it [03:58,  1.76it/s]Extractor Estimating: 405it [03:58,  1.71it/s]Extractor Estimating: 406it [03:59,  1.74it/s]Extractor Estimating: 407it [03:59,  1.80it/s]Extractor Estimating: 408it [04:00,  1.83it/s]Extractor Estimating: 409it [04:01,  1.78it/s]Extractor Estimating: 410it [04:01,  1.73it/s]Extractor Estimating: 411it [04:02,  1.72it/s]Extractor Estimating: 412it [04:02,  1.74it/s]Extractor Estimating: 413it [04:03,  1.71it/s]Extractor Estimating: 414it [04:03,  1.73it/s]Extractor Estimating: 415it [04:04,  1.68it/s]Extractor Estimating: 416it [04:05,  1.73it/s]Extractor Estimating: 417it [04:05,  1.72it/s]Extractor Estimating: 418it [04:06,  1.54it/s]Extractor Estimating: 419it [04:07,  1.63it/s]Extractor Estimating: 420it [04:07,  1.64it/s]Extractor Estimating: 421it [04:08,  1.68it/s]Extractor Estimating: 422it [04:08,  1.70it/s]Extractor Estimating: 423it [04:09,  1.75it/s]Extractor Estimating: 424it [04:09,  1.69it/s]Extractor Estimating: 425it [04:10,  1.65it/s]Extractor Estimating: 426it [04:11,  1.67it/s]Extractor Estimating: 427it [04:11,  1.64it/s]Extractor Estimating: 428it [04:12,  1.62it/s]Extractor Estimating: 429it [04:13,  1.65it/s]Extractor Estimating: 430it [04:13,  1.66it/s]Extractor Estimating: 431it [04:14,  1.63it/s]Extractor Estimating: 432it [04:14,  1.66it/s]Extractor Estimating: 433it [04:15,  1.70it/s]Extractor Estimating: 434it [04:15,  1.70it/s]Extractor Estimating: 435it [04:16,  1.70it/s]Extractor Estimating: 436it [04:17,  1.75it/s]Extractor Estimating: 437it [04:17,  1.75it/s]Extractor Estimating: 438it [04:18,  1.71it/s]Extractor Estimating: 439it [04:18,  1.71it/s]Extractor Estimating: 440it [04:19,  1.72it/s]Extractor Estimating: 441it [04:20,  1.63it/s]Extractor Estimating: 442it [04:20,  1.67it/s]Extractor Estimating: 443it [04:21,  1.65it/s]Extractor Estimating: 444it [04:21,  1.68it/s]Extractor Estimating: 445it [04:22,  1.73it/s]Extractor Estimating: 446it [04:23,  1.74it/s]Extractor Estimating: 447it [04:23,  1.71it/s]Extractor Estimating: 448it [04:24,  1.70it/s]Extractor Estimating: 449it [04:24,  1.72it/s]Extractor Estimating: 450it [04:25,  1.70it/s]Extractor Estimating: 451it [04:26,  1.63it/s]Extractor Estimating: 452it [04:26,  1.54it/s]Extractor Estimating: 453it [04:27,  1.54it/s]Extractor Estimating: 454it [04:28,  1.57it/s]Extractor Estimating: 455it [04:28,  1.57it/s]Extractor Estimating: 456it [04:29,  1.54it/s]Extractor Estimating: 457it [04:30,  1.52it/s]Extractor Estimating: 458it [04:30,  1.55it/s]Extractor Estimating: 459it [04:31,  1.58it/s]Extractor Estimating: 460it [04:31,  1.59it/s]Extractor Estimating: 461it [04:32,  1.60it/s]Extractor Estimating: 462it [04:33,  1.60it/s]Extractor Estimating: 463it [04:33,  1.60it/s]Extractor Estimating: 464it [04:34,  1.54it/s]Extractor Estimating: 465it [04:35,  1.49it/s]Extractor Estimating: 466it [04:35,  1.54it/s]Extractor Estimating: 467it [04:36,  1.51it/s]Extractor Estimating: 468it [04:37,  1.49it/s]Extractor Estimating: 469it [04:37,  1.47it/s]Extractor Estimating: 470it [04:38,  1.50it/s]Extractor Estimating: 471it [04:39,  1.50it/s]Extractor Estimating: 472it [04:39,  1.49it/s]Extractor Estimating: 473it [04:40,  1.45it/s]Extractor Estimating: 474it [04:41,  1.50it/s]Extractor Estimating: 475it [04:41,  1.51it/s]Extractor Estimating: 476it [04:42,  1.57it/s]Extractor Estimating: 477it [04:43,  1.60it/s]Extractor Estimating: 478it [04:43,  1.62it/s]Extractor Estimating: 479it [04:44,  1.70it/s]Extractor Estimating: 480it [04:44,  1.70it/s]Extractor Estimating: 481it [04:45,  1.74it/s]Extractor Estimating: 482it [04:45,  1.78it/s]Extractor Estimating: 483it [04:46,  1.77it/s]Extractor Estimating: 484it [04:46,  1.72it/s]Extractor Estimating: 485it [04:47,  1.74it/s]Extractor Estimating: 486it [04:48,  1.76it/s]Extractor Estimating: 487it [04:48,  1.76it/s]Extractor Estimating: 488it [04:49,  1.74it/s]Extractor Estimating: 489it [04:49,  1.75it/s]Extractor Estimating: 490it [04:50,  1.71it/s]Extractor Estimating: 491it [04:50,  1.75it/s]Extractor Estimating: 492it [04:51,  1.65it/s]Extractor Estimating: 493it [04:52,  1.71it/s]Extractor Estimating: 494it [04:52,  1.73it/s]Extractor Estimating: 495it [04:53,  1.72it/s]Extractor Estimating: 496it [04:53,  1.69it/s]Extractor Estimating: 497it [04:54,  1.67it/s]Extractor Estimating: 498it [04:55,  1.66it/s]Extractor Estimating: 499it [04:55,  1.70it/s]Extractor Estimating: 500it [04:56,  1.79it/s]Extractor Estimating: 500it [04:56,  1.69it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:46,557 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:46,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:46,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:46,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:46,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:28:47,293 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:28:47,294 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:28:47,875 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:28:48,970 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:28:48,971 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:51,628 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:51,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:51,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:51,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:28:51,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:28:52,078 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:28:52,079 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:28:52,365 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:28:52,579 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:28:52,580 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 10:28:44,656 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 10:28:45,283 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9990 mean pseudo reward: 0.9386928045581828
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl'}
train vocab size: 19667
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19767, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19767, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.998, loss:657.3107
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.976, loss:631.7985
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.985, loss:640.5133
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.973, loss:622.8359
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 0.966, loss:581.6176
>> valid entity prec:0.5391, rec:0.4260, f1:0.4759
>> valid relation prec:0.1734, rec:0.0169, f1:0.0308
>> valid relation with NER prec:0.1734, rec:0.0169, f1:0.0308
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.525, loss:613.5429
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 0.980, loss:595.4830
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 0.969, loss:616.4112
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 0.976, loss:592.4891
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 0.975, loss:601.2994
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4928, rec:0.5043, f1:0.4985
>> valid relation prec:0.2352, rec:0.0270, f1:0.0484
>> valid relation with NER prec:0.2352, rec:0.0270, f1:0.0484
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.521, loss:589.6387
g_step 1200, step 366, avg_time 0.984, loss:618.4639
g_step 1300, step 49, avg_time 0.989, loss:567.9905
g_step 1400, step 149, avg_time 0.985, loss:594.4658
g_step 1500, step 249, avg_time 0.986, loss:552.5965
>> valid entity prec:0.5304, rec:0.4634, f1:0.4946
>> valid relation prec:0.4067, rec:0.0830, f1:0.1379
>> valid relation with NER prec:0.4067, rec:0.0830, f1:0.1379
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.569, loss:576.5622
g_step 1700, step 32, avg_time 0.996, loss:573.3863
g_step 1800, step 132, avg_time 0.991, loss:534.7218
g_step 1900, step 232, avg_time 1.011, loss:546.6872
g_step 2000, step 332, avg_time 1.006, loss:545.2877
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5301, rec:0.4279, f1:0.4735
>> valid relation prec:0.2418, rec:0.0350, f1:0.0612
>> valid relation with NER prec:0.2418, rec:0.0350, f1:0.0612
g_step 2100, step 15, avg_time 2.569, loss:555.8087
g_step 2200, step 115, avg_time 1.003, loss:512.3526
g_step 2300, step 215, avg_time 0.994, loss:523.8630
g_step 2400, step 315, avg_time 1.002, loss:535.5952
g_step 2500, step 415, avg_time 1.011, loss:536.1658
>> valid entity prec:0.5328, rec:0.5008, f1:0.5163
>> valid relation prec:0.3671, rec:0.0620, f1:0.1061
>> valid relation with NER prec:0.3671, rec:0.0620, f1:0.1061
new max entity f1 on valid!
g_step 2600, step 98, avg_time 2.567, loss:492.8122
g_step 2700, step 198, avg_time 1.009, loss:508.6503
g_step 2800, step 298, avg_time 1.009, loss:536.5456
g_step 2900, step 398, avg_time 0.998, loss:502.2879
g_step 3000, step 81, avg_time 0.983, loss:473.6689
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5405, rec:0.4660, f1:0.5005
>> valid relation prec:0.1421, rec:0.0222, f1:0.0385
>> valid relation with NER prec:0.1421, rec:0.0222, f1:0.0385
g_step 3100, step 181, avg_time 2.574, loss:498.6332
g_step 3200, step 281, avg_time 1.003, loss:498.0436
g_step 3300, step 381, avg_time 0.998, loss:473.7418
g_step 3400, step 64, avg_time 0.998, loss:478.1564
g_step 3500, step 164, avg_time 1.016, loss:479.1852
>> valid entity prec:0.5030, rec:0.5279, f1:0.5151
>> valid relation prec:0.2686, rec:0.0468, f1:0.0796
>> valid relation with NER prec:0.2686, rec:0.0468, f1:0.0796
g_step 3600, step 264, avg_time 2.576, loss:475.0869
g_step 3700, step 364, avg_time 0.995, loss:482.7942
g_step 3800, step 47, avg_time 0.983, loss:465.2387
g_step 3900, step 147, avg_time 0.994, loss:465.2800
g_step 4000, step 247, avg_time 1.000, loss:462.8718
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5605, rec:0.5083, f1:0.5331
>> valid relation prec:0.3653, rec:0.0721, f1:0.1204
>> valid relation with NER prec:0.3653, rec:0.0721, f1:0.1204
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 347, avg_time 2.577, loss:467.3090
g_step 4200, step 30, avg_time 0.994, loss:458.5218
g_step 4300, step 130, avg_time 0.981, loss:434.8978
g_step 4400, step 230, avg_time 1.016, loss:449.6643
g_step 4500, step 330, avg_time 1.006, loss:450.5579
>> valid entity prec:0.5639, rec:0.4401, f1:0.4944
>> valid relation prec:0.3403, rec:0.0700, f1:0.1162
>> valid relation with NER prec:0.3403, rec:0.0700, f1:0.1162
g_step 4600, step 13, avg_time 2.560, loss:443.1475
g_step 4700, step 113, avg_time 1.002, loss:413.1613
g_step 4800, step 213, avg_time 0.976, loss:438.4951
g_step 4900, step 313, avg_time 0.996, loss:429.6675
g_step 5000, step 413, avg_time 0.999, loss:439.2639
learning rate was adjusted to 0.0008
>> valid entity prec:0.5650, rec:0.4720, f1:0.5143
>> valid relation prec:0.2743, rec:0.0540, f1:0.0902
>> valid relation with NER prec:0.2743, rec:0.0540, f1:0.0902
g_step 5100, step 96, avg_time 2.563, loss:413.1712
g_step 5200, step 196, avg_time 1.003, loss:425.7130
g_step 5300, step 296, avg_time 1.002, loss:414.6579
g_step 5400, step 396, avg_time 1.023, loss:439.8298
g_step 5500, step 79, avg_time 0.996, loss:396.1331
>> valid entity prec:0.5552, rec:0.4570, f1:0.5013
>> valid relation prec:0.2456, rec:0.0548, f1:0.0896
>> valid relation with NER prec:0.2456, rec:0.0548, f1:0.0896
g_step 5600, step 179, avg_time 2.551, loss:407.8465
g_step 5700, step 279, avg_time 1.010, loss:402.8576
g_step 5800, step 379, avg_time 0.997, loss:426.4439
g_step 5900, step 62, avg_time 0.974, loss:385.8343
g_step 6000, step 162, avg_time 0.991, loss:394.9112
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5783, rec:0.4139, f1:0.4824
>> valid relation prec:0.2803, rec:0.0457, f1:0.0786
>> valid relation with NER prec:0.2803, rec:0.0457, f1:0.0786
g_step 6100, step 262, avg_time 2.549, loss:403.0840
g_step 6200, step 362, avg_time 1.013, loss:406.8722
g_step 6300, step 45, avg_time 1.003, loss:380.8573
g_step 6400, step 145, avg_time 0.992, loss:381.3985
g_step 6500, step 245, avg_time 0.993, loss:387.5534
>> valid entity prec:0.5670, rec:0.4895, f1:0.5255
>> valid relation prec:0.3233, rec:0.0867, f1:0.1368
>> valid relation with NER prec:0.3233, rec:0.0867, f1:0.1368
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6600, step 345, avg_time 2.575, loss:389.5349
g_step 6700, step 28, avg_time 0.972, loss:379.0214
g_step 6800, step 128, avg_time 0.995, loss:369.0716
g_step 6900, step 228, avg_time 1.006, loss:378.3041
g_step 7000, step 328, avg_time 1.008, loss:384.6347
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5565, rec:0.4505, f1:0.4979
>> valid relation prec:0.2803, rec:0.0606, f1:0.0996
>> valid relation with NER prec:0.2803, rec:0.0606, f1:0.0996
g_step 7100, step 11, avg_time 2.548, loss:382.1231
g_step 7200, step 111, avg_time 0.997, loss:357.7407
g_step 7300, step 211, avg_time 0.992, loss:369.5962
g_step 7400, step 311, avg_time 1.015, loss:375.4976
g_step 7500, step 411, avg_time 0.996, loss:367.8284
>> valid entity prec:0.5687, rec:0.4296, f1:0.4895
>> valid relation prec:0.2708, rec:0.0523, f1:0.0877
>> valid relation with NER prec:0.2708, rec:0.0523, f1:0.0877
g_step 7600, step 94, avg_time 2.551, loss:343.2435
g_step 7700, step 194, avg_time 0.994, loss:344.9009
g_step 7800, step 294, avg_time 0.983, loss:356.1358
g_step 7900, step 394, avg_time 0.998, loss:372.2237
g_step 8000, step 77, avg_time 0.992, loss:342.6907
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5656, rec:0.4351, f1:0.4919
>> valid relation prec:0.3145, rec:0.0713, f1:0.1162
>> valid relation with NER prec:0.3145, rec:0.0713, f1:0.1162
g_step 8100, step 177, avg_time 2.515, loss:324.1238
g_step 8200, step 277, avg_time 0.962, loss:353.0891
g_step 8300, step 377, avg_time 0.969, loss:355.9577
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 10:28:45 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 10:28:45 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_10-28-44_ctolab09.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 10:28:46 - WARNING - datasets.builder -   Using custom data configuration default-7909ccc4ccd295bf
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7909ccc4ccd295bf/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 10:28:50,566 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:28:50,567 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 10:28:50,567 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 10:28:50,568 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 10:28:50,836 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:28:50,944 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:28:50,944 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:28:50,944 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:28:50,944 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:28:50,944 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 10:28:50,944 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 10:28:51,572 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 10:28:54,695 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 10:28:54,750 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7909ccc4ccd295bf/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:04,  1.82ba/s] 20%|██        | 2/10 [00:00<00:02,  2.91ba/s] 30%|███       | 3/10 [00:00<00:01,  3.60ba/s] 40%|████      | 4/10 [00:01<00:01,  4.03ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.32ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.55ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.69ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.78ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.85ba/s]100%|██████████| 10/10 [00:02<00:00,  4.87ba/s]100%|██████████| 10/10 [00:02<00:00,  4.25ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.03ba/s] 40%|████      | 2/5 [00:00<00:00,  3.45ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.91ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.15ba/s]100%|██████████| 5/5 [00:01<00:00,  4.50ba/s]100%|██████████| 5/5 [00:01<00:00,  4.12ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  5.31ba/s] 30%|███       | 3/10 [00:00<00:00,  8.64ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.83ba/s] 70%|███████   | 7/10 [00:00<00:00, 10.41ba/s] 90%|█████████ | 9/10 [00:00<00:00, 10.65ba/s]100%|██████████| 10/10 [00:00<00:00, 10.09ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.94ba/s] 60%|██████    | 3/5 [00:00<00:00,  7.46ba/s] 80%|████████  | 4/5 [00:00<00:00,  7.38ba/s]100%|██████████| 5/5 [00:00<00:00,  7.63ba/s]
[INFO|trainer.py:414] 2023-08-29 10:29:01,754 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 10:29:01,874 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 10:29:01,874 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 10:29:01,874 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 10:29:01,875 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 10:29:01,875 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 10:29:01,875 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 10:29:01,875 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:54,  3.32it/s]  0%|          | 2/780 [00:00<03:48,  3.40it/s]  0%|          | 3/780 [00:00<03:46,  3.43it/s]  1%|          | 4/780 [00:01<03:45,  3.45it/s]  1%|          | 5/780 [00:01<03:44,  3.46it/s]  1%|          | 6/780 [00:01<03:43,  3.46it/s]  1%|          | 7/780 [00:02<03:44,  3.45it/s]  1%|          | 8/780 [00:02<03:44,  3.44it/s]  1%|          | 9/780 [00:02<03:56,  3.26it/s]  1%|▏         | 10/780 [00:02<03:52,  3.31it/s]  1%|▏         | 11/780 [00:03<03:50,  3.34it/s]  2%|▏         | 12/780 [00:03<03:48,  3.36it/s]  2%|▏         | 13/780 [00:03<03:46,  3.38it/s]  2%|▏         | 14/780 [00:04<03:45,  3.39it/s]  2%|▏         | 15/780 [00:04<03:43,  3.42it/s]  2%|▏         | 16/780 [00:04<03:42,  3.43it/s]  2%|▏         | 17/780 [00:04<03:41,  3.44it/s]  2%|▏         | 18/780 [00:05<03:40,  3.45it/s]  2%|▏         | 19/780 [00:05<03:40,  3.46it/s]  3%|▎         | 20/780 [00:05<03:45,  3.37it/s]  3%|▎         | 21/780 [00:06<03:43,  3.40it/s]  3%|▎         | 22/780 [00:06<03:41,  3.42it/s]  3%|▎         | 23/780 [00:06<03:40,  3.43it/s]  3%|▎         | 24/780 [00:07<03:39,  3.44it/s]  3%|▎         | 25/780 [00:07<03:38,  3.45it/s]  3%|▎         | 26/780 [00:07<03:37,  3.46it/s]  3%|▎         | 27/780 [00:07<03:37,  3.46it/s]  4%|▎         | 28/780 [00:08<03:37,  3.46it/s]  4%|▎         | 29/780 [00:08<03:36,  3.47it/s]  4%|▍         | 30/780 [00:08<03:36,  3.46it/s]  4%|▍         | 31/780 [00:09<03:43,  3.35it/s]  4%|▍         | 32/780 [00:09<03:40,  3.39it/s]  4%|▍         | 33/780 [00:09<03:39,  3.41it/s]  4%|▍         | 34/780 [00:09<03:37,  3.42it/s]  4%|▍         | 35/780 [00:10<03:36,  3.44it/s]  5%|▍         | 36/780 [00:10<03:35,  3.45it/s]  5%|▍         | 37/780 [00:10<03:35,  3.45it/s]  5%|▍         | 38/780 [00:11<03:34,  3.45it/s]  5%|▌         | 39/780 [00:11<03:34,  3.46it/s]  5%|▌         | 40/780 [00:11<03:33,  3.46it/s]  5%|▌         | 41/780 [00:11<03:33,  3.46it/s]  5%|▌         | 42/780 [00:12<03:44,  3.28it/s]  6%|▌         | 43/780 [00:12<03:41,  3.33it/s]  6%|▌         | 44/780 [00:12<03:38,  3.37it/s]  6%|▌         | 45/780 [00:13<03:36,  3.40it/s]  6%|▌         | 46/780 [00:13<03:34,  3.42it/s]  6%|▌         | 47/780 [00:13<03:33,  3.43it/s]  6%|▌         | 48/780 [00:14<03:32,  3.44it/s]  6%|▋         | 49/780 [00:14<03:32,  3.45it/s]  6%|▋         | 50/780 [00:14<03:31,  3.45it/s]  7%|▋         | 51/780 [00:14<03:30,  3.46it/s]  7%|▋         | 52/780 [00:15<03:30,  3.46it/s]  7%|▋         | 53/780 [00:15<03:33,  3.41it/s]  7%|▋         | 54/780 [00:15<03:31,  3.43it/s]  7%|▋         | 55/780 [00:16<03:31,  3.43it/s]  7%|▋         | 56/780 [00:16<03:30,  3.44it/s]  7%|▋         | 57/780 [00:16<03:29,  3.45it/s]  7%|▋         | 58/780 [00:16<03:29,  3.45it/s]  8%|▊         | 59/780 [00:17<03:28,  3.45it/s]  8%|▊         | 60/780 [00:17<03:28,  3.46it/s]  8%|▊         | 61/780 [00:17<03:27,  3.46it/s]  8%|▊         | 62/780 [00:18<03:27,  3.46it/s]  8%|▊         | 63/780 [00:18<03:45,  3.17it/s]  8%|▊         | 64/780 [00:18<03:39,  3.26it/s]  8%|▊         | 65/780 [00:19<03:35,  3.31it/s]  8%|▊         | 66/780 [00:19<03:32,  3.36it/s]  9%|▊         | 67/780 [00:19<03:30,  3.39it/s]  9%|▊         | 68/780 [00:19<03:28,  3.41it/s]  9%|▉         | 69/780 [00:20<03:27,  3.43it/s]  9%|▉         | 70/780 [00:20<03:26,  3.44it/s]  9%|▉         | 71/780 [00:20<03:25,  3.45it/s]  9%|▉         | 72/780 [00:21<03:25,  3.45it/s]  9%|▉         | 73/780 [00:21<03:24,  3.46it/s]  9%|▉         | 74/780 [00:21<03:33,  3.31it/s] 10%|▉         | 75/780 [00:21<03:30,  3.35it/s] 10%|▉         | 76/780 [00:22<03:28,  3.38it/s] 10%|▉         | 77/780 [00:22<03:26,  3.41it/s] 10%|█         | 78/780 [00:22<03:25,  3.42it/s] 10%|█         | 79/780 [00:23<03:24,  3.43it/s] 10%|█         | 80/780 [00:23<03:23,  3.44it/s] 10%|█         | 81/780 [00:23<03:22,  3.45it/s] 11%|█         | 82/780 [00:24<03:22,  3.45it/s] 11%|█         | 83/780 [00:24<03:21,  3.45it/s] 11%|█         | 84/780 [00:24<03:21,  3.46it/s] 11%|█         | 85/780 [00:24<03:32,  3.27it/s] 11%|█         | 86/780 [00:25<03:28,  3.33it/s] 11%|█         | 87/780 [00:25<03:25,  3.37it/s] 11%|█▏        | 88/780 [00:25<03:24,  3.39it/s] 11%|█▏        | 89/780 [00:26<03:22,  3.41it/s] 12%|█▏        | 90/780 [00:26<03:21,  3.43it/s] 12%|█▏        | 91/780 [00:26<03:20,  3.44it/s] 12%|█▏        | 92/780 [00:26<03:19,  3.44it/s] 12%|█▏        | 93/780 [00:27<03:19,  3.45it/s] 12%|█▏        | 94/780 [00:27<03:18,  3.45it/s] 12%|█▏        | 95/780 [00:27<03:18,  3.45it/s] 12%|█▏        | 96/780 [00:28<03:35,  3.17it/s] 12%|█▏        | 97/780 [00:28<03:29,  3.26it/s] 13%|█▎        | 98/780 [00:28<03:25,  3.31it/s] 13%|█▎        | 99/780 [00:29<03:22,  3.36it/s] 13%|█▎        | 100/780 [00:29<03:20,  3.38it/s] 13%|█▎        | 101/780 [00:29<03:19,  3.41it/s] 13%|█▎        | 102/780 [00:29<03:18,  3.42it/s] 13%|█▎        | 103/780 [00:30<03:17,  3.44it/s] 13%|█▎        | 104/780 [00:30<03:16,  3.44it/s] 13%|█▎        | 105/780 [00:30<03:15,  3.44it/s] 14%|█▎        | 106/780 [00:31<03:15,  3.45it/s] 14%|█▎        | 107/780 [00:31<03:32,  3.17it/s] 14%|█▍        | 108/780 [00:31<03:26,  3.26it/s] 14%|█▍        | 109/780 [00:32<03:22,  3.32it/s] 14%|█▍        | 110/780 [00:32<03:19,  3.36it/s] 14%|█▍        | 111/780 [00:32<03:17,  3.39it/s] 14%|█▍        | 112/780 [00:32<03:15,  3.41it/s] 14%|█▍        | 113/780 [00:33<03:14,  3.42it/s] 15%|█▍        | 114/780 [00:33<03:13,  3.43it/s] 15%|█▍        | 115/780 [00:33<03:13,  3.44it/s] 15%|█▍        | 116/780 [00:34<03:12,  3.45it/s] 15%|█▌        | 117/780 [00:34<03:12,  3.45it/s] 15%|█▌        | 118/780 [00:34<03:30,  3.15it/s] 15%|█▌        | 119/780 [00:35<03:24,  3.24it/s] 15%|█▌        | 120/780 [00:35<03:20,  3.30it/s] 16%|█▌        | 121/780 [00:35<03:16,  3.35it/s] 16%|█▌        | 122/780 [00:35<03:14,  3.38it/s] 16%|█▌        | 123/780 [00:36<03:12,  3.41it/s] 16%|█▌        | 124/780 [00:36<03:11,  3.42it/s] 16%|█▌        | 125/780 [00:36<03:10,  3.43it/s] 16%|█▌        | 126/780 [00:37<03:09,  3.44it/s] 16%|█▋        | 127/780 [00:37<03:09,  3.45it/s] 16%|█▋        | 128/780 [00:37<03:08,  3.46it/s] 17%|█▋        | 129/780 [00:38<03:29,  3.11it/s] 17%|█▋        | 130/780 [00:38<03:22,  3.21it/s] 17%|█▋        | 131/780 [00:38<03:17,  3.28it/s] 17%|█▋        | 132/780 [00:38<03:14,  3.33it/s] 17%|█▋        | 133/780 [00:39<03:11,  3.37it/s] 17%|█▋        | 134/780 [00:39<03:10,  3.40it/s] 17%|█▋        | 135/780 [00:39<03:08,  3.42it/s] 17%|█▋        | 136/780 [00:40<03:28,  3.10it/s] 18%|█▊        | 137/780 [00:40<03:21,  3.20it/s] 18%|█▊        | 138/780 [00:40<03:16,  3.27it/s] 18%|█▊        | 139/780 [00:41<03:38,  2.93it/s] 18%|█▊        | 140/780 [00:41<03:28,  3.07it/s] 18%|█▊        | 141/780 [00:41<03:21,  3.18it/s] 18%|█▊        | 142/780 [00:42<03:15,  3.26it/s] 18%|█▊        | 143/780 [00:42<03:12,  3.32it/s] 18%|█▊        | 144/780 [00:42<03:09,  3.36it/s] 19%|█▊        | 145/780 [00:43<05:32,  1.91it/s] 19%|█▊        | 146/780 [00:43<04:47,  2.20it/s] 19%|█▉        | 147/780 [00:44<04:23,  2.40it/s] 19%|█▉        | 148/780 [00:44<03:59,  2.64it/s] 19%|█▉        | 149/780 [00:44<03:41,  2.85it/s] 19%|█▉        | 150/780 [00:45<03:29,  3.01it/s] 19%|█▉        | 151/780 [00:45<03:21,  3.13it/s] 19%|█▉        | 152/780 [00:45<03:14,  3.22it/s] 20%|█▉        | 153/780 [00:46<03:10,  3.29it/s] 20%|█▉        | 154/780 [00:46<03:07,  3.34it/s] 20%|█▉        | 155/780 [00:46<03:05,  3.38it/s] 20%|██        | 156/780 [00:46<03:03,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 10:29:48,784 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:29:48,784 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 10:29:48,784 >>   Batch size = 8

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 55.99it/s][A
  2%|▏         | 12/608 [00:00<00:12, 48.29it/s][A
  3%|▎         | 17/608 [00:00<00:12, 46.98it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.25it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.76it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.57it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.38it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.15it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.16it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.20it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.20it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.24it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.21it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 45.08it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.16it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 45.10it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 45.03it/s][A
 15%|█▌        | 92/608 [00:02<00:12, 41.36it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 42.67it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 43.60it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.21it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 44.48it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 44.74it/s][A
 20%|██        | 122/608 [00:02<00:10, 44.87it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.88it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.47it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.53it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.89it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.20it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.33it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.29it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.31it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.24it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.08it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.88it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.85it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.90it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.24it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.35it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.30it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.30it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.24it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.06it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.95it/s][A
 37%|███▋      | 227/608 [00:05<00:09, 42.13it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 43.11it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 43.94it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.38it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.81it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.88it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.97it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.85it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.33it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.65it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.85it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.07it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.28it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.44it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.41it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.21it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.03it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 44.62it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.74it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.94it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.04it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.21it/s][A
 55%|█████▌    | 337/608 [00:07<00:05, 45.26it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.38it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.25it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.04it/s][A
 59%|█████▊    | 357/608 [00:08<00:05, 44.87it/s][A
 60%|█████▉    | 362/608 [00:08<00:06, 39.31it/s][A
 60%|██████    | 367/608 [00:08<00:05, 41.06it/s][A
 61%|██████    | 372/608 [00:08<00:05, 42.35it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 43.32it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.01it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.58it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.98it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 44.97it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.59it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.44it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.54it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 44.79it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.04it/s][A
 70%|███████   | 427/608 [00:09<00:03, 45.26it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.42it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.59it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.28it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.02it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.81it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.69it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.93it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.08it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.28it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.45it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.54it/s][A
 80%|████████  | 487/608 [00:10<00:02, 45.13it/s][A
 81%|████████  | 492/608 [00:11<00:02, 45.04it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 41.15it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 42.58it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 43.48it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.17it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.66it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.98it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.98it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 44.91it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 44.43it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.45it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.68it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.93it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.17it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.38it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.34it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.35it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.23it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 44.80it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 44.71it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.83it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 45.01it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 45.23it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.38it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.38it/s][A 20%|██        | 156/780 [01:00<03:03,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:30:02,887 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 10:30:03,312 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:30:08,318 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:30:08,596 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:30:08,736 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:17<1:37:00,  9.34s/it] 20%|██        | 158/780 [01:17<1:08:41,  6.63s/it] 20%|██        | 159/780 [01:17<48:55,  4.73s/it]   21%|██        | 160/780 [01:18<35:05,  3.40s/it] 21%|██        | 161/780 [01:18<25:25,  2.47s/it] 21%|██        | 162/780 [01:18<18:40,  1.81s/it] 21%|██        | 163/780 [01:19<13:57,  1.36s/it] 21%|██        | 164/780 [01:19<10:39,  1.04s/it] 21%|██        | 165/780 [01:19<08:20,  1.23it/s] 21%|██▏       | 166/780 [01:19<06:49,  1.50it/s] 21%|██▏       | 167/780 [01:20<05:40,  1.80it/s] 22%|██▏       | 168/780 [01:20<04:51,  2.10it/s] 22%|██▏       | 169/780 [01:20<04:17,  2.37it/s] 22%|██▏       | 170/780 [01:21<03:53,  2.61it/s] 22%|██▏       | 171/780 [01:21<03:36,  2.81it/s] 22%|██▏       | 172/780 [01:21<03:24,  2.97it/s] 22%|██▏       | 173/780 [01:22<03:16,  3.09it/s] 22%|██▏       | 174/780 [01:22<03:10,  3.18it/s] 22%|██▏       | 175/780 [01:22<03:06,  3.25it/s] 23%|██▎       | 176/780 [01:22<03:03,  3.30it/s] 23%|██▎       | 177/780 [01:23<03:08,  3.20it/s] 23%|██▎       | 178/780 [01:23<03:04,  3.26it/s] 23%|██▎       | 179/780 [01:23<03:01,  3.30it/s] 23%|██▎       | 180/780 [01:24<02:59,  3.34it/s] 23%|██▎       | 181/780 [01:24<02:58,  3.36it/s] 23%|██▎       | 182/780 [01:24<02:57,  3.38it/s] 23%|██▎       | 183/780 [01:25<02:56,  3.39it/s] 24%|██▎       | 184/780 [01:25<02:55,  3.39it/s] 24%|██▎       | 185/780 [01:25<02:54,  3.40it/s] 24%|██▍       | 186/780 [01:25<02:54,  3.41it/s] 24%|██▍       | 187/780 [01:26<02:54,  3.41it/s] 24%|██▍       | 188/780 [01:26<03:02,  3.25it/s] 24%|██▍       | 189/780 [01:26<02:59,  3.30it/s] 24%|██▍       | 190/780 [01:27<02:57,  3.33it/s] 24%|██▍       | 191/780 [01:27<02:55,  3.35it/s] 25%|██▍       | 192/780 [01:27<02:54,  3.37it/s] 25%|██▍       | 193/780 [01:27<02:53,  3.38it/s] 25%|██▍       | 194/780 [01:28<02:52,  3.40it/s] 25%|██▌       | 195/780 [01:28<02:51,  3.40it/s] 25%|██▌       | 196/780 [01:28<02:51,  3.41it/s] 25%|██▌       | 197/780 [01:29<02:50,  3.41it/s] 25%|██▌       | 198/780 [01:29<02:50,  3.41it/s] 26%|██▌       | 199/780 [01:29<03:11,  3.04it/s] 26%|██▌       | 200/780 [01:30<03:04,  3.15it/s] 26%|██▌       | 201/780 [01:30<02:59,  3.23it/s] 26%|██▌       | 202/780 [01:30<02:56,  3.28it/s] 26%|██▌       | 203/780 [01:31<02:53,  3.32it/s] 26%|██▌       | 204/780 [01:31<02:52,  3.35it/s] 26%|██▋       | 205/780 [01:31<02:50,  3.37it/s] 26%|██▋       | 206/780 [01:31<02:49,  3.38it/s] 27%|██▋       | 207/780 [01:32<02:48,  3.39it/s] 27%|██▋       | 208/780 [01:32<02:48,  3.40it/s] 27%|██▋       | 209/780 [01:32<03:01,  3.15it/s] 27%|██▋       | 210/780 [01:33<02:56,  3.23it/s] 27%|██▋       | 211/780 [01:33<02:53,  3.29it/s] 27%|██▋       | 212/780 [01:33<02:51,  3.32it/s] 27%|██▋       | 213/780 [01:34<02:49,  3.35it/s] 27%|██▋       | 214/780 [01:34<02:47,  3.37it/s] 28%|██▊       | 215/780 [01:34<02:47,  3.38it/s] 28%|██▊       | 216/780 [01:34<02:46,  3.39it/s] 28%|██▊       | 217/780 [01:35<02:45,  3.40it/s] 28%|██▊       | 218/780 [01:35<02:45,  3.40it/s] 28%|██▊       | 219/780 [01:35<03:03,  3.06it/s] 28%|██▊       | 220/780 [01:36<02:57,  3.16it/s] 28%|██▊       | 221/780 [01:36<02:53,  3.23it/s] 28%|██▊       | 222/780 [01:36<02:49,  3.28it/s] 29%|██▊       | 223/780 [01:37<02:47,  3.32it/s] 29%|██▊       | 224/780 [01:37<02:45,  3.35it/s] 29%|██▉       | 225/780 [01:37<02:44,  3.37it/s] 29%|██▉       | 226/780 [01:37<02:43,  3.38it/s] 29%|██▉       | 227/780 [01:38<02:42,  3.39it/s] 29%|██▉       | 228/780 [01:38<02:42,  3.40it/s] 29%|██▉       | 229/780 [01:38<03:00,  3.05it/s] 29%|██▉       | 230/780 [01:39<02:54,  3.15it/s] 30%|██▉       | 231/780 [01:39<02:50,  3.23it/s] 30%|██▉       | 232/780 [01:39<02:47,  3.28it/s] 30%|██▉       | 233/780 [01:40<02:58,  3.06it/s] 30%|███       | 234/780 [01:40<02:53,  3.15it/s] 30%|███       | 235/780 [01:40<02:48,  3.23it/s] 30%|███       | 236/780 [01:41<02:45,  3.28it/s] 30%|███       | 237/780 [01:41<02:43,  3.31it/s] 31%|███       | 238/780 [01:41<02:42,  3.34it/s] 31%|███       | 239/780 [01:42<02:47,  3.23it/s] 31%|███       | 240/780 [01:42<02:44,  3.28it/s] 31%|███       | 241/780 [01:42<02:42,  3.33it/s] 31%|███       | 242/780 [01:42<02:47,  3.20it/s] 31%|███       | 243/780 [01:43<03:32,  2.53it/s] 31%|███▏      | 244/780 [01:43<03:35,  2.48it/s] 31%|███▏      | 245/780 [01:44<03:17,  2.71it/s] 32%|███▏      | 246/780 [01:44<03:05,  2.89it/s] 32%|███▏      | 247/780 [01:44<02:56,  3.02it/s] 32%|███▏      | 248/780 [01:45<03:00,  2.95it/s] 32%|███▏      | 249/780 [01:45<02:52,  3.08it/s] 32%|███▏      | 250/780 [01:45<02:47,  3.17it/s] 32%|███▏      | 251/780 [01:46<02:43,  3.24it/s] 32%|███▏      | 252/780 [01:46<02:40,  3.29it/s] 32%|███▏      | 253/780 [01:46<02:38,  3.33it/s] 33%|███▎      | 254/780 [01:46<02:36,  3.35it/s] 33%|███▎      | 255/780 [01:47<02:35,  3.37it/s] 33%|███▎      | 256/780 [01:47<02:34,  3.38it/s] 33%|███▎      | 257/780 [01:47<02:34,  3.39it/s] 33%|███▎      | 258/780 [01:48<02:33,  3.40it/s] 33%|███▎      | 259/780 [01:48<02:33,  3.40it/s] 33%|███▎      | 260/780 [01:48<02:32,  3.41it/s] 33%|███▎      | 261/780 [01:48<02:32,  3.41it/s] 34%|███▎      | 262/780 [01:49<02:32,  3.41it/s] 34%|███▎      | 263/780 [01:49<02:31,  3.41it/s] 34%|███▍      | 264/780 [01:49<02:31,  3.41it/s] 34%|███▍      | 265/780 [01:50<02:31,  3.41it/s] 34%|███▍      | 266/780 [01:50<02:30,  3.41it/s] 34%|███▍      | 267/780 [01:50<02:39,  3.22it/s] 34%|███▍      | 268/780 [01:51<02:36,  3.27it/s] 34%|███▍      | 269/780 [01:51<02:34,  3.31it/s] 35%|███▍      | 270/780 [01:51<02:32,  3.34it/s] 35%|███▍      | 271/780 [01:51<02:31,  3.36it/s] 35%|███▍      | 272/780 [01:52<02:30,  3.38it/s] 35%|███▌      | 273/780 [01:52<02:29,  3.39it/s] 35%|███▌      | 274/780 [01:52<02:28,  3.40it/s] 35%|███▌      | 275/780 [01:53<02:28,  3.40it/s] 35%|███▌      | 276/780 [01:53<02:28,  3.40it/s] 36%|███▌      | 277/780 [01:53<02:27,  3.40it/s] 36%|███▌      | 278/780 [01:54<02:38,  3.17it/s] 36%|███▌      | 279/780 [01:54<02:34,  3.24it/s] 36%|███▌      | 280/780 [01:54<02:31,  3.29it/s] 36%|███▌      | 281/780 [01:54<02:30,  3.33it/s] 36%|███▌      | 282/780 [01:55<02:28,  3.35it/s] 36%|███▋      | 283/780 [01:55<02:27,  3.37it/s] 36%|███▋      | 284/780 [01:55<02:26,  3.38it/s] 37%|███▋      | 285/780 [01:56<02:26,  3.39it/s] 37%|███▋      | 286/780 [01:56<02:25,  3.40it/s] 37%|███▋      | 287/780 [01:56<02:25,  3.40it/s] 37%|███▋      | 288/780 [01:57<02:29,  3.29it/s] 37%|███▋      | 289/780 [01:57<02:27,  3.32it/s] 37%|███▋      | 290/780 [01:57<02:26,  3.35it/s] 37%|███▋      | 291/780 [01:57<02:25,  3.37it/s] 37%|███▋      | 292/780 [01:58<02:24,  3.38it/s] 38%|███▊      | 293/780 [01:58<02:23,  3.38it/s] 38%|███▊      | 294/780 [01:58<02:23,  3.39it/s] 38%|███▊      | 295/780 [01:59<02:22,  3.40it/s] 38%|███▊      | 296/780 [01:59<02:22,  3.40it/s] 38%|███▊      | 297/780 [01:59<02:21,  3.41it/s] 38%|███▊      | 298/780 [01:59<02:21,  3.41it/s] 38%|███▊      | 299/780 [02:00<02:25,  3.31it/s] 38%|███▊      | 300/780 [02:00<02:23,  3.33it/s] 39%|███▊      | 301/780 [02:00<02:22,  3.36it/s] 39%|███▊      | 302/780 [02:01<02:21,  3.37it/s] 39%|███▉      | 303/780 [02:01<02:20,  3.38it/s] 39%|███▉      | 304/780 [02:01<02:20,  3.39it/s] 39%|███▉      | 305/780 [02:02<02:19,  3.40it/s] 39%|███▉      | 306/780 [02:02<02:19,  3.40it/s] 39%|███▉      | 307/780 [02:02<02:18,  3.41it/s] 39%|███▉      | 308/780 [02:02<02:18,  3.41it/s] 40%|███▉      | 309/780 [02:03<02:18,  3.41it/s] 40%|███▉      | 310/780 [02:03<02:22,  3.30it/s] 40%|███▉      | 311/780 [02:03<02:20,  3.33it/s] 40%|████      | 312/780 [02:04<02:19,  3.35it/s][INFO|trainer.py:2140] 2023-08-29 10:31:06,085 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:31:06,085 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 10:31:06,085 >>   Batch size = 8
{'eval_loss': 1.008566975593567, 'eval_runtime': 13.644, 'eval_samples_per_second': 356.493, 'eval_steps_per_second': 44.562, 'epoch': 1.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 55.96it/s][A
  2%|▏         | 12/608 [00:00<00:12, 48.72it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.07it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.14it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.47it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.36it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.23it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.05it/s][A
  8%|▊         | 47/608 [00:01<00:12, 45.13it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.26it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.39it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.28it/s][A
 11%|█         | 67/608 [00:01<00:11, 45.10it/s][A
 12%|█▏        | 72/608 [00:01<00:11, 44.93it/s][A
 13%|█▎        | 77/608 [00:01<00:11, 45.00it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.95it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.84it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.88it/s][A
 16%|█▌        | 97/608 [00:02<00:12, 41.67it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 42.72it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 43.57it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 44.02it/s][A
 19%|█▉        | 117/608 [00:02<00:11, 44.46it/s][A
 20%|██        | 122/608 [00:02<00:10, 44.58it/s][A
 21%|██        | 127/608 [00:02<00:10, 44.71it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.76it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.39it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.60it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.85it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.03it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.16it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.15it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.14it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.10it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.96it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.57it/s][A
 31%|███       | 187/608 [00:04<00:09, 44.79it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.00it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.18it/s][A
 33%|███▎      | 202/608 [00:04<00:09, 45.09it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.22it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 45.16it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 45.06it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.88it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.84it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 42.72it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 43.56it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.11it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.54it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.76it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.87it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.84it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 44.68it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 44.49it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 44.59it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 44.81it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.97it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 45.14it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.21it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.19it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.13it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 44.95it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 44.74it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 44.83it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 44.90it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.12it/s][A
 55%|█████▌    | 337/608 [00:07<00:05, 45.23it/s][A
 56%|█████▋    | 342/608 [00:07<00:05, 45.22it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 45.22it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 45.20it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 44.89it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.77it/s][A
 60%|██████    | 367/608 [00:08<00:05, 43.05it/s][A
 61%|██████    | 372/608 [00:08<00:05, 43.74it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.21it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.61it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.82it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.88it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.05it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 44.96it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.75it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.76it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.00it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.02it/s][A
 70%|███████   | 427/608 [00:09<00:04, 45.09it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.20it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.24it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.15it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 44.95it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.78it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.79it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.06it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.17it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.17it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.14it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 45.15it/s][A
 80%|████████  | 487/608 [00:10<00:02, 44.92it/s][A
 81%|████████  | 492/608 [00:10<00:02, 44.75it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.75it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 43.52it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.05it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.53it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.84it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.93it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.00it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 44.99it/s][A
 88%|████████▊ | 537/608 [00:11<00:01, 44.81it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.49it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.77it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.93it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.16it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.21it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.25it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.20it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.10it/s][A
 96%|█████████▌| 582/608 [00:12<00:00, 44.87it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 44.64it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 44.85it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 44.99it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 45.20it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 45.20it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 45.20it/s][A 40%|████      | 312/780 [02:17<02:19,  3.35it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:31:20,096 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 10:31:20,384 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:31:25,239 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:31:25,416 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:31:25,502 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:35<1:14:06,  9.52s/it] 40%|████      | 314/780 [02:35<52:36,  6.77s/it]   40%|████      | 315/780 [02:35<37:25,  4.83s/it] 41%|████      | 316/780 [02:36<26:49,  3.47s/it] 41%|████      | 317/780 [02:36<19:24,  2.52s/it] 41%|████      | 318/780 [02:36<14:13,  1.85s/it] 41%|████      | 319/780 [02:37<10:36,  1.38s/it] 41%|████      | 320/780 [02:37<08:05,  1.05s/it] 41%|████      | 321/780 [02:37<06:18,  1.21it/s] 41%|████▏     | 322/780 [02:37<05:04,  1.50it/s] 41%|████▏     | 323/780 [02:38<04:12,  1.81it/s] 42%|████▏     | 324/780 [02:38<03:36,  2.11it/s] 42%|████▏     | 325/780 [02:38<03:24,  2.22it/s] 42%|████▏     | 326/780 [02:39<03:02,  2.49it/s] 42%|████▏     | 327/780 [02:39<02:47,  2.71it/s] 42%|████▏     | 328/780 [02:39<02:36,  2.89it/s] 42%|████▏     | 329/780 [02:40<02:28,  3.03it/s] 42%|████▏     | 330/780 [02:40<02:27,  3.04it/s] 42%|████▏     | 331/780 [02:40<02:22,  3.15it/s] 43%|████▎     | 332/780 [02:40<02:18,  3.23it/s] 43%|████▎     | 333/780 [02:41<02:16,  3.28it/s] 43%|████▎     | 334/780 [02:41<02:14,  3.32it/s] 43%|████▎     | 335/780 [02:41<02:18,  3.21it/s] 43%|████▎     | 336/780 [02:42<02:15,  3.27it/s] 43%|████▎     | 337/780 [02:42<02:13,  3.32it/s] 43%|████▎     | 338/780 [02:42<02:11,  3.37it/s] 43%|████▎     | 339/780 [02:43<02:14,  3.29it/s] 44%|████▎     | 340/780 [02:43<02:40,  2.74it/s] 44%|████▎     | 341/780 [02:43<02:38,  2.78it/s] 44%|████▍     | 342/780 [02:44<02:28,  2.95it/s] 44%|████▍     | 343/780 [02:44<02:21,  3.09it/s] 44%|████▍     | 344/780 [02:44<02:16,  3.20it/s] 44%|████▍     | 345/780 [02:45<02:18,  3.13it/s] 44%|████▍     | 346/780 [02:45<02:14,  3.22it/s] 44%|████▍     | 347/780 [02:45<02:11,  3.30it/s] 45%|████▍     | 348/780 [02:45<02:09,  3.35it/s] 45%|████▍     | 349/780 [02:46<02:07,  3.38it/s] 45%|████▍     | 350/780 [02:46<02:06,  3.41it/s] 45%|████▌     | 351/780 [02:46<02:05,  3.43it/s] 45%|████▌     | 352/780 [02:47<02:04,  3.44it/s] 45%|████▌     | 353/780 [02:47<02:03,  3.45it/s] 45%|████▌     | 354/780 [02:47<02:03,  3.45it/s] 46%|████▌     | 355/780 [02:48<02:02,  3.46it/s] 46%|████▌     | 356/780 [02:48<02:08,  3.31it/s] 46%|████▌     | 357/780 [02:48<02:06,  3.35it/s] 46%|████▌     | 358/780 [02:48<02:04,  3.39it/s] 46%|████▌     | 359/780 [02:49<02:03,  3.41it/s] 46%|████▌     | 360/780 [02:49<02:02,  3.43it/s] 46%|████▋     | 361/780 [02:49<02:01,  3.44it/s] 46%|████▋     | 362/780 [02:50<02:01,  3.45it/s] 47%|████▋     | 363/780 [02:50<02:00,  3.45it/s] 47%|████▋     | 364/780 [02:50<02:00,  3.46it/s] 47%|████▋     | 365/780 [02:50<01:59,  3.46it/s] 47%|████▋     | 366/780 [02:51<01:59,  3.46it/s] 47%|████▋     | 367/780 [02:51<01:59,  3.46it/s] 47%|████▋     | 368/780 [02:51<01:58,  3.47it/s] 47%|████▋     | 369/780 [02:52<02:01,  3.38it/s] 47%|████▋     | 370/780 [02:52<02:00,  3.41it/s] 48%|████▊     | 371/780 [02:52<01:59,  3.43it/s] 48%|████▊     | 372/780 [02:52<01:58,  3.44it/s] 48%|████▊     | 373/780 [02:53<01:58,  3.45it/s] 48%|████▊     | 374/780 [02:53<01:57,  3.45it/s] 48%|████▊     | 375/780 [02:53<01:57,  3.46it/s] 48%|████▊     | 376/780 [02:54<01:56,  3.46it/s] 48%|████▊     | 377/780 [02:54<01:56,  3.46it/s] 48%|████▊     | 378/780 [02:54<01:55,  3.47it/s] 49%|████▊     | 379/780 [02:54<01:55,  3.47it/s] 49%|████▊     | 380/780 [02:55<02:00,  3.32it/s] 49%|████▉     | 381/780 [02:55<01:58,  3.36it/s] 49%|████▉     | 382/780 [02:55<01:57,  3.40it/s] 49%|████▉     | 383/780 [02:56<01:56,  3.42it/s] 49%|████▉     | 384/780 [02:56<01:55,  3.43it/s] 49%|████▉     | 385/780 [02:56<01:54,  3.44it/s] 49%|████▉     | 386/780 [02:57<01:54,  3.45it/s] 50%|████▉     | 387/780 [02:57<01:53,  3.46it/s] 50%|████▉     | 388/780 [02:57<01:53,  3.46it/s] 50%|████▉     | 389/780 [02:57<01:52,  3.47it/s] 50%|█████     | 390/780 [02:58<01:52,  3.47it/s] 50%|█████     | 391/780 [02:58<01:55,  3.36it/s] 50%|█████     | 392/780 [02:58<01:54,  3.39it/s] 50%|█████     | 393/780 [02:59<01:53,  3.42it/s] 51%|█████     | 394/780 [02:59<01:52,  3.43it/s] 51%|█████     | 395/780 [02:59<01:51,  3.44it/s] 51%|█████     | 396/780 [02:59<01:51,  3.45it/s] 51%|█████     | 397/780 [03:00<01:50,  3.46it/s] 51%|█████     | 398/780 [03:00<01:50,  3.46it/s] 51%|█████     | 399/780 [03:00<01:50,  3.46it/s] 51%|█████▏    | 400/780 [03:01<01:49,  3.47it/s] 51%|█████▏    | 401/780 [03:01<01:49,  3.47it/s] 52%|█████▏    | 402/780 [03:01<01:57,  3.22it/s] 52%|█████▏    | 403/780 [03:02<01:54,  3.29it/s] 52%|█████▏    | 404/780 [03:02<01:52,  3.34it/s] 52%|█████▏    | 405/780 [03:02<01:51,  3.38it/s] 52%|█████▏    | 406/780 [03:02<01:49,  3.41it/s] 52%|█████▏    | 407/780 [03:03<01:49,  3.42it/s] 52%|█████▏    | 408/780 [03:03<01:48,  3.44it/s] 52%|█████▏    | 409/780 [03:03<01:47,  3.45it/s] 53%|█████▎    | 410/780 [03:04<01:47,  3.46it/s] 53%|█████▎    | 411/780 [03:04<01:46,  3.46it/s] 53%|█████▎    | 412/780 [03:04<01:46,  3.46it/s] 53%|█████▎    | 413/780 [03:04<01:51,  3.29it/s] 53%|█████▎    | 414/780 [03:05<01:49,  3.34it/s] 53%|█████▎    | 415/780 [03:05<01:47,  3.38it/s] 53%|█████▎    | 416/780 [03:05<01:46,  3.41it/s] 53%|█████▎    | 417/780 [03:06<01:45,  3.43it/s] 54%|█████▎    | 418/780 [03:06<01:45,  3.44it/s] 54%|█████▎    | 419/780 [03:06<01:44,  3.44it/s] 54%|█████▍    | 420/780 [03:07<01:44,  3.45it/s] 54%|█████▍    | 421/780 [03:07<01:43,  3.46it/s] 54%|█████▍    | 422/780 [03:07<01:43,  3.46it/s] 54%|█████▍    | 423/780 [03:07<01:43,  3.46it/s] 54%|█████▍    | 424/780 [03:08<01:46,  3.35it/s] 54%|█████▍    | 425/780 [03:08<01:44,  3.38it/s] 55%|█████▍    | 426/780 [03:08<01:43,  3.40it/s] 55%|█████▍    | 427/780 [03:09<01:43,  3.43it/s] 55%|█████▍    | 428/780 [03:09<01:42,  3.44it/s] 55%|█████▌    | 429/780 [03:09<01:41,  3.45it/s] 55%|█████▌    | 430/780 [03:09<01:41,  3.45it/s] 55%|█████▌    | 431/780 [03:10<01:41,  3.45it/s] 55%|█████▌    | 432/780 [03:10<01:40,  3.46it/s] 56%|█████▌    | 433/780 [03:10<01:40,  3.46it/s] 56%|█████▌    | 434/780 [03:11<01:39,  3.46it/s] 56%|█████▌    | 435/780 [03:11<01:43,  3.33it/s] 56%|█████▌    | 436/780 [03:11<01:42,  3.37it/s] 56%|█████▌    | 437/780 [03:11<01:40,  3.40it/s] 56%|█████▌    | 438/780 [03:12<01:40,  3.42it/s] 56%|█████▋    | 439/780 [03:12<01:39,  3.43it/s] 56%|█████▋    | 440/780 [03:12<01:38,  3.44it/s] 57%|█████▋    | 441/780 [03:13<01:38,  3.45it/s] 57%|█████▋    | 442/780 [03:13<01:37,  3.45it/s] 57%|█████▋    | 443/780 [03:13<01:37,  3.46it/s] 57%|█████▋    | 444/780 [03:14<01:37,  3.46it/s] 57%|█████▋    | 445/780 [03:14<01:36,  3.46it/s] 57%|█████▋    | 446/780 [03:14<01:40,  3.33it/s] 57%|█████▋    | 447/780 [03:14<01:38,  3.37it/s] 57%|█████▋    | 448/780 [03:15<01:37,  3.40it/s] 58%|█████▊    | 449/780 [03:15<01:36,  3.42it/s] 58%|█████▊    | 450/780 [03:15<01:36,  3.43it/s] 58%|█████▊    | 451/780 [03:16<01:35,  3.44it/s] 58%|█████▊    | 452/780 [03:16<01:35,  3.45it/s] 58%|█████▊    | 453/780 [03:16<01:34,  3.46it/s] 58%|█████▊    | 454/780 [03:16<01:34,  3.46it/s] 58%|█████▊    | 455/780 [03:17<01:33,  3.46it/s] 58%|█████▊    | 456/780 [03:17<01:33,  3.46it/s] 59%|█████▊    | 457/780 [03:17<01:37,  3.30it/s] 59%|█████▊    | 458/780 [03:18<01:36,  3.34it/s] 59%|█████▉    | 459/780 [03:18<01:35,  3.38it/s] 59%|█████▉    | 460/780 [03:18<01:34,  3.40it/s] 59%|█████▉    | 461/780 [03:18<01:33,  3.43it/s] 59%|█████▉    | 462/780 [03:19<01:32,  3.44it/s] 59%|█████▉    | 463/780 [03:19<01:31,  3.45it/s] 59%|█████▉    | 464/780 [03:19<01:31,  3.45it/s] 60%|█████▉    | 465/780 [03:20<01:31,  3.46it/s] 60%|█████▉    | 466/780 [03:20<01:30,  3.46it/s] 60%|█████▉    | 467/780 [03:20<01:30,  3.46it/s] 60%|██████    | 468/780 [03:21<01:30,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 10:32:22,931 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:32:22,931 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 10:32:22,931 >>   Batch size = 8
{'eval_loss': 1.0289438962936401, 'eval_runtime': 13.6081, 'eval_samples_per_second': 357.433, 'eval_steps_per_second': 44.679, 'epoch': 2.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 56.45it/s][A
  2%|▏         | 12/608 [00:00<00:12, 48.92it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.18it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.28it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.71it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.33it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.37it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.24it/s][A
  8%|▊         | 47/608 [00:01<00:12, 44.95it/s][A
  9%|▊         | 52/608 [00:01<00:12, 45.35it/s][A
  9%|▉         | 57/608 [00:01<00:12, 45.35it/s][A
 10%|█         | 62/608 [00:01<00:12, 45.22it/s][A
 11%|█         | 67/608 [00:01<00:12, 43.17it/s][A
 12%|█▏        | 72/608 [00:01<00:12, 43.78it/s][A
 13%|█▎        | 77/608 [00:01<00:12, 44.11it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.33it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.56it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.77it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 44.96it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 45.15it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.94it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 44.86it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 44.93it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.04it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.04it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 44.93it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.90it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.08it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.15it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.06it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 44.85it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 44.92it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.07it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.16it/s][A
 29%|██▉       | 177/608 [00:03<00:09, 44.98it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 45.14it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.15it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.16it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.06it/s][A
 33%|███▎      | 202/608 [00:04<00:09, 42.70it/s][A
 34%|███▍      | 207/608 [00:04<00:09, 43.58it/s][A
 35%|███▍      | 212/608 [00:04<00:08, 44.14it/s][A
 36%|███▌      | 217/608 [00:04<00:08, 44.45it/s][A
 37%|███▋      | 222/608 [00:04<00:08, 44.62it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.76it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 44.90it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 44.98it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.78it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.68it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.94it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 45.02it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 45.11it/s][A
 44%|████▍     | 267/608 [00:05<00:07, 45.04it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.20it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.15it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.07it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 44.96it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 45.00it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 44.94it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 45.01it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.09it/s][A
 51%|█████▏    | 312/608 [00:06<00:06, 45.15it/s][A
 52%|█████▏    | 317/608 [00:07<00:06, 45.17it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 45.11it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 45.04it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 45.00it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 42.67it/s][A
 56%|█████▋    | 342/608 [00:07<00:06, 43.42it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 44.01it/s][A
 58%|█████▊    | 352/608 [00:07<00:05, 44.36it/s][A
 59%|█████▊    | 357/608 [00:07<00:05, 44.67it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.82it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.87it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.94it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.66it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 44.77it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 44.87it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 44.99it/s][A
 65%|██████▌   | 397/608 [00:08<00:04, 45.17it/s][A
 66%|██████▌   | 402/608 [00:08<00:04, 45.22it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 45.24it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 45.14it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.07it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 44.79it/s][A
 70%|███████   | 427/608 [00:09<00:04, 44.83it/s][A
 71%|███████   | 432/608 [00:09<00:03, 44.96it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.03it/s][A
 73%|███████▎  | 442/608 [00:09<00:03, 45.19it/s][A
 74%|███████▎  | 447/608 [00:09<00:03, 45.23it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 45.29it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 45.12it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 45.07it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 44.92it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 38.83it/s][A
 78%|███████▊  | 477/608 [00:10<00:03, 40.50it/s][A
 79%|███████▉  | 482/608 [00:10<00:03, 41.96it/s][A
 80%|████████  | 487/608 [00:10<00:02, 43.04it/s][A
 81%|████████  | 492/608 [00:11<00:02, 43.85it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.35it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 44.70it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.74it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.48it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 44.31it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 44.37it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 44.59it/s][A
 88%|████████▊ | 532/608 [00:11<00:01, 44.88it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 45.08it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 45.20it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.36it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.22it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.02it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 44.65it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 44.77it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 44.90it/s][A
 95%|█████████▍| 577/608 [00:12<00:00, 45.07it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 45.20it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.30it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 45.38it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 45.25it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 45.00it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 40.57it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 40.57it/s][A 60%|██████    | 468/780 [03:34<01:30,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:32:37,125 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 10:32:37,478 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:32:42,422 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:32:42,895 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:32:43,084 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:50<46:18,  8.93s/it] 60%|██████    | 470/780 [03:50<32:48,  6.35s/it] 60%|██████    | 471/780 [03:50<23:20,  4.53s/it] 61%|██████    | 472/780 [03:51<16:44,  3.26s/it] 61%|██████    | 473/780 [03:51<12:07,  2.37s/it] 61%|██████    | 474/780 [03:51<08:54,  1.75s/it] 61%|██████    | 475/780 [03:51<06:39,  1.31s/it] 61%|██████    | 476/780 [03:52<05:05,  1.00s/it] 61%|██████    | 477/780 [03:52<03:59,  1.26it/s] 61%|██████▏   | 478/780 [03:52<03:13,  1.56it/s] 61%|██████▏   | 479/780 [03:53<02:41,  1.87it/s] 62%|██████▏   | 480/780 [03:53<02:18,  2.16it/s] 62%|██████▏   | 481/780 [03:53<02:07,  2.35it/s] 62%|██████▏   | 482/780 [03:53<01:54,  2.60it/s] 62%|██████▏   | 483/780 [03:54<01:46,  2.80it/s] 62%|██████▏   | 484/780 [03:54<01:39,  2.96it/s] 62%|██████▏   | 485/780 [03:54<01:35,  3.09it/s] 62%|██████▏   | 486/780 [03:55<01:32,  3.18it/s] 62%|██████▏   | 487/780 [03:55<01:30,  3.25it/s] 63%|██████▎   | 488/780 [03:55<01:28,  3.30it/s] 63%|██████▎   | 489/780 [03:56<01:27,  3.34it/s] 63%|██████▎   | 490/780 [03:56<01:26,  3.36it/s] 63%|██████▎   | 491/780 [03:56<01:25,  3.38it/s] 63%|██████▎   | 492/780 [03:56<01:27,  3.28it/s] 63%|██████▎   | 493/780 [03:57<01:26,  3.32it/s] 63%|██████▎   | 494/780 [03:57<01:25,  3.35it/s] 63%|██████▎   | 495/780 [03:57<01:24,  3.37it/s] 64%|██████▎   | 496/780 [03:58<01:23,  3.39it/s] 64%|██████▎   | 497/780 [03:58<01:23,  3.40it/s] 64%|██████▍   | 498/780 [03:58<01:22,  3.40it/s] 64%|██████▍   | 499/780 [03:58<01:22,  3.41it/s] 64%|██████▍   | 500/780 [03:59<01:22,  3.41it/s]                                                  64%|██████▍   | 500/780 [03:59<01:22,  3.41it/s] 64%|██████▍   | 501/780 [03:59<01:21,  3.41it/s] 64%|██████▍   | 502/780 [03:59<01:21,  3.41it/s] 64%|██████▍   | 503/780 [04:00<01:25,  3.23it/s] 65%|██████▍   | 504/780 [04:00<01:24,  3.28it/s] 65%|██████▍   | 505/780 [04:00<01:22,  3.32it/s] 65%|██████▍   | 506/780 [04:01<01:21,  3.35it/s] 65%|██████▌   | 507/780 [04:01<01:20,  3.37it/s] 65%|██████▌   | 508/780 [04:01<01:20,  3.39it/s] 65%|██████▌   | 509/780 [04:01<01:19,  3.40it/s] 65%|██████▌   | 510/780 [04:02<01:19,  3.40it/s] 66%|██████▌   | 511/780 [04:02<01:18,  3.41it/s] 66%|██████▌   | 512/780 [04:02<01:18,  3.41it/s] 66%|██████▌   | 513/780 [04:03<01:18,  3.41it/s] 66%|██████▌   | 514/780 [04:03<01:20,  3.31it/s] 66%|██████▌   | 515/780 [04:03<01:19,  3.34it/s] 66%|██████▌   | 516/780 [04:04<01:18,  3.36it/s] 66%|██████▋   | 517/780 [04:04<01:17,  3.38it/s] 66%|██████▋   | 518/780 [04:04<01:17,  3.39it/s] 67%|██████▋   | 519/780 [04:04<01:16,  3.40it/s] 67%|██████▋   | 520/780 [04:05<01:16,  3.41it/s] 67%|██████▋   | 521/780 [04:05<01:15,  3.41it/s] 67%|██████▋   | 522/780 [04:05<01:15,  3.42it/s] 67%|██████▋   | 523/780 [04:06<01:15,  3.41it/s] 67%|██████▋   | 524/780 [04:06<01:14,  3.42it/s] 67%|██████▋   | 525/780 [04:06<01:17,  3.28it/s] 67%|██████▋   | 526/780 [04:07<01:16,  3.32it/s] 68%|██████▊   | 527/780 [04:07<01:15,  3.36it/s] 68%|██████▊   | 528/780 [04:07<01:14,  3.37it/s] 68%|██████▊   | 529/780 [04:07<01:14,  3.39it/s] 68%|██████▊   | 530/780 [04:08<01:13,  3.40it/s] 68%|██████▊   | 531/780 [04:08<01:13,  3.40it/s] 68%|██████▊   | 532/780 [04:08<01:12,  3.41it/s] 68%|██████▊   | 533/780 [04:09<01:12,  3.41it/s] 68%|██████▊   | 534/780 [04:09<01:12,  3.41it/s] 69%|██████▊   | 535/780 [04:09<01:11,  3.41it/s] 69%|██████▊   | 536/780 [04:09<01:14,  3.28it/s] 69%|██████▉   | 537/780 [04:10<01:13,  3.32it/s] 69%|██████▉   | 538/780 [04:10<01:12,  3.35it/s] 69%|██████▉   | 539/780 [04:10<01:11,  3.37it/s] 69%|██████▉   | 540/780 [04:11<01:10,  3.38it/s] 69%|██████▉   | 541/780 [04:11<01:10,  3.40it/s] 69%|██████▉   | 542/780 [04:11<01:09,  3.40it/s] 70%|██████▉   | 543/780 [04:12<01:09,  3.41it/s] 70%|██████▉   | 544/780 [04:12<01:09,  3.41it/s] 70%|██████▉   | 545/780 [04:12<01:08,  3.41it/s] 70%|███████   | 546/780 [04:12<01:08,  3.42it/s] 70%|███████   | 547/780 [04:13<01:10,  3.30it/s] 70%|███████   | 548/780 [04:13<01:09,  3.34it/s] 70%|███████   | 549/780 [04:13<01:08,  3.36it/s] 71%|███████   | 550/780 [04:14<01:08,  3.38it/s] 71%|███████   | 551/780 [04:14<01:07,  3.39it/s] 71%|███████   | 552/780 [04:14<01:07,  3.40it/s] 71%|███████   | 553/780 [04:14<01:06,  3.41it/s] 71%|███████   | 554/780 [04:15<01:06,  3.41it/s] 71%|███████   | 555/780 [04:15<01:05,  3.42it/s] 71%|███████▏  | 556/780 [04:15<01:05,  3.42it/s] 71%|███████▏  | 557/780 [04:16<01:05,  3.41it/s] 72%|███████▏  | 558/780 [04:16<01:07,  3.29it/s] 72%|███████▏  | 559/780 [04:16<01:06,  3.33it/s] 72%|███████▏  | 560/780 [04:17<01:05,  3.36it/s] 72%|███████▏  | 561/780 [04:17<01:04,  3.38it/s] 72%|███████▏  | 562/780 [04:17<01:04,  3.39it/s] 72%|███████▏  | 563/780 [04:17<01:03,  3.40it/s] 72%|███████▏  | 564/780 [04:18<01:03,  3.41it/s] 72%|███████▏  | 565/780 [04:18<01:03,  3.41it/s] 73%|███████▎  | 566/780 [04:18<01:02,  3.42it/s] 73%|███████▎  | 567/780 [04:19<01:02,  3.42it/s] 73%|███████▎  | 568/780 [04:19<01:02,  3.42it/s] 73%|███████▎  | 569/780 [04:19<01:03,  3.32it/s] 73%|███████▎  | 570/780 [04:20<01:02,  3.35it/s] 73%|███████▎  | 571/780 [04:20<01:02,  3.37it/s] 73%|███████▎  | 572/780 [04:20<01:01,  3.38it/s] 73%|███████▎  | 573/780 [04:20<01:00,  3.39it/s] 74%|███████▎  | 574/780 [04:21<01:03,  3.26it/s] 74%|███████▎  | 575/780 [04:21<01:01,  3.31it/s] 74%|███████▍  | 576/780 [04:21<01:01,  3.34it/s] 74%|███████▍  | 577/780 [04:22<01:00,  3.37it/s] 74%|███████▍  | 578/780 [04:22<00:59,  3.38it/s] 74%|███████▍  | 579/780 [04:22<00:59,  3.39it/s] 74%|███████▍  | 580/780 [04:22<00:58,  3.40it/s] 74%|███████▍  | 581/780 [04:23<00:58,  3.40it/s] 75%|███████▍  | 582/780 [04:23<00:58,  3.41it/s] 75%|███████▍  | 583/780 [04:23<00:57,  3.41it/s] 75%|███████▍  | 584/780 [04:24<00:57,  3.41it/s] 75%|███████▌  | 585/780 [04:24<00:59,  3.27it/s] 75%|███████▌  | 586/780 [04:24<00:58,  3.31it/s] 75%|███████▌  | 587/780 [04:25<00:57,  3.35it/s] 75%|███████▌  | 588/780 [04:25<00:57,  3.37it/s] 76%|███████▌  | 589/780 [04:25<00:56,  3.37it/s] 76%|███████▌  | 590/780 [04:25<00:56,  3.39it/s] 76%|███████▌  | 591/780 [04:26<00:55,  3.39it/s] 76%|███████▌  | 592/780 [04:26<00:55,  3.40it/s] 76%|███████▌  | 593/780 [04:26<00:54,  3.41it/s] 76%|███████▌  | 594/780 [04:27<00:54,  3.41it/s] 76%|███████▋  | 595/780 [04:27<00:54,  3.41it/s] 76%|███████▋  | 596/780 [04:27<00:56,  3.27it/s] 77%|███████▋  | 597/780 [04:28<00:55,  3.32it/s] 77%|███████▋  | 598/780 [04:28<00:54,  3.35it/s] 77%|███████▋  | 599/780 [04:28<00:53,  3.37it/s] 77%|███████▋  | 600/780 [04:28<00:53,  3.38it/s] 77%|███████▋  | 601/780 [04:29<00:52,  3.39it/s] 77%|███████▋  | 602/780 [04:29<00:52,  3.40it/s] 77%|███████▋  | 603/780 [04:29<00:52,  3.40it/s] 77%|███████▋  | 604/780 [04:30<00:51,  3.41it/s] 78%|███████▊  | 605/780 [04:30<00:51,  3.41it/s] 78%|███████▊  | 606/780 [04:30<00:50,  3.41it/s] 78%|███████▊  | 607/780 [04:31<00:53,  3.23it/s] 78%|███████▊  | 608/780 [04:31<00:52,  3.28it/s] 78%|███████▊  | 609/780 [04:31<00:51,  3.32it/s] 78%|███████▊  | 610/780 [04:31<00:50,  3.35it/s] 78%|███████▊  | 611/780 [04:32<00:50,  3.37it/s] 78%|███████▊  | 612/780 [04:32<00:49,  3.38it/s] 79%|███████▊  | 613/780 [04:32<00:49,  3.39it/s] 79%|███████▊  | 614/780 [04:33<00:48,  3.40it/s] 79%|███████▉  | 615/780 [04:33<00:48,  3.40it/s] 79%|███████▉  | 616/780 [04:33<00:48,  3.41it/s] 79%|███████▉  | 617/780 [04:33<00:47,  3.41it/s] 79%|███████▉  | 618/780 [04:34<00:49,  3.25it/s] 79%|███████▉  | 619/780 [04:34<00:48,  3.31it/s] 79%|███████▉  | 620/780 [04:34<00:47,  3.36it/s] 80%|███████▉  | 621/780 [04:35<00:46,  3.39it/s] 80%|███████▉  | 622/780 [04:35<00:46,  3.41it/s] 80%|███████▉  | 623/780 [04:35<00:45,  3.43it/s] 80%|████████  | 624/780 [04:36<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 10:33:37,944 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:33:37,944 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 10:33:37,944 >>   Batch size = 8
{'eval_loss': 1.0440447330474854, 'eval_runtime': 13.678, 'eval_samples_per_second': 355.607, 'eval_steps_per_second': 44.451, 'epoch': 3.0}
{'loss': 0.4323, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 55.93it/s][A
  2%|▏         | 12/608 [00:00<00:12, 48.59it/s][A
  3%|▎         | 17/608 [00:00<00:12, 46.83it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.12it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.79it/s][A
  5%|▌         | 32/608 [00:00<00:12, 45.30it/s][A
  6%|▌         | 37/608 [00:00<00:12, 45.36it/s][A
  7%|▋         | 42/608 [00:00<00:12, 45.21it/s][A
  8%|▊         | 47/608 [00:01<00:13, 41.89it/s][A
  9%|▊         | 52/608 [00:01<00:14, 39.07it/s][A
  9%|▉         | 57/608 [00:01<00:13, 40.78it/s][A
 10%|█         | 62/608 [00:01<00:12, 42.21it/s][A
 11%|█         | 67/608 [00:01<00:12, 43.06it/s][A
 12%|█▏        | 72/608 [00:01<00:12, 43.84it/s][A
 13%|█▎        | 77/608 [00:01<00:12, 44.23it/s][A
 13%|█▎        | 82/608 [00:01<00:11, 44.61it/s][A
 14%|█▍        | 87/608 [00:01<00:11, 44.43it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 44.22it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 44.46it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 44.72it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.93it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 45.03it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.16it/s][A
 20%|██        | 122/608 [00:02<00:10, 45.27it/s][A
 21%|██        | 127/608 [00:02<00:10, 45.27it/s][A
 22%|██▏       | 132/608 [00:02<00:10, 45.02it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.89it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 44.90it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 44.92it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 44.99it/s][A
 26%|██▌       | 157/608 [00:03<00:10, 45.04it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.20it/s][A
 27%|██▋       | 167/608 [00:03<00:09, 45.26it/s][A
 28%|██▊       | 172/608 [00:03<00:09, 45.23it/s][A
 29%|██▉       | 177/608 [00:04<00:09, 44.98it/s][A
 30%|██▉       | 182/608 [00:04<00:10, 40.15it/s][A
 31%|███       | 187/608 [00:04<00:10, 41.76it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 42.82it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 43.53it/s][A
 33%|███▎      | 202/608 [00:04<00:10, 38.76it/s][A
 34%|███▍      | 207/608 [00:04<00:09, 40.69it/s][A
 35%|███▍      | 212/608 [00:04<00:09, 42.09it/s][A
 36%|███▌      | 217/608 [00:04<00:09, 42.92it/s][A
 37%|███▋      | 222/608 [00:05<00:08, 43.56it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 44.07it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 44.51it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 44.76it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 44.59it/s][A
 41%|████      | 247/608 [00:05<00:08, 44.61it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 44.56it/s][A
 42%|████▏     | 257/608 [00:05<00:07, 44.83it/s][A
 43%|████▎     | 262/608 [00:05<00:07, 44.94it/s][A
 44%|████▍     | 267/608 [00:06<00:07, 44.96it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.05it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.24it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.28it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.06it/s][A
 48%|████▊     | 292/608 [00:06<00:07, 44.76it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 44.78it/s][A
 50%|████▉     | 302/608 [00:06<00:06, 44.87it/s][A
 50%|█████     | 307/608 [00:06<00:06, 45.00it/s][A
 51%|█████▏    | 312/608 [00:07<00:06, 45.11it/s][A
 52%|█████▏    | 317/608 [00:07<00:07, 37.98it/s][A
 53%|█████▎    | 322/608 [00:07<00:07, 39.97it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 41.51it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 42.61it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 43.40it/s][A
 56%|█████▋    | 342/608 [00:07<00:06, 43.95it/s][A
 57%|█████▋    | 347/608 [00:07<00:05, 44.49it/s][A
 58%|█████▊    | 352/608 [00:08<00:05, 44.65it/s][A
 59%|█████▊    | 357/608 [00:08<00:05, 44.12it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.16it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.36it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.71it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 44.74it/s][A
 63%|██████▎   | 382/608 [00:08<00:05, 45.02it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.07it/s][A
 64%|██████▍   | 392/608 [00:08<00:04, 45.21it/s][A
 65%|██████▌   | 397/608 [00:09<00:04, 45.27it/s][A
 66%|██████▌   | 402/608 [00:09<00:04, 44.96it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.88it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.83it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.09it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 44.95it/s][A
 70%|███████   | 427/608 [00:09<00:04, 45.12it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.15it/s][A
 72%|███████▏  | 437/608 [00:09<00:03, 45.26it/s][A
 73%|███████▎  | 442/608 [00:10<00:03, 45.11it/s][A
 74%|███████▎  | 447/608 [00:10<00:03, 45.04it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 42.38it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 43.23it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 43.68it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 44.07it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 44.35it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 44.64it/s][A
 79%|███████▉  | 482/608 [00:10<00:02, 44.79it/s][A
 80%|████████  | 487/608 [00:11<00:02, 44.59it/s][A
 81%|████████  | 492/608 [00:11<00:02, 44.46it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 44.56it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 44.73it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 44.85it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 44.97it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.09it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.16it/s][A
 87%|████████▋ | 527/608 [00:11<00:01, 45.11it/s][A
 88%|████████▊ | 532/608 [00:12<00:01, 45.01it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 44.72it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 44.76it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 44.75it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 44.91it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.13it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.20it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.30it/s][A
 94%|█████████▍| 572/608 [00:12<00:00, 45.19it/s][A
 95%|█████████▍| 577/608 [00:13<00:00, 45.04it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 44.82it/s][A
 97%|█████████▋| 587/608 [00:13<00:01, 19.07it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 23.18it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 27.18it/s][A
 99%|█████████▉| 602/608 [00:14<00:00, 30.90it/s][A
100%|█████████▉| 607/608 [00:14<00:00, 34.22it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:14<00:00, 34.22it/s][A 80%|████████  | 624/780 [04:50<00:45,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:33:52,589 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 10:33:52,846 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:33:56,676 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:33:56,903 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:33:56,994 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [05:03<21:33,  8.34s/it] 80%|████████  | 626/780 [05:03<15:14,  5.94s/it] 80%|████████  | 627/780 [05:03<10:49,  4.24s/it] 81%|████████  | 628/780 [05:04<07:44,  3.06s/it] 81%|████████  | 629/780 [05:04<05:36,  2.23s/it] 81%|████████  | 630/780 [05:04<04:07,  1.65s/it] 81%|████████  | 631/780 [05:04<03:05,  1.24s/it] 81%|████████  | 632/780 [05:05<02:21,  1.04it/s] 81%|████████  | 633/780 [05:05<01:51,  1.32it/s] 81%|████████▏ | 634/780 [05:05<01:30,  1.62it/s] 81%|████████▏ | 635/780 [05:06<01:15,  1.92it/s] 82%|████████▏ | 636/780 [05:06<01:05,  2.21it/s] 82%|████████▏ | 637/780 [05:06<00:59,  2.41it/s] 82%|████████▏ | 638/780 [05:07<00:53,  2.64it/s] 82%|████████▏ | 639/780 [05:07<00:49,  2.83it/s] 82%|████████▏ | 640/780 [05:07<00:46,  2.99it/s] 82%|████████▏ | 641/780 [05:07<00:44,  3.11it/s] 82%|████████▏ | 642/780 [05:08<00:43,  3.20it/s] 82%|████████▏ | 643/780 [05:08<00:42,  3.26it/s] 83%|████████▎ | 644/780 [05:08<00:41,  3.31it/s] 83%|████████▎ | 645/780 [05:09<00:40,  3.36it/s] 83%|████████▎ | 646/780 [05:09<00:39,  3.39it/s] 83%|████████▎ | 647/780 [05:09<00:38,  3.41it/s] 83%|████████▎ | 648/780 [05:09<00:40,  3.30it/s] 83%|████████▎ | 649/780 [05:10<00:39,  3.35it/s] 83%|████████▎ | 650/780 [05:10<00:38,  3.39it/s] 83%|████████▎ | 651/780 [05:10<00:37,  3.41it/s] 84%|████████▎ | 652/780 [05:11<00:37,  3.43it/s] 84%|████████▎ | 653/780 [05:11<00:36,  3.44it/s] 84%|████████▍ | 654/780 [05:11<00:36,  3.45it/s] 84%|████████▍ | 655/780 [05:11<00:36,  3.45it/s] 84%|████████▍ | 656/780 [05:12<00:35,  3.46it/s] 84%|████████▍ | 657/780 [05:12<00:35,  3.46it/s] 84%|████████▍ | 658/780 [05:12<00:35,  3.47it/s] 84%|████████▍ | 659/780 [05:13<00:36,  3.34it/s] 85%|████████▍ | 660/780 [05:13<00:35,  3.38it/s] 85%|████████▍ | 661/780 [05:13<00:34,  3.41it/s] 85%|████████▍ | 662/780 [05:14<00:34,  3.42it/s] 85%|████████▌ | 663/780 [05:14<00:34,  3.44it/s] 85%|████████▌ | 664/780 [05:14<00:33,  3.45it/s] 85%|████████▌ | 665/780 [05:14<00:33,  3.46it/s] 85%|████████▌ | 666/780 [05:15<00:32,  3.46it/s] 86%|████████▌ | 667/780 [05:15<00:32,  3.47it/s] 86%|████████▌ | 668/780 [05:15<00:32,  3.47it/s] 86%|████████▌ | 669/780 [05:16<00:31,  3.47it/s] 86%|████████▌ | 670/780 [05:16<00:33,  3.28it/s] 86%|████████▌ | 671/780 [05:16<00:32,  3.34it/s] 86%|████████▌ | 672/780 [05:16<00:32,  3.37it/s] 86%|████████▋ | 673/780 [05:17<00:31,  3.40it/s] 86%|████████▋ | 674/780 [05:17<00:30,  3.42it/s] 87%|████████▋ | 675/780 [05:17<00:30,  3.44it/s] 87%|████████▋ | 676/780 [05:18<00:30,  3.45it/s] 87%|████████▋ | 677/780 [05:18<00:29,  3.46it/s] 87%|████████▋ | 678/780 [05:18<00:29,  3.46it/s] 87%|████████▋ | 679/780 [05:19<00:29,  3.46it/s] 87%|████████▋ | 680/780 [05:19<00:28,  3.46it/s] 87%|████████▋ | 681/780 [05:19<00:30,  3.27it/s] 87%|████████▋ | 682/780 [05:19<00:29,  3.33it/s] 88%|████████▊ | 683/780 [05:20<00:28,  3.37it/s] 88%|████████▊ | 684/780 [05:20<00:28,  3.40it/s] 88%|████████▊ | 685/780 [05:20<00:27,  3.42it/s] 88%|████████▊ | 686/780 [05:21<00:27,  3.44it/s] 88%|████████▊ | 687/780 [05:21<00:26,  3.45it/s] 88%|████████▊ | 688/780 [05:21<00:26,  3.45it/s] 88%|████████▊ | 689/780 [05:21<00:26,  3.46it/s] 88%|████████▊ | 690/780 [05:22<00:26,  3.46it/s] 89%|████████▊ | 691/780 [05:22<00:25,  3.46it/s] 89%|████████▊ | 692/780 [05:22<00:25,  3.46it/s] 89%|████████▉ | 693/780 [05:23<00:25,  3.46it/s] 89%|████████▉ | 694/780 [05:23<00:24,  3.46it/s] 89%|████████▉ | 695/780 [05:23<00:24,  3.47it/s] 89%|████████▉ | 696/780 [05:23<00:24,  3.47it/s] 89%|████████▉ | 697/780 [05:24<00:23,  3.47it/s] 89%|████████▉ | 698/780 [05:24<00:23,  3.47it/s] 90%|████████▉ | 699/780 [05:24<00:23,  3.47it/s] 90%|████████▉ | 700/780 [05:25<00:23,  3.47it/s] 90%|████████▉ | 701/780 [05:25<00:22,  3.47it/s] 90%|█████████ | 702/780 [05:25<00:23,  3.32it/s] 90%|█████████ | 703/780 [05:26<00:22,  3.37it/s] 90%|█████████ | 704/780 [05:26<00:22,  3.40it/s] 90%|█████████ | 705/780 [05:26<00:21,  3.42it/s] 91%|█████████ | 706/780 [05:26<00:21,  3.43it/s] 91%|█████████ | 707/780 [05:27<00:21,  3.44it/s] 91%|█████████ | 708/780 [05:27<00:20,  3.45it/s] 91%|█████████ | 709/780 [05:27<00:20,  3.46it/s] 91%|█████████ | 710/780 [05:28<00:20,  3.46it/s] 91%|█████████ | 711/780 [05:28<00:19,  3.46it/s] 91%|█████████▏| 712/780 [05:28<00:19,  3.46it/s] 91%|█████████▏| 713/780 [05:28<00:19,  3.37it/s] 92%|█████████▏| 714/780 [05:29<00:19,  3.40it/s] 92%|█████████▏| 715/780 [05:29<00:19,  3.42it/s] 92%|█████████▏| 716/780 [05:29<00:18,  3.43it/s] 92%|█████████▏| 717/780 [05:30<00:18,  3.44it/s] 92%|█████████▏| 718/780 [05:30<00:17,  3.45it/s] 92%|█████████▏| 719/780 [05:30<00:17,  3.45it/s] 92%|█████████▏| 720/780 [05:30<00:17,  3.46it/s] 92%|█████████▏| 721/780 [05:31<00:17,  3.46it/s] 93%|█████████▎| 722/780 [05:31<00:16,  3.46it/s] 93%|█████████▎| 723/780 [05:31<00:16,  3.46it/s] 93%|█████████▎| 724/780 [05:32<00:17,  3.26it/s] 93%|█████████▎| 725/780 [05:32<00:16,  3.32it/s] 93%|█████████▎| 726/780 [05:32<00:16,  3.36it/s] 93%|█████████▎| 727/780 [05:33<00:15,  3.39it/s] 93%|█████████▎| 728/780 [05:33<00:15,  3.41it/s] 93%|█████████▎| 729/780 [05:33<00:14,  3.43it/s] 94%|█████████▎| 730/780 [05:33<00:14,  3.44it/s] 94%|█████████▎| 731/780 [05:34<00:14,  3.44it/s] 94%|█████████▍| 732/780 [05:34<00:13,  3.45it/s] 94%|█████████▍| 733/780 [05:34<00:13,  3.46it/s] 94%|█████████▍| 734/780 [05:35<00:13,  3.46it/s] 94%|█████████▍| 735/780 [05:35<00:13,  3.28it/s] 94%|█████████▍| 736/780 [05:35<00:13,  3.33it/s] 94%|█████████▍| 737/780 [05:35<00:12,  3.37it/s] 95%|█████████▍| 738/780 [05:36<00:12,  3.40it/s] 95%|█████████▍| 739/780 [05:36<00:11,  3.42it/s] 95%|█████████▍| 740/780 [05:36<00:11,  3.43it/s] 95%|█████████▌| 741/780 [05:37<00:11,  3.44it/s] 95%|█████████▌| 742/780 [05:37<00:11,  3.45it/s] 95%|█████████▌| 743/780 [05:37<00:10,  3.45it/s] 95%|█████████▌| 744/780 [05:37<00:10,  3.46it/s] 96%|█████████▌| 745/780 [05:38<00:10,  3.46it/s] 96%|█████████▌| 746/780 [05:38<00:10,  3.14it/s] 96%|█████████▌| 747/780 [05:38<00:10,  3.23it/s] 96%|█████████▌| 748/780 [05:39<00:09,  3.30it/s] 96%|█████████▌| 749/780 [05:39<00:09,  3.35it/s] 96%|█████████▌| 750/780 [05:39<00:08,  3.38it/s] 96%|█████████▋| 751/780 [05:40<00:08,  3.41it/s] 96%|█████████▋| 752/780 [05:40<00:08,  3.43it/s] 97%|█████████▋| 753/780 [05:40<00:08,  3.17it/s] 97%|█████████▋| 754/780 [05:41<00:07,  3.25it/s] 97%|█████████▋| 755/780 [05:41<00:07,  3.32it/s] 97%|█████████▋| 756/780 [05:41<00:07,  3.09it/s] 97%|█████████▋| 757/780 [05:42<00:07,  3.19it/s] 97%|█████████▋| 758/780 [05:42<00:06,  3.27it/s] 97%|█████████▋| 759/780 [05:42<00:06,  3.33it/s] 97%|█████████▋| 760/780 [05:42<00:05,  3.37it/s] 98%|█████████▊| 761/780 [05:43<00:05,  3.40it/s] 98%|█████████▊| 762/780 [05:43<00:05,  3.42it/s] 98%|█████████▊| 763/780 [05:43<00:04,  3.44it/s] 98%|█████████▊| 764/780 [05:44<00:04,  3.44it/s] 98%|█████████▊| 765/780 [05:44<00:04,  3.45it/s] 98%|█████████▊| 766/780 [05:44<00:04,  3.46it/s] 98%|█████████▊| 767/780 [05:44<00:04,  3.19it/s] 98%|█████████▊| 768/780 [05:45<00:03,  3.27it/s] 99%|█████████▊| 769/780 [05:45<00:03,  3.32it/s] 99%|█████████▊| 770/780 [05:45<00:02,  3.36it/s] 99%|█████████▉| 771/780 [05:46<00:02,  3.39it/s] 99%|█████████▉| 772/780 [05:46<00:02,  3.41it/s] 99%|█████████▉| 773/780 [05:46<00:02,  3.43it/s] 99%|█████████▉| 774/780 [05:46<00:01,  3.44it/s] 99%|█████████▉| 775/780 [05:47<00:01,  3.45it/s] 99%|█████████▉| 776/780 [05:47<00:01,  3.45it/s]100%|█████████▉| 777/780 [05:47<00:00,  3.45it/s]100%|█████████▉| 778/780 [05:48<00:00,  3.33it/s]100%|█████████▉| 779/780 [05:48<00:00,  3.37it/s]100%|██████████| 780/780 [05:48<00:00,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 10:34:50,645 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:34:50,645 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 10:34:50,645 >>   Batch size = 8
{'eval_loss': 1.0545063018798828, 'eval_runtime': 14.2743, 'eval_samples_per_second': 340.753, 'eval_steps_per_second': 42.594, 'epoch': 4.0}

  0%|          | 0/608 [00:00<?, ?it/s][A
  1%|          | 6/608 [00:00<00:10, 57.07it/s][A
  2%|▏         | 12/608 [00:00<00:12, 49.21it/s][A
  3%|▎         | 17/608 [00:00<00:12, 47.19it/s][A
  4%|▎         | 22/608 [00:00<00:12, 46.28it/s][A
  4%|▍         | 27/608 [00:00<00:12, 45.93it/s][A
  5%|▌         | 32/608 [00:01<00:28, 20.42it/s][A
  6%|▌         | 37/608 [00:01<00:22, 24.95it/s][A
  7%|▋         | 42/608 [00:01<00:19, 29.10it/s][A
  8%|▊         | 47/608 [00:01<00:17, 32.78it/s][A
  9%|▊         | 52/608 [00:01<00:15, 35.93it/s][A
  9%|▉         | 57/608 [00:01<00:14, 38.40it/s][A
 10%|█         | 62/608 [00:01<00:13, 40.24it/s][A
 11%|█         | 67/608 [00:01<00:12, 41.67it/s][A
 12%|█▏        | 72/608 [00:01<00:12, 42.53it/s][A
 13%|█▎        | 77/608 [00:02<00:12, 43.02it/s][A
 13%|█▎        | 82/608 [00:02<00:12, 40.78it/s][A
 14%|█▍        | 87/608 [00:02<00:12, 42.06it/s][A
 15%|█▌        | 92/608 [00:02<00:11, 43.09it/s][A
 16%|█▌        | 97/608 [00:02<00:11, 43.65it/s][A
 17%|█▋        | 102/608 [00:02<00:11, 44.28it/s][A
 18%|█▊        | 107/608 [00:02<00:11, 44.72it/s][A
 18%|█▊        | 112/608 [00:02<00:11, 45.08it/s][A
 19%|█▉        | 117/608 [00:02<00:10, 45.14it/s][A
 20%|██        | 122/608 [00:03<00:10, 44.77it/s][A
 21%|██        | 127/608 [00:03<00:10, 44.64it/s][A
 22%|██▏       | 132/608 [00:03<00:10, 44.88it/s][A
 23%|██▎       | 137/608 [00:03<00:10, 44.95it/s][A
 23%|██▎       | 142/608 [00:03<00:10, 45.13it/s][A
 24%|██▍       | 147/608 [00:03<00:10, 45.30it/s][A
 25%|██▌       | 152/608 [00:03<00:10, 45.37it/s][A
 26%|██▌       | 157/608 [00:03<00:09, 45.46it/s][A
 27%|██▋       | 162/608 [00:03<00:09, 45.23it/s][A
 27%|██▋       | 167/608 [00:04<00:09, 45.02it/s][A
 28%|██▊       | 172/608 [00:04<00:09, 44.73it/s][A
 29%|██▉       | 177/608 [00:04<00:09, 44.87it/s][A
 30%|██▉       | 182/608 [00:04<00:09, 44.91it/s][A
 31%|███       | 187/608 [00:04<00:09, 45.12it/s][A
 32%|███▏      | 192/608 [00:04<00:09, 45.27it/s][A
 32%|███▏      | 197/608 [00:04<00:09, 45.54it/s][A
 33%|███▎      | 202/608 [00:04<00:08, 45.64it/s][A
 34%|███▍      | 207/608 [00:04<00:08, 45.37it/s][A
 35%|███▍      | 212/608 [00:05<00:08, 45.18it/s][A
 36%|███▌      | 217/608 [00:05<00:08, 44.97it/s][A
 37%|███▋      | 222/608 [00:05<00:08, 44.90it/s][A
 37%|███▋      | 227/608 [00:05<00:08, 45.06it/s][A
 38%|███▊      | 232/608 [00:05<00:08, 45.18it/s][A
 39%|███▉      | 237/608 [00:05<00:08, 45.36it/s][A
 40%|███▉      | 242/608 [00:05<00:08, 45.45it/s][A
 41%|████      | 247/608 [00:05<00:07, 45.50it/s][A
 41%|████▏     | 252/608 [00:05<00:07, 45.46it/s][A
 42%|████▏     | 257/608 [00:06<00:07, 45.18it/s][A
 43%|████▎     | 262/608 [00:06<00:07, 45.06it/s][A
 44%|████▍     | 267/608 [00:06<00:07, 44.92it/s][A
 45%|████▍     | 272/608 [00:06<00:07, 45.03it/s][A
 46%|████▌     | 277/608 [00:06<00:07, 45.02it/s][A
 46%|████▋     | 282/608 [00:06<00:07, 45.29it/s][A
 47%|████▋     | 287/608 [00:06<00:07, 45.47it/s][A
 48%|████▊     | 292/608 [00:06<00:06, 45.44it/s][A
 49%|████▉     | 297/608 [00:06<00:06, 45.39it/s][A
 50%|████▉     | 302/608 [00:07<00:06, 45.24it/s][A
 50%|█████     | 307/608 [00:07<00:06, 45.09it/s][A
 51%|█████▏    | 312/608 [00:07<00:06, 45.04it/s][A
 52%|█████▏    | 317/608 [00:07<00:07, 41.22it/s][A
 53%|█████▎    | 322/608 [00:07<00:06, 42.49it/s][A
 54%|█████▍    | 327/608 [00:07<00:06, 43.44it/s][A
 55%|█████▍    | 332/608 [00:07<00:06, 44.02it/s][A
 55%|█████▌    | 337/608 [00:07<00:06, 44.47it/s][A
 56%|█████▋    | 342/608 [00:08<00:05, 44.75it/s][A
 57%|█████▋    | 347/608 [00:08<00:05, 45.05it/s][A
 58%|█████▊    | 352/608 [00:08<00:05, 45.08it/s][A
 59%|█████▊    | 357/608 [00:08<00:05, 44.65it/s][A
 60%|█████▉    | 362/608 [00:08<00:05, 44.54it/s][A
 60%|██████    | 367/608 [00:08<00:05, 44.81it/s][A
 61%|██████    | 372/608 [00:08<00:05, 44.92it/s][A
 62%|██████▏   | 377/608 [00:08<00:05, 45.11it/s][A
 63%|██████▎   | 382/608 [00:08<00:04, 45.31it/s][A
 64%|██████▎   | 387/608 [00:08<00:04, 45.43it/s][A
 64%|██████▍   | 392/608 [00:09<00:04, 45.40it/s][A
 65%|██████▌   | 397/608 [00:09<00:04, 45.15it/s][A
 66%|██████▌   | 402/608 [00:09<00:04, 44.85it/s][A
 67%|██████▋   | 407/608 [00:09<00:04, 44.85it/s][A
 68%|██████▊   | 412/608 [00:09<00:04, 44.91it/s][A
 69%|██████▊   | 417/608 [00:09<00:04, 45.11it/s][A
 69%|██████▉   | 422/608 [00:09<00:04, 45.22it/s][A
 70%|███████   | 427/608 [00:09<00:03, 45.41it/s][A
 71%|███████   | 432/608 [00:09<00:03, 45.52it/s][A
 72%|███████▏  | 437/608 [00:10<00:03, 45.39it/s][A
 73%|███████▎  | 442/608 [00:10<00:03, 45.23it/s][A
 74%|███████▎  | 447/608 [00:10<00:03, 45.01it/s][A
 74%|███████▍  | 452/608 [00:10<00:03, 44.22it/s][A
 75%|███████▌  | 457/608 [00:10<00:03, 44.55it/s][A
 76%|███████▌  | 462/608 [00:10<00:03, 44.78it/s][A
 77%|███████▋  | 467/608 [00:10<00:03, 45.01it/s][A
 78%|███████▊  | 472/608 [00:10<00:03, 45.24it/s][A
 78%|███████▊  | 477/608 [00:10<00:02, 45.29it/s][A
 79%|███████▉  | 482/608 [00:11<00:02, 45.32it/s][A
 80%|████████  | 487/608 [00:11<00:02, 45.10it/s][A
 81%|████████  | 492/608 [00:11<00:02, 44.89it/s][A
 82%|████████▏ | 497/608 [00:11<00:02, 45.02it/s][A
 83%|████████▎ | 502/608 [00:11<00:02, 45.04it/s][A
 83%|████████▎ | 507/608 [00:11<00:02, 45.18it/s][A
 84%|████████▍ | 512/608 [00:11<00:02, 45.26it/s][A
 85%|████████▌ | 517/608 [00:11<00:02, 45.35it/s][A
 86%|████████▌ | 522/608 [00:11<00:01, 45.40it/s][A
 87%|████████▋ | 527/608 [00:12<00:01, 45.40it/s][A
 88%|████████▊ | 532/608 [00:12<00:01, 45.04it/s][A
 88%|████████▊ | 537/608 [00:12<00:01, 45.03it/s][A
 89%|████████▉ | 542/608 [00:12<00:01, 45.09it/s][A
 90%|████████▉ | 547/608 [00:12<00:01, 45.17it/s][A
 91%|█████████ | 552/608 [00:12<00:01, 45.14it/s][A
 92%|█████████▏| 557/608 [00:12<00:01, 45.26it/s][A
 92%|█████████▏| 562/608 [00:12<00:01, 45.36it/s][A
 93%|█████████▎| 567/608 [00:12<00:00, 45.37it/s][A
 94%|█████████▍| 572/608 [00:13<00:00, 45.44it/s][A
 95%|█████████▍| 577/608 [00:13<00:00, 45.30it/s][A
 96%|█████████▌| 582/608 [00:13<00:00, 45.17it/s][A
 97%|█████████▋| 587/608 [00:13<00:00, 45.15it/s][A
 97%|█████████▋| 592/608 [00:13<00:00, 40.40it/s][A
 98%|█████████▊| 597/608 [00:13<00:00, 41.73it/s][A
 99%|█████████▉| 602/608 [00:13<00:00, 42.87it/s][A
100%|█████████▉| 607/608 [00:13<00:00, 43.58it/s][A
                                                 [A                                                 
100%|██████████| 608/608 [00:13<00:00, 43.58it/s][A100%|██████████| 780/780 [06:02<00:00,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 10:35:04,898 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 10:35:05,078 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:35:08,849 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:35:09,052 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:35:09,179 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 10:35:17,066 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 10:35:17,097 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156 (score: 1.008566975593567).
                                                 100%|██████████| 780/780 [06:26<00:00,  3.39it/s]100%|██████████| 780/780 [06:26<00:00,  2.02it/s]
[INFO|trainer.py:1894] 2023-08-29 10:35:28,579 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 10:35:28,766 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 10:35:32,563 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 10:35:32,756 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 10:35:32,835 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:35:33,473 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:33,473 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:33,473 >>   train_loss               =     0.4249
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:33,473 >>   train_runtime            = 0:06:26.67
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:33,474 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:33,474 >>   train_samples_per_second =    129.309
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:33,474 >>   train_steps_per_second   =      2.017
{'eval_loss': 1.0582475662231445, 'eval_runtime': 13.9614, 'eval_samples_per_second': 348.39, 'eval_steps_per_second': 43.549, 'epoch': 5.0}
{'train_runtime': 386.6708, 'train_samples_per_second': 129.309, 'train_steps_per_second': 2.017, 'train_loss': 0.4249013362786709, 'epoch': 5.0}
08/29/2023 10:35:33 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 10:35:33,829 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 10:35:33,829 >>   Num examples = 4864
[INFO|trainer.py:2145] 2023-08-29 10:35:33,829 >>   Batch size = 8
  0%|          | 0/608 [00:00<?, ?it/s]  1%|          | 6/608 [00:00<00:10, 55.93it/s]  2%|▏         | 12/608 [00:00<00:11, 50.04it/s]  3%|▎         | 18/608 [00:00<00:12, 48.01it/s]  4%|▍         | 23/608 [00:00<00:12, 47.38it/s]  5%|▍         | 28/608 [00:00<00:12, 46.97it/s]  5%|▌         | 33/608 [00:00<00:12, 46.60it/s]  6%|▋         | 38/608 [00:00<00:12, 46.33it/s]  7%|▋         | 43/608 [00:00<00:12, 45.94it/s]  8%|▊         | 48/608 [00:01<00:12, 45.39it/s]  9%|▊         | 53/608 [00:01<00:12, 45.20it/s] 10%|▉         | 58/608 [00:01<00:12, 45.34it/s] 10%|█         | 63/608 [00:01<00:12, 45.40it/s] 11%|█         | 68/608 [00:01<00:11, 45.66it/s] 12%|█▏        | 73/608 [00:01<00:11, 45.69it/s] 13%|█▎        | 78/608 [00:01<00:11, 45.86it/s] 14%|█▎        | 83/608 [00:01<00:11, 45.89it/s] 14%|█▍        | 88/608 [00:01<00:11, 43.43it/s] 15%|█▌        | 93/608 [00:02<00:11, 43.79it/s] 16%|█▌        | 98/608 [00:02<00:11, 44.15it/s] 17%|█▋        | 103/608 [00:02<00:11, 44.49it/s] 18%|█▊        | 108/608 [00:02<00:11, 44.91it/s] 19%|█▊        | 113/608 [00:02<00:10, 45.08it/s] 19%|█▉        | 118/608 [00:02<00:10, 45.39it/s] 20%|██        | 123/608 [00:02<00:10, 45.64it/s] 21%|██        | 128/608 [00:02<00:10, 45.46it/s] 22%|██▏       | 133/608 [00:02<00:10, 45.37it/s] 23%|██▎       | 138/608 [00:03<00:10, 45.29it/s] 24%|██▎       | 143/608 [00:03<00:10, 45.26it/s] 24%|██▍       | 148/608 [00:03<00:10, 45.26it/s] 25%|██▌       | 153/608 [00:03<00:10, 45.37it/s] 26%|██▌       | 158/608 [00:03<00:09, 45.64it/s] 27%|██▋       | 163/608 [00:03<00:09, 45.73it/s] 28%|██▊       | 168/608 [00:03<00:09, 45.76it/s] 28%|██▊       | 173/608 [00:03<00:09, 45.63it/s] 29%|██▉       | 178/608 [00:03<00:09, 45.49it/s] 30%|███       | 183/608 [00:04<00:09, 45.40it/s] 31%|███       | 188/608 [00:04<00:09, 45.32it/s] 32%|███▏      | 193/608 [00:04<00:09, 45.25it/s] 33%|███▎      | 198/608 [00:04<00:09, 45.37it/s] 33%|███▎      | 203/608 [00:04<00:08, 45.41it/s] 34%|███▍      | 208/608 [00:04<00:08, 45.54it/s] 35%|███▌      | 213/608 [00:04<00:08, 45.58it/s] 36%|███▌      | 218/608 [00:04<00:08, 45.64it/s] 37%|███▋      | 223/608 [00:04<00:08, 45.52it/s] 38%|███▊      | 228/608 [00:05<00:09, 41.79it/s] 38%|███▊      | 233/608 [00:05<00:08, 42.90it/s] 39%|███▉      | 238/608 [00:05<00:08, 43.81it/s] 40%|███▉      | 243/608 [00:05<00:09, 39.96it/s] 41%|████      | 248/608 [00:05<00:08, 41.80it/s] 42%|████▏     | 253/608 [00:05<00:08, 42.97it/s] 42%|████▏     | 258/608 [00:05<00:08, 43.70it/s] 43%|████▎     | 263/608 [00:05<00:07, 44.31it/s] 44%|████▍     | 268/608 [00:05<00:07, 44.57it/s] 45%|████▍     | 273/608 [00:06<00:07, 44.64it/s] 46%|████▌     | 278/608 [00:06<00:07, 45.21it/s] 47%|████▋     | 283/608 [00:06<00:07, 45.22it/s] 47%|████▋     | 288/608 [00:06<00:07, 45.13it/s] 48%|████▊     | 293/608 [00:06<00:06, 45.21it/s] 49%|████▉     | 298/608 [00:06<00:06, 45.43it/s] 50%|████▉     | 303/608 [00:06<00:06, 45.45it/s] 51%|█████     | 308/608 [00:06<00:06, 45.55it/s] 51%|█████▏    | 313/608 [00:06<00:06, 45.57it/s] 52%|█████▏    | 318/608 [00:07<00:06, 45.55it/s] 53%|█████▎    | 323/608 [00:07<00:06, 45.52it/s] 54%|█████▍    | 328/608 [00:07<00:06, 45.48it/s] 55%|█████▍    | 333/608 [00:07<00:06, 45.34it/s] 56%|█████▌    | 338/608 [00:07<00:05, 45.38it/s] 56%|█████▋    | 343/608 [00:07<00:05, 45.47it/s] 57%|█████▋    | 348/608 [00:07<00:05, 45.53it/s] 58%|█████▊    | 353/608 [00:07<00:05, 45.50it/s] 59%|█████▉    | 358/608 [00:07<00:05, 45.62it/s] 60%|█████▉    | 363/608 [00:08<00:06, 38.78it/s] 61%|██████    | 368/608 [00:08<00:05, 40.78it/s] 61%|██████▏   | 373/608 [00:08<00:05, 42.21it/s] 62%|██████▏   | 378/608 [00:08<00:05, 43.33it/s] 63%|██████▎   | 383/608 [00:08<00:05, 44.08it/s] 64%|██████▍   | 388/608 [00:08<00:04, 44.68it/s] 65%|██████▍   | 393/608 [00:08<00:05, 39.90it/s] 65%|██████▌   | 398/608 [00:08<00:05, 41.64it/s] 66%|██████▋   | 403/608 [00:09<00:04, 42.49it/s] 67%|██████▋   | 408/608 [00:09<00:04, 43.22it/s] 68%|██████▊   | 413/608 [00:09<00:04, 44.08it/s] 69%|██████▉   | 418/608 [00:09<00:04, 44.68it/s] 70%|██████▉   | 423/608 [00:09<00:04, 45.03it/s] 70%|███████   | 428/608 [00:09<00:03, 45.37it/s] 71%|███████   | 433/608 [00:09<00:03, 45.36it/s] 72%|███████▏  | 438/608 [00:09<00:03, 45.08it/s] 73%|███████▎  | 443/608 [00:09<00:03, 45.01it/s] 74%|███████▎  | 448/608 [00:10<00:03, 44.84it/s] 75%|███████▍  | 453/608 [00:10<00:03, 45.02it/s] 75%|███████▌  | 458/608 [00:10<00:03, 45.21it/s] 76%|███████▌  | 463/608 [00:10<00:03, 45.34it/s] 77%|███████▋  | 468/608 [00:10<00:03, 45.52it/s] 78%|███████▊  | 473/608 [00:10<00:02, 45.67it/s] 79%|███████▊  | 478/608 [00:10<00:02, 45.60it/s] 79%|███████▉  | 483/608 [00:10<00:02, 45.45it/s] 80%|████████  | 488/608 [00:10<00:02, 45.17it/s] 81%|████████  | 493/608 [00:10<00:02, 45.07it/s] 82%|████████▏ | 498/608 [00:11<00:02, 40.17it/s] 83%|████████▎ | 503/608 [00:11<00:02, 41.82it/s] 84%|████████▎ | 508/608 [00:11<00:02, 42.98it/s] 84%|████████▍ | 513/608 [00:11<00:02, 43.75it/s] 85%|████████▌ | 518/608 [00:11<00:02, 44.39it/s] 86%|████████▌ | 523/608 [00:11<00:01, 44.83it/s] 87%|████████▋ | 528/608 [00:11<00:01, 44.94it/s] 88%|████████▊ | 533/608 [00:11<00:01, 45.00it/s] 88%|████████▊ | 538/608 [00:12<00:01, 44.82it/s] 89%|████████▉ | 543/608 [00:12<00:01, 44.70it/s] 90%|█████████ | 548/608 [00:12<00:01, 44.92it/s] 91%|█████████ | 553/608 [00:12<00:01, 45.08it/s] 92%|█████████▏| 558/608 [00:12<00:01, 45.33it/s] 93%|█████████▎| 563/608 [00:12<00:00, 45.55it/s] 93%|█████████▎| 568/608 [00:12<00:00, 45.58it/s] 94%|█████████▍| 573/608 [00:12<00:00, 45.62it/s] 95%|█████████▌| 578/608 [00:12<00:00, 45.35it/s] 96%|█████████▌| 583/608 [00:13<00:00, 45.16it/s] 97%|█████████▋| 588/608 [00:13<00:00, 45.08it/s] 98%|█████████▊| 593/608 [00:13<00:00, 44.99it/s] 98%|█████████▊| 598/608 [00:13<00:00, 45.14it/s] 99%|█████████▉| 603/608 [00:13<00:00, 45.23it/s]100%|██████████| 608/608 [00:13<00:00, 45.52it/s]100%|██████████| 608/608 [00:13<00:00, 44.74it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 10:35:47,437 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:47,437 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:47,437 >>   eval_loss               =     1.0086
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:47,437 >>   eval_runtime            = 0:00:13.60
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:47,437 >>   eval_samples            =       4864
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:47,437 >>   eval_samples_per_second =    357.437
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:47,437 >>   eval_steps_per_second   =      44.68
[INFO|trainer_pt_utils.py:913] 2023-08-29 10:35:47,437 >>   perplexity              =     2.7417
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:35:59,669 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:35:59,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:35:59,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:35:59,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:35:59,703 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:36:00,515 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:36:00,516 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:36:01,274 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:36:02,421 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:36:02,422 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:36:05,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:36:05,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:36:05,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:36:05,804 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:36:05,804 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:36:06,639 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:36:06,640 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:36:07,275 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:36:07,513 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:36:07,514 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/generator/iter5/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/dev.jsonl', 'labels': ['country of citizenship', 'product or material produced', 'said to be the same as', 'student', 'winner'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 15233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.75it/s]Extractor Predicting: 6it [00:03,  1.79it/s]Extractor Predicting: 7it [00:04,  1.74it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.77it/s]Extractor Predicting: 11it [00:06,  1.76it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.79it/s]Extractor Predicting: 14it [00:08,  1.75it/s]Extractor Predicting: 15it [00:08,  1.81it/s]Extractor Predicting: 16it [00:09,  1.80it/s]Extractor Predicting: 17it [00:09,  1.78it/s]Extractor Predicting: 18it [00:10,  1.77it/s]Extractor Predicting: 19it [00:10,  1.73it/s]Extractor Predicting: 20it [00:11,  1.79it/s]Extractor Predicting: 21it [00:11,  1.79it/s]Extractor Predicting: 22it [00:12,  1.77it/s]Extractor Predicting: 23it [00:13,  1.59it/s]Extractor Predicting: 24it [00:13,  1.56it/s]Extractor Predicting: 25it [00:14,  1.54it/s]Extractor Predicting: 26it [00:15,  1.55it/s]Extractor Predicting: 27it [00:15,  1.58it/s]Extractor Predicting: 28it [00:16,  1.57it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:17,  1.64it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:18,  1.68it/s]Extractor Predicting: 33it [00:19,  1.68it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:20,  1.63it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:21,  1.62it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.59it/s]Extractor Predicting: 40it [00:23,  1.61it/s]Extractor Predicting: 41it [00:24,  1.63it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:25,  1.61it/s]Extractor Predicting: 44it [00:26,  1.62it/s]Extractor Predicting: 45it [00:26,  1.61it/s]Extractor Predicting: 46it [00:27,  1.63it/s]Extractor Predicting: 47it [00:28,  1.64it/s]Extractor Predicting: 48it [00:28,  1.65it/s]Extractor Predicting: 49it [00:29,  1.58it/s]Extractor Predicting: 50it [00:30,  1.60it/s]Extractor Predicting: 51it [00:30,  1.60it/s]Extractor Predicting: 52it [00:31,  1.61it/s]Extractor Predicting: 53it [00:31,  1.60it/s]Extractor Predicting: 54it [00:32,  1.62it/s]Extractor Predicting: 55it [00:33,  1.65it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:34,  1.63it/s]Extractor Predicting: 59it [00:35,  1.58it/s]Extractor Predicting: 60it [00:36,  1.58it/s]Extractor Predicting: 61it [00:36,  1.55it/s]Extractor Predicting: 62it [00:37,  1.57it/s]Extractor Predicting: 63it [00:38,  1.59it/s]Extractor Predicting: 64it [00:38,  1.60it/s]Extractor Predicting: 65it [00:39,  1.60it/s]Extractor Predicting: 66it [00:39,  1.63it/s]Extractor Predicting: 67it [00:40,  1.64it/s]Extractor Predicting: 68it [00:41,  1.61it/s]Extractor Predicting: 69it [00:41,  1.59it/s]Extractor Predicting: 70it [00:42,  1.63it/s]Extractor Predicting: 71it [00:42,  1.68it/s]Extractor Predicting: 72it [00:43,  1.68it/s]Extractor Predicting: 73it [00:44,  1.70it/s]Extractor Predicting: 74it [00:44,  1.68it/s]Extractor Predicting: 75it [00:45,  1.69it/s]Extractor Predicting: 76it [00:45,  1.68it/s]Extractor Predicting: 77it [00:46,  1.67it/s]Extractor Predicting: 78it [00:47,  1.68it/s]Extractor Predicting: 79it [00:47,  1.68it/s]Extractor Predicting: 80it [00:48,  1.70it/s]Extractor Predicting: 81it [00:48,  1.68it/s]Extractor Predicting: 82it [00:49,  1.66it/s]Extractor Predicting: 83it [00:50,  1.65it/s]Extractor Predicting: 84it [00:50,  1.65it/s]Extractor Predicting: 85it [00:51,  1.65it/s]Extractor Predicting: 86it [00:51,  1.64it/s]Extractor Predicting: 87it [00:52,  1.66it/s]Extractor Predicting: 88it [00:53,  1.65it/s]Extractor Predicting: 89it [00:53,  1.55it/s]Extractor Predicting: 90it [00:54,  1.61it/s]Extractor Predicting: 91it [00:55,  1.65it/s]Extractor Predicting: 92it [00:55,  1.66it/s]Extractor Predicting: 93it [00:56,  1.67it/s]Extractor Predicting: 94it [00:56,  1.65it/s]Extractor Predicting: 95it [00:57,  1.60it/s]Extractor Predicting: 96it [00:58,  1.63it/s]Extractor Predicting: 97it [00:58,  1.63it/s]Extractor Predicting: 98it [00:59,  1.65it/s]Extractor Predicting: 99it [00:59,  1.66it/s]Extractor Predicting: 100it [01:00,  1.65it/s]Extractor Predicting: 101it [01:01,  1.66it/s]Extractor Predicting: 102it [01:01,  1.68it/s]Extractor Predicting: 103it [01:02,  1.71it/s]Extractor Predicting: 104it [01:02,  1.70it/s]Extractor Predicting: 105it [01:03,  1.64it/s]Extractor Predicting: 106it [01:04,  1.65it/s]Extractor Predicting: 107it [01:04,  1.66it/s]Extractor Predicting: 108it [01:05,  1.69it/s]Extractor Predicting: 109it [01:05,  1.65it/s]Extractor Predicting: 110it [01:06,  1.60it/s]Extractor Predicting: 111it [01:07,  1.63it/s]Extractor Predicting: 112it [01:07,  1.64it/s]Extractor Predicting: 113it [01:08,  1.66it/s]Extractor Predicting: 114it [01:08,  1.70it/s]Extractor Predicting: 115it [01:09,  1.72it/s]Extractor Predicting: 116it [01:10,  1.73it/s]Extractor Predicting: 117it [01:10,  1.72it/s]Extractor Predicting: 118it [01:11,  1.75it/s]Extractor Predicting: 119it [01:11,  1.72it/s]Extractor Predicting: 120it [01:12,  1.75it/s]Extractor Predicting: 121it [01:13,  1.51it/s]Extractor Predicting: 122it [01:13,  1.52it/s]Extractor Predicting: 123it [01:14,  1.52it/s]Extractor Predicting: 124it [01:15,  1.51it/s]Extractor Predicting: 125it [01:15,  1.55it/s]Extractor Predicting: 126it [01:16,  1.60it/s]Extractor Predicting: 127it [01:16,  1.60it/s]Extractor Predicting: 128it [01:17,  1.57it/s]Extractor Predicting: 129it [01:18,  1.58it/s]Extractor Predicting: 130it [01:18,  1.61it/s]Extractor Predicting: 131it [01:19,  1.66it/s]Extractor Predicting: 132it [01:20,  1.61it/s]Extractor Predicting: 133it [01:20,  1.61it/s]Extractor Predicting: 134it [01:21,  1.61it/s]Extractor Predicting: 135it [01:21,  1.64it/s]Extractor Predicting: 136it [01:22,  1.65it/s]Extractor Predicting: 137it [01:23,  1.67it/s]Extractor Predicting: 138it [01:23,  1.65it/s]Extractor Predicting: 139it [01:24,  1.62it/s]Extractor Predicting: 140it [01:24,  1.61it/s]Extractor Predicting: 141it [01:25,  1.66it/s]Extractor Predicting: 142it [01:26,  1.63it/s]Extractor Predicting: 143it [01:26,  1.67it/s]Extractor Predicting: 144it [01:27,  1.65it/s]Extractor Predicting: 145it [01:27,  1.67it/s]Extractor Predicting: 146it [01:28,  1.62it/s]Extractor Predicting: 147it [01:29,  1.60it/s]Extractor Predicting: 148it [01:29,  1.62it/s]Extractor Predicting: 149it [01:30,  1.57it/s]Extractor Predicting: 150it [01:31,  1.60it/s]Extractor Predicting: 151it [01:31,  1.59it/s]Extractor Predicting: 152it [01:32,  1.65it/s]Extractor Predicting: 153it [01:32,  1.63it/s]Extractor Predicting: 154it [01:33,  1.59it/s]Extractor Predicting: 155it [01:34,  1.62it/s]Extractor Predicting: 156it [01:34,  1.66it/s]Extractor Predicting: 157it [01:35,  1.67it/s]Extractor Predicting: 158it [01:35,  1.69it/s]Extractor Predicting: 159it [01:36,  1.67it/s]Extractor Predicting: 160it [01:37,  1.62it/s]Extractor Predicting: 161it [01:37,  1.61it/s]Extractor Predicting: 162it [01:38,  1.60it/s]Extractor Predicting: 163it [01:39,  1.62it/s]Extractor Predicting: 164it [01:39,  1.62it/s]Extractor Predicting: 165it [01:40,  1.64it/s]Extractor Predicting: 166it [01:40,  1.61it/s]Extractor Predicting: 167it [01:41,  1.60it/s]Extractor Predicting: 168it [01:42,  1.58it/s]Extractor Predicting: 169it [01:42,  1.54it/s]Extractor Predicting: 170it [01:43,  1.54it/s]Extractor Predicting: 171it [01:44,  1.54it/s]Extractor Predicting: 172it [01:44,  1.57it/s]Extractor Predicting: 173it [01:45,  1.55it/s]Extractor Predicting: 174it [01:46,  1.52it/s]Extractor Predicting: 175it [01:46,  1.57it/s]Extractor Predicting: 176it [01:47,  1.56it/s]Extractor Predicting: 177it [01:48,  1.53it/s]Extractor Predicting: 178it [01:48,  1.49it/s]Extractor Predicting: 179it [01:49,  1.48it/s]Extractor Predicting: 180it [01:50,  1.49it/s]Extractor Predicting: 181it [01:50,  1.52it/s]Extractor Predicting: 182it [01:51,  1.52it/s]Extractor Predicting: 183it [01:52,  1.49it/s]Extractor Predicting: 184it [01:52,  1.50it/s]Extractor Predicting: 185it [01:53,  1.54it/s]Extractor Predicting: 186it [01:54,  1.55it/s]Extractor Predicting: 187it [01:54,  1.64it/s]Extractor Predicting: 187it [01:54,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:20,008 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:20,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:20,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:20,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:20,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:38:20,950 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:38:20,951 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:38:21,561 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:38:22,730 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:38:22,730 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:25,867 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:25,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:25,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:25,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:38:25,909 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:38:26,764 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:38:26,765 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:38:27,418 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:38:27,654 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:38:27,654 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5955056179775281,
  "recall": 0.08717105263157894,
  "score": 0.15208034433285508,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 28550
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28650, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.62it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.68it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:10,  1.60it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.61it/s]Extractor Predicting: 22it [00:13,  1.53it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.55it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:17,  1.60it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:19,  1.53it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.65it/s]Extractor Predicting: 37it [00:22,  1.61it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:24,  1.64it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.67it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.66it/s]Extractor Predicting: 44it [00:27,  1.64it/s]Extractor Predicting: 45it [00:27,  1.68it/s]Extractor Predicting: 46it [00:28,  1.66it/s]Extractor Predicting: 47it [00:28,  1.61it/s]Extractor Predicting: 48it [00:29,  1.61it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:30,  1.59it/s]Extractor Predicting: 51it [00:31,  1.60it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:32,  1.67it/s]Extractor Predicting: 54it [00:33,  1.64it/s]Extractor Predicting: 55it [00:33,  1.66it/s]Extractor Predicting: 56it [00:34,  1.48it/s]Extractor Predicting: 57it [00:35,  1.50it/s]Extractor Predicting: 58it [00:35,  1.56it/s]Extractor Predicting: 59it [00:36,  1.57it/s]Extractor Predicting: 60it [00:37,  1.62it/s]Extractor Predicting: 61it [00:37,  1.60it/s]Extractor Predicting: 62it [00:38,  1.62it/s]Extractor Predicting: 63it [00:38,  1.63it/s]Extractor Predicting: 64it [00:39,  1.60it/s]Extractor Predicting: 65it [00:40,  1.62it/s]Extractor Predicting: 66it [00:40,  1.65it/s]Extractor Predicting: 67it [00:41,  1.67it/s]Extractor Predicting: 68it [00:42,  1.57it/s]Extractor Predicting: 69it [00:42,  1.59it/s]Extractor Predicting: 70it [00:43,  1.56it/s]Extractor Predicting: 71it [00:43,  1.60it/s]Extractor Predicting: 72it [00:44,  1.61it/s]Extractor Predicting: 73it [00:45,  1.66it/s]Extractor Predicting: 74it [00:45,  1.65it/s]Extractor Predicting: 75it [00:46,  1.66it/s]Extractor Predicting: 76it [00:47,  1.62it/s]Extractor Predicting: 77it [00:47,  1.66it/s]Extractor Predicting: 78it [00:48,  1.70it/s]Extractor Predicting: 79it [00:48,  1.68it/s]Extractor Predicting: 80it [00:49,  1.68it/s]Extractor Predicting: 81it [00:49,  1.68it/s]Extractor Predicting: 82it [00:50,  1.68it/s]Extractor Predicting: 83it [00:51,  1.68it/s]Extractor Predicting: 84it [00:51,  1.64it/s]Extractor Predicting: 85it [00:52,  1.64it/s]Extractor Predicting: 86it [00:52,  1.68it/s]Extractor Predicting: 87it [00:53,  1.72it/s]Extractor Predicting: 88it [00:54,  1.75it/s]Extractor Predicting: 89it [00:54,  1.73it/s]Extractor Predicting: 90it [00:55,  1.68it/s]Extractor Predicting: 91it [00:55,  1.71it/s]Extractor Predicting: 92it [00:56,  1.74it/s]Extractor Predicting: 93it [00:56,  1.72it/s]Extractor Predicting: 94it [00:57,  1.74it/s]Extractor Predicting: 95it [00:58,  1.74it/s]Extractor Predicting: 96it [00:58,  1.76it/s]Extractor Predicting: 97it [00:59,  1.74it/s]Extractor Predicting: 98it [00:59,  1.72it/s]Extractor Predicting: 99it [01:00,  1.70it/s]Extractor Predicting: 100it [01:01,  1.70it/s]Extractor Predicting: 101it [01:01,  1.70it/s]Extractor Predicting: 102it [01:02,  1.71it/s]Extractor Predicting: 103it [01:02,  1.69it/s]Extractor Predicting: 104it [01:03,  1.69it/s]Extractor Predicting: 105it [01:04,  1.66it/s]Extractor Predicting: 106it [01:04,  1.62it/s]Extractor Predicting: 107it [01:05,  1.65it/s]Extractor Predicting: 108it [01:05,  1.66it/s]Extractor Predicting: 109it [01:06,  1.63it/s]Extractor Predicting: 110it [01:07,  1.63it/s]Extractor Predicting: 111it [01:07,  1.59it/s]Extractor Predicting: 112it [01:08,  1.64it/s]Extractor Predicting: 113it [01:08,  1.65it/s]Extractor Predicting: 114it [01:09,  1.67it/s]Extractor Predicting: 115it [01:10,  1.66it/s]Extractor Predicting: 116it [01:10,  1.61it/s]Extractor Predicting: 117it [01:11,  1.63it/s]Extractor Predicting: 118it [01:12,  1.64it/s]Extractor Predicting: 119it [01:12,  1.66it/s]Extractor Predicting: 120it [01:13,  1.64it/s]Extractor Predicting: 121it [01:13,  1.62it/s]Extractor Predicting: 122it [01:14,  1.65it/s]Extractor Predicting: 123it [01:15,  1.66it/s]Extractor Predicting: 124it [01:15,  1.68it/s]Extractor Predicting: 125it [01:16,  1.67it/s]Extractor Predicting: 126it [01:16,  1.67it/s]Extractor Predicting: 127it [01:17,  1.63it/s]Extractor Predicting: 128it [01:18,  1.61it/s]Extractor Predicting: 129it [01:18,  1.60it/s]Extractor Predicting: 130it [01:19,  1.65it/s]Extractor Predicting: 131it [01:19,  1.66it/s]Extractor Predicting: 132it [01:20,  1.57it/s]Extractor Predicting: 133it [01:21,  1.61it/s]Extractor Predicting: 134it [01:21,  1.61it/s]Extractor Predicting: 135it [01:22,  1.64it/s]Extractor Predicting: 136it [01:22,  1.65it/s]Extractor Predicting: 137it [01:23,  1.62it/s]Extractor Predicting: 138it [01:24,  1.62it/s]Extractor Predicting: 139it [01:24,  1.65it/s]Extractor Predicting: 140it [01:25,  1.64it/s]Extractor Predicting: 141it [01:26,  1.64it/s]Extractor Predicting: 142it [01:26,  1.60it/s]Extractor Predicting: 143it [01:27,  1.61it/s]Extractor Predicting: 144it [01:27,  1.62it/s]Extractor Predicting: 145it [01:28,  1.61it/s]Extractor Predicting: 146it [01:29,  1.62it/s]Extractor Predicting: 147it [01:29,  1.62it/s]Extractor Predicting: 148it [01:30,  1.65it/s]Extractor Predicting: 149it [01:31,  1.63it/s]Extractor Predicting: 150it [01:31,  1.58it/s]Extractor Predicting: 151it [01:32,  1.58it/s]Extractor Predicting: 152it [01:32,  1.61it/s]Extractor Predicting: 153it [01:33,  1.66it/s]Extractor Predicting: 154it [01:34,  1.66it/s]Extractor Predicting: 155it [01:34,  1.66it/s]Extractor Predicting: 156it [01:35,  1.64it/s]Extractor Predicting: 157it [01:36,  1.44it/s]Extractor Predicting: 158it [01:36,  1.49it/s]Extractor Predicting: 159it [01:37,  1.52it/s]Extractor Predicting: 160it [01:38,  1.53it/s]Extractor Predicting: 161it [01:38,  1.57it/s]Extractor Predicting: 162it [01:39,  1.58it/s]Extractor Predicting: 163it [01:39,  1.61it/s]Extractor Predicting: 164it [01:40,  1.62it/s]Extractor Predicting: 165it [01:41,  1.60it/s]Extractor Predicting: 166it [01:41,  1.61it/s]Extractor Predicting: 167it [01:42,  1.63it/s]Extractor Predicting: 168it [01:42,  1.61it/s]Extractor Predicting: 169it [01:43,  1.61it/s]Extractor Predicting: 170it [01:44,  1.59it/s]Extractor Predicting: 171it [01:44,  1.66it/s]Extractor Predicting: 172it [01:45,  1.66it/s]Extractor Predicting: 173it [01:46,  1.65it/s]Extractor Predicting: 174it [01:46,  1.64it/s]Extractor Predicting: 175it [01:47,  1.57it/s]Extractor Predicting: 176it [01:47,  1.57it/s]Extractor Predicting: 177it [01:48,  1.61it/s]Extractor Predicting: 178it [01:49,  1.60it/s]Extractor Predicting: 179it [01:49,  1.61it/s]Extractor Predicting: 180it [01:50,  1.55it/s]Extractor Predicting: 181it [01:51,  1.56it/s]Extractor Predicting: 182it [01:51,  1.57it/s]Extractor Predicting: 183it [01:52,  1.57it/s]Extractor Predicting: 184it [01:53,  1.57it/s]Extractor Predicting: 185it [01:53,  1.56it/s]Extractor Predicting: 186it [01:54,  1.58it/s]Extractor Predicting: 187it [01:54,  1.56it/s]Extractor Predicting: 188it [01:55,  1.60it/s]Extractor Predicting: 189it [01:56,  1.61it/s]Extractor Predicting: 190it [01:56,  1.58it/s]Extractor Predicting: 191it [01:57,  1.60it/s]Extractor Predicting: 192it [01:58,  1.60it/s]Extractor Predicting: 193it [01:58,  1.60it/s]Extractor Predicting: 194it [01:59,  1.62it/s]Extractor Predicting: 195it [01:59,  1.67it/s]Extractor Predicting: 196it [02:00,  1.69it/s]Extractor Predicting: 197it [02:01,  1.68it/s]Extractor Predicting: 198it [02:01,  1.68it/s]Extractor Predicting: 199it [02:02,  1.68it/s]Extractor Predicting: 200it [02:02,  1.68it/s]Extractor Predicting: 201it [02:03,  1.67it/s]Extractor Predicting: 202it [02:04,  1.66it/s]Extractor Predicting: 203it [02:04,  1.67it/s]Extractor Predicting: 204it [02:05,  1.66it/s]Extractor Predicting: 205it [02:05,  1.61it/s]Extractor Predicting: 206it [02:06,  1.64it/s]Extractor Predicting: 207it [02:07,  1.63it/s]Extractor Predicting: 208it [02:07,  1.68it/s]Extractor Predicting: 209it [02:08,  1.70it/s]Extractor Predicting: 210it [02:08,  1.67it/s]Extractor Predicting: 211it [02:09,  1.65it/s]Extractor Predicting: 212it [02:10,  1.67it/s]Extractor Predicting: 213it [02:10,  1.64it/s]Extractor Predicting: 214it [02:11,  1.62it/s]Extractor Predicting: 215it [02:11,  1.63it/s]Extractor Predicting: 216it [02:12,  1.58it/s]Extractor Predicting: 217it [02:13,  1.63it/s]Extractor Predicting: 218it [02:13,  1.62it/s]Extractor Predicting: 219it [02:14,  1.61it/s]Extractor Predicting: 220it [02:14,  1.66it/s]Extractor Predicting: 221it [02:15,  1.57it/s]Extractor Predicting: 222it [02:16,  1.58it/s]Extractor Predicting: 223it [02:16,  1.60it/s]Extractor Predicting: 224it [02:17,  1.64it/s]Extractor Predicting: 225it [02:18,  1.67it/s]Extractor Predicting: 226it [02:18,  1.64it/s]Extractor Predicting: 227it [02:19,  1.67it/s]Extractor Predicting: 228it [02:19,  1.68it/s]Extractor Predicting: 229it [02:20,  1.67it/s]Extractor Predicting: 230it [02:21,  1.70it/s]Extractor Predicting: 231it [02:21,  1.70it/s]Extractor Predicting: 232it [02:22,  1.64it/s]Extractor Predicting: 233it [02:22,  1.65it/s]Extractor Predicting: 234it [02:23,  1.63it/s]Extractor Predicting: 235it [02:24,  1.63it/s]Extractor Predicting: 236it [02:24,  1.65it/s]Extractor Predicting: 237it [02:25,  1.60it/s]Extractor Predicting: 238it [02:25,  1.63it/s]Extractor Predicting: 239it [02:26,  1.63it/s]Extractor Predicting: 240it [02:27,  1.66it/s]Extractor Predicting: 241it [02:27,  1.70it/s]Extractor Predicting: 242it [02:28,  1.68it/s]Extractor Predicting: 243it [02:28,  1.69it/s]Extractor Predicting: 244it [02:29,  1.70it/s]Extractor Predicting: 245it [02:30,  1.67it/s]Extractor Predicting: 246it [02:30,  1.69it/s]Extractor Predicting: 247it [02:31,  1.68it/s]Extractor Predicting: 248it [02:31,  1.71it/s]Extractor Predicting: 249it [02:32,  1.70it/s]Extractor Predicting: 250it [02:33,  1.67it/s]Extractor Predicting: 251it [02:33,  1.66it/s]Extractor Predicting: 252it [02:34,  1.70it/s]Extractor Predicting: 253it [02:34,  1.76it/s]Extractor Predicting: 254it [02:35,  1.72it/s]Extractor Predicting: 255it [02:35,  1.72it/s]Extractor Predicting: 256it [02:36,  1.70it/s]Extractor Predicting: 257it [02:37,  1.71it/s]Extractor Predicting: 258it [02:37,  1.72it/s]Extractor Predicting: 259it [02:38,  1.72it/s]Extractor Predicting: 260it [02:38,  1.71it/s]Extractor Predicting: 261it [02:39,  1.68it/s]Extractor Predicting: 262it [02:40,  1.67it/s]Extractor Predicting: 263it [02:40,  1.62it/s]Extractor Predicting: 264it [02:41,  1.67it/s]Extractor Predicting: 265it [02:41,  1.66it/s]Extractor Predicting: 266it [02:42,  1.64it/s]Extractor Predicting: 267it [02:43,  1.63it/s]Extractor Predicting: 268it [02:44,  1.42it/s]Extractor Predicting: 269it [02:44,  1.45it/s]Extractor Predicting: 270it [02:45,  1.51it/s]Extractor Predicting: 271it [02:45,  1.55it/s]Extractor Predicting: 272it [02:46,  1.59it/s]Extractor Predicting: 273it [02:47,  1.60it/s]Extractor Predicting: 274it [02:47,  1.59it/s]Extractor Predicting: 275it [02:48,  1.60it/s]Extractor Predicting: 276it [02:49,  1.64it/s]Extractor Predicting: 277it [02:49,  1.61it/s]Extractor Predicting: 278it [02:50,  1.63it/s]Extractor Predicting: 279it [02:50,  1.61it/s]Extractor Predicting: 280it [02:51,  1.62it/s]Extractor Predicting: 281it [02:52,  1.63it/s]Extractor Predicting: 282it [02:52,  1.59it/s]Extractor Predicting: 283it [02:53,  1.60it/s]Extractor Predicting: 284it [02:53,  1.64it/s]Extractor Predicting: 285it [02:54,  1.66it/s]Extractor Predicting: 286it [02:55,  1.66it/s]Extractor Predicting: 287it [02:55,  1.61it/s]Extractor Predicting: 288it [02:56,  1.62it/s]Extractor Predicting: 289it [02:57,  1.61it/s]Extractor Predicting: 290it [02:57,  1.60it/s]Extractor Predicting: 291it [02:58,  1.61it/s]Extractor Predicting: 292it [02:58,  1.58it/s]Extractor Predicting: 293it [02:59,  1.58it/s]Extractor Predicting: 294it [03:00,  1.56it/s]Extractor Predicting: 295it [03:00,  1.58it/s]Extractor Predicting: 296it [03:01,  1.59it/s]Extractor Predicting: 297it [03:02,  1.59it/s]Extractor Predicting: 298it [03:02,  1.59it/s]Extractor Predicting: 299it [03:03,  1.59it/s]Extractor Predicting: 300it [03:04,  1.52it/s]Extractor Predicting: 301it [03:04,  1.48it/s]Extractor Predicting: 302it [03:05,  1.48it/s]Extractor Predicting: 303it [03:06,  1.51it/s]Extractor Predicting: 304it [03:06,  1.47it/s]Extractor Predicting: 305it [03:07,  1.44it/s]Extractor Predicting: 306it [03:08,  1.51it/s]Extractor Predicting: 307it [03:08,  1.55it/s]Extractor Predicting: 308it [03:09,  1.59it/s]Extractor Predicting: 309it [03:09,  1.66it/s]Extractor Predicting: 310it [03:10,  1.63it/s]Extractor Predicting: 311it [03:11,  1.58it/s]Extractor Predicting: 312it [03:11,  1.60it/s]Extractor Predicting: 313it [03:12,  1.61it/s]Extractor Predicting: 314it [03:13,  1.61it/s]Extractor Predicting: 315it [03:13,  1.63it/s]Extractor Predicting: 316it [03:14,  1.65it/s]Extractor Predicting: 317it [03:14,  1.64it/s]Extractor Predicting: 318it [03:15,  1.65it/s]Extractor Predicting: 319it [03:16,  1.63it/s]Extractor Predicting: 320it [03:16,  1.60it/s]Extractor Predicting: 321it [03:17,  1.63it/s]Extractor Predicting: 322it [03:17,  1.68it/s]Extractor Predicting: 323it [03:18,  1.69it/s]Extractor Predicting: 324it [03:19,  1.70it/s]Extractor Predicting: 325it [03:19,  1.69it/s]Extractor Predicting: 326it [03:20,  1.67it/s]Extractor Predicting: 327it [03:20,  1.67it/s]Extractor Predicting: 328it [03:21,  1.66it/s]Extractor Predicting: 329it [03:22,  1.66it/s]Extractor Predicting: 330it [03:22,  1.65it/s]Extractor Predicting: 331it [03:23,  1.61it/s]Extractor Predicting: 332it [03:23,  1.65it/s]Extractor Predicting: 333it [03:24,  1.64it/s]Extractor Predicting: 334it [03:25,  1.65it/s]Extractor Predicting: 335it [03:25,  1.66it/s]Extractor Predicting: 336it [03:26,  1.67it/s]Extractor Predicting: 337it [03:26,  1.67it/s]Extractor Predicting: 338it [03:27,  1.68it/s]Extractor Predicting: 339it [03:28,  1.68it/s]Extractor Predicting: 340it [03:28,  1.70it/s]Extractor Predicting: 341it [03:29,  1.70it/s]Extractor Predicting: 342it [03:29,  1.65it/s]Extractor Predicting: 343it [03:30,  1.65it/s]Extractor Predicting: 344it [03:31,  1.71it/s]Extractor Predicting: 345it [03:31,  1.73it/s]Extractor Predicting: 346it [03:32,  1.76it/s]Extractor Predicting: 347it [03:32,  1.80it/s]Extractor Predicting: 348it [03:33,  1.77it/s]Extractor Predicting: 349it [03:33,  1.78it/s]Extractor Predicting: 350it [03:34,  1.77it/s]Extractor Predicting: 351it [03:34,  1.73it/s]Extractor Predicting: 352it [03:35,  1.74it/s]Extractor Predicting: 353it [03:36,  1.75it/s]Extractor Predicting: 354it [03:36,  1.75it/s]Extractor Predicting: 355it [03:37,  1.76it/s]Extractor Predicting: 356it [03:37,  1.79it/s]Extractor Predicting: 357it [03:38,  1.73it/s]Extractor Predicting: 358it [03:38,  1.73it/s]Extractor Predicting: 359it [03:39,  1.73it/s]Extractor Predicting: 360it [03:40,  1.72it/s]Extractor Predicting: 361it [03:40,  1.72it/s]Extractor Predicting: 362it [03:41,  1.74it/s]Extractor Predicting: 363it [03:41,  1.73it/s]Extractor Predicting: 364it [03:42,  1.78it/s]Extractor Predicting: 365it [03:43,  1.72it/s]Extractor Predicting: 366it [03:43,  1.68it/s]Extractor Predicting: 367it [03:44,  1.63it/s]Extractor Predicting: 368it [03:44,  1.58it/s]Extractor Predicting: 369it [03:45,  1.63it/s]Extractor Predicting: 370it [03:46,  1.61it/s]Extractor Predicting: 371it [03:46,  1.64it/s]Extractor Predicting: 372it [03:47,  1.66it/s]Extractor Predicting: 373it [03:47,  1.66it/s]Extractor Predicting: 374it [03:48,  1.66it/s]Extractor Predicting: 375it [03:49,  1.65it/s]Extractor Predicting: 376it [03:49,  1.64it/s]Extractor Predicting: 377it [03:50,  1.63it/s]Extractor Predicting: 378it [03:51,  1.40it/s]Extractor Predicting: 379it [03:51,  1.46it/s]Extractor Predicting: 380it [03:52,  1.50it/s]Extractor Predicting: 381it [03:53,  1.56it/s]Extractor Predicting: 382it [03:53,  1.61it/s]Extractor Predicting: 383it [03:54,  1.62it/s]Extractor Predicting: 384it [03:54,  1.65it/s]Extractor Predicting: 385it [03:55,  1.64it/s]Extractor Predicting: 386it [03:56,  1.68it/s]Extractor Predicting: 387it [03:56,  1.68it/s]Extractor Predicting: 388it [03:57,  1.72it/s]Extractor Predicting: 389it [03:57,  1.64it/s]Extractor Predicting: 390it [03:58,  1.62it/s]Extractor Predicting: 391it [03:59,  1.65it/s]Extractor Predicting: 392it [03:59,  1.71it/s]Extractor Predicting: 393it [04:00,  1.74it/s]Extractor Predicting: 394it [04:00,  1.74it/s]Extractor Predicting: 395it [04:01,  1.74it/s]Extractor Predicting: 396it [04:01,  1.76it/s]Extractor Predicting: 397it [04:02,  1.74it/s]Extractor Predicting: 398it [04:03,  1.72it/s]Extractor Predicting: 399it [04:03,  1.73it/s]Extractor Predicting: 400it [04:04,  1.70it/s]Extractor Predicting: 401it [04:04,  1.73it/s]Extractor Predicting: 402it [04:05,  1.73it/s]Extractor Predicting: 403it [04:06,  1.72it/s]Extractor Predicting: 404it [04:06,  1.73it/s]Extractor Predicting: 405it [04:07,  1.69it/s]Extractor Predicting: 406it [04:07,  1.70it/s]Extractor Predicting: 407it [04:08,  1.71it/s]Extractor Predicting: 408it [04:08,  1.70it/s]Extractor Predicting: 409it [04:09,  1.54it/s]Extractor Predicting: 410it [04:10,  1.58it/s]Extractor Predicting: 411it [04:10,  1.60it/s]Extractor Predicting: 412it [04:11,  1.62it/s]Extractor Predicting: 413it [04:12,  1.55it/s]Extractor Predicting: 414it [04:12,  1.52it/s]Extractor Predicting: 415it [04:13,  1.50it/s]Extractor Predicting: 416it [04:14,  1.51it/s]Extractor Predicting: 417it [04:14,  1.53it/s]Extractor Predicting: 418it [04:15,  1.50it/s]Extractor Predicting: 419it [04:16,  1.53it/s]Extractor Predicting: 420it [04:16,  1.52it/s]Extractor Predicting: 421it [04:17,  1.53it/s]Extractor Predicting: 422it [04:18,  1.55it/s]Extractor Predicting: 423it [04:18,  1.55it/s]Extractor Predicting: 424it [04:19,  1.56it/s]Extractor Predicting: 425it [04:20,  1.55it/s]Extractor Predicting: 426it [04:20,  1.55it/s]Extractor Predicting: 427it [04:21,  1.57it/s]Extractor Predicting: 428it [04:22,  1.55it/s]Extractor Predicting: 429it [04:22,  1.58it/s]Extractor Predicting: 430it [04:23,  1.60it/s]Extractor Predicting: 431it [04:23,  1.61it/s]Extractor Predicting: 432it [04:24,  1.61it/s]Extractor Predicting: 433it [04:25,  1.59it/s]Extractor Predicting: 434it [04:25,  1.60it/s]Extractor Predicting: 435it [04:26,  1.62it/s]Extractor Predicting: 436it [04:26,  1.66it/s]Extractor Predicting: 437it [04:27,  1.65it/s]Extractor Predicting: 438it [04:28,  1.64it/s]Extractor Predicting: 439it [04:28,  1.63it/s]Extractor Predicting: 440it [04:29,  1.63it/s]Extractor Predicting: 441it [04:30,  1.62it/s]Extractor Predicting: 442it [04:30,  1.63it/s]Extractor Predicting: 443it [04:31,  1.58it/s]Extractor Predicting: 444it [04:31,  1.59it/s]Extractor Predicting: 445it [04:32,  1.58it/s]Extractor Predicting: 446it [04:33,  1.60it/s]Extractor Predicting: 447it [04:33,  1.61it/s]Extractor Predicting: 448it [04:34,  1.59it/s]Extractor Predicting: 449it [04:35,  1.63it/s]Extractor Predicting: 450it [04:35,  1.66it/s]Extractor Predicting: 451it [04:36,  1.62it/s]Extractor Predicting: 452it [04:36,  1.61it/s]Extractor Predicting: 453it [04:37,  1.61it/s]Extractor Predicting: 454it [04:38,  1.62it/s]Extractor Predicting: 455it [04:38,  1.59it/s]Extractor Predicting: 456it [04:39,  1.59it/s]Extractor Predicting: 457it [04:40,  1.56it/s]Extractor Predicting: 458it [04:40,  1.59it/s]Extractor Predicting: 459it [04:41,  1.59it/s]Extractor Predicting: 460it [04:41,  1.59it/s]Extractor Predicting: 461it [04:42,  1.60it/s]Extractor Predicting: 462it [04:43,  1.61it/s]Extractor Predicting: 463it [04:43,  1.57it/s]Extractor Predicting: 464it [04:44,  1.54it/s]Extractor Predicting: 465it [04:45,  1.54it/s]Extractor Predicting: 466it [04:45,  1.55it/s]Extractor Predicting: 467it [04:46,  1.61it/s]Extractor Predicting: 468it [04:46,  1.63it/s]Extractor Predicting: 469it [04:47,  1.68it/s]Extractor Predicting: 470it [04:48,  1.66it/s]Extractor Predicting: 471it [04:48,  1.62it/s]Extractor Predicting: 472it [04:49,  1.60it/s]Extractor Predicting: 473it [04:50,  1.59it/s]Extractor Predicting: 474it [04:50,  1.58it/s]Extractor Predicting: 475it [04:51,  1.58it/s]Extractor Predicting: 476it [04:51,  1.60it/s]Extractor Predicting: 477it [04:52,  1.56it/s]Extractor Predicting: 478it [04:53,  1.58it/s]Extractor Predicting: 479it [04:53,  1.53it/s]Extractor Predicting: 480it [04:54,  1.50it/s]Extractor Predicting: 481it [04:55,  1.50it/s]Extractor Predicting: 482it [04:55,  1.48it/s]Extractor Predicting: 483it [04:56,  1.47it/s]Extractor Predicting: 484it [04:57,  1.46it/s]Extractor Predicting: 485it [04:58,  1.47it/s]Extractor Predicting: 486it [04:58,  1.51it/s]Extractor Predicting: 487it [04:59,  1.53it/s]Extractor Predicting: 488it [04:59,  1.51it/s]Extractor Predicting: 489it [05:00,  1.54it/s]Extractor Predicting: 490it [05:01,  1.51it/s]Extractor Predicting: 491it [05:01,  1.50it/s]Extractor Predicting: 492it [05:02,  1.47it/s]Extractor Predicting: 493it [05:03,  1.50it/s]Extractor Predicting: 494it [05:03,  1.50it/s]Extractor Predicting: 495it [05:04,  1.50it/s]Extractor Predicting: 496it [05:05,  1.51it/s]Extractor Predicting: 497it [05:05,  1.51it/s]Extractor Predicting: 498it [05:06,  1.53it/s]Extractor Predicting: 499it [05:07,  1.28it/s]Extractor Predicting: 500it [05:08,  1.35it/s]Extractor Predicting: 501it [05:08,  1.40it/s]Extractor Predicting: 502it [05:09,  1.41it/s]Extractor Predicting: 503it [05:10,  1.41it/s]Extractor Predicting: 504it [05:11,  1.40it/s]Extractor Predicting: 505it [05:11,  1.41it/s]Extractor Predicting: 506it [05:12,  1.41it/s]Extractor Predicting: 507it [05:13,  1.45it/s]Extractor Predicting: 508it [05:13,  1.60it/s]Extractor Predicting: 508it [05:13,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:00,896 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:00,995 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:00,995 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:00,995 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:00,995 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 10:44:02,087 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 10:44:02,088 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:44:02,540 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 10:44:03,804 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:44:03,804 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:05,541 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:05,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:05,585 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:05,585 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 10:44:05,585 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 10:44:06,174 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 10:44:06,175 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 10:44:06,528 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 10:44:06,789 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 10:44:06,789 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.191935929301298,
  "recall": 0.0570560709301371,
  "score": 0.08796354891785851,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7738
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7838, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:17,  1.54it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.62it/s]Extractor Predicting: 33it [00:21,  1.64it/s]Extractor Predicting: 34it [00:21,  1.67it/s]Extractor Predicting: 35it [00:22,  1.65it/s]Extractor Predicting: 36it [00:22,  1.65it/s]Extractor Predicting: 37it [00:23,  1.63it/s]Extractor Predicting: 38it [00:24,  1.65it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:25,  1.67it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:26,  1.68it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:27,  1.63it/s]Extractor Predicting: 45it [00:28,  1.62it/s]Extractor Predicting: 46it [00:28,  1.62it/s]Extractor Predicting: 47it [00:29,  1.64it/s]Extractor Predicting: 48it [00:30,  1.63it/s]Extractor Predicting: 49it [00:30,  1.63it/s]Extractor Predicting: 50it [00:31,  1.62it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:32,  1.61it/s]Extractor Predicting: 53it [00:33,  1.61it/s]Extractor Predicting: 54it [00:33,  1.57it/s]Extractor Predicting: 55it [00:34,  1.60it/s]Extractor Predicting: 56it [00:35,  1.63it/s]Extractor Predicting: 57it [00:35,  1.64it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:36,  1.64it/s]Extractor Predicting: 60it [00:37,  1.62it/s]Extractor Predicting: 61it [00:38,  1.63it/s]Extractor Predicting: 62it [00:38,  1.64it/s]Extractor Predicting: 63it [00:39,  1.61it/s]Extractor Predicting: 64it [00:40,  1.62it/s]Extractor Predicting: 65it [00:40,  1.60it/s]Extractor Predicting: 66it [00:41,  1.62it/s]Extractor Predicting: 67it [00:41,  1.59it/s]Extractor Predicting: 68it [00:42,  1.53it/s]Extractor Predicting: 69it [00:43,  1.51it/s]Extractor Predicting: 70it [00:44,  1.52it/s]Extractor Predicting: 71it [00:44,  1.53it/s]Extractor Predicting: 72it [00:45,  1.54it/s]Extractor Predicting: 73it [00:45,  1.52it/s]Extractor Predicting: 74it [00:46,  1.49it/s]Extractor Predicting: 75it [00:47,  1.68it/s]Extractor Predicting: 75it [00:47,  1.59it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.32771535580524347,
  "recall": 0.04408060453400504,
  "score": 0.07770870337477798,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_15_seed_3/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/', 'labels': ['conflict', 'continent', 'field of this occupation', 'field of work', 'founded by', 'given name', 'lyrics by', 'movement', 'owned by', 'performer', 'place of birth', 'producer', 'publisher', 'record label', 'replaces'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_15_seed_3/extractor/iter1/results_single_is_eval_True_limit5000.json'
