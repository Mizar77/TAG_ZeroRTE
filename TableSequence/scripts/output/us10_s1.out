/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_10_seed_1', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 67703
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 67803, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/model', pretrained_wv='outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=67803, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.322, loss:50887.6640
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.074, loss:2516.9839
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.073, loss:2237.6553
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.071, loss:2054.6185
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.077, loss:1996.4093
>> valid entity prec:0.2626, rec:0.4730, f1:0.3377
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 2.581, loss:1921.3064
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 1.061, loss:1833.1752
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 1.043, loss:1584.2607
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 1.046, loss:1484.6630
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 1.049, loss:1462.3469
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4300, rec:0.5285, f1:0.4742
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 1100, avg_time 2.493, loss:1369.8550
g_step 1200, step 1200, avg_time 1.044, loss:1294.8983
g_step 1300, step 1300, avg_time 1.043, loss:1264.5249
g_step 1400, step 1400, avg_time 1.047, loss:1185.7371
g_step 1500, step 1500, avg_time 1.043, loss:1171.0886
>> valid entity prec:0.4174, rec:0.6345, f1:0.5036
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 1600, avg_time 2.501, loss:1131.6799
g_step 1700, step 1700, avg_time 1.042, loss:1080.8947
g_step 1800, step 1800, avg_time 1.038, loss:1094.3710
g_step 1900, step 73, avg_time 1.051, loss:1074.8871
g_step 2000, step 173, avg_time 1.041, loss:1050.1739
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4894, rec:0.4353, f1:0.4608
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 273, avg_time 2.481, loss:1054.6498
g_step 2200, step 373, avg_time 1.046, loss:1064.7617
g_step 2300, step 473, avg_time 1.047, loss:1040.6146
g_step 2400, step 573, avg_time 1.038, loss:1004.4409
g_step 2500, step 673, avg_time 1.043, loss:982.4954
>> valid entity prec:0.4155, rec:0.5986, f1:0.4905
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 773, avg_time 2.484, loss:975.9316
g_step 2700, step 873, avg_time 1.040, loss:947.1365
g_step 2800, step 973, avg_time 1.046, loss:1005.5206
g_step 2900, step 1073, avg_time 1.049, loss:973.7539
g_step 3000, step 1173, avg_time 1.048, loss:917.4748
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4794, rec:0.4162, f1:0.4456
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 1273, avg_time 2.471, loss:946.4016
g_step 3200, step 1373, avg_time 1.048, loss:925.3685
g_step 3300, step 1473, avg_time 1.044, loss:926.1832
g_step 3400, step 1573, avg_time 1.045, loss:914.5696
g_step 3500, step 1673, avg_time 1.048, loss:973.8811
>> valid entity prec:0.4699, rec:0.5397, f1:0.5024
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 1773, avg_time 2.479, loss:907.6806
g_step 3700, step 46, avg_time 1.038, loss:899.3348
g_step 3800, step 146, avg_time 1.042, loss:886.8276
g_step 3900, step 246, avg_time 1.046, loss:902.9618
g_step 4000, step 346, avg_time 1.046, loss:840.3017
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4237, rec:0.5796, f1:0.4895
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 446, avg_time 2.495, loss:913.6331
g_step 4200, step 546, avg_time 1.046, loss:888.4188
g_step 4300, step 646, avg_time 1.041, loss:838.8782
g_step 4400, step 746, avg_time 1.046, loss:879.4985
g_step 4500, step 846, avg_time 1.038, loss:835.1205
>> valid entity prec:0.4448, rec:0.5354, f1:0.4859
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 946, avg_time 2.479, loss:855.5166
g_step 4700, step 1046, avg_time 1.039, loss:833.4559
g_step 4800, step 1146, avg_time 1.052, loss:889.1277
g_step 4900, step 1246, avg_time 1.047, loss:848.4204
g_step 5000, step 1346, avg_time 1.041, loss:862.4946
learning rate was adjusted to 0.0008
>> valid entity prec:0.4289, rec:0.5159, f1:0.4684
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 1446, avg_time 2.491, loss:855.1076
g_step 5200, step 1546, avg_time 1.041, loss:838.9352
g_step 5300, step 1646, avg_time 1.044, loss:842.3073
g_step 5400, step 1746, avg_time 1.046, loss:831.6027
g_step 5500, step 19, avg_time 1.044, loss:801.2710
>> valid entity prec:0.4806, rec:0.5097, f1:0.4947
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 119, avg_time 2.483, loss:800.1016
g_step 5700, step 219, avg_time 1.047, loss:830.9255
g_step 5800, step 319, avg_time 1.047, loss:844.7005
g_step 5900, step 419, avg_time 1.045, loss:809.6327
g_step 6000, step 519, avg_time 1.043, loss:814.6381
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5060, rec:0.4806, f1:0.4930
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 619, avg_time 2.484, loss:774.3755
g_step 6200, step 719, avg_time 1.037, loss:770.4024
g_step 6300, step 819, avg_time 1.044, loss:803.4320
g_step 6400, step 919, avg_time 1.034, loss:776.2223
g_step 6500, step 1019, avg_time 1.049, loss:807.6606
>> valid entity prec:0.5190, rec:0.4900, f1:0.5041
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6600, step 1119, avg_time 2.492, loss:794.6818
g_step 6700, step 1219, avg_time 1.047, loss:800.8989
g_step 6800, step 1319, avg_time 1.044, loss:786.0913
g_step 6900, step 1419, avg_time 1.049, loss:767.8673
g_step 7000, step 1519, avg_time 1.048, loss:764.8988
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4774, rec:0.5208, f1:0.4981
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 1619, avg_time 2.478, loss:791.5806
g_step 7200, step 1719, avg_time 1.032, loss:778.7143
g_step 7300, step 1819, avg_time 1.049, loss:777.8262
g_step 7400, step 92, avg_time 1.037, loss:734.5499
g_step 7500, step 192, avg_time 1.044, loss:741.7508
>> valid entity prec:0.5205, rec:0.5168, f1:0.5186
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 7600, step 292, avg_time 2.494, loss:756.3957
g_step 7700, step 392, avg_time 1.041, loss:736.1971
g_step 7800, step 492, avg_time 1.050, loss:725.1543
g_step 7900, step 592, avg_time 1.044, loss:752.9174
g_step 8000, step 692, avg_time 1.042, loss:748.1852
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4989, rec:0.4903, f1:0.4946
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 792, avg_time 2.483, loss:753.6102
g_step 8200, step 892, avg_time 1.042, loss:762.5632
g_step 8300, step 992, avg_time 1.044, loss:771.1457
g_step 8400, step 1092, avg_time 1.044, loss:743.6985
g_step 8500, step 1192, avg_time 1.047, loss:747.6296
>> valid entity prec:0.4662, rec:0.5753, f1:0.5150
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 1292, avg_time 2.477, loss:745.2733
g_step 8700, step 1392, avg_time 1.050, loss:779.6186
g_step 8800, step 1492, avg_time 1.044, loss:737.5079
g_step 8900, step 1592, avg_time 1.041, loss:720.5248
g_step 9000, step 1692, avg_time 1.048, loss:741.0692
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.5049, rec:0.5043, f1:0.5046
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9100, step 1792, avg_time 2.472, loss:773.6733
g_step 9200, step 65, avg_time 1.043, loss:716.8550
g_step 9300, step 165, avg_time 1.037, loss:729.7554
g_step 9400, step 265, avg_time 1.051, loss:720.0630
g_step 9500, step 365, avg_time 1.049, loss:703.7367
>> valid entity prec:0.4800, rec:0.5350, f1:0.5060
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9600, step 465, avg_time 2.477, loss:729.4041
g_step 9700, step 565, avg_time 1.042, loss:710.0498
g_step 9800, step 665, avg_time 1.054, loss:708.8534
g_step 9900, step 765, avg_time 1.047, loss:715.7722
g_step 10000, step 865, avg_time 1.046, loss:691.4998
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.4999, rec:0.4493, f1:0.4732
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.58s/it]Extractor Predicting: 2it [00:08,  3.55s/it]Extractor Predicting: 3it [00:09,  2.26s/it]Extractor Predicting: 4it [00:09,  1.65s/it]Extractor Predicting: 5it [00:10,  1.32s/it]Extractor Predicting: 6it [00:11,  1.13s/it]Extractor Predicting: 7it [00:11,  1.00it/s]Extractor Predicting: 8it [00:12,  1.10it/s]Extractor Predicting: 9it [00:13,  1.18it/s]Extractor Predicting: 10it [00:14,  1.26it/s]Extractor Predicting: 11it [00:14,  1.28it/s]Extractor Predicting: 12it [00:15,  1.30it/s]Extractor Predicting: 13it [00:16,  1.31it/s]Extractor Predicting: 14it [00:17,  1.32it/s]Extractor Predicting: 15it [00:17,  1.37it/s]Extractor Predicting: 16it [00:18,  1.36it/s]Extractor Predicting: 17it [00:19,  1.39it/s]Extractor Predicting: 18it [00:19,  1.43it/s]Extractor Predicting: 19it [00:20,  1.41it/s]Extractor Predicting: 20it [00:21,  1.41it/s]Extractor Predicting: 21it [00:21,  1.41it/s]Extractor Predicting: 22it [00:22,  1.44it/s]Extractor Predicting: 23it [00:23,  1.43it/s]Extractor Predicting: 24it [00:24,  1.41it/s]Extractor Predicting: 25it [00:24,  1.39it/s]Extractor Predicting: 26it [00:25,  1.37it/s]Extractor Predicting: 27it [00:26,  1.37it/s]Extractor Predicting: 28it [00:26,  1.41it/s]Extractor Predicting: 29it [00:27,  1.39it/s]Extractor Predicting: 30it [00:28,  1.36it/s]Extractor Predicting: 31it [00:29,  1.37it/s]Extractor Predicting: 32it [00:29,  1.38it/s]Extractor Predicting: 33it [00:30,  1.38it/s]Extractor Predicting: 34it [00:31,  1.41it/s]Extractor Predicting: 35it [00:31,  1.42it/s]Extractor Predicting: 36it [00:32,  1.42it/s]Extractor Predicting: 37it [00:33,  1.43it/s]Extractor Predicting: 38it [00:34,  1.44it/s]Extractor Predicting: 39it [00:34,  1.45it/s]Extractor Predicting: 40it [00:35,  1.42it/s]Extractor Predicting: 41it [00:36,  1.43it/s]Extractor Predicting: 42it [00:36,  1.43it/s]Extractor Predicting: 43it [00:37,  1.45it/s]Extractor Predicting: 44it [00:38,  1.46it/s]Extractor Predicting: 45it [00:38,  1.44it/s]Extractor Predicting: 46it [00:39,  1.43it/s]Extractor Predicting: 47it [00:40,  1.42it/s]Extractor Predicting: 48it [00:41,  1.44it/s]Extractor Predicting: 49it [00:41,  1.43it/s]Extractor Predicting: 50it [00:42,  1.44it/s]Extractor Predicting: 51it [00:43,  1.44it/s]Extractor Predicting: 52it [00:43,  1.44it/s]Extractor Predicting: 53it [00:44,  1.45it/s]Extractor Predicting: 54it [00:45,  1.43it/s]Extractor Predicting: 55it [00:45,  1.40it/s]Extractor Predicting: 56it [00:46,  1.34it/s]Extractor Predicting: 57it [00:47,  1.35it/s]Extractor Predicting: 58it [00:48,  1.36it/s]Extractor Predicting: 59it [00:48,  1.40it/s]Extractor Predicting: 60it [00:49,  1.44it/s]Extractor Predicting: 61it [00:50,  1.46it/s]Extractor Predicting: 62it [00:50,  1.44it/s]Extractor Predicting: 63it [00:51,  1.42it/s]Extractor Predicting: 64it [00:52,  1.42it/s]Extractor Predicting: 65it [00:53,  1.43it/s]Extractor Predicting: 66it [00:53,  1.43it/s]Extractor Predicting: 67it [00:54,  1.44it/s]Extractor Predicting: 68it [00:55,  1.44it/s]Extractor Predicting: 69it [00:55,  1.49it/s]Extractor Predicting: 70it [00:56,  1.48it/s]Extractor Predicting: 71it [00:57,  1.50it/s]Extractor Predicting: 72it [00:57,  1.46it/s]Extractor Predicting: 73it [00:58,  1.47it/s]Extractor Predicting: 74it [00:59,  1.46it/s]Extractor Predicting: 75it [00:59,  1.43it/s]Extractor Predicting: 76it [01:00,  1.42it/s]Extractor Predicting: 77it [01:01,  1.44it/s]Extractor Predicting: 78it [01:01,  1.46it/s]Extractor Predicting: 79it [01:02,  1.49it/s]Extractor Predicting: 80it [01:03,  1.47it/s]Extractor Predicting: 81it [01:03,  1.46it/s]Extractor Predicting: 82it [01:04,  1.46it/s]Extractor Predicting: 83it [01:05,  1.47it/s]Extractor Predicting: 84it [01:05,  1.48it/s]Extractor Predicting: 85it [01:06,  1.49it/s]Extractor Predicting: 86it [01:07,  1.49it/s]Extractor Predicting: 87it [01:07,  1.52it/s]Extractor Predicting: 88it [01:08,  1.50it/s]Extractor Predicting: 89it [01:09,  1.51it/s]Extractor Predicting: 90it [01:09,  1.52it/s]Extractor Predicting: 91it [01:10,  1.54it/s]Extractor Predicting: 92it [01:11,  1.58it/s]Extractor Predicting: 93it [01:11,  1.55it/s]Extractor Predicting: 94it [01:12,  1.53it/s]Extractor Predicting: 95it [01:13,  1.53it/s]Extractor Predicting: 96it [01:13,  1.53it/s]Extractor Predicting: 97it [01:14,  1.52it/s]Extractor Predicting: 98it [01:15,  1.51it/s]Extractor Predicting: 99it [01:15,  1.49it/s]Extractor Predicting: 100it [01:16,  1.46it/s]Extractor Predicting: 101it [01:17,  1.46it/s]Extractor Predicting: 102it [01:17,  1.53it/s]Extractor Predicting: 103it [01:18,  1.54it/s]Extractor Predicting: 104it [01:19,  1.53it/s]Extractor Predicting: 105it [01:19,  1.52it/s]Extractor Predicting: 106it [01:20,  1.52it/s]Extractor Predicting: 107it [01:21,  1.52it/s]Extractor Predicting: 108it [01:21,  1.52it/s]Extractor Predicting: 109it [01:22,  1.52it/s]Extractor Predicting: 110it [01:23,  1.52it/s]Extractor Predicting: 111it [01:23,  1.52it/s]Extractor Predicting: 112it [01:24,  1.53it/s]Extractor Predicting: 113it [01:25,  1.57it/s]Extractor Predicting: 114it [01:25,  1.57it/s]Extractor Predicting: 115it [01:26,  1.58it/s]Extractor Predicting: 116it [01:26,  1.54it/s]Extractor Predicting: 117it [01:27,  1.50it/s]Extractor Predicting: 118it [01:28,  1.50it/s]Extractor Predicting: 119it [01:29,  1.46it/s]Extractor Predicting: 120it [01:29,  1.47it/s]Extractor Predicting: 121it [01:30,  1.46it/s]Extractor Predicting: 122it [01:31,  1.44it/s]Extractor Predicting: 123it [01:31,  1.45it/s]Extractor Predicting: 124it [01:32,  1.46it/s]Extractor Predicting: 125it [01:33,  1.48it/s]Extractor Predicting: 126it [01:33,  1.45it/s]Extractor Predicting: 127it [01:34,  1.48it/s]Extractor Predicting: 128it [01:35,  1.48it/s]Extractor Predicting: 129it [01:35,  1.46it/s]Extractor Predicting: 130it [01:36,  1.45it/s]Extractor Predicting: 131it [01:37,  1.47it/s]Extractor Predicting: 132it [01:38,  1.34it/s]Extractor Predicting: 133it [01:38,  1.36it/s]Extractor Predicting: 134it [01:39,  1.36it/s]Extractor Predicting: 135it [01:40,  1.40it/s]Extractor Predicting: 136it [01:40,  1.39it/s]Extractor Predicting: 137it [01:41,  1.40it/s]Extractor Predicting: 138it [01:42,  1.38it/s]Extractor Predicting: 139it [01:43,  1.40it/s]Extractor Predicting: 140it [01:43,  1.39it/s]Extractor Predicting: 141it [01:44,  1.42it/s]Extractor Predicting: 142it [01:45,  1.40it/s]Extractor Predicting: 143it [01:45,  1.41it/s]Extractor Predicting: 144it [01:46,  1.73it/s]Extractor Predicting: 144it [01:46,  1.36it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:23,  1.51it/s]Extractor Predicting: 36it [00:23,  1.52it/s]Extractor Predicting: 37it [00:24,  1.54it/s]Extractor Predicting: 38it [00:25,  1.56it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:26,  1.51it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.50it/s]Extractor Predicting: 44it [00:29,  1.49it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.47it/s]Extractor Predicting: 47it [00:31,  1.49it/s]Extractor Predicting: 48it [00:31,  1.47it/s]Extractor Predicting: 49it [00:32,  1.51it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:34,  1.47it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:35,  1.47it/s]Extractor Predicting: 55it [00:36,  1.51it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:38,  1.37it/s]Extractor Predicting: 58it [00:38,  1.40it/s]Extractor Predicting: 59it [00:39,  1.42it/s]Extractor Predicting: 60it [00:40,  1.43it/s]Extractor Predicting: 61it [00:40,  1.43it/s]Extractor Predicting: 62it [00:41,  1.47it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:44,  1.45it/s]Extractor Predicting: 68it [00:45,  1.45it/s]Extractor Predicting: 69it [00:46,  1.44it/s]Extractor Predicting: 70it [00:46,  1.45it/s]Extractor Predicting: 71it [00:47,  1.45it/s]Extractor Predicting: 72it [00:48,  1.44it/s]Extractor Predicting: 73it [00:48,  1.46it/s]Extractor Predicting: 74it [00:49,  1.46it/s]Extractor Predicting: 75it [00:50,  1.47it/s]Extractor Predicting: 76it [00:51,  1.45it/s]Extractor Predicting: 77it [00:51,  1.49it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:52,  1.52it/s]Extractor Predicting: 80it [00:53,  1.54it/s]Extractor Predicting: 81it [00:54,  1.54it/s]Extractor Predicting: 82it [00:54,  1.53it/s]Extractor Predicting: 83it [00:55,  1.51it/s]Extractor Predicting: 84it [00:56,  1.49it/s]Extractor Predicting: 85it [00:56,  1.45it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:58,  1.44it/s]Extractor Predicting: 88it [00:59,  1.44it/s]Extractor Predicting: 89it [00:59,  1.42it/s]Extractor Predicting: 90it [01:00,  1.42it/s]Extractor Predicting: 91it [01:01,  1.40it/s]Extractor Predicting: 92it [01:01,  1.43it/s]Extractor Predicting: 93it [01:02,  1.44it/s]Extractor Predicting: 94it [01:03,  1.45it/s]Extractor Predicting: 95it [01:03,  1.46it/s]Extractor Predicting: 96it [01:04,  1.43it/s]Extractor Predicting: 97it [01:05,  1.41it/s]Extractor Predicting: 98it [01:06,  1.40it/s]Extractor Predicting: 99it [01:06,  1.43it/s]Extractor Predicting: 100it [01:07,  1.44it/s]Extractor Predicting: 101it [01:08,  1.47it/s]Extractor Predicting: 102it [01:08,  1.44it/s]Extractor Predicting: 103it [01:09,  1.42it/s]Extractor Predicting: 104it [01:10,  1.46it/s]Extractor Predicting: 105it [01:10,  1.44it/s]Extractor Predicting: 106it [01:11,  1.44it/s]Extractor Predicting: 107it [01:12,  1.47it/s]Extractor Predicting: 108it [01:13,  1.44it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:14,  1.43it/s]Extractor Predicting: 111it [01:15,  1.45it/s]Extractor Predicting: 112it [01:15,  1.43it/s]Extractor Predicting: 113it [01:16,  1.45it/s]Extractor Predicting: 114it [01:17,  1.46it/s]Extractor Predicting: 115it [01:17,  1.41it/s]Extractor Predicting: 116it [01:18,  1.44it/s]Extractor Predicting: 117it [01:19,  1.44it/s]Extractor Predicting: 118it [01:19,  1.43it/s]Extractor Predicting: 119it [01:20,  1.43it/s]Extractor Predicting: 120it [01:21,  1.45it/s]Extractor Predicting: 121it [01:22,  1.47it/s]Extractor Predicting: 122it [01:22,  1.47it/s]Extractor Predicting: 123it [01:23,  1.47it/s]Extractor Predicting: 124it [01:24,  1.48it/s]Extractor Predicting: 125it [01:24,  1.51it/s]Extractor Predicting: 126it [01:25,  1.50it/s]Extractor Predicting: 127it [01:26,  1.49it/s]Extractor Predicting: 128it [01:26,  1.51it/s]Extractor Predicting: 129it [01:27,  1.52it/s]Extractor Predicting: 130it [01:28,  1.48it/s]Extractor Predicting: 131it [01:28,  1.53it/s]Extractor Predicting: 132it [01:29,  1.53it/s]Extractor Predicting: 133it [01:29,  1.55it/s]Extractor Predicting: 134it [01:30,  1.51it/s]Extractor Predicting: 135it [01:31,  1.54it/s]Extractor Predicting: 136it [01:31,  1.54it/s]Extractor Predicting: 137it [01:32,  1.56it/s]Extractor Predicting: 138it [01:33,  1.54it/s]Extractor Predicting: 139it [01:33,  1.53it/s]Extractor Predicting: 140it [01:34,  1.54it/s]Extractor Predicting: 141it [01:35,  1.51it/s]Extractor Predicting: 142it [01:35,  1.50it/s]Extractor Predicting: 143it [01:36,  1.51it/s]Extractor Predicting: 144it [01:37,  1.50it/s]Extractor Predicting: 145it [01:37,  1.54it/s]Extractor Predicting: 146it [01:38,  1.56it/s]Extractor Predicting: 147it [01:39,  1.57it/s]Extractor Predicting: 148it [01:39,  1.60it/s]Extractor Predicting: 149it [01:40,  1.59it/s]Extractor Predicting: 150it [01:40,  1.60it/s]Extractor Predicting: 151it [01:41,  1.44it/s]Extractor Predicting: 152it [01:42,  1.50it/s]Extractor Predicting: 153it [01:43,  1.51it/s]Extractor Predicting: 154it [01:43,  1.53it/s]Extractor Predicting: 155it [01:44,  1.59it/s]Extractor Predicting: 156it [01:44,  1.58it/s]Extractor Predicting: 157it [01:45,  1.66it/s]Extractor Predicting: 158it [01:45,  1.69it/s]Extractor Predicting: 159it [01:46,  1.64it/s]Extractor Predicting: 160it [01:47,  1.60it/s]Extractor Predicting: 161it [01:47,  1.59it/s]Extractor Predicting: 162it [01:48,  1.60it/s]Extractor Predicting: 163it [01:49,  1.63it/s]Extractor Predicting: 164it [01:49,  1.63it/s]Extractor Predicting: 165it [01:50,  1.64it/s]Extractor Predicting: 166it [01:50,  1.60it/s]Extractor Predicting: 167it [01:51,  1.63it/s]Extractor Predicting: 168it [01:52,  1.61it/s]Extractor Predicting: 169it [01:52,  1.66it/s]Extractor Predicting: 170it [01:53,  1.64it/s]Extractor Predicting: 171it [01:53,  1.65it/s]Extractor Predicting: 172it [01:54,  1.58it/s]Extractor Predicting: 173it [01:55,  1.56it/s]Extractor Predicting: 174it [01:56,  1.52it/s]Extractor Predicting: 175it [01:56,  1.47it/s]Extractor Predicting: 176it [01:57,  1.47it/s]Extractor Predicting: 177it [01:58,  1.47it/s]Extractor Predicting: 178it [01:58,  1.46it/s]Extractor Predicting: 179it [01:59,  1.46it/s]Extractor Predicting: 180it [02:00,  1.47it/s]Extractor Predicting: 181it [02:00,  1.47it/s]Extractor Predicting: 182it [02:01,  1.46it/s]Extractor Predicting: 183it [02:02,  1.46it/s]Extractor Predicting: 184it [02:02,  1.45it/s]Extractor Predicting: 185it [02:03,  1.43it/s]Extractor Predicting: 186it [02:04,  1.42it/s]Extractor Predicting: 187it [02:05,  1.43it/s]Extractor Predicting: 188it [02:05,  1.43it/s]Extractor Predicting: 189it [02:06,  1.44it/s]Extractor Predicting: 190it [02:07,  1.45it/s]Extractor Predicting: 191it [02:07,  1.42it/s]Extractor Predicting: 192it [02:08,  1.40it/s]Extractor Predicting: 193it [02:09,  1.41it/s]Extractor Predicting: 194it [02:10,  1.41it/s]Extractor Predicting: 195it [02:10,  1.43it/s]Extractor Predicting: 196it [02:11,  1.44it/s]Extractor Predicting: 197it [02:12,  1.43it/s]Extractor Predicting: 198it [02:12,  1.45it/s]Extractor Predicting: 199it [02:13,  1.47it/s]Extractor Predicting: 200it [02:14,  1.44it/s]Extractor Predicting: 201it [02:14,  1.42it/s]Extractor Predicting: 202it [02:15,  1.49it/s]Extractor Predicting: 203it [02:16,  1.47it/s]Extractor Predicting: 204it [02:16,  1.49it/s]Extractor Predicting: 205it [02:17,  1.48it/s]Extractor Predicting: 206it [02:18,  1.47it/s]Extractor Predicting: 207it [02:18,  1.45it/s]Extractor Predicting: 208it [02:19,  1.45it/s]Extractor Predicting: 209it [02:20,  1.40it/s]Extractor Predicting: 210it [02:21,  1.41it/s]Extractor Predicting: 211it [02:21,  1.41it/s]Extractor Predicting: 212it [02:22,  1.41it/s]Extractor Predicting: 213it [02:23,  1.41it/s]Extractor Predicting: 214it [02:23,  1.40it/s]Extractor Predicting: 215it [02:24,  1.43it/s]Extractor Predicting: 216it [02:25,  1.41it/s]Extractor Predicting: 217it [02:26,  1.42it/s]Extractor Predicting: 218it [02:26,  1.43it/s]Extractor Predicting: 219it [02:27,  1.41it/s]Extractor Predicting: 220it [02:28,  1.43it/s]Extractor Predicting: 221it [02:28,  1.40it/s]Extractor Predicting: 222it [02:29,  1.39it/s]Extractor Predicting: 223it [02:30,  1.42it/s]Extractor Predicting: 224it [02:30,  1.43it/s]Extractor Predicting: 225it [02:31,  1.43it/s]Extractor Predicting: 226it [02:32,  1.42it/s]Extractor Predicting: 227it [02:32,  1.46it/s]Extractor Predicting: 228it [02:33,  1.45it/s]Extractor Predicting: 229it [02:34,  1.44it/s]Extractor Predicting: 230it [02:35,  1.49it/s]Extractor Predicting: 231it [02:35,  1.50it/s]Extractor Predicting: 232it [02:36,  1.52it/s]Extractor Predicting: 233it [02:37,  1.50it/s]Extractor Predicting: 234it [02:37,  1.50it/s]Extractor Predicting: 235it [02:38,  1.49it/s]Extractor Predicting: 236it [02:39,  1.47it/s]Extractor Predicting: 237it [02:39,  1.45it/s]Extractor Predicting: 238it [02:40,  1.46it/s]Extractor Predicting: 239it [02:41,  1.45it/s]Extractor Predicting: 240it [02:41,  1.45it/s]Extractor Predicting: 241it [02:42,  1.40it/s]Extractor Predicting: 242it [02:43,  1.44it/s]Extractor Predicting: 243it [02:43,  1.43it/s]Extractor Predicting: 244it [02:44,  1.45it/s]Extractor Predicting: 245it [02:45,  1.47it/s]Extractor Predicting: 246it [02:45,  1.46it/s]Extractor Predicting: 247it [02:46,  1.46it/s]Extractor Predicting: 248it [02:47,  1.46it/s]Extractor Predicting: 249it [02:48,  1.44it/s]Extractor Predicting: 250it [02:48,  1.33it/s]Extractor Predicting: 251it [02:49,  1.38it/s]Extractor Predicting: 252it [02:50,  1.41it/s]Extractor Predicting: 253it [02:51,  1.41it/s]Extractor Predicting: 254it [02:51,  1.43it/s]Extractor Predicting: 255it [02:52,  1.40it/s]Extractor Predicting: 256it [02:53,  1.37it/s]Extractor Predicting: 257it [02:53,  1.37it/s]Extractor Predicting: 258it [02:54,  1.37it/s]Extractor Predicting: 259it [02:55,  1.40it/s]Extractor Predicting: 260it [02:56,  1.39it/s]Extractor Predicting: 261it [02:56,  1.43it/s]Extractor Predicting: 262it [02:57,  1.41it/s]Extractor Predicting: 263it [02:58,  1.39it/s]Extractor Predicting: 264it [02:58,  1.39it/s]Extractor Predicting: 265it [02:59,  1.35it/s]Extractor Predicting: 266it [03:00,  1.35it/s]Extractor Predicting: 267it [03:01,  1.36it/s]Extractor Predicting: 268it [03:01,  1.33it/s]Extractor Predicting: 269it [03:02,  1.36it/s]Extractor Predicting: 270it [03:03,  1.38it/s]Extractor Predicting: 271it [03:04,  1.36it/s]Extractor Predicting: 272it [03:04,  1.37it/s]Extractor Predicting: 273it [03:05,  1.37it/s]Extractor Predicting: 274it [03:06,  1.40it/s]Extractor Predicting: 275it [03:06,  1.42it/s]Extractor Predicting: 276it [03:07,  1.46it/s]Extractor Predicting: 277it [03:08,  1.45it/s]Extractor Predicting: 278it [03:08,  1.45it/s]Extractor Predicting: 279it [03:09,  1.44it/s]Extractor Predicting: 280it [03:10,  1.40it/s]Extractor Predicting: 281it [03:11,  1.41it/s]Extractor Predicting: 282it [03:11,  1.50it/s]Extractor Predicting: 282it [03:11,  1.47it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 5it [00:03,  1.41it/s]
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_10_seed_1', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
train vocab size: 84597
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 84697, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/model', pretrained_wv='outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=84697, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.325, loss:51051.0346
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.083, loss:2759.3865
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.047, loss:2440.4019
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.071, loss:2380.8264
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.046, loss:2373.9528
>> valid entity prec:0.4410, rec:0.6341, f1:0.5202
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 3.008, loss:2230.2280
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 1.053, loss:2213.5840
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 1.066, loss:2096.1188
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 1.061, loss:1841.5427
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 1.070, loss:1904.7906
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4868, rec:0.6938, f1:0.5722
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 1100, avg_time 2.956, loss:1722.3506
g_step 1200, step 1200, avg_time 1.064, loss:1735.9631
g_step 1300, step 1300, avg_time 1.062, loss:1629.3763
g_step 1400, step 1400, avg_time 1.063, loss:1584.7670
g_step 1500, step 1500, avg_time 1.060, loss:1501.6615
>> valid entity prec:0.5257, rec:0.6401, f1:0.5773
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 1600, avg_time 2.917, loss:1478.9400
g_step 1700, step 1700, avg_time 1.055, loss:1536.7136
g_step 1800, step 1800, avg_time 1.058, loss:1416.1298
g_step 1900, step 1900, avg_time 1.058, loss:1417.9126
g_step 2000, step 2000, avg_time 1.057, loss:1400.2988
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5025, rec:0.7951, f1:0.6159
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 2100, avg_time 2.930, loss:1400.1357
g_step 2200, step 2200, avg_time 1.060, loss:1454.4778
g_step 2300, step 2300, avg_time 1.061, loss:1333.5915
g_step 2400, step 2400, avg_time 1.036, loss:1378.9749
g_step 2500, step 2500, avg_time 1.056, loss:1317.9676
>> valid entity prec:0.4787, rec:0.7077, f1:0.5711
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 2600, avg_time 2.913, loss:1328.2596
g_step 2700, step 2700, avg_time 1.046, loss:1327.4837
g_step 2800, step 2800, avg_time 1.045, loss:1291.2136
g_step 2900, step 2900, avg_time 1.039, loss:1258.7493
g_step 3000, step 3000, avg_time 1.037, loss:1288.2813
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4958, rec:0.7339, f1:0.5918
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 3100, avg_time 2.907, loss:1272.0081
g_step 3200, step 3200, avg_time 1.047, loss:1281.7338
g_step 3300, step 23, avg_time 1.076, loss:1245.0799
g_step 3400, step 123, avg_time 1.034, loss:1222.6817
g_step 3500, step 223, avg_time 1.055, loss:1293.2443
>> valid entity prec:0.5399, rec:0.7163, f1:0.6157
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 323, avg_time 2.893, loss:1225.4573
g_step 3700, step 423, avg_time 1.052, loss:1174.4254
g_step 3800, step 523, avg_time 1.053, loss:1196.3688
g_step 3900, step 623, avg_time 1.055, loss:1182.9573
g_step 4000, step 723, avg_time 1.036, loss:1175.1764
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4944, rec:0.7094, f1:0.5827
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 823, avg_time 2.905, loss:1189.7661
g_step 4200, step 923, avg_time 1.052, loss:1149.8964
g_step 4300, step 1023, avg_time 1.051, loss:1194.1363
g_step 4400, step 1123, avg_time 1.042, loss:1182.6938
g_step 4500, step 1223, avg_time 1.048, loss:1198.7430
>> valid entity prec:0.4577, rec:0.6474, f1:0.5363
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 1323, avg_time 2.917, loss:1156.3356
g_step 4700, step 1423, avg_time 1.041, loss:1129.4465
g_step 4800, step 1523, avg_time 1.050, loss:1109.6499
g_step 4900, step 1623, avg_time 1.039, loss:1132.1795
g_step 5000, step 1723, avg_time 1.037, loss:1138.8614
learning rate was adjusted to 0.0008
>> valid entity prec:0.5419, rec:0.6580, f1:0.5944
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 1823, avg_time 2.919, loss:1201.2232
g_step 5200, step 1923, avg_time 1.050, loss:1122.7081
g_step 5300, step 2023, avg_time 1.054, loss:1118.0667
g_step 5400, step 2123, avg_time 1.051, loss:1154.3394
g_step 5500, step 2223, avg_time 1.037, loss:1090.8054
>> valid entity prec:0.5019, rec:0.6527, f1:0.5674
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 2323, avg_time 2.907, loss:1131.8269
g_step 5700, step 2423, avg_time 1.046, loss:1146.1115
g_step 5800, step 2523, avg_time 1.043, loss:1106.5170
g_step 5900, step 2623, avg_time 1.053, loss:1176.8666
g_step 6000, step 2723, avg_time 1.051, loss:1118.3613
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4557, rec:0.6131, f1:0.5228
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 2823, avg_time 2.908, loss:1080.6301
g_step 6200, step 2923, avg_time 1.044, loss:1066.9991
g_step 6300, step 3023, avg_time 1.054, loss:1091.3648
g_step 6400, step 3123, avg_time 1.042, loss:1064.7183
g_step 6500, step 3223, avg_time 1.042, loss:1077.4259
>> valid entity prec:0.5794, rec:0.5013, f1:0.5375
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 46, avg_time 2.880, loss:1137.9560
g_step 6700, step 146, avg_time 1.058, loss:1059.6850
g_step 6800, step 246, avg_time 1.035, loss:1086.6237
g_step 6900, step 346, avg_time 1.048, loss:1064.6440
g_step 7000, step 446, avg_time 1.038, loss:1079.2556
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5300, rec:0.5422, f1:0.5361
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 546, avg_time 2.889, loss:1025.6440
g_step 7200, step 646, avg_time 1.043, loss:1059.6048
g_step 7300, step 746, avg_time 1.071, loss:1099.2581
g_step 7400, step 846, avg_time 1.054, loss:1047.1963
g_step 7500, step 946, avg_time 1.045, loss:1085.6431
>> valid entity prec:0.4630, rec:0.7634, f1:0.5764
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 1046, avg_time 2.921, loss:1051.4375
g_step 7700, step 1146, avg_time 1.044, loss:1051.4020
g_step 7800, step 1246, avg_time 1.037, loss:1045.8981
g_step 7900, step 1346, avg_time 1.050, loss:1073.3525
g_step 8000, step 1446, avg_time 1.042, loss:1039.5396
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5277, rec:0.6144, f1:0.5677
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 1546, avg_time 2.895, loss:1009.1333
g_step 8200, step 1646, avg_time 1.043, loss:1030.5439
g_step 8300, step 1746, avg_time 1.036, loss:1071.1834
g_step 8400, step 1846, avg_time 1.049, loss:993.1948
g_step 8500, step 1946, avg_time 1.061, loss:1011.5029
>> valid entity prec:0.5485, rec:0.5879, f1:0.5675
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 2046, avg_time 2.903, loss:1017.6835
g_step 8700, step 2146, avg_time 1.043, loss:1019.0089
g_step 8800, step 2246, avg_time 1.040, loss:1045.6533
g_step 8900, step 2346, avg_time 1.042, loss:1050.7837
g_step 9000, step 2446, avg_time 1.050, loss:1026.7072
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.5148, rec:0.6679, f1:0.5815
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9100, step 2546, avg_time 2.898, loss:964.5096
g_step 9200, step 2646, avg_time 1.059, loss:1020.5022
g_step 9300, step 2746, avg_time 1.037, loss:1036.9320
g_step 9400, step 2846, avg_time 1.045, loss:1043.4046
g_step 9500, step 2946, avg_time 1.042, loss:1081.8858
>> valid entity prec:0.5176, rec:0.6208, f1:0.5645
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9600, step 3046, avg_time 2.890, loss:1037.0669
g_step 9700, step 3146, avg_time 1.046, loss:989.9323
g_step 9800, step 3246, avg_time 1.055, loss:1017.3996
g_step 9900, step 69, avg_time 1.045, loss:1021.3010
g_step 10000, step 169, avg_time 1.046, loss:1009.7217
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.5070, rec:0.5992, f1:0.5493
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13345
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13445, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.27s/it]Extractor Predicting: 2it [00:07,  3.38s/it]Extractor Predicting: 3it [00:08,  2.15s/it]Extractor Predicting: 4it [00:09,  1.57s/it]Extractor Predicting: 5it [00:09,  1.26s/it]Extractor Predicting: 6it [00:11,  1.24s/it]Extractor Predicting: 7it [00:12,  1.23s/it]Extractor Predicting: 8it [00:13,  1.05s/it]Extractor Predicting: 9it [00:13,  1.09it/s]Extractor Predicting: 10it [00:14,  1.22it/s]Extractor Predicting: 11it [00:14,  1.28it/s]Extractor Predicting: 12it [00:16,  1.11it/s]Extractor Predicting: 13it [00:16,  1.22it/s]Extractor Predicting: 14it [00:17,  1.30it/s]Extractor Predicting: 15it [00:18,  1.35it/s]Extractor Predicting: 16it [00:18,  1.40it/s]Extractor Predicting: 17it [00:19,  1.43it/s]Extractor Predicting: 18it [00:20,  1.45it/s]Extractor Predicting: 19it [00:20,  1.50it/s]Extractor Predicting: 20it [00:21,  1.52it/s]Extractor Predicting: 21it [00:22,  1.53it/s]Extractor Predicting: 22it [00:22,  1.57it/s]Extractor Predicting: 23it [00:23,  1.57it/s]Extractor Predicting: 24it [00:23,  1.58it/s]Extractor Predicting: 25it [00:24,  1.55it/s]Extractor Predicting: 26it [00:25,  1.53it/s]Extractor Predicting: 27it [00:25,  1.54it/s]Extractor Predicting: 28it [00:26,  1.53it/s]Extractor Predicting: 29it [00:27,  1.53it/s]Extractor Predicting: 30it [00:27,  1.54it/s]Extractor Predicting: 31it [00:28,  1.55it/s]Extractor Predicting: 32it [00:29,  1.58it/s]Extractor Predicting: 33it [00:29,  1.56it/s]Extractor Predicting: 34it [00:30,  1.54it/s]Extractor Predicting: 35it [00:31,  1.54it/s]Extractor Predicting: 36it [00:31,  1.53it/s]Extractor Predicting: 37it [00:32,  1.55it/s]Extractor Predicting: 38it [00:32,  1.55it/s]Extractor Predicting: 39it [00:33,  1.53it/s]Extractor Predicting: 40it [00:34,  1.56it/s]Extractor Predicting: 41it [00:34,  1.52it/s]Extractor Predicting: 42it [00:35,  1.53it/s]Extractor Predicting: 43it [00:36,  1.52it/s]Extractor Predicting: 44it [00:36,  1.52it/s]Extractor Predicting: 45it [00:37,  1.55it/s]Extractor Predicting: 46it [00:38,  1.54it/s]Extractor Predicting: 47it [00:38,  1.54it/s]Extractor Predicting: 48it [00:39,  1.53it/s]Extractor Predicting: 49it [00:40,  1.52it/s]Extractor Predicting: 50it [00:40,  1.49it/s]Extractor Predicting: 51it [00:41,  1.49it/s]Extractor Predicting: 52it [00:42,  1.53it/s]Extractor Predicting: 53it [00:42,  1.43it/s]Extractor Predicting: 54it [00:43,  1.45it/s]Extractor Predicting: 55it [00:44,  1.42it/s]Extractor Predicting: 56it [00:45,  1.41it/s]Extractor Predicting: 57it [00:45,  1.43it/s]Extractor Predicting: 58it [00:46,  1.43it/s]Extractor Predicting: 59it [00:47,  1.45it/s]Extractor Predicting: 60it [00:47,  1.44it/s]Extractor Predicting: 61it [00:48,  1.44it/s]Extractor Predicting: 62it [00:49,  1.42it/s]Extractor Predicting: 63it [00:49,  1.42it/s]Extractor Predicting: 64it [00:50,  1.43it/s]Extractor Predicting: 65it [00:51,  1.42it/s]Extractor Predicting: 66it [00:52,  1.40it/s]Extractor Predicting: 67it [00:52,  1.42it/s]Extractor Predicting: 68it [00:53,  1.45it/s]Extractor Predicting: 69it [00:54,  1.44it/s]Extractor Predicting: 70it [00:54,  1.41it/s]Extractor Predicting: 71it [00:55,  1.42it/s]Extractor Predicting: 72it [00:56,  1.39it/s]Extractor Predicting: 73it [00:56,  1.42it/s]Extractor Predicting: 74it [00:57,  1.42it/s]Extractor Predicting: 75it [00:58,  1.43it/s]Extractor Predicting: 76it [00:59,  1.43it/s]Extractor Predicting: 77it [00:59,  1.42it/s]Extractor Predicting: 78it [01:00,  1.45it/s]Extractor Predicting: 79it [01:01,  1.47it/s]Extractor Predicting: 80it [01:01,  1.45it/s]Extractor Predicting: 81it [01:02,  1.45it/s]Extractor Predicting: 82it [01:03,  1.42it/s]Extractor Predicting: 83it [01:03,  1.43it/s]Extractor Predicting: 84it [01:04,  1.43it/s]Extractor Predicting: 85it [01:05,  1.42it/s]Extractor Predicting: 86it [01:06,  1.40it/s]Extractor Predicting: 87it [01:06,  1.40it/s]Extractor Predicting: 88it [01:07,  1.37it/s]Extractor Predicting: 89it [01:08,  1.38it/s]Extractor Predicting: 90it [01:09,  1.38it/s]Extractor Predicting: 91it [01:09,  1.38it/s]Extractor Predicting: 92it [01:10,  1.43it/s]Extractor Predicting: 93it [01:11,  1.48it/s]Extractor Predicting: 94it [01:11,  1.47it/s]Extractor Predicting: 95it [01:12,  1.47it/s]Extractor Predicting: 96it [01:13,  1.47it/s]Extractor Predicting: 97it [01:13,  1.49it/s]Extractor Predicting: 98it [01:14,  1.45it/s]Extractor Predicting: 99it [01:15,  1.39it/s]Extractor Predicting: 100it [01:15,  1.42it/s]Extractor Predicting: 101it [01:17,  1.01s/it]Extractor Predicting: 102it [01:18,  1.08it/s]Extractor Predicting: 103it [01:19,  1.16it/s]Extractor Predicting: 104it [01:19,  1.24it/s]Extractor Predicting: 105it [01:20,  1.29it/s]Extractor Predicting: 106it [01:21,  1.37it/s]Extractor Predicting: 107it [01:21,  1.40it/s]Extractor Predicting: 108it [01:22,  1.44it/s]Extractor Predicting: 109it [01:23,  1.43it/s]Extractor Predicting: 110it [01:23,  1.44it/s]Extractor Predicting: 111it [01:24,  1.47it/s]Extractor Predicting: 112it [01:25,  1.47it/s]Extractor Predicting: 113it [01:25,  1.43it/s]Extractor Predicting: 114it [01:26,  1.42it/s]Extractor Predicting: 115it [01:27,  1.44it/s]Extractor Predicting: 116it [01:27,  1.42it/s]Extractor Predicting: 117it [01:28,  1.40it/s]Extractor Predicting: 118it [01:29,  1.40it/s]Extractor Predicting: 119it [01:30,  1.39it/s]Extractor Predicting: 120it [01:30,  1.35it/s]Extractor Predicting: 121it [01:31,  1.37it/s]Extractor Predicting: 122it [01:32,  1.38it/s]Extractor Predicting: 123it [01:33,  1.40it/s]Extractor Predicting: 124it [01:33,  1.41it/s]Extractor Predicting: 125it [01:34,  1.42it/s]Extractor Predicting: 126it [01:35,  1.42it/s]Extractor Predicting: 127it [01:35,  1.42it/s]Extractor Predicting: 128it [01:36,  1.42it/s]Extractor Predicting: 129it [01:37,  1.44it/s]Extractor Predicting: 130it [01:37,  1.39it/s]Extractor Predicting: 131it [01:38,  1.29it/s]Extractor Predicting: 132it [01:39,  1.34it/s]Extractor Predicting: 133it [01:40,  1.35it/s]Extractor Predicting: 134it [01:41,  1.37it/s]Extractor Predicting: 135it [01:41,  1.37it/s]Extractor Predicting: 136it [01:42,  1.40it/s]Extractor Predicting: 137it [01:43,  1.38it/s]Extractor Predicting: 138it [01:43,  1.40it/s]Extractor Predicting: 139it [01:44,  1.39it/s]Extractor Predicting: 140it [01:45,  1.38it/s]Extractor Predicting: 141it [01:46,  1.40it/s]Extractor Predicting: 142it [01:46,  1.41it/s]Extractor Predicting: 143it [01:47,  1.39it/s]Extractor Predicting: 144it [01:48,  1.43it/s]Extractor Predicting: 145it [01:48,  1.46it/s]Extractor Predicting: 146it [01:49,  1.45it/s]Extractor Predicting: 147it [01:50,  1.42it/s]Extractor Predicting: 148it [01:50,  1.43it/s]Extractor Predicting: 149it [01:51,  1.42it/s]Extractor Predicting: 150it [01:52,  1.40it/s]Extractor Predicting: 151it [01:53,  1.40it/s]Extractor Predicting: 152it [01:53,  1.40it/s]Extractor Predicting: 153it [01:54,  1.39it/s]Extractor Predicting: 154it [01:55,  1.40it/s]Extractor Predicting: 155it [01:55,  1.40it/s]Extractor Predicting: 156it [01:56,  1.34it/s]Extractor Predicting: 157it [01:57,  1.31it/s]Extractor Predicting: 158it [01:58,  1.28it/s]Extractor Predicting: 159it [01:59,  1.30it/s]Extractor Predicting: 160it [01:59,  1.33it/s]Extractor Predicting: 161it [02:00,  1.35it/s]Extractor Predicting: 162it [02:01,  1.36it/s]Extractor Predicting: 163it [02:01,  1.36it/s]Extractor Predicting: 164it [02:02,  1.39it/s]Extractor Predicting: 165it [02:03,  1.39it/s]Extractor Predicting: 166it [02:04,  1.42it/s]Extractor Predicting: 167it [02:04,  1.41it/s]Extractor Predicting: 168it [02:05,  1.40it/s]Extractor Predicting: 169it [02:06,  1.41it/s]Extractor Predicting: 170it [02:06,  1.40it/s]Extractor Predicting: 171it [02:07,  1.43it/s]Extractor Predicting: 172it [02:08,  1.44it/s]Extractor Predicting: 173it [02:09,  1.40it/s]Extractor Predicting: 174it [02:09,  1.42it/s]Extractor Predicting: 175it [02:10,  1.40it/s]Extractor Predicting: 176it [02:11,  1.42it/s]Extractor Predicting: 177it [02:11,  1.39it/s]Extractor Predicting: 178it [02:12,  1.40it/s]Extractor Predicting: 179it [02:13,  1.40it/s]Extractor Predicting: 180it [02:14,  1.40it/s]Extractor Predicting: 181it [02:14,  1.40it/s]Extractor Predicting: 182it [02:15,  1.40it/s]Extractor Predicting: 183it [02:16,  1.36it/s]Extractor Predicting: 184it [02:16,  1.38it/s]Extractor Predicting: 185it [02:17,  1.48it/s]Extractor Predicting: 185it [02:17,  1.35it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 23558
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23658, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.49it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.51it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.44it/s]Extractor Predicting: 21it [00:14,  1.43it/s]Extractor Predicting: 22it [00:14,  1.43it/s]Extractor Predicting: 23it [00:15,  1.44it/s]Extractor Predicting: 24it [00:16,  1.45it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:19,  1.47it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.53it/s]Extractor Predicting: 31it [00:20,  1.58it/s]Extractor Predicting: 32it [00:21,  1.57it/s]Extractor Predicting: 33it [00:22,  1.57it/s]Extractor Predicting: 34it [00:22,  1.57it/s]Extractor Predicting: 35it [00:23,  1.57it/s]Extractor Predicting: 36it [00:24,  1.56it/s]Extractor Predicting: 37it [00:24,  1.58it/s]Extractor Predicting: 38it [00:25,  1.56it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.54it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:28,  1.56it/s]Extractor Predicting: 44it [00:29,  1.57it/s]Extractor Predicting: 45it [00:29,  1.59it/s]Extractor Predicting: 46it [00:30,  1.60it/s]Extractor Predicting: 47it [00:31,  1.60it/s]Extractor Predicting: 48it [00:31,  1.60it/s]Extractor Predicting: 49it [00:32,  1.58it/s]Extractor Predicting: 50it [00:33,  1.57it/s]Extractor Predicting: 51it [00:33,  1.58it/s]Extractor Predicting: 52it [00:34,  1.57it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:36,  1.52it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:38,  1.60it/s]Extractor Predicting: 59it [00:38,  1.57it/s]Extractor Predicting: 60it [00:39,  1.55it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:40,  1.47it/s]Extractor Predicting: 63it [00:41,  1.40it/s]Extractor Predicting: 64it [00:42,  1.36it/s]Extractor Predicting: 65it [00:43,  1.35it/s]Extractor Predicting: 66it [00:43,  1.35it/s]Extractor Predicting: 67it [00:44,  1.35it/s]Extractor Predicting: 68it [00:45,  1.36it/s]Extractor Predicting: 69it [00:46,  1.38it/s]Extractor Predicting: 70it [00:46,  1.39it/s]Extractor Predicting: 71it [00:47,  1.40it/s]Extractor Predicting: 72it [00:48,  1.42it/s]Extractor Predicting: 73it [00:48,  1.45it/s]Extractor Predicting: 74it [00:49,  1.45it/s]Extractor Predicting: 75it [00:50,  1.46it/s]Extractor Predicting: 76it [00:50,  1.47it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:52,  1.48it/s]Extractor Predicting: 79it [00:52,  1.53it/s]Extractor Predicting: 80it [00:53,  1.56it/s]Extractor Predicting: 81it [00:54,  1.38it/s]Extractor Predicting: 82it [00:55,  1.44it/s]Extractor Predicting: 83it [00:55,  1.43it/s]Extractor Predicting: 84it [00:56,  1.44it/s]Extractor Predicting: 85it [00:57,  1.41it/s]Extractor Predicting: 86it [00:57,  1.40it/s]Extractor Predicting: 87it [00:58,  1.41it/s]Extractor Predicting: 88it [00:59,  1.44it/s]Extractor Predicting: 89it [00:59,  1.43it/s]Extractor Predicting: 90it [01:00,  1.45it/s]Extractor Predicting: 91it [01:01,  1.43it/s]Extractor Predicting: 92it [01:02,  1.43it/s]Extractor Predicting: 93it [01:02,  1.43it/s]Extractor Predicting: 94it [01:03,  1.44it/s]Extractor Predicting: 95it [01:04,  1.43it/s]Extractor Predicting: 96it [01:04,  1.43it/s]Extractor Predicting: 97it [01:05,  1.45it/s]Extractor Predicting: 98it [01:06,  1.44it/s]Extractor Predicting: 99it [01:06,  1.43it/s]Extractor Predicting: 100it [01:07,  1.43it/s]Extractor Predicting: 101it [01:08,  1.44it/s]Extractor Predicting: 102it [01:08,  1.46it/s]Extractor Predicting: 103it [01:09,  1.42it/s]Extractor Predicting: 104it [01:10,  1.45it/s]Extractor Predicting: 105it [01:11,  1.46it/s]Extractor Predicting: 106it [01:11,  1.48it/s]Extractor Predicting: 107it [01:12,  1.46it/s]Extractor Predicting: 108it [01:13,  1.48it/s]Extractor Predicting: 109it [01:13,  1.46it/s]Extractor Predicting: 110it [01:14,  1.48it/s]Extractor Predicting: 111it [01:15,  1.48it/s]Extractor Predicting: 112it [01:15,  1.46it/s]Extractor Predicting: 113it [01:16,  1.45it/s]Extractor Predicting: 114it [01:17,  1.44it/s]Extractor Predicting: 115it [01:17,  1.44it/s]Extractor Predicting: 116it [01:18,  1.44it/s]Extractor Predicting: 117it [01:19,  1.47it/s]Extractor Predicting: 118it [01:19,  1.45it/s]Extractor Predicting: 119it [01:20,  1.45it/s]Extractor Predicting: 120it [01:21,  1.47it/s]Extractor Predicting: 121it [01:21,  1.50it/s]Extractor Predicting: 122it [01:22,  1.49it/s]Extractor Predicting: 123it [01:23,  1.46it/s]Extractor Predicting: 124it [01:24,  1.44it/s]Extractor Predicting: 125it [01:24,  1.45it/s]Extractor Predicting: 126it [01:25,  1.44it/s]Extractor Predicting: 127it [01:26,  1.47it/s]Extractor Predicting: 128it [01:26,  1.42it/s]Extractor Predicting: 129it [01:27,  1.43it/s]Extractor Predicting: 130it [01:28,  1.46it/s]Extractor Predicting: 131it [01:28,  1.44it/s]Extractor Predicting: 132it [01:29,  1.43it/s]Extractor Predicting: 133it [01:30,  1.43it/s]Extractor Predicting: 134it [01:31,  1.43it/s]Extractor Predicting: 135it [01:31,  1.44it/s]Extractor Predicting: 136it [01:32,  1.48it/s]Extractor Predicting: 137it [01:33,  1.42it/s]Extractor Predicting: 138it [01:33,  1.43it/s]Extractor Predicting: 139it [01:34,  1.45it/s]Extractor Predicting: 140it [01:35,  1.45it/s]Extractor Predicting: 141it [01:35,  1.44it/s]Extractor Predicting: 142it [01:36,  1.46it/s]Extractor Predicting: 143it [01:39,  1.38s/it]Extractor Predicting: 144it [01:40,  1.16s/it]Extractor Predicting: 145it [01:40,  1.04s/it]Extractor Predicting: 146it [01:41,  1.08it/s]Extractor Predicting: 147it [01:42,  1.19it/s]Extractor Predicting: 148it [01:42,  1.25it/s]Extractor Predicting: 149it [01:43,  1.34it/s]Extractor Predicting: 150it [01:44,  1.39it/s]Extractor Predicting: 151it [01:44,  1.43it/s]Extractor Predicting: 152it [01:45,  1.42it/s]Extractor Predicting: 153it [01:46,  1.45it/s]Extractor Predicting: 154it [01:46,  1.42it/s]Extractor Predicting: 155it [01:47,  1.43it/s]Extractor Predicting: 156it [01:48,  1.48it/s]Extractor Predicting: 157it [01:48,  1.45it/s]Extractor Predicting: 158it [01:49,  1.44it/s]Extractor Predicting: 159it [01:50,  1.46it/s]Extractor Predicting: 160it [01:51,  1.46it/s]Extractor Predicting: 161it [01:51,  1.49it/s]Extractor Predicting: 162it [01:52,  1.47it/s]Extractor Predicting: 163it [01:53,  1.47it/s]Extractor Predicting: 164it [01:53,  1.46it/s]Extractor Predicting: 165it [01:54,  1.41it/s]Extractor Predicting: 166it [01:55,  1.40it/s]Extractor Predicting: 167it [01:55,  1.40it/s]Extractor Predicting: 168it [01:56,  1.42it/s]Extractor Predicting: 169it [01:57,  1.45it/s]Extractor Predicting: 170it [01:57,  1.47it/s]Extractor Predicting: 171it [01:58,  1.47it/s]Extractor Predicting: 172it [01:59,  1.49it/s]Extractor Predicting: 173it [01:59,  1.49it/s]Extractor Predicting: 174it [02:00,  1.47it/s]Extractor Predicting: 175it [02:01,  1.33it/s]Extractor Predicting: 176it [02:02,  1.38it/s]Extractor Predicting: 177it [02:02,  1.39it/s]Extractor Predicting: 178it [02:03,  1.41it/s]Extractor Predicting: 179it [02:04,  1.40it/s]Extractor Predicting: 180it [02:05,  1.44it/s]Extractor Predicting: 181it [02:05,  1.44it/s]Extractor Predicting: 182it [02:06,  1.47it/s]Extractor Predicting: 183it [02:07,  1.50it/s]Extractor Predicting: 184it [02:07,  1.49it/s]Extractor Predicting: 185it [02:08,  1.49it/s]Extractor Predicting: 186it [02:09,  1.45it/s]Extractor Predicting: 187it [02:09,  1.46it/s]Extractor Predicting: 188it [02:10,  1.46it/s]Extractor Predicting: 189it [02:11,  1.46it/s]Extractor Predicting: 190it [02:11,  1.47it/s]Extractor Predicting: 191it [02:12,  1.48it/s]Extractor Predicting: 192it [02:13,  1.52it/s]Extractor Predicting: 193it [02:13,  1.50it/s]Extractor Predicting: 194it [02:14,  1.50it/s]Extractor Predicting: 195it [02:15,  1.48it/s]Extractor Predicting: 196it [02:15,  1.49it/s]Extractor Predicting: 197it [02:16,  1.48it/s]Extractor Predicting: 198it [02:17,  1.46it/s]Extractor Predicting: 199it [02:17,  1.46it/s]Extractor Predicting: 200it [02:18,  1.46it/s]Extractor Predicting: 201it [02:19,  1.48it/s]Extractor Predicting: 202it [02:19,  1.48it/s]Extractor Predicting: 203it [02:20,  1.49it/s]Extractor Predicting: 204it [02:21,  1.48it/s]Extractor Predicting: 205it [02:21,  1.49it/s]Extractor Predicting: 206it [02:22,  1.48it/s]Extractor Predicting: 207it [02:23,  1.49it/s]Extractor Predicting: 208it [02:23,  1.47it/s]Extractor Predicting: 209it [02:24,  1.42it/s]Extractor Predicting: 210it [02:25,  1.48it/s]Extractor Predicting: 211it [02:26,  1.47it/s]Extractor Predicting: 212it [02:26,  1.48it/s]Extractor Predicting: 213it [02:27,  1.49it/s]Extractor Predicting: 214it [02:27,  1.52it/s]Extractor Predicting: 215it [02:28,  1.50it/s]Extractor Predicting: 216it [02:29,  1.47it/s]Extractor Predicting: 217it [02:30,  1.49it/s]Extractor Predicting: 218it [02:30,  1.47it/s]Extractor Predicting: 219it [02:31,  1.46it/s]Extractor Predicting: 220it [02:32,  1.47it/s]Extractor Predicting: 221it [02:32,  1.49it/s]Extractor Predicting: 222it [02:33,  1.43it/s]Extractor Predicting: 223it [02:34,  1.39it/s]Extractor Predicting: 224it [02:34,  1.41it/s]Extractor Predicting: 225it [02:35,  1.43it/s]Extractor Predicting: 226it [02:36,  1.46it/s]Extractor Predicting: 227it [02:36,  1.44it/s]Extractor Predicting: 228it [02:37,  1.45it/s]Extractor Predicting: 229it [02:38,  1.46it/s]Extractor Predicting: 230it [02:39,  1.47it/s]Extractor Predicting: 231it [02:39,  1.48it/s]Extractor Predicting: 232it [02:40,  1.45it/s]Extractor Predicting: 233it [02:41,  1.45it/s]Extractor Predicting: 234it [02:41,  1.45it/s]Extractor Predicting: 235it [02:42,  1.41it/s]Extractor Predicting: 236it [02:43,  1.41it/s]Extractor Predicting: 237it [02:43,  1.42it/s]Extractor Predicting: 238it [02:44,  1.40it/s]Extractor Predicting: 239it [02:45,  1.43it/s]Extractor Predicting: 240it [02:46,  1.45it/s]Extractor Predicting: 241it [02:46,  1.46it/s]Extractor Predicting: 242it [02:47,  1.45it/s]Extractor Predicting: 243it [02:48,  1.41it/s]Extractor Predicting: 244it [02:48,  1.42it/s]Extractor Predicting: 245it [02:49,  1.45it/s]Extractor Predicting: 246it [02:50,  1.45it/s]Extractor Predicting: 247it [02:50,  1.48it/s]Extractor Predicting: 248it [02:51,  1.43it/s]Extractor Predicting: 249it [02:52,  1.41it/s]Extractor Predicting: 250it [02:52,  1.43it/s]Extractor Predicting: 251it [02:53,  1.43it/s]Extractor Predicting: 252it [02:54,  1.42it/s]Extractor Predicting: 253it [02:55,  1.37it/s]Extractor Predicting: 254it [02:55,  1.36it/s]Extractor Predicting: 255it [02:56,  1.38it/s]Extractor Predicting: 256it [02:57,  1.39it/s]Extractor Predicting: 257it [02:58,  1.41it/s]Extractor Predicting: 258it [02:58,  1.41it/s]Extractor Predicting: 259it [02:59,  1.40it/s]Extractor Predicting: 260it [03:00,  1.39it/s]Extractor Predicting: 261it [03:00,  1.41it/s]Extractor Predicting: 262it [03:01,  1.40it/s]Extractor Predicting: 263it [03:02,  1.41it/s]Extractor Predicting: 264it [03:02,  1.42it/s]Extractor Predicting: 265it [03:03,  1.44it/s]Extractor Predicting: 266it [03:04,  1.39it/s]Extractor Predicting: 267it [03:05,  1.38it/s]Extractor Predicting: 268it [03:05,  1.38it/s]Extractor Predicting: 269it [03:06,  1.38it/s]Extractor Predicting: 270it [03:07,  1.37it/s]Extractor Predicting: 271it [03:08,  1.38it/s]Extractor Predicting: 272it [03:08,  1.38it/s]Extractor Predicting: 273it [03:09,  1.38it/s]Extractor Predicting: 274it [03:10,  1.36it/s]Extractor Predicting: 275it [03:11,  1.37it/s]Extractor Predicting: 276it [03:11,  1.37it/s]Extractor Predicting: 277it [03:12,  1.38it/s]Extractor Predicting: 278it [03:13,  1.40it/s]Extractor Predicting: 279it [03:13,  1.39it/s]Extractor Predicting: 280it [03:14,  1.41it/s]Extractor Predicting: 281it [03:15,  1.37it/s]Extractor Predicting: 282it [03:16,  1.37it/s]Extractor Predicting: 283it [03:16,  1.39it/s]Extractor Predicting: 284it [03:17,  1.41it/s]Extractor Predicting: 285it [03:18,  1.41it/s]Extractor Predicting: 286it [03:18,  1.43it/s]Extractor Predicting: 287it [03:19,  1.39it/s]Extractor Predicting: 288it [03:20,  1.42it/s]Extractor Predicting: 289it [03:20,  1.44it/s]Extractor Predicting: 290it [03:21,  1.41it/s]Extractor Predicting: 291it [03:22,  1.38it/s]Extractor Predicting: 292it [03:23,  1.28it/s]Extractor Predicting: 293it [03:24,  1.32it/s]Extractor Predicting: 294it [03:24,  1.33it/s]Extractor Predicting: 295it [03:25,  1.35it/s]Extractor Predicting: 296it [03:26,  1.39it/s]Extractor Predicting: 297it [03:26,  1.40it/s]Extractor Predicting: 298it [03:27,  1.39it/s]Extractor Predicting: 299it [03:28,  1.40it/s]Extractor Predicting: 300it [03:29,  1.38it/s]Extractor Predicting: 301it [03:29,  1.41it/s]Extractor Predicting: 302it [03:30,  1.38it/s]Extractor Predicting: 303it [03:31,  1.40it/s]Extractor Predicting: 304it [03:31,  1.40it/s]Extractor Predicting: 305it [03:32,  1.42it/s]Extractor Predicting: 306it [03:33,  1.44it/s]Extractor Predicting: 307it [03:33,  1.43it/s]Extractor Predicting: 308it [03:34,  1.45it/s]Extractor Predicting: 309it [03:35,  1.43it/s]Extractor Predicting: 310it [03:36,  1.43it/s]Extractor Predicting: 311it [03:36,  1.40it/s]Extractor Predicting: 312it [03:37,  1.42it/s]Extractor Predicting: 313it [03:38,  1.35it/s]Extractor Predicting: 314it [03:39,  1.36it/s]Extractor Predicting: 315it [03:39,  1.39it/s]Extractor Predicting: 316it [03:40,  1.43it/s]Extractor Predicting: 317it [03:41,  1.43it/s]Extractor Predicting: 318it [03:41,  1.46it/s]Extractor Predicting: 319it [03:42,  1.42it/s]Extractor Predicting: 320it [03:43,  1.38it/s]Extractor Predicting: 321it [03:43,  1.39it/s]Extractor Predicting: 322it [03:44,  1.41it/s]Extractor Predicting: 323it [03:45,  1.36it/s]Extractor Predicting: 324it [03:46,  1.34it/s]Extractor Predicting: 325it [03:46,  1.39it/s]Extractor Predicting: 326it [03:47,  1.40it/s]Extractor Predicting: 327it [03:48,  1.41it/s]Extractor Predicting: 328it [03:49,  1.39it/s]Extractor Predicting: 329it [03:49,  1.41it/s]Extractor Predicting: 330it [03:50,  1.39it/s]Extractor Predicting: 331it [03:51,  1.41it/s]Extractor Predicting: 332it [03:51,  1.40it/s]Extractor Predicting: 333it [03:52,  1.39it/s]Extractor Predicting: 334it [03:53,  1.41it/s]Extractor Predicting: 335it [03:53,  1.40it/s]Extractor Predicting: 336it [03:54,  1.33it/s]Extractor Predicting: 337it [03:55,  1.34it/s]Extractor Predicting: 338it [03:56,  1.34it/s]Extractor Predicting: 339it [03:57,  1.36it/s]Extractor Predicting: 340it [03:57,  1.36it/s]Extractor Predicting: 341it [03:58,  1.36it/s]Extractor Predicting: 342it [03:59,  1.37it/s]Extractor Predicting: 343it [03:59,  1.39it/s]Extractor Predicting: 344it [04:00,  1.40it/s]Extractor Predicting: 345it [04:01,  1.36it/s]Extractor Predicting: 346it [04:02,  1.37it/s]Extractor Predicting: 347it [04:02,  1.38it/s]Extractor Predicting: 348it [04:03,  1.40it/s]Extractor Predicting: 349it [04:04,  1.40it/s]Extractor Predicting: 350it [04:04,  1.39it/s]Extractor Predicting: 351it [04:05,  1.42it/s]Extractor Predicting: 352it [04:06,  1.38it/s]Extractor Predicting: 353it [04:07,  1.36it/s]Extractor Predicting: 354it [04:07,  1.40it/s]Extractor Predicting: 355it [04:08,  1.42it/s]Extractor Predicting: 356it [04:09,  1.41it/s]Extractor Predicting: 357it [04:09,  1.41it/s]Extractor Predicting: 358it [04:10,  1.42it/s]Extractor Predicting: 359it [04:11,  1.41it/s]Extractor Predicting: 360it [04:12,  1.38it/s]Extractor Predicting: 361it [04:12,  1.37it/s]Extractor Predicting: 362it [04:13,  1.39it/s]Extractor Predicting: 363it [04:14,  1.43it/s]Extractor Predicting: 364it [04:14,  1.44it/s]Extractor Predicting: 365it [04:15,  1.41it/s]Extractor Predicting: 366it [04:16,  1.37it/s]Extractor Predicting: 367it [04:17,  1.37it/s]Extractor Predicting: 368it [04:17,  1.35it/s]Extractor Predicting: 369it [04:18,  1.30it/s]Extractor Predicting: 370it [04:19,  1.31it/s]Extractor Predicting: 371it [04:20,  1.33it/s]Extractor Predicting: 372it [04:20,  1.58it/s]Extractor Predicting: 372it [04:20,  1.43it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 7392
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7492, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:06,  1.52it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.43it/s]Extractor Predicting: 22it [00:14,  1.42it/s]Extractor Predicting: 23it [00:15,  1.42it/s]Extractor Predicting: 24it [00:16,  1.39it/s]Extractor Predicting: 25it [00:17,  1.42it/s]Extractor Predicting: 26it [00:17,  1.40it/s]Extractor Predicting: 27it [00:18,  1.39it/s]Extractor Predicting: 28it [00:19,  1.38it/s]Extractor Predicting: 29it [00:19,  1.41it/s]Extractor Predicting: 30it [00:20,  1.42it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:21,  1.42it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:23,  1.45it/s]Extractor Predicting: 35it [00:24,  1.42it/s]Extractor Predicting: 36it [00:24,  1.42it/s]Extractor Predicting: 37it [00:25,  1.41it/s]Extractor Predicting: 38it [00:26,  1.36it/s]Extractor Predicting: 39it [00:27,  1.33it/s]Extractor Predicting: 40it [00:27,  1.32it/s]Extractor Predicting: 41it [00:28,  1.32it/s]Extractor Predicting: 42it [00:29,  1.33it/s]Extractor Predicting: 43it [00:30,  1.33it/s]Extractor Predicting: 44it [00:30,  1.33it/s]Extractor Predicting: 45it [00:31,  1.31it/s]Extractor Predicting: 46it [00:32,  1.33it/s]Extractor Predicting: 47it [00:33,  1.32it/s]Extractor Predicting: 48it [00:34,  1.24it/s]Extractor Predicting: 49it [00:34,  1.27it/s]Extractor Predicting: 50it [00:35,  1.28it/s]Extractor Predicting: 51it [00:36,  1.28it/s]Extractor Predicting: 52it [00:37,  1.28it/s]Extractor Predicting: 53it [00:37,  1.30it/s]Extractor Predicting: 54it [00:38,  1.29it/s]Extractor Predicting: 55it [00:39,  1.35it/s]Extractor Predicting: 56it [00:40,  1.36it/s]Extractor Predicting: 57it [00:40,  1.34it/s]Extractor Predicting: 58it [00:41,  1.33it/s]Extractor Predicting: 59it [00:42,  1.33it/s]Extractor Predicting: 60it [00:43,  1.32it/s]Extractor Predicting: 61it [00:43,  1.29it/s]Extractor Predicting: 62it [00:44,  1.29it/s]Extractor Predicting: 63it [00:45,  1.41it/s]Extractor Predicting: 63it [00:45,  1.39it/s]
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_10_seed_1/extractor/results_multi_is_eval_False.json"
}
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_10_seed_1', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/fewrel/unseen_10_seed_1/generator/synthetic.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_10_seed_1', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/wiki/unseen_10_seed_1/generator/synthetic.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_10_seed_1', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/fewrel/unseen_10_seed_1/generator/synthetic.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_1/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_1/extractor/filtered.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_10_seed_1', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/wiki/unseen_10_seed_1/generator/synthetic.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_1/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_1/extractor/filtered.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl'}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/dev.jsonl', 'labels': ['conflict', 'developer', 'parent astronomical body', 'subsidiary', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_1/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_1/test.jsonl', 'labels': ['composer', 'country of citizenship', 'creator', 'field of work', 'lyrics by', 'member of sports team', 'occupation', 'residence', 'sports discipline competed in', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
