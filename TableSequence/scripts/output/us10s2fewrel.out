Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_2', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_2/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:53, 16.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:40, 16.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:49<03:18, 16.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:05<02:56, 16.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:21<02:42, 16.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:37<02:25, 16.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:56<02:15, 16.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:13<01:58, 16.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:29<01:40, 16.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:45<01:22, 16.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:02<01:06, 16.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:18<00:49, 16.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:34<00:32, 16.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:51<00:16, 16.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:07<00:00, 16.34s/it]Generating: 100%|██████████| 15/15 [04:07<00:00, 16.49s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 565, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : follows .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 617, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.77125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 558, 'raw': 736}
{'target': 600, 'success': 580, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : owned by . Context : Later in the year , the department built a high - speed commuter railway crossing the Bordeaux River into New York City from Union Station in the Empire State Building . Head Entity : Union Station , Tail Entity : Department of Transportation .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 526, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 579, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : owned by .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 383, 'raw': 480}
{'target': 600, 'success': 406, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.7981770833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 89, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 180, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 227, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 272, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 334, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 404, 'raw': 576}
{'target': 600, 'success': 425, 'raw': 608}
{'target': 600, 'success': 448, 'raw': 640}
{'target': 600, 'success': 473, 'raw': 672}
{'target': 600, 'success': 497, 'raw': 704}
{'target': 600, 'success': 520, 'raw': 736}
{'target': 600, 'success': 544, 'raw': 768}
{'target': 600, 'success': 569, 'raw': 800}
{'target': 600, 'success': 587, 'raw': 832}
{'target': 600, 'success': 607, 'raw': 864}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7025462962962963, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 312, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 432, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 494, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 593, 'raw': 800}
{'target': 600, 'success': 618, 'raw': 832}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.7427884615384616, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : location of formation . Context : Later in the year ( 1190 ) , Pheidole and his friends made a series of expeditions to the Old Town of Antwerp , which was a medieval church , to find remnants of the Church of St John the Evangelists , or John XII , with the exception of the dome . Head Entity : Pheidole , Tail Entity : New Town of Antwerp .\n']
['Relation : location of formation . Context : Later in the year ( 1190 ) , Pheidole and his friends made a series of expeditions to the Old Town of Antwerp , which was a medieval church , to find remnants of the Church of St John the Evangelists , or John XII , with the exception of the dome . Head Entity : Pheidole , Tail Entity : New Town of Antwerp .\n', "Relation : location of formation . Context : After the death of Emperor Nefertiti ( 9 January 1789 - 7 February 1819 ) , St Peter was succeeded as Archbishop by the emperor , St Peter 's son , Emperor Frederick II ( 18 November 1861 - 12 December 1955 ) . Head Entity : Emperor Frederick II , Tail Entity : Emperor St Peter .\n"]
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 547, 'raw': 704}
{'target': 600, 'success': 576, 'raw': 736}
{'target': 600, 'success': 600, 'raw': 768}
{'prompt': 'Relation : occupant .', 'success_rate': 0.78125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : place served by transport hub . Context : The city of Stuttgart is a medieval city located in the eastern part of Germany . Head Entity : Struttgart , Tail Entity : Struttgart .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.7955729166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : record label . Context : Later in 2008 , the band became a major draw for their debut solo album entitled " The Way I Am " . Head Entity : The Way I Am , Tail Entity : The Bizarre Bizarre .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8033854166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 189, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 353, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 399, 'raw': 544}
{'target': 600, 'success': 423, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 470, 'raw': 640}
{'target': 600, 'success': 495, 'raw': 672}
{'target': 600, 'success': 517, 'raw': 704}
{'target': 600, 'success': 542, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 583, 'raw': 800}
{'target': 600, 'success': 600, 'raw': 832}
{'prompt': 'Relation : winner .', 'success_rate': 0.7211538461538461, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : work location .', 'success_rate': 0.78515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/0_ext.jsonl'}}
estimate vocab size: 14638
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14738, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_2/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.66s/it]Extractor Estimating: 2it [00:18,  8.02s/it]Extractor Estimating: 3it [00:19,  4.66s/it]Extractor Estimating: 4it [00:20,  3.06s/it]Extractor Estimating: 5it [00:20,  2.19s/it]Extractor Estimating: 6it [00:21,  1.66s/it]Extractor Estimating: 7it [00:22,  1.32s/it]Extractor Estimating: 8it [00:22,  1.10s/it]Extractor Estimating: 9it [00:23,  1.06it/s]Extractor Estimating: 10it [00:24,  1.14it/s]Extractor Estimating: 11it [00:24,  1.21it/s]Extractor Estimating: 12it [00:26,  1.04it/s]Extractor Estimating: 13it [00:26,  1.18it/s]Extractor Estimating: 14it [00:27,  1.26it/s]Extractor Estimating: 15it [00:27,  1.35it/s]Extractor Estimating: 16it [00:28,  1.42it/s]Extractor Estimating: 17it [00:29,  1.45it/s]Extractor Estimating: 18it [00:29,  1.49it/s]Extractor Estimating: 19it [00:30,  1.51it/s]Extractor Estimating: 20it [00:31,  1.49it/s]Extractor Estimating: 21it [00:31,  1.57it/s]Extractor Estimating: 22it [00:35,  1.51s/it]Extractor Estimating: 23it [00:35,  1.26s/it]Extractor Estimating: 24it [00:36,  1.07s/it]Extractor Estimating: 25it [00:37,  1.06it/s]Extractor Estimating: 26it [00:37,  1.18it/s]Extractor Estimating: 27it [00:38,  1.26it/s]Extractor Estimating: 28it [00:39,  1.31it/s]Extractor Estimating: 29it [00:39,  1.34it/s]Extractor Estimating: 30it [00:40,  1.41it/s]Extractor Estimating: 31it [00:41,  1.42it/s]Extractor Estimating: 32it [00:42,  1.37it/s]Extractor Estimating: 33it [00:42,  1.42it/s]Extractor Estimating: 34it [00:43,  1.43it/s]Extractor Estimating: 35it [00:43,  1.46it/s]Extractor Estimating: 36it [00:44,  1.46it/s]Extractor Estimating: 37it [00:45,  1.43it/s]Extractor Estimating: 38it [00:46,  1.43it/s]Extractor Estimating: 39it [00:46,  1.46it/s]Extractor Estimating: 40it [00:47,  1.46it/s]Extractor Estimating: 41it [00:48,  1.50it/s]Extractor Estimating: 42it [00:48,  1.45it/s]Extractor Estimating: 43it [00:49,  1.51it/s]Extractor Estimating: 44it [00:50,  1.47it/s]Extractor Estimating: 45it [00:50,  1.50it/s]Extractor Estimating: 46it [00:51,  1.51it/s]Extractor Estimating: 47it [00:52,  1.54it/s]Extractor Estimating: 48it [00:52,  1.52it/s]Extractor Estimating: 49it [00:53,  1.49it/s]Extractor Estimating: 50it [00:54,  1.52it/s]Extractor Estimating: 51it [00:54,  1.57it/s]Extractor Estimating: 52it [00:55,  1.60it/s]Extractor Estimating: 53it [00:55,  1.58it/s]Extractor Estimating: 54it [00:56,  1.60it/s]Extractor Estimating: 55it [00:57,  1.61it/s]Extractor Estimating: 56it [00:57,  1.61it/s]Extractor Estimating: 57it [00:58,  1.58it/s]Extractor Estimating: 58it [00:59,  1.56it/s]Extractor Estimating: 59it [00:59,  1.59it/s]Extractor Estimating: 60it [01:00,  1.58it/s]Extractor Estimating: 61it [01:00,  1.57it/s]Extractor Estimating: 62it [01:01,  1.58it/s]Extractor Estimating: 63it [01:02,  1.57it/s]Extractor Estimating: 64it [01:02,  1.57it/s]Extractor Estimating: 65it [01:03,  1.59it/s]Extractor Estimating: 66it [01:04,  1.60it/s]Extractor Estimating: 67it [01:04,  1.64it/s]Extractor Estimating: 68it [01:05,  1.64it/s]Extractor Estimating: 69it [01:05,  1.69it/s]Extractor Estimating: 70it [01:06,  1.60it/s]Extractor Estimating: 71it [01:07,  1.61it/s]Extractor Estimating: 72it [01:07,  1.60it/s]Extractor Estimating: 73it [01:08,  1.61it/s]Extractor Estimating: 74it [01:08,  1.62it/s]Extractor Estimating: 75it [01:09,  1.66it/s]Extractor Estimating: 76it [01:10,  1.64it/s]Extractor Estimating: 77it [01:10,  1.65it/s]Extractor Estimating: 78it [01:11,  1.66it/s]Extractor Estimating: 79it [01:11,  1.70it/s]Extractor Estimating: 80it [01:12,  1.69it/s]Extractor Estimating: 81it [01:13,  1.70it/s]Extractor Estimating: 82it [01:13,  1.71it/s]Extractor Estimating: 83it [01:14,  1.67it/s]Extractor Estimating: 84it [01:14,  1.65it/s]Extractor Estimating: 85it [01:15,  1.59it/s]Extractor Estimating: 86it [01:16,  1.58it/s]Extractor Estimating: 87it [01:16,  1.51it/s]Extractor Estimating: 88it [01:17,  1.55it/s]Extractor Estimating: 89it [01:18,  1.60it/s]Extractor Estimating: 90it [01:18,  1.61it/s]Extractor Estimating: 91it [01:19,  1.62it/s]Extractor Estimating: 92it [01:19,  1.64it/s]Extractor Estimating: 93it [01:20,  1.71it/s]Extractor Estimating: 94it [01:21,  1.73it/s]Extractor Estimating: 95it [01:21,  1.72it/s]Extractor Estimating: 96it [01:22,  1.71it/s]Extractor Estimating: 97it [01:22,  1.71it/s]Extractor Estimating: 98it [01:23,  1.64it/s]Extractor Estimating: 99it [01:24,  1.61it/s]Extractor Estimating: 100it [01:24,  1.69it/s]Extractor Estimating: 101it [01:25,  1.68it/s]Extractor Estimating: 102it [01:25,  1.71it/s]Extractor Estimating: 103it [01:26,  1.67it/s]Extractor Estimating: 104it [01:27,  1.49it/s]Extractor Estimating: 105it [01:27,  1.56it/s]Extractor Estimating: 106it [01:28,  1.52it/s]Extractor Estimating: 107it [01:29,  1.55it/s]Extractor Estimating: 108it [01:29,  1.57it/s]Extractor Estimating: 109it [01:30,  1.57it/s]Extractor Estimating: 110it [01:31,  1.53it/s]Extractor Estimating: 111it [01:31,  1.40it/s]Extractor Estimating: 112it [01:32,  1.42it/s]Extractor Estimating: 113it [01:33,  1.50it/s]Extractor Estimating: 114it [01:33,  1.51it/s]Extractor Estimating: 115it [01:34,  1.50it/s]Extractor Estimating: 116it [01:35,  1.57it/s]Extractor Estimating: 117it [01:35,  1.57it/s]Extractor Estimating: 118it [01:36,  1.60it/s]Extractor Estimating: 119it [01:37,  1.54it/s]Extractor Estimating: 120it [01:37,  1.56it/s]Extractor Estimating: 121it [01:38,  1.59it/s]Extractor Estimating: 122it [01:38,  1.58it/s]Extractor Estimating: 123it [01:39,  1.54it/s]Extractor Estimating: 124it [01:40,  1.55it/s]Extractor Estimating: 125it [01:40,  1.61it/s]Extractor Estimating: 126it [01:41,  1.59it/s]Extractor Estimating: 127it [01:42,  1.52it/s]Extractor Estimating: 128it [01:42,  1.55it/s]Extractor Estimating: 129it [01:43,  1.49it/s]Extractor Estimating: 130it [01:44,  1.56it/s]Extractor Estimating: 131it [01:44,  1.58it/s]Extractor Estimating: 132it [01:45,  1.54it/s]Extractor Estimating: 133it [01:46,  1.55it/s]Extractor Estimating: 134it [01:46,  1.57it/s]Extractor Estimating: 135it [01:47,  1.57it/s]Extractor Estimating: 136it [01:47,  1.59it/s]Extractor Estimating: 137it [01:48,  1.59it/s]Extractor Estimating: 138it [01:49,  1.64it/s]Extractor Estimating: 139it [01:49,  1.62it/s]Extractor Estimating: 140it [01:50,  1.65it/s]Extractor Estimating: 141it [01:50,  1.63it/s]Extractor Estimating: 142it [01:51,  1.55it/s]Extractor Estimating: 143it [01:52,  1.53it/s]Extractor Estimating: 144it [01:52,  1.52it/s]Extractor Estimating: 145it [01:53,  1.53it/s]Extractor Estimating: 146it [01:54,  1.55it/s]Extractor Estimating: 147it [01:54,  1.50it/s]Extractor Estimating: 148it [01:55,  1.45it/s]Extractor Estimating: 149it [01:56,  1.44it/s]Extractor Estimating: 150it [01:57,  1.47it/s]Extractor Estimating: 151it [01:57,  1.48it/s]Extractor Estimating: 152it [01:58,  1.47it/s]Extractor Estimating: 153it [01:59,  1.49it/s]Extractor Estimating: 154it [01:59,  1.48it/s]Extractor Estimating: 155it [02:00,  1.50it/s]Extractor Estimating: 156it [02:01,  1.48it/s]Extractor Estimating: 157it [02:01,  1.45it/s]Extractor Estimating: 158it [02:02,  1.44it/s]Extractor Estimating: 159it [02:03,  1.46it/s]Extractor Estimating: 160it [02:03,  1.42it/s]Extractor Estimating: 161it [02:04,  1.46it/s]Extractor Estimating: 162it [02:05,  1.43it/s]Extractor Estimating: 163it [02:06,  1.39it/s]Extractor Estimating: 164it [02:06,  1.38it/s]Extractor Estimating: 165it [02:07,  1.39it/s]Extractor Estimating: 166it [02:08,  1.42it/s]Extractor Estimating: 167it [02:08,  1.41it/s]Extractor Estimating: 168it [02:09,  1.46it/s]Extractor Estimating: 169it [02:10,  1.43it/s]Extractor Estimating: 170it [02:10,  1.51it/s]Extractor Estimating: 171it [02:11,  1.58it/s]Extractor Estimating: 172it [02:12,  1.49it/s]Extractor Estimating: 173it [02:12,  1.47it/s]Extractor Estimating: 174it [02:13,  1.52it/s]Extractor Estimating: 175it [02:14,  1.51it/s]Extractor Estimating: 176it [02:14,  1.55it/s]Extractor Estimating: 177it [02:15,  1.49it/s]Extractor Estimating: 178it [02:16,  1.56it/s]Extractor Estimating: 179it [02:16,  1.56it/s]Extractor Estimating: 180it [02:17,  1.62it/s]Extractor Estimating: 181it [02:17,  1.66it/s]Extractor Estimating: 182it [02:18,  1.59it/s]Extractor Estimating: 183it [02:19,  1.46it/s]Extractor Estimating: 184it [02:19,  1.50it/s]Extractor Estimating: 185it [02:20,  1.53it/s]Extractor Estimating: 186it [02:21,  1.52it/s]Extractor Estimating: 187it [02:21,  1.48it/s]Extractor Estimating: 188it [02:22,  1.53it/s]Extractor Estimating: 189it [02:23,  1.55it/s]Extractor Estimating: 190it [02:23,  1.57it/s]Extractor Estimating: 191it [02:24,  1.59it/s]Extractor Estimating: 192it [02:25,  1.60it/s]Extractor Estimating: 193it [02:25,  1.59it/s]Extractor Estimating: 194it [02:26,  1.54it/s]Extractor Estimating: 195it [02:27,  1.54it/s]Extractor Estimating: 196it [02:27,  1.53it/s]Extractor Estimating: 197it [02:28,  1.52it/s]Extractor Estimating: 198it [02:28,  1.61it/s]Extractor Estimating: 199it [02:29,  1.57it/s]Extractor Estimating: 200it [02:30,  1.46it/s]Extractor Estimating: 201it [02:31,  1.47it/s]Extractor Estimating: 202it [02:31,  1.53it/s]Extractor Estimating: 203it [02:32,  1.54it/s]Extractor Estimating: 204it [02:32,  1.50it/s]Extractor Estimating: 205it [02:33,  1.47it/s]Extractor Estimating: 206it [02:34,  1.54it/s]Extractor Estimating: 207it [02:34,  1.52it/s]Extractor Estimating: 208it [02:35,  1.48it/s]Extractor Estimating: 209it [02:36,  1.48it/s]Extractor Estimating: 210it [02:36,  1.53it/s]Extractor Estimating: 211it [02:37,  1.54it/s]Extractor Estimating: 212it [02:38,  1.50it/s]Extractor Estimating: 213it [02:39,  1.47it/s]Extractor Estimating: 214it [02:39,  1.52it/s]Extractor Estimating: 215it [02:40,  1.49it/s]Extractor Estimating: 216it [02:40,  1.53it/s]Extractor Estimating: 217it [02:41,  1.49it/s]Extractor Estimating: 218it [02:42,  1.48it/s]Extractor Estimating: 219it [02:43,  1.46it/s]Extractor Estimating: 220it [02:43,  1.43it/s]Extractor Estimating: 221it [02:44,  1.40it/s]Extractor Estimating: 222it [02:45,  1.37it/s]Extractor Estimating: 223it [02:45,  1.41it/s]Extractor Estimating: 224it [02:46,  1.40it/s]Extractor Estimating: 225it [02:47,  1.42it/s]Extractor Estimating: 226it [02:47,  1.49it/s]Extractor Estimating: 227it [02:48,  1.54it/s]Extractor Estimating: 228it [02:49,  1.54it/s]Extractor Estimating: 229it [02:49,  1.56it/s]Extractor Estimating: 230it [02:50,  1.58it/s]Extractor Estimating: 231it [02:51,  1.52it/s]Extractor Estimating: 232it [02:51,  1.57it/s]Extractor Estimating: 233it [02:52,  1.59it/s]Extractor Estimating: 234it [02:52,  1.59it/s]Extractor Estimating: 235it [02:53,  1.59it/s]Extractor Estimating: 236it [02:54,  1.58it/s]Extractor Estimating: 237it [02:54,  1.58it/s]Extractor Estimating: 238it [02:55,  1.49it/s]Extractor Estimating: 239it [02:56,  1.54it/s]Extractor Estimating: 240it [02:56,  1.61it/s]Extractor Estimating: 241it [02:57,  1.57it/s]Extractor Estimating: 242it [02:58,  1.60it/s]Extractor Estimating: 243it [02:58,  1.62it/s]Extractor Estimating: 244it [02:59,  1.60it/s]Extractor Estimating: 245it [02:59,  1.61it/s]Extractor Estimating: 246it [03:00,  1.58it/s]Extractor Estimating: 247it [03:01,  1.55it/s]Extractor Estimating: 248it [03:01,  1.59it/s]Extractor Estimating: 249it [03:02,  1.57it/s]Extractor Estimating: 250it [03:03,  1.59it/s]Extractor Estimating: 251it [03:03,  1.63it/s]Extractor Estimating: 252it [03:04,  1.49it/s]Extractor Estimating: 253it [03:05,  1.48it/s]Extractor Estimating: 254it [03:05,  1.39it/s]Extractor Estimating: 255it [03:06,  1.44it/s]Extractor Estimating: 256it [03:07,  1.54it/s]Extractor Estimating: 257it [03:07,  1.59it/s]Extractor Estimating: 258it [03:08,  1.59it/s]Extractor Estimating: 259it [03:09,  1.45it/s]Extractor Estimating: 260it [03:09,  1.48it/s]Extractor Estimating: 261it [03:10,  1.43it/s]Extractor Estimating: 262it [03:11,  1.43it/s]Extractor Estimating: 263it [03:11,  1.51it/s]Extractor Estimating: 264it [03:12,  1.55it/s]Extractor Estimating: 265it [03:13,  1.53it/s]Extractor Estimating: 266it [03:13,  1.52it/s]Extractor Estimating: 267it [03:14,  1.53it/s]Extractor Estimating: 268it [03:15,  1.51it/s]Extractor Estimating: 269it [03:15,  1.53it/s]Extractor Estimating: 270it [03:16,  1.57it/s]Extractor Estimating: 271it [03:17,  1.57it/s]Extractor Estimating: 272it [03:17,  1.61it/s]Extractor Estimating: 273it [03:18,  1.60it/s]Extractor Estimating: 274it [03:18,  1.61it/s]Extractor Estimating: 275it [03:19,  1.56it/s]Extractor Estimating: 276it [03:20,  1.58it/s]Extractor Estimating: 277it [03:20,  1.57it/s]Extractor Estimating: 278it [03:21,  1.62it/s]Extractor Estimating: 279it [03:21,  1.63it/s]Extractor Estimating: 280it [03:22,  1.63it/s]Extractor Estimating: 281it [03:23,  1.68it/s]Extractor Estimating: 282it [03:23,  1.64it/s]Extractor Estimating: 283it [03:24,  1.58it/s]Extractor Estimating: 284it [03:25,  1.61it/s]Extractor Estimating: 285it [03:25,  1.60it/s]Extractor Estimating: 286it [03:26,  1.57it/s]Extractor Estimating: 287it [03:26,  1.60it/s]Extractor Estimating: 288it [03:27,  1.52it/s]Extractor Estimating: 289it [03:28,  1.57it/s]Extractor Estimating: 290it [03:28,  1.61it/s]Extractor Estimating: 291it [03:29,  1.67it/s]Extractor Estimating: 292it [03:29,  1.70it/s]Extractor Estimating: 293it [03:30,  1.67it/s]Extractor Estimating: 294it [03:31,  1.66it/s]Extractor Estimating: 295it [03:31,  1.66it/s]Extractor Estimating: 296it [03:32,  1.63it/s]Extractor Estimating: 297it [03:33,  1.62it/s]Extractor Estimating: 298it [03:33,  1.64it/s]Extractor Estimating: 299it [03:34,  1.65it/s]Extractor Estimating: 300it [03:34,  1.68it/s]Extractor Estimating: 301it [03:35,  1.63it/s]Extractor Estimating: 302it [03:36,  1.56it/s]Extractor Estimating: 303it [03:36,  1.54it/s]Extractor Estimating: 304it [03:37,  1.51it/s]Extractor Estimating: 305it [03:38,  1.54it/s]Extractor Estimating: 306it [03:38,  1.56it/s]Extractor Estimating: 307it [03:39,  1.61it/s]Extractor Estimating: 308it [03:40,  1.62it/s]Extractor Estimating: 309it [03:40,  1.65it/s]Extractor Estimating: 310it [03:41,  1.63it/s]Extractor Estimating: 311it [03:41,  1.65it/s]Extractor Estimating: 312it [03:42,  1.62it/s]Extractor Estimating: 313it [03:43,  1.61it/s]Extractor Estimating: 314it [03:43,  1.59it/s]Extractor Estimating: 315it [03:44,  1.57it/s]Extractor Estimating: 316it [03:44,  1.61it/s]Extractor Estimating: 317it [03:45,  1.57it/s]Extractor Estimating: 318it [03:46,  1.51it/s]Extractor Estimating: 319it [03:47,  1.50it/s]Extractor Estimating: 320it [03:47,  1.52it/s]Extractor Estimating: 321it [03:48,  1.48it/s]Extractor Estimating: 322it [03:48,  1.52it/s]Extractor Estimating: 323it [03:49,  1.54it/s]Extractor Estimating: 324it [03:50,  1.56it/s]Extractor Estimating: 325it [03:50,  1.56it/s]Extractor Estimating: 326it [03:51,  1.57it/s]Extractor Estimating: 327it [03:52,  1.56it/s]Extractor Estimating: 328it [03:52,  1.53it/s]Extractor Estimating: 329it [03:53,  1.55it/s]Extractor Estimating: 330it [03:54,  1.56it/s]Extractor Estimating: 331it [03:54,  1.58it/s]Extractor Estimating: 332it [03:55,  1.57it/s]Extractor Estimating: 333it [03:56,  1.50it/s]Extractor Estimating: 334it [03:56,  1.54it/s]Extractor Estimating: 335it [03:57,  1.58it/s]Extractor Estimating: 336it [03:57,  1.56it/s]Extractor Estimating: 337it [03:58,  1.52it/s]Extractor Estimating: 338it [03:59,  1.51it/s]Extractor Estimating: 339it [03:59,  1.54it/s]Extractor Estimating: 340it [04:00,  1.62it/s]Extractor Estimating: 341it [04:01,  1.60it/s]Extractor Estimating: 342it [04:01,  1.61it/s]Extractor Estimating: 343it [04:02,  1.60it/s]Extractor Estimating: 344it [04:03,  1.60it/s]Extractor Estimating: 345it [04:03,  1.57it/s]Extractor Estimating: 346it [04:04,  1.58it/s]Extractor Estimating: 347it [04:04,  1.55it/s]Extractor Estimating: 348it [04:05,  1.59it/s]Extractor Estimating: 349it [04:06,  1.58it/s]Extractor Estimating: 350it [04:06,  1.60it/s]Extractor Estimating: 351it [04:07,  1.59it/s]Extractor Estimating: 352it [04:08,  1.57it/s]Extractor Estimating: 353it [04:08,  1.52it/s]Extractor Estimating: 354it [04:09,  1.53it/s]Extractor Estimating: 355it [04:10,  1.61it/s]Extractor Estimating: 356it [04:10,  1.54it/s]Extractor Estimating: 357it [04:11,  1.51it/s]Extractor Estimating: 358it [04:11,  1.56it/s]Extractor Estimating: 359it [04:12,  1.57it/s]Extractor Estimating: 360it [04:13,  1.55it/s]Extractor Estimating: 361it [04:13,  1.52it/s]Extractor Estimating: 362it [04:14,  1.48it/s]Extractor Estimating: 363it [04:15,  1.46it/s]Extractor Estimating: 364it [04:16,  1.48it/s]Extractor Estimating: 365it [04:16,  1.47it/s]Extractor Estimating: 366it [04:17,  1.50it/s]Extractor Estimating: 367it [04:18,  1.51it/s]Extractor Estimating: 368it [04:18,  1.51it/s]Extractor Estimating: 369it [04:19,  1.51it/s]Extractor Estimating: 370it [04:20,  1.53it/s]Extractor Estimating: 371it [04:20,  1.52it/s]Extractor Estimating: 372it [04:21,  1.50it/s]Extractor Estimating: 373it [04:21,  1.51it/s]Extractor Estimating: 374it [04:22,  1.54it/s]Extractor Estimating: 375it [04:23,  1.66it/s]Extractor Estimating: 375it [04:23,  1.43it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7478 mean pseudo reward: 0.9129361620168619
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 27229
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27329, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_10_seed_2/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27329, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.261, loss:1252.3748
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.974, loss:1219.9240
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.992, loss:1181.0820
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.983, loss:1127.9893
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 0.979, loss:1157.0601
>> valid entity prec:0.5901, rec:0.5493, f1:0.5690
>> valid relation prec:0.1207, rec:0.0160, f1:0.0283
>> valid relation with NER prec:0.1207, rec:0.0160, f1:0.0283
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.175, loss:1110.8620
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.983, loss:1133.3068
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.992, loss:1101.2313
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 0.975, loss:1104.5486
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 0.991, loss:1098.0083
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6047, rec:0.3966, f1:0.4790
>> valid relation prec:0.1901, rec:0.0209, f1:0.0377
>> valid relation with NER prec:0.1901, rec:0.0209, f1:0.0377
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.154, loss:1085.9766
g_step 1200, step 264, avg_time 0.965, loss:1050.5179
g_step 1300, step 52, avg_time 0.982, loss:1054.3910
g_step 1400, step 152, avg_time 0.983, loss:984.2089
g_step 1500, step 252, avg_time 0.980, loss:1017.7517
>> valid entity prec:0.6123, rec:0.4935, f1:0.5466
>> valid relation prec:0.2581, rec:0.0501, f1:0.0840
>> valid relation with NER prec:0.2581, rec:0.0501, f1:0.0840
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 40, avg_time 2.182, loss:1004.2840
g_step 1700, step 140, avg_time 0.974, loss:949.6022
g_step 1800, step 240, avg_time 0.980, loss:971.4277
g_step 1900, step 28, avg_time 0.980, loss:952.0253
g_step 2000, step 128, avg_time 0.985, loss:887.0937
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5855, rec:0.5631, f1:0.5741
>> valid relation prec:0.2132, rec:0.0619, f1:0.0959
>> valid relation with NER prec:0.2132, rec:0.0619, f1:0.0959
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.181, loss:933.0910
g_step 2200, step 16, avg_time 0.975, loss:910.2798
g_step 2300, step 116, avg_time 0.982, loss:866.5285
g_step 2400, step 216, avg_time 0.974, loss:861.4826
g_step 2500, step 4, avg_time 0.995, loss:893.9596
>> valid entity prec:0.5957, rec:0.5144, f1:0.5521
>> valid relation prec:0.2769, rec:0.0501, f1:0.0849
>> valid relation with NER prec:0.2769, rec:0.0501, f1:0.0849
g_step 2600, step 104, avg_time 2.177, loss:833.0799
g_step 2700, step 204, avg_time 0.978, loss:832.5083
g_step 2800, step 304, avg_time 0.980, loss:843.6843
g_step 2900, step 92, avg_time 0.979, loss:768.6989
g_step 3000, step 192, avg_time 0.985, loss:815.8242
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6471, rec:0.4448, f1:0.5272
>> valid relation prec:0.2118, rec:0.0370, f1:0.0629
>> valid relation with NER prec:0.2118, rec:0.0370, f1:0.0629
g_step 3100, step 292, avg_time 2.170, loss:827.1950
g_step 3200, step 80, avg_time 0.980, loss:745.8703
g_step 3300, step 180, avg_time 0.980, loss:768.6957
g_step 3400, step 280, avg_time 0.981, loss:765.8887
g_step 3500, step 68, avg_time 0.981, loss:705.7519
>> valid entity prec:0.5996, rec:0.5309, f1:0.5632
>> valid relation prec:0.1305, rec:0.0398, f1:0.0610
>> valid relation with NER prec:0.1305, rec:0.0398, f1:0.0610
g_step 3600, step 168, avg_time 2.164, loss:705.7287
g_step 3700, step 268, avg_time 0.988, loss:769.2689
g_step 3800, step 56, avg_time 0.986, loss:708.0089
g_step 3900, step 156, avg_time 0.984, loss:684.5962
g_step 4000, step 256, avg_time 0.992, loss:726.4674
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5969, rec:0.5151, f1:0.5530
>> valid relation prec:0.2109, rec:0.0521, f1:0.0836
>> valid relation with NER prec:0.2109, rec:0.0521, f1:0.0836
g_step 4100, step 44, avg_time 2.167, loss:683.6964
g_step 4200, step 144, avg_time 0.991, loss:650.6314
g_step 4300, step 244, avg_time 0.979, loss:676.0307
g_step 4400, step 32, avg_time 0.992, loss:656.2832
g_step 4500, step 132, avg_time 0.982, loss:608.5944
>> valid entity prec:0.6313, rec:0.4715, f1:0.5398
>> valid relation prec:0.1703, rec:0.0401, f1:0.0649
>> valid relation with NER prec:0.1703, rec:0.0401, f1:0.0649
g_step 4600, step 232, avg_time 2.159, loss:646.4381
g_step 4700, step 20, avg_time 0.977, loss:635.1763
g_step 4800, step 120, avg_time 0.978, loss:592.6994
g_step 4900, step 220, avg_time 0.985, loss:610.3071
g_step 5000, step 8, avg_time 0.977, loss:639.1331
learning rate was adjusted to 0.0008
>> valid entity prec:0.5884, rec:0.5075, f1:0.5450
>> valid relation prec:0.1588, rec:0.0355, f1:0.0581
>> valid relation with NER prec:0.1588, rec:0.0355, f1:0.0581
g_step 5100, step 108, avg_time 2.157, loss:585.1028
g_step 5200, step 208, avg_time 0.995, loss:583.6317
g_step 5300, step 308, avg_time 0.980, loss:597.3085
g_step 5400, step 96, avg_time 0.977, loss:526.5050
g_step 5500, step 196, avg_time 0.981, loss:555.4087
>> valid entity prec:0.6236, rec:0.4904, f1:0.5490
>> valid relation prec:0.1355, rec:0.0392, f1:0.0609
>> valid relation with NER prec:0.1355, rec:0.0392, f1:0.0609
g_step 5600, step 296, avg_time 2.179, loss:587.1990
g_step 5700, step 84, avg_time 0.975, loss:521.1293
g_step 5800, step 184, avg_time 0.986, loss:550.4910
g_step 5900, step 284, avg_time 0.983, loss:542.2577
g_step 6000, step 72, avg_time 0.977, loss:533.8570
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5714, rec:0.5490, f1:0.5600
>> valid relation prec:0.1502, rec:0.0441, f1:0.0682
>> valid relation with NER prec:0.1502, rec:0.0441, f1:0.0682
g_step 6100, step 172, avg_time 2.174, loss:521.7158
g_step 6200, step 272, avg_time 0.971, loss:518.4904
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:34:13 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:34:13 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-34-13_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:34:14 - WARNING - datasets.builder -   Using custom data configuration default-462ac7b754a7127b
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-462ac7b754a7127b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:34:14,934 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:34:14,936 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:34:14,936 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:34:14,937 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:34:14,951 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:34:14,956 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:34:14,956 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:34:14,956 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:34:14,956 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:34:14,956 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:34:14,956 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:34:15,088 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:34:18,132 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:34:18,135 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_2/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-462ac7b754a7127b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 14:34:18 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1454ccd0b050> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.90ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.73ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.05ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.50ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.80ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.01ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.16ba/s]100%|██████████| 8/8 [00:01<00:00,  4.97ba/s]100%|██████████| 8/8 [00:01<00:00,  4.19ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.04ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.27ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.35ba/s]100%|██████████| 4/4 [00:00<00:00,  5.40ba/s]100%|██████████| 4/4 [00:00<00:00,  4.91ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.11ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.29ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.48ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.45ba/s]100%|██████████| 8/8 [00:00<00:00, 11.03ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.11ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.10ba/s]100%|██████████| 4/4 [00:00<00:00, 11.38ba/s]
[INFO|trainer.py:414] 2023-08-28 14:34:22,344 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:34:22,357 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:34:22,357 >>   Num examples = 7504
[INFO|trainer.py:1149] 2023-08-28 14:34:22,357 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:34:22,357 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:34:22,357 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:34:22,357 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:34:22,357 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.32it/s]  0%|          | 2/585 [00:00<02:51,  3.41it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:48,  3.45it/s]  1%|          | 5/585 [00:01<02:47,  3.46it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:46,  3.46it/s]  1%|▏         | 8/585 [00:02<02:47,  3.45it/s]  2%|▏         | 9/585 [00:02<02:47,  3.44it/s]  2%|▏         | 10/585 [00:02<02:47,  3.43it/s]  2%|▏         | 11/585 [00:03<02:47,  3.43it/s]  2%|▏         | 12/585 [00:03<02:47,  3.42it/s]  2%|▏         | 13/585 [00:03<02:47,  3.42it/s]  2%|▏         | 14/585 [00:04<02:47,  3.42it/s]  3%|▎         | 15/585 [00:04<02:46,  3.42it/s]  3%|▎         | 16/585 [00:04<02:46,  3.42it/s]  3%|▎         | 17/585 [00:04<02:46,  3.42it/s]  3%|▎         | 18/585 [00:05<02:45,  3.42it/s]  3%|▎         | 19/585 [00:05<02:46,  3.40it/s]  3%|▎         | 20/585 [00:05<02:46,  3.40it/s]  4%|▎         | 21/585 [00:06<02:45,  3.40it/s]  4%|▍         | 22/585 [00:06<02:45,  3.41it/s]  4%|▍         | 23/585 [00:06<02:44,  3.41it/s]  4%|▍         | 24/585 [00:07<02:44,  3.41it/s]  4%|▍         | 25/585 [00:07<02:44,  3.41it/s]  4%|▍         | 26/585 [00:07<02:43,  3.41it/s]  5%|▍         | 27/585 [00:07<02:43,  3.41it/s]  5%|▍         | 28/585 [00:08<02:43,  3.41it/s]  5%|▍         | 29/585 [00:08<02:42,  3.41it/s]  5%|▌         | 30/585 [00:08<02:43,  3.39it/s]  5%|▌         | 31/585 [00:09<02:43,  3.39it/s]  5%|▌         | 32/585 [00:09<02:42,  3.40it/s]  6%|▌         | 33/585 [00:09<02:42,  3.40it/s]  6%|▌         | 34/585 [00:09<02:41,  3.41it/s]  6%|▌         | 35/585 [00:10<02:41,  3.41it/s]  6%|▌         | 36/585 [00:10<02:40,  3.41it/s]  6%|▋         | 37/585 [00:10<02:40,  3.41it/s]  6%|▋         | 38/585 [00:11<02:40,  3.41it/s]  7%|▋         | 39/585 [00:11<02:39,  3.41it/s]  7%|▋         | 40/585 [00:11<02:39,  3.41it/s]  7%|▋         | 41/585 [00:12<02:40,  3.40it/s]  7%|▋         | 42/585 [00:12<02:39,  3.40it/s]  7%|▋         | 43/585 [00:12<02:39,  3.41it/s]  8%|▊         | 44/585 [00:12<02:38,  3.41it/s]  8%|▊         | 45/585 [00:13<02:38,  3.41it/s]  8%|▊         | 46/585 [00:13<02:38,  3.41it/s]  8%|▊         | 47/585 [00:13<02:37,  3.41it/s]  8%|▊         | 48/585 [00:14<02:37,  3.41it/s]  8%|▊         | 49/585 [00:14<02:37,  3.41it/s]  9%|▊         | 50/585 [00:14<02:36,  3.41it/s]  9%|▊         | 51/585 [00:14<02:36,  3.41it/s]  9%|▉         | 52/585 [00:15<02:36,  3.40it/s]  9%|▉         | 53/585 [00:15<02:36,  3.41it/s]  9%|▉         | 54/585 [00:15<02:35,  3.41it/s]  9%|▉         | 55/585 [00:16<02:35,  3.41it/s] 10%|▉         | 56/585 [00:16<02:35,  3.41it/s] 10%|▉         | 57/585 [00:16<02:34,  3.41it/s] 10%|▉         | 58/585 [00:16<02:34,  3.41it/s] 10%|█         | 59/585 [00:17<02:34,  3.41it/s] 10%|█         | 60/585 [00:17<02:33,  3.41it/s] 10%|█         | 61/585 [00:17<02:33,  3.41it/s] 11%|█         | 62/585 [00:18<02:33,  3.41it/s] 11%|█         | 63/585 [00:18<02:33,  3.39it/s] 11%|█         | 64/585 [00:18<02:33,  3.40it/s] 11%|█         | 65/585 [00:19<02:32,  3.40it/s] 11%|█▏        | 66/585 [00:19<02:32,  3.40it/s] 11%|█▏        | 67/585 [00:19<02:32,  3.40it/s] 12%|█▏        | 68/585 [00:19<02:31,  3.41it/s] 12%|█▏        | 69/585 [00:20<02:31,  3.41it/s] 12%|█▏        | 70/585 [00:20<02:31,  3.41it/s] 12%|█▏        | 71/585 [00:20<02:30,  3.41it/s] 12%|█▏        | 72/585 [00:21<02:30,  3.41it/s] 12%|█▏        | 73/585 [00:21<02:30,  3.41it/s] 13%|█▎        | 74/585 [00:21<02:30,  3.40it/s] 13%|█▎        | 75/585 [00:21<02:29,  3.40it/s] 13%|█▎        | 76/585 [00:22<02:29,  3.40it/s] 13%|█▎        | 77/585 [00:22<02:29,  3.41it/s] 13%|█▎        | 78/585 [00:22<02:28,  3.41it/s] 14%|█▎        | 79/585 [00:23<02:28,  3.41it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.41it/s] 14%|█▍        | 81/585 [00:23<02:27,  3.41it/s] 14%|█▍        | 82/585 [00:24<02:27,  3.41it/s] 14%|█▍        | 83/585 [00:24<02:27,  3.40it/s] 14%|█▍        | 84/585 [00:24<02:27,  3.41it/s] 15%|█▍        | 85/585 [00:24<02:26,  3.41it/s] 15%|█▍        | 86/585 [00:25<02:26,  3.41it/s] 15%|█▍        | 87/585 [00:25<02:26,  3.40it/s] 15%|█▌        | 88/585 [00:25<02:26,  3.40it/s] 15%|█▌        | 89/585 [00:26<02:25,  3.40it/s] 15%|█▌        | 90/585 [00:26<02:25,  3.41it/s] 16%|█▌        | 91/585 [00:26<02:24,  3.41it/s] 16%|█▌        | 92/585 [00:26<02:24,  3.41it/s] 16%|█▌        | 93/585 [00:27<02:24,  3.41it/s] 16%|█▌        | 94/585 [00:27<02:24,  3.41it/s] 16%|█▌        | 95/585 [00:27<02:23,  3.40it/s] 16%|█▋        | 96/585 [00:28<02:23,  3.41it/s] 17%|█▋        | 97/585 [00:28<02:23,  3.41it/s] 17%|█▋        | 98/585 [00:28<02:23,  3.39it/s] 17%|█▋        | 99/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 100/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 101/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 102/585 [00:29<02:21,  3.41it/s] 18%|█▊        | 103/585 [00:30<02:21,  3.41it/s] 18%|█▊        | 104/585 [00:30<02:21,  3.41it/s] 18%|█▊        | 105/585 [00:30<02:20,  3.41it/s] 18%|█▊        | 106/585 [00:31<02:20,  3.41it/s] 18%|█▊        | 107/585 [00:31<02:20,  3.41it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.40it/s] 19%|█▊        | 109/585 [00:31<02:20,  3.39it/s] 19%|█▉        | 110/585 [00:32<02:19,  3.39it/s] 19%|█▉        | 111/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 112/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 113/585 [00:33<02:18,  3.40it/s] 19%|█▉        | 114/585 [00:33<02:18,  3.40it/s] 20%|█▉        | 115/585 [00:33<02:18,  3.40it/s] 20%|█▉        | 116/585 [00:34<02:17,  3.41it/s] 20%|██        | 117/585 [00:34<02:17,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 14:34:56,718 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:34:56,718 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:34:56,718 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.15it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.59it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.92it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.14it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.61it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.29it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.07it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.94it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.93it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.10it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.06it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.96it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.85it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.86it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.74it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.82it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.88it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.99it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 45.03it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.94it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.94it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.84it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.80it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.81it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.69it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.87it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.88it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 45.15it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 45.07it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.79it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.65it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.76it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.75it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.80it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.85it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 45.00it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 45.12it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 45.00it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.93it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.79it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.66it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.65it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.73it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.71it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.97it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 45.07it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 45.03it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.91it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.87it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.75it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.69it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.69it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.76it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.99it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 45.01it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 45.04it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.85it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.83it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.77it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.62it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.58it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.80it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 45.01it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 45.02it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.97it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.95it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.93it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.87it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.72it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.65it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.83it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.95it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.97it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.91it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.91it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.94it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.76it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.72it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.65it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.81it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.78it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.86it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.94it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.93it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.86it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.67it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:44<02:17,  3.41it/s]
100%|██████████| 437/437 [00:09<00:00, 44.67it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:35:06,506 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 14:35:06,543 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:35:09,078 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:35:09,096 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:35:09,106 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:51<41:34,  5.34s/it] 20%|██        | 119/585 [00:51<29:43,  3.83s/it] 21%|██        | 120/585 [00:52<21:27,  2.77s/it] 21%|██        | 121/585 [00:52<15:39,  2.03s/it] 21%|██        | 122/585 [00:52<11:37,  1.51s/it] 21%|██        | 123/585 [00:52<08:47,  1.14s/it] 21%|██        | 124/585 [00:53<06:49,  1.13it/s] 21%|██▏       | 125/585 [00:53<05:26,  1.41it/s] 22%|██▏       | 126/585 [00:53<04:28,  1.71it/s] 22%|██▏       | 127/585 [00:54<03:47,  2.01it/s] 22%|██▏       | 128/585 [00:54<03:19,  2.29it/s] 22%|██▏       | 129/585 [00:54<02:59,  2.54it/s] 22%|██▏       | 130/585 [00:54<02:45,  2.75it/s] 22%|██▏       | 131/585 [00:55<02:35,  2.92it/s] 23%|██▎       | 132/585 [00:55<02:28,  3.05it/s] 23%|██▎       | 133/585 [00:55<02:23,  3.15it/s] 23%|██▎       | 134/585 [00:56<02:20,  3.21it/s] 23%|██▎       | 135/585 [00:56<02:17,  3.27it/s] 23%|██▎       | 136/585 [00:56<02:15,  3.31it/s] 23%|██▎       | 137/585 [00:57<02:14,  3.34it/s] 24%|██▎       | 138/585 [00:57<02:13,  3.36it/s] 24%|██▍       | 139/585 [00:57<02:12,  3.37it/s] 24%|██▍       | 140/585 [00:57<02:11,  3.38it/s] 24%|██▍       | 141/585 [00:58<02:10,  3.39it/s] 24%|██▍       | 142/585 [00:58<02:10,  3.40it/s] 24%|██▍       | 143/585 [00:58<02:10,  3.40it/s] 25%|██▍       | 144/585 [00:59<02:09,  3.40it/s] 25%|██▍       | 145/585 [00:59<02:09,  3.39it/s] 25%|██▍       | 146/585 [00:59<02:09,  3.40it/s] 25%|██▌       | 147/585 [00:59<02:08,  3.40it/s] 25%|██▌       | 148/585 [01:00<02:08,  3.40it/s] 25%|██▌       | 149/585 [01:00<02:08,  3.40it/s] 26%|██▌       | 150/585 [01:00<02:07,  3.41it/s] 26%|██▌       | 151/585 [01:01<02:07,  3.41it/s] 26%|██▌       | 152/585 [01:01<02:07,  3.41it/s] 26%|██▌       | 153/585 [01:01<02:06,  3.41it/s] 26%|██▋       | 154/585 [01:02<02:06,  3.41it/s] 26%|██▋       | 155/585 [01:02<02:06,  3.41it/s] 27%|██▋       | 156/585 [01:02<02:06,  3.40it/s] 27%|██▋       | 157/585 [01:02<02:05,  3.40it/s] 27%|██▋       | 158/585 [01:03<02:05,  3.41it/s] 27%|██▋       | 159/585 [01:03<02:05,  3.41it/s] 27%|██▋       | 160/585 [01:03<02:04,  3.41it/s] 28%|██▊       | 161/585 [01:04<02:04,  3.41it/s] 28%|██▊       | 162/585 [01:04<02:04,  3.41it/s] 28%|██▊       | 163/585 [01:04<02:03,  3.41it/s] 28%|██▊       | 164/585 [01:04<02:03,  3.41it/s] 28%|██▊       | 165/585 [01:05<02:03,  3.41it/s] 28%|██▊       | 166/585 [01:05<02:02,  3.42it/s] 29%|██▊       | 167/585 [01:05<02:02,  3.42it/s] 29%|██▊       | 168/585 [01:06<02:01,  3.44it/s] 29%|██▉       | 169/585 [01:06<02:00,  3.44it/s] 29%|██▉       | 170/585 [01:06<02:00,  3.44it/s] 29%|██▉       | 171/585 [01:06<02:00,  3.45it/s] 29%|██▉       | 172/585 [01:07<01:59,  3.45it/s] 30%|██▉       | 173/585 [01:07<01:59,  3.45it/s] 30%|██▉       | 174/585 [01:07<01:58,  3.46it/s] 30%|██▉       | 175/585 [01:08<01:58,  3.46it/s] 30%|███       | 176/585 [01:08<01:58,  3.46it/s] 30%|███       | 177/585 [01:08<01:58,  3.46it/s] 30%|███       | 178/585 [01:09<01:59,  3.42it/s] 31%|███       | 179/585 [01:09<01:58,  3.43it/s] 31%|███       | 180/585 [01:09<01:57,  3.44it/s] 31%|███       | 181/585 [01:09<01:57,  3.44it/s] 31%|███       | 182/585 [01:10<01:56,  3.45it/s] 31%|███▏      | 183/585 [01:10<01:56,  3.45it/s] 31%|███▏      | 184/585 [01:10<01:56,  3.45it/s] 32%|███▏      | 185/585 [01:11<01:55,  3.46it/s] 32%|███▏      | 186/585 [01:11<01:55,  3.46it/s] 32%|███▏      | 187/585 [01:11<01:55,  3.46it/s] 32%|███▏      | 188/585 [01:11<01:54,  3.46it/s] 32%|███▏      | 189/585 [01:12<01:55,  3.44it/s] 32%|███▏      | 190/585 [01:12<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:12<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:13<01:53,  3.45it/s] 33%|███▎      | 193/585 [01:13<01:53,  3.45it/s] 33%|███▎      | 194/585 [01:13<01:53,  3.45it/s] 33%|███▎      | 195/585 [01:13<01:52,  3.46it/s] 34%|███▎      | 196/585 [01:14<01:52,  3.46it/s] 34%|███▎      | 197/585 [01:14<01:52,  3.46it/s] 34%|███▍      | 198/585 [01:14<01:51,  3.46it/s] 34%|███▍      | 199/585 [01:15<01:51,  3.46it/s] 34%|███▍      | 200/585 [01:15<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:15<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:15<01:50,  3.45it/s] 35%|███▍      | 203/585 [01:16<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:16<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:16<01:50,  3.45it/s] 35%|███▌      | 206/585 [01:17<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:17<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:17<01:49,  3.45it/s] 36%|███▌      | 209/585 [01:17<01:48,  3.45it/s] 36%|███▌      | 210/585 [01:18<01:48,  3.45it/s] 36%|███▌      | 211/585 [01:18<01:48,  3.44it/s] 36%|███▌      | 212/585 [01:18<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:19<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:19<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:19<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:20<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:20<01:46,  3.45it/s] 37%|███▋      | 218/585 [01:20<01:46,  3.45it/s] 37%|███▋      | 219/585 [01:20<01:45,  3.45it/s] 38%|███▊      | 220/585 [01:21<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:21<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:21<01:45,  3.44it/s] 38%|███▊      | 223/585 [01:22<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:22<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:22<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:22<01:44,  3.45it/s] 39%|███▉      | 227/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:23<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:23<01:42,  3.46it/s] 39%|███▉      | 230/585 [01:24<01:42,  3.46it/s] 39%|███▉      | 231/585 [01:24<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:24<01:42,  3.46it/s] 40%|███▉      | 233/585 [01:24<01:41,  3.46it/s] 40%|████      | 234/585 [01:25<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 14:35:47,629 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:35:47,629 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:35:47,629 >>   Batch size = 8
{'eval_loss': 1.0033109188079834, 'eval_runtime': 9.7694, 'eval_samples_per_second': 357.546, 'eval_steps_per_second': 44.732, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.04it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.39it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.64it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.94it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.53it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.24it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.99it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.82it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.98it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.11it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.96it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.86it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.83it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.87it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.74it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.62it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.67it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.69it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.93it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.92it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.88it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.89it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.81it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.72it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.65it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.72it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.73it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.90it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.97it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.96it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.86it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.80it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.68it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.69it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.67it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.77it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.60it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.81it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.93it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.99it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.74it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.72it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.78it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.67it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.80it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.78it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.80it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.97it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.94it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.78it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.84it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.73it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.74it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.71it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.91it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.85it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.95it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.97it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.91it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.82it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.81it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.68it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.67it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.84it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.73it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.90it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.92it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.80it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.91it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.78it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.70it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.70it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.75it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.55it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.88it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 45.01it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.96it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.94it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.93it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.72it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.75it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.70it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.76it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.83it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.92it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 45.00it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.97it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:35<01:41,  3.46it/s]
100%|██████████| 437/437 [00:09<00:00, 44.97it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:35:57,435 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 14:35:57,455 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:35:59,609 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:35:59,626 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:35:59,641 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:41<30:20,  5.20s/it] 40%|████      | 236/585 [01:42<21:41,  3.73s/it] 41%|████      | 237/585 [01:42<15:39,  2.70s/it] 41%|████      | 238/585 [01:42<11:26,  1.98s/it] 41%|████      | 239/585 [01:43<08:29,  1.47s/it] 41%|████      | 240/585 [01:43<06:25,  1.12s/it] 41%|████      | 241/585 [01:43<04:59,  1.15it/s] 41%|████▏     | 242/585 [01:43<03:59,  1.43it/s] 42%|████▏     | 243/585 [01:44<03:17,  1.74it/s] 42%|████▏     | 244/585 [01:44<02:47,  2.04it/s] 42%|████▏     | 245/585 [01:44<02:26,  2.31it/s] 42%|████▏     | 246/585 [01:45<02:12,  2.56it/s] 42%|████▏     | 247/585 [01:45<02:02,  2.76it/s] 42%|████▏     | 248/585 [01:45<01:55,  2.92it/s] 43%|████▎     | 249/585 [01:46<01:49,  3.06it/s] 43%|████▎     | 250/585 [01:46<01:45,  3.17it/s] 43%|████▎     | 251/585 [01:46<01:42,  3.25it/s] 43%|████▎     | 252/585 [01:46<01:40,  3.31it/s] 43%|████▎     | 253/585 [01:47<01:38,  3.35it/s] 43%|████▎     | 254/585 [01:47<01:37,  3.39it/s] 44%|████▎     | 255/585 [01:47<01:36,  3.41it/s] 44%|████▍     | 256/585 [01:48<01:36,  3.42it/s] 44%|████▍     | 257/585 [01:48<01:35,  3.43it/s] 44%|████▍     | 258/585 [01:48<01:37,  3.35it/s] 44%|████▍     | 259/585 [01:48<01:36,  3.38it/s] 44%|████▍     | 260/585 [01:49<01:35,  3.40it/s] 45%|████▍     | 261/585 [01:49<01:34,  3.41it/s] 45%|████▍     | 262/585 [01:49<01:34,  3.43it/s] 45%|████▍     | 263/585 [01:50<01:33,  3.44it/s] 45%|████▌     | 264/585 [01:50<01:33,  3.44it/s] 45%|████▌     | 265/585 [01:50<01:33,  3.44it/s] 45%|████▌     | 266/585 [01:50<01:32,  3.44it/s] 46%|████▌     | 267/585 [01:51<01:32,  3.44it/s] 46%|████▌     | 268/585 [01:51<02:00,  2.63it/s] 46%|████▌     | 269/585 [01:52<01:55,  2.74it/s] 46%|████▌     | 270/585 [01:52<01:47,  2.93it/s] 46%|████▋     | 271/585 [01:52<01:42,  3.07it/s] 46%|████▋     | 272/585 [01:53<01:38,  3.18it/s] 47%|████▋     | 273/585 [01:53<01:35,  3.26it/s] 47%|████▋     | 274/585 [01:53<01:33,  3.31it/s] 47%|████▋     | 275/585 [01:53<01:32,  3.36it/s] 47%|████▋     | 276/585 [01:54<01:31,  3.39it/s] 47%|████▋     | 277/585 [01:54<01:30,  3.41it/s] 48%|████▊     | 278/585 [01:54<01:29,  3.42it/s] 48%|████▊     | 279/585 [01:55<01:29,  3.43it/s] 48%|████▊     | 280/585 [01:55<01:28,  3.44it/s] 48%|████▊     | 281/585 [01:55<01:28,  3.44it/s] 48%|████▊     | 282/585 [01:55<01:27,  3.45it/s] 48%|████▊     | 283/585 [01:56<01:27,  3.45it/s] 49%|████▊     | 284/585 [01:56<01:27,  3.45it/s] 49%|████▊     | 285/585 [01:56<01:26,  3.46it/s] 49%|████▉     | 286/585 [01:57<01:26,  3.46it/s] 49%|████▉     | 287/585 [01:57<01:26,  3.46it/s] 49%|████▉     | 288/585 [01:57<01:26,  3.44it/s] 49%|████▉     | 289/585 [01:57<01:25,  3.45it/s] 50%|████▉     | 290/585 [01:58<01:25,  3.45it/s] 50%|████▉     | 291/585 [01:58<01:25,  3.45it/s] 50%|████▉     | 292/585 [01:58<01:24,  3.45it/s] 50%|█████     | 293/585 [01:59<01:24,  3.45it/s] 50%|█████     | 294/585 [01:59<01:24,  3.45it/s] 50%|█████     | 295/585 [01:59<01:23,  3.45it/s] 51%|█████     | 296/585 [01:59<01:23,  3.46it/s] 51%|█████     | 297/585 [02:00<01:23,  3.45it/s] 51%|█████     | 298/585 [02:00<01:23,  3.46it/s] 51%|█████     | 299/585 [02:00<01:22,  3.45it/s] 51%|█████▏    | 300/585 [02:01<01:22,  3.45it/s] 51%|█████▏    | 301/585 [02:01<01:22,  3.45it/s] 52%|█████▏    | 302/585 [02:01<01:21,  3.45it/s] 52%|█████▏    | 303/585 [02:02<01:21,  3.45it/s] 52%|█████▏    | 304/585 [02:02<01:21,  3.45it/s] 52%|█████▏    | 305/585 [02:02<01:21,  3.45it/s] 52%|█████▏    | 306/585 [02:02<01:20,  3.45it/s] 52%|█████▏    | 307/585 [02:03<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:03<01:20,  3.46it/s] 53%|█████▎    | 309/585 [02:03<01:19,  3.45it/s] 53%|█████▎    | 310/585 [02:04<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:04<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:04<01:19,  3.45it/s] 54%|█████▎    | 313/585 [02:04<01:18,  3.45it/s] 54%|█████▎    | 314/585 [02:05<01:18,  3.45it/s] 54%|█████▍    | 315/585 [02:05<01:18,  3.45it/s] 54%|█████▍    | 316/585 [02:05<01:17,  3.45it/s] 54%|█████▍    | 317/585 [02:06<01:17,  3.45it/s] 54%|█████▍    | 318/585 [02:06<01:17,  3.45it/s] 55%|█████▍    | 319/585 [02:06<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:06<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:07<01:16,  3.44it/s] 55%|█████▌    | 322/585 [02:07<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:07<01:15,  3.45it/s] 55%|█████▌    | 324/585 [02:08<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:08<01:15,  3.45it/s] 56%|█████▌    | 326/585 [02:08<01:15,  3.45it/s] 56%|█████▌    | 327/585 [02:08<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:09<01:14,  3.46it/s] 56%|█████▌    | 329/585 [02:09<01:14,  3.46it/s] 56%|█████▋    | 330/585 [02:09<01:13,  3.46it/s] 57%|█████▋    | 331/585 [02:10<01:13,  3.46it/s] 57%|█████▋    | 332/585 [02:10<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:10<01:13,  3.45it/s] 57%|█████▋    | 334/585 [02:10<01:12,  3.45it/s] 57%|█████▋    | 335/585 [02:11<01:12,  3.45it/s] 57%|█████▋    | 336/585 [02:11<01:12,  3.45it/s] 58%|█████▊    | 337/585 [02:11<01:11,  3.45it/s] 58%|█████▊    | 338/585 [02:12<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:12<01:11,  3.46it/s] 58%|█████▊    | 340/585 [02:12<01:10,  3.46it/s] 58%|█████▊    | 341/585 [02:13<01:10,  3.45it/s] 58%|█████▊    | 342/585 [02:13<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:13<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:13<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:14<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:14<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:14<01:08,  3.45it/s] 59%|█████▉    | 348/585 [02:15<01:08,  3.46it/s] 60%|█████▉    | 349/585 [02:15<01:08,  3.46it/s] 60%|█████▉    | 350/585 [02:15<01:07,  3.46it/s] 60%|██████    | 351/585 [02:15<01:07,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 14:36:38,305 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:36:38,305 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:36:38,305 >>   Batch size = 8
{'eval_loss': 0.9893708229064941, 'eval_runtime': 9.7794, 'eval_samples_per_second': 357.181, 'eval_steps_per_second': 44.686, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.31it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.45it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.85it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.07it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.47it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.24it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.11it/s][A
 10%|▉         | 42/437 [00:00<00:08, 45.01it/s][A
 11%|█         | 47/437 [00:01<00:08, 45.11it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.10it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.96it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.00it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.97it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.83it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.83it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.88it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.73it/s][A
 21%|██        | 92/437 [00:02<00:07, 45.03it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 45.08it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.93it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.86it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.97it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.90it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.90it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.80it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.83it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.85it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.93it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 45.00it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.94it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.86it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.91it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.83it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.90it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.85it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.89it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.93it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.99it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.86it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.88it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.97it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.98it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.95it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.87it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.84it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.85it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.87it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.92it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.85it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.89it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.79it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.93it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.95it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.76it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.80it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.86it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.87it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.89it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.90it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.89it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.80it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.89it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.77it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.84it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.81it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.97it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.95it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.93it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.92it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.92it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.91it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.82it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.82it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.75it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.87it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.95it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.85it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.89it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.71it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.85it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.85it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.84it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.84it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.79it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.92it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.90it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.90it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:25<01:07,  3.46it/s]
100%|██████████| 437/437 [00:09<00:00, 44.90it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:36:48,087 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 14:36:48,109 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:36:50,266 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:36:50,283 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:36:50,294 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:32<20:20,  5.24s/it] 60%|██████    | 353/585 [02:32<14:31,  3.76s/it] 61%|██████    | 354/585 [02:33<10:27,  2.72s/it] 61%|██████    | 355/585 [02:33<07:37,  1.99s/it] 61%|██████    | 356/585 [02:33<05:39,  1.48s/it] 61%|██████    | 357/585 [02:34<04:16,  1.12s/it] 61%|██████    | 358/585 [02:34<03:18,  1.15it/s] 61%|██████▏   | 359/585 [02:34<02:37,  1.43it/s] 62%|██████▏   | 360/585 [02:35<02:09,  1.74it/s] 62%|██████▏   | 361/585 [02:35<01:49,  2.04it/s] 62%|██████▏   | 362/585 [02:35<01:35,  2.33it/s] 62%|██████▏   | 363/585 [02:35<01:25,  2.58it/s] 62%|██████▏   | 364/585 [02:36<01:19,  2.79it/s] 62%|██████▏   | 365/585 [02:36<01:14,  2.96it/s] 63%|██████▎   | 366/585 [02:36<01:10,  3.09it/s] 63%|██████▎   | 367/585 [02:37<01:08,  3.20it/s] 63%|██████▎   | 368/585 [02:37<01:06,  3.27it/s] 63%|██████▎   | 369/585 [02:37<01:04,  3.33it/s] 63%|██████▎   | 370/585 [02:37<01:03,  3.37it/s] 63%|██████▎   | 371/585 [02:38<01:03,  3.40it/s] 64%|██████▎   | 372/585 [02:38<01:02,  3.42it/s] 64%|██████▍   | 373/585 [02:38<01:01,  3.43it/s] 64%|██████▍   | 374/585 [02:39<01:01,  3.44it/s] 64%|██████▍   | 375/585 [02:39<01:01,  3.43it/s] 64%|██████▍   | 376/585 [02:39<01:00,  3.44it/s] 64%|██████▍   | 377/585 [02:39<01:00,  3.45it/s] 65%|██████▍   | 378/585 [02:40<00:59,  3.45it/s] 65%|██████▍   | 379/585 [02:40<00:59,  3.45it/s] 65%|██████▍   | 380/585 [02:40<00:59,  3.45it/s] 65%|██████▌   | 381/585 [02:41<00:59,  3.46it/s] 65%|██████▌   | 382/585 [02:41<00:58,  3.46it/s] 65%|██████▌   | 383/585 [02:41<00:58,  3.46it/s] 66%|██████▌   | 384/585 [02:41<00:58,  3.46it/s] 66%|██████▌   | 385/585 [02:42<00:57,  3.46it/s] 66%|██████▌   | 386/585 [02:42<00:57,  3.44it/s] 66%|██████▌   | 387/585 [02:42<00:57,  3.45it/s] 66%|██████▋   | 388/585 [02:43<00:57,  3.45it/s] 66%|██████▋   | 389/585 [02:43<00:56,  3.45it/s] 67%|██████▋   | 390/585 [02:43<00:56,  3.45it/s] 67%|██████▋   | 391/585 [02:44<00:56,  3.46it/s] 67%|██████▋   | 392/585 [02:44<00:55,  3.46it/s] 67%|██████▋   | 393/585 [02:44<00:55,  3.46it/s] 67%|██████▋   | 394/585 [02:44<00:55,  3.46it/s] 68%|██████▊   | 395/585 [02:45<00:54,  3.46it/s] 68%|██████▊   | 396/585 [02:45<00:54,  3.46it/s] 68%|██████▊   | 397/585 [02:45<00:54,  3.44it/s] 68%|██████▊   | 398/585 [02:46<00:54,  3.44it/s] 68%|██████▊   | 399/585 [02:46<00:53,  3.45it/s] 68%|██████▊   | 400/585 [02:46<00:53,  3.45it/s] 69%|██████▊   | 401/585 [02:46<00:53,  3.45it/s] 69%|██████▊   | 402/585 [02:47<00:53,  3.45it/s] 69%|██████▉   | 403/585 [02:47<00:52,  3.45it/s] 69%|██████▉   | 404/585 [02:47<00:52,  3.46it/s] 69%|██████▉   | 405/585 [02:48<00:52,  3.46it/s] 69%|██████▉   | 406/585 [02:48<00:51,  3.46it/s] 70%|██████▉   | 407/585 [02:48<00:51,  3.46it/s] 70%|██████▉   | 408/585 [02:48<00:52,  3.35it/s] 70%|██████▉   | 409/585 [02:49<00:52,  3.38it/s] 70%|███████   | 410/585 [02:49<00:51,  3.40it/s] 70%|███████   | 411/585 [02:49<00:50,  3.42it/s] 70%|███████   | 412/585 [02:50<00:50,  3.43it/s] 71%|███████   | 413/585 [02:50<00:49,  3.44it/s] 71%|███████   | 414/585 [02:50<00:49,  3.45it/s] 71%|███████   | 415/585 [02:50<00:49,  3.45it/s] 71%|███████   | 416/585 [02:51<00:48,  3.45it/s] 71%|███████▏  | 417/585 [02:51<00:48,  3.46it/s] 71%|███████▏  | 418/585 [02:51<00:48,  3.46it/s] 72%|███████▏  | 419/585 [02:52<00:48,  3.43it/s] 72%|███████▏  | 420/585 [02:52<00:47,  3.44it/s] 72%|███████▏  | 421/585 [02:52<00:47,  3.45it/s] 72%|███████▏  | 422/585 [02:53<00:47,  3.45it/s] 72%|███████▏  | 423/585 [02:53<00:46,  3.45it/s] 72%|███████▏  | 424/585 [02:53<00:46,  3.45it/s] 73%|███████▎  | 425/585 [02:53<00:46,  3.46it/s] 73%|███████▎  | 426/585 [02:54<00:45,  3.46it/s] 73%|███████▎  | 427/585 [02:54<00:45,  3.46it/s] 73%|███████▎  | 428/585 [02:54<00:45,  3.46it/s] 73%|███████▎  | 429/585 [02:55<00:45,  3.46it/s] 74%|███████▎  | 430/585 [02:55<00:45,  3.43it/s] 74%|███████▎  | 431/585 [02:55<00:44,  3.44it/s] 74%|███████▍  | 432/585 [02:55<00:44,  3.45it/s] 74%|███████▍  | 433/585 [02:56<00:44,  3.45it/s] 74%|███████▍  | 434/585 [02:56<00:43,  3.45it/s] 74%|███████▍  | 435/585 [02:56<00:43,  3.46it/s] 75%|███████▍  | 436/585 [02:57<00:43,  3.46it/s] 75%|███████▍  | 437/585 [02:57<00:42,  3.46it/s] 75%|███████▍  | 438/585 [02:57<00:42,  3.46it/s] 75%|███████▌  | 439/585 [02:57<00:42,  3.46it/s] 75%|███████▌  | 440/585 [02:58<00:41,  3.46it/s] 75%|███████▌  | 441/585 [02:58<00:41,  3.46it/s] 76%|███████▌  | 442/585 [02:58<00:41,  3.46it/s] 76%|███████▌  | 443/585 [02:59<00:41,  3.44it/s] 76%|███████▌  | 444/585 [02:59<00:40,  3.45it/s] 76%|███████▌  | 445/585 [02:59<00:40,  3.45it/s] 76%|███████▌  | 446/585 [02:59<00:40,  3.46it/s] 76%|███████▋  | 447/585 [03:00<00:39,  3.45it/s] 77%|███████▋  | 448/585 [03:00<00:39,  3.46it/s] 77%|███████▋  | 449/585 [03:00<00:39,  3.46it/s] 77%|███████▋  | 450/585 [03:01<00:39,  3.46it/s] 77%|███████▋  | 451/585 [03:01<00:38,  3.46it/s] 77%|███████▋  | 452/585 [03:01<00:38,  3.44it/s] 77%|███████▋  | 453/585 [03:01<00:38,  3.43it/s] 78%|███████▊  | 454/585 [03:02<00:38,  3.41it/s] 78%|███████▊  | 455/585 [03:02<00:38,  3.41it/s] 78%|███████▊  | 456/585 [03:02<00:37,  3.41it/s] 78%|███████▊  | 457/585 [03:03<00:37,  3.41it/s] 78%|███████▊  | 458/585 [03:03<00:37,  3.41it/s] 78%|███████▊  | 459/585 [03:03<00:36,  3.41it/s] 79%|███████▊  | 460/585 [03:04<00:36,  3.41it/s] 79%|███████▉  | 461/585 [03:04<00:36,  3.41it/s] 79%|███████▉  | 462/585 [03:04<00:36,  3.41it/s] 79%|███████▉  | 463/585 [03:04<00:35,  3.41it/s] 79%|███████▉  | 464/585 [03:05<00:35,  3.41it/s] 79%|███████▉  | 465/585 [03:05<00:35,  3.40it/s] 80%|███████▉  | 466/585 [03:05<00:35,  3.40it/s] 80%|███████▉  | 467/585 [03:06<00:34,  3.40it/s] 80%|████████  | 468/585 [03:06<00:34,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 14:37:28,799 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:37:28,799 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:37:28,799 >>   Batch size = 8
{'eval_loss': 0.986352801322937, 'eval_runtime': 9.7652, 'eval_samples_per_second': 357.7, 'eval_steps_per_second': 44.751, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.41it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.76it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.95it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.04it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.44it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.28it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.08it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.87it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.96it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.90it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.27it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.91it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.97it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.93it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.79it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.81it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.72it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.79it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.96it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 45.06it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 45.01it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.94it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.93it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.75it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.75it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.81it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.76it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.92it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 45.01it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.97it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.96it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.87it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.76it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.76it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.80it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.82it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.86it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.88it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.94it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.96it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.90it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.78it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.79it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.91it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.92it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.82it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.79it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.90it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.95it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.88it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.89it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.87it/s][A
 61%|██████    | 267/437 [00:05<00:03, 45.00it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.97it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.82it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.86it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.91it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.92it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.80it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.68it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.89it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.96it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.73it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.79it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.84it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.79it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.68it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.57it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.53it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.74it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.08it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.29it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.48it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.55it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.64it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.65it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.73it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.72it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.80it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.76it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.95it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.82it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.81it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.75it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.71it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.75it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.83it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:16<00:34,  3.40it/s]
100%|██████████| 437/437 [00:09<00:00, 44.83it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:37:38,597 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 14:37:38,616 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:37:41,408 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:37:41,562 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:37:41,635 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:24<10:39,  5.52s/it] 80%|████████  | 470/585 [03:24<07:34,  3.95s/it] 81%|████████  | 471/585 [03:24<05:25,  2.85s/it] 81%|████████  | 472/585 [03:24<03:55,  2.09s/it] 81%|████████  | 473/585 [03:25<02:53,  1.55s/it] 81%|████████  | 474/585 [03:25<02:10,  1.17s/it] 81%|████████  | 475/585 [03:25<01:39,  1.10it/s] 81%|████████▏ | 476/585 [03:26<01:18,  1.38it/s] 82%|████████▏ | 477/585 [03:26<01:04,  1.68it/s] 82%|████████▏ | 478/585 [03:26<00:53,  1.99it/s] 82%|████████▏ | 479/585 [03:27<00:46,  2.27it/s] 82%|████████▏ | 480/585 [03:27<00:41,  2.52it/s] 82%|████████▏ | 481/585 [03:27<00:37,  2.74it/s] 82%|████████▏ | 482/585 [03:27<00:35,  2.91it/s] 83%|████████▎ | 483/585 [03:28<00:33,  3.05it/s] 83%|████████▎ | 484/585 [03:28<00:32,  3.15it/s] 83%|████████▎ | 485/585 [03:28<00:31,  3.22it/s] 83%|████████▎ | 486/585 [03:29<00:30,  3.28it/s] 83%|████████▎ | 487/585 [03:29<00:29,  3.32it/s] 83%|████████▎ | 488/585 [03:29<00:29,  3.33it/s] 84%|████████▎ | 489/585 [03:29<00:28,  3.36it/s] 84%|████████▍ | 490/585 [03:30<00:28,  3.38it/s] 84%|████████▍ | 491/585 [03:30<00:27,  3.39it/s] 84%|████████▍ | 492/585 [03:30<00:27,  3.40it/s] 84%|████████▍ | 493/585 [03:31<00:27,  3.40it/s] 84%|████████▍ | 494/585 [03:31<00:26,  3.40it/s] 85%|████████▍ | 495/585 [03:31<00:26,  3.41it/s] 85%|████████▍ | 496/585 [03:32<00:26,  3.41it/s] 85%|████████▍ | 497/585 [03:32<00:25,  3.41it/s] 85%|████████▌ | 498/585 [03:32<00:25,  3.41it/s] 85%|████████▌ | 499/585 [03:32<00:25,  3.40it/s] 85%|████████▌ | 500/585 [03:33<00:24,  3.40it/s]                                                  85%|████████▌ | 500/585 [03:33<00:24,  3.40it/s] 86%|████████▌ | 501/585 [03:33<00:24,  3.41it/s] 86%|████████▌ | 502/585 [03:33<00:24,  3.41it/s] 86%|████████▌ | 503/585 [03:34<00:24,  3.41it/s] 86%|████████▌ | 504/585 [03:34<00:23,  3.41it/s] 86%|████████▋ | 505/585 [03:34<00:23,  3.41it/s] 86%|████████▋ | 506/585 [03:34<00:23,  3.41it/s] 87%|████████▋ | 507/585 [03:35<00:22,  3.43it/s] 87%|████████▋ | 508/585 [03:35<00:22,  3.44it/s] 87%|████████▋ | 509/585 [03:35<00:22,  3.44it/s] 87%|████████▋ | 510/585 [03:36<00:21,  3.43it/s] 87%|████████▋ | 511/585 [03:36<00:21,  3.44it/s] 88%|████████▊ | 512/585 [03:36<00:21,  3.44it/s] 88%|████████▊ | 513/585 [03:36<00:20,  3.45it/s] 88%|████████▊ | 514/585 [03:37<00:20,  3.45it/s] 88%|████████▊ | 515/585 [03:37<00:20,  3.46it/s] 88%|████████▊ | 516/585 [03:37<00:19,  3.46it/s] 88%|████████▊ | 517/585 [03:38<00:19,  3.46it/s] 89%|████████▊ | 518/585 [03:38<00:19,  3.46it/s] 89%|████████▊ | 519/585 [03:38<00:19,  3.46it/s] 89%|████████▉ | 520/585 [03:39<00:18,  3.46it/s] 89%|████████▉ | 521/585 [03:39<00:18,  3.37it/s] 89%|████████▉ | 522/585 [03:39<00:18,  3.40it/s] 89%|████████▉ | 523/585 [03:39<00:18,  3.42it/s] 90%|████████▉ | 524/585 [03:40<00:17,  3.43it/s] 90%|████████▉ | 525/585 [03:40<00:17,  3.44it/s] 90%|████████▉ | 526/585 [03:40<00:17,  3.44it/s] 90%|█████████ | 527/585 [03:41<00:16,  3.45it/s] 90%|█████████ | 528/585 [03:41<00:16,  3.45it/s] 90%|█████████ | 529/585 [03:41<00:16,  3.45it/s] 91%|█████████ | 530/585 [03:41<00:15,  3.45it/s] 91%|█████████ | 531/585 [03:42<00:15,  3.45it/s] 91%|█████████ | 532/585 [03:42<00:15,  3.35it/s] 91%|█████████ | 533/585 [03:42<00:15,  3.38it/s] 91%|█████████▏| 534/585 [03:43<00:14,  3.40it/s] 91%|█████████▏| 535/585 [03:43<00:14,  3.42it/s] 92%|█████████▏| 536/585 [03:43<00:14,  3.43it/s] 92%|█████████▏| 537/585 [03:43<00:13,  3.44it/s] 92%|█████████▏| 538/585 [03:44<00:13,  3.44it/s] 92%|█████████▏| 539/585 [03:44<00:13,  3.44it/s] 92%|█████████▏| 540/585 [03:44<00:13,  3.45it/s] 92%|█████████▏| 541/585 [03:45<00:12,  3.45it/s] 93%|█████████▎| 542/585 [03:45<00:12,  3.45it/s] 93%|█████████▎| 543/585 [03:45<00:12,  3.45it/s] 93%|█████████▎| 544/585 [03:46<00:11,  3.45it/s] 93%|█████████▎| 545/585 [03:46<00:11,  3.45it/s] 93%|█████████▎| 546/585 [03:46<00:11,  3.45it/s] 94%|█████████▎| 547/585 [03:46<00:10,  3.46it/s] 94%|█████████▎| 548/585 [03:47<00:10,  3.46it/s] 94%|█████████▍| 549/585 [03:47<00:10,  3.46it/s] 94%|█████████▍| 550/585 [03:47<00:10,  3.45it/s] 94%|█████████▍| 551/585 [03:48<00:09,  3.45it/s] 94%|█████████▍| 552/585 [03:48<00:09,  3.46it/s] 95%|█████████▍| 553/585 [03:48<00:09,  3.46it/s] 95%|█████████▍| 554/585 [03:48<00:09,  3.44it/s] 95%|█████████▍| 555/585 [03:49<00:08,  3.45it/s] 95%|█████████▌| 556/585 [03:49<00:08,  3.45it/s] 95%|█████████▌| 557/585 [03:49<00:08,  3.45it/s] 95%|█████████▌| 558/585 [03:50<00:07,  3.46it/s] 96%|█████████▌| 559/585 [03:50<00:07,  3.46it/s] 96%|█████████▌| 560/585 [03:50<00:07,  3.46it/s] 96%|█████████▌| 561/585 [03:50<00:06,  3.46it/s] 96%|█████████▌| 562/585 [03:51<00:06,  3.46it/s] 96%|█████████▌| 563/585 [03:51<00:06,  3.46it/s] 96%|█████████▋| 564/585 [03:51<00:06,  3.46it/s] 97%|█████████▋| 565/585 [03:52<00:05,  3.45it/s] 97%|█████████▋| 566/585 [03:52<00:05,  3.45it/s] 97%|█████████▋| 567/585 [03:52<00:05,  3.45it/s] 97%|█████████▋| 568/585 [03:52<00:04,  3.45it/s] 97%|█████████▋| 569/585 [03:53<00:04,  3.46it/s] 97%|█████████▋| 570/585 [03:53<00:04,  3.46it/s] 98%|█████████▊| 571/585 [03:53<00:04,  3.46it/s] 98%|█████████▊| 572/585 [03:54<00:03,  3.46it/s] 98%|█████████▊| 573/585 [03:54<00:03,  3.46it/s] 98%|█████████▊| 574/585 [03:54<00:03,  3.46it/s] 98%|█████████▊| 575/585 [03:54<00:02,  3.46it/s] 98%|█████████▊| 576/585 [03:55<00:02,  3.44it/s] 99%|█████████▊| 577/585 [03:55<00:02,  3.45it/s] 99%|█████████▉| 578/585 [03:55<00:02,  3.45it/s] 99%|█████████▉| 579/585 [03:56<00:01,  3.45it/s] 99%|█████████▉| 580/585 [03:56<00:01,  3.45it/s] 99%|█████████▉| 581/585 [03:56<00:01,  3.46it/s] 99%|█████████▉| 582/585 [03:57<00:00,  3.46it/s]100%|█████████▉| 583/585 [03:57<00:00,  3.46it/s]100%|█████████▉| 584/585 [03:57<00:00,  3.46it/s]100%|██████████| 585/585 [03:57<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 14:38:20,232 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:38:20,232 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:38:20,232 >>   Batch size = 8
{'eval_loss': 0.98735111951828, 'eval_runtime': 9.7756, 'eval_samples_per_second': 357.317, 'eval_steps_per_second': 44.703, 'epoch': 4.0}
{'loss': 0.8453, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.10it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.85it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.06it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.00it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.56it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.30it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.19it/s][A
 10%|▉         | 42/437 [00:00<00:08, 45.10it/s][A
 11%|█         | 47/437 [00:01<00:08, 45.12it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.02it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.16it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.91it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.84it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.64it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.68it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.79it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.94it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.89it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.93it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 45.03it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 45.07it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.85it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.80it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.73it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.92it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.88it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.88it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.83it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 45.01it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.99it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.80it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.76it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.81it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.89it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.77it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.81it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.93it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.99it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 45.05it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.91it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.90it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.92it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.96it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.84it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.93it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.91it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.90it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 45.01it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.85it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.86it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.95it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.90it/s][A
 61%|██████    | 267/437 [00:05<00:03, 45.02it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.98it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.99it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.88it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.96it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.86it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.87it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.87it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.90it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.95it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.92it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.83it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.81it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.82it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.87it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.81it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.83it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.90it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.86it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.80it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.73it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.92it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.85it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.91it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.81it/s][A
 90%|████████▉ | 392/437 [00:08<00:00, 45.00it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 45.01it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.89it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.85it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.82it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.85it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.92it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.89it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.86it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.92it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:07<00:00,  3.46it/s]
100%|██████████| 437/437 [00:09<00:00, 44.92it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:38:29,974 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 14:38:30,000 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:38:32,031 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:38:32,047 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:38:32,062 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:38:36,356 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:38:36,359 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-351 (score: 0.986352801322937).
                                                 100%|██████████| 585/585 [04:16<00:00,  3.46it/s]100%|██████████| 585/585 [04:16<00:00,  2.28it/s]
[INFO|trainer.py:1894] 2023-08-28 14:38:38,469 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 14:38:38,488 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:38:40,756 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:38:40,789 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:38:40,799 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:38:41,005 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:41,005 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:41,005 >>   train_loss               =     0.8392
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:41,005 >>   train_runtime            = 0:04:16.10
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:41,005 >>   train_samples            =       7504
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:41,005 >>   train_samples_per_second =      146.5
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:41,005 >>   train_steps_per_second   =      2.284
{'eval_loss': 0.9885415434837341, 'eval_runtime': 9.7293, 'eval_samples_per_second': 359.017, 'eval_steps_per_second': 44.916, 'epoch': 5.0}
{'train_runtime': 256.1085, 'train_samples_per_second': 146.5, 'train_steps_per_second': 2.284, 'train_loss': 0.8391800578842815, 'epoch': 5.0}
08/28/2023 14:38:41 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:38:41,042 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:38:41,042 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 14:38:41,042 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.74it/s]  3%|▎         | 12/437 [00:00<00:08, 49.05it/s]  4%|▍         | 17/437 [00:00<00:08, 47.49it/s]  5%|▌         | 22/437 [00:00<00:08, 46.78it/s]  6%|▌         | 27/437 [00:00<00:08, 46.32it/s]  7%|▋         | 32/437 [00:00<00:08, 45.89it/s]  8%|▊         | 37/437 [00:00<00:08, 45.79it/s] 10%|▉         | 42/437 [00:00<00:08, 45.46it/s] 11%|█         | 47/437 [00:01<00:08, 44.92it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.61it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.68it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.79it/s] 15%|█▌        | 67/437 [00:01<00:08, 45.01it/s] 16%|█▋        | 72/437 [00:01<00:08, 45.10it/s] 18%|█▊        | 77/437 [00:01<00:07, 45.25it/s] 19%|█▉        | 82/437 [00:01<00:07, 45.27it/s] 20%|█▉        | 87/437 [00:01<00:07, 45.04it/s] 21%|██        | 92/437 [00:02<00:07, 44.73it/s] 22%|██▏       | 97/437 [00:02<00:07, 44.54it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.63it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.67it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.79it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.92it/s] 28%|██▊       | 122/437 [00:02<00:06, 45.09it/s] 29%|██▉       | 127/437 [00:02<00:06, 45.16it/s] 30%|███       | 132/437 [00:02<00:06, 45.12it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.92it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.78it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.79it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.69it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.82it/s] 37%|███▋      | 162/437 [00:03<00:06, 45.03it/s] 38%|███▊      | 167/437 [00:03<00:05, 45.03it/s] 39%|███▉      | 172/437 [00:03<00:05, 45.11it/s] 41%|████      | 177/437 [00:03<00:05, 44.97it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.82it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.62it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.45it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.59it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.65it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.81it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.92it/s] 50%|████▉     | 217/437 [00:04<00:04, 45.00it/s] 51%|█████     | 222/437 [00:04<00:04, 44.88it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.79it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.74it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.73it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.82it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.90it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.94it/s] 59%|█████▉    | 257/437 [00:05<00:03, 45.05it/s] 60%|█████▉    | 262/437 [00:05<00:03, 45.12it/s] 61%|██████    | 267/437 [00:05<00:03, 45.09it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.69it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.75it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.70it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.73it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.71it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.66it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.75it/s] 70%|███████   | 307/437 [00:06<00:02, 44.82it/s] 71%|███████▏  | 312/437 [00:06<00:02, 44.80it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.66it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.62it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.64it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.69it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.85it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.71it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.64it/s] 81%|████████  | 352/437 [00:07<00:01, 44.87it/s] 82%|████████▏ | 357/437 [00:07<00:01, 44.81it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.68it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.68it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.77it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.87it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.83it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.88it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.77it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.92it/s] 92%|█████████▏| 402/437 [00:08<00:00, 44.25it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.36it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.68it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.76it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.77it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.74it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.78it/s]100%|██████████| 437/437 [00:09<00:00, 44.91it/s]100%|██████████| 437/437 [00:09<00:00, 44.93it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:38:50,785 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:50,785 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:50,785 >>   eval_loss               =     0.9864
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:50,785 >>   eval_runtime            = 0:00:09.74
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:50,785 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:50,785 >>   eval_samples_per_second =    358.526
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:50,785 >>   eval_steps_per_second   =     44.854
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:38:50,785 >>   perplexity              =     2.6814
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:38:57,302 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:38:57,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:38:57,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:38:57,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:38:57,325 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:38:58,034 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:38:58,035 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:38:58,605 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:38:59,653 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:38:59,653 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:02,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:02,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:02,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:02,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:02,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:39:03,283 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:39:03,284 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:39:03,863 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:39:04,022 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:39:04,022 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.40it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.55it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.60it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:10,  1.62it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.63it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:14,  1.64it/s]Extractor Predicting: 24it [00:15,  1.64it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:19,  1.66it/s]Extractor Predicting: 32it [00:20,  1.65it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:21,  1.57it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:25,  1.59it/s]Extractor Predicting: 41it [00:25,  1.65it/s]Extractor Predicting: 42it [00:26,  1.66it/s]Extractor Predicting: 43it [00:26,  1.62it/s]Extractor Predicting: 44it [00:27,  1.67it/s]Extractor Predicting: 45it [00:28,  1.63it/s]Extractor Predicting: 46it [00:28,  1.62it/s]Extractor Predicting: 47it [00:29,  1.61it/s]Extractor Predicting: 48it [00:29,  1.67it/s]Extractor Predicting: 49it [00:30,  1.69it/s]Extractor Predicting: 50it [00:31,  1.65it/s]Extractor Predicting: 51it [00:31,  1.63it/s]Extractor Predicting: 52it [00:32,  1.65it/s]Extractor Predicting: 53it [00:32,  1.68it/s]Extractor Predicting: 54it [00:33,  1.69it/s]Extractor Predicting: 55it [00:34,  1.68it/s]Extractor Predicting: 56it [00:34,  1.65it/s]Extractor Predicting: 57it [00:35,  1.66it/s]Extractor Predicting: 58it [00:36,  1.50it/s]Extractor Predicting: 59it [00:36,  1.53it/s]Extractor Predicting: 60it [00:37,  1.53it/s]Extractor Predicting: 61it [00:37,  1.57it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:41,  1.59it/s]Extractor Predicting: 67it [00:41,  1.57it/s]Extractor Predicting: 68it [00:42,  1.54it/s]Extractor Predicting: 69it [00:43,  1.56it/s]Extractor Predicting: 70it [00:43,  1.57it/s]Extractor Predicting: 71it [00:44,  1.59it/s]Extractor Predicting: 72it [00:44,  1.63it/s]Extractor Predicting: 73it [00:45,  1.61it/s]Extractor Predicting: 74it [00:46,  1.57it/s]Extractor Predicting: 75it [00:46,  1.60it/s]Extractor Predicting: 76it [00:47,  1.57it/s]Extractor Predicting: 77it [00:48,  1.56it/s]Extractor Predicting: 78it [00:48,  1.53it/s]Extractor Predicting: 79it [00:49,  1.54it/s]Extractor Predicting: 80it [00:50,  1.52it/s]Extractor Predicting: 81it [00:50,  1.52it/s]Extractor Predicting: 82it [00:51,  1.53it/s]Extractor Predicting: 83it [00:52,  1.57it/s]Extractor Predicting: 84it [00:52,  1.58it/s]Extractor Predicting: 85it [00:53,  1.56it/s]Extractor Predicting: 86it [00:54,  1.49it/s]Extractor Predicting: 87it [00:54,  1.54it/s]Extractor Predicting: 88it [00:55,  1.54it/s]Extractor Predicting: 89it [00:55,  1.54it/s]Extractor Predicting: 90it [00:56,  1.50it/s]Extractor Predicting: 91it [00:57,  1.50it/s]Extractor Predicting: 92it [00:57,  1.53it/s]Extractor Predicting: 93it [00:58,  1.53it/s]Extractor Predicting: 94it [00:59,  1.55it/s]Extractor Predicting: 95it [00:59,  1.58it/s]Extractor Predicting: 96it [01:00,  1.59it/s]Extractor Predicting: 97it [01:01,  1.57it/s]Extractor Predicting: 98it [01:01,  1.58it/s]Extractor Predicting: 99it [01:02,  1.57it/s]Extractor Predicting: 100it [01:02,  1.60it/s]Extractor Predicting: 101it [01:03,  1.60it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:04,  1.58it/s]Extractor Predicting: 104it [01:05,  1.58it/s]Extractor Predicting: 105it [01:06,  1.58it/s]Extractor Predicting: 106it [01:06,  1.56it/s]Extractor Predicting: 107it [01:07,  1.56it/s]Extractor Predicting: 108it [01:08,  1.55it/s]Extractor Predicting: 109it [01:08,  1.56it/s]Extractor Predicting: 110it [01:09,  1.55it/s]Extractor Predicting: 111it [01:10,  1.55it/s]Extractor Predicting: 112it [01:10,  1.55it/s]Extractor Predicting: 113it [01:11,  1.53it/s]Extractor Predicting: 114it [01:11,  1.52it/s]Extractor Predicting: 115it [01:12,  1.51it/s]Extractor Predicting: 116it [01:13,  1.52it/s]Extractor Predicting: 117it [01:13,  1.51it/s]Extractor Predicting: 118it [01:14,  1.51it/s]Extractor Predicting: 119it [01:15,  1.52it/s]Extractor Predicting: 120it [01:15,  1.52it/s]Extractor Predicting: 121it [01:16,  1.51it/s]Extractor Predicting: 122it [01:17,  1.51it/s]Extractor Predicting: 123it [01:17,  1.57it/s]Extractor Predicting: 124it [01:18,  1.56it/s]Extractor Predicting: 125it [01:19,  1.53it/s]Extractor Predicting: 126it [01:19,  1.54it/s]Extractor Predicting: 127it [01:20,  1.54it/s]Extractor Predicting: 128it [01:21,  1.54it/s]Extractor Predicting: 129it [01:21,  1.53it/s]Extractor Predicting: 130it [01:22,  1.53it/s]Extractor Predicting: 131it [01:23,  1.39it/s]Extractor Predicting: 132it [01:24,  1.41it/s]Extractor Predicting: 133it [01:24,  1.43it/s]Extractor Predicting: 134it [01:25,  1.43it/s]Extractor Predicting: 135it [01:26,  1.46it/s]Extractor Predicting: 136it [01:26,  1.79it/s]Extractor Predicting: 136it [01:26,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:38,490 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:38,495 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:38,495 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:38,495 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:38,495 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:40:39,088 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:40:39,089 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:40:39,654 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:40:40,692 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:40:40,693 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:43,564 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:43,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:43,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:43,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:40:43,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:40:44,182 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:40:44,183 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:40:44,752 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:40:44,920 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:40:44,920 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5596868884540117,
  "recall": 0.08187804179788148,
  "score": 0.14285714285714285,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.81it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.74it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:05,  1.61it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:08,  1.66it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:11,  1.64it/s]Extractor Predicting: 21it [00:12,  1.61it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.67it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:15,  1.73it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.72it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:17,  1.65it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.72it/s]Extractor Predicting: 33it [00:19,  1.75it/s]Extractor Predicting: 34it [00:20,  1.73it/s]Extractor Predicting: 35it [00:20,  1.69it/s]Extractor Predicting: 36it [00:21,  1.65it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:22,  1.61it/s]Extractor Predicting: 39it [00:23,  1.65it/s]Extractor Predicting: 40it [00:23,  1.72it/s]Extractor Predicting: 41it [00:24,  1.71it/s]Extractor Predicting: 42it [00:25,  1.71it/s]Extractor Predicting: 43it [00:25,  1.74it/s]Extractor Predicting: 44it [00:26,  1.72it/s]Extractor Predicting: 45it [00:26,  1.69it/s]Extractor Predicting: 46it [00:27,  1.71it/s]Extractor Predicting: 47it [00:27,  1.72it/s]Extractor Predicting: 48it [00:28,  1.72it/s]Extractor Predicting: 49it [00:29,  1.73it/s]Extractor Predicting: 50it [00:29,  1.73it/s]Extractor Predicting: 51it [00:30,  1.78it/s]Extractor Predicting: 52it [00:30,  1.62it/s]Extractor Predicting: 53it [00:31,  1.63it/s]Extractor Predicting: 54it [00:32,  1.63it/s]Extractor Predicting: 55it [00:32,  1.68it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:33,  1.65it/s]Extractor Predicting: 58it [00:34,  1.68it/s]Extractor Predicting: 59it [00:35,  1.66it/s]Extractor Predicting: 60it [00:35,  1.66it/s]Extractor Predicting: 61it [00:36,  1.67it/s]Extractor Predicting: 62it [00:36,  1.67it/s]Extractor Predicting: 63it [00:37,  1.64it/s]Extractor Predicting: 64it [00:38,  1.70it/s]Extractor Predicting: 65it [00:38,  1.69it/s]Extractor Predicting: 66it [00:39,  1.67it/s]Extractor Predicting: 67it [00:39,  1.68it/s]Extractor Predicting: 68it [00:40,  1.73it/s]Extractor Predicting: 69it [00:41,  1.75it/s]Extractor Predicting: 70it [00:41,  1.73it/s]Extractor Predicting: 71it [00:42,  1.73it/s]Extractor Predicting: 72it [00:42,  1.69it/s]Extractor Predicting: 73it [00:43,  1.72it/s]Extractor Predicting: 74it [00:44,  1.68it/s]Extractor Predicting: 75it [00:44,  1.67it/s]Extractor Predicting: 76it [00:45,  1.68it/s]Extractor Predicting: 77it [00:45,  1.67it/s]Extractor Predicting: 78it [00:46,  1.67it/s]Extractor Predicting: 79it [00:46,  1.68it/s]Extractor Predicting: 80it [00:47,  1.70it/s]Extractor Predicting: 81it [00:48,  1.68it/s]Extractor Predicting: 82it [00:48,  1.70it/s]Extractor Predicting: 83it [00:49,  1.65it/s]Extractor Predicting: 84it [00:49,  1.66it/s]Extractor Predicting: 85it [00:50,  1.62it/s]Extractor Predicting: 86it [00:51,  1.59it/s]Extractor Predicting: 87it [00:51,  1.65it/s]Extractor Predicting: 88it [00:52,  1.61it/s]Extractor Predicting: 89it [00:53,  1.60it/s]Extractor Predicting: 90it [00:53,  1.58it/s]Extractor Predicting: 91it [00:54,  1.58it/s]Extractor Predicting: 92it [00:55,  1.57it/s]Extractor Predicting: 93it [00:55,  1.55it/s]Extractor Predicting: 94it [00:56,  1.56it/s]Extractor Predicting: 95it [00:56,  1.59it/s]Extractor Predicting: 96it [00:57,  1.58it/s]Extractor Predicting: 97it [00:58,  1.56it/s]Extractor Predicting: 98it [00:58,  1.56it/s]Extractor Predicting: 99it [00:59,  1.53it/s]Extractor Predicting: 100it [01:00,  1.53it/s]Extractor Predicting: 101it [01:00,  1.53it/s]Extractor Predicting: 102it [01:01,  1.52it/s]Extractor Predicting: 103it [01:02,  1.56it/s]Extractor Predicting: 104it [01:02,  1.56it/s]Extractor Predicting: 105it [01:03,  1.57it/s]Extractor Predicting: 106it [01:04,  1.53it/s]Extractor Predicting: 107it [01:04,  1.54it/s]Extractor Predicting: 108it [01:05,  1.51it/s]Extractor Predicting: 109it [01:06,  1.51it/s]Extractor Predicting: 110it [01:06,  1.51it/s]Extractor Predicting: 111it [01:07,  1.52it/s]Extractor Predicting: 112it [01:08,  1.51it/s]Extractor Predicting: 113it [01:08,  1.52it/s]Extractor Predicting: 114it [01:09,  1.52it/s]Extractor Predicting: 115it [01:10,  1.51it/s]Extractor Predicting: 116it [01:10,  1.52it/s]Extractor Predicting: 117it [01:11,  1.53it/s]Extractor Predicting: 118it [01:12,  1.55it/s]Extractor Predicting: 119it [01:12,  1.56it/s]Extractor Predicting: 120it [01:13,  1.61it/s]Extractor Predicting: 121it [01:13,  1.60it/s]Extractor Predicting: 122it [01:14,  1.58it/s]Extractor Predicting: 123it [01:15,  1.58it/s]Extractor Predicting: 124it [01:15,  1.57it/s]Extractor Predicting: 125it [01:16,  1.60it/s]Extractor Predicting: 126it [01:16,  1.64it/s]Extractor Predicting: 127it [01:17,  1.64it/s]Extractor Predicting: 128it [01:18,  1.67it/s]Extractor Predicting: 129it [01:18,  1.64it/s]Extractor Predicting: 130it [01:19,  1.59it/s]Extractor Predicting: 131it [01:20,  1.61it/s]Extractor Predicting: 132it [01:20,  1.61it/s]Extractor Predicting: 133it [01:21,  1.64it/s]Extractor Predicting: 134it [01:21,  1.66it/s]Extractor Predicting: 135it [01:22,  1.64it/s]Extractor Predicting: 136it [01:23,  1.66it/s]Extractor Predicting: 137it [01:23,  1.68it/s]Extractor Predicting: 138it [01:24,  1.65it/s]Extractor Predicting: 139it [01:24,  1.65it/s]Extractor Predicting: 140it [01:25,  1.64it/s]Extractor Predicting: 141it [01:26,  1.59it/s]Extractor Predicting: 142it [01:26,  1.57it/s]Extractor Predicting: 143it [01:27,  1.55it/s]Extractor Predicting: 144it [01:28,  1.54it/s]Extractor Predicting: 145it [01:28,  1.54it/s]Extractor Predicting: 146it [01:29,  1.58it/s]Extractor Predicting: 147it [01:29,  1.59it/s]Extractor Predicting: 148it [01:30,  1.58it/s]Extractor Predicting: 149it [01:31,  1.57it/s]Extractor Predicting: 150it [01:31,  1.57it/s]Extractor Predicting: 151it [01:32,  1.52it/s]Extractor Predicting: 152it [01:33,  1.54it/s]Extractor Predicting: 153it [01:34,  1.40it/s]Extractor Predicting: 154it [01:34,  1.47it/s]Extractor Predicting: 155it [01:35,  1.47it/s]Extractor Predicting: 156it [01:36,  1.50it/s]Extractor Predicting: 157it [01:36,  1.50it/s]Extractor Predicting: 158it [01:37,  1.54it/s]Extractor Predicting: 159it [01:37,  1.53it/s]Extractor Predicting: 160it [01:38,  1.52it/s]Extractor Predicting: 161it [01:39,  1.54it/s]Extractor Predicting: 162it [01:39,  1.54it/s]Extractor Predicting: 163it [01:40,  1.54it/s]Extractor Predicting: 164it [01:41,  1.56it/s]Extractor Predicting: 165it [01:41,  1.55it/s]Extractor Predicting: 166it [01:42,  1.56it/s]Extractor Predicting: 167it [01:43,  1.54it/s]Extractor Predicting: 168it [01:43,  1.54it/s]Extractor Predicting: 169it [01:44,  1.53it/s]Extractor Predicting: 170it [01:45,  1.53it/s]Extractor Predicting: 171it [01:45,  1.54it/s]Extractor Predicting: 172it [01:46,  1.53it/s]Extractor Predicting: 173it [01:47,  1.53it/s]Extractor Predicting: 174it [01:47,  1.54it/s]Extractor Predicting: 175it [01:48,  1.59it/s]Extractor Predicting: 176it [01:48,  1.58it/s]Extractor Predicting: 177it [01:49,  1.59it/s]Extractor Predicting: 178it [01:50,  1.60it/s]Extractor Predicting: 179it [01:50,  1.62it/s]Extractor Predicting: 180it [01:51,  1.61it/s]Extractor Predicting: 181it [01:52,  1.62it/s]Extractor Predicting: 182it [01:52,  1.64it/s]Extractor Predicting: 183it [01:53,  1.58it/s]Extractor Predicting: 184it [01:53,  1.65it/s]Extractor Predicting: 185it [01:54,  1.62it/s]Extractor Predicting: 186it [01:55,  1.63it/s]Extractor Predicting: 187it [01:55,  1.62it/s]Extractor Predicting: 188it [01:56,  1.58it/s]Extractor Predicting: 189it [01:57,  1.58it/s]Extractor Predicting: 190it [01:57,  1.58it/s]Extractor Predicting: 191it [01:58,  1.56it/s]Extractor Predicting: 192it [01:58,  1.57it/s]Extractor Predicting: 193it [01:59,  1.61it/s]Extractor Predicting: 194it [02:00,  1.60it/s]Extractor Predicting: 195it [02:00,  1.57it/s]Extractor Predicting: 196it [02:01,  1.54it/s]Extractor Predicting: 197it [02:02,  1.57it/s]Extractor Predicting: 198it [02:02,  1.56it/s]Extractor Predicting: 199it [02:03,  1.58it/s]Extractor Predicting: 200it [02:03,  1.58it/s]Extractor Predicting: 201it [02:04,  1.59it/s]Extractor Predicting: 202it [02:05,  1.60it/s]Extractor Predicting: 203it [02:05,  1.60it/s]Extractor Predicting: 204it [02:06,  1.64it/s]Extractor Predicting: 205it [02:07,  1.62it/s]Extractor Predicting: 206it [02:07,  1.65it/s]Extractor Predicting: 207it [02:08,  1.62it/s]Extractor Predicting: 208it [02:08,  1.61it/s]Extractor Predicting: 209it [02:09,  1.62it/s]Extractor Predicting: 210it [02:10,  1.62it/s]Extractor Predicting: 211it [02:10,  1.60it/s]Extractor Predicting: 212it [02:11,  1.61it/s]Extractor Predicting: 213it [02:11,  1.63it/s]Extractor Predicting: 214it [02:12,  1.62it/s]Extractor Predicting: 215it [02:13,  1.62it/s]Extractor Predicting: 216it [02:13,  1.62it/s]Extractor Predicting: 217it [02:14,  1.60it/s]Extractor Predicting: 218it [02:15,  1.62it/s]Extractor Predicting: 219it [02:15,  1.58it/s]Extractor Predicting: 220it [02:16,  1.61it/s]Extractor Predicting: 221it [02:16,  1.63it/s]Extractor Predicting: 222it [02:17,  1.63it/s]Extractor Predicting: 223it [02:18,  1.59it/s]Extractor Predicting: 224it [02:18,  1.64it/s]Extractor Predicting: 225it [02:19,  1.61it/s]Extractor Predicting: 226it [02:20,  1.61it/s]Extractor Predicting: 227it [02:20,  1.61it/s]Extractor Predicting: 228it [02:21,  1.63it/s]Extractor Predicting: 229it [02:21,  1.62it/s]Extractor Predicting: 230it [02:22,  1.64it/s]Extractor Predicting: 231it [02:23,  1.65it/s]Extractor Predicting: 232it [02:23,  1.63it/s]Extractor Predicting: 233it [02:24,  1.64it/s]Extractor Predicting: 234it [02:24,  1.62it/s]Extractor Predicting: 235it [02:25,  1.59it/s]Extractor Predicting: 236it [02:26,  1.59it/s]Extractor Predicting: 237it [02:27,  1.44it/s]Extractor Predicting: 238it [02:27,  1.48it/s]Extractor Predicting: 239it [02:28,  1.49it/s]Extractor Predicting: 240it [02:29,  1.52it/s]Extractor Predicting: 241it [02:29,  1.54it/s]Extractor Predicting: 242it [02:30,  1.55it/s]Extractor Predicting: 243it [02:30,  1.56it/s]Extractor Predicting: 244it [02:31,  1.55it/s]Extractor Predicting: 245it [02:32,  1.57it/s]Extractor Predicting: 246it [02:32,  1.56it/s]Extractor Predicting: 247it [02:33,  1.58it/s]Extractor Predicting: 248it [02:34,  1.58it/s]Extractor Predicting: 249it [02:34,  1.61it/s]Extractor Predicting: 250it [02:35,  1.60it/s]Extractor Predicting: 251it [02:35,  1.64it/s]Extractor Predicting: 252it [02:36,  1.61it/s]Extractor Predicting: 253it [02:37,  1.61it/s]Extractor Predicting: 254it [02:37,  1.61it/s]Extractor Predicting: 255it [02:38,  1.60it/s]Extractor Predicting: 256it [02:39,  1.56it/s]Extractor Predicting: 257it [02:39,  1.58it/s]Extractor Predicting: 258it [02:40,  1.58it/s]Extractor Predicting: 259it [02:40,  1.63it/s]Extractor Predicting: 260it [02:41,  1.61it/s]Extractor Predicting: 261it [02:42,  1.63it/s]Extractor Predicting: 262it [02:42,  1.62it/s]Extractor Predicting: 263it [02:43,  1.61it/s]Extractor Predicting: 264it [02:43,  1.66it/s]Extractor Predicting: 265it [02:44,  1.65it/s]Extractor Predicting: 266it [02:45,  1.67it/s]Extractor Predicting: 267it [02:45,  1.65it/s]Extractor Predicting: 268it [02:46,  1.64it/s]Extractor Predicting: 269it [02:47,  1.63it/s]Extractor Predicting: 270it [02:47,  1.59it/s]Extractor Predicting: 271it [02:48,  1.57it/s]Extractor Predicting: 272it [02:48,  1.59it/s]Extractor Predicting: 273it [02:49,  1.58it/s]Extractor Predicting: 274it [02:50,  1.56it/s]Extractor Predicting: 275it [02:50,  1.53it/s]Extractor Predicting: 276it [02:51,  1.55it/s]Extractor Predicting: 277it [02:52,  1.55it/s]Extractor Predicting: 278it [02:52,  1.60it/s]Extractor Predicting: 279it [02:53,  1.59it/s]Extractor Predicting: 280it [02:54,  1.58it/s]Extractor Predicting: 281it [02:54,  1.60it/s]Extractor Predicting: 282it [02:55,  1.59it/s]Extractor Predicting: 283it [02:55,  1.58it/s]Extractor Predicting: 284it [02:56,  1.59it/s]Extractor Predicting: 285it [02:57,  1.58it/s]Extractor Predicting: 286it [02:57,  1.62it/s]Extractor Predicting: 287it [02:58,  1.58it/s]Extractor Predicting: 288it [02:58,  1.80it/s]Extractor Predicting: 288it [02:58,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:51,524 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:51,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:51,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:51,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:51,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:43:52,130 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:43:52,131 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:43:52,681 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:43:53,749 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:43:53,749 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:56,645 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:56,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:56,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:56,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:43:56,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:43:57,285 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:43:57,286 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:43:57,858 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:43:58,027 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:43:58,028 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5540184453227931,
  "recall": 0.1218840579710145,
  "score": 0.19980993110002374,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 3it [00:02,  1.38it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:44:00,604 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:44:00,605 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:44:00,634 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:44:00,635 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:44:00,637 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:44:04,103 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:44:04,105 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:44:04,120 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:44:04,120 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:44:04,136 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:44:04,148 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:44:04,148 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:44:04,148 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:44:04,148 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:44:04,148 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:44:04,148 >> loading file outputs/wrapper/fewrel/unseen_10_seed_2/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4,
  "recall": 0.02,
  "score": 0.03809523809523809,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:44:04,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:04,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:05,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:06,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:06,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:07,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:08,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:08,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:09,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:09,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:10,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:11,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:11,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:12,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:13,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:13,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:14,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:15,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:15,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:16,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:17,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:10, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-28 14:44:17,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:18,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:19,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:19,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:20,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:21,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:21,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:22,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:22,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:23,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:24,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:24,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:25,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:26,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:26,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:27,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:27,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:28,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:29,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:29,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:30,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:55, 13.51s/it][WARNING|generation_utils.py:914] 2023-08-28 14:44:31,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:32,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:32,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:33,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:34,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:34,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:35,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:35,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:36,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:37,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:37,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:38,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:38,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:39,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:40,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:40,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:41,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:41,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:42,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:43,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:43,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:44,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:44,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:45,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:41<02:48, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 14:44:46,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:46,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:47,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:48,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:48,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:49,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:50,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:50,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:51,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:51,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:52,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:53,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:53,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:54,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:55,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:55,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:56,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:57,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:57,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:58,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:58,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:44:59,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:00,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:56<02:37, 14.35s/it][WARNING|generation_utils.py:914] 2023-08-28 14:45:00,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:01,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:02,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:02,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:03,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:04,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:05,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:05,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:06,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:06,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:07,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:08,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:08,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:09,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:10,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:11,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:11,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:12,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:13,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:14,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:15,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:15,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:11<02:27, 14.70s/it][WARNING|generation_utils.py:914] 2023-08-28 14:45:16,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:16,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:17,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:18,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:18,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:19,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:19,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:20,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:21,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:21,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:22,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:23,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:23,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:24,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:25,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:25,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:26,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:26,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:27,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:28,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:28,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:25<02:07, 14.20s/it][WARNING|generation_utils.py:914] 2023-08-28 14:45:29,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:30,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:30,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:31,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:32,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:32,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:33,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:33,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:34,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:35,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:35,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:36,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:37,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:37,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:38,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:39,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:39,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:40,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:41,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:41,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:42,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:43,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:43,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:44,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:40<01:57, 14.67s/it][WARNING|generation_utils.py:914] 2023-08-28 14:45:45,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:45,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:46,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:47,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:47,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:48,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:48,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:49,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:50,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:50,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:51,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:51,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:52,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:53,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:53,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:54,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:55,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:55,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:56,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:57,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:57,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:58,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:45:58,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:55<01:42, 14.60s/it][WARNING|generation_utils.py:914] 2023-08-28 14:45:59,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:00,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:01,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:01,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:02,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:03,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:04,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:04,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:05,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:06,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:07,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:07,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:08,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:09,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:09,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:10,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:11,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:11,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:12,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:13,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:13,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:14,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:10<01:29, 14.85s/it][WARNING|generation_utils.py:914] 2023-08-28 14:46:14,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:15,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:16,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:16,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:17,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:18,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:18,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:19,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:20,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:21,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:21,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:22,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:22,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:23,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:24,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:24,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:25,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:26,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:26,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:27,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:28,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:28,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:25<01:13, 14.79s/it][WARNING|generation_utils.py:914] 2023-08-28 14:46:29,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:30,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:31,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:31,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:32,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:33,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:33,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:34,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:34,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:35,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:36,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:36,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:37,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:38,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:38,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:39,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:39,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:40,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:41,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:41,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:42,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:43,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:43,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:40<00:59, 14.80s/it][WARNING|generation_utils.py:914] 2023-08-28 14:46:44,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:45,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:45,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:46,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:46,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:47,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:48,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:48,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:49,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:49,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:50,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:51,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:51,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:52,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:52,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:53,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:54,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:54,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:55,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:56,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:56,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:57,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:53<00:43, 14.41s/it][WARNING|generation_utils.py:914] 2023-08-28 14:46:57,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:58,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:59,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:46:59,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:00,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:00,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:01,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:02,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:02,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:03,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:04,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:04,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:05,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:05,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:06,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:07,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:07,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:08,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:09,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:09,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:10,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:06<00:27, 13.92s/it][WARNING|generation_utils.py:914] 2023-08-28 14:47:10,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:11,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:11,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:12,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:13,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:13,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:14,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:15,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:15,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:16,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:17,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:17,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:18,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:18,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:19,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:19,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:20,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:21,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:21,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:22,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:23,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:23,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:24,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:25,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:21<00:14, 14.25s/it][WARNING|generation_utils.py:914] 2023-08-28 14:47:25,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:26,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:27,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:27,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:28,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:29,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:29,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:30,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:31,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:31,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:32,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:33,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:33,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:34,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:35,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:35,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:36,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:37,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:37,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:38,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:47:39,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:35<00:00, 14.26s/it]Generating: 100%|██████████| 15/15 [03:35<00:00, 14.38s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:45,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:45,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:45,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:45,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:45,566 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:47:45,855 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:47:45,856 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:47:46,109 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:47:47,205 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:47:47,205 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:48,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:48,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:48,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:48,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:47:48,494 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:47:48,801 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:47:48,805 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:47:49,050 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:47:49,197 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:47:49,197 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8519021739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : said to be the same as . Context : Later in the year ( October 1887 ) , a young French cardinal , Louis Bové , had been invited by King Louis VI to examine the connection between religion and natural science . Head Entity : Louis Bové , Tail Entity : Jean .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'most commonly spoken name\', \'said to be the same as\', \'\', \'For the purposes of the classification , the " name " is not the same as the most commonly spoken name , " .\')', 'too many values to unpack (expected 2)'}}
['Relation : after a work by . Context : Later in the year ( 1141–1230 ) he married Brigadier John Baskerville , daughter of the Marquis of Warwick , the King of Cornwall . Head Entity : Robert Baskerville , Tail Entity : John Baskerville .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 609, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.79296875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : headquarters location . Context : Later in 2003 , the U.S. Department of Justice and federal prosecutors investigated a 2011 criminal investigation into whether the Church of Scientology had been negligent in publishing information about its church , including documents critical of President Church founder and current President Jimmy Carter . Head Entity : Jimmy Carter , Tail Entity : FBI .\n']
['Relation : headquarters location . Context : Later in 2003 , the U.S. Department of Justice and federal prosecutors investigated a 2011 criminal investigation into whether the Church of Scientology had been negligent in publishing information about its church , including documents critical of President Church founder and current President Jimmy Carter . Head Entity : Jimmy Carter , Tail Entity : FBI .\n', 'Relation : headquarters location . Context : The Federal Communications Commission ( FCC ) is part of the Federal Government . Head Entity : Federal FCC , Tail Entity : Federal Government .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8934659090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : place served by transport hub . Context : Later in the year , the railway built the station " Alpace " located at the end of the town . Head Entity : Alpace , Tail Entity : Town .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : record label . Context : Later in 2008 , the band became a major player on their debut solo album entitled " The Way I Am " . Head Entity : The Way I Am , Tail Entity : The Bizarre .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9092261904761905, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 609, 'raw': 768}
{'prompt': 'Relation : winner .', 'success_rate': 0.79296875, 'errors': {''}}
['Relation : work location . Context : Later in the year , the band formed New River Band , which recorded " Little Bodies " on their own studio recording label . Head Entity : Little Bodies , Tail Entity : Record label .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : work location .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 12882
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12982, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.37it/s]Extractor Estimating: 2it [00:01,  1.39it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.46it/s]Extractor Estimating: 5it [00:03,  1.48it/s]Extractor Estimating: 6it [00:04,  1.43it/s]Extractor Estimating: 7it [00:04,  1.46it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:06,  1.52it/s]Extractor Estimating: 10it [00:06,  1.52it/s]Extractor Estimating: 11it [00:07,  1.58it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.51it/s]Extractor Estimating: 14it [00:09,  1.51it/s]Extractor Estimating: 15it [00:09,  1.53it/s]Extractor Estimating: 16it [00:10,  1.53it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:11,  1.52it/s]Extractor Estimating: 19it [00:12,  1.41it/s]Extractor Estimating: 20it [00:13,  1.45it/s]Extractor Estimating: 21it [00:14,  1.46it/s]Extractor Estimating: 22it [00:14,  1.51it/s]Extractor Estimating: 23it [00:15,  1.47it/s]Extractor Estimating: 24it [00:16,  1.46it/s]Extractor Estimating: 25it [00:16,  1.44it/s]Extractor Estimating: 26it [00:17,  1.47it/s]Extractor Estimating: 27it [00:18,  1.49it/s]Extractor Estimating: 28it [00:18,  1.50it/s]Extractor Estimating: 29it [00:19,  1.44it/s]Extractor Estimating: 30it [00:20,  1.49it/s]Extractor Estimating: 31it [00:20,  1.55it/s]Extractor Estimating: 32it [00:21,  1.55it/s]Extractor Estimating: 33it [00:22,  1.53it/s]Extractor Estimating: 34it [00:22,  1.56it/s]Extractor Estimating: 35it [00:23,  1.59it/s]Extractor Estimating: 36it [00:23,  1.58it/s]Extractor Estimating: 37it [00:24,  1.55it/s]Extractor Estimating: 38it [00:25,  1.58it/s]Extractor Estimating: 39it [00:25,  1.53it/s]Extractor Estimating: 40it [00:26,  1.52it/s]Extractor Estimating: 41it [00:27,  1.50it/s]Extractor Estimating: 42it [00:27,  1.51it/s]Extractor Estimating: 43it [00:28,  1.53it/s]Extractor Estimating: 44it [00:29,  1.55it/s]Extractor Estimating: 45it [00:29,  1.52it/s]Extractor Estimating: 46it [00:30,  1.51it/s]Extractor Estimating: 47it [00:31,  1.49it/s]Extractor Estimating: 48it [00:31,  1.47it/s]Extractor Estimating: 49it [00:32,  1.44it/s]Extractor Estimating: 50it [00:33,  1.46it/s]Extractor Estimating: 51it [00:33,  1.50it/s]Extractor Estimating: 52it [00:34,  1.54it/s]Extractor Estimating: 53it [00:35,  1.52it/s]Extractor Estimating: 54it [00:35,  1.54it/s]Extractor Estimating: 55it [00:36,  1.61it/s]Extractor Estimating: 56it [00:37,  1.64it/s]Extractor Estimating: 57it [00:37,  1.64it/s]Extractor Estimating: 58it [00:38,  1.59it/s]Extractor Estimating: 59it [00:39,  1.48it/s]Extractor Estimating: 60it [00:39,  1.52it/s]Extractor Estimating: 61it [00:40,  1.57it/s]Extractor Estimating: 62it [00:40,  1.61it/s]Extractor Estimating: 63it [00:41,  1.63it/s]Extractor Estimating: 64it [00:42,  1.60it/s]Extractor Estimating: 65it [00:42,  1.59it/s]Extractor Estimating: 66it [00:43,  1.62it/s]Extractor Estimating: 67it [00:43,  1.63it/s]Extractor Estimating: 68it [00:44,  1.65it/s]Extractor Estimating: 69it [00:45,  1.63it/s]Extractor Estimating: 70it [00:45,  1.60it/s]Extractor Estimating: 71it [00:46,  1.58it/s]Extractor Estimating: 72it [00:47,  1.55it/s]Extractor Estimating: 73it [00:47,  1.57it/s]Extractor Estimating: 74it [00:48,  1.61it/s]Extractor Estimating: 75it [00:48,  1.65it/s]Extractor Estimating: 76it [00:49,  1.61it/s]Extractor Estimating: 77it [00:50,  1.56it/s]Extractor Estimating: 78it [00:50,  1.63it/s]Extractor Estimating: 79it [00:51,  1.66it/s]Extractor Estimating: 80it [00:51,  1.67it/s]Extractor Estimating: 81it [00:52,  1.67it/s]Extractor Estimating: 82it [00:53,  1.69it/s]Extractor Estimating: 83it [00:53,  1.69it/s]Extractor Estimating: 84it [00:54,  1.66it/s]Extractor Estimating: 85it [00:54,  1.69it/s]Extractor Estimating: 86it [00:55,  1.68it/s]Extractor Estimating: 87it [00:56,  1.73it/s]Extractor Estimating: 88it [00:56,  1.69it/s]Extractor Estimating: 89it [00:57,  1.66it/s]Extractor Estimating: 90it [00:57,  1.64it/s]Extractor Estimating: 91it [00:58,  1.69it/s]Extractor Estimating: 92it [00:59,  1.74it/s]Extractor Estimating: 93it [00:59,  1.70it/s]Extractor Estimating: 94it [01:00,  1.67it/s]Extractor Estimating: 95it [01:00,  1.66it/s]Extractor Estimating: 96it [01:01,  1.65it/s]Extractor Estimating: 97it [01:02,  1.65it/s]Extractor Estimating: 98it [01:02,  1.61it/s]Extractor Estimating: 99it [01:03,  1.64it/s]Extractor Estimating: 100it [01:03,  1.65it/s]Extractor Estimating: 101it [01:04,  1.63it/s]Extractor Estimating: 102it [01:05,  1.68it/s]Extractor Estimating: 103it [01:05,  1.64it/s]Extractor Estimating: 104it [01:06,  1.57it/s]Extractor Estimating: 105it [01:07,  1.63it/s]Extractor Estimating: 106it [01:07,  1.63it/s]Extractor Estimating: 107it [01:08,  1.58it/s]Extractor Estimating: 108it [01:08,  1.59it/s]Extractor Estimating: 109it [01:09,  1.52it/s]Extractor Estimating: 110it [01:10,  1.54it/s]Extractor Estimating: 111it [01:10,  1.52it/s]Extractor Estimating: 112it [01:11,  1.50it/s]Extractor Estimating: 113it [01:12,  1.51it/s]Extractor Estimating: 114it [01:12,  1.54it/s]Extractor Estimating: 115it [01:13,  1.53it/s]Extractor Estimating: 116it [01:14,  1.56it/s]Extractor Estimating: 117it [01:14,  1.48it/s]Extractor Estimating: 118it [01:15,  1.52it/s]Extractor Estimating: 119it [01:16,  1.59it/s]Extractor Estimating: 120it [01:16,  1.56it/s]Extractor Estimating: 121it [01:17,  1.59it/s]Extractor Estimating: 122it [01:18,  1.51it/s]Extractor Estimating: 123it [01:18,  1.55it/s]Extractor Estimating: 124it [01:19,  1.61it/s]Extractor Estimating: 125it [01:19,  1.64it/s]Extractor Estimating: 126it [01:20,  1.64it/s]Extractor Estimating: 127it [01:21,  1.58it/s]Extractor Estimating: 128it [01:21,  1.57it/s]Extractor Estimating: 129it [01:22,  1.59it/s]Extractor Estimating: 130it [01:23,  1.59it/s]Extractor Estimating: 131it [01:23,  1.62it/s]Extractor Estimating: 132it [01:24,  1.59it/s]Extractor Estimating: 133it [01:24,  1.59it/s]Extractor Estimating: 134it [01:25,  1.62it/s]Extractor Estimating: 135it [01:26,  1.64it/s]Extractor Estimating: 136it [01:26,  1.66it/s]Extractor Estimating: 137it [01:27,  1.62it/s]Extractor Estimating: 138it [01:28,  1.62it/s]Extractor Estimating: 139it [01:28,  1.53it/s]Extractor Estimating: 140it [01:29,  1.55it/s]Extractor Estimating: 141it [01:30,  1.55it/s]Extractor Estimating: 142it [01:30,  1.57it/s]Extractor Estimating: 143it [01:31,  1.44it/s]Extractor Estimating: 144it [01:32,  1.43it/s]Extractor Estimating: 145it [01:32,  1.45it/s]Extractor Estimating: 146it [01:33,  1.50it/s]Extractor Estimating: 147it [01:34,  1.49it/s]Extractor Estimating: 148it [01:34,  1.53it/s]Extractor Estimating: 149it [01:35,  1.56it/s]Extractor Estimating: 150it [01:36,  1.55it/s]Extractor Estimating: 151it [01:36,  1.53it/s]Extractor Estimating: 152it [01:37,  1.51it/s]Extractor Estimating: 153it [01:38,  1.54it/s]Extractor Estimating: 154it [01:38,  1.62it/s]Extractor Estimating: 155it [01:39,  1.59it/s]Extractor Estimating: 156it [01:39,  1.60it/s]Extractor Estimating: 157it [01:40,  1.56it/s]Extractor Estimating: 158it [01:41,  1.57it/s]Extractor Estimating: 159it [01:41,  1.57it/s]Extractor Estimating: 160it [01:42,  1.53it/s]Extractor Estimating: 161it [01:43,  1.53it/s]Extractor Estimating: 162it [01:43,  1.50it/s]Extractor Estimating: 163it [01:44,  1.57it/s]Extractor Estimating: 164it [01:45,  1.52it/s]Extractor Estimating: 165it [01:45,  1.47it/s]Extractor Estimating: 166it [01:46,  1.51it/s]Extractor Estimating: 167it [01:47,  1.54it/s]Extractor Estimating: 168it [01:47,  1.48it/s]Extractor Estimating: 169it [01:48,  1.48it/s]Extractor Estimating: 170it [01:49,  1.49it/s]Extractor Estimating: 171it [01:49,  1.55it/s]Extractor Estimating: 172it [01:50,  1.51it/s]Extractor Estimating: 173it [01:51,  1.50it/s]Extractor Estimating: 174it [01:51,  1.44it/s]Extractor Estimating: 175it [01:52,  1.39it/s]Extractor Estimating: 176it [01:53,  1.40it/s]Extractor Estimating: 177it [01:53,  1.46it/s]Extractor Estimating: 178it [01:54,  1.50it/s]Extractor Estimating: 179it [01:55,  1.54it/s]Extractor Estimating: 180it [01:55,  1.61it/s]Extractor Estimating: 181it [01:56,  1.64it/s]Extractor Estimating: 182it [01:56,  1.69it/s]Extractor Estimating: 183it [01:57,  1.70it/s]Extractor Estimating: 184it [01:58,  1.64it/s]Extractor Estimating: 185it [01:58,  1.61it/s]Extractor Estimating: 186it [01:59,  1.63it/s]Extractor Estimating: 187it [01:59,  1.62it/s]Extractor Estimating: 188it [02:00,  1.62it/s]Extractor Estimating: 189it [02:01,  1.62it/s]Extractor Estimating: 190it [02:01,  1.65it/s]Extractor Estimating: 191it [02:02,  1.67it/s]Extractor Estimating: 192it [02:03,  1.65it/s]Extractor Estimating: 193it [02:03,  1.72it/s]Extractor Estimating: 194it [02:04,  1.66it/s]Extractor Estimating: 195it [02:04,  1.65it/s]Extractor Estimating: 196it [02:05,  1.65it/s]Extractor Estimating: 197it [02:06,  1.58it/s]Extractor Estimating: 198it [02:06,  1.60it/s]Extractor Estimating: 199it [02:07,  1.65it/s]Extractor Estimating: 200it [02:07,  1.61it/s]Extractor Estimating: 201it [02:08,  1.59it/s]Extractor Estimating: 202it [02:09,  1.61it/s]Extractor Estimating: 203it [02:09,  1.51it/s]Extractor Estimating: 204it [02:10,  1.58it/s]Extractor Estimating: 205it [02:11,  1.62it/s]Extractor Estimating: 206it [02:12,  1.40it/s]Extractor Estimating: 207it [02:12,  1.45it/s]Extractor Estimating: 208it [02:13,  1.47it/s]Extractor Estimating: 209it [02:13,  1.47it/s]Extractor Estimating: 210it [02:14,  1.51it/s]Extractor Estimating: 211it [02:15,  1.47it/s]Extractor Estimating: 212it [02:16,  1.43it/s]Extractor Estimating: 213it [02:16,  1.47it/s]Extractor Estimating: 214it [02:17,  1.49it/s]Extractor Estimating: 215it [02:18,  1.48it/s]Extractor Estimating: 216it [02:18,  1.49it/s]Extractor Estimating: 217it [02:19,  1.52it/s]Extractor Estimating: 218it [02:20,  1.50it/s]Extractor Estimating: 219it [02:20,  1.53it/s]Extractor Estimating: 220it [02:21,  1.55it/s]Extractor Estimating: 221it [02:21,  1.54it/s]Extractor Estimating: 222it [02:22,  1.52it/s]Extractor Estimating: 223it [02:23,  1.50it/s]Extractor Estimating: 224it [02:23,  1.54it/s]Extractor Estimating: 225it [02:24,  1.56it/s]Extractor Estimating: 226it [02:25,  1.60it/s]Extractor Estimating: 227it [02:25,  1.68it/s]Extractor Estimating: 228it [02:26,  1.68it/s]Extractor Estimating: 229it [02:26,  1.73it/s]Extractor Estimating: 230it [02:27,  1.69it/s]Extractor Estimating: 231it [02:27,  1.72it/s]Extractor Estimating: 232it [02:28,  1.72it/s]Extractor Estimating: 233it [02:29,  1.72it/s]Extractor Estimating: 234it [02:29,  1.70it/s]Extractor Estimating: 235it [02:30,  1.74it/s]Extractor Estimating: 236it [02:30,  1.73it/s]Extractor Estimating: 237it [02:31,  1.74it/s]Extractor Estimating: 238it [02:31,  1.75it/s]Extractor Estimating: 239it [02:32,  1.75it/s]Extractor Estimating: 240it [02:33,  1.75it/s]Extractor Estimating: 241it [02:33,  1.69it/s]Extractor Estimating: 242it [02:34,  1.70it/s]Extractor Estimating: 243it [02:34,  1.70it/s]Extractor Estimating: 244it [02:35,  1.68it/s]Extractor Estimating: 245it [02:36,  1.67it/s]Extractor Estimating: 246it [02:36,  1.63it/s]Extractor Estimating: 247it [02:37,  1.67it/s]Extractor Estimating: 248it [02:37,  1.71it/s]Extractor Estimating: 249it [02:38,  1.76it/s]Extractor Estimating: 250it [02:39,  1.74it/s]Extractor Estimating: 251it [02:39,  1.66it/s]Extractor Estimating: 252it [02:40,  1.62it/s]Extractor Estimating: 253it [02:40,  1.63it/s]Extractor Estimating: 254it [02:41,  1.58it/s]Extractor Estimating: 255it [02:42,  1.59it/s]Extractor Estimating: 256it [02:42,  1.57it/s]Extractor Estimating: 257it [02:43,  1.58it/s]Extractor Estimating: 258it [02:44,  1.60it/s]Extractor Estimating: 259it [02:44,  1.61it/s]Extractor Estimating: 260it [02:45,  1.58it/s]Extractor Estimating: 261it [02:46,  1.52it/s]Extractor Estimating: 262it [02:46,  1.53it/s]Extractor Estimating: 263it [02:47,  1.56it/s]Extractor Estimating: 264it [02:48,  1.54it/s]Extractor Estimating: 265it [02:48,  1.60it/s]Extractor Estimating: 266it [02:49,  1.60it/s]Extractor Estimating: 267it [02:49,  1.60it/s]Extractor Estimating: 268it [02:50,  1.65it/s]Extractor Estimating: 269it [02:51,  1.62it/s]Extractor Estimating: 270it [02:51,  1.62it/s]Extractor Estimating: 271it [02:52,  1.60it/s]Extractor Estimating: 272it [02:53,  1.56it/s]Extractor Estimating: 273it [02:53,  1.52it/s]Extractor Estimating: 274it [02:54,  1.58it/s]Extractor Estimating: 275it [02:54,  1.60it/s]Extractor Estimating: 276it [02:55,  1.66it/s]Extractor Estimating: 277it [02:56,  1.63it/s]Extractor Estimating: 278it [02:56,  1.66it/s]Extractor Estimating: 279it [02:57,  1.69it/s]Extractor Estimating: 280it [02:57,  1.70it/s]Extractor Estimating: 281it [02:58,  1.64it/s]Extractor Estimating: 282it [02:59,  1.54it/s]Extractor Estimating: 283it [02:59,  1.59it/s]Extractor Estimating: 284it [03:00,  1.65it/s]Extractor Estimating: 285it [03:00,  1.63it/s]Extractor Estimating: 286it [03:01,  1.68it/s]Extractor Estimating: 287it [03:02,  1.72it/s]Extractor Estimating: 288it [03:02,  1.74it/s]Extractor Estimating: 289it [03:03,  1.76it/s]Extractor Estimating: 290it [03:03,  1.76it/s]Extractor Estimating: 291it [03:04,  1.68it/s]Extractor Estimating: 292it [03:04,  1.72it/s]Extractor Estimating: 293it [03:05,  1.70it/s]Extractor Estimating: 294it [03:06,  1.73it/s]Extractor Estimating: 295it [03:06,  1.73it/s]Extractor Estimating: 296it [03:07,  1.65it/s]Extractor Estimating: 297it [03:08,  1.62it/s]Extractor Estimating: 298it [03:08,  1.66it/s]Extractor Estimating: 299it [03:09,  1.70it/s]Extractor Estimating: 300it [03:09,  1.73it/s]Extractor Estimating: 301it [03:10,  1.65it/s]Extractor Estimating: 302it [03:11,  1.61it/s]Extractor Estimating: 303it [03:11,  1.62it/s]Extractor Estimating: 304it [03:12,  1.59it/s]Extractor Estimating: 305it [03:12,  1.60it/s]Extractor Estimating: 306it [03:13,  1.58it/s]Extractor Estimating: 307it [03:14,  1.58it/s]Extractor Estimating: 308it [03:14,  1.58it/s]Extractor Estimating: 309it [03:15,  1.54it/s]Extractor Estimating: 310it [03:16,  1.57it/s]Extractor Estimating: 311it [03:16,  1.56it/s]Extractor Estimating: 312it [03:17,  1.57it/s]Extractor Estimating: 313it [03:18,  1.57it/s]Extractor Estimating: 314it [03:18,  1.56it/s]Extractor Estimating: 315it [03:19,  1.61it/s]Extractor Estimating: 316it [03:19,  1.56it/s]Extractor Estimating: 317it [03:20,  1.59it/s]Extractor Estimating: 318it [03:21,  1.55it/s]Extractor Estimating: 319it [03:21,  1.56it/s]Extractor Estimating: 320it [03:22,  1.54it/s]Extractor Estimating: 321it [03:23,  1.51it/s]Extractor Estimating: 322it [03:23,  1.53it/s]Extractor Estimating: 323it [03:24,  1.56it/s]Extractor Estimating: 324it [03:25,  1.57it/s]Extractor Estimating: 325it [03:25,  1.59it/s]Extractor Estimating: 326it [03:26,  1.62it/s]Extractor Estimating: 327it [03:26,  1.61it/s]Extractor Estimating: 328it [03:27,  1.64it/s]Extractor Estimating: 329it [03:28,  1.63it/s]Extractor Estimating: 330it [03:28,  1.65it/s]Extractor Estimating: 331it [03:29,  1.63it/s]Extractor Estimating: 332it [03:30,  1.59it/s]Extractor Estimating: 333it [03:30,  1.64it/s]Extractor Estimating: 334it [03:31,  1.57it/s]Extractor Estimating: 335it [03:31,  1.59it/s]Extractor Estimating: 336it [03:32,  1.59it/s]Extractor Estimating: 337it [03:33,  1.56it/s]Extractor Estimating: 338it [03:33,  1.52it/s]Extractor Estimating: 339it [03:34,  1.56it/s]Extractor Estimating: 340it [03:35,  1.60it/s]Extractor Estimating: 341it [03:35,  1.63it/s]Extractor Estimating: 342it [03:36,  1.53it/s]Extractor Estimating: 343it [03:36,  1.59it/s]Extractor Estimating: 344it [03:37,  1.67it/s]Extractor Estimating: 345it [03:38,  1.60it/s]Extractor Estimating: 346it [03:38,  1.55it/s]Extractor Estimating: 347it [03:39,  1.53it/s]Extractor Estimating: 348it [03:40,  1.56it/s]Extractor Estimating: 349it [03:40,  1.59it/s]Extractor Estimating: 350it [03:41,  1.59it/s]Extractor Estimating: 351it [03:42,  1.61it/s]Extractor Estimating: 352it [03:42,  1.58it/s]Extractor Estimating: 353it [03:43,  1.53it/s]Extractor Estimating: 354it [03:44,  1.50it/s]Extractor Estimating: 355it [03:44,  1.45it/s]Extractor Estimating: 356it [03:45,  1.47it/s]Extractor Estimating: 357it [03:46,  1.35it/s]Extractor Estimating: 358it [03:46,  1.41it/s]Extractor Estimating: 359it [03:47,  1.50it/s]Extractor Estimating: 360it [03:48,  1.47it/s]Extractor Estimating: 361it [03:48,  1.45it/s]Extractor Estimating: 362it [03:49,  1.50it/s]Extractor Estimating: 363it [03:50,  1.58it/s]Extractor Estimating: 364it [03:50,  1.56it/s]Extractor Estimating: 365it [03:51,  1.53it/s]Extractor Estimating: 366it [03:52,  1.53it/s]Extractor Estimating: 367it [03:52,  1.53it/s]Extractor Estimating: 368it [03:53,  1.52it/s]Extractor Estimating: 369it [03:54,  1.53it/s]Extractor Estimating: 370it [03:54,  1.56it/s]Extractor Estimating: 371it [03:55,  1.55it/s]Extractor Estimating: 372it [03:56,  1.54it/s]Extractor Estimating: 373it [03:56,  1.54it/s]Extractor Estimating: 374it [03:57,  1.51it/s]Extractor Estimating: 375it [03:58,  1.53it/s]Extractor Estimating: 375it [03:58,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:02,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:02,726 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:02,726 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:02,726 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:02,726 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:52:03,342 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:52:03,343 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:52:03,953 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:52:05,042 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:52:05,043 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:08,075 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:08,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:08,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:08,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:52:08,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:52:08,708 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:52:08,709 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:52:09,281 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:52:09,452 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:52:09,453 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:59:46,295 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:59:46,300 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7511 mean pseudo reward: 0.9309718261056585
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 25139
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25239, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25239, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.987, loss:950.7025
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.000, loss:898.5370
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.992, loss:889.7521
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.000, loss:834.8419
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.015, loss:901.2136
>> valid entity prec:0.6037, rec:0.5148, f1:0.5557
>> valid relation prec:0.2850, rec:0.0665, f1:0.1078
>> valid relation with NER prec:0.2850, rec:0.0665, f1:0.1078
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.178, loss:885.8882
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.000, loss:843.4259
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.000, loss:863.9251
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.007, loss:889.0912
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.993, loss:850.6780
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5264, rec:0.6009, f1:0.5612
>> valid relation prec:0.2254, rec:0.0748, f1:0.1123
>> valid relation with NER prec:0.2254, rec:0.0748, f1:0.1123
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.194, loss:874.8629
g_step 1200, step 261, avg_time 0.993, loss:841.5114
g_step 1300, step 48, avg_time 1.001, loss:844.6338
g_step 1400, step 148, avg_time 0.994, loss:843.3360
g_step 1500, step 248, avg_time 1.000, loss:811.1902
>> valid entity prec:0.5857, rec:0.6221, f1:0.6033
>> valid relation prec:0.2251, rec:0.0751, f1:0.1126
>> valid relation with NER prec:0.2251, rec:0.0751, f1:0.1126
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.212, loss:809.5202
g_step 1700, step 135, avg_time 0.993, loss:765.2178
g_step 1800, step 235, avg_time 0.987, loss:778.8398
g_step 1900, step 22, avg_time 1.003, loss:795.7661
g_step 2000, step 122, avg_time 1.002, loss:711.2257
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6702, rec:0.4420, f1:0.5327
>> valid relation prec:0.2712, rec:0.0567, f1:0.0938
>> valid relation with NER prec:0.2712, rec:0.0567, f1:0.0938
g_step 2100, step 222, avg_time 2.156, loss:773.0689
g_step 2200, step 9, avg_time 1.004, loss:727.2931
g_step 2300, step 109, avg_time 0.984, loss:671.1097
g_step 2400, step 209, avg_time 0.993, loss:717.1009
g_step 2500, step 309, avg_time 1.003, loss:715.9066
>> valid entity prec:0.5979, rec:0.5578, f1:0.5771
>> valid relation prec:0.2851, rec:0.0705, f1:0.1130
>> valid relation with NER prec:0.2851, rec:0.0705, f1:0.1130
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.169, loss:650.3341
g_step 2700, step 196, avg_time 0.985, loss:679.8560
g_step 2800, step 296, avg_time 1.006, loss:713.0337
g_step 2900, step 83, avg_time 0.984, loss:639.1009
g_step 3000, step 183, avg_time 0.996, loss:651.2773
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6138, rec:0.5174, f1:0.5615
>> valid relation prec:0.2491, rec:0.0613, f1:0.0984
>> valid relation with NER prec:0.2491, rec:0.0613, f1:0.0984
g_step 3100, step 283, avg_time 2.180, loss:663.8752
g_step 3200, step 70, avg_time 1.003, loss:616.2574
g_step 3300, step 170, avg_time 0.995, loss:610.8642
g_step 3400, step 270, avg_time 0.992, loss:643.8964
g_step 3500, step 57, avg_time 0.996, loss:584.5796
>> valid entity prec:0.6132, rec:0.4489, f1:0.5183
>> valid relation prec:0.1831, rec:0.0504, f1:0.0791
>> valid relation with NER prec:0.1831, rec:0.0504, f1:0.0791
g_step 3600, step 157, avg_time 2.175, loss:579.8971
g_step 3700, step 257, avg_time 0.997, loss:609.2563
g_step 3800, step 44, avg_time 1.002, loss:597.1774
g_step 3900, step 144, avg_time 1.000, loss:568.4169
g_step 4000, step 244, avg_time 0.984, loss:592.6221
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5586, rec:0.4777, f1:0.5150
>> valid relation prec:0.2481, rec:0.0665, f1:0.1048
>> valid relation with NER prec:0.2481, rec:0.0665, f1:0.1048
g_step 4100, step 31, avg_time 2.176, loss:565.6220
g_step 4200, step 131, avg_time 0.995, loss:528.2493
g_step 4300, step 231, avg_time 0.993, loss:554.1540
g_step 4400, step 18, avg_time 0.993, loss:549.3036
g_step 4500, step 118, avg_time 0.989, loss:512.7150
>> valid entity prec:0.5734, rec:0.5485, f1:0.5607
>> valid relation prec:0.1938, rec:0.0596, f1:0.0911
>> valid relation with NER prec:0.1938, rec:0.0596, f1:0.0911
g_step 4600, step 218, avg_time 2.186, loss:526.8329
g_step 4700, step 5, avg_time 0.989, loss:526.7169
g_step 4800, step 105, avg_time 1.002, loss:479.5012
g_step 4900, step 205, avg_time 1.003, loss:505.6500
g_step 5000, step 305, avg_time 0.976, loss:529.0756
learning rate was adjusted to 0.0008
>> valid entity prec:0.6010, rec:0.4414, f1:0.5090
>> valid relation prec:0.2603, rec:0.0636, f1:0.1022
>> valid relation with NER prec:0.2603, rec:0.0636, f1:0.1022
g_step 5100, step 92, avg_time 2.167, loss:463.9732
g_step 5200, step 192, avg_time 0.992, loss:478.4248
g_step 5300, step 292, avg_time 0.996, loss:489.2623
g_step 5400, step 79, avg_time 0.997, loss:453.4473
g_step 5500, step 179, avg_time 0.992, loss:456.3198
>> valid entity prec:0.5924, rec:0.5335, f1:0.5614
>> valid relation prec:0.2032, rec:0.0662, f1:0.0998
>> valid relation with NER prec:0.2032, rec:0.0662, f1:0.0998
g_step 5600, step 279, avg_time 2.172, loss:471.7700
g_step 5700, step 66, avg_time 0.998, loss:449.3313
g_step 5800, step 166, avg_time 0.982, loss:446.0763
g_step 5900, step 266, avg_time 1.005, loss:474.1236
g_step 6000, step 53, avg_time 0.986, loss:428.4234
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5848, rec:0.5285, f1:0.5552
>> valid relation prec:0.1857, rec:0.0670, f1:0.0985
>> valid relation with NER prec:0.1857, rec:0.0670, f1:0.0985
g_step 6100, step 153, avg_time 2.184, loss:436.7665
g_step 6200, step 253, avg_time 1.001, loss:452.5653
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:59:46 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:59:46 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-59-46_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:59:47 - WARNING - datasets.builder -   Using custom data configuration default-34b783beac7406eb
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-34b783beac7406eb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:59:47,758 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:59:47,759 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:59:47,760 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:59:47,760 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:59:47,775 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:59:47,779 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:59:47,779 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:59:47,779 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:59:47,780 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:59:47,780 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:59:47,780 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:59:47,919 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:59:51,116 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:59:51,133 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-34b783beac7406eb/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.84ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.73ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.08ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.29ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.36ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.42ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.47ba/s]100%|██████████| 8/8 [00:01<00:00,  5.24ba/s]100%|██████████| 8/8 [00:01<00:00,  4.51ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.98ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.14ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.63ba/s]100%|██████████| 4/4 [00:00<00:00,  4.71ba/s]100%|██████████| 4/4 [00:00<00:00,  4.18ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  5.89ba/s] 38%|███▊      | 3/8 [00:00<00:00,  8.92ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.85ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.28ba/s]100%|██████████| 8/8 [00:00<00:00, 10.36ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.77ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.13ba/s]100%|██████████| 4/4 [00:00<00:00, 11.48ba/s]
[INFO|trainer.py:414] 2023-08-28 16:59:55,817 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:59:55,823 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:59:55,823 >>   Num examples = 7521
[INFO|trainer.py:1149] 2023-08-28 16:59:55,823 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:59:55,824 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:59:55,824 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:59:55,824 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:59:55,824 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:54,  3.38it/s]  0%|          | 2/590 [00:00<02:51,  3.43it/s]  1%|          | 3/590 [00:00<02:50,  3.45it/s]  1%|          | 4/590 [00:01<02:49,  3.46it/s]  1%|          | 5/590 [00:01<02:48,  3.47it/s]  1%|          | 6/590 [00:01<02:50,  3.43it/s]  1%|          | 7/590 [00:02<02:50,  3.42it/s]  1%|▏         | 8/590 [00:02<02:49,  3.43it/s]  2%|▏         | 9/590 [00:02<02:49,  3.43it/s]  2%|▏         | 10/590 [00:02<02:49,  3.43it/s]  2%|▏         | 11/590 [00:03<02:48,  3.43it/s]  2%|▏         | 12/590 [00:03<02:48,  3.43it/s]  2%|▏         | 13/590 [00:03<02:48,  3.43it/s]  2%|▏         | 14/590 [00:04<02:48,  3.43it/s]  3%|▎         | 15/590 [00:04<02:47,  3.42it/s]  3%|▎         | 16/590 [00:04<02:47,  3.42it/s]  3%|▎         | 17/590 [00:05<03:01,  3.15it/s]  3%|▎         | 18/590 [00:05<02:57,  3.23it/s]  3%|▎         | 19/590 [00:05<02:53,  3.29it/s]  3%|▎         | 20/590 [00:05<02:51,  3.33it/s]  4%|▎         | 21/590 [00:06<02:49,  3.36it/s]  4%|▎         | 22/590 [00:06<02:48,  3.38it/s]  4%|▍         | 23/590 [00:06<02:47,  3.39it/s]  4%|▍         | 24/590 [00:07<02:46,  3.40it/s]  4%|▍         | 25/590 [00:07<02:45,  3.41it/s]  4%|▍         | 26/590 [00:07<02:45,  3.41it/s]  5%|▍         | 27/590 [00:08<03:05,  3.04it/s]  5%|▍         | 28/590 [00:08<02:58,  3.14it/s]  5%|▍         | 29/590 [00:08<02:53,  3.23it/s]  5%|▌         | 30/590 [00:08<02:50,  3.28it/s]  5%|▌         | 31/590 [00:09<02:48,  3.33it/s]  5%|▌         | 32/590 [00:09<02:46,  3.36it/s]  6%|▌         | 33/590 [00:09<02:44,  3.38it/s]  6%|▌         | 34/590 [00:10<02:44,  3.39it/s]  6%|▌         | 35/590 [00:10<02:43,  3.39it/s]  6%|▌         | 36/590 [00:10<02:43,  3.40it/s]  6%|▋         | 37/590 [00:11<02:44,  3.35it/s]  6%|▋         | 38/590 [00:11<02:43,  3.37it/s]  7%|▋         | 39/590 [00:11<02:42,  3.39it/s]  7%|▋         | 40/590 [00:11<02:41,  3.40it/s]  7%|▋         | 41/590 [00:12<02:41,  3.40it/s]  7%|▋         | 42/590 [00:12<02:40,  3.41it/s]  7%|▋         | 43/590 [00:12<02:40,  3.41it/s]  7%|▋         | 44/590 [00:13<02:40,  3.41it/s]  8%|▊         | 45/590 [00:13<02:39,  3.42it/s]  8%|▊         | 46/590 [00:13<02:39,  3.42it/s]  8%|▊         | 47/590 [00:13<02:38,  3.42it/s]  8%|▊         | 48/590 [00:14<02:57,  3.06it/s]  8%|▊         | 49/590 [00:14<02:51,  3.16it/s]  8%|▊         | 50/590 [00:14<02:47,  3.23it/s]  9%|▊         | 51/590 [00:15<02:44,  3.28it/s]  9%|▉         | 52/590 [00:15<02:41,  3.32it/s]  9%|▉         | 53/590 [00:15<02:40,  3.35it/s]  9%|▉         | 54/590 [00:16<02:39,  3.37it/s]  9%|▉         | 55/590 [00:16<02:38,  3.38it/s]  9%|▉         | 56/590 [00:16<02:37,  3.39it/s] 10%|▉         | 57/590 [00:16<02:36,  3.40it/s] 10%|▉         | 58/590 [00:17<03:26,  2.58it/s] 10%|█         | 59/590 [00:17<03:10,  2.78it/s] 10%|█         | 60/590 [00:18<02:59,  2.95it/s] 10%|█         | 61/590 [00:18<02:52,  3.07it/s] 11%|█         | 62/590 [00:18<02:46,  3.17it/s] 11%|█         | 63/590 [00:19<02:42,  3.24it/s] 11%|█         | 64/590 [00:19<02:39,  3.29it/s] 11%|█         | 65/590 [00:19<02:37,  3.33it/s] 11%|█         | 66/590 [00:19<02:36,  3.35it/s] 11%|█▏        | 67/590 [00:20<02:35,  3.37it/s] 12%|█▏        | 68/590 [00:20<02:37,  3.32it/s] 12%|█▏        | 69/590 [00:20<02:35,  3.35it/s] 12%|█▏        | 70/590 [00:21<02:34,  3.37it/s] 12%|█▏        | 71/590 [00:21<02:33,  3.38it/s] 12%|█▏        | 72/590 [00:21<02:32,  3.39it/s] 12%|█▏        | 73/590 [00:22<02:31,  3.40it/s] 13%|█▎        | 74/590 [00:22<02:31,  3.40it/s] 13%|█▎        | 75/590 [00:22<02:31,  3.41it/s] 13%|█▎        | 76/590 [00:22<02:30,  3.41it/s] 13%|█▎        | 77/590 [00:23<02:30,  3.41it/s] 13%|█▎        | 78/590 [00:23<02:29,  3.42it/s] 13%|█▎        | 79/590 [00:23<02:29,  3.42it/s] 14%|█▎        | 80/590 [00:24<02:32,  3.35it/s] 14%|█▎        | 81/590 [00:24<02:30,  3.37it/s] 14%|█▍        | 82/590 [00:24<02:29,  3.39it/s] 14%|█▍        | 83/590 [00:24<02:32,  3.32it/s] 14%|█▍        | 84/590 [00:25<02:30,  3.35it/s] 14%|█▍        | 85/590 [00:25<02:29,  3.37it/s] 15%|█▍        | 86/590 [00:25<02:28,  3.39it/s] 15%|█▍        | 87/590 [00:26<02:28,  3.40it/s] 15%|█▍        | 88/590 [00:26<02:27,  3.40it/s] 15%|█▌        | 89/590 [00:26<02:27,  3.41it/s] 15%|█▌        | 90/590 [00:27<02:26,  3.41it/s] 15%|█▌        | 91/590 [00:27<02:27,  3.39it/s] 16%|█▌        | 92/590 [00:27<02:26,  3.40it/s] 16%|█▌        | 93/590 [00:27<02:25,  3.41it/s] 16%|█▌        | 94/590 [00:28<02:25,  3.41it/s] 16%|█▌        | 95/590 [00:28<02:25,  3.41it/s] 16%|█▋        | 96/590 [00:28<02:24,  3.41it/s] 16%|█▋        | 97/590 [00:29<02:24,  3.41it/s] 17%|█▋        | 98/590 [00:29<02:24,  3.41it/s] 17%|█▋        | 99/590 [00:29<02:23,  3.41it/s] 17%|█▋        | 100/590 [00:29<02:23,  3.41it/s] 17%|█▋        | 101/590 [00:30<02:23,  3.41it/s] 17%|█▋        | 102/590 [00:30<02:25,  3.36it/s] 17%|█▋        | 103/590 [00:30<02:24,  3.38it/s] 18%|█▊        | 104/590 [00:31<02:23,  3.39it/s] 18%|█▊        | 105/590 [00:31<02:22,  3.40it/s] 18%|█▊        | 106/590 [00:31<02:22,  3.40it/s] 18%|█▊        | 107/590 [00:32<02:21,  3.41it/s] 18%|█▊        | 108/590 [00:32<02:21,  3.41it/s] 18%|█▊        | 109/590 [00:32<02:20,  3.42it/s] 19%|█▊        | 110/590 [00:32<02:20,  3.41it/s] 19%|█▉        | 111/590 [00:33<02:20,  3.41it/s] 19%|█▉        | 112/590 [00:33<02:19,  3.42it/s] 19%|█▉        | 113/590 [00:33<02:39,  2.98it/s] 19%|█▉        | 114/590 [00:34<02:33,  3.10it/s] 19%|█▉        | 115/590 [00:34<02:28,  3.19it/s] 20%|█▉        | 116/590 [00:34<02:25,  3.25it/s] 20%|█▉        | 117/590 [00:35<02:23,  3.30it/s] 20%|██        | 118/590 [00:35<02:06,  3.72it/s][INFO|trainer.py:2140] 2023-08-28 17:00:31,098 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:00:31,098 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:00:31,098 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.86it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.48it/s][A
  4%|▍         | 18/437 [00:00<00:08, 47.42it/s][A
  5%|▌         | 23/437 [00:00<00:08, 46.56it/s][A
  6%|▋         | 28/437 [00:00<00:08, 45.98it/s][A
  8%|▊         | 33/437 [00:00<00:08, 45.56it/s][A
  9%|▊         | 38/437 [00:00<00:08, 45.25it/s][A
 10%|▉         | 43/437 [00:00<00:08, 44.93it/s][A
 11%|█         | 48/437 [00:01<00:08, 44.97it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 45.11it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 45.26it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 45.14it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 45.19it/s][A
 17%|█▋        | 73/437 [00:01<00:08, 44.96it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 44.96it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 44.89it/s][A
 20%|██        | 88/437 [00:01<00:07, 44.69it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 44.81it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 44.87it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 44.94it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 45.03it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 45.26it/s][A
 27%|██▋       | 118/437 [00:02<00:07, 45.09it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 45.07it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 44.92it/s][A
 30%|███       | 133/437 [00:02<00:06, 44.77it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 44.85it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 44.88it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 44.94it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 45.03it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 45.19it/s][A
 37%|███▋      | 163/437 [00:03<00:06, 45.17it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 45.06it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 44.95it/s][A
 41%|████      | 178/437 [00:03<00:05, 44.90it/s][A
 42%|████▏     | 183/437 [00:04<00:05, 44.84it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 44.94it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 45.02it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 45.11it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 45.19it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 45.13it/s][A
 49%|████▊     | 213/437 [00:04<00:04, 45.00it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 44.86it/s][A
 51%|█████     | 223/437 [00:04<00:04, 44.84it/s][A
 52%|█████▏    | 228/437 [00:05<00:04, 44.94it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 44.83it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 45.01it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 45.03it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 45.11it/s][A
 58%|█████▊    | 253/437 [00:05<00:04, 45.14it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 44.87it/s][A
 60%|██████    | 263/437 [00:05<00:03, 44.72it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 44.90it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 44.88it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 44.96it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 45.04it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 45.17it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 45.15it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 45.17it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 44.99it/s][A
 70%|███████   | 308/437 [00:06<00:02, 44.91it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 44.88it/s][A
 73%|███████▎  | 318/437 [00:07<00:02, 44.88it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 44.87it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 45.00it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 44.51it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 44.75it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 44.84it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 44.79it/s][A
 81%|████████  | 353/437 [00:07<00:01, 44.81it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 44.77it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 44.71it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 44.91it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 44.88it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 44.94it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.01it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 45.06it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 45.02it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 44.90it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 44.85it/s][A
 93%|█████████▎| 408/437 [00:09<00:00, 44.84it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 44.95it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 44.98it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 45.08it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 45.07it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 45.06it/s][A                                                 
                                                 [A 20%|██        | 118/590 [00:44<02:06,  3.72it/s]
100%|██████████| 437/437 [00:09<00:00, 45.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:00:40,833 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-28 17:00:40,854 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:00:45,083 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:00:45,101 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:00:45,111 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [01:02<1:05:47,  8.38s/it] 20%|██        | 120/590 [01:02<46:39,  5.96s/it]   21%|██        | 121/590 [01:03<33:16,  4.26s/it] 21%|██        | 122/590 [01:03<23:55,  3.07s/it] 21%|██        | 123/590 [01:03<17:23,  2.23s/it] 21%|██        | 124/590 [01:04<12:49,  1.65s/it] 21%|██        | 125/590 [01:04<09:38,  1.24s/it] 21%|██▏       | 126/590 [01:04<07:24,  1.04it/s] 22%|██▏       | 127/590 [01:04<05:51,  1.32it/s] 22%|██▏       | 128/590 [01:05<04:45,  1.62it/s] 22%|██▏       | 129/590 [01:05<03:59,  1.92it/s] 22%|██▏       | 130/590 [01:05<03:27,  2.21it/s] 22%|██▏       | 131/590 [01:06<03:06,  2.46it/s] 22%|██▏       | 132/590 [01:06<02:50,  2.69it/s] 23%|██▎       | 133/590 [01:06<02:39,  2.87it/s] 23%|██▎       | 134/590 [01:06<02:31,  3.01it/s] 23%|██▎       | 135/590 [01:07<02:25,  3.13it/s] 23%|██▎       | 136/590 [01:07<02:21,  3.21it/s] 23%|██▎       | 137/590 [01:07<02:18,  3.27it/s] 23%|██▎       | 138/590 [01:08<02:16,  3.32it/s] 24%|██▎       | 139/590 [01:08<02:14,  3.35it/s] 24%|██▎       | 140/590 [01:08<02:13,  3.37it/s] 24%|██▍       | 141/590 [01:09<02:12,  3.38it/s] 24%|██▍       | 142/590 [01:09<02:17,  3.26it/s] 24%|██▍       | 143/590 [01:09<02:15,  3.31it/s] 24%|██▍       | 144/590 [01:09<02:13,  3.34it/s] 25%|██▍       | 145/590 [01:10<02:12,  3.37it/s] 25%|██▍       | 146/590 [01:10<02:11,  3.38it/s] 25%|██▍       | 147/590 [01:10<02:10,  3.39it/s] 25%|██▌       | 148/590 [01:11<02:09,  3.40it/s] 25%|██▌       | 149/590 [01:11<02:09,  3.40it/s] 25%|██▌       | 150/590 [01:11<02:09,  3.41it/s] 26%|██▌       | 151/590 [01:11<02:08,  3.42it/s] 26%|██▌       | 152/590 [01:12<02:08,  3.42it/s] 26%|██▌       | 153/590 [01:12<02:08,  3.41it/s] 26%|██▌       | 154/590 [01:12<02:07,  3.41it/s] 26%|██▋       | 155/590 [01:13<02:07,  3.41it/s] 26%|██▋       | 156/590 [01:13<02:07,  3.42it/s] 27%|██▋       | 157/590 [01:13<02:06,  3.42it/s] 27%|██▋       | 158/590 [01:14<02:06,  3.42it/s] 27%|██▋       | 159/590 [01:14<02:06,  3.42it/s] 27%|██▋       | 160/590 [01:14<02:05,  3.42it/s] 27%|██▋       | 161/590 [01:14<02:05,  3.42it/s] 27%|██▋       | 162/590 [01:15<02:05,  3.42it/s] 28%|██▊       | 163/590 [01:15<02:04,  3.42it/s] 28%|██▊       | 164/590 [01:15<02:12,  3.23it/s] 28%|██▊       | 165/590 [01:16<02:09,  3.28it/s] 28%|██▊       | 166/590 [01:16<02:07,  3.32it/s] 28%|██▊       | 167/590 [01:16<02:06,  3.35it/s] 28%|██▊       | 168/590 [01:17<02:05,  3.37it/s] 29%|██▊       | 169/590 [01:17<02:04,  3.39it/s] 29%|██▉       | 170/590 [01:17<02:03,  3.39it/s] 29%|██▉       | 171/590 [01:17<02:03,  3.40it/s] 29%|██▉       | 172/590 [01:18<02:02,  3.40it/s] 29%|██▉       | 173/590 [01:18<02:02,  3.41it/s] 29%|██▉       | 174/590 [01:18<02:02,  3.41it/s] 30%|██▉       | 175/590 [01:19<02:06,  3.28it/s] 30%|██▉       | 176/590 [01:19<02:04,  3.32it/s] 30%|███       | 177/590 [01:19<02:03,  3.35it/s] 30%|███       | 178/590 [01:19<02:02,  3.37it/s] 30%|███       | 179/590 [01:20<02:01,  3.38it/s] 31%|███       | 180/590 [01:20<02:00,  3.39it/s] 31%|███       | 181/590 [01:20<02:00,  3.40it/s] 31%|███       | 182/590 [01:21<01:59,  3.40it/s] 31%|███       | 183/590 [01:21<01:59,  3.40it/s] 31%|███       | 184/590 [01:21<01:59,  3.41it/s] 31%|███▏      | 185/590 [01:22<01:58,  3.41it/s] 32%|███▏      | 186/590 [01:22<01:58,  3.40it/s] 32%|███▏      | 187/590 [01:22<01:58,  3.40it/s] 32%|███▏      | 188/590 [01:22<01:58,  3.40it/s] 32%|███▏      | 189/590 [01:23<01:57,  3.40it/s] 32%|███▏      | 190/590 [01:23<01:57,  3.41it/s] 32%|███▏      | 191/590 [01:23<01:57,  3.41it/s] 33%|███▎      | 192/590 [01:24<02:55,  2.27it/s] 33%|███▎      | 193/590 [01:24<02:37,  2.52it/s] 33%|███▎      | 194/590 [01:25<03:13,  2.05it/s] 33%|███▎      | 195/590 [01:25<02:53,  2.27it/s] 33%|███▎      | 196/590 [01:26<02:36,  2.52it/s] 33%|███▎      | 197/590 [01:26<02:23,  2.74it/s] 34%|███▎      | 198/590 [01:26<02:14,  2.91it/s] 34%|███▎      | 199/590 [01:27<02:08,  3.04it/s] 34%|███▍      | 200/590 [01:27<02:03,  3.15it/s] 34%|███▍      | 201/590 [01:27<02:00,  3.22it/s] 34%|███▍      | 202/590 [01:27<01:58,  3.28it/s] 34%|███▍      | 203/590 [01:28<01:56,  3.32it/s] 35%|███▍      | 204/590 [01:28<01:55,  3.35it/s] 35%|███▍      | 205/590 [01:28<01:53,  3.39it/s] 35%|███▍      | 206/590 [01:29<02:35,  2.48it/s] 35%|███▌      | 207/590 [01:29<02:21,  2.71it/s] 35%|███▌      | 208/590 [01:30<02:11,  2.90it/s] 35%|███▌      | 209/590 [01:30<02:05,  3.05it/s] 36%|███▌      | 210/590 [01:30<02:00,  3.16it/s] 36%|███▌      | 211/590 [01:30<01:56,  3.25it/s] 36%|███▌      | 212/590 [01:31<01:54,  3.31it/s] 36%|███▌      | 213/590 [01:31<01:52,  3.35it/s] 36%|███▋      | 214/590 [01:31<01:51,  3.39it/s] 36%|███▋      | 215/590 [01:32<01:49,  3.41it/s] 37%|███▋      | 216/590 [01:32<01:49,  3.41it/s] 37%|███▋      | 217/590 [01:32<01:49,  3.42it/s] 37%|███▋      | 218/590 [01:32<01:48,  3.42it/s] 37%|███▋      | 219/590 [01:33<01:48,  3.41it/s] 37%|███▋      | 220/590 [01:33<01:48,  3.42it/s] 37%|███▋      | 221/590 [01:33<01:48,  3.42it/s] 38%|███▊      | 222/590 [01:34<01:47,  3.41it/s] 38%|███▊      | 223/590 [01:34<01:47,  3.42it/s] 38%|███▊      | 224/590 [01:34<01:47,  3.41it/s] 38%|███▊      | 225/590 [01:35<01:46,  3.41it/s] 38%|███▊      | 226/590 [01:35<01:46,  3.42it/s] 38%|███▊      | 227/590 [01:35<01:49,  3.31it/s] 39%|███▊      | 228/590 [01:35<01:48,  3.34it/s] 39%|███▉      | 229/590 [01:36<01:47,  3.36it/s] 39%|███▉      | 230/590 [01:36<01:46,  3.38it/s] 39%|███▉      | 231/590 [01:36<01:45,  3.39it/s] 39%|███▉      | 232/590 [01:37<01:45,  3.40it/s] 39%|███▉      | 233/590 [01:37<01:44,  3.40it/s] 40%|███▉      | 234/590 [01:37<01:44,  3.41it/s] 40%|███▉      | 235/590 [01:37<01:44,  3.41it/s] 40%|████      | 236/590 [01:38<01:32,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 17:01:33,984 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:01:33,984 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:01:33,984 >>   Batch size = 8
{'eval_loss': 0.9916616678237915, 'eval_runtime': 9.7129, 'eval_samples_per_second': 359.626, 'eval_steps_per_second': 44.992, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.95it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.34it/s][A
  4%|▍         | 17/437 [00:00<00:09, 44.83it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.09it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.16it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.07it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.96it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.70it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.76it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.89it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.86it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.96it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 45.13it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 45.08it/s][A
 18%|█▊        | 77/437 [00:01<00:07, 45.21it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.96it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.77it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.82it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.86it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.91it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 45.02it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 45.10it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 45.03it/s][A
 28%|██▊       | 122/437 [00:02<00:06, 45.05it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 45.01it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.73it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.88it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.86it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.84it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.94it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 45.02it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 45.08it/s][A
 38%|███▊      | 167/437 [00:03<00:05, 45.17it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.96it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.90it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.92it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.83it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.95it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 45.00it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 45.07it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 45.12it/s][A
 49%|████▊     | 212/437 [00:04<00:04, 45.17it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 45.03it/s][A
 51%|█████     | 222/437 [00:04<00:04, 45.00it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.82it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.82it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.83it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.95it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 45.02it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 45.12it/s][A
 59%|█████▉    | 257/437 [00:05<00:03, 45.05it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 45.04it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.92it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.95it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.83it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.89it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 42.55it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.41it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.11it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.43it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.64it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.74it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.70it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.69it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.53it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.71it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.90it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.98it/s][A
 79%|███████▉  | 347/437 [00:07<00:01, 45.07it/s][A
 81%|████████  | 352/437 [00:07<00:01, 45.19it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.98it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.93it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.74it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.74it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.78it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.76it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 45.03it/s][A
 90%|████████▉ | 392/437 [00:08<00:00, 45.13it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 45.13it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 45.01it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.87it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.81it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.81it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 41.70it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 42.78it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.54it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.19it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.19it/s][A 40%|████      | 236/590 [01:47<01:32,  3.81it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:01:43,803 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-28 17:01:43,877 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:01:48,155 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:01:48,178 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:01:48,188 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [02:01<41:55,  7.13s/it] 40%|████      | 238/590 [02:01<29:48,  5.08s/it] 41%|████      | 239/590 [02:01<21:19,  3.64s/it] 41%|████      | 240/590 [02:02<15:23,  2.64s/it] 41%|████      | 241/590 [02:02<11:15,  1.93s/it] 41%|████      | 242/590 [02:02<08:21,  1.44s/it] 41%|████      | 243/590 [02:03<06:20,  1.10s/it] 41%|████▏     | 244/590 [02:03<04:55,  1.17it/s] 42%|████▏     | 245/590 [02:03<03:56,  1.46it/s] 42%|████▏     | 246/590 [02:03<03:15,  1.76it/s] 42%|████▏     | 247/590 [02:04<02:46,  2.06it/s] 42%|████▏     | 248/590 [02:04<02:26,  2.34it/s] 42%|████▏     | 249/590 [02:04<02:12,  2.58it/s] 42%|████▏     | 250/590 [02:05<02:02,  2.79it/s] 43%|████▎     | 251/590 [02:05<01:54,  2.95it/s] 43%|████▎     | 252/590 [02:05<01:49,  3.08it/s] 43%|████▎     | 253/590 [02:05<01:46,  3.17it/s] 43%|████▎     | 254/590 [02:06<01:43,  3.24it/s] 43%|████▎     | 255/590 [02:06<01:41,  3.29it/s] 43%|████▎     | 256/590 [02:06<01:40,  3.33it/s] 44%|████▎     | 257/590 [02:07<01:39,  3.36it/s] 44%|████▎     | 258/590 [02:07<01:38,  3.37it/s] 44%|████▍     | 259/590 [02:07<01:37,  3.39it/s] 44%|████▍     | 260/590 [02:08<01:39,  3.32it/s] 44%|████▍     | 261/590 [02:08<01:38,  3.35it/s] 44%|████▍     | 262/590 [02:08<01:37,  3.37it/s] 45%|████▍     | 263/590 [02:08<01:36,  3.38it/s] 45%|████▍     | 264/590 [02:09<01:35,  3.40it/s] 45%|████▍     | 265/590 [02:09<01:35,  3.40it/s] 45%|████▌     | 266/590 [02:09<01:35,  3.41it/s] 45%|████▌     | 267/590 [02:10<01:34,  3.41it/s] 45%|████▌     | 268/590 [02:10<01:34,  3.42it/s] 46%|████▌     | 269/590 [02:10<01:33,  3.42it/s] 46%|████▌     | 270/590 [02:10<01:33,  3.43it/s] 46%|████▌     | 271/590 [02:11<01:33,  3.40it/s] 46%|████▌     | 272/590 [02:11<01:33,  3.42it/s] 46%|████▋     | 273/590 [02:11<01:32,  3.43it/s] 46%|████▋     | 274/590 [02:12<01:31,  3.45it/s] 47%|████▋     | 275/590 [02:12<01:31,  3.45it/s] 47%|████▋     | 276/590 [02:12<01:30,  3.46it/s] 47%|████▋     | 277/590 [02:13<01:30,  3.45it/s] 47%|████▋     | 278/590 [02:13<01:30,  3.46it/s] 47%|████▋     | 279/590 [02:13<01:29,  3.46it/s] 47%|████▋     | 280/590 [02:13<01:29,  3.46it/s] 48%|████▊     | 281/590 [02:14<01:29,  3.46it/s] 48%|████▊     | 282/590 [02:14<01:29,  3.46it/s] 48%|████▊     | 283/590 [02:14<01:28,  3.46it/s] 48%|████▊     | 284/590 [02:15<01:28,  3.46it/s] 48%|████▊     | 285/590 [02:15<01:27,  3.47it/s] 48%|████▊     | 286/590 [02:15<01:27,  3.47it/s] 49%|████▊     | 287/590 [02:15<01:27,  3.47it/s] 49%|████▉     | 288/590 [02:16<01:27,  3.47it/s] 49%|████▉     | 289/590 [02:16<01:26,  3.47it/s] 49%|████▉     | 290/590 [02:16<01:26,  3.47it/s] 49%|████▉     | 291/590 [02:17<01:26,  3.47it/s] 49%|████▉     | 292/590 [02:17<01:25,  3.47it/s] 50%|████▉     | 293/590 [02:17<01:26,  3.43it/s] 50%|████▉     | 294/590 [02:17<01:26,  3.44it/s] 50%|█████     | 295/590 [02:18<01:25,  3.45it/s] 50%|█████     | 296/590 [02:18<01:25,  3.46it/s] 50%|█████     | 297/590 [02:18<01:24,  3.46it/s] 51%|█████     | 298/590 [02:19<01:24,  3.46it/s] 51%|█████     | 299/590 [02:19<01:24,  3.46it/s] 51%|█████     | 300/590 [02:19<01:23,  3.47it/s] 51%|█████     | 301/590 [02:19<01:23,  3.47it/s] 51%|█████     | 302/590 [02:20<01:23,  3.47it/s] 51%|█████▏    | 303/590 [02:20<01:22,  3.47it/s] 52%|█████▏    | 304/590 [02:20<01:22,  3.45it/s] 52%|█████▏    | 305/590 [02:21<01:22,  3.46it/s] 52%|█████▏    | 306/590 [02:21<01:22,  3.46it/s] 52%|█████▏    | 307/590 [02:21<01:21,  3.46it/s] 52%|█████▏    | 308/590 [02:21<01:21,  3.46it/s] 52%|█████▏    | 309/590 [02:22<01:21,  3.47it/s] 53%|█████▎    | 310/590 [02:22<01:20,  3.47it/s] 53%|█████▎    | 311/590 [02:22<01:20,  3.47it/s] 53%|█████▎    | 312/590 [02:23<01:20,  3.47it/s] 53%|█████▎    | 313/590 [02:23<01:19,  3.47it/s] 53%|█████▎    | 314/590 [02:23<01:19,  3.47it/s] 53%|█████▎    | 315/590 [02:23<01:19,  3.47it/s] 54%|█████▎    | 316/590 [02:24<01:19,  3.47it/s] 54%|█████▎    | 317/590 [02:24<01:18,  3.47it/s] 54%|█████▍    | 318/590 [02:24<01:18,  3.47it/s] 54%|█████▍    | 319/590 [02:25<01:18,  3.47it/s] 54%|█████▍    | 320/590 [02:25<01:17,  3.46it/s] 54%|█████▍    | 321/590 [02:25<01:17,  3.46it/s] 55%|█████▍    | 322/590 [02:26<01:17,  3.46it/s] 55%|█████▍    | 323/590 [02:26<01:17,  3.47it/s] 55%|█████▍    | 324/590 [02:26<01:16,  3.47it/s] 55%|█████▌    | 325/590 [02:26<01:20,  3.28it/s] 55%|█████▌    | 326/590 [02:27<01:19,  3.33it/s] 55%|█████▌    | 327/590 [02:27<01:17,  3.37it/s] 56%|█████▌    | 328/590 [02:27<01:17,  3.40it/s] 56%|█████▌    | 329/590 [02:28<01:16,  3.42it/s] 56%|█████▌    | 330/590 [02:28<01:15,  3.43it/s] 56%|█████▌    | 331/590 [02:28<01:15,  3.44it/s] 56%|█████▋    | 332/590 [02:28<01:14,  3.44it/s] 56%|█████▋    | 333/590 [02:29<01:14,  3.45it/s] 57%|█████▋    | 334/590 [02:29<01:14,  3.45it/s] 57%|█████▋    | 335/590 [02:29<01:13,  3.46it/s] 57%|█████▋    | 336/590 [02:30<01:16,  3.34it/s] 57%|█████▋    | 337/590 [02:30<01:15,  3.37it/s] 57%|█████▋    | 338/590 [02:30<01:14,  3.40it/s] 57%|█████▋    | 339/590 [02:31<01:13,  3.42it/s] 58%|█████▊    | 340/590 [02:31<01:12,  3.43it/s] 58%|█████▊    | 341/590 [02:31<01:12,  3.44it/s] 58%|█████▊    | 342/590 [02:31<01:11,  3.45it/s] 58%|█████▊    | 343/590 [02:32<01:11,  3.45it/s] 58%|█████▊    | 344/590 [02:32<01:11,  3.45it/s] 58%|█████▊    | 345/590 [02:32<01:10,  3.46it/s] 59%|█████▊    | 346/590 [02:33<01:10,  3.46it/s] 59%|█████▉    | 347/590 [02:33<01:14,  3.27it/s] 59%|█████▉    | 348/590 [02:33<01:12,  3.32it/s] 59%|█████▉    | 349/590 [02:33<01:11,  3.36it/s] 59%|█████▉    | 350/590 [02:34<01:10,  3.39it/s] 59%|█████▉    | 351/590 [02:34<01:10,  3.41it/s] 60%|█████▉    | 352/590 [02:34<01:09,  3.42it/s] 60%|█████▉    | 353/590 [02:35<01:09,  3.43it/s] 60%|██████    | 354/590 [02:35<01:00,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 17:02:31,113 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:02:31,113 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:02:31,113 >>   Batch size = 8
{'eval_loss': 0.9997642636299133, 'eval_runtime': 9.7692, 'eval_samples_per_second': 357.553, 'eval_steps_per_second': 44.732, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.55it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.33it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.51it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.45it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.80it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.46it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.33it/s][A
 10%|▉         | 42/437 [00:00<00:09, 42.16it/s][A
 11%|█         | 47/437 [00:01<00:09, 43.15it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.81it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.08it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.27it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.42it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.50it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.66it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.41it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.55it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.85it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 45.17it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 45.14it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 45.01it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 45.02it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.82it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.76it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.65it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.64it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.94it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.91it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.96it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 45.05it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.99it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.90it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.67it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.71it/s][A
 41%|████      | 177/437 [00:04<00:08, 32.16it/s][A
 42%|████▏     | 182/437 [00:04<00:07, 35.15it/s][A
 43%|████▎     | 187/437 [00:04<00:06, 37.68it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 39.68it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 41.26it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 42.38it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.23it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.88it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.82it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.95it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.10it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.42it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.58it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.87it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.99it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 45.21it/s][A
 59%|█████▉    | 257/437 [00:05<00:03, 45.04it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.77it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.65it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.56it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.62it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.79it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.90it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 45.05it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 45.19it/s][A
 69%|██████▉   | 302/437 [00:06<00:02, 45.10it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.84it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.70it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.65it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.73it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.76it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.96it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 45.10it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 45.23it/s][A
 79%|███████▉  | 347/437 [00:07<00:01, 45.12it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.75it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.69it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.72it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.67it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.78it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.84it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.94it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 45.08it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.93it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 45.10it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.92it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.92it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.80it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.90it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.90it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 45.05it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 45.12it/s][A
100%|██████████| 437/437 [00:09<00:00, 45.10it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 45.10it/s][A 60%|██████    | 354/590 [02:45<01:00,  3.90it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:02:41,022 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-28 17:02:41,044 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:02:46,263 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:02:46,292 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:02:46,301 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [03:03<34:24,  8.78s/it] 60%|██████    | 356/590 [03:04<24:20,  6.24s/it] 61%|██████    | 357/590 [03:04<17:18,  4.46s/it] 61%|██████    | 358/590 [03:04<12:23,  3.21s/it] 61%|██████    | 359/590 [03:05<08:58,  2.33s/it] 61%|██████    | 360/590 [03:05<06:35,  1.72s/it] 61%|██████    | 361/590 [03:05<04:55,  1.29s/it] 61%|██████▏   | 362/590 [03:06<03:46,  1.01it/s] 62%|██████▏   | 363/590 [03:06<02:57,  1.28it/s] 62%|██████▏   | 364/590 [03:06<02:23,  1.58it/s] 62%|██████▏   | 365/590 [03:06<01:59,  1.88it/s] 62%|██████▏   | 366/590 [03:07<01:43,  2.17it/s] 62%|██████▏   | 367/590 [03:07<01:31,  2.43it/s] 62%|██████▏   | 368/590 [03:07<01:23,  2.66it/s] 63%|██████▎   | 369/590 [03:08<01:17,  2.85it/s] 63%|██████▎   | 370/590 [03:08<01:13,  3.00it/s] 63%|██████▎   | 371/590 [03:08<01:10,  3.11it/s] 63%|██████▎   | 372/590 [03:08<01:08,  3.20it/s] 63%|██████▎   | 373/590 [03:09<01:06,  3.26it/s] 63%|██████▎   | 374/590 [03:09<01:05,  3.31it/s] 64%|██████▎   | 375/590 [03:09<01:04,  3.35it/s] 64%|██████▎   | 376/590 [03:10<01:03,  3.37it/s] 64%|██████▍   | 377/590 [03:10<01:02,  3.39it/s] 64%|██████▍   | 378/590 [03:10<01:05,  3.22it/s] 64%|██████▍   | 379/590 [03:11<01:04,  3.28it/s] 64%|██████▍   | 380/590 [03:11<01:03,  3.32it/s] 65%|██████▍   | 381/590 [03:11<01:02,  3.35it/s] 65%|██████▍   | 382/590 [03:11<01:01,  3.37it/s] 65%|██████▍   | 383/590 [03:12<01:01,  3.39it/s] 65%|██████▌   | 384/590 [03:12<01:00,  3.40it/s] 65%|██████▌   | 385/590 [03:12<01:00,  3.41it/s] 65%|██████▌   | 386/590 [03:13<00:59,  3.41it/s] 66%|██████▌   | 387/590 [03:13<00:59,  3.42it/s] 66%|██████▌   | 388/590 [03:13<00:59,  3.42it/s] 66%|██████▌   | 389/590 [03:13<00:59,  3.40it/s] 66%|██████▌   | 390/590 [03:14<00:58,  3.42it/s] 66%|██████▋   | 391/590 [03:14<00:58,  3.41it/s] 66%|██████▋   | 392/590 [03:14<00:57,  3.42it/s] 67%|██████▋   | 393/590 [03:15<00:57,  3.42it/s] 67%|██████▋   | 394/590 [03:15<00:57,  3.42it/s] 67%|██████▋   | 395/590 [03:15<00:56,  3.43it/s] 67%|██████▋   | 396/590 [03:16<00:56,  3.42it/s] 67%|██████▋   | 397/590 [03:16<00:56,  3.42it/s] 67%|██████▋   | 398/590 [03:16<00:56,  3.42it/s] 68%|██████▊   | 399/590 [03:16<00:55,  3.42it/s] 68%|██████▊   | 400/590 [03:17<00:56,  3.38it/s] 68%|██████▊   | 401/590 [03:17<00:55,  3.39it/s] 68%|██████▊   | 402/590 [03:17<00:55,  3.40it/s] 68%|██████▊   | 403/590 [03:18<00:54,  3.41it/s] 68%|██████▊   | 404/590 [03:18<00:54,  3.41it/s] 69%|██████▊   | 405/590 [03:18<00:54,  3.42it/s] 69%|██████▉   | 406/590 [03:18<00:53,  3.42it/s] 69%|██████▉   | 407/590 [03:19<00:53,  3.42it/s] 69%|██████▉   | 408/590 [03:19<00:53,  3.42it/s] 69%|██████▉   | 409/590 [03:19<00:52,  3.42it/s] 69%|██████▉   | 410/590 [03:20<00:52,  3.42it/s] 70%|██████▉   | 411/590 [03:20<00:53,  3.35it/s] 70%|██████▉   | 412/590 [03:20<00:52,  3.37it/s] 70%|███████   | 413/590 [03:21<00:52,  3.39it/s] 70%|███████   | 414/590 [03:21<00:51,  3.40it/s] 70%|███████   | 415/590 [03:21<00:51,  3.40it/s] 71%|███████   | 416/590 [03:21<00:51,  3.40it/s] 71%|███████   | 417/590 [03:22<00:50,  3.41it/s] 71%|███████   | 418/590 [03:22<00:50,  3.41it/s] 71%|███████   | 419/590 [03:22<00:50,  3.41it/s] 71%|███████   | 420/590 [03:23<00:49,  3.42it/s] 71%|███████▏  | 421/590 [03:23<00:49,  3.42it/s] 72%|███████▏  | 422/590 [03:23<00:52,  3.18it/s] 72%|███████▏  | 423/590 [03:24<00:51,  3.25it/s] 72%|███████▏  | 424/590 [03:24<00:51,  3.24it/s] 72%|███████▏  | 425/590 [03:24<00:50,  3.29it/s] 72%|███████▏  | 426/590 [03:24<00:49,  3.33it/s] 72%|███████▏  | 427/590 [03:25<00:49,  3.27it/s] 73%|███████▎  | 428/590 [03:25<00:50,  3.21it/s] 73%|███████▎  | 429/590 [03:25<00:49,  3.27it/s] 73%|███████▎  | 430/590 [03:26<00:48,  3.31it/s] 73%|███████▎  | 431/590 [03:26<00:47,  3.34it/s] 73%|███████▎  | 432/590 [03:26<00:46,  3.36it/s] 73%|███████▎  | 433/590 [03:27<00:46,  3.38it/s] 74%|███████▎  | 434/590 [03:27<00:46,  3.39it/s] 74%|███████▎  | 435/590 [03:27<00:45,  3.40it/s] 74%|███████▍  | 436/590 [03:27<00:45,  3.40it/s] 74%|███████▍  | 437/590 [03:28<00:44,  3.40it/s] 74%|███████▍  | 438/590 [03:28<00:45,  3.37it/s] 74%|███████▍  | 439/590 [03:28<00:44,  3.38it/s] 75%|███████▍  | 440/590 [03:29<00:44,  3.40it/s] 75%|███████▍  | 441/590 [03:29<00:43,  3.40it/s] 75%|███████▍  | 442/590 [03:29<00:43,  3.41it/s] 75%|███████▌  | 443/590 [03:29<00:43,  3.41it/s] 75%|███████▌  | 444/590 [03:30<00:42,  3.42it/s] 75%|███████▌  | 445/590 [03:30<00:42,  3.42it/s] 76%|███████▌  | 446/590 [03:30<00:42,  3.42it/s] 76%|███████▌  | 447/590 [03:31<00:41,  3.42it/s] 76%|███████▌  | 448/590 [03:31<00:41,  3.42it/s] 76%|███████▌  | 449/590 [03:31<00:41,  3.40it/s] 76%|███████▋  | 450/590 [03:32<00:41,  3.41it/s] 76%|███████▋  | 451/590 [03:32<00:40,  3.41it/s] 77%|███████▋  | 452/590 [03:32<00:40,  3.41it/s] 77%|███████▋  | 453/590 [03:32<00:40,  3.42it/s] 77%|███████▋  | 454/590 [03:33<00:39,  3.42it/s] 77%|███████▋  | 455/590 [03:33<00:39,  3.42it/s] 77%|███████▋  | 456/590 [03:33<00:39,  3.42it/s] 77%|███████▋  | 457/590 [03:34<00:38,  3.42it/s] 78%|███████▊  | 458/590 [03:34<00:38,  3.42it/s] 78%|███████▊  | 459/590 [03:34<00:38,  3.42it/s] 78%|███████▊  | 460/590 [03:34<00:39,  3.30it/s] 78%|███████▊  | 461/590 [03:35<00:38,  3.34it/s] 78%|███████▊  | 462/590 [03:35<00:38,  3.36it/s] 78%|███████▊  | 463/590 [03:35<00:37,  3.38it/s] 79%|███████▊  | 464/590 [03:36<00:37,  3.39it/s] 79%|███████▉  | 465/590 [03:36<00:36,  3.40it/s] 79%|███████▉  | 466/590 [03:36<00:36,  3.40it/s] 79%|███████▉  | 467/590 [03:37<00:36,  3.41it/s] 79%|███████▉  | 468/590 [03:37<00:35,  3.41it/s] 79%|███████▉  | 469/590 [03:37<00:35,  3.41it/s] 80%|███████▉  | 470/590 [03:37<00:35,  3.41it/s] 80%|███████▉  | 471/590 [03:38<00:36,  3.30it/s] 80%|████████  | 472/590 [03:38<00:31,  3.70it/s][INFO|trainer.py:2140] 2023-08-28 17:03:34,242 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:03:34,242 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:03:34,242 >>   Batch size = 8
{'eval_loss': 1.011512041091919, 'eval_runtime': 9.8908, 'eval_samples_per_second': 353.156, 'eval_steps_per_second': 44.182, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.57it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.17it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.45it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.38it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.75it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.41it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.10it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.83it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.67it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.10it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.18it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.28it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 45.20it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 45.12it/s][A
 18%|█▊        | 77/437 [00:01<00:07, 45.03it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.86it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.74it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.72it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.80it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.90it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.98it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.98it/s][A
 27%|██▋       | 117/437 [00:02<00:13, 23.22it/s][A
 28%|██▊       | 122/437 [00:03<00:11, 27.23it/s][A
 29%|██▉       | 127/437 [00:03<00:10, 30.92it/s][A
 30%|███       | 132/437 [00:03<00:08, 34.18it/s][A
 31%|███▏      | 137/437 [00:03<00:08, 36.87it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 39.17it/s][A
 34%|███▎      | 147/437 [00:03<00:07, 40.81it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 42.11it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 42.66it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 42.96it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 43.41it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 43.84it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.30it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.56it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.79it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 45.01it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 45.10it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.82it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.57it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 44.45it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.50it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.63it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.65it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 45.07it/s][A
 54%|█████▍    | 237/437 [00:05<00:06, 33.07it/s][A
 55%|█████▌    | 242/437 [00:05<00:05, 36.06it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 38.45it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 40.28it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 41.70it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 42.64it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.47it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.94it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.89it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.91it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.23it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.56it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.79it/s][A
 69%|██████▉   | 302/437 [00:07<00:02, 45.01it/s][A
 70%|███████   | 307/437 [00:07<00:02, 45.22it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 45.18it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.98it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.83it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.54it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.64it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.73it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 44.92it/s][A
 79%|███████▉  | 347/437 [00:08<00:01, 45.24it/s][A
 81%|████████  | 352/437 [00:08<00:01, 45.30it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 45.32it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.98it/s][A
 84%|████████▍ | 367/437 [00:08<00:02, 25.98it/s][A
 85%|████████▌ | 372/437 [00:08<00:02, 29.86it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 33.23it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 36.14it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 38.53it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 40.35it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 41.74it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 42.71it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 42.87it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.24it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.84it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 44.22it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.67it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.93it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.97it/s][A                                                 
                                                 [A 80%|████████  | 472/590 [03:48<00:31,  3.70it/s]
100%|██████████| 437/437 [00:10<00:00, 44.97it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:03:45,181 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-28 17:03:46,288 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:03:49,666 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:03:49,682 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:03:49,688 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [04:02<14:10,  7.27s/it] 80%|████████  | 474/590 [04:02<10:00,  5.18s/it] 81%|████████  | 475/590 [04:02<07:06,  3.71s/it] 81%|████████  | 476/590 [04:02<05:06,  2.69s/it] 81%|████████  | 477/590 [04:03<03:42,  1.97s/it] 81%|████████  | 478/590 [04:03<02:44,  1.47s/it] 81%|████████  | 479/590 [04:03<02:03,  1.11s/it] 81%|████████▏ | 480/590 [04:04<01:35,  1.15it/s] 82%|████████▏ | 481/590 [04:04<01:15,  1.44it/s] 82%|████████▏ | 482/590 [04:04<01:01,  1.74it/s] 82%|████████▏ | 483/590 [04:04<00:52,  2.04it/s] 82%|████████▏ | 484/590 [04:05<00:45,  2.33it/s] 82%|████████▏ | 485/590 [04:05<00:42,  2.46it/s] 82%|████████▏ | 486/590 [04:05<00:38,  2.69it/s] 83%|████████▎ | 487/590 [04:06<00:35,  2.87it/s] 83%|████████▎ | 488/590 [04:06<00:33,  3.02it/s] 83%|████████▎ | 489/590 [04:06<00:32,  3.13it/s] 83%|████████▎ | 490/590 [04:07<00:31,  3.21it/s] 83%|████████▎ | 491/590 [04:07<00:30,  3.27it/s] 83%|████████▎ | 492/590 [04:07<00:29,  3.32it/s] 84%|████████▎ | 493/590 [04:07<00:28,  3.35it/s] 84%|████████▎ | 494/590 [04:08<00:28,  3.38it/s] 84%|████████▍ | 495/590 [04:08<00:27,  3.40it/s] 84%|████████▍ | 496/590 [04:08<00:27,  3.39it/s] 84%|████████▍ | 497/590 [04:09<00:27,  3.42it/s] 84%|████████▍ | 498/590 [04:09<00:26,  3.43it/s] 85%|████████▍ | 499/590 [04:09<00:26,  3.44it/s] 85%|████████▍ | 500/590 [04:09<00:26,  3.45it/s]                                                  85%|████████▍ | 500/590 [04:09<00:26,  3.45it/s] 85%|████████▍ | 501/590 [04:10<00:25,  3.46it/s] 85%|████████▌ | 502/590 [04:10<00:25,  3.46it/s] 85%|████████▌ | 503/590 [04:10<00:25,  3.46it/s] 85%|████████▌ | 504/590 [04:11<00:24,  3.47it/s] 86%|████████▌ | 505/590 [04:11<00:24,  3.47it/s] 86%|████████▌ | 506/590 [04:11<00:24,  3.47it/s] 86%|████████▌ | 507/590 [04:11<00:24,  3.44it/s] 86%|████████▌ | 508/590 [04:12<00:23,  3.45it/s] 86%|████████▋ | 509/590 [04:12<00:23,  3.45it/s] 86%|████████▋ | 510/590 [04:12<00:23,  3.46it/s] 87%|████████▋ | 511/590 [04:13<00:22,  3.46it/s] 87%|████████▋ | 512/590 [04:13<00:22,  3.46it/s] 87%|████████▋ | 513/590 [04:13<00:22,  3.47it/s] 87%|████████▋ | 514/590 [04:13<00:21,  3.47it/s] 87%|████████▋ | 515/590 [04:14<00:21,  3.47it/s] 87%|████████▋ | 516/590 [04:14<00:21,  3.47it/s] 88%|████████▊ | 517/590 [04:14<00:21,  3.47it/s] 88%|████████▊ | 518/590 [04:15<00:20,  3.44it/s] 88%|████████▊ | 519/590 [04:15<00:20,  3.45it/s] 88%|████████▊ | 520/590 [04:15<00:20,  3.46it/s] 88%|████████▊ | 521/590 [04:16<00:19,  3.46it/s] 88%|████████▊ | 522/590 [04:16<00:19,  3.46it/s] 89%|████████▊ | 523/590 [04:16<00:19,  3.46it/s] 89%|████████▉ | 524/590 [04:16<00:19,  3.46it/s] 89%|████████▉ | 525/590 [04:17<00:18,  3.46it/s] 89%|████████▉ | 526/590 [04:17<00:18,  3.46it/s] 89%|████████▉ | 527/590 [04:17<00:18,  3.47it/s] 89%|████████▉ | 528/590 [04:18<00:17,  3.47it/s] 90%|████████▉ | 529/590 [04:18<00:18,  3.34it/s] 90%|████████▉ | 530/590 [04:18<00:17,  3.38it/s] 90%|█████████ | 531/590 [04:18<00:17,  3.40it/s] 90%|█████████ | 532/590 [04:19<00:16,  3.42it/s] 90%|█████████ | 533/590 [04:19<00:16,  3.44it/s] 91%|█████████ | 534/590 [04:19<00:16,  3.45it/s] 91%|█████████ | 535/590 [04:20<00:15,  3.45it/s] 91%|█████████ | 536/590 [04:20<00:15,  3.46it/s] 91%|█████████ | 537/590 [04:20<00:15,  3.46it/s] 91%|█████████ | 538/590 [04:20<00:15,  3.46it/s] 91%|█████████▏| 539/590 [04:21<00:14,  3.46it/s] 92%|█████████▏| 540/590 [04:21<00:15,  3.28it/s] 92%|█████████▏| 541/590 [04:21<00:14,  3.33it/s] 92%|█████████▏| 542/590 [04:22<00:14,  3.37it/s] 92%|█████████▏| 543/590 [04:22<00:13,  3.40it/s] 92%|█████████▏| 544/590 [04:22<00:13,  3.42it/s] 92%|█████████▏| 545/590 [04:23<00:13,  3.43it/s] 93%|█████████▎| 546/590 [04:23<00:12,  3.44it/s] 93%|█████████▎| 547/590 [04:23<00:12,  3.45it/s] 93%|█████████▎| 548/590 [04:23<00:12,  3.45it/s] 93%|█████████▎| 549/590 [04:24<00:11,  3.46it/s] 93%|█████████▎| 550/590 [04:24<00:12,  3.14it/s] 93%|█████████▎| 551/590 [04:24<00:13,  2.95it/s] 94%|█████████▎| 552/590 [04:25<00:12,  3.09it/s] 94%|█████████▎| 553/590 [04:25<00:13,  2.72it/s] 94%|█████████▍| 554/590 [04:26<00:12,  2.90it/s] 94%|█████████▍| 555/590 [04:26<00:11,  3.05it/s] 94%|█████████▍| 556/590 [04:26<00:10,  3.16it/s] 94%|█████████▍| 557/590 [04:26<00:10,  3.25it/s] 95%|█████████▍| 558/590 [04:27<00:09,  3.31it/s] 95%|█████████▍| 559/590 [04:27<00:09,  3.36it/s] 95%|█████████▍| 560/590 [04:27<00:08,  3.39it/s] 95%|█████████▌| 561/590 [04:28<00:08,  3.41it/s] 95%|█████████▌| 562/590 [04:28<00:08,  3.43it/s] 95%|█████████▌| 563/590 [04:28<00:07,  3.44it/s] 96%|█████████▌| 564/590 [04:28<00:07,  3.45it/s] 96%|█████████▌| 565/590 [04:29<00:07,  3.45it/s] 96%|█████████▌| 566/590 [04:29<00:06,  3.46it/s] 96%|█████████▌| 567/590 [04:29<00:06,  3.44it/s] 96%|█████████▋| 568/590 [04:30<00:06,  3.44it/s] 96%|█████████▋| 569/590 [04:30<00:06,  3.45it/s] 97%|█████████▋| 570/590 [04:30<00:05,  3.45it/s] 97%|█████████▋| 571/590 [04:30<00:05,  3.46it/s] 97%|█████████▋| 572/590 [04:31<00:05,  3.46it/s] 97%|█████████▋| 573/590 [04:31<00:04,  3.46it/s] 97%|█████████▋| 574/590 [04:31<00:04,  3.46it/s] 97%|█████████▋| 575/590 [04:32<00:04,  3.46it/s] 98%|█████████▊| 576/590 [04:32<00:04,  3.46it/s] 98%|█████████▊| 577/590 [04:32<00:03,  3.46it/s] 98%|█████████▊| 578/590 [04:32<00:03,  3.45it/s] 98%|█████████▊| 579/590 [04:33<00:03,  3.45it/s] 98%|█████████▊| 580/590 [04:33<00:02,  3.46it/s] 98%|█████████▊| 581/590 [04:33<00:02,  3.46it/s] 99%|█████████▊| 582/590 [04:34<00:02,  3.46it/s] 99%|█████████▉| 583/590 [04:34<00:02,  3.46it/s] 99%|█████████▉| 584/590 [04:34<00:01,  3.46it/s] 99%|█████████▉| 585/590 [04:34<00:01,  3.46it/s] 99%|█████████▉| 586/590 [04:35<00:01,  3.46it/s] 99%|█████████▉| 587/590 [04:35<00:00,  3.47it/s]100%|█████████▉| 588/590 [04:35<00:00,  3.46it/s]100%|█████████▉| 589/590 [04:36<00:00,  3.44it/s]100%|██████████| 590/590 [04:36<00:00,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 17:04:32,140 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:04:32,140 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:04:32,140 >>   Batch size = 8
{'eval_loss': 1.02096426486969, 'eval_runtime': 10.456, 'eval_samples_per_second': 334.068, 'eval_steps_per_second': 41.794, 'epoch': 4.0}
{'loss': 0.7266, 'learning_rate': 5.783898305084746e-06, 'epoch': 4.24}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.07it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.63it/s][A
  4%|▍         | 18/437 [00:00<00:08, 47.66it/s][A
  5%|▌         | 23/437 [00:00<00:08, 46.44it/s][A
  6%|▋         | 28/437 [00:00<00:08, 45.79it/s][A
  8%|▊         | 33/437 [00:00<00:08, 45.41it/s][A
  9%|▊         | 38/437 [00:00<00:08, 45.14it/s][A
 10%|▉         | 43/437 [00:00<00:08, 44.88it/s][A
 11%|█         | 48/437 [00:01<00:08, 44.94it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 45.06it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 45.15it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 45.23it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 45.30it/s][A
 17%|█▋        | 73/437 [00:01<00:08, 45.05it/s][A
 18%|█▊        | 78/437 [00:01<00:07, 44.98it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 44.87it/s][A
 20%|██        | 88/437 [00:01<00:07, 44.80it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 44.87it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 44.94it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 45.06it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 45.21it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 45.10it/s][A
 27%|██▋       | 118/437 [00:02<00:07, 44.95it/s][A
 28%|██▊       | 123/437 [00:02<00:06, 44.86it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 44.75it/s][A
 30%|███       | 133/437 [00:02<00:06, 44.69it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 44.77it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 44.88it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 45.11it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 45.20it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 45.23it/s][A
 37%|███▋      | 163/437 [00:03<00:06, 44.94it/s][A
 38%|███▊      | 168/437 [00:03<00:05, 44.84it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 44.72it/s][A
 41%|████      | 178/437 [00:03<00:05, 44.71it/s][A
 42%|████▏     | 183/437 [00:04<00:05, 44.78it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 44.80it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 45.01it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 44.99it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 45.00it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 44.85it/s][A
 49%|████▊     | 213/437 [00:04<00:05, 44.74it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 44.73it/s][A
 51%|█████     | 223/437 [00:04<00:04, 44.50it/s][A
 52%|█████▏    | 228/437 [00:05<00:04, 44.65it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 44.81it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 44.98it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 45.07it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 45.10it/s][A
 58%|█████▊    | 253/437 [00:05<00:04, 45.08it/s][A
 59%|█████▉    | 258/437 [00:05<00:03, 44.95it/s][A
 60%|██████    | 263/437 [00:05<00:03, 44.83it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 44.64it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 44.68it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 44.81it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 44.89it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 45.08it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 45.11it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 45.16it/s][A
 69%|██████▉   | 303/437 [00:06<00:02, 45.00it/s][A
 70%|███████   | 308/437 [00:06<00:02, 44.74it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 44.71it/s][A
 73%|███████▎  | 318/437 [00:07<00:02, 44.70it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 44.86it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 44.90it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 44.99it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 45.05it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 45.16it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 44.99it/s][A
 81%|████████  | 353/437 [00:07<00:01, 44.72it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 44.68it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 44.73it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 44.83it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 44.88it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 45.02it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 45.10it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 45.11it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 44.94it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 44.77it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 44.67it/s][A
 93%|█████████▎| 408/437 [00:09<00:00, 44.65it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 44.73it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 44.83it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 44.90it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 45.00it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 44.99it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.99it/s][A100%|██████████| 590/590 [04:46<00:00,  3.86it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:04:41,917 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-28 17:04:42,026 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:04:46,009 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:04:46,035 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:04:46,059 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:05:01,037 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:05:01,178 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-118 (score: 0.9916616678237915).
                                                 100%|██████████| 590/590 [05:18<00:00,  3.86it/s]100%|██████████| 590/590 [05:18<00:00,  1.85it/s]
[INFO|trainer.py:1894] 2023-08-28 17:05:14,269 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 17:05:14,424 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:05:20,631 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:05:21,336 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:05:21,438 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:05:22,626 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:22,626 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:22,626 >>   train_loss               =     0.7219
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:22,626 >>   train_runtime            = 0:05:18.43
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:22,626 >>   train_samples            =       7521
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:22,626 >>   train_samples_per_second =    118.094
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:22,626 >>   train_steps_per_second   =      1.853
{'eval_loss': 1.022419810295105, 'eval_runtime': 9.7204, 'eval_samples_per_second': 359.347, 'eval_steps_per_second': 44.957, 'epoch': 5.0}
{'train_runtime': 318.4328, 'train_samples_per_second': 118.094, 'train_steps_per_second': 1.853, 'train_loss': 0.7219002545890162, 'epoch': 5.0}
08/28/2023 17:05:23 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:05:23,061 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:05:23,062 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 17:05:23,062 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 56.38it/s]  3%|▎         | 12/437 [00:00<00:08, 49.79it/s]  4%|▍         | 18/437 [00:00<00:08, 47.79it/s]  5%|▌         | 23/437 [00:00<00:08, 47.11it/s]  6%|▋         | 28/437 [00:00<00:08, 46.55it/s]  8%|▊         | 33/437 [00:00<00:10, 40.00it/s]  9%|▊         | 38/437 [00:00<00:09, 41.68it/s] 10%|▉         | 43/437 [00:00<00:09, 42.71it/s] 11%|█         | 48/437 [00:01<00:08, 43.56it/s] 12%|█▏        | 53/437 [00:01<00:08, 44.23it/s] 13%|█▎        | 58/437 [00:01<00:08, 44.71it/s] 14%|█▍        | 63/437 [00:01<00:08, 44.88it/s] 16%|█▌        | 68/437 [00:01<00:08, 45.03it/s] 17%|█▋        | 73/437 [00:01<00:08, 44.78it/s] 18%|█▊        | 78/437 [00:01<00:08, 44.64it/s] 19%|█▉        | 83/437 [00:01<00:07, 44.78it/s] 20%|██        | 88/437 [00:01<00:07, 44.97it/s] 21%|██▏       | 93/437 [00:02<00:07, 45.15it/s] 22%|██▏       | 98/437 [00:02<00:07, 45.41it/s] 24%|██▎       | 103/437 [00:02<00:07, 45.40it/s] 25%|██▍       | 108/437 [00:02<00:07, 45.51it/s] 26%|██▌       | 113/437 [00:02<00:07, 45.38it/s] 27%|██▋       | 118/437 [00:02<00:07, 45.08it/s] 28%|██▊       | 123/437 [00:02<00:06, 44.96it/s] 29%|██▉       | 128/437 [00:02<00:06, 44.91it/s] 30%|███       | 133/437 [00:02<00:06, 45.04it/s] 32%|███▏      | 138/437 [00:03<00:06, 45.22it/s] 33%|███▎      | 143/437 [00:03<00:06, 45.37it/s] 34%|███▍      | 148/437 [00:03<00:06, 45.47it/s] 35%|███▌      | 153/437 [00:03<00:06, 45.53it/s] 36%|███▌      | 158/437 [00:03<00:06, 45.37it/s] 37%|███▋      | 163/437 [00:03<00:06, 45.20it/s] 38%|███▊      | 168/437 [00:03<00:06, 38.49it/s] 40%|███▉      | 173/437 [00:03<00:06, 40.45it/s] 41%|████      | 178/437 [00:04<00:06, 41.84it/s] 42%|████▏     | 183/437 [00:04<00:05, 42.98it/s] 43%|████▎     | 188/437 [00:04<00:05, 43.74it/s] 44%|████▍     | 193/437 [00:04<00:05, 44.26it/s] 45%|████▌     | 198/437 [00:04<00:05, 44.57it/s] 46%|████▋     | 203/437 [00:04<00:05, 44.88it/s] 48%|████▊     | 208/437 [00:04<00:05, 44.66it/s] 49%|████▊     | 213/437 [00:04<00:05, 44.46it/s] 50%|████▉     | 218/437 [00:04<00:04, 44.69it/s] 51%|█████     | 223/437 [00:05<00:04, 44.91it/s] 52%|█████▏    | 228/437 [00:05<00:04, 45.16it/s] 53%|█████▎    | 233/437 [00:05<00:04, 45.30it/s] 54%|█████▍    | 238/437 [00:05<00:04, 45.40it/s] 56%|█████▌    | 243/437 [00:05<00:04, 45.26it/s] 57%|█████▋    | 248/437 [00:05<00:04, 45.19it/s] 58%|█████▊    | 253/437 [00:05<00:04, 45.00it/s] 59%|█████▉    | 258/437 [00:05<00:03, 44.87it/s] 60%|██████    | 263/437 [00:05<00:03, 44.84it/s] 61%|██████▏   | 268/437 [00:06<00:03, 45.01it/s] 62%|██████▏   | 273/437 [00:06<00:03, 45.14it/s] 64%|██████▎   | 278/437 [00:06<00:03, 45.35it/s] 65%|██████▍   | 283/437 [00:06<00:03, 45.49it/s] 66%|██████▌   | 288/437 [00:06<00:03, 45.45it/s] 67%|██████▋   | 293/437 [00:06<00:03, 45.33it/s] 68%|██████▊   | 298/437 [00:06<00:03, 45.06it/s] 69%|██████▉   | 303/437 [00:06<00:03, 43.94it/s] 70%|███████   | 308/437 [00:06<00:02, 44.36it/s] 72%|███████▏  | 313/437 [00:07<00:02, 44.62it/s] 73%|███████▎  | 318/437 [00:07<00:02, 44.93it/s] 74%|███████▍  | 323/437 [00:07<00:02, 45.14it/s] 75%|███████▌  | 328/437 [00:07<00:02, 45.22it/s] 76%|███████▌  | 333/437 [00:07<00:02, 45.28it/s] 77%|███████▋  | 338/437 [00:07<00:02, 45.13it/s] 78%|███████▊  | 343/437 [00:07<00:02, 44.99it/s] 80%|███████▉  | 348/437 [00:07<00:01, 44.89it/s] 81%|████████  | 353/437 [00:07<00:01, 44.97it/s] 82%|████████▏ | 358/437 [00:08<00:01, 45.05it/s] 83%|████████▎ | 363/437 [00:08<00:01, 45.13it/s] 84%|████████▍ | 368/437 [00:08<00:01, 45.27it/s] 85%|████████▌ | 373/437 [00:08<00:01, 45.41it/s] 86%|████████▋ | 378/437 [00:08<00:01, 45.37it/s] 88%|████████▊ | 383/437 [00:08<00:01, 45.23it/s] 89%|████████▉ | 388/437 [00:08<00:01, 45.12it/s] 90%|████████▉ | 393/437 [00:08<00:00, 44.95it/s] 91%|█████████ | 398/437 [00:08<00:00, 45.02it/s] 92%|█████████▏| 403/437 [00:09<00:00, 45.08it/s] 93%|█████████▎| 408/437 [00:09<00:00, 45.20it/s] 95%|█████████▍| 413/437 [00:09<00:00, 45.17it/s] 96%|█████████▌| 418/437 [00:09<00:00, 45.32it/s] 97%|█████████▋| 423/437 [00:09<00:00, 45.40it/s] 98%|█████████▊| 428/437 [00:09<00:00, 45.30it/s] 99%|█████████▉| 433/437 [00:09<00:00, 45.22it/s]100%|██████████| 437/437 [00:09<00:00, 44.78it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:05:32,882 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:32,883 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:32,883 >>   eval_loss               =     0.9917
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:32,883 >>   eval_runtime            = 0:00:09.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:32,883 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:32,883 >>   eval_samples_per_second =    357.189
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:32,883 >>   eval_steps_per_second   =     44.687
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:05:32,883 >>   perplexity              =     2.6957
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:39,021 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:39,112 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:39,113 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:39,113 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:39,113 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:05:39,690 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:05:39,691 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:05:39,950 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:05:40,994 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:05:40,994 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:42,333 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:42,338 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:42,338 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:42,338 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:05:42,338 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:05:42,690 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:05:42,692 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:05:42,943 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:05:43,079 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:05:43,079 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-118
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-236
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-354
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-590
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/checkpoint-472
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.44it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.54it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.57it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.59it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:09,  1.60it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.62it/s]Extractor Predicting: 19it [00:12,  1.59it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:14,  1.61it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:15,  1.62it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:19,  1.65it/s]Extractor Predicting: 32it [00:20,  1.64it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:23,  1.50it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:26,  1.63it/s]Extractor Predicting: 43it [00:27,  1.60it/s]Extractor Predicting: 44it [00:27,  1.64it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:30,  1.64it/s]Extractor Predicting: 49it [00:30,  1.66it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:32,  1.65it/s]Extractor Predicting: 53it [00:33,  1.68it/s]Extractor Predicting: 54it [00:33,  1.68it/s]Extractor Predicting: 55it [00:34,  1.68it/s]Extractor Predicting: 56it [00:35,  1.65it/s]Extractor Predicting: 57it [00:35,  1.67it/s]Extractor Predicting: 58it [00:36,  1.63it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:38,  1.59it/s]Extractor Predicting: 63it [00:39,  1.59it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:41,  1.58it/s]Extractor Predicting: 67it [00:41,  1.57it/s]Extractor Predicting: 68it [00:42,  1.54it/s]Extractor Predicting: 69it [00:43,  1.56it/s]Extractor Predicting: 70it [00:43,  1.57it/s]Extractor Predicting: 71it [00:44,  1.58it/s]Extractor Predicting: 72it [00:45,  1.62it/s]Extractor Predicting: 73it [00:45,  1.60it/s]Extractor Predicting: 74it [00:46,  1.58it/s]Extractor Predicting: 75it [00:46,  1.62it/s]Extractor Predicting: 76it [00:47,  1.58it/s]Extractor Predicting: 77it [00:48,  1.57it/s]Extractor Predicting: 78it [00:48,  1.55it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:50,  1.56it/s]Extractor Predicting: 81it [00:50,  1.55it/s]Extractor Predicting: 82it [00:51,  1.56it/s]Extractor Predicting: 83it [00:52,  1.60it/s]Extractor Predicting: 84it [00:52,  1.60it/s]Extractor Predicting: 85it [00:53,  1.60it/s]Extractor Predicting: 86it [00:54,  1.50it/s]Extractor Predicting: 87it [00:54,  1.56it/s]Extractor Predicting: 88it [00:55,  1.56it/s]Extractor Predicting: 89it [00:55,  1.56it/s]Extractor Predicting: 90it [00:56,  1.53it/s]Extractor Predicting: 91it [00:57,  1.48it/s]Extractor Predicting: 92it [00:58,  1.51it/s]Extractor Predicting: 93it [00:58,  1.53it/s]Extractor Predicting: 94it [00:59,  1.56it/s]Extractor Predicting: 95it [00:59,  1.60it/s]Extractor Predicting: 96it [01:00,  1.61it/s]Extractor Predicting: 97it [01:01,  1.59it/s]Extractor Predicting: 98it [01:01,  1.60it/s]Extractor Predicting: 99it [01:02,  1.59it/s]Extractor Predicting: 100it [01:02,  1.61it/s]Extractor Predicting: 101it [01:03,  1.60it/s]Extractor Predicting: 102it [01:04,  1.59it/s]Extractor Predicting: 103it [01:04,  1.58it/s]Extractor Predicting: 104it [01:05,  1.45it/s]Extractor Predicting: 105it [01:06,  1.49it/s]Extractor Predicting: 106it [01:07,  1.49it/s]Extractor Predicting: 107it [01:07,  1.51it/s]Extractor Predicting: 108it [01:08,  1.52it/s]Extractor Predicting: 109it [01:08,  1.53it/s]Extractor Predicting: 110it [01:09,  1.53it/s]Extractor Predicting: 111it [01:10,  1.53it/s]Extractor Predicting: 112it [01:10,  1.54it/s]Extractor Predicting: 113it [01:11,  1.52it/s]Extractor Predicting: 114it [01:12,  1.51it/s]Extractor Predicting: 115it [01:12,  1.50it/s]Extractor Predicting: 116it [01:13,  1.51it/s]Extractor Predicting: 117it [01:14,  1.51it/s]Extractor Predicting: 118it [01:14,  1.51it/s]Extractor Predicting: 119it [01:15,  1.50it/s]Extractor Predicting: 120it [01:16,  1.52it/s]Extractor Predicting: 121it [01:16,  1.51it/s]Extractor Predicting: 122it [01:17,  1.51it/s]Extractor Predicting: 123it [01:18,  1.56it/s]Extractor Predicting: 124it [01:18,  1.55it/s]Extractor Predicting: 125it [01:19,  1.53it/s]Extractor Predicting: 126it [01:20,  1.53it/s]Extractor Predicting: 127it [01:20,  1.54it/s]Extractor Predicting: 128it [01:21,  1.54it/s]Extractor Predicting: 129it [01:22,  1.50it/s]Extractor Predicting: 130it [01:22,  1.50it/s]Extractor Predicting: 131it [01:23,  1.48it/s]Extractor Predicting: 132it [01:24,  1.47it/s]Extractor Predicting: 133it [01:24,  1.47it/s]Extractor Predicting: 134it [01:25,  1.43it/s]Extractor Predicting: 135it [01:26,  1.46it/s]Extractor Predicting: 136it [01:26,  1.79it/s]Extractor Predicting: 136it [01:26,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:18,854 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:18,863 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:18,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:18,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:18,864 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:07:19,457 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:07:19,458 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:07:20,033 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:07:21,077 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:07:21,077 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:24,025 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:24,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:24,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:24,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:07:24,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:07:24,665 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:07:24,666 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:07:25,254 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:07:25,416 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:07:25,416 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6558044806517311,
  "recall": 0.09218436873747494,
  "score": 0.16164658634538154,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.61it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:14,  1.66it/s]Extractor Predicting: 25it [00:15,  1.66it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:18,  1.64it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:19,  1.72it/s]Extractor Predicting: 34it [00:20,  1.72it/s]Extractor Predicting: 35it [00:21,  1.69it/s]Extractor Predicting: 36it [00:21,  1.67it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:23,  1.65it/s]Extractor Predicting: 40it [00:24,  1.71it/s]Extractor Predicting: 41it [00:24,  1.70it/s]Extractor Predicting: 42it [00:25,  1.71it/s]Extractor Predicting: 43it [00:25,  1.74it/s]Extractor Predicting: 44it [00:26,  1.62it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:27,  1.66it/s]Extractor Predicting: 47it [00:28,  1.69it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:30,  1.75it/s]Extractor Predicting: 52it [00:31,  1.77it/s]Extractor Predicting: 53it [00:31,  1.72it/s]Extractor Predicting: 54it [00:32,  1.70it/s]Extractor Predicting: 55it [00:33,  1.72it/s]Extractor Predicting: 56it [00:33,  1.67it/s]Extractor Predicting: 57it [00:34,  1.67it/s]Extractor Predicting: 58it [00:34,  1.69it/s]Extractor Predicting: 59it [00:35,  1.67it/s]Extractor Predicting: 60it [00:36,  1.66it/s]Extractor Predicting: 61it [00:36,  1.68it/s]Extractor Predicting: 62it [00:37,  1.68it/s]Extractor Predicting: 63it [00:37,  1.65it/s]Extractor Predicting: 64it [00:38,  1.71it/s]Extractor Predicting: 65it [00:38,  1.69it/s]Extractor Predicting: 66it [00:39,  1.67it/s]Extractor Predicting: 67it [00:40,  1.69it/s]Extractor Predicting: 68it [00:40,  1.74it/s]Extractor Predicting: 69it [00:41,  1.75it/s]Extractor Predicting: 70it [00:41,  1.73it/s]Extractor Predicting: 71it [00:42,  1.73it/s]Extractor Predicting: 72it [00:43,  1.70it/s]Extractor Predicting: 73it [00:43,  1.72it/s]Extractor Predicting: 74it [00:44,  1.68it/s]Extractor Predicting: 75it [00:44,  1.66it/s]Extractor Predicting: 76it [00:45,  1.68it/s]Extractor Predicting: 77it [00:46,  1.67it/s]Extractor Predicting: 78it [00:46,  1.68it/s]Extractor Predicting: 79it [00:47,  1.68it/s]Extractor Predicting: 80it [00:47,  1.70it/s]Extractor Predicting: 81it [00:48,  1.69it/s]Extractor Predicting: 82it [00:49,  1.70it/s]Extractor Predicting: 83it [00:49,  1.66it/s]Extractor Predicting: 84it [00:50,  1.66it/s]Extractor Predicting: 85it [00:50,  1.64it/s]Extractor Predicting: 86it [00:51,  1.58it/s]Extractor Predicting: 87it [00:52,  1.65it/s]Extractor Predicting: 88it [00:52,  1.61it/s]Extractor Predicting: 89it [00:53,  1.60it/s]Extractor Predicting: 90it [00:54,  1.58it/s]Extractor Predicting: 91it [00:54,  1.58it/s]Extractor Predicting: 92it [00:55,  1.57it/s]Extractor Predicting: 93it [00:55,  1.55it/s]Extractor Predicting: 94it [00:56,  1.56it/s]Extractor Predicting: 95it [00:57,  1.60it/s]Extractor Predicting: 96it [00:57,  1.56it/s]Extractor Predicting: 97it [00:58,  1.55it/s]Extractor Predicting: 98it [00:59,  1.56it/s]Extractor Predicting: 99it [00:59,  1.54it/s]Extractor Predicting: 100it [01:00,  1.54it/s]Extractor Predicting: 101it [01:01,  1.53it/s]Extractor Predicting: 102it [01:01,  1.52it/s]Extractor Predicting: 103it [01:02,  1.56it/s]Extractor Predicting: 104it [01:03,  1.57it/s]Extractor Predicting: 105it [01:03,  1.58it/s]Extractor Predicting: 106it [01:04,  1.55it/s]Extractor Predicting: 107it [01:05,  1.54it/s]Extractor Predicting: 108it [01:05,  1.52it/s]Extractor Predicting: 109it [01:06,  1.38it/s]Extractor Predicting: 110it [01:07,  1.42it/s]Extractor Predicting: 111it [01:07,  1.46it/s]Extractor Predicting: 112it [01:08,  1.47it/s]Extractor Predicting: 113it [01:09,  1.48it/s]Extractor Predicting: 114it [01:09,  1.49it/s]Extractor Predicting: 115it [01:10,  1.49it/s]Extractor Predicting: 116it [01:11,  1.51it/s]Extractor Predicting: 117it [01:11,  1.45it/s]Extractor Predicting: 118it [01:12,  1.48it/s]Extractor Predicting: 119it [01:13,  1.52it/s]Extractor Predicting: 120it [01:13,  1.58it/s]Extractor Predicting: 121it [01:14,  1.57it/s]Extractor Predicting: 122it [01:15,  1.40it/s]Extractor Predicting: 123it [01:15,  1.45it/s]Extractor Predicting: 124it [01:16,  1.47it/s]Extractor Predicting: 125it [01:17,  1.52it/s]Extractor Predicting: 126it [01:17,  1.58it/s]Extractor Predicting: 127it [01:18,  1.50it/s]Extractor Predicting: 128it [01:19,  1.55it/s]Extractor Predicting: 129it [01:19,  1.55it/s]Extractor Predicting: 130it [01:20,  1.54it/s]Extractor Predicting: 131it [01:21,  1.56it/s]Extractor Predicting: 132it [01:21,  1.57it/s]Extractor Predicting: 133it [01:22,  1.62it/s]Extractor Predicting: 134it [01:22,  1.65it/s]Extractor Predicting: 135it [01:23,  1.62it/s]Extractor Predicting: 136it [01:24,  1.65it/s]Extractor Predicting: 137it [01:24,  1.66it/s]Extractor Predicting: 138it [01:25,  1.63it/s]Extractor Predicting: 139it [01:25,  1.64it/s]Extractor Predicting: 140it [01:26,  1.64it/s]Extractor Predicting: 141it [01:27,  1.59it/s]Extractor Predicting: 142it [01:27,  1.58it/s]Extractor Predicting: 143it [01:28,  1.58it/s]Extractor Predicting: 144it [01:29,  1.58it/s]Extractor Predicting: 145it [01:29,  1.57it/s]Extractor Predicting: 146it [01:30,  1.61it/s]Extractor Predicting: 147it [01:30,  1.63it/s]Extractor Predicting: 148it [01:31,  1.60it/s]Extractor Predicting: 149it [01:32,  1.61it/s]Extractor Predicting: 150it [01:32,  1.60it/s]Extractor Predicting: 151it [01:33,  1.56it/s]Extractor Predicting: 152it [01:34,  1.57it/s]Extractor Predicting: 153it [01:34,  1.58it/s]Extractor Predicting: 154it [01:35,  1.61it/s]Extractor Predicting: 155it [01:35,  1.58it/s]Extractor Predicting: 156it [01:36,  1.58it/s]Extractor Predicting: 157it [01:37,  1.56it/s]Extractor Predicting: 158it [01:37,  1.57it/s]Extractor Predicting: 159it [01:38,  1.55it/s]Extractor Predicting: 160it [01:39,  1.54it/s]Extractor Predicting: 161it [01:39,  1.55it/s]Extractor Predicting: 162it [01:40,  1.55it/s]Extractor Predicting: 163it [01:41,  1.54it/s]Extractor Predicting: 164it [01:41,  1.56it/s]Extractor Predicting: 165it [01:42,  1.55it/s]Extractor Predicting: 166it [01:43,  1.57it/s]Extractor Predicting: 167it [01:43,  1.55it/s]Extractor Predicting: 168it [01:44,  1.54it/s]Extractor Predicting: 169it [01:45,  1.53it/s]Extractor Predicting: 170it [01:45,  1.53it/s]Extractor Predicting: 171it [01:46,  1.53it/s]Extractor Predicting: 172it [01:47,  1.54it/s]Extractor Predicting: 173it [01:47,  1.53it/s]Extractor Predicting: 174it [01:48,  1.53it/s]Extractor Predicting: 175it [01:48,  1.58it/s]Extractor Predicting: 176it [01:49,  1.58it/s]Extractor Predicting: 177it [01:50,  1.59it/s]Extractor Predicting: 178it [01:50,  1.60it/s]Extractor Predicting: 179it [01:51,  1.62it/s]Extractor Predicting: 180it [01:51,  1.61it/s]Extractor Predicting: 181it [01:52,  1.62it/s]Extractor Predicting: 182it [01:53,  1.65it/s]Extractor Predicting: 183it [01:53,  1.59it/s]Extractor Predicting: 184it [01:54,  1.64it/s]Extractor Predicting: 185it [01:55,  1.62it/s]Extractor Predicting: 186it [01:55,  1.64it/s]Extractor Predicting: 187it [01:56,  1.63it/s]Extractor Predicting: 188it [01:56,  1.59it/s]Extractor Predicting: 189it [01:57,  1.54it/s]Extractor Predicting: 190it [01:58,  1.56it/s]Extractor Predicting: 191it [01:58,  1.55it/s]Extractor Predicting: 192it [01:59,  1.56it/s]Extractor Predicting: 193it [02:00,  1.60it/s]Extractor Predicting: 194it [02:01,  1.42it/s]Extractor Predicting: 195it [02:01,  1.45it/s]Extractor Predicting: 196it [02:02,  1.45it/s]Extractor Predicting: 197it [02:02,  1.50it/s]Extractor Predicting: 198it [02:03,  1.50it/s]Extractor Predicting: 199it [02:04,  1.54it/s]Extractor Predicting: 200it [02:04,  1.55it/s]Extractor Predicting: 201it [02:05,  1.57it/s]Extractor Predicting: 202it [02:06,  1.56it/s]Extractor Predicting: 203it [02:06,  1.58it/s]Extractor Predicting: 204it [02:07,  1.62it/s]Extractor Predicting: 205it [02:07,  1.60it/s]Extractor Predicting: 206it [02:08,  1.63it/s]Extractor Predicting: 207it [02:09,  1.60it/s]Extractor Predicting: 208it [02:09,  1.59it/s]Extractor Predicting: 209it [02:10,  1.60it/s]Extractor Predicting: 210it [02:11,  1.60it/s]Extractor Predicting: 211it [02:11,  1.59it/s]Extractor Predicting: 212it [02:12,  1.55it/s]Extractor Predicting: 213it [02:13,  1.58it/s]Extractor Predicting: 214it [02:13,  1.58it/s]Extractor Predicting: 215it [02:14,  1.59it/s]Extractor Predicting: 216it [02:14,  1.60it/s]Extractor Predicting: 217it [02:15,  1.58it/s]Extractor Predicting: 218it [02:16,  1.60it/s]Extractor Predicting: 219it [02:16,  1.57it/s]Extractor Predicting: 220it [02:17,  1.60it/s]Extractor Predicting: 221it [02:18,  1.63it/s]Extractor Predicting: 222it [02:18,  1.39it/s]Extractor Predicting: 223it [02:19,  1.41it/s]Extractor Predicting: 224it [02:20,  1.50it/s]Extractor Predicting: 225it [02:20,  1.52it/s]Extractor Predicting: 226it [02:21,  1.54it/s]Extractor Predicting: 227it [02:22,  1.50it/s]Extractor Predicting: 228it [02:22,  1.55it/s]Extractor Predicting: 229it [02:23,  1.56it/s]Extractor Predicting: 230it [02:24,  1.59it/s]Extractor Predicting: 231it [02:24,  1.61it/s]Extractor Predicting: 232it [02:25,  1.59it/s]Extractor Predicting: 233it [02:25,  1.61it/s]Extractor Predicting: 234it [02:26,  1.60it/s]Extractor Predicting: 235it [02:27,  1.58it/s]Extractor Predicting: 236it [02:27,  1.59it/s]Extractor Predicting: 237it [02:28,  1.59it/s]Extractor Predicting: 238it [02:29,  1.59it/s]Extractor Predicting: 239it [02:29,  1.57it/s]Extractor Predicting: 240it [02:30,  1.58it/s]Extractor Predicting: 241it [02:30,  1.58it/s]Extractor Predicting: 242it [02:31,  1.52it/s]Extractor Predicting: 243it [02:32,  1.55it/s]Extractor Predicting: 244it [02:32,  1.55it/s]Extractor Predicting: 245it [02:33,  1.57it/s]Extractor Predicting: 246it [02:34,  1.58it/s]Extractor Predicting: 247it [02:34,  1.59it/s]Extractor Predicting: 248it [02:35,  1.59it/s]Extractor Predicting: 249it [02:36,  1.62it/s]Extractor Predicting: 250it [02:36,  1.62it/s]Extractor Predicting: 251it [02:37,  1.65it/s]Extractor Predicting: 252it [02:37,  1.62it/s]Extractor Predicting: 253it [02:38,  1.62it/s]Extractor Predicting: 254it [02:39,  1.62it/s]Extractor Predicting: 255it [02:39,  1.60it/s]Extractor Predicting: 256it [02:40,  1.55it/s]Extractor Predicting: 257it [02:41,  1.57it/s]Extractor Predicting: 258it [02:41,  1.57it/s]Extractor Predicting: 259it [02:42,  1.63it/s]Extractor Predicting: 260it [02:42,  1.61it/s]Extractor Predicting: 261it [02:43,  1.63it/s]Extractor Predicting: 262it [02:44,  1.61it/s]Extractor Predicting: 263it [02:44,  1.60it/s]Extractor Predicting: 264it [02:45,  1.66it/s]Extractor Predicting: 265it [02:45,  1.65it/s]Extractor Predicting: 266it [02:46,  1.64it/s]Extractor Predicting: 267it [02:47,  1.63it/s]Extractor Predicting: 268it [02:47,  1.63it/s]Extractor Predicting: 269it [02:48,  1.62it/s]Extractor Predicting: 270it [02:49,  1.59it/s]Extractor Predicting: 271it [02:49,  1.56it/s]Extractor Predicting: 272it [02:50,  1.58it/s]Extractor Predicting: 273it [02:50,  1.58it/s]Extractor Predicting: 274it [02:51,  1.42it/s]Extractor Predicting: 275it [02:52,  1.43it/s]Extractor Predicting: 276it [02:53,  1.44it/s]Extractor Predicting: 277it [02:53,  1.47it/s]Extractor Predicting: 278it [02:54,  1.53it/s]Extractor Predicting: 279it [02:55,  1.54it/s]Extractor Predicting: 280it [02:55,  1.54it/s]Extractor Predicting: 281it [02:56,  1.55it/s]Extractor Predicting: 282it [02:56,  1.56it/s]Extractor Predicting: 283it [02:57,  1.55it/s]Extractor Predicting: 284it [02:58,  1.56it/s]Extractor Predicting: 285it [02:58,  1.56it/s]Extractor Predicting: 286it [02:59,  1.61it/s]Extractor Predicting: 287it [03:00,  1.58it/s]Extractor Predicting: 288it [03:00,  1.80it/s]Extractor Predicting: 288it [03:00,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:34,507 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:34,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:34,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:34,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:34,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:10:35,109 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:10:35,110 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:10:35,688 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:10:36,757 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:10:36,757 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:39,635 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:39,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:39,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:39,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:10:39,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:10:40,389 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:10:40,390 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:10:40,966 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:10:41,138 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:10:41,138 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5758157389635317,
  "recall": 0.13043478260869565,
  "score": 0.2126905352711804,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:01,  2.22it/s]Extractor Predicting: 3it [00:01,  1.95it/s]
[INFO|configuration_utils.py:515] 2023-08-28 17:10:43,832 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:10:43,833 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:10:43,839 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:10:43,840 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 17:10:43,844 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:10:57,671 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 17:10:57,680 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 17:10:57,692 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:10:57,693 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:10:57,706 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:10:57,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:10:57,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:10:57,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:10:57,711 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:10:57,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:10:57,711 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6666666666666666,
  "recall": 0.02,
  "score": 0.038834951456310676,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 17:10:57,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:10:58,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:10:59,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:10:59,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:00,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:01,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:01,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:02,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:02,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:03,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:04,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:04,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:05,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:06,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:06,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:07,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:08,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:08,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:09,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:09,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:10,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:02, 13.00s/it][WARNING|generation_utils.py:914] 2023-08-28 17:11:10,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:11,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:12,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:12,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:13,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:14,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:14,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:15,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:16,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:16,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:17,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:17,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:18,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:19,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:19,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:20,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:21,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:21,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:22,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:23,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:23,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:26<02:52, 13.29s/it][WARNING|generation_utils.py:914] 2023-08-28 17:11:24,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:25,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:25,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:26,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:26,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:27,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:28,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:28,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:29,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:29,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:30,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:30,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:31,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:32,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:32,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:33,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:33,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:34,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:34,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:35,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:36,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:36,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:37,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:39<02:40, 13.35s/it][WARNING|generation_utils.py:914] 2023-08-28 17:11:37,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:38,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:39,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:39,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:40,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:41,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:41,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:42,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:42,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:43,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:44,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:44,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:45,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:45,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:46,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:47,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:47,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:48,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:49,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:50,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:50,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:51,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:51,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:54<02:31, 13.82s/it][WARNING|generation_utils.py:914] 2023-08-28 17:11:52,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:53,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:53,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:54,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:55,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:55,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:56,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:57,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:57,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:58,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:58,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:11:59,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:00,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:00,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:01,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:02,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:03,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:03,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:04,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:05,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:06,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:07,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:09<02:24, 14.41s/it][WARNING|generation_utils.py:914] 2023-08-28 17:12:07,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:08,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:09,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:09,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:10,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:10,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:11,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:12,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:12,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:13,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:13,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:14,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:15,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:15,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:16,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:17,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:17,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:18,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:18,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:19,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:20,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:22<02:04, 13.88s/it][WARNING|generation_utils.py:914] 2023-08-28 17:12:20,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:21,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:22,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:22,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:23,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:24,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:25,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:25,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:26,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:27,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:27,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:28,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:29,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:29,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:30,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:31,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:31,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:32,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:33,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:33,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:34,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:37<01:52, 14.09s/it][WARNING|generation_utils.py:914] 2023-08-28 17:12:35,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:35,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:36,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:37,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:37,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:38,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:38,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:39,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:40,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:41,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:42,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:42,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:43,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:44,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:44,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:45,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:46,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:46,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:47,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:48,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:48,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:49,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:52<01:40, 14.30s/it][WARNING|generation_utils.py:914] 2023-08-28 17:12:49,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:50,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:51,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:52,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:52,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:53,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:54,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:55,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:55,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:56,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:57,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:57,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:58,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:12:59,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:00,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:00,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:01,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:02,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:03,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:03,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:04,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:05,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:08<01:29, 14.87s/it][WARNING|generation_utils.py:914] 2023-08-28 17:13:06,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:06,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:07,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:08,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:08,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:09,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:09,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:10,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:11,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:11,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:12,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:13,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:13,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:14,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:15,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:15,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:16,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:17,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:17,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:18,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:18,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:21<01:12, 14.43s/it][WARNING|generation_utils.py:914] 2023-08-28 17:13:19,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:20,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:20,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:21,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:22,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:22,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:23,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:23,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:24,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:25,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:26,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:26,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:27,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:28,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:28,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:29,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:29,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:30,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:31,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:31,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:32,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:33,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:35<00:57, 14.41s/it][WARNING|generation_utils.py:914] 2023-08-28 17:13:33,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:34,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:35,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:35,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:36,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:36,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:37,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:38,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:38,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:39,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:40,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:40,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:41,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:41,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:42,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:43,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:43,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:44,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:45,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:45,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:46,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:47,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:49<00:42, 14.22s/it][WARNING|generation_utils.py:914] 2023-08-28 17:13:47,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:48,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:48,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:49,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:49,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:50,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:51,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:51,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:52,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:52,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:53,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:54,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:54,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:55,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:55,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:56,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:56,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:57,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:57,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:58,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:01<00:26, 13.39s/it][WARNING|generation_utils.py:914] 2023-08-28 17:13:59,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:13:59,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:00,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:00,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:01,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:02,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:02,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:03,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:03,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:04,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:05,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:05,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:06,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:06,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:07,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:08,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:08,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:09,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:10,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:10,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:11,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:12,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:13,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:15<00:13, 13.78s/it][WARNING|generation_utils.py:914] 2023-08-28 17:14:13,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:14,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:15,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:15,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:16,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:17,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:17,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:18,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:19,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:19,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:20,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:20,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:21,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:22,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:22,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:23,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:23,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:24,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:25,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:25,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:14:26,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:29<00:00, 13.60s/it]Generating: 100%|██████████| 15/15 [03:29<00:00, 13.94s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:34,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:34,009 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:34,009 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:34,009 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:34,009 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:14:34,290 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:14:34,291 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:14:34,992 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:14:36,067 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:14:36,067 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:37,497 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:37,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:37,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:37,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:37,518 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:14:37,847 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:14:37,848 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:14:38,118 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:14:38,266 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:14:38,266 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of political party . Context : Later in 2008 , the party became a part of a coalition government of former Prime Minister John Key and Deputy Prime Minister Barnaby Joyce . Head Entity : Barnaby Joyce , Tail Entity : National .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'most commonly spoken name\', \'said to be the same as\', \'\', \'For the purposes of the classification , the " name " is not the same as the most commonly spoken name , " .\')', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1143–1230 ) , at the behest of Empress Alexander the Great , the Kingdom of Georgia lost thirty ships ; the remaining two remaining ships were destroyed by the British fleet following the landing of the emperor . Head Entity : King of Georgia , Tail Entity : Empress Alexander the Great .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8821022727272727, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 426, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9484375, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : winner . Context : Later in the year , the competition was decided by a split decision match between the champions and the runner - up . Head Entity : split , Tail Entity : winner .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : winner .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : work location .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 11470
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11570, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.26it/s]Extractor Estimating: 2it [00:01,  1.38it/s]Extractor Estimating: 3it [00:02,  1.42it/s]Extractor Estimating: 4it [00:02,  1.47it/s]Extractor Estimating: 5it [00:03,  1.54it/s]Extractor Estimating: 6it [00:04,  1.55it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.45it/s]Extractor Estimating: 9it [00:06,  1.54it/s]Extractor Estimating: 10it [00:06,  1.56it/s]Extractor Estimating: 11it [00:07,  1.56it/s]Extractor Estimating: 12it [00:08,  1.43it/s]Extractor Estimating: 13it [00:08,  1.50it/s]Extractor Estimating: 14it [00:09,  1.53it/s]Extractor Estimating: 15it [00:09,  1.56it/s]Extractor Estimating: 16it [00:10,  1.59it/s]Extractor Estimating: 17it [00:11,  1.57it/s]Extractor Estimating: 18it [00:11,  1.54it/s]Extractor Estimating: 19it [00:12,  1.56it/s]Extractor Estimating: 20it [00:13,  1.56it/s]Extractor Estimating: 21it [00:13,  1.59it/s]Extractor Estimating: 22it [00:14,  1.59it/s]Extractor Estimating: 23it [00:15,  1.58it/s]Extractor Estimating: 24it [00:15,  1.56it/s]Extractor Estimating: 25it [00:16,  1.56it/s]Extractor Estimating: 26it [00:16,  1.59it/s]Extractor Estimating: 27it [00:17,  1.60it/s]Extractor Estimating: 28it [00:18,  1.59it/s]Extractor Estimating: 29it [00:18,  1.55it/s]Extractor Estimating: 30it [00:19,  1.54it/s]Extractor Estimating: 31it [00:20,  1.54it/s]Extractor Estimating: 32it [00:20,  1.54it/s]Extractor Estimating: 33it [00:21,  1.42it/s]Extractor Estimating: 34it [00:22,  1.44it/s]Extractor Estimating: 35it [00:22,  1.46it/s]Extractor Estimating: 36it [00:23,  1.45it/s]Extractor Estimating: 37it [00:24,  1.50it/s]Extractor Estimating: 38it [00:24,  1.51it/s]Extractor Estimating: 39it [00:25,  1.56it/s]Extractor Estimating: 40it [00:26,  1.50it/s]Extractor Estimating: 41it [00:26,  1.53it/s]Extractor Estimating: 42it [00:27,  1.50it/s]Extractor Estimating: 43it [00:28,  1.47it/s]Extractor Estimating: 44it [00:28,  1.50it/s]Extractor Estimating: 45it [00:29,  1.53it/s]Extractor Estimating: 46it [00:30,  1.51it/s]Extractor Estimating: 47it [00:30,  1.56it/s]Extractor Estimating: 48it [00:31,  1.55it/s]Extractor Estimating: 49it [00:32,  1.57it/s]Extractor Estimating: 50it [00:32,  1.52it/s]Extractor Estimating: 51it [00:33,  1.57it/s]Extractor Estimating: 52it [00:33,  1.60it/s]Extractor Estimating: 53it [00:34,  1.61it/s]Extractor Estimating: 54it [00:35,  1.63it/s]Extractor Estimating: 55it [00:35,  1.61it/s]Extractor Estimating: 56it [00:36,  1.62it/s]Extractor Estimating: 57it [00:36,  1.71it/s]Extractor Estimating: 58it [00:37,  1.71it/s]Extractor Estimating: 59it [00:38,  1.74it/s]Extractor Estimating: 60it [00:38,  1.74it/s]Extractor Estimating: 61it [00:39,  1.76it/s]Extractor Estimating: 62it [00:39,  1.78it/s]Extractor Estimating: 63it [00:40,  1.73it/s]Extractor Estimating: 64it [00:41,  1.67it/s]Extractor Estimating: 65it [00:41,  1.69it/s]Extractor Estimating: 66it [00:42,  1.67it/s]Extractor Estimating: 67it [00:42,  1.66it/s]Extractor Estimating: 68it [00:43,  1.64it/s]Extractor Estimating: 69it [00:44,  1.66it/s]Extractor Estimating: 70it [00:44,  1.67it/s]Extractor Estimating: 71it [00:45,  1.69it/s]Extractor Estimating: 72it [00:45,  1.70it/s]Extractor Estimating: 73it [00:46,  1.77it/s]Extractor Estimating: 74it [00:47,  1.52it/s]Extractor Estimating: 75it [00:47,  1.52it/s]Extractor Estimating: 76it [00:48,  1.60it/s]Extractor Estimating: 77it [00:48,  1.63it/s]Extractor Estimating: 78it [00:49,  1.63it/s]Extractor Estimating: 79it [00:50,  1.67it/s]Extractor Estimating: 80it [00:50,  1.59it/s]Extractor Estimating: 81it [00:51,  1.62it/s]Extractor Estimating: 82it [00:51,  1.70it/s]Extractor Estimating: 83it [00:52,  1.76it/s]Extractor Estimating: 84it [00:53,  1.73it/s]Extractor Estimating: 85it [00:53,  1.79it/s]Extractor Estimating: 86it [00:54,  1.80it/s]Extractor Estimating: 87it [00:54,  1.81it/s]Extractor Estimating: 88it [00:55,  1.84it/s]Extractor Estimating: 89it [00:55,  1.83it/s]Extractor Estimating: 90it [00:56,  1.83it/s]Extractor Estimating: 91it [00:56,  1.79it/s]Extractor Estimating: 92it [00:57,  1.81it/s]Extractor Estimating: 93it [00:57,  1.81it/s]Extractor Estimating: 94it [00:58,  1.71it/s]Extractor Estimating: 95it [00:59,  1.71it/s]Extractor Estimating: 96it [00:59,  1.73it/s]Extractor Estimating: 97it [01:00,  1.74it/s]Extractor Estimating: 98it [01:00,  1.72it/s]Extractor Estimating: 99it [01:01,  1.75it/s]Extractor Estimating: 100it [01:02,  1.79it/s]Extractor Estimating: 101it [01:02,  1.78it/s]Extractor Estimating: 102it [01:03,  1.71it/s]Extractor Estimating: 103it [01:03,  1.66it/s]Extractor Estimating: 104it [01:04,  1.73it/s]Extractor Estimating: 105it [01:04,  1.74it/s]Extractor Estimating: 106it [01:05,  1.75it/s]Extractor Estimating: 107it [01:06,  1.74it/s]Extractor Estimating: 108it [01:06,  1.66it/s]Extractor Estimating: 109it [01:07,  1.62it/s]Extractor Estimating: 110it [01:08,  1.61it/s]Extractor Estimating: 111it [01:08,  1.64it/s]Extractor Estimating: 112it [01:09,  1.63it/s]Extractor Estimating: 113it [01:09,  1.61it/s]Extractor Estimating: 114it [01:10,  1.51it/s]Extractor Estimating: 115it [01:11,  1.54it/s]Extractor Estimating: 116it [01:11,  1.56it/s]Extractor Estimating: 117it [01:12,  1.53it/s]Extractor Estimating: 118it [01:13,  1.59it/s]Extractor Estimating: 119it [01:13,  1.62it/s]Extractor Estimating: 120it [01:14,  1.65it/s]Extractor Estimating: 121it [01:15,  1.56it/s]Extractor Estimating: 122it [01:15,  1.57it/s]Extractor Estimating: 123it [01:16,  1.55it/s]Extractor Estimating: 124it [01:16,  1.58it/s]Extractor Estimating: 125it [01:17,  1.61it/s]Extractor Estimating: 126it [01:18,  1.62it/s]Extractor Estimating: 127it [01:18,  1.60it/s]Extractor Estimating: 128it [01:19,  1.64it/s]Extractor Estimating: 129it [01:19,  1.64it/s]Extractor Estimating: 130it [01:20,  1.62it/s]Extractor Estimating: 131it [01:21,  1.59it/s]Extractor Estimating: 132it [01:21,  1.64it/s]Extractor Estimating: 133it [01:22,  1.60it/s]Extractor Estimating: 134it [01:23,  1.65it/s]Extractor Estimating: 135it [01:23,  1.62it/s]Extractor Estimating: 136it [01:24,  1.60it/s]Extractor Estimating: 137it [01:24,  1.59it/s]Extractor Estimating: 138it [01:25,  1.63it/s]Extractor Estimating: 139it [01:26,  1.62it/s]Extractor Estimating: 140it [01:26,  1.64it/s]Extractor Estimating: 141it [01:27,  1.55it/s]Extractor Estimating: 142it [01:28,  1.57it/s]Extractor Estimating: 143it [01:28,  1.63it/s]Extractor Estimating: 144it [01:29,  1.63it/s]Extractor Estimating: 145it [01:29,  1.65it/s]Extractor Estimating: 146it [01:30,  1.65it/s]Extractor Estimating: 147it [01:31,  1.60it/s]Extractor Estimating: 148it [01:31,  1.61it/s]Extractor Estimating: 149it [01:32,  1.54it/s]Extractor Estimating: 150it [01:33,  1.52it/s]Extractor Estimating: 151it [01:33,  1.51it/s]Extractor Estimating: 152it [01:34,  1.53it/s]Extractor Estimating: 153it [01:35,  1.52it/s]Extractor Estimating: 154it [01:35,  1.48it/s]Extractor Estimating: 155it [01:36,  1.41it/s]Extractor Estimating: 156it [01:37,  1.45it/s]Extractor Estimating: 157it [01:38,  1.42it/s]Extractor Estimating: 158it [01:38,  1.49it/s]Extractor Estimating: 159it [01:39,  1.53it/s]Extractor Estimating: 160it [01:39,  1.57it/s]Extractor Estimating: 161it [01:40,  1.59it/s]Extractor Estimating: 162it [01:41,  1.56it/s]Extractor Estimating: 163it [01:41,  1.53it/s]Extractor Estimating: 164it [01:42,  1.49it/s]Extractor Estimating: 165it [01:43,  1.53it/s]Extractor Estimating: 166it [01:43,  1.51it/s]Extractor Estimating: 167it [01:44,  1.51it/s]Extractor Estimating: 168it [01:45,  1.53it/s]Extractor Estimating: 169it [01:45,  1.57it/s]Extractor Estimating: 170it [01:46,  1.63it/s]Extractor Estimating: 171it [01:46,  1.67it/s]Extractor Estimating: 172it [01:47,  1.64it/s]Extractor Estimating: 173it [01:48,  1.61it/s]Extractor Estimating: 174it [01:48,  1.65it/s]Extractor Estimating: 175it [01:49,  1.63it/s]Extractor Estimating: 176it [01:49,  1.64it/s]Extractor Estimating: 177it [01:50,  1.57it/s]Extractor Estimating: 178it [01:51,  1.61it/s]Extractor Estimating: 179it [01:51,  1.62it/s]Extractor Estimating: 180it [01:52,  1.58it/s]Extractor Estimating: 181it [01:53,  1.59it/s]Extractor Estimating: 182it [01:53,  1.62it/s]Extractor Estimating: 183it [01:54,  1.60it/s]Extractor Estimating: 184it [01:54,  1.63it/s]Extractor Estimating: 185it [01:55,  1.48it/s]Extractor Estimating: 186it [01:56,  1.54it/s]Extractor Estimating: 187it [01:56,  1.55it/s]Extractor Estimating: 188it [01:57,  1.54it/s]Extractor Estimating: 189it [01:58,  1.58it/s]Extractor Estimating: 190it [01:58,  1.59it/s]Extractor Estimating: 191it [01:59,  1.58it/s]Extractor Estimating: 192it [02:00,  1.58it/s]Extractor Estimating: 193it [02:00,  1.65it/s]Extractor Estimating: 194it [02:01,  1.65it/s]Extractor Estimating: 195it [02:01,  1.66it/s]Extractor Estimating: 196it [02:02,  1.64it/s]Extractor Estimating: 197it [02:03,  1.64it/s]Extractor Estimating: 198it [02:03,  1.55it/s]Extractor Estimating: 199it [02:04,  1.63it/s]Extractor Estimating: 200it [02:04,  1.63it/s]Extractor Estimating: 201it [02:05,  1.54it/s]Extractor Estimating: 202it [02:06,  1.57it/s]Extractor Estimating: 203it [02:07,  1.51it/s]Extractor Estimating: 204it [02:07,  1.53it/s]Extractor Estimating: 205it [02:08,  1.55it/s]Extractor Estimating: 206it [02:08,  1.57it/s]Extractor Estimating: 207it [02:09,  1.55it/s]Extractor Estimating: 208it [02:10,  1.52it/s]Extractor Estimating: 209it [02:10,  1.57it/s]Extractor Estimating: 210it [02:11,  1.55it/s]Extractor Estimating: 211it [02:12,  1.52it/s]Extractor Estimating: 212it [02:12,  1.49it/s]Extractor Estimating: 213it [02:13,  1.49it/s]Extractor Estimating: 214it [02:14,  1.45it/s]Extractor Estimating: 215it [02:14,  1.52it/s]Extractor Estimating: 216it [02:15,  1.52it/s]Extractor Estimating: 217it [02:16,  1.50it/s]Extractor Estimating: 218it [02:17,  1.42it/s]Extractor Estimating: 219it [02:17,  1.48it/s]Extractor Estimating: 220it [02:18,  1.40it/s]Extractor Estimating: 221it [02:19,  1.48it/s]Extractor Estimating: 222it [02:19,  1.46it/s]Extractor Estimating: 223it [02:20,  1.41it/s]Extractor Estimating: 224it [02:21,  1.47it/s]Extractor Estimating: 225it [02:21,  1.47it/s]Extractor Estimating: 226it [02:22,  1.53it/s]Extractor Estimating: 227it [02:22,  1.60it/s]Extractor Estimating: 228it [02:23,  1.62it/s]Extractor Estimating: 229it [02:24,  1.47it/s]Extractor Estimating: 230it [02:24,  1.53it/s]Extractor Estimating: 231it [02:25,  1.61it/s]Extractor Estimating: 232it [02:26,  1.68it/s]Extractor Estimating: 233it [02:26,  1.68it/s]Extractor Estimating: 234it [02:27,  1.72it/s]Extractor Estimating: 235it [02:27,  1.78it/s]Extractor Estimating: 236it [02:28,  1.78it/s]Extractor Estimating: 237it [02:28,  1.80it/s]Extractor Estimating: 238it [02:29,  1.83it/s]Extractor Estimating: 239it [02:29,  1.79it/s]Extractor Estimating: 240it [02:30,  1.83it/s]Extractor Estimating: 241it [02:31,  1.79it/s]Extractor Estimating: 242it [02:31,  1.78it/s]Extractor Estimating: 243it [02:32,  1.72it/s]Extractor Estimating: 244it [02:32,  1.74it/s]Extractor Estimating: 245it [02:33,  1.72it/s]Extractor Estimating: 246it [02:33,  1.80it/s]Extractor Estimating: 247it [02:34,  1.77it/s]Extractor Estimating: 248it [02:35,  1.75it/s]Extractor Estimating: 249it [02:35,  1.79it/s]Extractor Estimating: 250it [02:36,  1.74it/s]Extractor Estimating: 251it [02:36,  1.70it/s]Extractor Estimating: 252it [02:37,  1.70it/s]Extractor Estimating: 253it [02:38,  1.66it/s]Extractor Estimating: 254it [02:38,  1.63it/s]Extractor Estimating: 255it [02:39,  1.61it/s]Extractor Estimating: 256it [02:39,  1.61it/s]Extractor Estimating: 257it [02:40,  1.65it/s]Extractor Estimating: 258it [02:41,  1.64it/s]Extractor Estimating: 259it [02:41,  1.56it/s]Extractor Estimating: 260it [02:42,  1.55it/s]Extractor Estimating: 261it [02:43,  1.56it/s]Extractor Estimating: 262it [02:43,  1.61it/s]Extractor Estimating: 263it [02:44,  1.70it/s]Extractor Estimating: 264it [02:44,  1.70it/s]Extractor Estimating: 265it [02:45,  1.78it/s]Extractor Estimating: 266it [02:45,  1.76it/s]Extractor Estimating: 267it [02:46,  1.70it/s]Extractor Estimating: 268it [02:47,  1.73it/s]Extractor Estimating: 269it [02:47,  1.72it/s]Extractor Estimating: 270it [02:48,  1.68it/s]Extractor Estimating: 271it [02:48,  1.77it/s]Extractor Estimating: 272it [02:49,  1.75it/s]Extractor Estimating: 273it [02:50,  1.71it/s]Extractor Estimating: 274it [02:50,  1.72it/s]Extractor Estimating: 275it [02:51,  1.74it/s]Extractor Estimating: 276it [02:51,  1.78it/s]Extractor Estimating: 277it [02:52,  1.69it/s]Extractor Estimating: 278it [02:52,  1.71it/s]Extractor Estimating: 279it [02:53,  1.75it/s]Extractor Estimating: 280it [02:53,  1.79it/s]Extractor Estimating: 281it [02:54,  1.73it/s]Extractor Estimating: 282it [02:55,  1.70it/s]Extractor Estimating: 283it [02:55,  1.79it/s]Extractor Estimating: 284it [02:56,  1.81it/s]Extractor Estimating: 285it [02:56,  1.79it/s]Extractor Estimating: 286it [02:57,  1.79it/s]Extractor Estimating: 287it [02:57,  1.82it/s]Extractor Estimating: 288it [02:58,  1.83it/s]Extractor Estimating: 289it [02:58,  1.84it/s]Extractor Estimating: 290it [02:59,  1.87it/s]Extractor Estimating: 291it [03:00,  1.82it/s]Extractor Estimating: 292it [03:00,  1.76it/s]Extractor Estimating: 293it [03:01,  1.64it/s]Extractor Estimating: 294it [03:01,  1.70it/s]Extractor Estimating: 295it [03:02,  1.70it/s]Extractor Estimating: 296it [03:03,  1.67it/s]Extractor Estimating: 297it [03:03,  1.69it/s]Extractor Estimating: 298it [03:04,  1.72it/s]Extractor Estimating: 299it [03:04,  1.69it/s]Extractor Estimating: 300it [03:05,  1.68it/s]Extractor Estimating: 301it [03:06,  1.66it/s]Extractor Estimating: 302it [03:06,  1.67it/s]Extractor Estimating: 303it [03:07,  1.63it/s]Extractor Estimating: 304it [03:07,  1.67it/s]Extractor Estimating: 305it [03:08,  1.59it/s]Extractor Estimating: 306it [03:09,  1.58it/s]Extractor Estimating: 307it [03:09,  1.63it/s]Extractor Estimating: 308it [03:10,  1.62it/s]Extractor Estimating: 309it [03:11,  1.59it/s]Extractor Estimating: 310it [03:11,  1.58it/s]Extractor Estimating: 311it [03:12,  1.54it/s]Extractor Estimating: 312it [03:13,  1.55it/s]Extractor Estimating: 313it [03:13,  1.43it/s]Extractor Estimating: 314it [03:14,  1.47it/s]Extractor Estimating: 315it [03:15,  1.50it/s]Extractor Estimating: 316it [03:15,  1.51it/s]Extractor Estimating: 317it [03:16,  1.54it/s]Extractor Estimating: 318it [03:16,  1.61it/s]Extractor Estimating: 319it [03:17,  1.62it/s]Extractor Estimating: 320it [03:18,  1.63it/s]Extractor Estimating: 321it [03:18,  1.64it/s]Extractor Estimating: 322it [03:19,  1.63it/s]Extractor Estimating: 323it [03:20,  1.61it/s]Extractor Estimating: 324it [03:20,  1.57it/s]Extractor Estimating: 325it [03:21,  1.52it/s]Extractor Estimating: 326it [03:22,  1.56it/s]Extractor Estimating: 327it [03:22,  1.62it/s]Extractor Estimating: 328it [03:23,  1.60it/s]Extractor Estimating: 329it [03:23,  1.67it/s]Extractor Estimating: 330it [03:24,  1.62it/s]Extractor Estimating: 331it [03:25,  1.65it/s]Extractor Estimating: 332it [03:25,  1.70it/s]Extractor Estimating: 333it [03:26,  1.69it/s]Extractor Estimating: 334it [03:26,  1.68it/s]Extractor Estimating: 335it [03:27,  1.58it/s]Extractor Estimating: 336it [03:28,  1.61it/s]Extractor Estimating: 337it [03:28,  1.63it/s]Extractor Estimating: 338it [03:29,  1.68it/s]Extractor Estimating: 339it [03:29,  1.69it/s]Extractor Estimating: 340it [03:30,  1.64it/s]Extractor Estimating: 341it [03:31,  1.66it/s]Extractor Estimating: 342it [03:31,  1.70it/s]Extractor Estimating: 343it [03:32,  1.74it/s]Extractor Estimating: 344it [03:32,  1.66it/s]Extractor Estimating: 345it [03:33,  1.67it/s]Extractor Estimating: 346it [03:33,  1.69it/s]Extractor Estimating: 347it [03:34,  1.61it/s]Extractor Estimating: 348it [03:35,  1.61it/s]Extractor Estimating: 349it [03:35,  1.59it/s]Extractor Estimating: 350it [03:36,  1.54it/s]Extractor Estimating: 351it [03:37,  1.56it/s]Extractor Estimating: 352it [03:37,  1.54it/s]Extractor Estimating: 353it [03:38,  1.53it/s]Extractor Estimating: 354it [03:39,  1.45it/s]Extractor Estimating: 355it [03:39,  1.53it/s]Extractor Estimating: 356it [03:40,  1.53it/s]Extractor Estimating: 357it [03:41,  1.54it/s]Extractor Estimating: 358it [03:41,  1.55it/s]Extractor Estimating: 359it [03:42,  1.58it/s]Extractor Estimating: 360it [03:43,  1.55it/s]Extractor Estimating: 361it [03:43,  1.60it/s]Extractor Estimating: 362it [03:44,  1.62it/s]Extractor Estimating: 363it [03:44,  1.63it/s]Extractor Estimating: 364it [03:45,  1.62it/s]Extractor Estimating: 365it [03:46,  1.66it/s]Extractor Estimating: 366it [03:46,  1.61it/s]Extractor Estimating: 367it [03:47,  1.60it/s]Extractor Estimating: 368it [03:47,  1.64it/s]Extractor Estimating: 369it [03:48,  1.57it/s]Extractor Estimating: 370it [03:49,  1.57it/s]Extractor Estimating: 371it [03:49,  1.55it/s]Extractor Estimating: 372it [03:50,  1.49it/s]Extractor Estimating: 373it [03:51,  1.52it/s]Extractor Estimating: 374it [03:51,  1.55it/s]Extractor Estimating: 375it [03:52,  1.55it/s]Extractor Estimating: 375it [03:52,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:44,535 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:44,540 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:44,541 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:44,541 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:44,541 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:18:45,889 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:18:45,890 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:18:46,395 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:18:47,487 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:18:47,487 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:49,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:49,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:49,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:49,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:49,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:18:50,426 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:18:50,427 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:18:51,134 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:18:51,354 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:18:51,354 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:29:10,016 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:29:10,043 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7497 mean pseudo reward: 0.9392943202854656
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 22740
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22840, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22840, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.023, loss:874.7801
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.075, loss:829.4597
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.015, loss:789.2412
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.002, loss:772.7732
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.015, loss:803.2100
>> valid entity prec:0.5825, rec:0.5602, f1:0.5712
>> valid relation prec:0.3388, rec:0.0948, f1:0.1482
>> valid relation with NER prec:0.3388, rec:0.0948, f1:0.1482
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.212, loss:814.2204
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.015, loss:769.3148
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.023, loss:780.6469
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.033, loss:843.8269
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.012, loss:777.2454
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5973, rec:0.5912, f1:0.5942
>> valid relation prec:0.3379, rec:0.0848, f1:0.1356
>> valid relation with NER prec:0.3379, rec:0.0848, f1:0.1356
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.223, loss:776.9641
g_step 1200, step 261, avg_time 1.027, loss:792.6875
g_step 1300, step 48, avg_time 1.005, loss:755.6010
g_step 1400, step 148, avg_time 1.031, loss:745.2681
g_step 1500, step 248, avg_time 1.014, loss:793.8948
>> valid entity prec:0.5671, rec:0.5946, f1:0.5805
>> valid relation prec:0.3004, rec:0.0828, f1:0.1298
>> valid relation with NER prec:0.3004, rec:0.0828, f1:0.1298
g_step 1600, step 35, avg_time 2.204, loss:725.9616
g_step 1700, step 135, avg_time 1.000, loss:691.0186
g_step 1800, step 235, avg_time 1.048, loss:750.3890
g_step 1900, step 22, avg_time 1.019, loss:700.8143
g_step 2000, step 122, avg_time 1.018, loss:646.5511
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5517, rec:0.5839, f1:0.5673
>> valid relation prec:0.2641, rec:0.0819, f1:0.1251
>> valid relation with NER prec:0.2641, rec:0.0819, f1:0.1251
g_step 2100, step 222, avg_time 2.235, loss:684.2286
g_step 2200, step 9, avg_time 1.019, loss:690.3057
g_step 2300, step 109, avg_time 1.034, loss:623.0424
g_step 2400, step 209, avg_time 1.024, loss:648.8402
g_step 2500, step 309, avg_time 0.997, loss:658.4155
>> valid entity prec:0.6176, rec:0.5115, f1:0.5595
>> valid relation prec:0.2425, rec:0.0670, f1:0.1050
>> valid relation with NER prec:0.2425, rec:0.0670, f1:0.1050
g_step 2600, step 96, avg_time 2.196, loss:611.8455
g_step 2700, step 196, avg_time 1.028, loss:638.0219
g_step 2800, step 296, avg_time 1.024, loss:631.6258
g_step 2900, step 83, avg_time 1.013, loss:576.4122
g_step 3000, step 183, avg_time 1.027, loss:597.0255
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5880, rec:0.6041, f1:0.5959
>> valid relation prec:0.2414, rec:0.0888, f1:0.1298
>> valid relation with NER prec:0.2414, rec:0.0888, f1:0.1298
new max entity f1 on valid!
g_step 3100, step 283, avg_time 2.215, loss:607.1820
g_step 3200, step 70, avg_time 1.017, loss:561.5955
g_step 3300, step 170, avg_time 1.020, loss:551.7612
g_step 3400, step 270, avg_time 1.026, loss:596.1553
g_step 3500, step 57, avg_time 1.006, loss:535.7220
>> valid entity prec:0.5817, rec:0.5232, f1:0.5509
>> valid relation prec:0.3116, rec:0.0905, f1:0.1403
>> valid relation with NER prec:0.3116, rec:0.0905, f1:0.1403
g_step 3600, step 157, avg_time 2.201, loss:532.1495
g_step 3700, step 257, avg_time 1.021, loss:556.8072
g_step 3800, step 44, avg_time 1.031, loss:558.5700
g_step 3900, step 144, avg_time 1.014, loss:518.5909
g_step 4000, step 244, avg_time 1.011, loss:527.8378
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5568, rec:0.5379, f1:0.5472
>> valid relation prec:0.2416, rec:0.0765, f1:0.1162
>> valid relation with NER prec:0.2416, rec:0.0765, f1:0.1162
g_step 4100, step 31, avg_time 2.206, loss:507.3386
g_step 4200, step 131, avg_time 1.025, loss:489.2158
g_step 4300, step 231, avg_time 1.021, loss:502.7376
g_step 4400, step 18, avg_time 1.021, loss:496.4989
g_step 4500, step 118, avg_time 1.017, loss:452.9635
>> valid entity prec:0.6034, rec:0.5563, f1:0.5789
>> valid relation prec:0.2471, rec:0.0805, f1:0.1214
>> valid relation with NER prec:0.2471, rec:0.0805, f1:0.1214
g_step 4600, step 218, avg_time 2.212, loss:490.9652
g_step 4700, step 5, avg_time 1.012, loss:483.0916
g_step 4800, step 105, avg_time 1.009, loss:454.9666
g_step 4900, step 205, avg_time 1.024, loss:475.2102
g_step 5000, step 305, avg_time 1.025, loss:473.7785
learning rate was adjusted to 0.0008
>> valid entity prec:0.5824, rec:0.5426, f1:0.5618
>> valid relation prec:0.2936, rec:0.1172, f1:0.1675
>> valid relation with NER prec:0.2936, rec:0.1172, f1:0.1675
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 92, avg_time 2.190, loss:427.4636
g_step 5200, step 192, avg_time 1.030, loss:426.9734
g_step 5300, step 292, avg_time 1.025, loss:448.2816
g_step 5400, step 79, avg_time 1.017, loss:417.1771
g_step 5500, step 179, avg_time 1.025, loss:397.9136
>> valid entity prec:0.6023, rec:0.5087, f1:0.5516
>> valid relation prec:0.2585, rec:0.0828, f1:0.1254
>> valid relation with NER prec:0.2585, rec:0.0828, f1:0.1254
g_step 5600, step 279, avg_time 2.198, loss:435.3714
g_step 5700, step 66, avg_time 1.021, loss:414.7304
g_step 5800, step 166, avg_time 1.013, loss:406.6915
g_step 5900, step 266, avg_time 1.034, loss:406.4518
g_step 6000, step 53, avg_time 1.005, loss:403.4550
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5585, rec:0.5739, f1:0.5661
>> valid relation prec:0.2076, rec:0.0942, f1:0.1296
>> valid relation with NER prec:0.2076, rec:0.0942, f1:0.1296
g_step 6100, step 153, avg_time 2.214, loss:388.0139
g_step 6200, step 253, avg_time 1.013, loss:396.1812
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:29:10 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:29:10 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-29-10_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:29:11 - WARNING - datasets.builder -   Using custom data configuration default-daa7bda96e6790b4
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-daa7bda96e6790b4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:29:12,168 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:29:12,169 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:29:12,170 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:29:12,171 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:29:12,230 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:29:12,245 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:29:12,245 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:29:12,245 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:29:12,245 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:29:12,245 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:29:12,245 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:29:12,537 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:29:15,670 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:29:15,683 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-daa7bda96e6790b4/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.13ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.94ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.42ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.83ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.07ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.26ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.38ba/s]100%|██████████| 8/8 [00:01<00:00,  5.22ba/s]100%|██████████| 8/8 [00:01<00:00,  4.35ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.52ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.02ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.21ba/s]100%|██████████| 4/4 [00:00<00:00,  5.28ba/s]100%|██████████| 4/4 [00:00<00:00,  4.71ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  6.86ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.41ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.03ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.45ba/s]100%|██████████| 8/8 [00:00<00:00, 10.71ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.26ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.10ba/s]100%|██████████| 4/4 [00:00<00:00, 11.51ba/s]
[INFO|trainer.py:414] 2023-08-28 19:29:20,478 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:29:20,501 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:29:20,501 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 19:29:20,501 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:29:20,501 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:29:20,501 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:29:20,502 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:29:20,502 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.32it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:48,  3.44it/s]  1%|          | 5/585 [00:01<02:47,  3.45it/s]  1%|          | 6/585 [00:01<02:49,  3.42it/s]  1%|          | 7/585 [00:02<02:49,  3.41it/s]  1%|▏         | 8/585 [00:02<02:48,  3.41it/s]  2%|▏         | 9/585 [00:02<02:48,  3.42it/s]  2%|▏         | 10/585 [00:02<02:48,  3.41it/s]  2%|▏         | 11/585 [00:03<02:47,  3.42it/s]  2%|▏         | 12/585 [00:03<02:47,  3.42it/s]  2%|▏         | 13/585 [00:03<02:47,  3.41it/s]  2%|▏         | 14/585 [00:04<02:47,  3.41it/s]  3%|▎         | 15/585 [00:04<02:47,  3.41it/s]  3%|▎         | 16/585 [00:04<02:46,  3.41it/s]  3%|▎         | 17/585 [00:04<02:48,  3.37it/s]  3%|▎         | 18/585 [00:05<02:47,  3.39it/s]  3%|▎         | 19/585 [00:05<02:46,  3.39it/s]  3%|▎         | 20/585 [00:05<02:46,  3.40it/s]  4%|▎         | 21/585 [00:06<02:45,  3.40it/s]  4%|▍         | 22/585 [00:06<02:45,  3.41it/s]  4%|▍         | 23/585 [00:06<02:45,  3.40it/s]  4%|▍         | 24/585 [00:07<02:44,  3.40it/s]  4%|▍         | 25/585 [00:07<02:44,  3.41it/s]  4%|▍         | 26/585 [00:07<02:44,  3.41it/s]  5%|▍         | 27/585 [00:07<02:43,  3.41it/s]  5%|▍         | 28/585 [00:08<02:46,  3.36it/s]  5%|▍         | 29/585 [00:08<02:44,  3.37it/s]  5%|▌         | 30/585 [00:08<02:43,  3.38it/s]  5%|▌         | 31/585 [00:09<02:43,  3.39it/s]  5%|▌         | 32/585 [00:09<02:42,  3.40it/s]  6%|▌         | 33/585 [00:09<02:42,  3.40it/s]  6%|▌         | 34/585 [00:09<02:41,  3.40it/s]  6%|▌         | 35/585 [00:10<02:41,  3.40it/s]  6%|▌         | 36/585 [00:10<02:41,  3.40it/s]  6%|▋         | 37/585 [00:10<02:41,  3.40it/s]  6%|▋         | 38/585 [00:11<02:40,  3.41it/s]  7%|▋         | 39/585 [00:11<02:40,  3.40it/s]  7%|▋         | 40/585 [00:11<02:40,  3.40it/s]  7%|▋         | 41/585 [00:12<02:39,  3.40it/s]  7%|▋         | 42/585 [00:12<02:39,  3.41it/s]  7%|▋         | 43/585 [00:12<02:39,  3.41it/s]  8%|▊         | 44/585 [00:12<02:38,  3.40it/s]  8%|▊         | 45/585 [00:13<02:38,  3.41it/s]  8%|▊         | 46/585 [00:13<02:38,  3.41it/s]  8%|▊         | 47/585 [00:13<02:37,  3.41it/s]  8%|▊         | 48/585 [00:14<02:37,  3.41it/s]  8%|▊         | 49/585 [00:14<02:37,  3.41it/s]  9%|▊         | 50/585 [00:14<02:41,  3.32it/s]  9%|▊         | 51/585 [00:15<02:39,  3.34it/s]  9%|▉         | 52/585 [00:15<02:38,  3.36it/s]  9%|▉         | 53/585 [00:15<02:37,  3.38it/s]  9%|▉         | 54/585 [00:15<02:36,  3.39it/s]  9%|▉         | 55/585 [00:16<02:36,  3.39it/s] 10%|▉         | 56/585 [00:16<02:35,  3.40it/s] 10%|▉         | 57/585 [00:16<02:35,  3.40it/s] 10%|▉         | 58/585 [00:17<02:34,  3.40it/s] 10%|█         | 59/585 [00:17<02:34,  3.40it/s] 10%|█         | 60/585 [00:17<02:34,  3.40it/s] 10%|█         | 61/585 [00:17<02:34,  3.38it/s] 11%|█         | 62/585 [00:18<02:34,  3.39it/s] 11%|█         | 63/585 [00:18<02:33,  3.40it/s] 11%|█         | 64/585 [00:18<02:33,  3.40it/s] 11%|█         | 65/585 [00:19<02:33,  3.40it/s] 11%|█▏        | 66/585 [00:19<02:32,  3.40it/s] 11%|█▏        | 67/585 [00:19<02:32,  3.40it/s] 12%|█▏        | 68/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 69/585 [00:20<02:31,  3.40it/s] 12%|█▏        | 70/585 [00:20<02:31,  3.41it/s] 12%|█▏        | 71/585 [00:20<02:30,  3.41it/s] 12%|█▏        | 72/585 [00:21<02:30,  3.41it/s] 12%|█▏        | 73/585 [00:21<02:30,  3.41it/s] 13%|█▎        | 74/585 [00:21<02:30,  3.41it/s] 13%|█▎        | 75/585 [00:22<02:29,  3.41it/s] 13%|█▎        | 76/585 [00:22<02:29,  3.41it/s] 13%|█▎        | 77/585 [00:22<02:29,  3.40it/s] 13%|█▎        | 78/585 [00:22<02:29,  3.40it/s] 14%|█▎        | 79/585 [00:23<02:28,  3.40it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.40it/s] 14%|█▍        | 81/585 [00:23<02:28,  3.40it/s] 14%|█▍        | 82/585 [00:24<02:28,  3.38it/s] 14%|█▍        | 83/585 [00:24<02:28,  3.39it/s] 14%|█▍        | 84/585 [00:24<02:27,  3.40it/s] 15%|█▍        | 85/585 [00:25<02:27,  3.40it/s] 15%|█▍        | 86/585 [00:25<02:26,  3.40it/s] 15%|█▍        | 87/585 [00:25<02:26,  3.40it/s] 15%|█▌        | 88/585 [00:25<02:26,  3.40it/s] 15%|█▌        | 89/585 [00:26<02:25,  3.40it/s] 15%|█▌        | 90/585 [00:26<02:25,  3.41it/s] 16%|█▌        | 91/585 [00:26<02:25,  3.40it/s] 16%|█▌        | 92/585 [00:27<02:24,  3.40it/s] 16%|█▌        | 93/585 [00:27<02:25,  3.38it/s] 16%|█▌        | 94/585 [00:27<02:24,  3.39it/s] 16%|█▌        | 95/585 [00:27<02:24,  3.39it/s] 16%|█▋        | 96/585 [00:28<02:24,  3.39it/s] 17%|█▋        | 97/585 [00:28<02:23,  3.40it/s] 17%|█▋        | 98/585 [00:28<02:23,  3.40it/s] 17%|█▋        | 99/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 100/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 101/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 102/585 [00:30<02:22,  3.40it/s] 18%|█▊        | 103/585 [00:30<02:22,  3.39it/s] 18%|█▊        | 104/585 [00:30<02:28,  3.25it/s] 18%|█▊        | 105/585 [00:30<02:25,  3.30it/s] 18%|█▊        | 106/585 [00:31<02:23,  3.34it/s] 18%|█▊        | 107/585 [00:31<02:21,  3.38it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.40it/s] 19%|█▊        | 109/585 [00:32<02:19,  3.41it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.42it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 113/585 [00:33<02:17,  3.44it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.44it/s] 20%|█▉        | 115/585 [00:33<02:20,  3.34it/s] 20%|█▉        | 116/585 [00:34<02:19,  3.37it/s] 20%|██        | 117/585 [00:34<02:17,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 19:29:54,981 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:29:54,982 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:29:54,982 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.68it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.55it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.84it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.95it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.56it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.32it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.05it/s][A
 10%|▉         | 42/437 [00:00<00:08, 45.07it/s][A
 11%|█         | 47/437 [00:01<00:08, 45.04it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.11it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.06it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.89it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.99it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.79it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.87it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.81it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.78it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.85it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.94it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.93it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.93it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.86it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.90it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.86it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.78it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.74it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.75it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.87it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.88it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.91it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 45.07it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.91it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.80it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.83it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.56it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.06it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.29it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.49it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.70it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.73it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.73it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.80it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.59it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.61it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.75it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.67it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.94it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.97it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.88it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.88it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.87it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.75it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.66it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.65it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.76it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.85it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.93it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.89it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.97it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.51it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.69it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.58it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.66it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.66it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.78it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.79it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.77it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.85it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.97it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.81it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.83it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.68it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.83it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.77it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.84it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.92it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.99it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.99it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.82it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.78it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.74it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.75it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.71it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.81it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.99it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.90it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.86it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.86it/s][A 20%|██        | 117/585 [00:44<02:17,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:30:04,812 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 19:30:04,838 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:30:10,733 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:30:10,798 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:30:10,810 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:57<55:33,  7.14s/it] 20%|██        | 119/585 [00:57<39:30,  5.09s/it] 21%|██        | 120/585 [00:58<28:16,  3.65s/it] 21%|██        | 121/585 [00:58<20:26,  2.64s/it] 21%|██        | 122/585 [00:58<14:57,  1.94s/it] 21%|██        | 123/585 [00:59<11:07,  1.44s/it] 21%|██        | 124/585 [00:59<08:26,  1.10s/it] 21%|██▏       | 125/585 [00:59<06:34,  1.17it/s] 22%|██▏       | 126/585 [00:59<05:15,  1.45it/s] 22%|██▏       | 127/585 [01:00<04:21,  1.75it/s] 22%|██▏       | 128/585 [01:00<03:42,  2.05it/s] 22%|██▏       | 129/585 [01:00<03:15,  2.33it/s] 22%|██▏       | 130/585 [01:01<02:58,  2.55it/s] 22%|██▏       | 131/585 [01:01<02:44,  2.76it/s] 23%|██▎       | 132/585 [01:01<02:34,  2.92it/s] 23%|██▎       | 133/585 [01:01<02:28,  3.05it/s] 23%|██▎       | 134/585 [01:02<02:23,  3.15it/s] 23%|██▎       | 135/585 [01:02<02:19,  3.23it/s] 23%|██▎       | 136/585 [01:02<02:17,  3.28it/s] 23%|██▎       | 137/585 [01:03<02:15,  3.31it/s] 24%|██▎       | 138/585 [01:03<02:13,  3.34it/s] 24%|██▍       | 139/585 [01:03<02:12,  3.36it/s] 24%|██▍       | 140/585 [01:04<02:11,  3.37it/s] 24%|██▍       | 141/585 [01:04<02:12,  3.35it/s] 24%|██▍       | 142/585 [01:04<02:11,  3.37it/s] 24%|██▍       | 143/585 [01:04<02:10,  3.38it/s] 25%|██▍       | 144/585 [01:05<02:10,  3.38it/s] 25%|██▍       | 145/585 [01:05<02:09,  3.39it/s] 25%|██▍       | 146/585 [01:05<02:09,  3.40it/s] 25%|██▌       | 147/585 [01:06<02:08,  3.40it/s] 25%|██▌       | 148/585 [01:06<02:08,  3.40it/s] 25%|██▌       | 149/585 [01:06<02:08,  3.40it/s] 26%|██▌       | 150/585 [01:06<02:07,  3.40it/s] 26%|██▌       | 151/585 [01:07<02:07,  3.40it/s] 26%|██▌       | 152/585 [01:07<02:08,  3.36it/s] 26%|██▌       | 153/585 [01:07<02:08,  3.37it/s] 26%|██▋       | 154/585 [01:08<02:07,  3.38it/s] 26%|██▋       | 155/585 [01:08<02:06,  3.39it/s] 27%|██▋       | 156/585 [01:08<02:06,  3.39it/s] 27%|██▋       | 157/585 [01:09<02:06,  3.40it/s] 27%|██▋       | 158/585 [01:09<02:05,  3.40it/s] 27%|██▋       | 159/585 [01:09<02:05,  3.40it/s] 27%|██▋       | 160/585 [01:09<02:05,  3.40it/s] 28%|██▊       | 161/585 [01:10<02:04,  3.40it/s] 28%|██▊       | 162/585 [01:10<02:04,  3.40it/s] 28%|██▊       | 163/585 [01:10<02:04,  3.39it/s] 28%|██▊       | 164/585 [01:11<02:04,  3.39it/s] 28%|██▊       | 165/585 [01:11<02:03,  3.40it/s] 28%|██▊       | 166/585 [01:11<02:03,  3.40it/s] 29%|██▊       | 167/585 [01:11<02:03,  3.40it/s] 29%|██▊       | 168/585 [01:12<02:02,  3.40it/s] 29%|██▉       | 169/585 [01:12<02:02,  3.40it/s] 29%|██▉       | 170/585 [01:12<02:02,  3.40it/s] 29%|██▉       | 171/585 [01:13<02:01,  3.40it/s] 29%|██▉       | 172/585 [01:13<02:01,  3.40it/s] 30%|██▉       | 173/585 [01:13<02:01,  3.40it/s] 30%|██▉       | 174/585 [01:14<02:01,  3.38it/s] 30%|██▉       | 175/585 [01:14<02:00,  3.40it/s] 30%|███       | 176/585 [01:14<02:00,  3.41it/s] 30%|███       | 177/585 [01:14<01:59,  3.42it/s] 30%|███       | 178/585 [01:15<01:58,  3.43it/s] 31%|███       | 179/585 [01:15<01:58,  3.44it/s] 31%|███       | 180/585 [01:15<01:57,  3.44it/s] 31%|███       | 181/585 [01:16<01:57,  3.44it/s] 31%|███       | 182/585 [01:16<01:56,  3.44it/s] 31%|███▏      | 183/585 [01:16<01:56,  3.44it/s] 31%|███▏      | 184/585 [01:16<01:56,  3.45it/s] 32%|███▏      | 185/585 [01:17<01:56,  3.42it/s] 32%|███▏      | 186/585 [01:17<01:56,  3.43it/s] 32%|███▏      | 187/585 [01:17<01:55,  3.44it/s] 32%|███▏      | 188/585 [01:18<01:55,  3.44it/s] 32%|███▏      | 189/585 [01:18<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:18<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:18<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:19<01:53,  3.45it/s] 33%|███▎      | 193/585 [01:19<01:53,  3.45it/s] 33%|███▎      | 194/585 [01:19<01:53,  3.45it/s] 33%|███▎      | 195/585 [01:20<01:53,  3.45it/s] 34%|███▎      | 196/585 [01:20<01:53,  3.42it/s] 34%|███▎      | 197/585 [01:20<01:53,  3.43it/s] 34%|███▍      | 198/585 [01:21<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:21<01:52,  3.44it/s] 34%|███▍      | 200/585 [01:21<01:51,  3.44it/s] 34%|███▍      | 201/585 [01:21<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:22<01:51,  3.45it/s] 35%|███▍      | 203/585 [01:22<01:50,  3.44it/s] 35%|███▍      | 204/585 [01:22<01:50,  3.44it/s] 35%|███▌      | 205/585 [01:23<01:50,  3.44it/s] 35%|███▌      | 206/585 [01:23<01:50,  3.44it/s] 35%|███▌      | 207/585 [01:23<01:49,  3.44it/s] 36%|███▌      | 208/585 [01:23<01:49,  3.44it/s] 36%|███▌      | 209/585 [01:24<01:49,  3.45it/s] 36%|███▌      | 210/585 [01:24<01:48,  3.45it/s] 36%|███▌      | 211/585 [01:24<01:48,  3.45it/s] 36%|███▌      | 212/585 [01:25<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:25<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:25<01:49,  3.39it/s] 37%|███▋      | 215/585 [01:25<01:48,  3.41it/s] 37%|███▋      | 216/585 [01:26<01:47,  3.42it/s] 37%|███▋      | 217/585 [01:26<01:47,  3.43it/s] 37%|███▋      | 218/585 [01:26<01:46,  3.43it/s] 37%|███▋      | 219/585 [01:27<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:27<01:46,  3.44it/s] 38%|███▊      | 221/585 [01:27<01:45,  3.44it/s] 38%|███▊      | 222/585 [01:28<01:45,  3.44it/s] 38%|███▊      | 223/585 [01:28<01:45,  3.45it/s] 38%|███▊      | 224/585 [01:28<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:28<01:45,  3.42it/s] 39%|███▊      | 226/585 [01:29<01:44,  3.43it/s] 39%|███▉      | 227/585 [01:29<01:44,  3.44it/s] 39%|███▉      | 228/585 [01:29<01:43,  3.44it/s] 39%|███▉      | 229/585 [01:30<01:43,  3.44it/s] 39%|███▉      | 230/585 [01:30<01:43,  3.45it/s] 39%|███▉      | 231/585 [01:30<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:30<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:31<01:42,  3.45it/s] 40%|████      | 234/585 [01:31<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 19:30:52,034 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:30:52,034 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:30:52,034 >>   Batch size = 8
{'eval_loss': 1.0108461380004883, 'eval_runtime': 9.7683, 'eval_samples_per_second': 357.587, 'eval_steps_per_second': 44.737, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.57it/s][A
  3%|▎         | 12/437 [00:00<00:09, 44.79it/s][A
  4%|▍         | 17/437 [00:00<00:09, 44.87it/s][A
  5%|▌         | 22/437 [00:00<00:09, 44.85it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.76it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.82it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.77it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.71it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.86it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.92it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.94it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.82it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.90it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.77it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.83it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.66it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.65it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.89it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.92it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.89it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.96it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.86it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.89it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.79it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.65it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.69it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.87it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.90it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.10it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.40it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.56it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.66it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.66it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.59it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.83it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.86it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.86it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.85it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.78it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.85it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.80it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.83it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.81it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.81it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.86it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.95it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.86it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.85it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.96it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.88it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.82it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.70it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.66it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.79it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.92it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 39.17it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 40.89it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 42.05it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.05it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.74it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.05it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.41it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.43it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.33it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.26it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.35it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.65it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.79it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.97it/s][A
 81%|████████  | 352/437 [00:07<00:01, 45.00it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 45.10it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.77it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.48it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.52it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.66it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.82it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.89it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.94it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.95it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.90it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.68it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.66it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.41it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.54it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.61it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.83it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.90it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:41<01:41,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 44.90it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:31:01,906 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 19:31:01,936 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:31:08,178 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:31:08,200 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:31:08,215 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:59<50:00,  8.57s/it] 40%|████      | 236/585 [01:59<35:28,  6.10s/it] 41%|████      | 237/585 [02:00<25:16,  4.36s/it] 41%|████      | 238/585 [02:00<18:08,  3.14s/it] 41%|████      | 239/585 [02:00<13:10,  2.28s/it] 41%|████      | 240/585 [02:00<09:42,  1.69s/it] 41%|████      | 241/585 [02:01<07:16,  1.27s/it] 41%|████▏     | 242/585 [02:01<05:34,  1.02it/s] 42%|████▏     | 243/585 [02:01<04:23,  1.30it/s] 42%|████▏     | 244/585 [02:02<03:34,  1.59it/s] 42%|████▏     | 245/585 [02:02<02:59,  1.90it/s] 42%|████▏     | 246/585 [02:02<02:34,  2.19it/s] 42%|████▏     | 247/585 [02:02<02:18,  2.45it/s] 42%|████▏     | 248/585 [02:03<02:05,  2.67it/s] 43%|████▎     | 249/585 [02:03<01:57,  2.86it/s] 43%|████▎     | 250/585 [02:03<01:51,  3.01it/s] 43%|████▎     | 251/585 [02:04<01:47,  3.11it/s] 43%|████▎     | 252/585 [02:04<01:44,  3.20it/s] 43%|████▎     | 253/585 [02:04<01:41,  3.26it/s] 43%|████▎     | 254/585 [02:04<01:40,  3.30it/s] 44%|████▎     | 255/585 [02:05<01:39,  3.33it/s] 44%|████▍     | 256/585 [02:05<01:37,  3.36it/s] 44%|████▍     | 257/585 [02:05<01:37,  3.37it/s] 44%|████▍     | 258/585 [02:06<01:36,  3.37it/s] 44%|████▍     | 259/585 [02:06<01:36,  3.38it/s] 44%|████▍     | 260/585 [02:06<01:35,  3.39it/s] 45%|████▍     | 261/585 [02:07<01:35,  3.40it/s] 45%|████▍     | 262/585 [02:07<01:34,  3.40it/s] 45%|████▍     | 263/585 [02:07<01:34,  3.40it/s] 45%|████▌     | 264/585 [02:07<01:34,  3.40it/s] 45%|████▌     | 265/585 [02:08<01:33,  3.40it/s] 45%|████▌     | 266/585 [02:08<01:33,  3.41it/s] 46%|████▌     | 267/585 [02:08<01:33,  3.40it/s] 46%|████▌     | 268/585 [02:09<01:33,  3.41it/s] 46%|████▌     | 269/585 [02:09<01:33,  3.40it/s] 46%|████▌     | 270/585 [02:09<01:32,  3.40it/s] 46%|████▋     | 271/585 [02:09<01:32,  3.40it/s] 46%|████▋     | 272/585 [02:10<01:31,  3.41it/s] 47%|████▋     | 273/585 [02:10<01:31,  3.41it/s] 47%|████▋     | 274/585 [02:10<01:31,  3.41it/s] 47%|████▋     | 275/585 [02:11<01:30,  3.41it/s] 47%|████▋     | 276/585 [02:11<01:30,  3.41it/s] 47%|████▋     | 277/585 [02:11<01:30,  3.41it/s] 48%|████▊     | 278/585 [02:12<01:30,  3.41it/s] 48%|████▊     | 279/585 [02:12<01:29,  3.41it/s] 48%|████▊     | 280/585 [02:12<01:30,  3.39it/s] 48%|████▊     | 281/585 [02:12<01:29,  3.39it/s] 48%|████▊     | 282/585 [02:13<01:29,  3.40it/s] 48%|████▊     | 283/585 [02:13<01:28,  3.40it/s] 49%|████▊     | 284/585 [02:13<01:28,  3.40it/s] 49%|████▊     | 285/585 [02:14<01:28,  3.40it/s] 49%|████▉     | 286/585 [02:14<01:27,  3.40it/s] 49%|████▉     | 287/585 [02:14<01:27,  3.40it/s] 49%|████▉     | 288/585 [02:14<01:27,  3.40it/s] 49%|████▉     | 289/585 [02:15<01:26,  3.40it/s] 50%|████▉     | 290/585 [02:15<01:26,  3.40it/s] 50%|████▉     | 291/585 [02:15<01:26,  3.39it/s] 50%|████▉     | 292/585 [02:16<01:26,  3.40it/s] 50%|█████     | 293/585 [02:16<01:25,  3.40it/s] 50%|█████     | 294/585 [02:16<01:25,  3.40it/s] 50%|█████     | 295/585 [02:17<01:25,  3.40it/s] 51%|█████     | 296/585 [02:17<01:24,  3.41it/s] 51%|█████     | 297/585 [02:17<01:24,  3.40it/s] 51%|█████     | 298/585 [02:17<01:24,  3.40it/s] 51%|█████     | 299/585 [02:18<01:23,  3.41it/s] 51%|█████▏    | 300/585 [02:18<01:23,  3.41it/s] 51%|█████▏    | 301/585 [02:18<01:23,  3.41it/s] 52%|█████▏    | 302/585 [02:19<01:24,  3.37it/s] 52%|█████▏    | 303/585 [02:19<01:23,  3.38it/s] 52%|█████▏    | 304/585 [02:19<01:22,  3.39it/s] 52%|█████▏    | 305/585 [02:19<01:22,  3.39it/s] 52%|█████▏    | 306/585 [02:20<01:22,  3.40it/s] 52%|█████▏    | 307/585 [02:20<01:21,  3.40it/s] 53%|█████▎    | 308/585 [02:20<01:21,  3.40it/s] 53%|█████▎    | 309/585 [02:21<01:21,  3.40it/s] 53%|█████▎    | 310/585 [02:21<01:20,  3.41it/s] 53%|█████▎    | 311/585 [02:21<01:20,  3.41it/s] 53%|█████▎    | 312/585 [02:22<01:20,  3.41it/s] 54%|█████▎    | 313/585 [02:22<01:21,  3.34it/s] 54%|█████▎    | 314/585 [02:22<01:20,  3.36it/s] 54%|█████▍    | 315/585 [02:22<01:20,  3.37it/s] 54%|█████▍    | 316/585 [02:23<01:19,  3.38it/s] 54%|█████▍    | 317/585 [02:23<01:19,  3.39it/s] 54%|█████▍    | 318/585 [02:23<01:18,  3.39it/s] 55%|█████▍    | 319/585 [02:24<01:18,  3.40it/s] 55%|█████▍    | 320/585 [02:24<01:17,  3.40it/s] 55%|█████▍    | 321/585 [02:24<01:17,  3.40it/s] 55%|█████▌    | 322/585 [02:24<01:17,  3.40it/s] 55%|█████▌    | 323/585 [02:25<01:16,  3.40it/s] 55%|█████▌    | 324/585 [02:25<01:16,  3.40it/s] 56%|█████▌    | 325/585 [02:25<01:16,  3.41it/s] 56%|█████▌    | 326/585 [02:26<01:15,  3.41it/s] 56%|█████▌    | 327/585 [02:26<01:15,  3.41it/s] 56%|█████▌    | 328/585 [02:26<01:15,  3.41it/s] 56%|█████▌    | 329/585 [02:27<01:15,  3.39it/s] 56%|█████▋    | 330/585 [02:27<01:15,  3.40it/s] 57%|█████▋    | 331/585 [02:27<01:14,  3.40it/s] 57%|█████▋    | 332/585 [02:27<01:14,  3.40it/s] 57%|█████▋    | 333/585 [02:28<01:14,  3.40it/s] 57%|█████▋    | 334/585 [02:28<01:13,  3.40it/s] 57%|█████▋    | 335/585 [02:28<01:13,  3.41it/s] 57%|█████▋    | 336/585 [02:29<01:13,  3.41it/s] 58%|█████▊    | 337/585 [02:29<01:12,  3.41it/s] 58%|█████▊    | 338/585 [02:29<01:12,  3.40it/s] 58%|█████▊    | 339/585 [02:29<01:12,  3.41it/s] 58%|█████▊    | 340/585 [02:30<01:12,  3.39it/s] 58%|█████▊    | 341/585 [02:30<01:11,  3.39it/s] 58%|█████▊    | 342/585 [02:30<01:11,  3.40it/s] 59%|█████▊    | 343/585 [02:31<01:11,  3.40it/s] 59%|█████▉    | 344/585 [02:31<01:10,  3.40it/s] 59%|█████▉    | 345/585 [02:31<01:10,  3.40it/s] 59%|█████▉    | 346/585 [02:32<01:10,  3.40it/s] 59%|█████▉    | 347/585 [02:32<01:09,  3.40it/s] 59%|█████▉    | 348/585 [02:32<01:09,  3.40it/s] 60%|█████▉    | 349/585 [02:32<01:09,  3.41it/s] 60%|█████▉    | 350/585 [02:33<01:08,  3.41it/s] 60%|██████    | 351/585 [02:33<01:09,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 19:31:54,072 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:31:54,072 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:31:54,072 >>   Batch size = 8
{'eval_loss': 1.019426703453064, 'eval_runtime': 9.8295, 'eval_samples_per_second': 355.358, 'eval_steps_per_second': 44.458, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.38it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.93it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.10it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.23it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.64it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.29it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.93it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.78it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.93it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.12it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.24it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.06it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 45.00it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.73it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.54it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.61it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.72it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.77it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.83it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 45.11it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 45.06it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 45.00it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.73it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.55it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.74it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.80it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.81it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.85it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.99it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 45.09it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 45.02it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.70it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.69it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.80it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.80it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.86it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.99it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 45.00it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 45.08it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 45.01it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.79it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.71it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.66it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.75it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.73it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.92it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 45.02it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 45.09it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.98it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.93it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.55it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 43.92it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.15it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.50it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.72it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.91it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.96it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.90it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.80it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.48it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.59it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.72it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.85it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.93it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.98it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.98it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.97it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.80it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.61it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.56it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.73it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.86it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 45.05it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 45.11it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 45.06it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.93it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.70it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.71it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.58it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.74it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.80it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.92it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.99it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 45.10it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.99it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.86it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.78it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.78it/s][A 60%|██████    | 351/585 [02:43<01:09,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:32:03,888 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 19:32:03,935 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:32:07,871 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:32:07,890 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:32:07,901 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:55<26:05,  6.72s/it] 60%|██████    | 353/585 [02:55<18:31,  4.79s/it] 61%|██████    | 354/585 [02:55<13:15,  3.44s/it] 61%|██████    | 355/585 [02:56<09:34,  2.50s/it] 61%|██████    | 356/585 [02:56<07:00,  1.84s/it] 61%|██████    | 357/585 [02:56<05:13,  1.37s/it] 61%|██████    | 358/585 [02:56<03:58,  1.05s/it] 61%|██████▏   | 359/585 [02:57<03:05,  1.22it/s] 62%|██████▏   | 360/585 [02:57<02:29,  1.51it/s] 62%|██████▏   | 361/585 [02:57<02:05,  1.79it/s] 62%|██████▏   | 362/585 [02:58<01:46,  2.09it/s] 62%|██████▏   | 363/585 [02:58<01:33,  2.36it/s] 62%|██████▏   | 364/585 [02:58<01:25,  2.58it/s] 62%|██████▏   | 365/585 [02:59<01:21,  2.70it/s] 63%|██████▎   | 366/585 [02:59<01:16,  2.88it/s] 63%|██████▎   | 367/585 [02:59<01:12,  3.02it/s] 63%|██████▎   | 368/585 [02:59<01:09,  3.13it/s] 63%|██████▎   | 369/585 [03:00<01:07,  3.21it/s] 63%|██████▎   | 370/585 [03:00<01:05,  3.27it/s] 63%|██████▎   | 371/585 [03:00<01:05,  3.28it/s] 64%|██████▎   | 372/585 [03:01<01:04,  3.31it/s] 64%|██████▍   | 373/585 [03:01<01:03,  3.36it/s] 64%|██████▍   | 374/585 [03:01<01:02,  3.38it/s] 64%|██████▍   | 375/585 [03:02<01:01,  3.40it/s] 64%|██████▍   | 376/585 [03:02<01:01,  3.42it/s] 64%|██████▍   | 377/585 [03:02<01:00,  3.43it/s] 65%|██████▍   | 378/585 [03:02<01:00,  3.44it/s] 65%|██████▍   | 379/585 [03:03<00:59,  3.45it/s] 65%|██████▍   | 380/585 [03:03<00:59,  3.45it/s] 65%|██████▌   | 381/585 [03:03<00:59,  3.45it/s] 65%|██████▌   | 382/585 [03:04<00:59,  3.43it/s] 65%|██████▌   | 383/585 [03:04<00:58,  3.43it/s] 66%|██████▌   | 384/585 [03:04<00:58,  3.44it/s] 66%|██████▌   | 385/585 [03:04<00:58,  3.45it/s] 66%|██████▌   | 386/585 [03:05<00:57,  3.45it/s] 66%|██████▌   | 387/585 [03:05<00:57,  3.45it/s] 66%|██████▋   | 388/585 [03:05<00:57,  3.45it/s] 66%|██████▋   | 389/585 [03:06<00:56,  3.45it/s] 67%|██████▋   | 390/585 [03:06<00:56,  3.45it/s] 67%|██████▋   | 391/585 [03:06<00:56,  3.46it/s] 67%|██████▋   | 392/585 [03:06<00:55,  3.45it/s] 67%|██████▋   | 393/585 [03:07<00:55,  3.44it/s] 67%|██████▋   | 394/585 [03:07<00:55,  3.44it/s] 68%|██████▊   | 395/585 [03:07<00:55,  3.45it/s] 68%|██████▊   | 396/585 [03:08<00:54,  3.45it/s] 68%|██████▊   | 397/585 [03:08<00:54,  3.45it/s] 68%|██████▊   | 398/585 [03:08<00:54,  3.45it/s] 68%|██████▊   | 399/585 [03:09<00:53,  3.45it/s] 68%|██████▊   | 400/585 [03:09<00:53,  3.45it/s] 69%|██████▊   | 401/585 [03:09<00:53,  3.45it/s] 69%|██████▊   | 402/585 [03:09<00:53,  3.45it/s] 69%|██████▉   | 403/585 [03:10<00:52,  3.45it/s] 69%|██████▉   | 404/585 [03:10<00:53,  3.41it/s] 69%|██████▉   | 405/585 [03:10<00:52,  3.42it/s] 69%|██████▉   | 406/585 [03:11<00:52,  3.43it/s] 70%|██████▉   | 407/585 [03:11<00:51,  3.44it/s] 70%|██████▉   | 408/585 [03:11<00:51,  3.44it/s] 70%|██████▉   | 409/585 [03:11<00:51,  3.45it/s] 70%|███████   | 410/585 [03:12<00:50,  3.45it/s] 70%|███████   | 411/585 [03:12<00:50,  3.45it/s] 70%|███████   | 412/585 [03:12<00:50,  3.45it/s] 71%|███████   | 413/585 [03:13<00:49,  3.45it/s] 71%|███████   | 414/585 [03:13<00:49,  3.45it/s] 71%|███████   | 415/585 [03:13<00:49,  3.42it/s] 71%|███████   | 416/585 [03:13<00:49,  3.43it/s] 71%|███████▏  | 417/585 [03:14<00:48,  3.44it/s] 71%|███████▏  | 418/585 [03:14<00:48,  3.44it/s] 72%|███████▏  | 419/585 [03:14<00:48,  3.45it/s] 72%|███████▏  | 420/585 [03:15<00:47,  3.45it/s] 72%|███████▏  | 421/585 [03:15<00:47,  3.45it/s] 72%|███████▏  | 422/585 [03:15<00:47,  3.45it/s] 72%|███████▏  | 423/585 [03:15<00:46,  3.45it/s] 72%|███████▏  | 424/585 [03:16<00:46,  3.45it/s] 73%|███████▎  | 425/585 [03:16<00:46,  3.45it/s] 73%|███████▎  | 426/585 [03:16<00:48,  3.29it/s] 73%|███████▎  | 427/585 [03:17<00:47,  3.33it/s] 73%|███████▎  | 428/585 [03:17<00:46,  3.37it/s] 73%|███████▎  | 429/585 [03:17<00:45,  3.39it/s] 74%|███████▎  | 430/585 [03:18<00:45,  3.41it/s] 74%|███████▎  | 431/585 [03:18<00:45,  3.42it/s] 74%|███████▍  | 432/585 [03:18<00:44,  3.43it/s] 74%|███████▍  | 433/585 [03:18<00:44,  3.44it/s] 74%|███████▍  | 434/585 [03:19<00:43,  3.44it/s] 74%|███████▍  | 435/585 [03:19<00:43,  3.44it/s] 75%|███████▍  | 436/585 [03:19<00:43,  3.45it/s] 75%|███████▍  | 437/585 [03:20<00:43,  3.43it/s] 75%|███████▍  | 438/585 [03:20<00:42,  3.44it/s] 75%|███████▌  | 439/585 [03:20<00:42,  3.44it/s] 75%|███████▌  | 440/585 [03:20<00:42,  3.45it/s] 75%|███████▌  | 441/585 [03:21<00:41,  3.45it/s] 76%|███████▌  | 442/585 [03:21<00:41,  3.45it/s] 76%|███████▌  | 443/585 [03:21<00:41,  3.45it/s] 76%|███████▌  | 444/585 [03:22<00:40,  3.45it/s] 76%|███████▌  | 445/585 [03:22<00:40,  3.45it/s] 76%|███████▌  | 446/585 [03:22<00:40,  3.45it/s] 76%|███████▋  | 447/585 [03:22<00:40,  3.45it/s] 77%|███████▋  | 448/585 [03:23<00:40,  3.41it/s] 77%|███████▋  | 449/585 [03:23<00:39,  3.42it/s] 77%|███████▋  | 450/585 [03:23<00:39,  3.43it/s] 77%|███████▋  | 451/585 [03:24<00:38,  3.44it/s] 77%|███████▋  | 452/585 [03:24<00:38,  3.44it/s] 77%|███████▋  | 453/585 [03:24<00:38,  3.44it/s] 78%|███████▊  | 454/585 [03:25<00:38,  3.45it/s] 78%|███████▊  | 455/585 [03:25<00:37,  3.45it/s] 78%|███████▊  | 456/585 [03:25<00:37,  3.45it/s] 78%|███████▊  | 457/585 [03:25<00:37,  3.45it/s] 78%|███████▊  | 458/585 [03:26<00:36,  3.45it/s] 78%|███████▊  | 459/585 [03:26<00:36,  3.45it/s] 79%|███████▊  | 460/585 [03:26<00:36,  3.45it/s] 79%|███████▉  | 461/585 [03:27<00:35,  3.45it/s] 79%|███████▉  | 462/585 [03:27<00:35,  3.45it/s] 79%|███████▉  | 463/585 [03:27<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:27<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:28<00:34,  3.45it/s] 80%|███████▉  | 466/585 [03:28<00:34,  3.44it/s] 80%|███████▉  | 467/585 [03:28<00:34,  3.44it/s] 80%|████████  | 468/585 [03:29<00:33,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 19:32:49,628 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:32:49,628 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:32:49,628 >>   Batch size = 8
{'eval_loss': 1.032065749168396, 'eval_runtime': 9.7578, 'eval_samples_per_second': 357.972, 'eval_steps_per_second': 44.785, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.20it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.66it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.94it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.92it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.47it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.19it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.98it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.84it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.99it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.08it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.09it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.10it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.85it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.80it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.81it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.65it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.51it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.69it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.78it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.82it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.85it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.82it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.82it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.67it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.72it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.69it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.77it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.90it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.96it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.94it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.82it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.89it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.79it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.64it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.69it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.75it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.97it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.91it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.89it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.73it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.84it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.79it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.71it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.60it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.71it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.90it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.91it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.98it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.92it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.75it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.71it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.63it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.66it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.68it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.72it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.84it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.95it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.97it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.98it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.77it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.75it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.61it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.66it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.75it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.93it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.84it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.86it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.87it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.88it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.79it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.69it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.54it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.78it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.88it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 45.03it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.99it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.93it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.87it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.73it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.65it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.61it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.82it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.97it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 45.03it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.99it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.95it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.95it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.95it/s][A 80%|████████  | 468/585 [03:38<00:33,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:32:59,416 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 19:32:59,436 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:33:02,754 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:33:02,807 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:33:02,818 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:53<14:44,  7.63s/it] 80%|████████  | 470/585 [03:54<10:25,  5.43s/it] 81%|████████  | 471/585 [03:54<07:23,  3.89s/it] 81%|████████  | 472/585 [03:54<05:17,  2.81s/it] 81%|████████  | 473/585 [03:55<03:50,  2.06s/it] 81%|████████  | 474/585 [03:55<02:49,  1.53s/it] 81%|████████  | 475/585 [03:55<02:07,  1.16s/it] 81%|████████▏ | 476/585 [03:55<01:37,  1.11it/s] 82%|████████▏ | 477/585 [03:56<01:17,  1.40it/s] 82%|████████▏ | 478/585 [03:56<01:03,  1.70it/s] 82%|████████▏ | 479/585 [03:56<00:53,  2.00it/s] 82%|████████▏ | 480/585 [03:57<00:46,  2.28it/s] 82%|████████▏ | 481/585 [03:57<00:41,  2.53it/s] 82%|████████▏ | 482/585 [03:57<00:37,  2.75it/s] 83%|████████▎ | 483/585 [03:57<00:34,  2.92it/s] 83%|████████▎ | 484/585 [03:58<00:33,  3.05it/s] 83%|████████▎ | 485/585 [03:58<00:31,  3.15it/s] 83%|████████▎ | 486/585 [03:58<00:31,  3.13it/s] 83%|████████▎ | 487/585 [03:59<00:33,  2.96it/s] 83%|████████▎ | 488/585 [03:59<00:31,  3.08it/s] 84%|████████▎ | 489/585 [03:59<00:30,  3.17it/s] 84%|████████▍ | 490/585 [04:00<00:29,  3.24it/s] 84%|████████▍ | 491/585 [04:00<00:28,  3.29it/s] 84%|████████▍ | 492/585 [04:00<00:27,  3.32it/s] 84%|████████▍ | 493/585 [04:01<00:27,  3.35it/s] 84%|████████▍ | 494/585 [04:01<00:27,  3.37it/s] 85%|████████▍ | 495/585 [04:01<00:26,  3.38it/s] 85%|████████▍ | 496/585 [04:01<00:26,  3.39it/s] 85%|████████▍ | 497/585 [04:02<00:26,  3.35it/s] 85%|████████▌ | 498/585 [04:02<00:25,  3.37it/s] 85%|████████▌ | 499/585 [04:02<00:25,  3.38it/s] 85%|████████▌ | 500/585 [04:03<00:25,  3.39it/s]                                                  85%|████████▌ | 500/585 [04:03<00:25,  3.39it/s] 86%|████████▌ | 501/585 [04:03<00:24,  3.39it/s] 86%|████████▌ | 502/585 [04:03<00:24,  3.40it/s] 86%|████████▌ | 503/585 [04:03<00:24,  3.40it/s] 86%|████████▌ | 504/585 [04:04<00:23,  3.40it/s] 86%|████████▋ | 505/585 [04:04<00:23,  3.41it/s] 86%|████████▋ | 506/585 [04:04<00:23,  3.40it/s] 87%|████████▋ | 507/585 [04:05<00:22,  3.40it/s] 87%|████████▋ | 508/585 [04:05<00:22,  3.39it/s] 87%|████████▋ | 509/585 [04:05<00:22,  3.40it/s] 87%|████████▋ | 510/585 [04:06<00:22,  3.40it/s] 87%|████████▋ | 511/585 [04:06<00:21,  3.40it/s] 88%|████████▊ | 512/585 [04:06<00:21,  3.40it/s] 88%|████████▊ | 513/585 [04:06<00:21,  3.40it/s] 88%|████████▊ | 514/585 [04:07<00:20,  3.40it/s] 88%|████████▊ | 515/585 [04:07<00:20,  3.40it/s] 88%|████████▊ | 516/585 [04:07<00:20,  3.40it/s] 88%|████████▊ | 517/585 [04:08<00:19,  3.40it/s] 89%|████████▊ | 518/585 [04:08<00:19,  3.41it/s] 89%|████████▊ | 519/585 [04:08<00:19,  3.39it/s] 89%|████████▉ | 520/585 [04:08<00:19,  3.39it/s] 89%|████████▉ | 521/585 [04:09<00:18,  3.40it/s] 89%|████████▉ | 522/585 [04:09<00:18,  3.40it/s] 89%|████████▉ | 523/585 [04:09<00:18,  3.40it/s] 90%|████████▉ | 524/585 [04:10<00:17,  3.40it/s] 90%|████████▉ | 525/585 [04:10<00:17,  3.40it/s] 90%|████████▉ | 526/585 [04:10<00:17,  3.41it/s] 90%|█████████ | 527/585 [04:11<00:17,  3.41it/s] 90%|█████████ | 528/585 [04:11<00:16,  3.41it/s] 90%|█████████ | 529/585 [04:11<00:16,  3.41it/s] 91%|█████████ | 530/585 [04:11<00:16,  3.39it/s] 91%|█████████ | 531/585 [04:12<00:15,  3.40it/s] 91%|█████████ | 532/585 [04:12<00:15,  3.40it/s] 91%|█████████ | 533/585 [04:12<00:15,  3.40it/s] 91%|█████████▏| 534/585 [04:13<00:14,  3.40it/s] 91%|█████████▏| 535/585 [04:13<00:14,  3.40it/s] 92%|█████████▏| 536/585 [04:13<00:14,  3.41it/s] 92%|█████████▏| 537/585 [04:13<00:14,  3.40it/s] 92%|█████████▏| 538/585 [04:14<00:13,  3.40it/s] 92%|█████████▏| 539/585 [04:14<00:13,  3.41it/s] 92%|█████████▏| 540/585 [04:14<00:13,  3.40it/s] 92%|█████████▏| 541/585 [04:15<00:13,  3.31it/s] 93%|█████████▎| 542/585 [04:15<00:12,  3.34it/s] 93%|█████████▎| 543/585 [04:15<00:12,  3.36it/s] 93%|█████████▎| 544/585 [04:16<00:12,  3.37it/s] 93%|█████████▎| 545/585 [04:16<00:11,  3.38it/s] 93%|█████████▎| 546/585 [04:16<00:11,  3.39it/s] 94%|█████████▎| 547/585 [04:16<00:11,  3.40it/s] 94%|█████████▎| 548/585 [04:17<00:10,  3.39it/s] 94%|█████████▍| 549/585 [04:17<00:10,  3.40it/s] 94%|█████████▍| 550/585 [04:17<00:10,  3.40it/s] 94%|█████████▍| 551/585 [04:18<00:09,  3.40it/s] 94%|█████████▍| 552/585 [04:18<00:09,  3.33it/s] 95%|█████████▍| 553/585 [04:18<00:09,  3.35it/s] 95%|█████████▍| 554/585 [04:18<00:09,  3.37it/s] 95%|█████████▍| 555/585 [04:19<00:08,  3.38it/s] 95%|█████████▌| 556/585 [04:19<00:08,  3.39it/s] 95%|█████████▌| 557/585 [04:19<00:08,  3.39it/s] 95%|█████████▌| 558/585 [04:20<00:07,  3.40it/s] 96%|█████████▌| 559/585 [04:20<00:07,  3.40it/s] 96%|█████████▌| 560/585 [04:20<00:07,  3.40it/s] 96%|█████████▌| 561/585 [04:21<00:07,  3.39it/s] 96%|█████████▌| 562/585 [04:21<00:06,  3.40it/s] 96%|█████████▌| 563/585 [04:21<00:06,  3.38it/s] 96%|█████████▋| 564/585 [04:21<00:06,  3.39it/s] 97%|█████████▋| 565/585 [04:22<00:05,  3.39it/s] 97%|█████████▋| 566/585 [04:22<00:05,  3.39it/s] 97%|█████████▋| 567/585 [04:22<00:05,  3.40it/s] 97%|█████████▋| 568/585 [04:23<00:04,  3.40it/s] 97%|█████████▋| 569/585 [04:23<00:04,  3.40it/s] 97%|█████████▋| 570/585 [04:23<00:04,  3.40it/s] 98%|█████████▊| 571/585 [04:23<00:04,  3.40it/s] 98%|█████████▊| 572/585 [04:24<00:03,  3.40it/s] 98%|█████████▊| 573/585 [04:24<00:03,  3.41it/s] 98%|█████████▊| 574/585 [04:24<00:03,  3.39it/s] 98%|█████████▊| 575/585 [04:25<00:02,  3.39it/s] 98%|█████████▊| 576/585 [04:25<00:02,  3.40it/s] 99%|█████████▊| 577/585 [04:25<00:02,  3.39it/s] 99%|█████████▉| 578/585 [04:26<00:02,  3.40it/s] 99%|█████████▉| 579/585 [04:26<00:01,  3.40it/s] 99%|█████████▉| 580/585 [04:26<00:01,  3.40it/s] 99%|█████████▉| 581/585 [04:26<00:01,  3.40it/s] 99%|█████████▉| 582/585 [04:27<00:00,  3.40it/s]100%|█████████▉| 583/585 [04:27<00:00,  3.40it/s]100%|█████████▉| 584/585 [04:27<00:00,  3.40it/s]100%|██████████| 585/585 [04:28<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 19:33:48,616 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:33:48,616 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:33:48,616 >>   Batch size = 8
{'eval_loss': 1.043594479560852, 'eval_runtime': 9.7623, 'eval_samples_per_second': 357.805, 'eval_steps_per_second': 44.764, 'epoch': 4.0}
{'loss': 0.6382, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.72it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.81it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.80it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.10it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.68it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.10it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.96it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.96it/s][A
 11%|█         | 47/437 [00:01<00:08, 45.02it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.12it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.10it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.98it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.89it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.35it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.54it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.60it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.62it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.80it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 45.01it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.91it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.81it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.70it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.78it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.73it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.69it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.76it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.94it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 45.02it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.97it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.79it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.68it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.70it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.75it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.82it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.77it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.89it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 45.09it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 45.10it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.96it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.80it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.67it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.66it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.77it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.82it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.88it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.90it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.99it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.94it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.85it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.77it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.81it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.85it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.76it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.92it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.84it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 45.00it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.96it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.68it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.72it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.84it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.90it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.84it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.89it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.98it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.94it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.83it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.72it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.76it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.77it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.81it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.88it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.88it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.97it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.88it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.85it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.79it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.82it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.78it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.72it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.91it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 45.02it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 45.07it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.88it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.80it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.78it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.71it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.80it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.80it/s][A100%|██████████| 585/585 [04:37<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:33:58,400 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 19:33:58,436 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:34:02,008 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:34:02,035 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:34:02,042 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 19:34:12,346 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 19:34:12,356 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117 (score: 1.0108461380004883).
                                                 100%|██████████| 585/585 [04:58<00:00,  3.42it/s]100%|██████████| 585/585 [04:58<00:00,  1.96it/s]
[INFO|trainer.py:1894] 2023-08-28 19:34:19,079 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 19:34:19,092 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:34:25,635 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:34:25,695 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:34:25,703 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:34:26,175 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:26,175 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:26,175 >>   train_loss               =     0.6339
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:26,175 >>   train_runtime            = 0:04:58.55
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:26,175 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:26,175 >>   train_samples_per_second =    125.606
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:26,175 >>   train_steps_per_second   =      1.959
{'eval_loss': 1.0469757318496704, 'eval_runtime': 9.7423, 'eval_samples_per_second': 358.54, 'eval_steps_per_second': 44.856, 'epoch': 5.0}
{'train_runtime': 298.5526, 'train_samples_per_second': 125.606, 'train_steps_per_second': 1.959, 'train_loss': 0.6339294955261752, 'epoch': 5.0}
08/28/2023 19:34:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 19:34:26,271 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:34:26,271 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 19:34:26,271 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 56.39it/s]  3%|▎         | 12/437 [00:00<00:08, 49.35it/s]  4%|▍         | 17/437 [00:00<00:08, 47.93it/s]  5%|▌         | 22/437 [00:00<00:08, 46.94it/s]  6%|▌         | 27/437 [00:00<00:08, 46.48it/s]  7%|▋         | 32/437 [00:00<00:08, 46.18it/s]  8%|▊         | 37/437 [00:00<00:08, 45.96it/s] 10%|▉         | 42/437 [00:00<00:08, 45.53it/s] 11%|█         | 47/437 [00:01<00:08, 45.04it/s] 12%|█▏        | 52/437 [00:01<00:08, 45.04it/s] 13%|█▎        | 57/437 [00:01<00:08, 45.04it/s] 14%|█▍        | 62/437 [00:01<00:08, 45.14it/s] 15%|█▌        | 67/437 [00:01<00:08, 45.32it/s] 16%|█▋        | 72/437 [00:01<00:08, 45.46it/s] 18%|█▊        | 77/437 [00:01<00:07, 45.39it/s] 19%|█▉        | 82/437 [00:01<00:07, 45.43it/s] 20%|█▉        | 87/437 [00:01<00:07, 45.11it/s] 21%|██        | 92/437 [00:02<00:07, 44.99it/s] 22%|██▏       | 97/437 [00:02<00:07, 44.85it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.98it/s] 24%|██▍       | 107/437 [00:02<00:07, 45.08it/s] 26%|██▌       | 112/437 [00:02<00:07, 45.16it/s] 27%|██▋       | 117/437 [00:02<00:07, 45.24it/s] 28%|██▊       | 122/437 [00:02<00:06, 45.34it/s] 29%|██▉       | 127/437 [00:02<00:06, 45.24it/s] 30%|███       | 132/437 [00:02<00:06, 45.09it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.97it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.80it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.96it/s] 35%|███▍      | 152/437 [00:03<00:06, 45.01it/s] 36%|███▌      | 157/437 [00:03<00:06, 45.13it/s] 37%|███▋      | 162/437 [00:03<00:06, 45.23it/s] 38%|███▊      | 167/437 [00:03<00:05, 45.26it/s] 39%|███▉      | 172/437 [00:03<00:05, 45.22it/s] 41%|████      | 177/437 [00:03<00:05, 44.97it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.90it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.93it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.99it/s] 45%|████▌     | 197/437 [00:04<00:05, 45.02it/s] 46%|████▌     | 202/437 [00:04<00:05, 45.19it/s] 47%|████▋     | 207/437 [00:04<00:05, 45.18it/s] 49%|████▊     | 212/437 [00:04<00:04, 45.40it/s] 50%|████▉     | 217/437 [00:04<00:04, 45.24it/s] 51%|█████     | 222/437 [00:04<00:04, 45.06it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.95it/s] 53%|█████▎    | 232/437 [00:05<00:04, 45.02it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.94it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.98it/s] 57%|█████▋    | 247/437 [00:05<00:04, 45.10it/s] 58%|█████▊    | 252/437 [00:05<00:04, 45.21it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.73it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.86it/s] 61%|██████    | 267/437 [00:05<00:03, 44.78it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.78it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.79it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.81it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.87it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.95it/s] 68%|██████▊   | 297/437 [00:06<00:03, 45.07it/s] 69%|██████▉   | 302/437 [00:06<00:02, 45.21it/s] 70%|███████   | 307/437 [00:06<00:02, 45.11it/s] 71%|███████▏  | 312/437 [00:06<00:02, 45.02it/s] 73%|███████▎  | 317/437 [00:07<00:02, 45.04it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.96it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.92it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.95it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.96it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.99it/s] 79%|███████▉  | 347/437 [00:07<00:01, 45.15it/s] 81%|████████  | 352/437 [00:07<00:01, 45.15it/s] 82%|████████▏ | 357/437 [00:07<00:01, 45.02it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.92it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.87it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.87it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.88it/s] 87%|████████▋ | 382/437 [00:08<00:01, 45.04it/s] 89%|████████▊ | 387/437 [00:08<00:01, 45.04it/s] 90%|████████▉ | 392/437 [00:08<00:00, 45.18it/s] 91%|█████████ | 397/437 [00:08<00:00, 45.18it/s] 92%|█████████▏| 402/437 [00:08<00:00, 45.13it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.97it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.86it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.85it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.90it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.99it/s] 99%|█████████▉| 432/437 [00:09<00:00, 45.05it/s]100%|██████████| 437/437 [00:09<00:00, 45.17it/s]100%|██████████| 437/437 [00:09<00:00, 45.16it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 19:34:35,965 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:35,965 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:35,965 >>   eval_loss               =     1.0108
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:35,965 >>   eval_runtime            = 0:00:09.69
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:35,965 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:35,965 >>   eval_samples_per_second =    360.322
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:35,965 >>   eval_steps_per_second   =     45.079
[INFO|trainer_pt_utils.py:913] 2023-08-28 19:34:35,966 >>   perplexity              =     2.7479
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:42,538 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:42,551 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:42,551 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:42,551 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:42,551 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:34:42,900 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:34:42,901 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:34:43,583 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:34:44,654 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:34:44,654 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:46,881 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:46,891 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:46,892 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:46,892 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:34:46,892 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:34:47,260 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:34:47,261 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:34:47,523 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:34:47,724 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:34:47,724 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.51it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:08,  1.57it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:09,  1.61it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:11,  1.61it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:13,  1.55it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:14,  1.60it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:16,  1.61it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:17,  1.61it/s]Extractor Predicting: 28it [00:18,  1.57it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.64it/s]Extractor Predicting: 31it [00:19,  1.65it/s]Extractor Predicting: 32it [00:20,  1.64it/s]Extractor Predicting: 33it [00:21,  1.61it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.58it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.63it/s]Extractor Predicting: 42it [00:26,  1.64it/s]Extractor Predicting: 43it [00:27,  1.60it/s]Extractor Predicting: 44it [00:27,  1.65it/s]Extractor Predicting: 45it [00:28,  1.61it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:29,  1.60it/s]Extractor Predicting: 48it [00:30,  1.66it/s]Extractor Predicting: 49it [00:30,  1.68it/s]Extractor Predicting: 50it [00:31,  1.64it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:32,  1.64it/s]Extractor Predicting: 53it [00:33,  1.67it/s]Extractor Predicting: 54it [00:34,  1.62it/s]Extractor Predicting: 55it [00:34,  1.64it/s]Extractor Predicting: 56it [00:35,  1.61it/s]Extractor Predicting: 57it [00:35,  1.63it/s]Extractor Predicting: 58it [00:36,  1.61it/s]Extractor Predicting: 59it [00:37,  1.60it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.59it/s]Extractor Predicting: 62it [00:39,  1.59it/s]Extractor Predicting: 63it [00:39,  1.60it/s]Extractor Predicting: 64it [00:40,  1.56it/s]Extractor Predicting: 65it [00:41,  1.55it/s]Extractor Predicting: 66it [00:41,  1.56it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:42,  1.53it/s]Extractor Predicting: 69it [00:43,  1.54it/s]Extractor Predicting: 70it [00:44,  1.55it/s]Extractor Predicting: 71it [00:44,  1.57it/s]Extractor Predicting: 72it [00:45,  1.61it/s]Extractor Predicting: 73it [00:46,  1.60it/s]Extractor Predicting: 74it [00:46,  1.57it/s]Extractor Predicting: 75it [00:47,  1.61it/s]Extractor Predicting: 76it [00:47,  1.58it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:49,  1.56it/s]Extractor Predicting: 79it [00:49,  1.55it/s]Extractor Predicting: 80it [00:50,  1.55it/s]Extractor Predicting: 81it [00:51,  1.54it/s]Extractor Predicting: 82it [00:51,  1.55it/s]Extractor Predicting: 83it [00:52,  1.59it/s]Extractor Predicting: 84it [00:53,  1.57it/s]Extractor Predicting: 85it [00:53,  1.57it/s]Extractor Predicting: 86it [00:54,  1.41it/s]Extractor Predicting: 87it [00:55,  1.48it/s]Extractor Predicting: 88it [00:55,  1.50it/s]Extractor Predicting: 89it [00:56,  1.50it/s]Extractor Predicting: 90it [00:57,  1.48it/s]Extractor Predicting: 91it [00:57,  1.49it/s]Extractor Predicting: 92it [00:58,  1.52it/s]Extractor Predicting: 93it [00:59,  1.53it/s]Extractor Predicting: 94it [00:59,  1.55it/s]Extractor Predicting: 95it [01:00,  1.59it/s]Extractor Predicting: 96it [01:01,  1.60it/s]Extractor Predicting: 97it [01:01,  1.57it/s]Extractor Predicting: 98it [01:02,  1.58it/s]Extractor Predicting: 99it [01:02,  1.56it/s]Extractor Predicting: 100it [01:03,  1.59it/s]Extractor Predicting: 101it [01:04,  1.58it/s]Extractor Predicting: 102it [01:04,  1.55it/s]Extractor Predicting: 103it [01:05,  1.55it/s]Extractor Predicting: 104it [01:06,  1.55it/s]Extractor Predicting: 105it [01:06,  1.56it/s]Extractor Predicting: 106it [01:07,  1.54it/s]Extractor Predicting: 107it [01:08,  1.55it/s]Extractor Predicting: 108it [01:08,  1.54it/s]Extractor Predicting: 109it [01:09,  1.54it/s]Extractor Predicting: 110it [01:10,  1.53it/s]Extractor Predicting: 111it [01:10,  1.53it/s]Extractor Predicting: 112it [01:11,  1.53it/s]Extractor Predicting: 113it [01:12,  1.51it/s]Extractor Predicting: 114it [01:12,  1.50it/s]Extractor Predicting: 115it [01:13,  1.50it/s]Extractor Predicting: 116it [01:14,  1.51it/s]Extractor Predicting: 117it [01:14,  1.49it/s]Extractor Predicting: 118it [01:15,  1.50it/s]Extractor Predicting: 119it [01:16,  1.51it/s]Extractor Predicting: 120it [01:16,  1.52it/s]Extractor Predicting: 121it [01:17,  1.51it/s]Extractor Predicting: 122it [01:18,  1.50it/s]Extractor Predicting: 123it [01:18,  1.56it/s]Extractor Predicting: 124it [01:19,  1.56it/s]Extractor Predicting: 125it [01:19,  1.53it/s]Extractor Predicting: 126it [01:20,  1.53it/s]Extractor Predicting: 127it [01:21,  1.53it/s]Extractor Predicting: 128it [01:21,  1.54it/s]Extractor Predicting: 129it [01:22,  1.52it/s]Extractor Predicting: 130it [01:23,  1.52it/s]Extractor Predicting: 131it [01:23,  1.49it/s]Extractor Predicting: 132it [01:24,  1.48it/s]Extractor Predicting: 133it [01:25,  1.47it/s]Extractor Predicting: 134it [01:26,  1.46it/s]Extractor Predicting: 135it [01:26,  1.48it/s]Extractor Predicting: 136it [01:26,  1.82it/s]Extractor Predicting: 136it [01:26,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:23,283 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:23,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:23,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:23,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:23,286 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:36:23,652 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:36:23,653 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:36:23,916 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:36:24,990 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:36:24,991 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:26,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:26,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:26,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:26,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:36:26,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:36:27,162 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:36:27,166 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:36:27,425 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:36:27,595 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:36:27,596 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.7192982456140351,
  "recall": 0.09390208989407386,
  "score": 0.16611800455811596,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.79it/s]Extractor Predicting: 3it [00:01,  1.72it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.67it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.68it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.73it/s]Extractor Predicting: 33it [00:19,  1.75it/s]Extractor Predicting: 34it [00:20,  1.59it/s]Extractor Predicting: 35it [00:21,  1.60it/s]Extractor Predicting: 36it [00:21,  1.60it/s]Extractor Predicting: 37it [00:22,  1.60it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.71it/s]Extractor Predicting: 41it [00:24,  1.70it/s]Extractor Predicting: 42it [00:25,  1.70it/s]Extractor Predicting: 43it [00:25,  1.73it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:27,  1.69it/s]Extractor Predicting: 46it [00:27,  1.71it/s]Extractor Predicting: 47it [00:28,  1.72it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.71it/s]Extractor Predicting: 50it [00:30,  1.71it/s]Extractor Predicting: 51it [00:30,  1.76it/s]Extractor Predicting: 52it [00:31,  1.77it/s]Extractor Predicting: 53it [00:31,  1.73it/s]Extractor Predicting: 54it [00:32,  1.70it/s]Extractor Predicting: 55it [00:32,  1.72it/s]Extractor Predicting: 56it [00:33,  1.67it/s]Extractor Predicting: 57it [00:34,  1.67it/s]Extractor Predicting: 58it [00:34,  1.70it/s]Extractor Predicting: 59it [00:35,  1.67it/s]Extractor Predicting: 60it [00:36,  1.61it/s]Extractor Predicting: 61it [00:36,  1.64it/s]Extractor Predicting: 62it [00:37,  1.65it/s]Extractor Predicting: 63it [00:37,  1.62it/s]Extractor Predicting: 64it [00:38,  1.69it/s]Extractor Predicting: 65it [00:39,  1.67it/s]Extractor Predicting: 66it [00:39,  1.66it/s]Extractor Predicting: 67it [00:40,  1.68it/s]Extractor Predicting: 68it [00:40,  1.73it/s]Extractor Predicting: 69it [00:41,  1.75it/s]Extractor Predicting: 70it [00:41,  1.73it/s]Extractor Predicting: 71it [00:42,  1.73it/s]Extractor Predicting: 72it [00:43,  1.69it/s]Extractor Predicting: 73it [00:43,  1.72it/s]Extractor Predicting: 74it [00:44,  1.67it/s]Extractor Predicting: 75it [00:44,  1.67it/s]Extractor Predicting: 76it [00:45,  1.68it/s]Extractor Predicting: 77it [00:46,  1.66it/s]Extractor Predicting: 78it [00:46,  1.67it/s]Extractor Predicting: 79it [00:47,  1.67it/s]Extractor Predicting: 80it [00:47,  1.69it/s]Extractor Predicting: 81it [00:48,  1.69it/s]Extractor Predicting: 82it [00:49,  1.70it/s]Extractor Predicting: 83it [00:49,  1.62it/s]Extractor Predicting: 84it [00:50,  1.63it/s]Extractor Predicting: 85it [00:50,  1.62it/s]Extractor Predicting: 86it [00:51,  1.59it/s]Extractor Predicting: 87it [00:52,  1.65it/s]Extractor Predicting: 88it [00:52,  1.59it/s]Extractor Predicting: 89it [00:53,  1.58it/s]Extractor Predicting: 90it [00:54,  1.57it/s]Extractor Predicting: 91it [00:54,  1.58it/s]Extractor Predicting: 92it [00:55,  1.56it/s]Extractor Predicting: 93it [00:56,  1.54it/s]Extractor Predicting: 94it [00:56,  1.55it/s]Extractor Predicting: 95it [00:57,  1.43it/s]Extractor Predicting: 96it [00:58,  1.46it/s]Extractor Predicting: 97it [00:58,  1.47it/s]Extractor Predicting: 98it [00:59,  1.47it/s]Extractor Predicting: 99it [01:00,  1.47it/s]Extractor Predicting: 100it [01:00,  1.47it/s]Extractor Predicting: 101it [01:01,  1.48it/s]Extractor Predicting: 102it [01:02,  1.48it/s]Extractor Predicting: 103it [01:02,  1.52it/s]Extractor Predicting: 104it [01:03,  1.52it/s]Extractor Predicting: 105it [01:04,  1.54it/s]Extractor Predicting: 106it [01:04,  1.50it/s]Extractor Predicting: 107it [01:05,  1.51it/s]Extractor Predicting: 108it [01:06,  1.47it/s]Extractor Predicting: 109it [01:06,  1.47it/s]Extractor Predicting: 110it [01:07,  1.47it/s]Extractor Predicting: 111it [01:08,  1.50it/s]Extractor Predicting: 112it [01:08,  1.49it/s]Extractor Predicting: 113it [01:09,  1.49it/s]Extractor Predicting: 114it [01:10,  1.50it/s]Extractor Predicting: 115it [01:10,  1.49it/s]Extractor Predicting: 116it [01:11,  1.50it/s]Extractor Predicting: 117it [01:12,  1.51it/s]Extractor Predicting: 118it [01:12,  1.51it/s]Extractor Predicting: 119it [01:13,  1.54it/s]Extractor Predicting: 120it [01:14,  1.59it/s]Extractor Predicting: 121it [01:14,  1.58it/s]Extractor Predicting: 122it [01:15,  1.57it/s]Extractor Predicting: 123it [01:16,  1.57it/s]Extractor Predicting: 124it [01:16,  1.57it/s]Extractor Predicting: 125it [01:17,  1.60it/s]Extractor Predicting: 126it [01:17,  1.63it/s]Extractor Predicting: 127it [01:18,  1.63it/s]Extractor Predicting: 128it [01:19,  1.66it/s]Extractor Predicting: 129it [01:19,  1.63it/s]Extractor Predicting: 130it [01:20,  1.59it/s]Extractor Predicting: 131it [01:20,  1.60it/s]Extractor Predicting: 132it [01:21,  1.60it/s]Extractor Predicting: 133it [01:22,  1.62it/s]Extractor Predicting: 134it [01:22,  1.65it/s]Extractor Predicting: 135it [01:23,  1.62it/s]Extractor Predicting: 136it [01:23,  1.65it/s]Extractor Predicting: 137it [01:24,  1.67it/s]Extractor Predicting: 138it [01:25,  1.64it/s]Extractor Predicting: 139it [01:25,  1.65it/s]Extractor Predicting: 140it [01:26,  1.64it/s]Extractor Predicting: 141it [01:27,  1.57it/s]Extractor Predicting: 142it [01:27,  1.57it/s]Extractor Predicting: 143it [01:28,  1.58it/s]Extractor Predicting: 144it [01:29,  1.57it/s]Extractor Predicting: 145it [01:29,  1.56it/s]Extractor Predicting: 146it [01:30,  1.60it/s]Extractor Predicting: 147it [01:30,  1.61it/s]Extractor Predicting: 148it [01:31,  1.60it/s]Extractor Predicting: 149it [01:32,  1.60it/s]Extractor Predicting: 150it [01:32,  1.60it/s]Extractor Predicting: 151it [01:33,  1.54it/s]Extractor Predicting: 152it [01:34,  1.56it/s]Extractor Predicting: 153it [01:34,  1.56it/s]Extractor Predicting: 154it [01:35,  1.61it/s]Extractor Predicting: 155it [01:35,  1.57it/s]Extractor Predicting: 156it [01:36,  1.57it/s]Extractor Predicting: 157it [01:37,  1.55it/s]Extractor Predicting: 158it [01:37,  1.57it/s]Extractor Predicting: 159it [01:38,  1.55it/s]Extractor Predicting: 160it [01:39,  1.54it/s]Extractor Predicting: 161it [01:39,  1.53it/s]Extractor Predicting: 162it [01:40,  1.53it/s]Extractor Predicting: 163it [01:41,  1.53it/s]Extractor Predicting: 164it [01:41,  1.55it/s]Extractor Predicting: 165it [01:42,  1.55it/s]Extractor Predicting: 166it [01:43,  1.55it/s]Extractor Predicting: 167it [01:43,  1.53it/s]Extractor Predicting: 168it [01:44,  1.53it/s]Extractor Predicting: 169it [01:45,  1.53it/s]Extractor Predicting: 170it [01:45,  1.52it/s]Extractor Predicting: 171it [01:46,  1.53it/s]Extractor Predicting: 172it [01:47,  1.53it/s]Extractor Predicting: 173it [01:47,  1.52it/s]Extractor Predicting: 174it [01:48,  1.53it/s]Extractor Predicting: 175it [01:48,  1.58it/s]Extractor Predicting: 176it [01:49,  1.54it/s]Extractor Predicting: 177it [01:50,  1.41it/s]Extractor Predicting: 178it [01:51,  1.47it/s]Extractor Predicting: 179it [01:51,  1.52it/s]Extractor Predicting: 180it [01:52,  1.53it/s]Extractor Predicting: 181it [01:52,  1.55it/s]Extractor Predicting: 182it [01:53,  1.59it/s]Extractor Predicting: 183it [01:54,  1.54it/s]Extractor Predicting: 184it [01:54,  1.61it/s]Extractor Predicting: 185it [01:55,  1.60it/s]Extractor Predicting: 186it [01:56,  1.61it/s]Extractor Predicting: 187it [01:56,  1.61it/s]Extractor Predicting: 188it [01:57,  1.57it/s]Extractor Predicting: 189it [01:57,  1.56it/s]Extractor Predicting: 190it [01:58,  1.57it/s]Extractor Predicting: 191it [01:59,  1.55it/s]Extractor Predicting: 192it [01:59,  1.56it/s]Extractor Predicting: 193it [02:00,  1.59it/s]Extractor Predicting: 194it [02:01,  1.58it/s]Extractor Predicting: 195it [02:01,  1.56it/s]Extractor Predicting: 196it [02:02,  1.52it/s]Extractor Predicting: 197it [02:03,  1.55it/s]Extractor Predicting: 198it [02:03,  1.54it/s]Extractor Predicting: 199it [02:04,  1.57it/s]Extractor Predicting: 200it [02:05,  1.56it/s]Extractor Predicting: 201it [02:05,  1.58it/s]Extractor Predicting: 202it [02:06,  1.59it/s]Extractor Predicting: 203it [02:06,  1.59it/s]Extractor Predicting: 204it [02:07,  1.62it/s]Extractor Predicting: 205it [02:08,  1.61it/s]Extractor Predicting: 206it [02:08,  1.64it/s]Extractor Predicting: 207it [02:09,  1.61it/s]Extractor Predicting: 208it [02:10,  1.60it/s]Extractor Predicting: 209it [02:10,  1.60it/s]Extractor Predicting: 210it [02:11,  1.61it/s]Extractor Predicting: 211it [02:11,  1.59it/s]Extractor Predicting: 212it [02:12,  1.61it/s]Extractor Predicting: 213it [02:13,  1.62it/s]Extractor Predicting: 214it [02:13,  1.60it/s]Extractor Predicting: 215it [02:14,  1.61it/s]Extractor Predicting: 216it [02:14,  1.61it/s]Extractor Predicting: 217it [02:15,  1.59it/s]Extractor Predicting: 218it [02:16,  1.61it/s]Extractor Predicting: 219it [02:16,  1.57it/s]Extractor Predicting: 220it [02:17,  1.60it/s]Extractor Predicting: 221it [02:18,  1.63it/s]Extractor Predicting: 222it [02:18,  1.63it/s]Extractor Predicting: 223it [02:19,  1.59it/s]Extractor Predicting: 224it [02:19,  1.63it/s]Extractor Predicting: 225it [02:20,  1.61it/s]Extractor Predicting: 226it [02:21,  1.60it/s]Extractor Predicting: 227it [02:21,  1.60it/s]Extractor Predicting: 228it [02:22,  1.62it/s]Extractor Predicting: 229it [02:23,  1.59it/s]Extractor Predicting: 230it [02:23,  1.62it/s]Extractor Predicting: 231it [02:24,  1.63it/s]Extractor Predicting: 232it [02:24,  1.62it/s]Extractor Predicting: 233it [02:25,  1.63it/s]Extractor Predicting: 234it [02:26,  1.61it/s]Extractor Predicting: 235it [02:26,  1.59it/s]Extractor Predicting: 236it [02:27,  1.59it/s]Extractor Predicting: 237it [02:28,  1.59it/s]Extractor Predicting: 238it [02:28,  1.59it/s]Extractor Predicting: 239it [02:29,  1.57it/s]Extractor Predicting: 240it [02:29,  1.58it/s]Extractor Predicting: 241it [02:30,  1.58it/s]Extractor Predicting: 242it [02:31,  1.58it/s]Extractor Predicting: 243it [02:31,  1.58it/s]Extractor Predicting: 244it [02:32,  1.56it/s]Extractor Predicting: 245it [02:33,  1.59it/s]Extractor Predicting: 246it [02:33,  1.58it/s]Extractor Predicting: 247it [02:34,  1.59it/s]Extractor Predicting: 248it [02:35,  1.58it/s]Extractor Predicting: 249it [02:35,  1.61it/s]Extractor Predicting: 250it [02:36,  1.61it/s]Extractor Predicting: 251it [02:36,  1.64it/s]Extractor Predicting: 252it [02:37,  1.61it/s]Extractor Predicting: 253it [02:38,  1.60it/s]Extractor Predicting: 254it [02:38,  1.60it/s]Extractor Predicting: 255it [02:39,  1.60it/s]Extractor Predicting: 256it [02:40,  1.56it/s]Extractor Predicting: 257it [02:40,  1.42it/s]Extractor Predicting: 258it [02:41,  1.46it/s]Extractor Predicting: 259it [02:42,  1.53it/s]Extractor Predicting: 260it [02:42,  1.53it/s]Extractor Predicting: 261it [02:43,  1.57it/s]Extractor Predicting: 262it [02:44,  1.57it/s]Extractor Predicting: 263it [02:44,  1.56it/s]Extractor Predicting: 264it [02:45,  1.63it/s]Extractor Predicting: 265it [02:45,  1.62it/s]Extractor Predicting: 266it [02:46,  1.64it/s]Extractor Predicting: 267it [02:47,  1.62it/s]Extractor Predicting: 268it [02:47,  1.56it/s]Extractor Predicting: 269it [02:48,  1.57it/s]Extractor Predicting: 270it [02:49,  1.55it/s]Extractor Predicting: 271it [02:49,  1.53it/s]Extractor Predicting: 272it [02:50,  1.56it/s]Extractor Predicting: 273it [02:51,  1.53it/s]Extractor Predicting: 274it [02:51,  1.52it/s]Extractor Predicting: 275it [02:52,  1.50it/s]Extractor Predicting: 276it [02:52,  1.53it/s]Extractor Predicting: 277it [02:53,  1.53it/s]Extractor Predicting: 278it [02:54,  1.57it/s]Extractor Predicting: 279it [02:54,  1.56it/s]Extractor Predicting: 280it [02:55,  1.57it/s]Extractor Predicting: 281it [02:56,  1.59it/s]Extractor Predicting: 282it [02:56,  1.58it/s]Extractor Predicting: 283it [02:57,  1.57it/s]Extractor Predicting: 284it [02:58,  1.58it/s]Extractor Predicting: 285it [02:58,  1.57it/s]Extractor Predicting: 286it [02:59,  1.57it/s]Extractor Predicting: 287it [02:59,  1.54it/s]Extractor Predicting: 288it [03:00,  1.77it/s]Extractor Predicting: 288it [03:00,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:37,125 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:37,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:37,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:37,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:37,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:39:37,927 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:39:37,928 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:39:38,539 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:39:39,594 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:39:39,594 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:42,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:42,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:42,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:42,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:39:42,636 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:39:43,426 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:39:43,428 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:39:44,013 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:39:44,190 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:39:44,190 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.6482122260668973,
  "recall": 0.08144927536231884,
  "score": 0.14471481910647613,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  2.41it/s]Extractor Predicting: 3it [00:01,  2.08it/s]
[INFO|configuration_utils.py:515] 2023-08-28 19:39:46,554 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:39:46,554 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:39:46,559 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:39:46,559 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 19:39:46,561 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:39:53,645 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 19:39:53,649 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 19:39:53,663 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:39:53,664 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:39:53,670 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:39:53,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:39:53,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:39:53,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:39:53,676 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:39:53,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:39:53,676 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 2.0,
  "recall": 0.02,
  "score": 0.039603960396039604,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 19:39:53,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:39:54,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:39:55,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:39:55,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:39:56,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:39:57,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:39:57,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:39:58,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:39:59,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:39:59,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:00,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:00,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:01,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:02,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:03,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:04,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:04,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:05,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:05,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:06,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:07,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:16, 14.03s/it][WARNING|generation_utils.py:914] 2023-08-28 19:40:07,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:08,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:09,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:10,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:10,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:11,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:12,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:13,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:13,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:14,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:15,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:15,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:16,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:16,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:17,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:18,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:18,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:19,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:20,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:20,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:21,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<03:01, 13.97s/it][WARNING|generation_utils.py:914] 2023-08-28 19:40:22,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:22,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:23,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:23,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:24,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:24,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:25,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:26,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:26,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:27,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:27,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:28,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:29,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:29,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:30,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:30,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:31,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:32,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:32,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:33,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:33,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:34,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:35,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:41<02:47, 13.99s/it][WARNING|generation_utils.py:914] 2023-08-28 19:40:35,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:36,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:37,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:37,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:38,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:38,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:39,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:40,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:40,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:41,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:42,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:43,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:43,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:44,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:44,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:45,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:46,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:46,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:47,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:47,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:48,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:48,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:55<02:31, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-28 19:40:49,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:50,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:50,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:51,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:51,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:52,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:53,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:53,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:54,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:55,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:55,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:56,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:57,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:57,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:58,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:59,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:40:59,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:00,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:01,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:02,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:02,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:09<02:18, 13.88s/it][WARNING|generation_utils.py:914] 2023-08-28 19:41:03,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:04,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:04,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:05,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:05,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:06,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:07,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:07,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:08,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:08,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:09,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:10,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:10,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:11,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:11,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:12,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:13,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:13,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:14,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:15,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:22<02:00, 13.43s/it][WARNING|generation_utils.py:914] 2023-08-28 19:41:15,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:16,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:17,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:17,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:18,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:19,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:19,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:20,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:20,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:21,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:22,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:22,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:23,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:24,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:25,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:25,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:26,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:27,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:27,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:28,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:29,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:36<01:48, 13.61s/it][WARNING|generation_utils.py:914] 2023-08-28 19:41:29,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:30,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:31,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:31,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:32,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:33,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:33,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:34,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:35,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:35,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:36,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:36,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:37,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:37,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:38,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:38,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:39,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:40,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:40,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:41,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:42,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:42,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:49<01:34, 13.55s/it][WARNING|generation_utils.py:914] 2023-08-28 19:41:43,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:43,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:44,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:45,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:46,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:46,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:47,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:48,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:48,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:49,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:49,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:50,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:51,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:51,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:52,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:53,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:53,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:54,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:54,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:55,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:56,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:56,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:03<01:23, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 19:41:57,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:58,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:59,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:41:59,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:00,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:00,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:01,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:02,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:02,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:03,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:03,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:04,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:05,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:05,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:06,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:06,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:07,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:07,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:08,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:09,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:09,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:16<01:07, 13.43s/it][WARNING|generation_utils.py:914] 2023-08-28 19:42:10,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:10,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:11,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:12,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:12,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:13,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:13,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:14,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:15,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:15,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:16,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:17,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:17,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:18,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:19,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:19,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:20,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:20,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:21,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:22,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:22,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:23,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:30<00:53, 13.48s/it][WARNING|generation_utils.py:914] 2023-08-28 19:42:23,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:24,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:24,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:25,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:26,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:26,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:27,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:27,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:28,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:28,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:29,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:29,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:30,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:30,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:31,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:32,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:32,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:33,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:34,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:34,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:35,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:35,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:42<00:39, 13.19s/it][WARNING|generation_utils.py:914] 2023-08-28 19:42:36,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:37,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:37,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:38,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:39,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:39,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:40,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:40,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:41,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:41,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:42,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:43,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:43,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:44,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:44,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:45,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:47,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:47,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:48,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:48,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:55<00:26, 13.18s/it][WARNING|generation_utils.py:914] 2023-08-28 19:42:49,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:50,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:51,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:51,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:52,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:53,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:53,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:54,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:54,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:55,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:56,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:56,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:57,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:57,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:58,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:42:59,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:00,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:00,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:01,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:01,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:02,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:03,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:09<00:13, 13.43s/it][WARNING|generation_utils.py:914] 2023-08-28 19:43:03,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:04,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:04,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:05,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:05,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:06,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:07,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:07,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:08,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:08,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:09,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:10,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:10,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:11,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:11,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:12,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:13,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:13,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:14,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:14,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 19:43:15,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:22<00:00, 13.15s/it]Generating: 100%|██████████| 15/15 [03:22<00:00, 13.48s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:22,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:22,194 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:22,194 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:22,194 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:22,194 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:43:22,552 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:43:22,553 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:43:22,862 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:43:23,991 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:43:23,991 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:25,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:25,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:25,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:25,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:43:25,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:43:26,162 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:43:26,164 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:43:26,440 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:43:26,619 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:43:26,619 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9285714285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : Later in the year , the band formed with keyboardist and songwriter Billie Joe Armstrong in Los Angeles , bringing along their guitarist guitarist , Steve Jones , drummer Tim McGraw , bassist John Ritchie , drummer Mike Darnell , keyboardist John Hanks , and keyboardist Neil Young . Head Entity : William Henry Harrison , Tail Entity : guitar .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 630, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9077380952380952, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8920454545454546, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9270833333333334, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8806818181818182, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9609375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : work location .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 9644
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9744, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.15it/s]Extractor Estimating: 2it [00:01,  1.26it/s]Extractor Estimating: 3it [00:02,  1.40it/s]Extractor Estimating: 4it [00:02,  1.46it/s]Extractor Estimating: 5it [00:03,  1.44it/s]Extractor Estimating: 6it [00:04,  1.45it/s]Extractor Estimating: 7it [00:04,  1.46it/s]Extractor Estimating: 8it [00:05,  1.43it/s]Extractor Estimating: 9it [00:06,  1.44it/s]Extractor Estimating: 10it [00:06,  1.55it/s]Extractor Estimating: 11it [00:07,  1.56it/s]Extractor Estimating: 12it [00:08,  1.58it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:09,  1.54it/s]Extractor Estimating: 15it [00:10,  1.48it/s]Extractor Estimating: 16it [00:10,  1.47it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:12,  1.48it/s]Extractor Estimating: 19it [00:12,  1.45it/s]Extractor Estimating: 20it [00:13,  1.49it/s]Extractor Estimating: 21it [00:14,  1.47it/s]Extractor Estimating: 22it [00:14,  1.48it/s]Extractor Estimating: 23it [00:15,  1.42it/s]Extractor Estimating: 24it [00:16,  1.43it/s]Extractor Estimating: 25it [00:17,  1.43it/s]Extractor Estimating: 26it [00:17,  1.45it/s]Extractor Estimating: 27it [00:18,  1.45it/s]Extractor Estimating: 28it [00:19,  1.42it/s]Extractor Estimating: 29it [00:19,  1.50it/s]Extractor Estimating: 30it [00:20,  1.53it/s]Extractor Estimating: 31it [00:21,  1.50it/s]Extractor Estimating: 32it [00:21,  1.54it/s]Extractor Estimating: 33it [00:22,  1.45it/s]Extractor Estimating: 34it [00:23,  1.46it/s]Extractor Estimating: 35it [00:23,  1.50it/s]Extractor Estimating: 36it [00:24,  1.46it/s]Extractor Estimating: 37it [00:25,  1.51it/s]Extractor Estimating: 38it [00:26,  1.31it/s]Extractor Estimating: 39it [00:26,  1.34it/s]Extractor Estimating: 40it [00:27,  1.44it/s]Extractor Estimating: 41it [00:28,  1.47it/s]Extractor Estimating: 42it [00:28,  1.48it/s]Extractor Estimating: 43it [00:29,  1.44it/s]Extractor Estimating: 44it [00:30,  1.45it/s]Extractor Estimating: 45it [00:30,  1.48it/s]Extractor Estimating: 46it [00:31,  1.52it/s]Extractor Estimating: 47it [00:32,  1.53it/s]Extractor Estimating: 48it [00:32,  1.53it/s]Extractor Estimating: 49it [00:33,  1.52it/s]Extractor Estimating: 50it [00:33,  1.53it/s]Extractor Estimating: 51it [00:34,  1.54it/s]Extractor Estimating: 52it [00:35,  1.63it/s]Extractor Estimating: 53it [00:35,  1.51it/s]Extractor Estimating: 54it [00:36,  1.55it/s]Extractor Estimating: 55it [00:37,  1.60it/s]Extractor Estimating: 56it [00:37,  1.60it/s]Extractor Estimating: 57it [00:38,  1.58it/s]Extractor Estimating: 58it [00:39,  1.58it/s]Extractor Estimating: 59it [00:39,  1.58it/s]Extractor Estimating: 60it [00:40,  1.60it/s]Extractor Estimating: 61it [00:40,  1.66it/s]Extractor Estimating: 62it [00:41,  1.60it/s]Extractor Estimating: 63it [00:42,  1.66it/s]Extractor Estimating: 64it [00:42,  1.60it/s]Extractor Estimating: 65it [00:43,  1.63it/s]Extractor Estimating: 66it [00:43,  1.70it/s]Extractor Estimating: 67it [00:44,  1.65it/s]Extractor Estimating: 68it [00:45,  1.68it/s]Extractor Estimating: 69it [00:45,  1.70it/s]Extractor Estimating: 70it [00:46,  1.70it/s]Extractor Estimating: 71it [00:46,  1.65it/s]Extractor Estimating: 72it [00:47,  1.66it/s]Extractor Estimating: 73it [00:48,  1.61it/s]Extractor Estimating: 74it [00:48,  1.65it/s]Extractor Estimating: 75it [00:49,  1.65it/s]Extractor Estimating: 76it [00:49,  1.68it/s]Extractor Estimating: 77it [00:50,  1.73it/s]Extractor Estimating: 78it [00:50,  1.71it/s]Extractor Estimating: 79it [00:51,  1.69it/s]Extractor Estimating: 80it [00:52,  1.65it/s]Extractor Estimating: 81it [00:52,  1.70it/s]Extractor Estimating: 82it [00:53,  1.72it/s]Extractor Estimating: 83it [00:53,  1.71it/s]Extractor Estimating: 84it [00:54,  1.70it/s]Extractor Estimating: 85it [00:55,  1.72it/s]Extractor Estimating: 86it [00:55,  1.71it/s]Extractor Estimating: 87it [00:56,  1.76it/s]Extractor Estimating: 88it [00:56,  1.73it/s]Extractor Estimating: 89it [00:57,  1.73it/s]Extractor Estimating: 90it [00:57,  1.78it/s]Extractor Estimating: 91it [00:58,  1.71it/s]Extractor Estimating: 92it [00:59,  1.70it/s]Extractor Estimating: 93it [00:59,  1.73it/s]Extractor Estimating: 94it [01:00,  1.74it/s]Extractor Estimating: 95it [01:00,  1.75it/s]Extractor Estimating: 96it [01:01,  1.69it/s]Extractor Estimating: 97it [01:02,  1.69it/s]Extractor Estimating: 98it [01:02,  1.70it/s]Extractor Estimating: 99it [01:03,  1.71it/s]Extractor Estimating: 100it [01:03,  1.76it/s]Extractor Estimating: 101it [01:04,  1.71it/s]Extractor Estimating: 102it [01:04,  1.71it/s]Extractor Estimating: 103it [01:05,  1.68it/s]Extractor Estimating: 104it [01:06,  1.69it/s]Extractor Estimating: 105it [01:06,  1.66it/s]Extractor Estimating: 106it [01:07,  1.68it/s]Extractor Estimating: 107it [01:07,  1.67it/s]Extractor Estimating: 108it [01:08,  1.54it/s]Extractor Estimating: 109it [01:09,  1.63it/s]Extractor Estimating: 110it [01:09,  1.57it/s]Extractor Estimating: 111it [01:10,  1.58it/s]Extractor Estimating: 112it [01:11,  1.61it/s]Extractor Estimating: 113it [01:11,  1.64it/s]Extractor Estimating: 114it [01:12,  1.65it/s]Extractor Estimating: 115it [01:12,  1.66it/s]Extractor Estimating: 116it [01:13,  1.62it/s]Extractor Estimating: 117it [01:14,  1.65it/s]Extractor Estimating: 118it [01:14,  1.63it/s]Extractor Estimating: 119it [01:15,  1.65it/s]Extractor Estimating: 120it [01:15,  1.67it/s]Extractor Estimating: 121it [01:16,  1.65it/s]Extractor Estimating: 122it [01:17,  1.59it/s]Extractor Estimating: 123it [01:17,  1.63it/s]Extractor Estimating: 124it [01:18,  1.56it/s]Extractor Estimating: 125it [01:19,  1.48it/s]Extractor Estimating: 126it [01:20,  1.46it/s]Extractor Estimating: 127it [01:20,  1.51it/s]Extractor Estimating: 128it [01:21,  1.53it/s]Extractor Estimating: 129it [01:21,  1.62it/s]Extractor Estimating: 130it [01:22,  1.61it/s]Extractor Estimating: 131it [01:23,  1.63it/s]Extractor Estimating: 132it [01:23,  1.65it/s]Extractor Estimating: 133it [01:24,  1.64it/s]Extractor Estimating: 134it [01:24,  1.65it/s]Extractor Estimating: 135it [01:25,  1.52it/s]Extractor Estimating: 136it [01:26,  1.54it/s]Extractor Estimating: 137it [01:26,  1.59it/s]Extractor Estimating: 138it [01:27,  1.53it/s]Extractor Estimating: 139it [01:28,  1.61it/s]Extractor Estimating: 140it [01:28,  1.47it/s]Extractor Estimating: 141it [01:29,  1.52it/s]Extractor Estimating: 142it [01:30,  1.50it/s]Extractor Estimating: 143it [01:30,  1.51it/s]Extractor Estimating: 144it [01:31,  1.37it/s]Extractor Estimating: 145it [01:32,  1.37it/s]Extractor Estimating: 146it [01:33,  1.48it/s]Extractor Estimating: 147it [01:33,  1.48it/s]Extractor Estimating: 148it [01:34,  1.59it/s]Extractor Estimating: 149it [01:34,  1.61it/s]Extractor Estimating: 150it [01:35,  1.63it/s]Extractor Estimating: 151it [01:35,  1.67it/s]Extractor Estimating: 152it [01:36,  1.64it/s]Extractor Estimating: 153it [01:37,  1.57it/s]Extractor Estimating: 154it [01:37,  1.56it/s]Extractor Estimating: 155it [01:38,  1.63it/s]Extractor Estimating: 156it [01:39,  1.61it/s]Extractor Estimating: 157it [01:39,  1.62it/s]Extractor Estimating: 158it [01:40,  1.58it/s]Extractor Estimating: 159it [01:41,  1.56it/s]Extractor Estimating: 160it [01:41,  1.59it/s]Extractor Estimating: 161it [01:42,  1.56it/s]Extractor Estimating: 162it [01:42,  1.62it/s]Extractor Estimating: 163it [01:43,  1.60it/s]Extractor Estimating: 164it [01:44,  1.45it/s]Extractor Estimating: 165it [01:45,  1.48it/s]Extractor Estimating: 166it [01:45,  1.45it/s]Extractor Estimating: 167it [01:46,  1.47it/s]Extractor Estimating: 168it [01:47,  1.47it/s]Extractor Estimating: 169it [01:47,  1.51it/s]Extractor Estimating: 170it [01:48,  1.53it/s]Extractor Estimating: 171it [01:48,  1.62it/s]Extractor Estimating: 172it [01:49,  1.60it/s]Extractor Estimating: 173it [01:50,  1.60it/s]Extractor Estimating: 174it [01:50,  1.61it/s]Extractor Estimating: 175it [01:51,  1.58it/s]Extractor Estimating: 176it [01:52,  1.62it/s]Extractor Estimating: 177it [01:52,  1.64it/s]Extractor Estimating: 178it [01:53,  1.65it/s]Extractor Estimating: 179it [01:53,  1.67it/s]Extractor Estimating: 180it [01:54,  1.66it/s]Extractor Estimating: 181it [01:55,  1.55it/s]Extractor Estimating: 182it [01:55,  1.55it/s]Extractor Estimating: 183it [01:56,  1.57it/s]Extractor Estimating: 184it [01:57,  1.58it/s]Extractor Estimating: 185it [01:57,  1.67it/s]Extractor Estimating: 186it [01:58,  1.70it/s]Extractor Estimating: 187it [01:58,  1.71it/s]Extractor Estimating: 188it [01:59,  1.74it/s]Extractor Estimating: 189it [01:59,  1.72it/s]Extractor Estimating: 190it [02:00,  1.64it/s]Extractor Estimating: 191it [02:01,  1.67it/s]Extractor Estimating: 192it [02:01,  1.67it/s]Extractor Estimating: 193it [02:02,  1.65it/s]Extractor Estimating: 194it [02:02,  1.64it/s]Extractor Estimating: 195it [02:03,  1.65it/s]Extractor Estimating: 196it [02:04,  1.62it/s]Extractor Estimating: 197it [02:04,  1.65it/s]Extractor Estimating: 198it [02:05,  1.67it/s]Extractor Estimating: 199it [02:05,  1.74it/s]Extractor Estimating: 200it [02:06,  1.75it/s]Extractor Estimating: 201it [02:07,  1.70it/s]Extractor Estimating: 202it [02:07,  1.64it/s]Extractor Estimating: 203it [02:08,  1.60it/s]Extractor Estimating: 204it [02:09,  1.58it/s]Extractor Estimating: 205it [02:09,  1.56it/s]Extractor Estimating: 206it [02:10,  1.60it/s]Extractor Estimating: 207it [02:10,  1.60it/s]Extractor Estimating: 208it [02:11,  1.68it/s]Extractor Estimating: 209it [02:12,  1.66it/s]Extractor Estimating: 210it [02:12,  1.64it/s]Extractor Estimating: 211it [02:13,  1.68it/s]Extractor Estimating: 212it [02:13,  1.67it/s]Extractor Estimating: 213it [02:14,  1.59it/s]Extractor Estimating: 214it [02:15,  1.61it/s]Extractor Estimating: 215it [02:15,  1.61it/s]Extractor Estimating: 216it [02:16,  1.65it/s]Extractor Estimating: 217it [02:16,  1.62it/s]Extractor Estimating: 218it [02:17,  1.63it/s]Extractor Estimating: 219it [02:18,  1.57it/s]Extractor Estimating: 220it [02:18,  1.61it/s]Extractor Estimating: 221it [02:19,  1.50it/s]Extractor Estimating: 222it [02:20,  1.54it/s]Extractor Estimating: 223it [02:20,  1.55it/s]Extractor Estimating: 224it [02:21,  1.52it/s]Extractor Estimating: 225it [02:22,  1.56it/s]Extractor Estimating: 226it [02:22,  1.66it/s]Extractor Estimating: 227it [02:23,  1.71it/s]Extractor Estimating: 228it [02:23,  1.75it/s]Extractor Estimating: 229it [02:24,  1.77it/s]Extractor Estimating: 230it [02:25,  1.44it/s]Extractor Estimating: 231it [02:25,  1.54it/s]Extractor Estimating: 232it [02:26,  1.62it/s]Extractor Estimating: 233it [02:26,  1.70it/s]Extractor Estimating: 234it [02:27,  1.70it/s]Extractor Estimating: 235it [02:28,  1.57it/s]Extractor Estimating: 236it [02:28,  1.59it/s]Extractor Estimating: 237it [02:29,  1.66it/s]Extractor Estimating: 238it [02:29,  1.69it/s]Extractor Estimating: 239it [02:30,  1.64it/s]Extractor Estimating: 240it [02:31,  1.58it/s]Extractor Estimating: 241it [02:31,  1.67it/s]Extractor Estimating: 242it [02:32,  1.74it/s]Extractor Estimating: 243it [02:32,  1.76it/s]Extractor Estimating: 244it [02:33,  1.81it/s]Extractor Estimating: 245it [02:33,  1.85it/s]Extractor Estimating: 246it [02:34,  1.83it/s]Extractor Estimating: 247it [02:35,  1.83it/s]Extractor Estimating: 248it [02:35,  1.80it/s]Extractor Estimating: 249it [02:36,  1.81it/s]Extractor Estimating: 250it [02:36,  1.81it/s]Extractor Estimating: 251it [02:37,  1.81it/s]Extractor Estimating: 252it [02:38,  1.63it/s]Extractor Estimating: 253it [02:38,  1.66it/s]Extractor Estimating: 254it [02:39,  1.67it/s]Extractor Estimating: 255it [02:39,  1.76it/s]Extractor Estimating: 256it [02:40,  1.75it/s]Extractor Estimating: 257it [02:40,  1.77it/s]Extractor Estimating: 258it [02:41,  1.79it/s]Extractor Estimating: 259it [02:41,  1.74it/s]Extractor Estimating: 260it [02:42,  1.71it/s]Extractor Estimating: 261it [02:43,  1.69it/s]Extractor Estimating: 262it [02:43,  1.68it/s]Extractor Estimating: 263it [02:44,  1.65it/s]Extractor Estimating: 264it [02:45,  1.65it/s]Extractor Estimating: 265it [02:45,  1.68it/s]Extractor Estimating: 266it [02:46,  1.70it/s]Extractor Estimating: 267it [02:46,  1.76it/s]Extractor Estimating: 268it [02:47,  1.76it/s]Extractor Estimating: 269it [02:47,  1.74it/s]Extractor Estimating: 270it [02:48,  1.69it/s]Extractor Estimating: 271it [02:49,  1.70it/s]Extractor Estimating: 272it [02:49,  1.70it/s]Extractor Estimating: 273it [02:50,  1.70it/s]Extractor Estimating: 274it [02:50,  1.76it/s]Extractor Estimating: 275it [02:51,  1.81it/s]Extractor Estimating: 276it [02:51,  1.86it/s]Extractor Estimating: 277it [02:52,  1.85it/s]Extractor Estimating: 278it [02:52,  1.83it/s]Extractor Estimating: 279it [02:53,  1.90it/s]Extractor Estimating: 280it [02:53,  1.86it/s]Extractor Estimating: 281it [02:54,  1.84it/s]Extractor Estimating: 282it [02:55,  1.85it/s]Extractor Estimating: 283it [02:55,  1.85it/s]Extractor Estimating: 284it [02:56,  1.87it/s]Extractor Estimating: 285it [02:56,  1.89it/s]Extractor Estimating: 286it [02:57,  1.91it/s]Extractor Estimating: 287it [02:57,  1.89it/s]Extractor Estimating: 288it [02:58,  1.90it/s]Extractor Estimating: 289it [02:58,  1.84it/s]Extractor Estimating: 290it [02:59,  1.88it/s]Extractor Estimating: 291it [02:59,  1.83it/s]Extractor Estimating: 292it [03:00,  1.75it/s]Extractor Estimating: 293it [03:00,  1.78it/s]Extractor Estimating: 294it [03:01,  1.83it/s]Extractor Estimating: 295it [03:02,  1.73it/s]Extractor Estimating: 296it [03:02,  1.76it/s]Extractor Estimating: 297it [03:03,  1.82it/s]Extractor Estimating: 298it [03:03,  1.64it/s]Extractor Estimating: 299it [03:04,  1.66it/s]Extractor Estimating: 300it [03:05,  1.65it/s]Extractor Estimating: 301it [03:05,  1.60it/s]Extractor Estimating: 302it [03:06,  1.49it/s]Extractor Estimating: 303it [03:07,  1.46it/s]Extractor Estimating: 304it [03:07,  1.48it/s]Extractor Estimating: 305it [03:08,  1.51it/s]Extractor Estimating: 306it [03:09,  1.57it/s]Extractor Estimating: 307it [03:09,  1.65it/s]Extractor Estimating: 308it [03:10,  1.68it/s]Extractor Estimating: 309it [03:10,  1.68it/s]Extractor Estimating: 310it [03:11,  1.64it/s]Extractor Estimating: 311it [03:12,  1.63it/s]Extractor Estimating: 312it [03:12,  1.65it/s]Extractor Estimating: 313it [03:13,  1.63it/s]Extractor Estimating: 314it [03:13,  1.66it/s]Extractor Estimating: 315it [03:14,  1.54it/s]Extractor Estimating: 316it [03:15,  1.56it/s]Extractor Estimating: 317it [03:15,  1.57it/s]Extractor Estimating: 318it [03:16,  1.54it/s]Extractor Estimating: 319it [03:17,  1.58it/s]Extractor Estimating: 320it [03:17,  1.60it/s]Extractor Estimating: 321it [03:18,  1.59it/s]Extractor Estimating: 322it [03:19,  1.55it/s]Extractor Estimating: 323it [03:19,  1.53it/s]Extractor Estimating: 324it [03:20,  1.52it/s]Extractor Estimating: 325it [03:21,  1.53it/s]Extractor Estimating: 326it [03:21,  1.59it/s]Extractor Estimating: 327it [03:22,  1.60it/s]Extractor Estimating: 328it [03:22,  1.67it/s]Extractor Estimating: 329it [03:23,  1.62it/s]Extractor Estimating: 330it [03:24,  1.71it/s]Extractor Estimating: 331it [03:24,  1.65it/s]Extractor Estimating: 332it [03:25,  1.72it/s]Extractor Estimating: 333it [03:25,  1.75it/s]Extractor Estimating: 334it [03:26,  1.71it/s]Extractor Estimating: 335it [03:27,  1.67it/s]Extractor Estimating: 336it [03:27,  1.70it/s]Extractor Estimating: 337it [03:28,  1.70it/s]Extractor Estimating: 338it [03:28,  1.72it/s]Extractor Estimating: 339it [03:29,  1.75it/s]Extractor Estimating: 340it [03:29,  1.70it/s]Extractor Estimating: 341it [03:30,  1.75it/s]Extractor Estimating: 342it [03:30,  1.80it/s]Extractor Estimating: 343it [03:31,  1.72it/s]Extractor Estimating: 344it [03:32,  1.71it/s]Extractor Estimating: 345it [03:32,  1.70it/s]Extractor Estimating: 346it [03:33,  1.60it/s]Extractor Estimating: 347it [03:34,  1.57it/s]Extractor Estimating: 348it [03:34,  1.60it/s]Extractor Estimating: 349it [03:35,  1.63it/s]Extractor Estimating: 350it [03:35,  1.66it/s]Extractor Estimating: 351it [03:36,  1.55it/s]Extractor Estimating: 352it [03:37,  1.39it/s]Extractor Estimating: 353it [03:38,  1.46it/s]Extractor Estimating: 354it [03:38,  1.50it/s]Extractor Estimating: 355it [03:39,  1.54it/s]Extractor Estimating: 356it [03:39,  1.57it/s]Extractor Estimating: 357it [03:40,  1.58it/s]Extractor Estimating: 358it [03:41,  1.65it/s]Extractor Estimating: 359it [03:41,  1.64it/s]Extractor Estimating: 360it [03:42,  1.62it/s]Extractor Estimating: 361it [03:42,  1.71it/s]Extractor Estimating: 362it [03:43,  1.69it/s]Extractor Estimating: 363it [03:44,  1.66it/s]Extractor Estimating: 364it [03:44,  1.59it/s]Extractor Estimating: 365it [03:45,  1.62it/s]Extractor Estimating: 366it [03:46,  1.64it/s]Extractor Estimating: 367it [03:46,  1.56it/s]Extractor Estimating: 368it [03:47,  1.39it/s]Extractor Estimating: 369it [03:48,  1.42it/s]Extractor Estimating: 370it [03:48,  1.49it/s]Extractor Estimating: 371it [03:49,  1.56it/s]Extractor Estimating: 372it [03:50,  1.55it/s]Extractor Estimating: 373it [03:50,  1.57it/s]Extractor Estimating: 374it [03:51,  1.56it/s]Extractor Estimating: 375it [03:52,  1.45it/s]Extractor Estimating: 375it [03:52,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:32,658 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:32,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:32,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:32,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:32,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:47:33,123 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:47:33,124 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:47:33,802 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:47:34,868 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:47:34,868 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:36,737 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:36,739 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:36,740 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:36,740 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:47:36,740 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:47:37,248 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:47:37,249 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:47:37,578 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:47:37,776 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:47:37,776 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:58:28,974 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:58:29,018 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7497 mean pseudo reward: 0.9179908654196082
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 19789
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19889, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19889, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.021, loss:880.3648
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.040, loss:815.6315
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.021, loss:798.8212
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.029, loss:743.7843
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.014, loss:786.3908
>> valid entity prec:0.6155, rec:0.5232, f1:0.5656
>> valid relation prec:0.3978, rec:0.0836, f1:0.1382
>> valid relation with NER prec:0.3978, rec:0.0836, f1:0.1382
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.246, loss:802.5671
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.041, loss:738.8945
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.025, loss:753.7425
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.042, loss:781.3649
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.069, loss:772.0857
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5833, rec:0.4677, f1:0.5191
>> valid relation prec:0.3002, rec:0.0570, f1:0.0958
>> valid relation with NER prec:0.3002, rec:0.0570, f1:0.0958
g_step 1100, step 161, avg_time 2.209, loss:745.4607
g_step 1200, step 261, avg_time 1.024, loss:759.3730
g_step 1300, step 48, avg_time 1.009, loss:728.5681
g_step 1400, step 148, avg_time 1.039, loss:709.3429
g_step 1500, step 248, avg_time 1.029, loss:760.0707
>> valid entity prec:0.6404, rec:0.5203, f1:0.5741
>> valid relation prec:0.4055, rec:0.0848, f1:0.1403
>> valid relation with NER prec:0.4055, rec:0.0848, f1:0.1403
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.233, loss:684.0892
g_step 1700, step 135, avg_time 1.022, loss:683.0217
g_step 1800, step 235, avg_time 1.018, loss:679.4427
g_step 1900, step 22, avg_time 1.008, loss:691.4585
g_step 2000, step 122, avg_time 1.032, loss:621.2947
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6028, rec:0.5371, f1:0.5681
>> valid relation prec:0.3163, rec:0.0868, f1:0.1362
>> valid relation with NER prec:0.3163, rec:0.0868, f1:0.1362
g_step 2100, step 222, avg_time 2.212, loss:679.8222
g_step 2200, step 9, avg_time 1.030, loss:656.2247
g_step 2300, step 109, avg_time 1.017, loss:575.6451
g_step 2400, step 209, avg_time 1.019, loss:624.3148
g_step 2500, step 309, avg_time 1.029, loss:641.1547
>> valid entity prec:0.5725, rec:0.5880, f1:0.5802
>> valid relation prec:0.2513, rec:0.0859, f1:0.1281
>> valid relation with NER prec:0.2513, rec:0.0859, f1:0.1281
new max entity f1 on valid!
g_step 2600, step 96, avg_time 2.193, loss:541.3032
g_step 2700, step 196, avg_time 1.021, loss:577.8722
g_step 2800, step 296, avg_time 1.042, loss:615.9859
g_step 2900, step 83, avg_time 1.022, loss:552.4140
g_step 3000, step 183, avg_time 1.026, loss:562.1389
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5966, rec:0.5280, f1:0.5602
>> valid relation prec:0.3692, rec:0.0942, f1:0.1502
>> valid relation with NER prec:0.3692, rec:0.0942, f1:0.1502
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 283, avg_time 2.201, loss:569.6313
g_step 3200, step 70, avg_time 1.024, loss:541.1398
g_step 3300, step 170, avg_time 1.009, loss:534.2091
g_step 3400, step 270, avg_time 1.036, loss:558.6863
g_step 3500, step 57, avg_time 1.013, loss:507.0891
>> valid entity prec:0.5824, rec:0.5921, f1:0.5872
>> valid relation prec:0.2824, rec:0.0874, f1:0.1335
>> valid relation with NER prec:0.2824, rec:0.0874, f1:0.1335
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 157, avg_time 2.219, loss:529.0647
g_step 3700, step 257, avg_time 1.038, loss:526.0100
g_step 3800, step 44, avg_time 1.021, loss:509.3727
g_step 3900, step 144, avg_time 1.035, loss:485.0471
g_step 4000, step 244, avg_time 1.017, loss:513.3115
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6035, rec:0.5324, f1:0.5657
>> valid relation prec:0.2772, rec:0.0882, f1:0.1339
>> valid relation with NER prec:0.2772, rec:0.0882, f1:0.1339
g_step 4100, step 31, avg_time 2.201, loss:496.1977
g_step 4200, step 131, avg_time 1.044, loss:452.6600
g_step 4300, step 231, avg_time 1.019, loss:499.0704
g_step 4400, step 18, avg_time 1.025, loss:472.3239
g_step 4500, step 118, avg_time 1.013, loss:446.3935
>> valid entity prec:0.5904, rec:0.4739, f1:0.5258
>> valid relation prec:0.3310, rec:0.0957, f1:0.1484
>> valid relation with NER prec:0.3310, rec:0.0957, f1:0.1484
g_step 4600, step 218, avg_time 2.222, loss:457.0995
g_step 4700, step 5, avg_time 1.020, loss:469.0062
g_step 4800, step 105, avg_time 1.019, loss:429.5846
g_step 4900, step 205, avg_time 1.040, loss:447.5087
g_step 5000, step 305, avg_time 1.020, loss:464.8137
learning rate was adjusted to 0.0008
>> valid entity prec:0.6062, rec:0.4712, f1:0.5303
>> valid relation prec:0.2931, rec:0.0759, f1:0.1206
>> valid relation with NER prec:0.2931, rec:0.0759, f1:0.1206
g_step 5100, step 92, avg_time 2.214, loss:421.4150
g_step 5200, step 192, avg_time 1.041, loss:435.2282
g_step 5300, step 292, avg_time 1.007, loss:432.5309
g_step 5400, step 79, avg_time 1.018, loss:388.5244
g_step 5500, step 179, avg_time 1.040, loss:389.0042
>> valid entity prec:0.5412, rec:0.5999, f1:0.5690
>> valid relation prec:0.2271, rec:0.0942, f1:0.1332
>> valid relation with NER prec:0.2271, rec:0.0942, f1:0.1332
g_step 5600, step 279, avg_time 2.212, loss:432.5229
g_step 5700, step 66, avg_time 1.021, loss:383.9674
g_step 5800, step 166, avg_time 1.008, loss:387.5582
g_step 5900, step 266, avg_time 1.030, loss:420.8418
g_step 6000, step 53, avg_time 1.022, loss:381.1078
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5702, rec:0.5195, f1:0.5437
>> valid relation prec:0.2728, rec:0.0710, f1:0.1127
>> valid relation with NER prec:0.2728, rec:0.0710, f1:0.1127
g_step 6100, step 153, avg_time 2.215, loss:384.7842
g_step 6200, step 253, avg_time 1.012, loss:387.8561
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:58:29 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:58:29 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-58-28_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:58:30 - WARNING - datasets.builder -   Using custom data configuration default-43f4ce2fe121ce4c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-43f4ce2fe121ce4c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  6.90 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:58:31,293 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:58:31,294 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:58:31,295 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:58:31,296 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:58:31,306 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:58:31,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:58:31,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:58:31,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:58:31,309 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:58:31,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:58:31,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:58:31,532 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:58:34,628 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:58:34,665 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-43f4ce2fe121ce4c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.61ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.58ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.10ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.37ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.50ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.59ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.66ba/s]100%|██████████| 8/8 [00:01<00:00,  5.54ba/s]100%|██████████| 8/8 [00:01<00:00,  4.62ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.09ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.33ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.40ba/s]100%|██████████| 4/4 [00:00<00:00,  5.48ba/s]100%|██████████| 4/4 [00:00<00:00,  4.98ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  6.95ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.22ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.81ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.34ba/s]100%|██████████| 8/8 [00:00<00:00, 10.59ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.13ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.28ba/s]100%|██████████| 4/4 [00:00<00:00, 11.77ba/s]
[INFO|trainer.py:414] 2023-08-28 21:58:39,176 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:58:39,356 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:58:39,356 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 21:58:39,356 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:58:39,356 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:58:39,356 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:58:39,356 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:58:39,357 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<04:07,  2.36it/s]  0%|          | 2/585 [00:00<03:20,  2.90it/s]  1%|          | 3/585 [00:01<03:05,  3.13it/s]  1%|          | 4/585 [00:01<02:59,  3.24it/s]  1%|          | 5/585 [00:01<03:01,  3.19it/s]  1%|          | 6/585 [00:01<02:57,  3.26it/s]  1%|          | 7/585 [00:02<02:54,  3.31it/s]  1%|▏         | 8/585 [00:02<02:52,  3.34it/s]  2%|▏         | 9/585 [00:02<02:51,  3.36it/s]  2%|▏         | 10/585 [00:03<02:50,  3.38it/s]  2%|▏         | 11/585 [00:03<02:49,  3.39it/s]  2%|▏         | 12/585 [00:03<02:48,  3.40it/s]  2%|▏         | 13/585 [00:03<02:48,  3.40it/s]  2%|▏         | 14/585 [00:04<02:47,  3.40it/s]  3%|▎         | 15/585 [00:04<02:47,  3.41it/s]  3%|▎         | 16/585 [00:04<02:47,  3.39it/s]  3%|▎         | 17/585 [00:05<02:47,  3.39it/s]  3%|▎         | 18/585 [00:05<02:46,  3.40it/s]  3%|▎         | 19/585 [00:05<02:46,  3.39it/s]  3%|▎         | 20/585 [00:06<02:46,  3.40it/s]  4%|▎         | 21/585 [00:06<02:45,  3.40it/s]  4%|▍         | 22/585 [00:06<02:45,  3.40it/s]  4%|▍         | 23/585 [00:06<02:45,  3.40it/s]  4%|▍         | 24/585 [00:07<02:44,  3.40it/s]  4%|▍         | 25/585 [00:07<02:44,  3.41it/s]  4%|▍         | 26/585 [00:07<02:44,  3.40it/s]  5%|▍         | 27/585 [00:08<02:44,  3.38it/s]  5%|▍         | 28/585 [00:08<02:44,  3.39it/s]  5%|▍         | 29/585 [00:08<02:43,  3.40it/s]  5%|▌         | 30/585 [00:08<02:43,  3.40it/s]  5%|▌         | 31/585 [00:09<02:42,  3.40it/s]  5%|▌         | 32/585 [00:09<02:42,  3.40it/s]  6%|▌         | 33/585 [00:09<02:42,  3.40it/s]  6%|▌         | 34/585 [00:10<02:41,  3.40it/s]  6%|▌         | 35/585 [00:10<02:41,  3.41it/s]  6%|▌         | 36/585 [00:10<02:41,  3.41it/s]  6%|▋         | 37/585 [00:11<02:40,  3.41it/s]  6%|▋         | 38/585 [00:11<02:47,  3.26it/s]  7%|▋         | 39/585 [00:11<02:45,  3.31it/s]  7%|▋         | 40/585 [00:11<02:43,  3.33it/s]  7%|▋         | 41/585 [00:12<02:42,  3.35it/s]  7%|▋         | 42/585 [00:12<02:40,  3.38it/s]  7%|▋         | 43/585 [00:12<02:39,  3.40it/s]  8%|▊         | 44/585 [00:13<02:38,  3.42it/s]  8%|▊         | 45/585 [00:13<02:37,  3.43it/s]  8%|▊         | 46/585 [00:13<02:36,  3.44it/s]  8%|▊         | 47/585 [00:13<02:35,  3.45it/s]  8%|▊         | 48/585 [00:14<02:35,  3.45it/s]  8%|▊         | 49/585 [00:14<02:40,  3.34it/s]  9%|▊         | 50/585 [00:14<02:38,  3.38it/s]  9%|▊         | 51/585 [00:15<02:37,  3.40it/s]  9%|▉         | 52/585 [00:15<02:36,  3.42it/s]  9%|▉         | 53/585 [00:15<02:35,  3.43it/s]  9%|▉         | 54/585 [00:16<02:34,  3.44it/s]  9%|▉         | 55/585 [00:16<02:34,  3.44it/s] 10%|▉         | 56/585 [00:16<02:33,  3.45it/s] 10%|▉         | 57/585 [00:16<02:33,  3.45it/s] 10%|▉         | 58/585 [00:17<02:32,  3.45it/s] 10%|█         | 59/585 [00:17<02:32,  3.45it/s] 10%|█         | 60/585 [00:17<02:32,  3.44it/s] 10%|█         | 61/585 [00:18<02:32,  3.44it/s] 11%|█         | 62/585 [00:18<02:31,  3.45it/s] 11%|█         | 63/585 [00:18<02:31,  3.45it/s] 11%|█         | 64/585 [00:18<02:31,  3.45it/s] 11%|█         | 65/585 [00:19<02:30,  3.45it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.45it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.45it/s] 12%|█▏        | 68/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 69/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 72/585 [00:21<02:28,  3.45it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.45it/s] 13%|█▎        | 75/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.45it/s] 14%|█▎        | 79/585 [00:23<02:28,  3.40it/s] 14%|█▎        | 80/585 [00:23<02:27,  3.41it/s] 14%|█▍        | 81/585 [00:23<02:27,  3.43it/s] 14%|█▍        | 82/585 [00:24<02:26,  3.44it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.44it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.44it/s] 15%|█▍        | 85/585 [00:25<02:24,  3.45it/s] 15%|█▍        | 86/585 [00:25<02:24,  3.45it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.45it/s] 15%|█▌        | 89/585 [00:26<02:23,  3.45it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.44it/s] 16%|█▌        | 92/585 [00:27<02:23,  3.45it/s] 16%|█▌        | 93/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.45it/s] 16%|█▋        | 96/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.45it/s] 17%|█▋        | 99/585 [00:29<02:20,  3.45it/s] 17%|█▋        | 100/585 [00:29<02:20,  3.45it/s] 17%|█▋        | 101/585 [00:29<02:22,  3.40it/s] 17%|█▋        | 102/585 [00:29<02:21,  3.41it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.44it/s] 18%|█▊        | 106/585 [00:31<02:19,  3.44it/s] 18%|█▊        | 107/585 [00:31<02:18,  3.44it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.45it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.45it/s] 19%|█▉        | 110/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.45it/s] 19%|█▉        | 112/585 [00:32<02:19,  3.40it/s] 19%|█▉        | 113/585 [00:33<02:18,  3.41it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.42it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.43it/s] 20%|█▉        | 116/585 [00:34<02:16,  3.44it/s] 20%|██        | 117/585 [00:34<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 21:59:13,727 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:59:13,727 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 21:59:13,727 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.35it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.15it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.09it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.29it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.69it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.38it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.15it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.88it/s][A
 11%|█         | 47/437 [00:01<00:08, 45.02it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.15it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.24it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.17it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 45.16it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.94it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.81it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.68it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.69it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.83it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.95it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.91it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 45.04it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 45.20it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 45.17it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.91it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.85it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.81it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.79it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.76it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.89it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 45.04it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 45.21it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 45.10it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.96it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.92it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.84it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.79it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.78it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.87it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.26it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.87it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.36it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.58it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.63it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.62it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.65it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.70it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.57it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.77it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.89it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.98it/s][A
 59%|█████▉    | 257/437 [00:05<00:03, 45.06it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 45.07it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.90it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.84it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.77it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.69it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.77it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.93it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 45.09it/s][A
 69%|██████▉   | 302/437 [00:06<00:02, 45.11it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.96it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.71it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.77it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.75it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.71it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.75it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.90it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.95it/s][A
 79%|███████▉  | 347/437 [00:07<00:01, 45.01it/s][A
 81%|████████  | 352/437 [00:07<00:01, 45.02it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.88it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.80it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.88it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.69it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.82it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.87it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.95it/s][A
 90%|████████▉ | 392/437 [00:08<00:00, 45.09it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.97it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.95it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.76it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.83it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.71it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.83it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.91it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.94it/s][A
100%|██████████| 437/437 [00:09<00:00, 45.04it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 45.04it/s][A 20%|██        | 117/585 [00:44<02:16,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:59:23,525 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 21:59:23,557 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:59:27,863 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:59:27,888 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:59:27,899 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:57<55:22,  7.11s/it] 20%|██        | 119/585 [00:57<39:39,  5.11s/it] 21%|██        | 120/585 [00:58<28:22,  3.66s/it] 21%|██        | 121/585 [00:58<20:30,  2.65s/it] 21%|██        | 122/585 [00:58<15:00,  1.94s/it] 21%|██        | 123/585 [00:58<11:09,  1.45s/it] 21%|██        | 124/585 [00:59<08:27,  1.10s/it] 21%|██▏       | 125/585 [00:59<06:35,  1.16it/s] 22%|██▏       | 126/585 [00:59<05:16,  1.45it/s] 22%|██▏       | 127/585 [01:00<04:21,  1.75it/s] 22%|██▏       | 128/585 [01:00<03:42,  2.05it/s] 22%|██▏       | 129/585 [01:00<03:18,  2.29it/s] 22%|██▏       | 130/585 [01:01<02:58,  2.54it/s] 22%|██▏       | 131/585 [01:01<02:44,  2.76it/s] 23%|██▎       | 132/585 [01:01<02:34,  2.92it/s] 23%|██▎       | 133/585 [01:01<02:27,  3.06it/s] 23%|██▎       | 134/585 [01:02<02:22,  3.16it/s] 23%|██▎       | 135/585 [01:02<02:19,  3.23it/s] 23%|██▎       | 136/585 [01:02<02:16,  3.28it/s] 23%|██▎       | 137/585 [01:03<02:14,  3.32it/s] 24%|██▎       | 138/585 [01:03<02:13,  3.35it/s] 24%|██▍       | 139/585 [01:03<02:12,  3.37it/s] 24%|██▍       | 140/585 [01:03<02:11,  3.37it/s] 24%|██▍       | 141/585 [01:04<02:10,  3.39it/s] 24%|██▍       | 142/585 [01:04<02:10,  3.39it/s] 24%|██▍       | 143/585 [01:04<02:10,  3.40it/s] 25%|██▍       | 144/585 [01:05<02:09,  3.41it/s] 25%|██▍       | 145/585 [01:05<02:09,  3.41it/s] 25%|██▍       | 146/585 [01:05<02:08,  3.41it/s] 25%|██▌       | 147/585 [01:06<02:08,  3.41it/s] 25%|██▌       | 148/585 [01:06<02:08,  3.41it/s] 25%|██▌       | 149/585 [01:06<02:07,  3.41it/s] 26%|██▌       | 150/585 [01:06<02:07,  3.41it/s] 26%|██▌       | 151/585 [01:07<02:10,  3.33it/s] 26%|██▌       | 152/585 [01:07<02:09,  3.36it/s] 26%|██▌       | 153/585 [01:07<02:08,  3.37it/s] 26%|██▋       | 154/585 [01:08<02:07,  3.38it/s] 26%|██▋       | 155/585 [01:08<02:06,  3.39it/s] 27%|██▋       | 156/585 [01:08<02:06,  3.40it/s] 27%|██▋       | 157/585 [01:08<02:05,  3.40it/s] 27%|██▋       | 158/585 [01:09<02:05,  3.41it/s] 27%|██▋       | 159/585 [01:09<02:04,  3.41it/s] 27%|██▋       | 160/585 [01:09<02:04,  3.41it/s] 28%|██▊       | 161/585 [01:10<02:04,  3.41it/s] 28%|██▊       | 162/585 [01:10<02:05,  3.38it/s] 28%|██▊       | 163/585 [01:10<02:04,  3.39it/s] 28%|██▊       | 164/585 [01:11<02:03,  3.40it/s] 28%|██▊       | 165/585 [01:11<02:03,  3.40it/s] 28%|██▊       | 166/585 [01:11<02:03,  3.40it/s] 29%|██▊       | 167/585 [01:11<02:02,  3.40it/s] 29%|██▊       | 168/585 [01:12<02:02,  3.40it/s] 29%|██▉       | 169/585 [01:12<02:02,  3.41it/s] 29%|██▉       | 170/585 [01:12<02:01,  3.41it/s] 29%|██▉       | 171/585 [01:13<02:01,  3.41it/s] 29%|██▉       | 172/585 [01:13<02:01,  3.41it/s] 30%|██▉       | 173/585 [01:13<02:05,  3.29it/s] 30%|██▉       | 174/585 [01:13<02:03,  3.34it/s] 30%|██▉       | 175/585 [01:14<02:01,  3.37it/s] 30%|███       | 176/585 [01:14<02:00,  3.40it/s] 30%|███       | 177/585 [01:14<01:59,  3.41it/s] 30%|███       | 178/585 [01:15<01:58,  3.43it/s] 31%|███       | 179/585 [01:15<01:58,  3.44it/s] 31%|███       | 180/585 [01:15<01:57,  3.44it/s] 31%|███       | 181/585 [01:16<01:57,  3.45it/s] 31%|███       | 182/585 [01:16<01:56,  3.45it/s] 31%|███▏      | 183/585 [01:16<01:56,  3.45it/s] 31%|███▏      | 184/585 [01:16<01:58,  3.39it/s] 32%|███▏      | 185/585 [01:17<01:57,  3.41it/s] 32%|███▏      | 186/585 [01:17<01:56,  3.43it/s] 32%|███▏      | 187/585 [01:17<01:55,  3.43it/s] 32%|███▏      | 188/585 [01:18<01:55,  3.44it/s] 32%|███▏      | 189/585 [01:18<01:54,  3.45it/s] 32%|███▏      | 190/585 [01:18<01:54,  3.45it/s] 33%|███▎      | 191/585 [01:18<01:54,  3.45it/s] 33%|███▎      | 192/585 [01:19<01:53,  3.45it/s] 33%|███▎      | 193/585 [01:19<01:53,  3.46it/s] 33%|███▎      | 194/585 [01:19<01:53,  3.46it/s] 33%|███▎      | 195/585 [01:20<01:54,  3.41it/s] 34%|███▎      | 196/585 [01:20<01:53,  3.43it/s] 34%|███▎      | 197/585 [01:20<01:52,  3.44it/s] 34%|███▍      | 198/585 [01:20<01:52,  3.44it/s] 34%|███▍      | 199/585 [01:21<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:21<01:51,  3.45it/s] 34%|███▍      | 201/585 [01:21<01:51,  3.45it/s] 35%|███▍      | 202/585 [01:22<01:50,  3.45it/s] 35%|███▍      | 203/585 [01:22<01:50,  3.45it/s] 35%|███▍      | 204/585 [01:22<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:22<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:23<01:49,  3.45it/s] 35%|███▌      | 207/585 [01:23<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:23<01:49,  3.46it/s] 36%|███▌      | 209/585 [01:24<01:48,  3.46it/s] 36%|███▌      | 210/585 [01:24<01:48,  3.46it/s] 36%|███▌      | 211/585 [01:24<01:52,  3.31it/s] 36%|███▌      | 212/585 [01:25<01:51,  3.35it/s] 36%|███▋      | 213/585 [01:25<01:49,  3.38it/s] 37%|███▋      | 214/585 [01:25<01:49,  3.40it/s] 37%|███▋      | 215/585 [01:25<01:48,  3.42it/s] 37%|███▋      | 216/585 [01:26<01:47,  3.43it/s] 37%|███▋      | 217/585 [01:26<01:47,  3.44it/s] 37%|███▋      | 218/585 [01:26<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:27<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:27<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:27<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:27<01:47,  3.39it/s] 38%|███▊      | 223/585 [01:28<01:46,  3.41it/s] 38%|███▊      | 224/585 [01:28<01:45,  3.43it/s] 38%|███▊      | 225/585 [01:28<01:44,  3.43it/s] 39%|███▊      | 226/585 [01:29<01:44,  3.44it/s] 39%|███▉      | 227/585 [01:29<01:43,  3.44it/s] 39%|███▉      | 228/585 [01:29<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:29<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:30<01:42,  3.45it/s] 39%|███▉      | 231/585 [01:30<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:30<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:31<01:43,  3.39it/s] 40%|████      | 234/585 [01:31<01:42,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 22:00:10,855 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:00:10,855 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:00:10,855 >>   Batch size = 8
{'eval_loss': 1.0341764688491821, 'eval_runtime': 9.7512, 'eval_samples_per_second': 358.213, 'eval_steps_per_second': 44.815, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.48it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.95it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.63it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.43it/s][A
  6%|▌         | 27/437 [00:00<00:09, 44.81it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.13it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.04it/s][A
 10%|▉         | 42/437 [00:00<00:08, 45.18it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.84it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.07it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.24it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.97it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.85it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.58it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.81it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.60it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.91it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.96it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 45.03it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 45.16it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.98it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.89it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.65it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.75it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.89it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.92it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 45.01it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.90it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 45.06it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.97it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.87it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.64it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.66it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.84it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.86it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.84it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 45.12it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 45.07it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.90it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.75it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.80it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.70it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.77it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.91it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 45.11it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 45.17it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 45.01it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.85it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.84it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.85it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.68it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.77it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.99it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 45.10it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 45.05it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.94it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.90it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.81it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.78it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.65it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.75it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.96it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 45.01it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 45.11it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 45.02it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.95it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.82it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.67it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.70it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.79it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.96it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 45.04it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 45.03it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 45.04it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.90it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.82it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.68it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.66it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.81it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.83it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 45.01it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 45.01it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 45.00it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.84it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.83it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.79it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.80it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.80it/s][A 40%|████      | 234/585 [01:41<01:42,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:00:20,630 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 22:00:20,669 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:00:24,720 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:00:24,799 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:00:24,831 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:55<43:04,  7.38s/it] 40%|████      | 236/585 [01:55<30:35,  5.26s/it] 41%|████      | 237/585 [01:55<21:51,  3.77s/it] 41%|████      | 238/585 [01:56<15:45,  2.73s/it] 41%|████      | 239/585 [01:56<11:30,  2.00s/it] 41%|████      | 240/585 [01:56<08:32,  1.49s/it] 41%|████      | 241/585 [01:57<06:32,  1.14s/it] 41%|████▏     | 242/585 [01:57<05:04,  1.13it/s] 42%|████▏     | 243/585 [01:57<04:02,  1.41it/s] 42%|████▏     | 244/585 [01:58<03:19,  1.71it/s] 42%|████▏     | 245/585 [01:58<02:48,  2.01it/s] 42%|████▏     | 246/585 [01:58<02:27,  2.30it/s] 42%|████▏     | 247/585 [01:58<02:14,  2.51it/s] 42%|████▏     | 248/585 [01:59<02:03,  2.72it/s] 43%|████▎     | 249/585 [01:59<01:55,  2.90it/s] 43%|████▎     | 250/585 [01:59<01:50,  3.04it/s] 43%|████▎     | 251/585 [02:00<01:46,  3.14it/s] 43%|████▎     | 252/585 [02:00<01:43,  3.21it/s] 43%|████▎     | 253/585 [02:00<01:41,  3.27it/s] 43%|████▎     | 254/585 [02:01<01:39,  3.31it/s] 44%|████▎     | 255/585 [02:01<01:38,  3.34it/s] 44%|████▍     | 256/585 [02:01<01:37,  3.36it/s] 44%|████▍     | 257/585 [02:01<01:37,  3.38it/s] 44%|████▍     | 258/585 [02:02<01:38,  3.33it/s] 44%|████▍     | 259/585 [02:02<01:37,  3.35it/s] 44%|████▍     | 260/585 [02:02<01:36,  3.37it/s] 45%|████▍     | 261/585 [02:03<01:35,  3.38it/s] 45%|████▍     | 262/585 [02:03<01:35,  3.39it/s] 45%|████▍     | 263/585 [02:03<01:34,  3.39it/s] 45%|████▌     | 264/585 [02:03<01:34,  3.40it/s] 45%|████▌     | 265/585 [02:04<01:34,  3.40it/s] 45%|████▌     | 266/585 [02:04<01:33,  3.40it/s] 46%|████▌     | 267/585 [02:04<01:33,  3.40it/s] 46%|████▌     | 268/585 [02:05<01:33,  3.40it/s] 46%|████▌     | 269/585 [02:05<01:33,  3.40it/s] 46%|████▌     | 270/585 [02:05<01:32,  3.40it/s] 46%|████▋     | 271/585 [02:06<01:32,  3.40it/s] 46%|████▋     | 272/585 [02:06<01:32,  3.40it/s] 47%|████▋     | 273/585 [02:06<01:31,  3.40it/s] 47%|████▋     | 274/585 [02:06<01:31,  3.41it/s] 47%|████▋     | 275/585 [02:07<01:31,  3.41it/s] 47%|████▋     | 276/585 [02:07<01:30,  3.40it/s] 47%|████▋     | 277/585 [02:07<01:30,  3.41it/s] 48%|████▊     | 278/585 [02:08<01:30,  3.41it/s] 48%|████▊     | 279/585 [02:08<01:29,  3.41it/s] 48%|████▊     | 280/585 [02:08<01:29,  3.39it/s] 48%|████▊     | 281/585 [02:08<01:29,  3.40it/s] 48%|████▊     | 282/585 [02:09<01:29,  3.40it/s] 48%|████▊     | 283/585 [02:09<01:28,  3.40it/s] 49%|████▊     | 284/585 [02:09<01:28,  3.40it/s] 49%|████▊     | 285/585 [02:10<01:28,  3.41it/s] 49%|████▉     | 286/585 [02:10<01:27,  3.40it/s] 49%|████▉     | 287/585 [02:10<01:27,  3.41it/s] 49%|████▉     | 288/585 [02:11<01:27,  3.41it/s] 49%|████▉     | 289/585 [02:11<01:26,  3.41it/s] 50%|████▉     | 290/585 [02:11<01:26,  3.41it/s] 50%|████▉     | 291/585 [02:11<01:27,  3.35it/s] 50%|████▉     | 292/585 [02:12<01:26,  3.37it/s] 50%|█████     | 293/585 [02:12<01:26,  3.38it/s] 50%|█████     | 294/585 [02:12<01:25,  3.39it/s] 50%|█████     | 295/585 [02:13<01:25,  3.39it/s] 51%|█████     | 296/585 [02:13<01:25,  3.40it/s] 51%|█████     | 297/585 [02:13<01:24,  3.40it/s] 51%|█████     | 298/585 [02:13<01:24,  3.40it/s] 51%|█████     | 299/585 [02:14<01:24,  3.40it/s] 51%|█████▏    | 300/585 [02:14<01:23,  3.41it/s] 51%|█████▏    | 301/585 [02:14<01:23,  3.40it/s] 52%|█████▏    | 302/585 [02:15<01:23,  3.41it/s] 52%|█████▏    | 303/585 [02:15<01:22,  3.41it/s] 52%|█████▏    | 304/585 [02:15<01:22,  3.41it/s] 52%|█████▏    | 305/585 [02:16<01:23,  3.35it/s] 52%|█████▏    | 306/585 [02:16<01:22,  3.37it/s] 52%|█████▏    | 307/585 [02:16<01:22,  3.38it/s] 53%|█████▎    | 308/585 [02:16<01:21,  3.39it/s] 53%|█████▎    | 309/585 [02:17<01:21,  3.39it/s] 53%|█████▎    | 310/585 [02:17<01:21,  3.39it/s] 53%|█████▎    | 311/585 [02:17<01:20,  3.40it/s] 53%|█████▎    | 312/585 [02:18<01:20,  3.40it/s] 54%|█████▎    | 313/585 [02:18<01:19,  3.40it/s] 54%|█████▎    | 314/585 [02:18<01:19,  3.41it/s] 54%|█████▍    | 315/585 [02:18<01:19,  3.40it/s] 54%|█████▍    | 316/585 [02:19<01:19,  3.38it/s] 54%|█████▍    | 317/585 [02:19<01:19,  3.39it/s] 54%|█████▍    | 318/585 [02:19<01:18,  3.39it/s] 55%|█████▍    | 319/585 [02:20<01:18,  3.39it/s] 55%|█████▍    | 320/585 [02:20<01:18,  3.40it/s] 55%|█████▍    | 321/585 [02:20<01:17,  3.40it/s] 55%|█████▌    | 322/585 [02:21<01:17,  3.40it/s] 55%|█████▌    | 323/585 [02:21<01:16,  3.40it/s] 55%|█████▌    | 324/585 [02:21<01:16,  3.40it/s] 56%|█████▌    | 325/585 [02:21<01:16,  3.41it/s] 56%|█████▌    | 326/585 [02:22<01:16,  3.40it/s] 56%|█████▌    | 327/585 [02:22<01:16,  3.37it/s] 56%|█████▌    | 328/585 [02:22<01:16,  3.38it/s] 56%|█████▌    | 329/585 [02:23<01:15,  3.38it/s] 56%|█████▋    | 330/585 [02:23<01:15,  3.39it/s] 57%|█████▋    | 331/585 [02:23<01:14,  3.40it/s] 57%|█████▋    | 332/585 [02:23<01:14,  3.40it/s] 57%|█████▋    | 333/585 [02:24<01:14,  3.40it/s] 57%|█████▋    | 334/585 [02:24<01:13,  3.40it/s] 57%|█████▋    | 335/585 [02:24<01:13,  3.40it/s] 57%|█████▋    | 336/585 [02:25<01:13,  3.40it/s] 58%|█████▊    | 337/585 [02:25<01:12,  3.41it/s] 58%|█████▊    | 338/585 [02:25<01:13,  3.37it/s] 58%|█████▊    | 339/585 [02:26<01:12,  3.38it/s] 58%|█████▊    | 340/585 [02:26<01:12,  3.39it/s] 58%|█████▊    | 341/585 [02:26<01:11,  3.39it/s] 58%|█████▊    | 342/585 [02:26<01:11,  3.40it/s] 59%|█████▊    | 343/585 [02:27<01:11,  3.40it/s] 59%|█████▉    | 344/585 [02:27<01:10,  3.40it/s] 59%|█████▉    | 345/585 [02:27<01:10,  3.40it/s] 59%|█████▉    | 346/585 [02:28<01:10,  3.40it/s] 59%|█████▉    | 347/585 [02:28<01:09,  3.40it/s] 59%|█████▉    | 348/585 [02:28<01:09,  3.40it/s] 60%|█████▉    | 349/585 [02:29<01:09,  3.39it/s] 60%|█████▉    | 350/585 [02:29<01:09,  3.40it/s] 60%|██████    | 351/585 [02:29<01:08,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 22:01:08,992 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:01:08,992 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:01:08,992 >>   Batch size = 8
{'eval_loss': 1.0398377180099487, 'eval_runtime': 9.7501, 'eval_samples_per_second': 358.252, 'eval_steps_per_second': 44.82, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.11it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.68it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.97it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.05it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.56it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.19it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.09it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.94it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.95it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.12it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.23it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.23it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 45.11it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.98it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.82it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.82it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.67it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.84it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.93it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.28it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.80it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.28it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.58it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.73it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.67it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.63it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.69it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.62it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.74it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.85it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 45.03it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 45.12it/s][A
 38%|███▊      | 167/437 [00:03<00:05, 45.03it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.99it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.84it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.82it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.76it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.81it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.84it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.95it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 45.13it/s][A
 49%|████▊     | 212/437 [00:04<00:04, 45.04it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.94it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.73it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.90it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.90it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 42.87it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.64it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.16it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.51it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.71it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.72it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.77it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.71it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.44it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.67it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.79it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 45.01it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 45.06it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.95it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.86it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.83it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.74it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.58it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.69it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.94it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 45.03it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 45.08it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.98it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.88it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.77it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.72it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.66it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 40.67it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 41.93it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 42.99it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.70it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.20it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.59it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.68it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.45it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.34it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.29it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.36it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.78it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.98it/s][A
100%|██████████| 437/437 [00:09<00:00, 45.14it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:39<01:08,  3.40it/s]
100%|██████████| 437/437 [00:09<00:00, 45.14it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:01:18,824 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 22:01:18,939 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:01:23,683 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:01:23,708 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:01:23,717 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:53<29:11,  7.52s/it] 60%|██████    | 353/585 [02:54<20:42,  5.36s/it] 61%|██████    | 354/585 [02:54<14:46,  3.84s/it] 61%|██████    | 355/585 [02:54<10:37,  2.77s/it] 61%|██████    | 356/585 [02:55<07:45,  2.03s/it] 61%|██████    | 357/585 [02:55<05:44,  1.51s/it] 61%|██████    | 358/585 [02:55<04:19,  1.15s/it] 61%|██████▏   | 359/585 [02:56<03:21,  1.12it/s] 62%|██████▏   | 360/585 [02:56<02:39,  1.41it/s] 62%|██████▏   | 361/585 [02:56<02:11,  1.71it/s] 62%|██████▏   | 362/585 [02:56<01:50,  2.01it/s] 62%|██████▏   | 363/585 [02:57<01:40,  2.21it/s] 62%|██████▏   | 364/585 [02:57<01:29,  2.47it/s] 62%|██████▏   | 365/585 [02:57<01:21,  2.69it/s] 63%|██████▎   | 366/585 [02:58<01:16,  2.87it/s] 63%|██████▎   | 367/585 [02:58<01:12,  3.01it/s] 63%|██████▎   | 368/585 [02:58<01:09,  3.12it/s] 63%|██████▎   | 369/585 [02:59<01:07,  3.20it/s] 63%|██████▎   | 370/585 [02:59<01:05,  3.26it/s] 63%|██████▎   | 371/585 [02:59<01:04,  3.31it/s] 64%|██████▎   | 372/585 [02:59<01:03,  3.34it/s] 64%|██████▍   | 373/585 [03:00<01:03,  3.36it/s] 64%|██████▍   | 374/585 [03:00<01:02,  3.37it/s] 64%|██████▍   | 375/585 [03:00<01:02,  3.37it/s] 64%|██████▍   | 376/585 [03:01<01:01,  3.38it/s] 64%|██████▍   | 377/585 [03:01<01:01,  3.39it/s] 65%|██████▍   | 378/585 [03:01<01:00,  3.40it/s] 65%|██████▍   | 379/585 [03:01<01:00,  3.40it/s] 65%|██████▍   | 380/585 [03:02<01:00,  3.40it/s] 65%|██████▌   | 381/585 [03:02<00:59,  3.40it/s] 65%|██████▌   | 382/585 [03:02<00:59,  3.40it/s] 65%|██████▌   | 383/585 [03:03<00:59,  3.40it/s] 66%|██████▌   | 384/585 [03:03<00:59,  3.40it/s] 66%|██████▌   | 385/585 [03:03<00:58,  3.40it/s] 66%|██████▌   | 386/585 [03:04<00:58,  3.39it/s] 66%|██████▌   | 387/585 [03:04<00:58,  3.39it/s] 66%|██████▋   | 388/585 [03:04<00:57,  3.40it/s] 66%|██████▋   | 389/585 [03:04<00:57,  3.40it/s] 67%|██████▋   | 390/585 [03:05<00:57,  3.40it/s] 67%|██████▋   | 391/585 [03:05<00:56,  3.41it/s] 67%|██████▋   | 392/585 [03:05<00:56,  3.40it/s] 67%|██████▋   | 393/585 [03:06<00:56,  3.40it/s] 67%|██████▋   | 394/585 [03:06<00:56,  3.41it/s] 68%|██████▊   | 395/585 [03:06<00:55,  3.41it/s] 68%|██████▊   | 396/585 [03:06<00:55,  3.41it/s] 68%|██████▊   | 397/585 [03:07<00:55,  3.38it/s] 68%|██████▊   | 398/585 [03:07<00:55,  3.39it/s] 68%|██████▊   | 399/585 [03:07<00:54,  3.40it/s] 68%|██████▊   | 400/585 [03:08<00:54,  3.40it/s] 69%|██████▊   | 401/585 [03:08<00:54,  3.40it/s] 69%|██████▊   | 402/585 [03:08<00:53,  3.41it/s] 69%|██████▉   | 403/585 [03:09<00:53,  3.40it/s] 69%|██████▉   | 404/585 [03:09<00:53,  3.41it/s] 69%|██████▉   | 405/585 [03:09<00:52,  3.41it/s] 69%|██████▉   | 406/585 [03:09<00:52,  3.41it/s] 70%|██████▉   | 407/585 [03:10<00:52,  3.41it/s] 70%|██████▉   | 408/585 [03:10<00:52,  3.40it/s] 70%|██████▉   | 409/585 [03:10<00:51,  3.40it/s] 70%|███████   | 410/585 [03:11<00:51,  3.40it/s] 70%|███████   | 411/585 [03:11<00:51,  3.40it/s] 70%|███████   | 412/585 [03:11<00:50,  3.40it/s] 71%|███████   | 413/585 [03:11<00:50,  3.40it/s] 71%|███████   | 414/585 [03:12<00:50,  3.41it/s] 71%|███████   | 415/585 [03:12<00:49,  3.40it/s] 71%|███████   | 416/585 [03:12<00:49,  3.40it/s] 71%|███████▏  | 417/585 [03:13<00:49,  3.41it/s] 71%|███████▏  | 418/585 [03:13<00:49,  3.41it/s] 72%|███████▏  | 419/585 [03:13<00:50,  3.30it/s] 72%|███████▏  | 420/585 [03:14<00:49,  3.33it/s] 72%|███████▏  | 421/585 [03:14<00:48,  3.36it/s] 72%|███████▏  | 422/585 [03:14<00:48,  3.37it/s] 72%|███████▏  | 423/585 [03:14<00:47,  3.38it/s] 72%|███████▏  | 424/585 [03:15<00:47,  3.39it/s] 73%|███████▎  | 425/585 [03:15<00:47,  3.39it/s] 73%|███████▎  | 426/585 [03:15<00:46,  3.40it/s] 73%|███████▎  | 427/585 [03:16<00:46,  3.40it/s] 73%|███████▎  | 428/585 [03:16<00:46,  3.40it/s] 73%|███████▎  | 429/585 [03:16<00:45,  3.40it/s] 74%|███████▎  | 430/585 [03:16<00:45,  3.40it/s] 74%|███████▎  | 431/585 [03:17<00:46,  3.34it/s] 74%|███████▍  | 432/585 [03:17<00:45,  3.36it/s] 74%|███████▍  | 433/585 [03:17<00:45,  3.37it/s] 74%|███████▍  | 434/585 [03:18<00:44,  3.39it/s] 74%|███████▍  | 435/585 [03:18<00:44,  3.39it/s] 75%|███████▍  | 436/585 [03:18<00:43,  3.40it/s] 75%|███████▍  | 437/585 [03:19<00:43,  3.40it/s] 75%|███████▍  | 438/585 [03:19<00:43,  3.40it/s] 75%|███████▌  | 439/585 [03:19<00:42,  3.40it/s] 75%|███████▌  | 440/585 [03:19<00:42,  3.41it/s] 75%|███████▌  | 441/585 [03:20<00:42,  3.41it/s] 76%|███████▌  | 442/585 [03:20<00:42,  3.39it/s] 76%|███████▌  | 443/585 [03:20<00:41,  3.39it/s] 76%|███████▌  | 444/585 [03:21<00:41,  3.40it/s] 76%|███████▌  | 445/585 [03:21<00:41,  3.40it/s] 76%|███████▌  | 446/585 [03:21<00:40,  3.40it/s] 76%|███████▋  | 447/585 [03:21<00:40,  3.40it/s] 77%|███████▋  | 448/585 [03:22<00:40,  3.40it/s] 77%|███████▋  | 449/585 [03:22<00:39,  3.40it/s] 77%|███████▋  | 450/585 [03:22<00:39,  3.40it/s] 77%|███████▋  | 451/585 [03:23<00:39,  3.41it/s] 77%|███████▋  | 452/585 [03:23<00:39,  3.40it/s] 77%|███████▋  | 453/585 [03:23<00:40,  3.27it/s] 78%|███████▊  | 454/585 [03:24<00:39,  3.31it/s] 78%|███████▊  | 455/585 [03:24<00:38,  3.34it/s] 78%|███████▊  | 456/585 [03:24<00:38,  3.35it/s] 78%|███████▊  | 457/585 [03:24<00:37,  3.37it/s] 78%|███████▊  | 458/585 [03:25<00:37,  3.38it/s] 78%|███████▊  | 459/585 [03:25<00:37,  3.39it/s] 79%|███████▊  | 460/585 [03:25<00:36,  3.39it/s] 79%|███████▉  | 461/585 [03:26<00:36,  3.40it/s] 79%|███████▉  | 462/585 [03:26<00:36,  3.40it/s] 79%|███████▉  | 463/585 [03:26<00:35,  3.40it/s] 79%|███████▉  | 464/585 [03:27<00:36,  3.28it/s] 79%|███████▉  | 465/585 [03:27<00:36,  3.31it/s] 80%|███████▉  | 466/585 [03:27<00:35,  3.34it/s] 80%|███████▉  | 467/585 [03:27<00:35,  3.36it/s] 80%|████████  | 468/585 [03:28<00:34,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 22:02:07,645 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:02:07,645 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:02:07,645 >>   Batch size = 8
{'eval_loss': 1.058699131011963, 'eval_runtime': 9.8018, 'eval_samples_per_second': 356.363, 'eval_steps_per_second': 44.584, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.40it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.60it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.08it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.14it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.61it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.33it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.03it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.94it/s][A
 11%|█         | 47/437 [00:01<00:08, 45.02it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.05it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.19it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.12it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 45.14it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 41.88it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 42.80it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.37it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.71it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.24it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.47it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.69it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.72it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.58it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.70it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.81it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.81it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.83it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.89it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.91it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.98it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.95it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.82it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.84it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.90it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.90it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.98it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.99it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.93it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.92it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.95it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.87it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.85it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.87it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.90it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.95it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.84it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 45.01it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.98it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.90it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.83it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.93it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.89it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.89it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.77it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.90it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 45.08it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 45.01it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.91it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.88it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.90it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.86it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.85it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.84it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.93it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 45.03it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.95it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.92it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.87it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.84it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.78it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.71it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.85it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 45.03it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 45.02it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 45.01it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.93it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.85it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.80it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.63it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.77it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.87it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 45.00it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 45.02it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.95it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.91it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.95it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.77it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.67it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.67it/s][A 80%|████████  | 468/585 [03:38<00:34,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:02:17,493 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 22:02:17,577 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:02:21,448 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:02:21,469 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:02:21,481 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:50<13:11,  6.82s/it] 80%|████████  | 470/585 [03:50<09:19,  4.87s/it] 81%|████████  | 471/585 [03:50<06:38,  3.49s/it] 81%|████████  | 472/585 [03:51<04:46,  2.53s/it] 81%|████████  | 473/585 [03:51<03:28,  1.86s/it] 81%|████████  | 474/585 [03:51<02:34,  1.39s/it] 81%|████████  | 475/585 [03:52<01:56,  1.06s/it] 81%|████████▏ | 476/585 [03:52<01:30,  1.20it/s] 82%|████████▏ | 477/585 [03:52<01:12,  1.49it/s] 82%|████████▏ | 478/585 [03:52<00:59,  1.80it/s] 82%|████████▏ | 479/585 [03:53<00:50,  2.09it/s] 82%|████████▏ | 480/585 [03:53<00:44,  2.37it/s] 82%|████████▏ | 481/585 [03:53<00:39,  2.60it/s] 82%|████████▏ | 482/585 [03:54<00:36,  2.80it/s] 83%|████████▎ | 483/585 [03:54<00:34,  2.96it/s] 83%|████████▎ | 484/585 [03:54<00:32,  3.08it/s] 83%|████████▎ | 485/585 [03:54<00:31,  3.17it/s] 83%|████████▎ | 486/585 [03:55<00:30,  3.21it/s] 83%|████████▎ | 487/585 [03:55<00:29,  3.27it/s] 83%|████████▎ | 488/585 [03:55<00:29,  3.31it/s] 84%|████████▎ | 489/585 [03:56<00:28,  3.34it/s] 84%|████████▍ | 490/585 [03:56<00:28,  3.36it/s] 84%|████████▍ | 491/585 [03:56<00:27,  3.38it/s] 84%|████████▍ | 492/585 [03:57<00:27,  3.38it/s] 84%|████████▍ | 493/585 [03:57<00:32,  2.80it/s] 84%|████████▍ | 494/585 [03:57<00:30,  2.96it/s] 85%|████████▍ | 495/585 [03:58<00:29,  3.08it/s] 85%|████████▍ | 496/585 [03:58<00:28,  3.17it/s] 85%|████████▍ | 497/585 [03:58<00:27,  3.24it/s] 85%|████████▌ | 498/585 [03:59<00:26,  3.29it/s] 85%|████████▌ | 499/585 [03:59<00:25,  3.32it/s] 85%|████████▌ | 500/585 [03:59<00:25,  3.34it/s]                                                  85%|████████▌ | 500/585 [03:59<00:25,  3.34it/s] 86%|████████▌ | 501/585 [03:59<00:24,  3.37it/s] 86%|████████▌ | 502/585 [04:00<00:24,  3.38it/s] 86%|████████▌ | 503/585 [04:00<00:24,  3.40it/s] 86%|████████▌ | 504/585 [04:00<00:23,  3.42it/s] 86%|████████▋ | 505/585 [04:01<00:23,  3.43it/s] 86%|████████▋ | 506/585 [04:01<00:22,  3.44it/s] 87%|████████▋ | 507/585 [04:01<00:22,  3.44it/s] 87%|████████▋ | 508/585 [04:01<00:22,  3.45it/s] 87%|████████▋ | 509/585 [04:02<00:22,  3.45it/s] 87%|████████▋ | 510/585 [04:02<00:21,  3.45it/s] 87%|████████▋ | 511/585 [04:02<00:21,  3.46it/s] 88%|████████▊ | 512/585 [04:03<00:21,  3.46it/s] 88%|████████▊ | 513/585 [04:03<00:20,  3.43it/s] 88%|████████▊ | 514/585 [04:03<00:20,  3.44it/s] 88%|████████▊ | 515/585 [04:03<00:20,  3.45it/s] 88%|████████▊ | 516/585 [04:04<00:20,  3.45it/s] 88%|████████▊ | 517/585 [04:04<00:19,  3.45it/s] 89%|████████▊ | 518/585 [04:04<00:19,  3.45it/s] 89%|████████▊ | 519/585 [04:05<00:19,  3.46it/s] 89%|████████▉ | 520/585 [04:05<00:18,  3.45it/s] 89%|████████▉ | 521/585 [04:05<00:18,  3.46it/s] 89%|████████▉ | 522/585 [04:05<00:18,  3.46it/s] 89%|████████▉ | 523/585 [04:06<00:17,  3.46it/s] 90%|████████▉ | 524/585 [04:06<00:17,  3.44it/s] 90%|████████▉ | 525/585 [04:06<00:17,  3.45it/s] 90%|████████▉ | 526/585 [04:07<00:17,  3.45it/s] 90%|█████████ | 527/585 [04:07<00:16,  3.45it/s] 90%|█████████ | 528/585 [04:07<00:16,  3.45it/s] 90%|█████████ | 529/585 [04:08<00:16,  3.46it/s] 91%|█████████ | 530/585 [04:08<00:15,  3.45it/s] 91%|█████████ | 531/585 [04:08<00:15,  3.46it/s] 91%|█████████ | 532/585 [04:08<00:15,  3.45it/s] 91%|█████████ | 533/585 [04:09<00:15,  3.46it/s] 91%|█████████▏| 534/585 [04:09<00:14,  3.45it/s] 91%|█████████▏| 535/585 [04:09<00:14,  3.45it/s] 92%|█████████▏| 536/585 [04:10<00:14,  3.45it/s] 92%|█████████▏| 537/585 [04:10<00:13,  3.45it/s] 92%|█████████▏| 538/585 [04:10<00:13,  3.45it/s] 92%|█████████▏| 539/585 [04:10<00:13,  3.45it/s] 92%|█████████▏| 540/585 [04:11<00:13,  3.45it/s] 92%|█████████▏| 541/585 [04:11<00:12,  3.45it/s] 93%|█████████▎| 542/585 [04:11<00:12,  3.46it/s] 93%|█████████▎| 543/585 [04:12<00:12,  3.46it/s] 93%|█████████▎| 544/585 [04:12<00:11,  3.45it/s] 93%|█████████▎| 545/585 [04:12<00:11,  3.46it/s] 93%|█████████▎| 546/585 [04:12<00:11,  3.44it/s] 94%|█████████▎| 547/585 [04:13<00:11,  3.45it/s] 94%|█████████▎| 548/585 [04:13<00:10,  3.45it/s] 94%|█████████▍| 549/585 [04:13<00:10,  3.45it/s] 94%|█████████▍| 550/585 [04:14<00:10,  3.45it/s] 94%|█████████▍| 551/585 [04:14<00:09,  3.45it/s] 94%|█████████▍| 552/585 [04:14<00:09,  3.45it/s] 95%|█████████▍| 553/585 [04:14<00:09,  3.46it/s] 95%|█████████▍| 554/585 [04:15<00:08,  3.45it/s] 95%|█████████▍| 555/585 [04:15<00:08,  3.46it/s] 95%|█████████▌| 556/585 [04:15<00:08,  3.45it/s] 95%|█████████▌| 557/585 [04:16<00:08,  3.45it/s] 95%|█████████▌| 558/585 [04:16<00:07,  3.45it/s] 96%|█████████▌| 559/585 [04:16<00:07,  3.45it/s] 96%|█████████▌| 560/585 [04:16<00:07,  3.46it/s] 96%|█████████▌| 561/585 [04:17<00:06,  3.46it/s] 96%|█████████▌| 562/585 [04:17<00:06,  3.46it/s] 96%|█████████▌| 563/585 [04:17<00:06,  3.45it/s] 96%|█████████▋| 564/585 [04:18<00:06,  3.46it/s] 97%|█████████▋| 565/585 [04:18<00:05,  3.46it/s] 97%|█████████▋| 566/585 [04:18<00:05,  3.40it/s] 97%|█████████▋| 567/585 [04:19<00:05,  3.42it/s] 97%|█████████▋| 568/585 [04:19<00:04,  3.43it/s] 97%|█████████▋| 569/585 [04:19<00:04,  3.44it/s] 97%|█████████▋| 570/585 [04:19<00:04,  3.44it/s] 98%|█████████▊| 571/585 [04:20<00:04,  3.45it/s] 98%|█████████▊| 572/585 [04:20<00:03,  3.45it/s] 98%|█████████▊| 573/585 [04:20<00:03,  3.45it/s] 98%|█████████▊| 574/585 [04:21<00:03,  3.45it/s] 98%|█████████▊| 575/585 [04:21<00:02,  3.45it/s] 98%|█████████▊| 576/585 [04:21<00:02,  3.45it/s] 99%|█████████▊| 577/585 [04:21<00:02,  3.45it/s] 99%|█████████▉| 578/585 [04:22<00:02,  3.45it/s] 99%|█████████▉| 579/585 [04:22<00:01,  3.45it/s] 99%|█████████▉| 580/585 [04:22<00:01,  3.45it/s] 99%|█████████▉| 581/585 [04:23<00:01,  3.45it/s] 99%|█████████▉| 582/585 [04:23<00:00,  3.45it/s]100%|█████████▉| 583/585 [04:23<00:00,  3.45it/s]100%|█████████▉| 584/585 [04:23<00:00,  3.45it/s]100%|██████████| 585/585 [04:24<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 22:03:03,612 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:03:03,612 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:03:03,612 >>   Batch size = 8
{'eval_loss': 1.065119743347168, 'eval_runtime': 9.7661, 'eval_samples_per_second': 357.666, 'eval_steps_per_second': 44.747, 'epoch': 4.0}
{'loss': 0.529, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.24it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.23it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.23it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.24it/s][A
  6%|▌         | 27/437 [00:00<00:09, 42.78it/s][A
  7%|▋         | 32/437 [00:00<00:09, 43.52it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.92it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.97it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.38it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.53it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.75it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.94it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.59it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.84it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.93it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.85it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.83it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.90it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.73it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.89it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.91it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.86it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.93it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.96it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.90it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.80it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.87it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.84it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.94it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.92it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.82it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.89it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.91it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.94it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.94it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.85it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.90it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.86it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.95it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.88it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.87it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.73it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.90it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.99it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.98it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.90it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.98it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.85it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.97it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.91it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.83it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.86it/s][A
 61%|██████    | 267/437 [00:05<00:03, 45.00it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.86it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.99it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.92it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.90it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.98it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.66it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.70it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.81it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.88it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.94it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.96it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.96it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.93it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.86it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.75it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.66it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.80it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.84it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.79it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.89it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.90it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.94it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.86it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.87it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.78it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.86it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.88it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.90it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.90it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.91it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.87it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.91it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.96it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.86it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:34<00:00,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 44.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:03:13,390 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 22:03:13,425 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:03:17,868 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:03:18,030 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:03:18,055 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 22:03:26,693 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 22:03:26,694 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117 (score: 1.0341764688491821).
                                                 100%|██████████| 585/585 [04:51<00:00,  3.45it/s]100%|██████████| 585/585 [04:51<00:00,  2.01it/s]
[INFO|trainer.py:1894] 2023-08-28 22:03:31,038 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 22:03:31,056 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:03:35,139 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:03:35,158 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:03:35,170 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:03:35,381 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:35,381 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:35,381 >>   train_loss               =     0.5251
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:35,381 >>   train_runtime            = 0:04:51.64
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:35,381 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:35,381 >>   train_samples_per_second =    128.563
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:35,381 >>   train_steps_per_second   =      2.006
{'eval_loss': 1.073256015777588, 'eval_runtime': 9.7501, 'eval_samples_per_second': 358.252, 'eval_steps_per_second': 44.82, 'epoch': 5.0}
{'train_runtime': 291.6478, 'train_samples_per_second': 128.563, 'train_steps_per_second': 2.006, 'train_loss': 0.5251033228686732, 'epoch': 5.0}
08/28/2023 22:03:35 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 22:03:35,630 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:03:35,630 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-28 22:03:35,630 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 56.17it/s]  3%|▎         | 12/437 [00:00<00:08, 49.51it/s]  4%|▍         | 18/437 [00:00<00:08, 47.70it/s]  5%|▌         | 23/437 [00:00<00:08, 46.90it/s]  6%|▋         | 28/437 [00:00<00:08, 46.55it/s]  8%|▊         | 33/437 [00:00<00:08, 46.26it/s]  9%|▊         | 38/437 [00:00<00:08, 45.95it/s] 10%|▉         | 43/437 [00:00<00:08, 45.42it/s] 11%|█         | 48/437 [00:01<00:08, 45.06it/s] 12%|█▏        | 53/437 [00:01<00:08, 44.35it/s] 13%|█▎        | 58/437 [00:01<00:08, 44.28it/s] 14%|█▍        | 63/437 [00:01<00:08, 45.12it/s] 16%|█▌        | 68/437 [00:01<00:08, 45.16it/s] 17%|█▋        | 73/437 [00:01<00:08, 45.34it/s] 18%|█▊        | 78/437 [00:01<00:07, 45.22it/s] 19%|█▉        | 83/437 [00:01<00:07, 45.35it/s] 20%|██        | 88/437 [00:01<00:07, 45.17it/s] 21%|██▏       | 93/437 [00:02<00:07, 44.86it/s] 22%|██▏       | 98/437 [00:02<00:07, 44.72it/s] 24%|██▎       | 103/437 [00:02<00:07, 44.79it/s] 25%|██▍       | 108/437 [00:02<00:07, 45.00it/s] 26%|██▌       | 113/437 [00:02<00:07, 45.10it/s] 27%|██▋       | 118/437 [00:02<00:07, 45.13it/s] 28%|██▊       | 123/437 [00:02<00:06, 45.30it/s] 29%|██▉       | 128/437 [00:02<00:06, 45.25it/s] 30%|███       | 133/437 [00:02<00:06, 45.05it/s] 32%|███▏      | 138/437 [00:03<00:06, 44.78it/s] 33%|███▎      | 143/437 [00:03<00:06, 44.68it/s] 34%|███▍      | 148/437 [00:03<00:06, 44.79it/s] 35%|███▌      | 153/437 [00:03<00:06, 44.88it/s] 36%|███▌      | 158/437 [00:03<00:06, 45.02it/s] 37%|███▋      | 163/437 [00:03<00:06, 45.21it/s] 38%|███▊      | 168/437 [00:03<00:05, 45.29it/s] 40%|███▉      | 173/437 [00:03<00:05, 45.29it/s] 41%|████      | 178/437 [00:03<00:05, 45.07it/s] 42%|████▏     | 183/437 [00:04<00:05, 44.81it/s] 43%|████▎     | 188/437 [00:04<00:05, 44.81it/s] 44%|████▍     | 193/437 [00:04<00:05, 44.86it/s] 45%|████▌     | 198/437 [00:04<00:05, 44.88it/s] 46%|████▋     | 203/437 [00:04<00:05, 45.04it/s] 48%|████▊     | 208/437 [00:04<00:05, 45.23it/s] 49%|████▊     | 213/437 [00:04<00:04, 45.26it/s] 50%|████▉     | 218/437 [00:04<00:04, 45.33it/s] 51%|█████     | 223/437 [00:04<00:04, 45.18it/s] 52%|█████▏    | 228/437 [00:05<00:04, 44.90it/s] 53%|█████▎    | 233/437 [00:05<00:04, 44.79it/s] 54%|█████▍    | 238/437 [00:05<00:04, 44.84it/s] 56%|█████▌    | 243/437 [00:05<00:04, 44.83it/s] 57%|█████▋    | 248/437 [00:05<00:04, 45.03it/s] 58%|█████▊    | 253/437 [00:05<00:04, 45.19it/s] 59%|█████▉    | 258/437 [00:05<00:03, 45.29it/s] 60%|██████    | 263/437 [00:05<00:03, 45.30it/s] 61%|██████▏   | 268/437 [00:05<00:03, 45.18it/s] 62%|██████▏   | 273/437 [00:06<00:03, 44.91it/s] 64%|██████▎   | 278/437 [00:06<00:03, 44.87it/s] 65%|██████▍   | 283/437 [00:06<00:03, 44.85it/s] 66%|██████▌   | 288/437 [00:06<00:03, 44.95it/s] 67%|██████▋   | 293/437 [00:06<00:03, 45.03it/s] 68%|██████▊   | 298/437 [00:06<00:03, 45.15it/s] 69%|██████▉   | 303/437 [00:06<00:02, 45.21it/s] 70%|███████   | 308/437 [00:06<00:02, 45.25it/s] 72%|███████▏  | 313/437 [00:06<00:02, 45.05it/s] 73%|███████▎  | 318/437 [00:07<00:02, 44.86it/s] 74%|███████▍  | 323/437 [00:07<00:02, 44.81it/s] 75%|███████▌  | 328/437 [00:07<00:02, 44.73it/s] 76%|███████▌  | 333/437 [00:07<00:02, 44.90it/s] 77%|███████▋  | 338/437 [00:07<00:02, 44.81it/s] 78%|███████▊  | 343/437 [00:07<00:02, 45.08it/s] 80%|███████▉  | 348/437 [00:07<00:01, 45.18it/s] 81%|████████  | 353/437 [00:07<00:01, 45.22it/s] 82%|████████▏ | 358/437 [00:07<00:01, 45.07it/s] 83%|████████▎ | 363/437 [00:08<00:01, 44.83it/s] 84%|████████▍ | 368/437 [00:08<00:01, 44.73it/s] 85%|████████▌ | 373/437 [00:08<00:01, 44.86it/s] 86%|████████▋ | 378/437 [00:08<00:01, 44.78it/s] 88%|████████▊ | 383/437 [00:08<00:01, 44.99it/s] 89%|████████▉ | 388/437 [00:08<00:01, 45.11it/s] 90%|████████▉ | 393/437 [00:08<00:00, 44.61it/s] 91%|█████████ | 398/437 [00:08<00:00, 44.83it/s] 92%|█████████▏| 403/437 [00:08<00:00, 44.74it/s] 93%|█████████▎| 408/437 [00:09<00:00, 44.66it/s] 95%|█████████▍| 413/437 [00:09<00:00, 44.63it/s] 96%|█████████▌| 418/437 [00:09<00:00, 44.65it/s] 97%|█████████▋| 423/437 [00:09<00:00, 44.67it/s] 98%|█████████▊| 428/437 [00:09<00:00, 44.88it/s] 99%|█████████▉| 433/437 [00:09<00:00, 44.93it/s]100%|██████████| 437/437 [00:09<00:00, 45.10it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:03:45,338 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:45,338 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:45,338 >>   eval_loss               =     1.0342
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:45,339 >>   eval_runtime            = 0:00:09.70
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:45,339 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:45,339 >>   eval_samples_per_second =    359.798
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:45,339 >>   eval_steps_per_second   =     45.013
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:03:45,339 >>   perplexity              =     2.8128
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:05,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:05,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:05,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:05,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:05,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:04:05,836 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:04:05,837 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:04:06,118 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:04:10,384 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:04:10,402 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:12,276 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:12,278 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:12,278 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:12,278 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:04:12,278 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:04:12,813 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:04:12,814 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:04:13,100 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:04:14,195 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:04:14,212 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.52it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:09,  1.62it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.60it/s]Extractor Predicting: 22it [00:14,  1.62it/s]Extractor Predicting: 23it [00:14,  1.62it/s]Extractor Predicting: 24it [00:15,  1.62it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:17,  1.63it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:19,  1.66it/s]Extractor Predicting: 32it [00:20,  1.66it/s]Extractor Predicting: 33it [00:20,  1.62it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:22,  1.58it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:23,  1.61it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:25,  1.59it/s]Extractor Predicting: 41it [00:25,  1.52it/s]Extractor Predicting: 42it [00:26,  1.57it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:27,  1.61it/s]Extractor Predicting: 45it [00:28,  1.57it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:30,  1.64it/s]Extractor Predicting: 49it [00:30,  1.66it/s]Extractor Predicting: 50it [00:31,  1.62it/s]Extractor Predicting: 51it [00:32,  1.62it/s]Extractor Predicting: 52it [00:32,  1.63it/s]Extractor Predicting: 53it [00:33,  1.66it/s]Extractor Predicting: 54it [00:33,  1.67it/s]Extractor Predicting: 55it [00:34,  1.66it/s]Extractor Predicting: 56it [00:35,  1.63it/s]Extractor Predicting: 57it [00:35,  1.65it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:36,  1.61it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:38,  1.59it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:40,  1.57it/s]Extractor Predicting: 66it [00:41,  1.56it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:42,  1.52it/s]Extractor Predicting: 69it [00:43,  1.54it/s]Extractor Predicting: 70it [00:44,  1.55it/s]Extractor Predicting: 71it [00:44,  1.57it/s]Extractor Predicting: 72it [00:45,  1.61it/s]Extractor Predicting: 73it [00:45,  1.59it/s]Extractor Predicting: 74it [00:46,  1.56it/s]Extractor Predicting: 75it [00:47,  1.60it/s]Extractor Predicting: 76it [00:47,  1.57it/s]Extractor Predicting: 77it [00:48,  1.56it/s]Extractor Predicting: 78it [00:49,  1.54it/s]Extractor Predicting: 79it [00:49,  1.54it/s]Extractor Predicting: 80it [00:50,  1.54it/s]Extractor Predicting: 81it [00:51,  1.52it/s]Extractor Predicting: 82it [00:51,  1.53it/s]Extractor Predicting: 83it [00:52,  1.58it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:53,  1.58it/s]Extractor Predicting: 86it [00:54,  1.52it/s]Extractor Predicting: 87it [00:54,  1.57it/s]Extractor Predicting: 88it [00:55,  1.56it/s]Extractor Predicting: 89it [00:56,  1.54it/s]Extractor Predicting: 90it [00:56,  1.51it/s]Extractor Predicting: 91it [00:57,  1.51it/s]Extractor Predicting: 92it [00:58,  1.53it/s]Extractor Predicting: 93it [00:58,  1.53it/s]Extractor Predicting: 94it [00:59,  1.56it/s]Extractor Predicting: 95it [01:00,  1.60it/s]Extractor Predicting: 96it [01:00,  1.61it/s]Extractor Predicting: 97it [01:01,  1.58it/s]Extractor Predicting: 98it [01:01,  1.59it/s]Extractor Predicting: 99it [01:02,  1.58it/s]Extractor Predicting: 100it [01:03,  1.60it/s]Extractor Predicting: 101it [01:03,  1.60it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:05,  1.58it/s]Extractor Predicting: 104it [01:05,  1.57it/s]Extractor Predicting: 105it [01:06,  1.58it/s]Extractor Predicting: 106it [01:07,  1.56it/s]Extractor Predicting: 107it [01:07,  1.56it/s]Extractor Predicting: 108it [01:08,  1.55it/s]Extractor Predicting: 109it [01:09,  1.55it/s]Extractor Predicting: 110it [01:09,  1.54it/s]Extractor Predicting: 111it [01:10,  1.54it/s]Extractor Predicting: 112it [01:10,  1.55it/s]Extractor Predicting: 113it [01:11,  1.53it/s]Extractor Predicting: 114it [01:12,  1.52it/s]Extractor Predicting: 115it [01:12,  1.50it/s]Extractor Predicting: 116it [01:13,  1.52it/s]Extractor Predicting: 117it [01:14,  1.51it/s]Extractor Predicting: 118it [01:14,  1.51it/s]Extractor Predicting: 119it [01:15,  1.50it/s]Extractor Predicting: 120it [01:16,  1.52it/s]Extractor Predicting: 121it [01:16,  1.51it/s]Extractor Predicting: 122it [01:17,  1.51it/s]Extractor Predicting: 123it [01:18,  1.56it/s]Extractor Predicting: 124it [01:18,  1.54it/s]Extractor Predicting: 125it [01:19,  1.51it/s]Extractor Predicting: 126it [01:20,  1.52it/s]Extractor Predicting: 127it [01:21,  1.40it/s]Extractor Predicting: 128it [01:21,  1.44it/s]Extractor Predicting: 129it [01:22,  1.44it/s]Extractor Predicting: 130it [01:23,  1.46it/s]Extractor Predicting: 131it [01:23,  1.45it/s]Extractor Predicting: 132it [01:24,  1.45it/s]Extractor Predicting: 133it [01:25,  1.45it/s]Extractor Predicting: 134it [01:25,  1.45it/s]Extractor Predicting: 135it [01:26,  1.47it/s]Extractor Predicting: 136it [01:26,  1.80it/s]Extractor Predicting: 136it [01:26,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:48,765 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:48,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:48,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:48,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:48,769 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:05:49,173 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:05:49,174 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:05:49,466 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:05:50,507 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:05:50,507 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:52,312 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:52,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:52,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:52,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:05:52,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:05:52,671 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:05:52,672 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:05:52,931 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:05:53,101 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:05:53,101 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6228209191759112,
  "recall": 0.11251073575722874,
  "score": 0.19059165858389912,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.75it/s]Extractor Predicting: 2it [00:01,  1.80it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.59it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:19,  1.74it/s]Extractor Predicting: 34it [00:20,  1.72it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:21,  1.58it/s]Extractor Predicting: 37it [00:22,  1.59it/s]Extractor Predicting: 38it [00:23,  1.58it/s]Extractor Predicting: 39it [00:23,  1.62it/s]Extractor Predicting: 40it [00:24,  1.69it/s]Extractor Predicting: 41it [00:24,  1.68it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:25,  1.71it/s]Extractor Predicting: 44it [00:26,  1.69it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:27,  1.69it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.70it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:30,  1.75it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:31,  1.69it/s]Extractor Predicting: 54it [00:32,  1.66it/s]Extractor Predicting: 55it [00:33,  1.68it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.65it/s]Extractor Predicting: 58it [00:34,  1.67it/s]Extractor Predicting: 59it [00:35,  1.64it/s]Extractor Predicting: 60it [00:36,  1.64it/s]Extractor Predicting: 61it [00:36,  1.66it/s]Extractor Predicting: 62it [00:37,  1.65it/s]Extractor Predicting: 63it [00:37,  1.62it/s]Extractor Predicting: 64it [00:38,  1.69it/s]Extractor Predicting: 65it [00:39,  1.67it/s]Extractor Predicting: 66it [00:39,  1.66it/s]Extractor Predicting: 67it [00:40,  1.68it/s]Extractor Predicting: 68it [00:40,  1.72it/s]Extractor Predicting: 69it [00:41,  1.74it/s]Extractor Predicting: 70it [00:42,  1.73it/s]Extractor Predicting: 71it [00:42,  1.72it/s]Extractor Predicting: 72it [00:43,  1.69it/s]Extractor Predicting: 73it [00:43,  1.71it/s]Extractor Predicting: 74it [00:44,  1.66it/s]Extractor Predicting: 75it [00:45,  1.66it/s]Extractor Predicting: 76it [00:45,  1.67it/s]Extractor Predicting: 77it [00:46,  1.66it/s]Extractor Predicting: 78it [00:46,  1.67it/s]Extractor Predicting: 79it [00:47,  1.67it/s]Extractor Predicting: 80it [00:48,  1.69it/s]Extractor Predicting: 81it [00:48,  1.69it/s]Extractor Predicting: 82it [00:49,  1.70it/s]Extractor Predicting: 83it [00:49,  1.65it/s]Extractor Predicting: 84it [00:50,  1.66it/s]Extractor Predicting: 85it [00:51,  1.63it/s]Extractor Predicting: 86it [00:51,  1.60it/s]Extractor Predicting: 87it [00:52,  1.66it/s]Extractor Predicting: 88it [00:52,  1.61it/s]Extractor Predicting: 89it [00:53,  1.60it/s]Extractor Predicting: 90it [00:54,  1.57it/s]Extractor Predicting: 91it [00:54,  1.58it/s]Extractor Predicting: 92it [00:55,  1.54it/s]Extractor Predicting: 93it [00:56,  1.53it/s]Extractor Predicting: 94it [00:56,  1.54it/s]Extractor Predicting: 95it [00:57,  1.58it/s]Extractor Predicting: 96it [00:58,  1.58it/s]Extractor Predicting: 97it [00:58,  1.54it/s]Extractor Predicting: 98it [00:59,  1.54it/s]Extractor Predicting: 99it [01:00,  1.53it/s]Extractor Predicting: 100it [01:00,  1.53it/s]Extractor Predicting: 101it [01:01,  1.53it/s]Extractor Predicting: 102it [01:02,  1.50it/s]Extractor Predicting: 103it [01:02,  1.55it/s]Extractor Predicting: 104it [01:03,  1.56it/s]Extractor Predicting: 105it [01:03,  1.57it/s]Extractor Predicting: 106it [01:04,  1.53it/s]Extractor Predicting: 107it [01:05,  1.54it/s]Extractor Predicting: 108it [01:05,  1.51it/s]Extractor Predicting: 109it [01:06,  1.51it/s]Extractor Predicting: 110it [01:07,  1.51it/s]Extractor Predicting: 111it [01:07,  1.53it/s]Extractor Predicting: 112it [01:08,  1.51it/s]Extractor Predicting: 113it [01:09,  1.52it/s]Extractor Predicting: 114it [01:09,  1.52it/s]Extractor Predicting: 115it [01:10,  1.51it/s]Extractor Predicting: 116it [01:11,  1.53it/s]Extractor Predicting: 117it [01:11,  1.51it/s]Extractor Predicting: 118it [01:12,  1.53it/s]Extractor Predicting: 119it [01:13,  1.55it/s]Extractor Predicting: 120it [01:13,  1.60it/s]Extractor Predicting: 121it [01:14,  1.59it/s]Extractor Predicting: 122it [01:15,  1.57it/s]Extractor Predicting: 123it [01:15,  1.58it/s]Extractor Predicting: 124it [01:16,  1.57it/s]Extractor Predicting: 125it [01:16,  1.59it/s]Extractor Predicting: 126it [01:17,  1.63it/s]Extractor Predicting: 127it [01:18,  1.63it/s]Extractor Predicting: 128it [01:18,  1.66it/s]Extractor Predicting: 129it [01:19,  1.63it/s]Extractor Predicting: 130it [01:19,  1.57it/s]Extractor Predicting: 131it [01:20,  1.59it/s]Extractor Predicting: 132it [01:21,  1.60it/s]Extractor Predicting: 133it [01:21,  1.63it/s]Extractor Predicting: 134it [01:22,  1.66it/s]Extractor Predicting: 135it [01:23,  1.63it/s]Extractor Predicting: 136it [01:23,  1.66it/s]Extractor Predicting: 137it [01:24,  1.67it/s]Extractor Predicting: 138it [01:24,  1.64it/s]Extractor Predicting: 139it [01:25,  1.48it/s]Extractor Predicting: 140it [01:26,  1.52it/s]Extractor Predicting: 141it [01:26,  1.49it/s]Extractor Predicting: 142it [01:27,  1.51it/s]Extractor Predicting: 143it [01:28,  1.53it/s]Extractor Predicting: 144it [01:28,  1.54it/s]Extractor Predicting: 145it [01:29,  1.53it/s]Extractor Predicting: 146it [01:30,  1.58it/s]Extractor Predicting: 147it [01:30,  1.60it/s]Extractor Predicting: 148it [01:31,  1.58it/s]Extractor Predicting: 149it [01:32,  1.58it/s]Extractor Predicting: 150it [01:32,  1.56it/s]Extractor Predicting: 151it [01:33,  1.52it/s]Extractor Predicting: 152it [01:34,  1.54it/s]Extractor Predicting: 153it [01:34,  1.54it/s]Extractor Predicting: 154it [01:35,  1.59it/s]Extractor Predicting: 155it [01:35,  1.52it/s]Extractor Predicting: 156it [01:36,  1.54it/s]Extractor Predicting: 157it [01:37,  1.52it/s]Extractor Predicting: 158it [01:37,  1.55it/s]Extractor Predicting: 159it [01:38,  1.54it/s]Extractor Predicting: 160it [01:39,  1.50it/s]Extractor Predicting: 161it [01:39,  1.53it/s]Extractor Predicting: 162it [01:40,  1.53it/s]Extractor Predicting: 163it [01:41,  1.53it/s]Extractor Predicting: 164it [01:41,  1.55it/s]Extractor Predicting: 165it [01:42,  1.54it/s]Extractor Predicting: 166it [01:43,  1.56it/s]Extractor Predicting: 167it [01:43,  1.54it/s]Extractor Predicting: 168it [01:44,  1.54it/s]Extractor Predicting: 169it [01:45,  1.53it/s]Extractor Predicting: 170it [01:45,  1.52it/s]Extractor Predicting: 171it [01:46,  1.53it/s]Extractor Predicting: 172it [01:47,  1.53it/s]Extractor Predicting: 173it [01:47,  1.53it/s]Extractor Predicting: 174it [01:48,  1.53it/s]Extractor Predicting: 175it [01:48,  1.58it/s]Extractor Predicting: 176it [01:49,  1.57it/s]Extractor Predicting: 177it [01:50,  1.59it/s]Extractor Predicting: 178it [01:50,  1.59it/s]Extractor Predicting: 179it [01:51,  1.62it/s]Extractor Predicting: 180it [01:52,  1.61it/s]Extractor Predicting: 181it [01:52,  1.62it/s]Extractor Predicting: 182it [01:53,  1.64it/s]Extractor Predicting: 183it [01:53,  1.55it/s]Extractor Predicting: 184it [01:54,  1.63it/s]Extractor Predicting: 185it [01:55,  1.61it/s]Extractor Predicting: 186it [01:55,  1.63it/s]Extractor Predicting: 187it [01:56,  1.63it/s]Extractor Predicting: 188it [01:57,  1.58it/s]Extractor Predicting: 189it [01:57,  1.58it/s]Extractor Predicting: 190it [01:58,  1.58it/s]Extractor Predicting: 191it [01:58,  1.57it/s]Extractor Predicting: 192it [01:59,  1.58it/s]Extractor Predicting: 193it [02:00,  1.61it/s]Extractor Predicting: 194it [02:00,  1.60it/s]Extractor Predicting: 195it [02:01,  1.58it/s]Extractor Predicting: 196it [02:02,  1.54it/s]Extractor Predicting: 197it [02:02,  1.57it/s]Extractor Predicting: 198it [02:03,  1.55it/s]Extractor Predicting: 199it [02:04,  1.58it/s]Extractor Predicting: 200it [02:04,  1.58it/s]Extractor Predicting: 201it [02:05,  1.59it/s]Extractor Predicting: 202it [02:05,  1.60it/s]Extractor Predicting: 203it [02:06,  1.59it/s]Extractor Predicting: 204it [02:07,  1.62it/s]Extractor Predicting: 205it [02:07,  1.61it/s]Extractor Predicting: 206it [02:08,  1.65it/s]Extractor Predicting: 207it [02:08,  1.62it/s]Extractor Predicting: 208it [02:09,  1.60it/s]Extractor Predicting: 209it [02:10,  1.61it/s]Extractor Predicting: 210it [02:10,  1.62it/s]Extractor Predicting: 211it [02:11,  1.60it/s]Extractor Predicting: 212it [02:12,  1.61it/s]Extractor Predicting: 213it [02:12,  1.63it/s]Extractor Predicting: 214it [02:13,  1.62it/s]Extractor Predicting: 215it [02:13,  1.62it/s]Extractor Predicting: 216it [02:14,  1.62it/s]Extractor Predicting: 217it [02:15,  1.60it/s]Extractor Predicting: 218it [02:15,  1.62it/s]Extractor Predicting: 219it [02:16,  1.58it/s]Extractor Predicting: 220it [02:17,  1.61it/s]Extractor Predicting: 221it [02:17,  1.64it/s]Extractor Predicting: 222it [02:18,  1.63it/s]Extractor Predicting: 223it [02:18,  1.59it/s]Extractor Predicting: 224it [02:19,  1.64it/s]Extractor Predicting: 225it [02:20,  1.62it/s]Extractor Predicting: 226it [02:20,  1.60it/s]Extractor Predicting: 227it [02:21,  1.59it/s]Extractor Predicting: 228it [02:21,  1.62it/s]Extractor Predicting: 229it [02:22,  1.61it/s]Extractor Predicting: 230it [02:23,  1.64it/s]Extractor Predicting: 231it [02:23,  1.64it/s]Extractor Predicting: 232it [02:24,  1.62it/s]Extractor Predicting: 233it [02:25,  1.46it/s]Extractor Predicting: 234it [02:25,  1.49it/s]Extractor Predicting: 235it [02:26,  1.50it/s]Extractor Predicting: 236it [02:27,  1.51it/s]Extractor Predicting: 237it [02:27,  1.52it/s]Extractor Predicting: 238it [02:28,  1.53it/s]Extractor Predicting: 239it [02:29,  1.53it/s]Extractor Predicting: 240it [02:29,  1.55it/s]Extractor Predicting: 241it [02:30,  1.55it/s]Extractor Predicting: 242it [02:31,  1.54it/s]Extractor Predicting: 243it [02:31,  1.56it/s]Extractor Predicting: 244it [02:32,  1.54it/s]Extractor Predicting: 245it [02:33,  1.56it/s]Extractor Predicting: 246it [02:33,  1.57it/s]Extractor Predicting: 247it [02:34,  1.57it/s]Extractor Predicting: 248it [02:34,  1.57it/s]Extractor Predicting: 249it [02:35,  1.60it/s]Extractor Predicting: 250it [02:36,  1.59it/s]Extractor Predicting: 251it [02:36,  1.63it/s]Extractor Predicting: 252it [02:37,  1.59it/s]Extractor Predicting: 253it [02:38,  1.60it/s]Extractor Predicting: 254it [02:38,  1.60it/s]Extractor Predicting: 255it [02:39,  1.59it/s]Extractor Predicting: 256it [02:39,  1.55it/s]Extractor Predicting: 257it [02:40,  1.57it/s]Extractor Predicting: 258it [02:41,  1.56it/s]Extractor Predicting: 259it [02:41,  1.62it/s]Extractor Predicting: 260it [02:42,  1.59it/s]Extractor Predicting: 261it [02:43,  1.62it/s]Extractor Predicting: 262it [02:43,  1.60it/s]Extractor Predicting: 263it [02:44,  1.59it/s]Extractor Predicting: 264it [02:44,  1.65it/s]Extractor Predicting: 265it [02:45,  1.64it/s]Extractor Predicting: 266it [02:46,  1.66it/s]Extractor Predicting: 267it [02:46,  1.63it/s]Extractor Predicting: 268it [02:47,  1.62it/s]Extractor Predicting: 269it [02:47,  1.61it/s]Extractor Predicting: 270it [02:48,  1.58it/s]Extractor Predicting: 271it [02:49,  1.57it/s]Extractor Predicting: 272it [02:49,  1.58it/s]Extractor Predicting: 273it [02:50,  1.57it/s]Extractor Predicting: 274it [02:51,  1.55it/s]Extractor Predicting: 275it [02:51,  1.53it/s]Extractor Predicting: 276it [02:52,  1.55it/s]Extractor Predicting: 277it [02:53,  1.55it/s]Extractor Predicting: 278it [02:53,  1.59it/s]Extractor Predicting: 279it [02:54,  1.58it/s]Extractor Predicting: 280it [02:55,  1.58it/s]Extractor Predicting: 281it [02:55,  1.59it/s]Extractor Predicting: 282it [02:56,  1.59it/s]Extractor Predicting: 283it [02:56,  1.57it/s]Extractor Predicting: 284it [02:57,  1.57it/s]Extractor Predicting: 285it [02:58,  1.57it/s]Extractor Predicting: 286it [02:58,  1.61it/s]Extractor Predicting: 287it [02:59,  1.57it/s]Extractor Predicting: 288it [02:59,  1.79it/s]Extractor Predicting: 288it [02:59,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:01,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:01,966 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:01,967 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:01,967 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:01,967 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:09:02,681 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:09:02,682 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:09:03,750 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:09:04,884 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:09:04,884 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:12,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:12,585 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:12,585 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:12,585 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:09:12,585 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:09:15,104 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:09:15,105 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:09:16,287 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:09:16,460 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:09:16,460 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5305851063829787,
  "recall": 0.11565217391304349,
  "score": 0.18990956687291766,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  2.40it/s]Extractor Predicting: 3it [00:01,  2.08it/s]
[INFO|configuration_utils.py:515] 2023-08-28 22:09:18,482 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:09:18,483 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:09:18,490 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:09:18,491 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 22:09:18,494 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:09:25,260 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 22:09:25,267 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 22:09:25,326 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:09:25,326 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:09:25,340 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:09:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:09:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:09:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:09:25,353 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:09:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:09:25,353 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.25,
  "recall": 0.01,
  "score": 0.019230769230769232,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 22:09:25,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:26,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:26,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:27,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:28,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:28,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:29,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:30,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:30,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:31,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:31,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:32,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:33,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:33,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:34,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:35,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:35,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:36,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:36,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:37,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<02:55, 12.56s/it][WARNING|generation_utils.py:914] 2023-08-28 22:09:38,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:38,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:39,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:39,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:40,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:40,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:41,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:42,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:42,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:43,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:43,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:44,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:45,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:45,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:46,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:46,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:47,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:48,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:48,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:49,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:24<02:37, 12.15s/it][WARNING|generation_utils.py:914] 2023-08-28 22:09:50,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:50,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:51,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:51,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:52,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:52,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:53,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:54,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:54,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:55,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:56,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:56,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:57,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:57,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:58,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:59,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:09:59,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:00,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:00,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:01,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:02,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:02,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:37<02:31, 12.65s/it][WARNING|generation_utils.py:914] 2023-08-28 22:10:03,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:03,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:04,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:04,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:05,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:06,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:06,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:07,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:07,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:08,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:09,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:09,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:10,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:10,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:11,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:12,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:12,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:13,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:13,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:14,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:15,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:15,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:50<02:20, 12.79s/it][WARNING|generation_utils.py:914] 2023-08-28 22:10:16,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:16,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:17,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:18,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:18,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:19,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:20,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:20,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:21,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:21,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:22,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:23,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:24,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:24,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:25,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:25,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:26,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:27,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:27,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:28,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:28,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:03<02:09, 12.92s/it][WARNING|generation_utils.py:914] 2023-08-28 22:10:29,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:30,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:30,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:31,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:31,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:32,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:33,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:33,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:34,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:34,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:35,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:35,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:36,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:37,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:37,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:38,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:38,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:39,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:40,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:40,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:15<01:52, 12.51s/it][WARNING|generation_utils.py:914] 2023-08-28 22:10:41,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:41,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:42,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:43,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:43,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:44,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:45,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:46,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:46,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:47,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:48,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:49,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:50,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:50,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:51,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:52,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:52,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:53,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:54,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:54,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:30<01:45, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-28 22:10:55,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:56,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:56,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:57,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:58,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:58,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:10:59,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:00,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:00,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:01,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:01,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:02,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:02,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:03,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:04,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:04,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:05,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:05,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:06,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:07,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:07,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:42<01:31, 13.01s/it][WARNING|generation_utils.py:914] 2023-08-28 22:11:08,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:08,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:09,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:10,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:10,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:11,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:12,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:12,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:13,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:14,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:14,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:15,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:16,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:16,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:17,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:18,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:19,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:20,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:21,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:21,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:22,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:57<01:21, 13.51s/it][WARNING|generation_utils.py:914] 2023-08-28 22:11:22,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:23,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:24,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:24,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:25,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:25,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:26,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:27,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:27,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:28,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:28,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:29,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:30,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:30,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:31,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:31,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:32,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:33,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:33,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:34,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:34,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:09<01:06, 13.24s/it][WARNING|generation_utils.py:914] 2023-08-28 22:11:35,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:36,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:36,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:37,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:37,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:38,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:38,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:39,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:40,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:40,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:41,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:41,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:42,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:42,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:43,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:44,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:44,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:45,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:45,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:46,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:47,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:22<00:51, 12.89s/it][WARNING|generation_utils.py:914] 2023-08-28 22:11:47,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:48,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:48,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:49,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:49,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:50,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:51,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:51,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:52,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:52,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:53,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:53,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:54,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:54,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:55,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:56,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:56,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:57,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:57,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:58,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:11:58,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:33<00:37, 12.55s/it][WARNING|generation_utils.py:914] 2023-08-28 22:11:59,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:00,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:00,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:01,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:02,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:02,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:03,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:03,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:04,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:05,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:05,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:06,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:06,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:07,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:08,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:08,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:09,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:09,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:10,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:10,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:45<00:24, 12.39s/it][WARNING|generation_utils.py:914] 2023-08-28 22:12:11,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:12,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:13,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:13,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:14,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:14,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:15,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:16,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:16,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:17,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:17,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:18,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:19,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:19,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:20,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:20,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:21,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:22,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:22,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:23,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:23,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:24,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:59<00:12, 12.68s/it][WARNING|generation_utils.py:914] 2023-08-28 22:12:24,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:25,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:26,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:26,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:27,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:27,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:28,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:29,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:29,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:30,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:30,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:31,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:32,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:32,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:33,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:33,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:34,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:35,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:35,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:12:36,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:11<00:00, 12.54s/it]Generating: 100%|██████████| 15/15 [03:11<00:00, 12.76s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:43,538 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:43,581 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:43,581 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:43,581 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:43,581 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:12:44,865 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:12:44,867 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:12:45,701 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:12:46,844 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:12:46,860 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:51,890 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:51,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:51,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:51,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:12:51,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:12:52,838 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:12:52,839 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:12:53,886 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:12:54,101 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:12:54,101 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 592, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.975, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : field of work .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location of formation . Context : Later in the year , the battle at the Battle of Alpeth struck the capital of the Ottoman Empire , Amman . Head Entity : Battle of Amman , Tail Entity : Ottoman Empire .\n']
['Relation : location of formation . Context : Later in the year , the battle at the Battle of Alpeth struck the capital of the Ottoman Empire , Amman . Head Entity : Battle of Amman , Tail Entity : Ottoman Empire .\n', 'Relation : location of formation . Context : After the battle of Mount Katmandu , Emperor Sigismund II ordered his troops to enter the Valley of the Dead and enter the North as a separate force . Head Entity : Battle of Mount Katmandu , Tail Entity : Emperor Sigismund II .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9270833333333334, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 498, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 562, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9765625, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.94375, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 8410
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8510, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.23it/s]Extractor Estimating: 2it [00:01,  1.33it/s]Extractor Estimating: 3it [00:02,  1.40it/s]Extractor Estimating: 4it [00:02,  1.38it/s]Extractor Estimating: 5it [00:03,  1.44it/s]Extractor Estimating: 6it [00:04,  1.45it/s]Extractor Estimating: 7it [00:04,  1.49it/s]Extractor Estimating: 8it [00:05,  1.37it/s]Extractor Estimating: 9it [00:06,  1.40it/s]Extractor Estimating: 10it [00:07,  1.46it/s]Extractor Estimating: 11it [00:07,  1.39it/s]Extractor Estimating: 12it [00:08,  1.42it/s]Extractor Estimating: 13it [00:09,  1.45it/s]Extractor Estimating: 14it [00:09,  1.47it/s]Extractor Estimating: 15it [00:10,  1.54it/s]Extractor Estimating: 16it [00:11,  1.48it/s]Extractor Estimating: 17it [00:11,  1.49it/s]Extractor Estimating: 18it [00:12,  1.49it/s]Extractor Estimating: 19it [00:13,  1.51it/s]Extractor Estimating: 20it [00:13,  1.52it/s]Extractor Estimating: 21it [00:14,  1.55it/s]Extractor Estimating: 22it [00:15,  1.51it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:16,  1.47it/s]Extractor Estimating: 25it [00:17,  1.48it/s]Extractor Estimating: 26it [00:17,  1.46it/s]Extractor Estimating: 27it [00:18,  1.50it/s]Extractor Estimating: 28it [00:19,  1.55it/s]Extractor Estimating: 29it [00:19,  1.57it/s]Extractor Estimating: 30it [00:20,  1.58it/s]Extractor Estimating: 31it [00:20,  1.56it/s]Extractor Estimating: 32it [00:21,  1.51it/s]Extractor Estimating: 33it [00:22,  1.55it/s]Extractor Estimating: 34it [00:22,  1.56it/s]Extractor Estimating: 35it [00:23,  1.54it/s]Extractor Estimating: 36it [00:24,  1.53it/s]Extractor Estimating: 37it [00:25,  1.42it/s]Extractor Estimating: 38it [00:25,  1.47it/s]Extractor Estimating: 39it [00:26,  1.47it/s]Extractor Estimating: 40it [00:27,  1.48it/s]Extractor Estimating: 41it [00:27,  1.50it/s]Extractor Estimating: 42it [00:28,  1.55it/s]Extractor Estimating: 43it [00:28,  1.52it/s]Extractor Estimating: 44it [00:29,  1.54it/s]Extractor Estimating: 45it [00:30,  1.51it/s]Extractor Estimating: 46it [00:30,  1.49it/s]Extractor Estimating: 47it [00:31,  1.49it/s]Extractor Estimating: 48it [00:32,  1.53it/s]Extractor Estimating: 49it [00:32,  1.57it/s]Extractor Estimating: 50it [00:33,  1.56it/s]Extractor Estimating: 51it [00:34,  1.51it/s]Extractor Estimating: 52it [00:34,  1.54it/s]Extractor Estimating: 53it [00:35,  1.56it/s]Extractor Estimating: 54it [00:36,  1.58it/s]Extractor Estimating: 55it [00:36,  1.60it/s]Extractor Estimating: 56it [00:37,  1.63it/s]Extractor Estimating: 57it [00:37,  1.62it/s]Extractor Estimating: 58it [00:38,  1.68it/s]Extractor Estimating: 59it [00:39,  1.64it/s]Extractor Estimating: 60it [00:39,  1.63it/s]Extractor Estimating: 61it [00:40,  1.59it/s]Extractor Estimating: 62it [00:40,  1.64it/s]Extractor Estimating: 63it [00:41,  1.63it/s]Extractor Estimating: 64it [00:42,  1.62it/s]Extractor Estimating: 65it [00:42,  1.61it/s]Extractor Estimating: 66it [00:43,  1.65it/s]Extractor Estimating: 67it [00:43,  1.66it/s]Extractor Estimating: 68it [00:44,  1.71it/s]Extractor Estimating: 69it [00:45,  1.76it/s]Extractor Estimating: 70it [00:45,  1.68it/s]Extractor Estimating: 71it [00:46,  1.66it/s]Extractor Estimating: 72it [00:46,  1.63it/s]Extractor Estimating: 73it [00:47,  1.62it/s]Extractor Estimating: 74it [00:48,  1.63it/s]Extractor Estimating: 75it [00:48,  1.72it/s]Extractor Estimating: 76it [00:49,  1.73it/s]Extractor Estimating: 77it [00:49,  1.75it/s]Extractor Estimating: 78it [00:50,  1.78it/s]Extractor Estimating: 79it [00:50,  1.84it/s]Extractor Estimating: 80it [00:51,  1.80it/s]Extractor Estimating: 81it [00:52,  1.78it/s]Extractor Estimating: 82it [00:52,  1.84it/s]Extractor Estimating: 83it [00:53,  1.86it/s]Extractor Estimating: 84it [00:53,  1.80it/s]Extractor Estimating: 85it [00:54,  1.82it/s]Extractor Estimating: 86it [00:54,  1.76it/s]Extractor Estimating: 87it [00:55,  1.79it/s]Extractor Estimating: 88it [00:55,  1.82it/s]Extractor Estimating: 89it [00:56,  1.80it/s]Extractor Estimating: 90it [00:57,  1.77it/s]Extractor Estimating: 91it [00:57,  1.77it/s]Extractor Estimating: 92it [00:58,  1.75it/s]Extractor Estimating: 93it [00:58,  1.75it/s]Extractor Estimating: 94it [00:59,  1.74it/s]Extractor Estimating: 95it [00:59,  1.76it/s]Extractor Estimating: 96it [01:00,  1.80it/s]Extractor Estimating: 97it [01:00,  1.80it/s]Extractor Estimating: 98it [01:01,  1.82it/s]Extractor Estimating: 99it [01:02,  1.82it/s]Extractor Estimating: 100it [01:02,  1.81it/s]Extractor Estimating: 101it [01:03,  1.76it/s]Extractor Estimating: 102it [01:03,  1.64it/s]Extractor Estimating: 103it [01:04,  1.67it/s]Extractor Estimating: 104it [01:05,  1.65it/s]Extractor Estimating: 105it [01:05,  1.66it/s]Extractor Estimating: 106it [01:06,  1.64it/s]Extractor Estimating: 107it [01:07,  1.57it/s]Extractor Estimating: 108it [01:07,  1.66it/s]Extractor Estimating: 109it [01:08,  1.61it/s]Extractor Estimating: 110it [01:08,  1.64it/s]Extractor Estimating: 111it [01:09,  1.61it/s]Extractor Estimating: 112it [01:10,  1.60it/s]Extractor Estimating: 113it [01:10,  1.59it/s]Extractor Estimating: 114it [01:11,  1.56it/s]Extractor Estimating: 115it [01:12,  1.40it/s]Extractor Estimating: 116it [01:12,  1.43it/s]Extractor Estimating: 117it [01:13,  1.52it/s]Extractor Estimating: 118it [01:14,  1.54it/s]Extractor Estimating: 119it [01:14,  1.55it/s]Extractor Estimating: 120it [01:15,  1.56it/s]Extractor Estimating: 121it [01:16,  1.52it/s]Extractor Estimating: 122it [01:16,  1.59it/s]Extractor Estimating: 123it [01:17,  1.67it/s]Extractor Estimating: 124it [01:17,  1.63it/s]Extractor Estimating: 125it [01:18,  1.61it/s]Extractor Estimating: 126it [01:19,  1.59it/s]Extractor Estimating: 127it [01:19,  1.55it/s]Extractor Estimating: 128it [01:20,  1.56it/s]Extractor Estimating: 129it [01:20,  1.65it/s]Extractor Estimating: 130it [01:21,  1.60it/s]Extractor Estimating: 131it [01:22,  1.64it/s]Extractor Estimating: 132it [01:22,  1.66it/s]Extractor Estimating: 133it [01:23,  1.63it/s]Extractor Estimating: 134it [01:24,  1.64it/s]Extractor Estimating: 135it [01:24,  1.59it/s]Extractor Estimating: 136it [01:25,  1.56it/s]Extractor Estimating: 137it [01:25,  1.61it/s]Extractor Estimating: 138it [01:26,  1.65it/s]Extractor Estimating: 139it [01:27,  1.67it/s]Extractor Estimating: 140it [01:27,  1.70it/s]Extractor Estimating: 141it [01:28,  1.67it/s]Extractor Estimating: 142it [01:28,  1.68it/s]Extractor Estimating: 143it [01:29,  1.65it/s]Extractor Estimating: 144it [01:30,  1.67it/s]Extractor Estimating: 145it [01:30,  1.62it/s]Extractor Estimating: 146it [01:31,  1.70it/s]Extractor Estimating: 147it [01:31,  1.66it/s]Extractor Estimating: 148it [01:32,  1.64it/s]Extractor Estimating: 149it [01:33,  1.65it/s]Extractor Estimating: 150it [01:33,  1.68it/s]Extractor Estimating: 151it [01:34,  1.64it/s]Extractor Estimating: 152it [01:34,  1.64it/s]Extractor Estimating: 153it [01:35,  1.68it/s]Extractor Estimating: 154it [01:36,  1.64it/s]Extractor Estimating: 155it [01:36,  1.61it/s]Extractor Estimating: 156it [01:37,  1.55it/s]Extractor Estimating: 157it [01:38,  1.62it/s]Extractor Estimating: 158it [01:38,  1.54it/s]Extractor Estimating: 159it [01:39,  1.57it/s]Extractor Estimating: 160it [01:40,  1.51it/s]Extractor Estimating: 161it [01:40,  1.50it/s]Extractor Estimating: 162it [01:41,  1.52it/s]Extractor Estimating: 163it [01:42,  1.35it/s]Extractor Estimating: 164it [01:43,  1.37it/s]Extractor Estimating: 165it [01:43,  1.39it/s]Extractor Estimating: 166it [01:44,  1.44it/s]Extractor Estimating: 167it [01:44,  1.49it/s]Extractor Estimating: 168it [01:45,  1.50it/s]Extractor Estimating: 169it [01:46,  1.37it/s]Extractor Estimating: 170it [01:47,  1.43it/s]Extractor Estimating: 171it [01:47,  1.49it/s]Extractor Estimating: 172it [01:48,  1.51it/s]Extractor Estimating: 173it [01:49,  1.44it/s]Extractor Estimating: 174it [01:49,  1.49it/s]Extractor Estimating: 175it [01:50,  1.52it/s]Extractor Estimating: 176it [01:50,  1.61it/s]Extractor Estimating: 177it [01:51,  1.68it/s]Extractor Estimating: 178it [01:52,  1.68it/s]Extractor Estimating: 179it [01:52,  1.75it/s]Extractor Estimating: 180it [01:53,  1.72it/s]Extractor Estimating: 181it [01:53,  1.59it/s]Extractor Estimating: 182it [01:54,  1.59it/s]Extractor Estimating: 183it [01:55,  1.63it/s]Extractor Estimating: 184it [01:55,  1.60it/s]Extractor Estimating: 185it [01:56,  1.69it/s]Extractor Estimating: 186it [01:56,  1.72it/s]Extractor Estimating: 187it [01:57,  1.77it/s]Extractor Estimating: 188it [01:58,  1.71it/s]Extractor Estimating: 189it [01:58,  1.67it/s]Extractor Estimating: 190it [01:59,  1.72it/s]Extractor Estimating: 191it [01:59,  1.76it/s]Extractor Estimating: 192it [02:00,  1.59it/s]Extractor Estimating: 193it [02:01,  1.63it/s]Extractor Estimating: 194it [02:01,  1.65it/s]Extractor Estimating: 195it [02:02,  1.66it/s]Extractor Estimating: 196it [02:02,  1.65it/s]Extractor Estimating: 197it [02:03,  1.66it/s]Extractor Estimating: 198it [02:04,  1.62it/s]Extractor Estimating: 199it [02:04,  1.65it/s]Extractor Estimating: 200it [02:05,  1.68it/s]Extractor Estimating: 201it [02:05,  1.65it/s]Extractor Estimating: 202it [02:06,  1.62it/s]Extractor Estimating: 203it [02:07,  1.58it/s]Extractor Estimating: 204it [02:07,  1.62it/s]Extractor Estimating: 205it [02:08,  1.66it/s]Extractor Estimating: 206it [02:09,  1.59it/s]Extractor Estimating: 207it [02:09,  1.62it/s]Extractor Estimating: 208it [02:10,  1.66it/s]Extractor Estimating: 209it [02:10,  1.63it/s]Extractor Estimating: 210it [02:11,  1.62it/s]Extractor Estimating: 211it [02:12,  1.62it/s]Extractor Estimating: 212it [02:12,  1.62it/s]Extractor Estimating: 213it [02:13,  1.63it/s]Extractor Estimating: 214it [02:13,  1.60it/s]Extractor Estimating: 215it [02:14,  1.63it/s]Extractor Estimating: 216it [02:15,  1.64it/s]Extractor Estimating: 217it [02:15,  1.62it/s]Extractor Estimating: 218it [02:16,  1.54it/s]Extractor Estimating: 219it [02:17,  1.58it/s]Extractor Estimating: 220it [02:17,  1.57it/s]Extractor Estimating: 221it [02:18,  1.56it/s]Extractor Estimating: 222it [02:19,  1.57it/s]Extractor Estimating: 223it [02:19,  1.60it/s]Extractor Estimating: 224it [02:20,  1.64it/s]Extractor Estimating: 225it [02:20,  1.66it/s]Extractor Estimating: 226it [02:21,  1.71it/s]Extractor Estimating: 227it [02:21,  1.78it/s]Extractor Estimating: 228it [02:22,  1.77it/s]Extractor Estimating: 229it [02:22,  1.76it/s]Extractor Estimating: 230it [02:23,  1.76it/s]Extractor Estimating: 231it [02:24,  1.84it/s]Extractor Estimating: 232it [02:24,  1.88it/s]Extractor Estimating: 233it [02:25,  1.89it/s]Extractor Estimating: 234it [02:25,  1.89it/s]Extractor Estimating: 235it [02:26,  1.88it/s]Extractor Estimating: 236it [02:26,  1.85it/s]Extractor Estimating: 237it [02:27,  1.82it/s]Extractor Estimating: 238it [02:27,  1.86it/s]Extractor Estimating: 239it [02:28,  1.83it/s]Extractor Estimating: 240it [02:28,  1.81it/s]Extractor Estimating: 241it [02:29,  1.85it/s]Extractor Estimating: 242it [02:29,  1.91it/s]Extractor Estimating: 243it [02:30,  1.93it/s]Extractor Estimating: 244it [02:30,  1.88it/s]Extractor Estimating: 245it [02:31,  1.85it/s]Extractor Estimating: 246it [02:32,  1.86it/s]Extractor Estimating: 247it [02:32,  1.98it/s]Extractor Estimating: 248it [02:33,  1.99it/s]Extractor Estimating: 249it [02:33,  1.98it/s]Extractor Estimating: 250it [02:34,  1.92it/s]Extractor Estimating: 251it [02:34,  1.87it/s]Extractor Estimating: 252it [02:35,  1.80it/s]Extractor Estimating: 253it [02:35,  1.79it/s]Extractor Estimating: 254it [02:36,  1.85it/s]Extractor Estimating: 255it [02:36,  1.86it/s]Extractor Estimating: 256it [02:37,  1.85it/s]Extractor Estimating: 257it [02:37,  1.79it/s]Extractor Estimating: 258it [02:38,  1.75it/s]Extractor Estimating: 259it [02:39,  1.74it/s]Extractor Estimating: 260it [02:39,  1.82it/s]Extractor Estimating: 261it [02:40,  1.77it/s]Extractor Estimating: 262it [02:40,  1.83it/s]Extractor Estimating: 263it [02:41,  1.83it/s]Extractor Estimating: 264it [02:41,  1.82it/s]Extractor Estimating: 265it [02:42,  1.87it/s]Extractor Estimating: 266it [02:42,  1.91it/s]Extractor Estimating: 267it [02:43,  1.91it/s]Extractor Estimating: 268it [02:43,  1.87it/s]Extractor Estimating: 269it [02:44,  1.89it/s]Extractor Estimating: 270it [02:45,  1.85it/s]Extractor Estimating: 271it [02:45,  1.62it/s]Extractor Estimating: 272it [02:46,  1.69it/s]Extractor Estimating: 273it [02:46,  1.70it/s]Extractor Estimating: 274it [02:47,  1.76it/s]Extractor Estimating: 275it [02:48,  1.77it/s]Extractor Estimating: 276it [02:48,  1.84it/s]Extractor Estimating: 277it [02:49,  1.79it/s]Extractor Estimating: 278it [02:49,  1.82it/s]Extractor Estimating: 279it [02:50,  1.81it/s]Extractor Estimating: 280it [02:50,  1.77it/s]Extractor Estimating: 281it [02:51,  1.77it/s]Extractor Estimating: 282it [02:51,  1.83it/s]Extractor Estimating: 283it [02:52,  1.88it/s]Extractor Estimating: 284it [02:52,  1.89it/s]Extractor Estimating: 285it [02:53,  1.91it/s]Extractor Estimating: 286it [02:53,  1.90it/s]Extractor Estimating: 287it [02:54,  1.88it/s]Extractor Estimating: 288it [02:55,  1.82it/s]Extractor Estimating: 289it [02:55,  1.89it/s]Extractor Estimating: 290it [02:56,  1.80it/s]Extractor Estimating: 291it [02:56,  1.85it/s]Extractor Estimating: 292it [02:57,  1.87it/s]Extractor Estimating: 293it [02:57,  1.82it/s]Extractor Estimating: 294it [02:58,  1.78it/s]Extractor Estimating: 295it [02:58,  1.81it/s]Extractor Estimating: 296it [02:59,  1.84it/s]Extractor Estimating: 297it [02:59,  1.82it/s]Extractor Estimating: 298it [03:00,  1.80it/s]Extractor Estimating: 299it [03:01,  1.82it/s]Extractor Estimating: 300it [03:01,  1.77it/s]Extractor Estimating: 301it [03:02,  1.68it/s]Extractor Estimating: 302it [03:02,  1.68it/s]Extractor Estimating: 303it [03:03,  1.64it/s]Extractor Estimating: 304it [03:04,  1.60it/s]Extractor Estimating: 305it [03:04,  1.58it/s]Extractor Estimating: 306it [03:05,  1.59it/s]Extractor Estimating: 307it [03:06,  1.62it/s]Extractor Estimating: 308it [03:06,  1.63it/s]Extractor Estimating: 309it [03:07,  1.60it/s]Extractor Estimating: 310it [03:07,  1.62it/s]Extractor Estimating: 311it [03:08,  1.64it/s]Extractor Estimating: 312it [03:09,  1.62it/s]Extractor Estimating: 313it [03:09,  1.64it/s]Extractor Estimating: 314it [03:10,  1.57it/s]Extractor Estimating: 315it [03:11,  1.54it/s]Extractor Estimating: 316it [03:11,  1.57it/s]Extractor Estimating: 317it [03:12,  1.57it/s]Extractor Estimating: 318it [03:13,  1.57it/s]Extractor Estimating: 319it [03:13,  1.61it/s]Extractor Estimating: 320it [03:14,  1.55it/s]Extractor Estimating: 321it [03:14,  1.60it/s]Extractor Estimating: 322it [03:15,  1.67it/s]Extractor Estimating: 323it [03:16,  1.63it/s]Extractor Estimating: 324it [03:16,  1.63it/s]Extractor Estimating: 325it [03:17,  1.65it/s]Extractor Estimating: 326it [03:17,  1.67it/s]Extractor Estimating: 327it [03:18,  1.67it/s]Extractor Estimating: 328it [03:19,  1.70it/s]Extractor Estimating: 329it [03:19,  1.71it/s]Extractor Estimating: 330it [03:20,  1.76it/s]Extractor Estimating: 331it [03:20,  1.73it/s]Extractor Estimating: 332it [03:21,  1.73it/s]Extractor Estimating: 333it [03:21,  1.78it/s]Extractor Estimating: 334it [03:22,  1.68it/s]Extractor Estimating: 335it [03:23,  1.69it/s]Extractor Estimating: 336it [03:23,  1.65it/s]Extractor Estimating: 337it [03:24,  1.69it/s]Extractor Estimating: 338it [03:24,  1.66it/s]Extractor Estimating: 339it [03:25,  1.60it/s]Extractor Estimating: 340it [03:26,  1.64it/s]Extractor Estimating: 341it [03:26,  1.66it/s]Extractor Estimating: 342it [03:27,  1.68it/s]Extractor Estimating: 343it [03:27,  1.74it/s]Extractor Estimating: 344it [03:28,  1.80it/s]Extractor Estimating: 345it [03:28,  1.77it/s]Extractor Estimating: 346it [03:29,  1.77it/s]Extractor Estimating: 347it [03:30,  1.76it/s]Extractor Estimating: 348it [03:30,  1.77it/s]Extractor Estimating: 349it [03:31,  1.80it/s]Extractor Estimating: 350it [03:31,  1.77it/s]Extractor Estimating: 351it [03:32,  1.50it/s]Extractor Estimating: 352it [03:33,  1.55it/s]Extractor Estimating: 353it [03:33,  1.53it/s]Extractor Estimating: 354it [03:34,  1.56it/s]Extractor Estimating: 355it [03:35,  1.50it/s]Extractor Estimating: 356it [03:35,  1.51it/s]Extractor Estimating: 357it [03:36,  1.55it/s]Extractor Estimating: 358it [03:37,  1.55it/s]Extractor Estimating: 359it [03:37,  1.55it/s]Extractor Estimating: 360it [03:38,  1.58it/s]Extractor Estimating: 361it [03:39,  1.60it/s]Extractor Estimating: 362it [03:39,  1.57it/s]Extractor Estimating: 363it [03:40,  1.49it/s]Extractor Estimating: 364it [03:41,  1.50it/s]Extractor Estimating: 365it [03:41,  1.51it/s]Extractor Estimating: 366it [03:42,  1.50it/s]Extractor Estimating: 367it [03:43,  1.51it/s]Extractor Estimating: 368it [03:43,  1.55it/s]Extractor Estimating: 369it [03:44,  1.56it/s]Extractor Estimating: 370it [03:44,  1.58it/s]Extractor Estimating: 371it [03:45,  1.52it/s]Extractor Estimating: 372it [03:46,  1.52it/s]Extractor Estimating: 373it [03:46,  1.53it/s]Extractor Estimating: 374it [03:47,  1.59it/s]Extractor Estimating: 375it [03:48,  1.63it/s]Extractor Estimating: 375it [03:48,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:16:57,734 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:16:57,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:16:57,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:16:57,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:16:57,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:16:59,304 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:16:59,305 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:17:00,695 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:17:01,821 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:17:01,825 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:17:06,662 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:17:06,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:17:06,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:17:06,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:17:06,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:17:07,968 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:17:07,969 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:17:08,651 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:17:08,831 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:17:08,831 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:30:10,100 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:30:10,131 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7495 mean pseudo reward: 0.9322542249745859
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
train vocab size: 16385
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16485, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16485, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.027, loss:894.2936
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.016, loss:818.0832
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.095, loss:786.7608
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.016, loss:724.6012
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.058, loss:763.4489
>> valid entity prec:0.5753, rec:0.5501, f1:0.5624
>> valid relation prec:0.4462, rec:0.0997, f1:0.1630
>> valid relation with NER prec:0.4462, rec:0.0997, f1:0.1630
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.234, loss:755.9551
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.049, loss:715.2444
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.042, loss:718.6635
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.037, loss:715.0369
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.029, loss:743.5701
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5328, rec:0.5731, f1:0.5522
>> valid relation prec:0.3411, rec:0.1172, f1:0.1744
>> valid relation with NER prec:0.3411, rec:0.1172, f1:0.1744
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.229, loss:685.1670
g_step 1200, step 261, avg_time 1.051, loss:735.4309
g_step 1300, step 48, avg_time 1.023, loss:717.8196
g_step 1400, step 148, avg_time 1.047, loss:663.1841
g_step 1500, step 248, avg_time 1.042, loss:666.1764
>> valid entity prec:0.5480, rec:0.5687, f1:0.5582
>> valid relation prec:0.3374, rec:0.1114, f1:0.1675
>> valid relation with NER prec:0.3374, rec:0.1114, f1:0.1675
g_step 1600, step 35, avg_time 2.226, loss:689.3827
g_step 1700, step 135, avg_time 1.041, loss:661.0174
g_step 1800, step 235, avg_time 1.024, loss:661.2001
g_step 1900, step 22, avg_time 1.053, loss:627.2112
g_step 2000, step 122, avg_time 1.045, loss:585.5182
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5690, rec:0.5370, f1:0.5525
>> valid relation prec:0.4377, rec:0.0965, f1:0.1582
>> valid relation with NER prec:0.4377, rec:0.0965, f1:0.1582
g_step 2100, step 222, avg_time 2.209, loss:639.7763
g_step 2200, step 9, avg_time 1.038, loss:628.1596
g_step 2300, step 109, avg_time 1.032, loss:563.5891
g_step 2400, step 209, avg_time 1.037, loss:579.0990
g_step 2500, step 309, avg_time 1.050, loss:634.9818
>> valid entity prec:0.5579, rec:0.5431, f1:0.5504
>> valid relation prec:0.3855, rec:0.1017, f1:0.1609
>> valid relation with NER prec:0.3855, rec:0.1017, f1:0.1609
g_step 2600, step 96, avg_time 2.231, loss:549.8313
g_step 2700, step 196, avg_time 1.040, loss:579.9245
g_step 2800, step 296, avg_time 1.042, loss:569.7648
g_step 2900, step 83, avg_time 1.038, loss:523.8347
g_step 3000, step 183, avg_time 1.051, loss:537.3611
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5680, rec:0.5239, f1:0.5451
>> valid relation prec:0.2871, rec:0.1014, f1:0.1499
>> valid relation with NER prec:0.2871, rec:0.1014, f1:0.1499
g_step 3100, step 283, avg_time 2.261, loss:559.3270
g_step 3200, step 70, avg_time 1.059, loss:495.1949
g_step 3300, step 170, avg_time 1.065, loss:513.1242
g_step 3400, step 270, avg_time 1.027, loss:516.7433
g_step 3500, step 57, avg_time 1.054, loss:490.6395
>> valid entity prec:0.5789, rec:0.5177, f1:0.5466
>> valid relation prec:0.3136, rec:0.1077, f1:0.1603
>> valid relation with NER prec:0.3136, rec:0.1077, f1:0.1603
g_step 3600, step 157, avg_time 2.235, loss:473.5398
g_step 3700, step 257, avg_time 1.037, loss:520.5571
g_step 3800, step 44, avg_time 1.029, loss:508.8710
g_step 3900, step 144, avg_time 1.056, loss:478.4133
g_step 4000, step 244, avg_time 1.043, loss:488.1157
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5993, rec:0.4818, f1:0.5342
>> valid relation prec:0.3412, rec:0.1074, f1:0.1634
>> valid relation with NER prec:0.3412, rec:0.1074, f1:0.1634
g_step 4100, step 31, avg_time 2.268, loss:477.7359
g_step 4200, step 131, avg_time 1.030, loss:449.4931
g_step 4300, step 231, avg_time 1.048, loss:475.9167
g_step 4400, step 18, avg_time 1.071, loss:462.6168
g_step 4500, step 118, avg_time 1.065, loss:415.9963
>> valid entity prec:0.5657, rec:0.4825, f1:0.5208
>> valid relation prec:0.2912, rec:0.1134, f1:0.1633
>> valid relation with NER prec:0.2912, rec:0.1134, f1:0.1633
g_step 4600, step 218, avg_time 2.235, loss:458.9116
g_step 4700, step 5, avg_time 1.044, loss:446.0977
g_step 4800, step 105, avg_time 1.050, loss:424.2981
g_step 4900, step 205, avg_time 1.045, loss:442.9245
g_step 5000, step 305, avg_time 1.062, loss:444.7556
learning rate was adjusted to 0.0008
>> valid entity prec:0.5320, rec:0.5247, f1:0.5283
>> valid relation prec:0.3453, rec:0.1160, f1:0.1737
>> valid relation with NER prec:0.3453, rec:0.1160, f1:0.1737
g_step 5100, step 92, avg_time 2.264, loss:398.3615
g_step 5200, step 192, avg_time 1.038, loss:411.3011
g_step 5300, step 292, avg_time 1.049, loss:422.5513
g_step 5400, step 79, avg_time 1.072, loss:386.2851
g_step 5500, step 179, avg_time 1.044, loss:397.8555
>> valid entity prec:0.5495, rec:0.5349, f1:0.5421
>> valid relation prec:0.3293, rec:0.1237, f1:0.1799
>> valid relation with NER prec:0.3293, rec:0.1237, f1:0.1799
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 279, avg_time 2.237, loss:407.7926
g_step 5700, step 66, avg_time 1.041, loss:375.6320
g_step 5800, step 166, avg_time 1.041, loss:373.2033
g_step 5900, step 266, avg_time 1.053, loss:392.6757
g_step 6000, step 53, avg_time 1.066, loss:391.1518
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5402, rec:0.4910, f1:0.5144
>> valid relation prec:0.2768, rec:0.1160, f1:0.1635
>> valid relation with NER prec:0.2768, rec:0.1160, f1:0.1635
g_step 6100, step 153, avg_time 2.270, loss:358.2678
g_step 6200, step 253, avg_time 1.039, loss:381.5795
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:30:10 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:30:10 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-30-10_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:30:11 - WARNING - datasets.builder -   Using custom data configuration default-e14253b7a2bf3e73
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e14253b7a2bf3e73/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:30:11,372 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:30:11,373 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:30:11,374 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:30:11,375 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:30:11,383 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:30:11,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:30:11,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:30:11,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:30:11,388 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:30:11,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:30:11,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:30:11,533 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:30:14,653 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:30:14,657 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e14253b7a2bf3e73/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.22ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.27ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.84ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.18ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.39ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.57ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.67ba/s]100%|██████████| 8/8 [00:01<00:00,  5.55ba/s]100%|██████████| 8/8 [00:01<00:00,  4.47ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.07ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.32ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42ba/s]100%|██████████| 4/4 [00:00<00:00,  5.50ba/s]100%|██████████| 4/4 [00:00<00:00,  4.99ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.52ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.32ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.30ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.41ba/s]100%|██████████| 8/8 [00:00<00:00, 10.98ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.71ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.45ba/s]100%|██████████| 4/4 [00:00<00:00, 11.81ba/s]
[INFO|trainer.py:414] 2023-08-29 00:30:18,690 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:30:18,709 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:30:18,709 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 00:30:18,709 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:30:18,709 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:30:18,709 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:30:18,709 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:30:18,709 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.33it/s]  0%|          | 2/585 [00:00<02:50,  3.41it/s]  1%|          | 3/585 [00:00<02:49,  3.44it/s]  1%|          | 4/585 [00:01<02:48,  3.45it/s]  1%|          | 5/585 [00:01<02:47,  3.46it/s]  1%|          | 6/585 [00:01<02:47,  3.46it/s]  1%|          | 7/585 [00:02<02:46,  3.46it/s]  1%|▏         | 8/585 [00:02<02:47,  3.45it/s]  2%|▏         | 9/585 [00:02<02:46,  3.45it/s]  2%|▏         | 10/585 [00:02<02:46,  3.46it/s]  2%|▏         | 11/585 [00:03<02:45,  3.46it/s]  2%|▏         | 12/585 [00:03<02:45,  3.46it/s]  2%|▏         | 13/585 [00:03<02:45,  3.47it/s]  2%|▏         | 14/585 [00:04<02:44,  3.47it/s]  3%|▎         | 15/585 [00:04<02:44,  3.47it/s]  3%|▎         | 16/585 [00:04<02:44,  3.47it/s]  3%|▎         | 17/585 [00:04<02:43,  3.46it/s]  3%|▎         | 18/585 [00:05<02:43,  3.47it/s]  3%|▎         | 19/585 [00:05<02:43,  3.46it/s]  3%|▎         | 20/585 [00:05<02:43,  3.46it/s]  4%|▎         | 21/585 [00:06<02:42,  3.46it/s]  4%|▍         | 22/585 [00:06<02:42,  3.46it/s]  4%|▍         | 23/585 [00:06<02:42,  3.46it/s]  4%|▍         | 24/585 [00:06<02:41,  3.46it/s]  4%|▍         | 25/585 [00:07<02:41,  3.46it/s]  4%|▍         | 26/585 [00:07<02:41,  3.46it/s]  5%|▍         | 27/585 [00:07<02:41,  3.46it/s]  5%|▍         | 28/585 [00:08<02:40,  3.46it/s]  5%|▍         | 29/585 [00:08<02:40,  3.46it/s]  5%|▌         | 30/585 [00:08<02:40,  3.46it/s]  5%|▌         | 31/585 [00:08<02:40,  3.46it/s]  5%|▌         | 32/585 [00:09<02:39,  3.46it/s]  6%|▌         | 33/585 [00:09<02:39,  3.46it/s]  6%|▌         | 34/585 [00:09<02:39,  3.46it/s]  6%|▌         | 35/585 [00:10<02:38,  3.46it/s]  6%|▌         | 36/585 [00:10<02:38,  3.47it/s]  6%|▋         | 37/585 [00:10<02:38,  3.46it/s]  6%|▋         | 38/585 [00:10<02:37,  3.46it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:37,  3.46it/s]  7%|▋         | 41/585 [00:11<02:37,  3.45it/s]  7%|▋         | 42/585 [00:12<02:37,  3.45it/s]  7%|▋         | 43/585 [00:12<02:36,  3.46it/s]  8%|▊         | 44/585 [00:12<02:36,  3.46it/s]  8%|▊         | 45/585 [00:13<02:36,  3.46it/s]  8%|▊         | 46/585 [00:13<02:35,  3.46it/s]  8%|▊         | 47/585 [00:13<02:35,  3.46it/s]  8%|▊         | 48/585 [00:13<02:35,  3.46it/s]  8%|▊         | 49/585 [00:14<02:34,  3.46it/s]  9%|▊         | 50/585 [00:14<02:34,  3.46it/s]  9%|▊         | 51/585 [00:14<02:34,  3.46it/s]  9%|▉         | 52/585 [00:15<02:34,  3.45it/s]  9%|▉         | 53/585 [00:15<02:34,  3.45it/s]  9%|▉         | 54/585 [00:15<02:33,  3.46it/s]  9%|▉         | 55/585 [00:15<02:33,  3.46it/s] 10%|▉         | 56/585 [00:16<02:32,  3.46it/s] 10%|▉         | 57/585 [00:16<02:32,  3.46it/s] 10%|▉         | 58/585 [00:16<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:31,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.46it/s] 11%|█         | 62/585 [00:17<02:31,  3.46it/s] 11%|█         | 63/585 [00:18<02:38,  3.29it/s] 11%|█         | 64/585 [00:18<02:35,  3.34it/s] 11%|█         | 65/585 [00:18<02:34,  3.38it/s] 11%|█▏        | 66/585 [00:19<02:32,  3.40it/s] 11%|█▏        | 67/585 [00:19<02:31,  3.42it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 69/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.45it/s] 12%|█▏        | 72/585 [00:20<02:28,  3.45it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.45it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 79/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.46it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:24<02:25,  3.46it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 86/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.46it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.45it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.45it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.45it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 104/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.44it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.45it/s] 18%|█▊        | 107/585 [00:30<02:18,  3.45it/s] 18%|█▊        | 108/585 [00:31<02:18,  3.45it/s] 19%|█▊        | 109/585 [00:31<02:17,  3.45it/s] 19%|█▉        | 110/585 [00:31<02:17,  3.46it/s] 19%|█▉        | 111/585 [00:32<02:17,  3.46it/s] 19%|█▉        | 112/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 113/585 [00:32<02:16,  3.46it/s] 19%|█▉        | 114/585 [00:33<02:16,  3.46it/s] 20%|█▉        | 115/585 [00:33<02:15,  3.46it/s] 20%|█▉        | 116/585 [00:33<02:15,  3.45it/s] 20%|██        | 117/585 [00:33<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:30:52,635 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:30:52,635 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:30:52,635 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.88it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.33it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.77it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.95it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.40it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.76it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.04it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.92it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.99it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.06it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.96it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.88it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.77it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.71it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.74it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.70it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.71it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.80it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 45.02it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.91it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.99it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.81it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.69it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.66it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.70it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.66it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.82it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.95it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 45.02it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 45.02it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.92it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.74it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.72it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.70it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.64it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.70it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.81it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.97it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 45.01it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.91it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.88it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.81it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.76it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.64it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.82it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.93it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.95it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.98it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.96it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.86it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.77it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.67it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.66it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.70it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.82it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.94it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 45.06it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.87it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.75it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.75it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.69it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.73it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.73it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.84it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.92it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.85it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.80it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.73it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.84it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.73it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.79it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.83it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.82it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.87it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.80it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.77it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.72it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.58it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.64it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.71it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.85it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.88it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.84it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.90it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.92it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.86it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.84it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:43<02:15,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 44.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:31:02,415 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 00:31:02,439 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:31:04,174 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:31:04,188 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:31:04,196 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:49<37:38,  4.84s/it] 20%|██        | 119/585 [00:49<26:59,  3.47s/it] 21%|██        | 120/585 [00:49<19:32,  2.52s/it] 21%|██        | 121/585 [00:50<14:19,  1.85s/it] 21%|██        | 122/585 [00:50<10:41,  1.38s/it] 21%|██        | 123/585 [00:50<08:08,  1.06s/it] 21%|██        | 124/585 [00:51<06:21,  1.21it/s] 21%|██▏       | 125/585 [00:51<05:07,  1.50it/s] 22%|██▏       | 126/585 [00:51<04:17,  1.78it/s] 22%|██▏       | 127/585 [00:51<03:40,  2.08it/s] 22%|██▏       | 128/585 [00:52<03:14,  2.35it/s] 22%|██▏       | 129/585 [00:52<02:55,  2.60it/s] 22%|██▏       | 130/585 [00:52<02:43,  2.79it/s] 22%|██▏       | 131/585 [00:53<02:33,  2.95it/s] 23%|██▎       | 132/585 [00:53<02:27,  3.07it/s] 23%|██▎       | 133/585 [00:53<02:23,  3.16it/s] 23%|██▎       | 134/585 [00:54<02:20,  3.21it/s] 23%|██▎       | 135/585 [00:54<02:17,  3.26it/s] 23%|██▎       | 136/585 [00:54<02:15,  3.30it/s] 23%|██▎       | 137/585 [00:54<02:14,  3.33it/s] 24%|██▎       | 138/585 [00:55<02:13,  3.35it/s] 24%|██▍       | 139/585 [00:55<02:12,  3.37it/s] 24%|██▍       | 140/585 [00:55<02:11,  3.38it/s] 24%|██▍       | 141/585 [00:56<02:11,  3.38it/s] 24%|██▍       | 142/585 [00:56<02:10,  3.39it/s] 24%|██▍       | 143/585 [00:56<02:10,  3.39it/s] 25%|██▍       | 144/585 [00:57<02:09,  3.40it/s] 25%|██▍       | 145/585 [00:57<02:09,  3.40it/s] 25%|██▍       | 146/585 [00:57<02:08,  3.40it/s] 25%|██▌       | 147/585 [00:57<02:08,  3.40it/s] 25%|██▌       | 148/585 [00:58<02:08,  3.41it/s] 25%|██▌       | 149/585 [00:58<02:07,  3.41it/s] 26%|██▌       | 150/585 [00:58<02:07,  3.41it/s] 26%|██▌       | 151/585 [00:59<02:07,  3.41it/s] 26%|██▌       | 152/585 [00:59<02:07,  3.40it/s] 26%|██▌       | 153/585 [00:59<02:06,  3.40it/s] 26%|██▋       | 154/585 [00:59<02:06,  3.41it/s] 26%|██▋       | 155/585 [01:00<02:06,  3.41it/s] 27%|██▋       | 156/585 [01:00<02:05,  3.41it/s] 27%|██▋       | 157/585 [01:00<02:05,  3.41it/s] 27%|██▋       | 158/585 [01:01<02:05,  3.41it/s] 27%|██▋       | 159/585 [01:01<02:04,  3.41it/s] 27%|██▋       | 160/585 [01:01<02:04,  3.41it/s] 28%|██▊       | 161/585 [01:01<02:04,  3.41it/s] 28%|██▊       | 162/585 [01:02<02:04,  3.41it/s] 28%|██▊       | 163/585 [01:02<02:03,  3.41it/s] 28%|██▊       | 164/585 [01:02<02:02,  3.43it/s] 28%|██▊       | 165/585 [01:03<02:02,  3.44it/s] 28%|██▊       | 166/585 [01:03<02:01,  3.44it/s] 29%|██▊       | 167/585 [01:03<02:01,  3.45it/s] 29%|██▊       | 168/585 [01:04<02:00,  3.45it/s] 29%|██▉       | 169/585 [01:04<02:00,  3.45it/s] 29%|██▉       | 170/585 [01:04<02:00,  3.45it/s] 29%|██▉       | 171/585 [01:04<01:59,  3.46it/s] 29%|██▉       | 172/585 [01:05<01:59,  3.46it/s] 30%|██▉       | 173/585 [01:05<01:59,  3.46it/s] 30%|██▉       | 174/585 [01:05<01:59,  3.45it/s] 30%|██▉       | 175/585 [01:06<01:58,  3.45it/s] 30%|███       | 176/585 [01:06<01:58,  3.45it/s] 30%|███       | 177/585 [01:06<01:58,  3.46it/s] 30%|███       | 178/585 [01:06<01:57,  3.46it/s] 31%|███       | 179/585 [01:07<01:57,  3.46it/s] 31%|███       | 180/585 [01:07<01:57,  3.46it/s] 31%|███       | 181/585 [01:07<01:56,  3.46it/s] 31%|███       | 182/585 [01:08<01:56,  3.46it/s] 31%|███▏      | 183/585 [01:08<01:56,  3.46it/s] 31%|███▏      | 184/585 [01:08<01:55,  3.46it/s] 32%|███▏      | 185/585 [01:08<01:56,  3.45it/s] 32%|███▏      | 186/585 [01:09<01:55,  3.45it/s] 32%|███▏      | 187/585 [01:09<01:55,  3.45it/s] 32%|███▏      | 188/585 [01:09<01:54,  3.46it/s] 32%|███▏      | 189/585 [01:10<01:54,  3.46it/s] 32%|███▏      | 190/585 [01:10<01:54,  3.46it/s] 33%|███▎      | 191/585 [01:10<01:53,  3.46it/s] 33%|███▎      | 192/585 [01:10<01:53,  3.46it/s] 33%|███▎      | 193/585 [01:11<01:53,  3.45it/s] 33%|███▎      | 194/585 [01:11<01:53,  3.46it/s] 33%|███▎      | 195/585 [01:11<01:52,  3.45it/s] 34%|███▎      | 196/585 [01:12<01:52,  3.45it/s] 34%|███▎      | 197/585 [01:12<01:52,  3.45it/s] 34%|███▍      | 198/585 [01:12<01:52,  3.45it/s] 34%|███▍      | 199/585 [01:12<01:51,  3.45it/s] 34%|███▍      | 200/585 [01:13<01:51,  3.46it/s] 34%|███▍      | 201/585 [01:13<01:51,  3.46it/s] 35%|███▍      | 202/585 [01:13<01:50,  3.46it/s] 35%|███▍      | 203/585 [01:14<01:50,  3.46it/s] 35%|███▍      | 204/585 [01:14<01:50,  3.45it/s] 35%|███▌      | 205/585 [01:14<01:49,  3.46it/s] 35%|███▌      | 206/585 [01:15<01:49,  3.46it/s] 35%|███▌      | 207/585 [01:15<01:49,  3.46it/s] 36%|███▌      | 208/585 [01:15<01:49,  3.46it/s] 36%|███▌      | 209/585 [01:15<01:48,  3.45it/s] 36%|███▌      | 210/585 [01:16<01:48,  3.45it/s] 36%|███▌      | 211/585 [01:16<01:48,  3.45it/s] 36%|███▌      | 212/585 [01:16<01:48,  3.45it/s] 36%|███▋      | 213/585 [01:17<01:47,  3.45it/s] 37%|███▋      | 214/585 [01:17<01:47,  3.45it/s] 37%|███▋      | 215/585 [01:17<01:47,  3.45it/s] 37%|███▋      | 216/585 [01:17<01:46,  3.45it/s] 37%|███▋      | 217/585 [01:18<01:47,  3.43it/s] 37%|███▋      | 218/585 [01:18<01:46,  3.44it/s] 37%|███▋      | 219/585 [01:18<01:46,  3.44it/s] 38%|███▊      | 220/585 [01:19<01:45,  3.45it/s] 38%|███▊      | 221/585 [01:19<01:45,  3.45it/s] 38%|███▊      | 222/585 [01:19<01:45,  3.45it/s] 38%|███▊      | 223/585 [01:19<01:44,  3.45it/s] 38%|███▊      | 224/585 [01:20<01:44,  3.45it/s] 38%|███▊      | 225/585 [01:20<01:44,  3.45it/s] 39%|███▊      | 226/585 [01:20<01:44,  3.45it/s] 39%|███▉      | 227/585 [01:21<01:43,  3.45it/s] 39%|███▉      | 228/585 [01:21<01:43,  3.45it/s] 39%|███▉      | 229/585 [01:21<01:43,  3.45it/s] 39%|███▉      | 230/585 [01:21<01:42,  3.45it/s] 39%|███▉      | 231/585 [01:22<01:42,  3.45it/s] 40%|███▉      | 232/585 [01:22<01:42,  3.45it/s] 40%|███▉      | 233/585 [01:22<01:41,  3.45it/s] 40%|████      | 234/585 [01:23<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:31:41,883 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:31:41,883 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:31:41,883 >>   Batch size = 8
{'eval_loss': 1.0843169689178467, 'eval_runtime': 9.7643, 'eval_samples_per_second': 357.73, 'eval_steps_per_second': 44.755, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.39it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.47it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.77it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.99it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.40it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.97it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.87it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.79it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.96it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.05it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.07it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.00it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.81it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.70it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.67it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.62it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.64it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.76it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.90it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.98it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.81it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.68it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.56it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.55it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.64it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.74it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.83it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.86it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.99it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.83it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.60it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.46it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.62it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.71it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.74it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.74it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.95it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.80it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.84it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.66it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.55it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.52it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.63it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.83it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.85it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.89it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.85it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.78it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.73it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.60it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.54it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.51it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.77it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.86it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.86it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.89it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.79it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.63it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.66it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.53it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.67it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.71it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.83it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.94it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.84it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.84it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.78it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.57it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.53it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.63it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.84it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.80it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.82it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.90it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.82it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.72it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.49it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.49it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.61it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.78it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.80it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.81it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.92it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.91it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.74it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.46it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.60it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:32<01:41,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 44.60it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:31:51,685 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 00:31:51,702 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:31:53,509 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:31:53,524 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:31:53,543 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:38<28:54,  4.96s/it] 40%|████      | 236/585 [01:39<20:41,  3.56s/it] 41%|████      | 237/585 [01:39<14:57,  2.58s/it] 41%|████      | 238/585 [01:39<10:56,  1.89s/it] 41%|████      | 239/585 [01:40<08:08,  1.41s/it] 41%|████      | 240/585 [01:40<06:11,  1.08s/it] 41%|████      | 241/585 [01:40<04:49,  1.19it/s] 41%|████▏     | 242/585 [01:41<03:52,  1.48it/s] 42%|████▏     | 243/585 [01:41<03:12,  1.78it/s] 42%|████▏     | 244/585 [01:41<02:43,  2.08it/s] 42%|████▏     | 245/585 [01:41<02:23,  2.36it/s] 42%|████▏     | 246/585 [01:42<02:09,  2.61it/s] 42%|████▏     | 247/585 [01:42<02:00,  2.81it/s] 42%|████▏     | 248/585 [01:42<01:53,  2.98it/s] 43%|████▎     | 249/585 [01:43<01:48,  3.11it/s] 43%|████▎     | 250/585 [01:43<01:44,  3.21it/s] 43%|████▎     | 251/585 [01:43<01:41,  3.28it/s] 43%|████▎     | 252/585 [01:43<01:39,  3.33it/s] 43%|████▎     | 253/585 [01:44<01:38,  3.37it/s] 43%|████▎     | 254/585 [01:44<01:37,  3.39it/s] 44%|████▎     | 255/585 [01:44<01:36,  3.41it/s] 44%|████▍     | 256/585 [01:45<01:36,  3.43it/s] 44%|████▍     | 257/585 [01:45<01:35,  3.44it/s] 44%|████▍     | 258/585 [01:45<01:35,  3.41it/s] 44%|████▍     | 259/585 [01:45<01:35,  3.43it/s] 44%|████▍     | 260/585 [01:46<01:34,  3.44it/s] 45%|████▍     | 261/585 [01:46<01:34,  3.44it/s] 45%|████▍     | 262/585 [01:46<01:33,  3.44it/s] 45%|████▍     | 263/585 [01:47<01:33,  3.45it/s] 45%|████▌     | 264/585 [01:47<01:33,  3.45it/s] 45%|████▌     | 265/585 [01:47<01:32,  3.45it/s] 45%|████▌     | 266/585 [01:47<01:32,  3.45it/s] 46%|████▌     | 267/585 [01:48<01:32,  3.45it/s] 46%|████▌     | 268/585 [01:48<01:31,  3.45it/s] 46%|████▌     | 269/585 [01:48<01:31,  3.46it/s] 46%|████▌     | 270/585 [01:49<01:31,  3.44it/s] 46%|████▋     | 271/585 [01:49<01:31,  3.45it/s] 46%|████▋     | 272/585 [01:49<01:30,  3.45it/s] 47%|████▋     | 273/585 [01:50<01:30,  3.45it/s] 47%|████▋     | 274/585 [01:50<01:30,  3.45it/s] 47%|████▋     | 275/585 [01:50<01:29,  3.46it/s] 47%|████▋     | 276/585 [01:50<01:29,  3.46it/s] 47%|████▋     | 277/585 [01:51<01:29,  3.46it/s] 48%|████▊     | 278/585 [01:51<01:28,  3.46it/s] 48%|████▊     | 279/585 [01:51<01:30,  3.39it/s] 48%|████▊     | 280/585 [01:52<01:29,  3.41it/s] 48%|████▊     | 281/585 [01:52<01:29,  3.41it/s] 48%|████▊     | 282/585 [01:52<01:28,  3.42it/s] 48%|████▊     | 283/585 [01:52<01:27,  3.43it/s] 49%|████▊     | 284/585 [01:53<01:27,  3.44it/s] 49%|████▊     | 285/585 [01:53<01:27,  3.45it/s] 49%|████▉     | 286/585 [01:53<01:26,  3.45it/s] 49%|████▉     | 287/585 [01:54<01:26,  3.45it/s] 49%|████▉     | 288/585 [01:54<01:25,  3.45it/s] 49%|████▉     | 289/585 [01:54<01:25,  3.45it/s] 50%|████▉     | 290/585 [01:54<01:25,  3.45it/s] 50%|████▉     | 291/585 [01:55<01:25,  3.45it/s] 50%|████▉     | 292/585 [01:55<01:25,  3.44it/s] 50%|█████     | 293/585 [01:55<01:24,  3.45it/s] 50%|█████     | 294/585 [01:56<01:24,  3.45it/s] 50%|█████     | 295/585 [01:56<01:24,  3.45it/s] 51%|█████     | 296/585 [01:56<01:23,  3.46it/s] 51%|█████     | 297/585 [01:56<01:23,  3.45it/s] 51%|█████     | 298/585 [01:57<01:23,  3.46it/s] 51%|█████     | 299/585 [01:57<01:22,  3.45it/s] 51%|█████▏    | 300/585 [01:57<01:22,  3.46it/s] 51%|█████▏    | 301/585 [01:58<01:22,  3.46it/s] 52%|█████▏    | 302/585 [01:58<01:21,  3.46it/s] 52%|█████▏    | 303/585 [01:58<01:21,  3.44it/s] 52%|█████▏    | 304/585 [01:59<01:21,  3.45it/s] 52%|█████▏    | 305/585 [01:59<01:21,  3.45it/s] 52%|█████▏    | 306/585 [01:59<01:20,  3.45it/s] 52%|█████▏    | 307/585 [01:59<01:20,  3.45it/s] 53%|█████▎    | 308/585 [02:00<01:20,  3.45it/s] 53%|█████▎    | 309/585 [02:00<01:19,  3.45it/s] 53%|█████▎    | 310/585 [02:00<01:19,  3.45it/s] 53%|█████▎    | 311/585 [02:01<01:19,  3.45it/s] 53%|█████▎    | 312/585 [02:01<01:19,  3.45it/s] 54%|█████▎    | 313/585 [02:01<01:18,  3.45it/s] 54%|█████▎    | 314/585 [02:01<01:18,  3.44it/s] 54%|█████▍    | 315/585 [02:02<01:18,  3.45it/s] 54%|█████▍    | 316/585 [02:02<01:17,  3.45it/s] 54%|█████▍    | 317/585 [02:02<01:17,  3.45it/s] 54%|█████▍    | 318/585 [02:03<01:17,  3.45it/s] 55%|█████▍    | 319/585 [02:03<01:17,  3.45it/s] 55%|█████▍    | 320/585 [02:03<01:16,  3.45it/s] 55%|█████▍    | 321/585 [02:03<01:16,  3.45it/s] 55%|█████▌    | 322/585 [02:04<01:16,  3.45it/s] 55%|█████▌    | 323/585 [02:04<01:15,  3.46it/s] 55%|█████▌    | 324/585 [02:04<01:15,  3.45it/s] 56%|█████▌    | 325/585 [02:05<01:15,  3.44it/s] 56%|█████▌    | 326/585 [02:05<01:15,  3.44it/s] 56%|█████▌    | 327/585 [02:05<01:14,  3.45it/s] 56%|█████▌    | 328/585 [02:05<01:14,  3.45it/s] 56%|█████▌    | 329/585 [02:06<01:14,  3.45it/s] 56%|█████▋    | 330/585 [02:06<01:13,  3.45it/s] 57%|█████▋    | 331/585 [02:06<01:13,  3.45it/s] 57%|█████▋    | 332/585 [02:07<01:13,  3.45it/s] 57%|█████▋    | 333/585 [02:07<01:12,  3.45it/s] 57%|█████▋    | 334/585 [02:07<01:12,  3.46it/s] 57%|█████▋    | 335/585 [02:07<01:12,  3.46it/s] 57%|█████▋    | 336/585 [02:08<01:12,  3.45it/s] 58%|█████▊    | 337/585 [02:08<01:11,  3.45it/s] 58%|█████▊    | 338/585 [02:08<01:11,  3.45it/s] 58%|█████▊    | 339/585 [02:09<01:11,  3.45it/s] 58%|█████▊    | 340/585 [02:09<01:10,  3.45it/s] 58%|█████▊    | 341/585 [02:09<01:10,  3.45it/s] 58%|█████▊    | 342/585 [02:10<01:10,  3.45it/s] 59%|█████▊    | 343/585 [02:10<01:10,  3.45it/s] 59%|█████▉    | 344/585 [02:10<01:09,  3.45it/s] 59%|█████▉    | 345/585 [02:10<01:09,  3.45it/s] 59%|█████▉    | 346/585 [02:11<01:09,  3.45it/s] 59%|█████▉    | 347/585 [02:11<01:09,  3.44it/s] 59%|█████▉    | 348/585 [02:11<01:08,  3.45it/s] 60%|█████▉    | 349/585 [02:12<01:08,  3.45it/s] 60%|█████▉    | 350/585 [02:12<01:08,  3.45it/s] 60%|██████    | 351/585 [02:12<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:32:31,381 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:32:31,381 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:32:31,381 >>   Batch size = 8
{'eval_loss': 1.101241111755371, 'eval_runtime': 9.7813, 'eval_samples_per_second': 357.11, 'eval_steps_per_second': 44.677, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.97it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.17it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.54it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.74it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.16it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.93it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.89it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.99it/s][A
 11%|█         | 47/437 [00:01<00:08, 45.05it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.04it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.08it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.94it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.72it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.52it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.46it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.59it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.64it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.96it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.97it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 45.08it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.90it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.74it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.51it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.48it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.62it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.76it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.90it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.81it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.94it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.87it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.74it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.50it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.55it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.71it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.82it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.84it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.84it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.90it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.88it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.74it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.73it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.60it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.76it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.70it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.87it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.79it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.75it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.78it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.80it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.59it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.69it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.82it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.78it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.87it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.93it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.79it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.81it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.80it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.82it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.78it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.71it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.76it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.73it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.76it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.77it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.74it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.82it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.79it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.67it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.62it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.72it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.78it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.83it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.74it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.78it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.82it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.86it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.71it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.64it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.72it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.71it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.79it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.71it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.79it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.78it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.84it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.75it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:22<01:07,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 44.75it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:32:41,182 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 00:32:41,207 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:32:42,997 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:32:43,010 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:32:43,025 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:28<18:57,  4.88s/it] 60%|██████    | 353/585 [02:28<13:33,  3.51s/it] 61%|██████    | 354/585 [02:28<09:47,  2.54s/it] 61%|██████    | 355/585 [02:29<07:09,  1.87s/it] 61%|██████    | 356/585 [02:29<05:19,  1.40s/it] 61%|██████    | 357/585 [02:29<04:02,  1.06s/it] 61%|██████    | 358/585 [02:29<03:09,  1.20it/s] 61%|██████▏   | 359/585 [02:30<02:31,  1.49it/s] 62%|██████▏   | 360/585 [02:30<02:05,  1.79it/s] 62%|██████▏   | 361/585 [02:30<01:47,  2.09it/s] 62%|██████▏   | 362/585 [02:31<01:34,  2.37it/s] 62%|██████▏   | 363/585 [02:31<01:25,  2.60it/s] 62%|██████▏   | 364/585 [02:31<01:19,  2.80it/s] 62%|██████▏   | 365/585 [02:32<01:14,  2.96it/s] 63%|██████▎   | 366/585 [02:32<01:11,  3.08it/s] 63%|██████▎   | 367/585 [02:32<01:08,  3.17it/s] 63%|██████▎   | 368/585 [02:32<01:07,  3.24it/s] 63%|██████▎   | 369/585 [02:33<01:05,  3.29it/s] 63%|██████▎   | 370/585 [02:33<01:04,  3.32it/s] 63%|██████▎   | 371/585 [02:33<01:03,  3.35it/s] 64%|██████▎   | 372/585 [02:34<01:03,  3.37it/s] 64%|██████▍   | 373/585 [02:34<01:02,  3.38it/s] 64%|██████▍   | 374/585 [02:34<01:02,  3.39it/s] 64%|██████▍   | 375/585 [02:34<01:02,  3.38it/s] 64%|██████▍   | 376/585 [02:35<01:01,  3.37it/s] 64%|██████▍   | 377/585 [02:35<01:01,  3.38it/s] 65%|██████▍   | 378/585 [02:35<01:01,  3.39it/s] 65%|██████▍   | 379/585 [02:36<01:00,  3.40it/s] 65%|██████▍   | 380/585 [02:36<01:00,  3.40it/s] 65%|██████▌   | 381/585 [02:36<00:59,  3.40it/s] 65%|██████▌   | 382/585 [02:37<00:59,  3.41it/s] 65%|██████▌   | 383/585 [02:37<00:59,  3.41it/s] 66%|██████▌   | 384/585 [02:37<00:58,  3.41it/s] 66%|██████▌   | 385/585 [02:37<00:58,  3.41it/s] 66%|██████▌   | 386/585 [02:38<00:58,  3.40it/s] 66%|██████▌   | 387/585 [02:38<00:58,  3.40it/s] 66%|██████▋   | 388/585 [02:38<00:57,  3.41it/s] 66%|██████▋   | 389/585 [02:39<00:57,  3.41it/s] 67%|██████▋   | 390/585 [02:39<00:57,  3.41it/s] 67%|██████▋   | 391/585 [02:39<00:56,  3.41it/s] 67%|██████▋   | 392/585 [02:39<00:56,  3.41it/s] 67%|██████▋   | 393/585 [02:40<00:56,  3.41it/s] 67%|██████▋   | 394/585 [02:40<00:56,  3.41it/s] 68%|██████▊   | 395/585 [02:40<00:55,  3.41it/s] 68%|██████▊   | 396/585 [02:41<00:55,  3.41it/s] 68%|██████▊   | 397/585 [02:41<00:55,  3.40it/s] 68%|██████▊   | 398/585 [02:41<00:54,  3.41it/s] 68%|██████▊   | 399/585 [02:42<00:54,  3.41it/s] 68%|██████▊   | 400/585 [02:42<00:54,  3.41it/s] 69%|██████▊   | 401/585 [02:42<00:53,  3.41it/s] 69%|██████▊   | 402/585 [02:42<00:53,  3.41it/s] 69%|██████▉   | 403/585 [02:43<00:53,  3.41it/s] 69%|██████▉   | 404/585 [02:43<00:53,  3.41it/s] 69%|██████▉   | 405/585 [02:43<00:52,  3.41it/s] 69%|██████▉   | 406/585 [02:44<00:52,  3.41it/s] 70%|██████▉   | 407/585 [02:44<00:52,  3.41it/s] 70%|██████▉   | 408/585 [02:44<00:52,  3.39it/s] 70%|██████▉   | 409/585 [02:44<00:51,  3.40it/s] 70%|███████   | 410/585 [02:45<00:51,  3.40it/s] 70%|███████   | 411/585 [02:45<00:51,  3.40it/s] 70%|███████   | 412/585 [02:45<00:50,  3.40it/s] 71%|███████   | 413/585 [02:46<00:50,  3.41it/s] 71%|███████   | 414/585 [02:46<00:50,  3.41it/s] 71%|███████   | 415/585 [02:46<00:49,  3.41it/s] 71%|███████   | 416/585 [02:47<00:49,  3.41it/s] 71%|███████▏  | 417/585 [02:47<00:49,  3.41it/s] 71%|███████▏  | 418/585 [02:47<00:49,  3.41it/s] 72%|███████▏  | 419/585 [02:47<00:48,  3.41it/s] 72%|███████▏  | 420/585 [02:48<00:48,  3.41it/s] 72%|███████▏  | 421/585 [02:48<00:48,  3.41it/s] 72%|███████▏  | 422/585 [02:48<00:47,  3.41it/s] 72%|███████▏  | 423/585 [02:49<00:47,  3.41it/s] 72%|███████▏  | 424/585 [02:49<00:47,  3.41it/s] 73%|███████▎  | 425/585 [02:49<00:46,  3.41it/s] 73%|███████▎  | 426/585 [02:49<00:46,  3.41it/s] 73%|███████▎  | 427/585 [02:50<00:46,  3.41it/s] 73%|███████▎  | 428/585 [02:50<00:46,  3.40it/s] 73%|███████▎  | 429/585 [02:50<00:45,  3.42it/s] 74%|███████▎  | 430/585 [02:51<00:45,  3.43it/s] 74%|███████▎  | 431/585 [02:51<00:44,  3.44it/s] 74%|███████▍  | 432/585 [02:51<00:44,  3.45it/s] 74%|███████▍  | 433/585 [02:52<00:45,  3.36it/s] 74%|███████▍  | 434/585 [02:52<00:44,  3.39it/s] 74%|███████▍  | 435/585 [02:52<00:43,  3.41it/s] 75%|███████▍  | 436/585 [02:52<00:43,  3.43it/s] 75%|███████▍  | 437/585 [02:53<00:43,  3.44it/s] 75%|███████▍  | 438/585 [02:53<00:42,  3.44it/s] 75%|███████▌  | 439/585 [02:53<00:42,  3.43it/s] 75%|███████▌  | 440/585 [02:54<00:42,  3.44it/s] 75%|███████▌  | 441/585 [02:54<00:41,  3.44it/s] 76%|███████▌  | 442/585 [02:54<00:41,  3.45it/s] 76%|███████▌  | 443/585 [02:54<00:41,  3.45it/s] 76%|███████▌  | 444/585 [02:55<00:40,  3.45it/s] 76%|███████▌  | 445/585 [02:55<00:40,  3.45it/s] 76%|███████▌  | 446/585 [02:55<00:40,  3.46it/s] 76%|███████▋  | 447/585 [02:56<00:39,  3.45it/s] 77%|███████▋  | 448/585 [02:56<00:39,  3.46it/s] 77%|███████▋  | 449/585 [02:56<00:39,  3.45it/s] 77%|███████▋  | 450/585 [02:56<00:39,  3.44it/s] 77%|███████▋  | 451/585 [02:57<00:38,  3.44it/s] 77%|███████▋  | 452/585 [02:57<00:38,  3.45it/s] 77%|███████▋  | 453/585 [02:57<00:38,  3.45it/s] 78%|███████▊  | 454/585 [02:58<00:37,  3.45it/s] 78%|███████▊  | 455/585 [02:58<00:37,  3.46it/s] 78%|███████▊  | 456/585 [02:58<00:37,  3.46it/s] 78%|███████▊  | 457/585 [02:58<00:37,  3.46it/s] 78%|███████▊  | 458/585 [02:59<00:36,  3.46it/s] 78%|███████▊  | 459/585 [02:59<00:36,  3.46it/s] 79%|███████▊  | 460/585 [02:59<00:36,  3.46it/s] 79%|███████▉  | 461/585 [03:00<00:35,  3.45it/s] 79%|███████▉  | 462/585 [03:00<00:35,  3.45it/s] 79%|███████▉  | 463/585 [03:00<00:35,  3.45it/s] 79%|███████▉  | 464/585 [03:00<00:35,  3.45it/s] 79%|███████▉  | 465/585 [03:01<00:34,  3.46it/s] 80%|███████▉  | 466/585 [03:01<00:34,  3.46it/s] 80%|███████▉  | 467/585 [03:01<00:34,  3.46it/s] 80%|████████  | 468/585 [03:02<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 00:33:20,894 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:33:20,894 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:33:20,894 >>   Batch size = 8
{'eval_loss': 1.1180546283721924, 'eval_runtime': 9.7754, 'eval_samples_per_second': 357.327, 'eval_steps_per_second': 44.704, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.97it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.46it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.58it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.85it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.35it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.00it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.94it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.87it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.91it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.00it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.04it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.77it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.73it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.75it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.71it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.74it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.66it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.72it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.74it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.88it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.81it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.71it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.70it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.76it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.74it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.72it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.77it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.83it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.84it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.81it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.65it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.70it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.66it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.64it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.70it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.73it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.85it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.84it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.77it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.76it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.59it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.54it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.69it/s][A
 51%|█████     | 222/437 [00:04<00:04, 44.72it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.72it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.82it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.87it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.86it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.74it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.63it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.66it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.72it/s][A
 61%|██████    | 267/437 [00:05<00:03, 44.79it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.75it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.75it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.76it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.84it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.69it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.68it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.67it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.74it/s][A
 71%|███████▏  | 312/437 [00:06<00:02, 44.85it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.71it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.77it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.73it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.81it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.69it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.65it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.69it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.78it/s][A
 82%|████████▏ | 357/437 [00:07<00:01, 44.75it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.76it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.69it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.87it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.72it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.71it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.66it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.77it/s][A
 91%|█████████ | 397/437 [00:08<00:00, 44.78it/s][A
 92%|█████████▏| 402/437 [00:08<00:00, 44.74it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.72it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.80it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.72it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.67it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.75it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.76it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.74it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:11<00:33,  3.46it/s]
100%|██████████| 437/437 [00:09<00:00, 44.74it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:33:30,692 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 00:33:30,720 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:33:32,463 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:33:32,478 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:33:32,491 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:17<09:19,  4.82s/it] 80%|████████  | 470/585 [03:17<06:38,  3.47s/it] 81%|████████  | 471/585 [03:18<04:46,  2.51s/it] 81%|████████  | 472/585 [03:18<03:28,  1.85s/it] 81%|████████  | 473/585 [03:18<02:34,  1.38s/it] 81%|████████  | 474/585 [03:19<01:56,  1.05s/it] 81%|████████  | 475/585 [03:19<01:30,  1.21it/s] 81%|████████▏ | 476/585 [03:19<01:12,  1.51it/s] 82%|████████▏ | 477/585 [03:19<00:59,  1.82it/s] 82%|████████▏ | 478/585 [03:20<00:50,  2.12it/s] 82%|████████▏ | 479/585 [03:20<00:44,  2.40it/s] 82%|████████▏ | 480/585 [03:20<00:39,  2.64it/s] 82%|████████▏ | 481/585 [03:21<00:37,  2.78it/s] 82%|████████▏ | 482/585 [03:21<00:34,  2.95it/s] 83%|████████▎ | 483/585 [03:21<00:33,  3.09it/s] 83%|████████▎ | 484/585 [03:21<00:31,  3.19it/s] 83%|████████▎ | 485/585 [03:22<00:30,  3.27it/s] 83%|████████▎ | 486/585 [03:22<00:29,  3.32it/s] 83%|████████▎ | 487/585 [03:22<00:29,  3.36it/s] 83%|████████▎ | 488/585 [03:23<00:28,  3.39it/s] 84%|████████▎ | 489/585 [03:23<00:28,  3.41it/s] 84%|████████▍ | 490/585 [03:23<00:27,  3.43it/s] 84%|████████▍ | 491/585 [03:23<00:27,  3.44it/s] 84%|████████▍ | 492/585 [03:24<00:27,  3.43it/s] 84%|████████▍ | 493/585 [03:24<00:26,  3.44it/s] 84%|████████▍ | 494/585 [03:24<00:26,  3.45it/s] 85%|████████▍ | 495/585 [03:25<00:26,  3.45it/s] 85%|████████▍ | 496/585 [03:25<00:25,  3.45it/s] 85%|████████▍ | 497/585 [03:25<00:25,  3.46it/s] 85%|████████▌ | 498/585 [03:25<00:25,  3.45it/s] 85%|████████▌ | 499/585 [03:26<00:24,  3.46it/s] 85%|████████▌ | 500/585 [03:26<00:24,  3.46it/s]                                                  85%|████████▌ | 500/585 [03:26<00:24,  3.46it/s] 86%|████████▌ | 501/585 [03:26<00:24,  3.46it/s] 86%|████████▌ | 502/585 [03:27<00:24,  3.46it/s] 86%|████████▌ | 503/585 [03:27<00:23,  3.43it/s] 86%|████████▌ | 504/585 [03:27<00:23,  3.44it/s] 86%|████████▋ | 505/585 [03:28<00:23,  3.45it/s] 86%|████████▋ | 506/585 [03:28<00:22,  3.45it/s] 87%|████████▋ | 507/585 [03:28<00:22,  3.45it/s] 87%|████████▋ | 508/585 [03:28<00:22,  3.45it/s] 87%|████████▋ | 509/585 [03:29<00:21,  3.46it/s] 87%|████████▋ | 510/585 [03:29<00:21,  3.46it/s] 87%|████████▋ | 511/585 [03:29<00:21,  3.45it/s] 88%|████████▊ | 512/585 [03:30<00:21,  3.45it/s] 88%|████████▊ | 513/585 [03:30<00:20,  3.45it/s] 88%|████████▊ | 514/585 [03:30<00:20,  3.44it/s] 88%|████████▊ | 515/585 [03:30<00:20,  3.44it/s] 88%|████████▊ | 516/585 [03:31<00:20,  3.45it/s] 88%|████████▊ | 517/585 [03:31<00:19,  3.45it/s] 89%|████████▊ | 518/585 [03:31<00:19,  3.45it/s] 89%|████████▊ | 519/585 [03:32<00:19,  3.46it/s] 89%|████████▉ | 520/585 [03:32<00:18,  3.46it/s] 89%|████████▉ | 521/585 [03:32<00:18,  3.46it/s] 89%|████████▉ | 522/585 [03:32<00:18,  3.46it/s] 89%|████████▉ | 523/585 [03:33<00:17,  3.46it/s] 90%|████████▉ | 524/585 [03:33<00:17,  3.46it/s] 90%|████████▉ | 525/585 [03:33<00:17,  3.45it/s] 90%|████████▉ | 526/585 [03:34<00:17,  3.45it/s] 90%|█████████ | 527/585 [03:34<00:16,  3.45it/s] 90%|█████████ | 528/585 [03:34<00:16,  3.45it/s] 90%|█████████ | 529/585 [03:34<00:16,  3.45it/s] 91%|█████████ | 530/585 [03:35<00:15,  3.44it/s] 91%|█████████ | 531/585 [03:35<00:15,  3.43it/s] 91%|█████████ | 532/585 [03:35<00:15,  3.43it/s] 91%|█████████ | 533/585 [03:36<00:15,  3.44it/s] 91%|█████████▏| 534/585 [03:36<00:14,  3.45it/s] 91%|█████████▏| 535/585 [03:36<00:14,  3.45it/s] 92%|█████████▏| 536/585 [03:36<00:14,  3.44it/s] 92%|█████████▏| 537/585 [03:37<00:13,  3.44it/s] 92%|█████████▏| 538/585 [03:37<00:13,  3.45it/s] 92%|█████████▏| 539/585 [03:37<00:13,  3.45it/s] 92%|█████████▏| 540/585 [03:38<00:13,  3.45it/s] 92%|█████████▏| 541/585 [03:38<00:12,  3.46it/s] 93%|█████████▎| 542/585 [03:38<00:12,  3.46it/s] 93%|█████████▎| 543/585 [03:39<00:12,  3.46it/s] 93%|█████████▎| 544/585 [03:39<00:11,  3.46it/s] 93%|█████████▎| 545/585 [03:39<00:11,  3.46it/s] 93%|█████████▎| 546/585 [03:39<00:11,  3.46it/s] 94%|█████████▎| 547/585 [03:40<00:11,  3.45it/s] 94%|█████████▎| 548/585 [03:40<00:10,  3.45it/s] 94%|█████████▍| 549/585 [03:40<00:10,  3.45it/s] 94%|█████████▍| 550/585 [03:41<00:10,  3.46it/s] 94%|█████████▍| 551/585 [03:41<00:09,  3.46it/s] 94%|█████████▍| 552/585 [03:41<00:09,  3.46it/s] 95%|█████████▍| 553/585 [03:41<00:09,  3.46it/s] 95%|█████████▍| 554/585 [03:42<00:08,  3.46it/s] 95%|█████████▍| 555/585 [03:42<00:08,  3.46it/s] 95%|█████████▌| 556/585 [03:42<00:08,  3.46it/s] 95%|█████████▌| 557/585 [03:43<00:08,  3.46it/s] 95%|█████████▌| 558/585 [03:43<00:07,  3.44it/s] 96%|█████████▌| 559/585 [03:43<00:07,  3.45it/s] 96%|█████████▌| 560/585 [03:43<00:07,  3.45it/s] 96%|█████████▌| 561/585 [03:44<00:06,  3.45it/s] 96%|█████████▌| 562/585 [03:44<00:06,  3.45it/s] 96%|█████████▌| 563/585 [03:44<00:06,  3.45it/s] 96%|█████████▋| 564/585 [03:45<00:06,  3.46it/s] 97%|█████████▋| 565/585 [03:45<00:05,  3.46it/s] 97%|█████████▋| 566/585 [03:45<00:05,  3.46it/s] 97%|█████████▋| 567/585 [03:45<00:05,  3.46it/s] 97%|█████████▋| 568/585 [03:46<00:04,  3.46it/s] 97%|█████████▋| 569/585 [03:46<00:04,  3.44it/s] 97%|█████████▋| 570/585 [03:46<00:04,  3.44it/s] 98%|█████████▊| 571/585 [03:47<00:04,  3.45it/s] 98%|█████████▊| 572/585 [03:47<00:03,  3.45it/s] 98%|█████████▊| 573/585 [03:47<00:03,  3.45it/s] 98%|█████████▊| 574/585 [03:47<00:03,  3.45it/s] 98%|█████████▊| 575/585 [03:48<00:02,  3.45it/s] 98%|█████████▊| 576/585 [03:48<00:02,  3.45it/s] 99%|█████████▊| 577/585 [03:48<00:02,  3.46it/s] 99%|█████████▉| 578/585 [03:49<00:02,  3.46it/s] 99%|█████████▉| 579/585 [03:49<00:01,  3.46it/s] 99%|█████████▉| 580/585 [03:49<00:01,  3.46it/s] 99%|█████████▉| 581/585 [03:50<00:01,  3.46it/s] 99%|█████████▉| 582/585 [03:50<00:00,  3.46it/s]100%|█████████▉| 583/585 [03:50<00:00,  3.46it/s]100%|█████████▉| 584/585 [03:50<00:00,  3.46it/s]100%|██████████| 585/585 [03:51<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 00:34:09,893 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:34:09,893 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:34:09,893 >>   Batch size = 8
{'eval_loss': 1.1295918226242065, 'eval_runtime': 9.7799, 'eval_samples_per_second': 357.163, 'eval_steps_per_second': 44.684, 'epoch': 4.0}
{'loss': 0.4056, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.24it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.62it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.72it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.94it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.60it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.25it/s][A
  8%|▊         | 37/437 [00:00<00:09, 42.48it/s][A
 10%|▉         | 43/437 [00:00<00:08, 44.62it/s][A
 11%|█         | 48/437 [00:01<00:08, 44.93it/s][A
 12%|█▏        | 53/437 [00:01<00:08, 45.03it/s][A
 13%|█▎        | 58/437 [00:01<00:08, 44.84it/s][A
 14%|█▍        | 63/437 [00:01<00:08, 44.74it/s][A
 16%|█▌        | 68/437 [00:01<00:08, 44.73it/s][A
 17%|█▋        | 73/437 [00:01<00:08, 44.73it/s][A
 18%|█▊        | 78/437 [00:01<00:08, 44.52it/s][A
 19%|█▉        | 83/437 [00:01<00:07, 44.42it/s][A
 20%|██        | 88/437 [00:01<00:07, 44.67it/s][A
 21%|██▏       | 93/437 [00:02<00:07, 44.84it/s][A
 22%|██▏       | 98/437 [00:02<00:07, 44.90it/s][A
 24%|██▎       | 103/437 [00:02<00:07, 44.92it/s][A
 25%|██▍       | 108/437 [00:02<00:07, 44.86it/s][A
 26%|██▌       | 113/437 [00:02<00:07, 44.75it/s][A
 27%|██▋       | 118/437 [00:02<00:07, 44.75it/s][A
 28%|██▊       | 123/437 [00:02<00:07, 44.62it/s][A
 29%|██▉       | 128/437 [00:02<00:06, 44.58it/s][A
 30%|███       | 133/437 [00:02<00:06, 44.57it/s][A
 32%|███▏      | 138/437 [00:03<00:06, 44.75it/s][A
 33%|███▎      | 143/437 [00:03<00:06, 44.98it/s][A
 34%|███▍      | 148/437 [00:03<00:06, 44.89it/s][A
 35%|███▌      | 153/437 [00:03<00:06, 44.93it/s][A
 36%|███▌      | 158/437 [00:03<00:06, 44.90it/s][A
 37%|███▋      | 163/437 [00:03<00:06, 44.77it/s][A
 38%|███▊      | 168/437 [00:03<00:06, 44.71it/s][A
 40%|███▉      | 173/437 [00:03<00:05, 44.60it/s][A
 41%|████      | 178/437 [00:03<00:05, 44.64it/s][A
 42%|████▏     | 183/437 [00:04<00:05, 44.70it/s][A
 43%|████▎     | 188/437 [00:04<00:05, 44.89it/s][A
 44%|████▍     | 193/437 [00:04<00:05, 44.81it/s][A
 45%|████▌     | 198/437 [00:04<00:05, 44.90it/s][A
 46%|████▋     | 203/437 [00:04<00:05, 44.92it/s][A
 48%|████▊     | 208/437 [00:04<00:05, 44.76it/s][A
 49%|████▊     | 213/437 [00:04<00:05, 44.66it/s][A
 50%|████▉     | 218/437 [00:04<00:04, 44.65it/s][A
 51%|█████     | 223/437 [00:04<00:04, 44.60it/s][A
 52%|█████▏    | 228/437 [00:05<00:04, 44.77it/s][A
 53%|█████▎    | 233/437 [00:05<00:04, 44.75it/s][A
 54%|█████▍    | 238/437 [00:05<00:04, 44.90it/s][A
 56%|█████▌    | 243/437 [00:05<00:04, 44.90it/s][A
 57%|█████▋    | 248/437 [00:05<00:04, 44.66it/s][A
 58%|█████▊    | 253/437 [00:05<00:04, 44.78it/s][A
 59%|█████▉    | 258/437 [00:05<00:04, 44.67it/s][A
 60%|██████    | 263/437 [00:05<00:03, 44.69it/s][A
 61%|██████▏   | 268/437 [00:05<00:03, 44.71it/s][A
 62%|██████▏   | 273/437 [00:06<00:03, 44.87it/s][A
 64%|██████▎   | 278/437 [00:06<00:03, 44.84it/s][A
 65%|██████▍   | 283/437 [00:06<00:03, 44.94it/s][A
 66%|██████▌   | 288/437 [00:06<00:03, 44.93it/s][A
 67%|██████▋   | 293/437 [00:06<00:03, 44.90it/s][A
 68%|██████▊   | 298/437 [00:06<00:03, 44.74it/s][A
 69%|██████▉   | 303/437 [00:06<00:03, 44.67it/s][A
 70%|███████   | 308/437 [00:06<00:02, 44.52it/s][A
 72%|███████▏  | 313/437 [00:06<00:02, 44.69it/s][A
 73%|███████▎  | 318/437 [00:07<00:02, 44.86it/s][A
 74%|███████▍  | 323/437 [00:07<00:02, 44.88it/s][A
 75%|███████▌  | 328/437 [00:07<00:02, 44.89it/s][A
 76%|███████▌  | 333/437 [00:07<00:02, 44.94it/s][A
 77%|███████▋  | 338/437 [00:07<00:02, 44.93it/s][A
 78%|███████▊  | 343/437 [00:07<00:02, 44.76it/s][A
 80%|███████▉  | 348/437 [00:07<00:01, 44.66it/s][A
 81%|████████  | 353/437 [00:07<00:01, 44.65it/s][A
 82%|████████▏ | 358/437 [00:07<00:01, 44.75it/s][A
 83%|████████▎ | 363/437 [00:08<00:01, 44.65it/s][A
 84%|████████▍ | 368/437 [00:08<00:01, 44.79it/s][A
 85%|████████▌ | 373/437 [00:08<00:01, 44.95it/s][A
 86%|████████▋ | 378/437 [00:08<00:01, 44.99it/s][A
 88%|████████▊ | 383/437 [00:08<00:01, 44.92it/s][A
 89%|████████▉ | 388/437 [00:08<00:01, 44.68it/s][A
 90%|████████▉ | 393/437 [00:08<00:00, 44.60it/s][A
 91%|█████████ | 398/437 [00:08<00:00, 44.62it/s][A
 92%|█████████▏| 403/437 [00:08<00:00, 44.67it/s][A
 93%|█████████▎| 408/437 [00:09<00:00, 44.64it/s][A
 95%|█████████▍| 413/437 [00:09<00:00, 44.75it/s][A
 96%|█████████▌| 418/437 [00:09<00:00, 44.92it/s][A
 97%|█████████▋| 423/437 [00:09<00:00, 44.96it/s][A
 98%|█████████▊| 428/437 [00:09<00:00, 44.92it/s][A
 99%|█████████▉| 433/437 [00:09<00:00, 44.69it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:00<00:00,  3.46it/s]
100%|██████████| 437/437 [00:09<00:00, 44.69it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:34:19,675 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 00:34:19,696 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:34:21,585 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:34:21,602 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:34:21,612 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:34:25,486 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:34:25,489 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117 (score: 1.0843169689178467).
                                                 100%|██████████| 585/585 [04:08<00:00,  3.46it/s]100%|██████████| 585/585 [04:08<00:00,  2.35it/s]
[INFO|trainer.py:1894] 2023-08-29 00:34:27,313 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 00:34:27,330 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:34:29,321 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:34:29,345 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:34:29,356 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:34:29,560 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:29,560 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:29,560 >>   train_loss               =     0.4027
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:29,560 >>   train_runtime            = 0:04:08.59
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:29,560 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:29,560 >>   train_samples_per_second =    150.845
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:29,560 >>   train_steps_per_second   =      2.353
{'eval_loss': 1.1348763704299927, 'eval_runtime': 9.7637, 'eval_samples_per_second': 357.753, 'eval_steps_per_second': 44.758, 'epoch': 5.0}
{'train_runtime': 248.5988, 'train_samples_per_second': 150.845, 'train_steps_per_second': 2.353, 'train_loss': 0.40268782917250934, 'epoch': 5.0}
08/29/2023 00:34:29 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:34:29,603 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:34:29,604 >>   Num examples = 3493
[INFO|trainer.py:2145] 2023-08-29 00:34:29,604 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.88it/s]  3%|▎         | 12/437 [00:00<00:08, 49.30it/s]  4%|▍         | 17/437 [00:00<00:08, 47.63it/s]  5%|▌         | 22/437 [00:00<00:08, 46.94it/s]  6%|▌         | 27/437 [00:00<00:08, 46.48it/s]  7%|▋         | 32/437 [00:00<00:08, 46.11it/s]  8%|▊         | 37/437 [00:00<00:08, 45.98it/s] 10%|▉         | 42/437 [00:00<00:08, 45.50it/s] 11%|█         | 47/437 [00:01<00:08, 44.84it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.66it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.71it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.95it/s] 15%|█▌        | 67/437 [00:01<00:08, 45.16it/s] 16%|█▋        | 72/437 [00:01<00:08, 45.20it/s] 18%|█▊        | 77/437 [00:01<00:07, 45.28it/s] 19%|█▉        | 82/437 [00:01<00:07, 45.29it/s] 20%|█▉        | 87/437 [00:01<00:07, 45.12it/s] 21%|██        | 92/437 [00:02<00:07, 44.72it/s] 22%|██▏       | 97/437 [00:02<00:07, 44.60it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.59it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.85it/s] 26%|██▌       | 112/437 [00:02<00:07, 45.03it/s] 27%|██▋       | 117/437 [00:02<00:07, 45.21it/s] 28%|██▊       | 122/437 [00:02<00:06, 45.31it/s] 29%|██▉       | 127/437 [00:02<00:06, 45.24it/s] 30%|███       | 132/437 [00:02<00:06, 45.01it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.71it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.60it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.77it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.84it/s] 36%|███▌      | 157/437 [00:03<00:06, 45.01it/s] 37%|███▋      | 162/437 [00:03<00:06, 45.08it/s] 38%|███▊      | 167/437 [00:03<00:05, 45.22it/s] 39%|███▉      | 172/437 [00:03<00:05, 45.27it/s] 41%|████      | 177/437 [00:03<00:05, 45.11it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.93it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.79it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.85it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.87it/s] 46%|████▌     | 202/437 [00:04<00:05, 45.05it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.96it/s] 49%|████▊     | 212/437 [00:04<00:04, 45.07it/s] 50%|████▉     | 217/437 [00:04<00:04, 45.20it/s] 51%|█████     | 222/437 [00:04<00:04, 45.13it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.81it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.79it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.88it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.96it/s] 57%|█████▋    | 247/437 [00:05<00:04, 45.06it/s] 58%|█████▊    | 252/437 [00:05<00:04, 45.04it/s] 59%|█████▉    | 257/437 [00:05<00:03, 45.05it/s] 60%|█████▉    | 262/437 [00:05<00:03, 45.16it/s] 61%|██████    | 267/437 [00:05<00:03, 45.08it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.85it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.76it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.77it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.87it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.99it/s] 68%|██████▊   | 297/437 [00:06<00:03, 45.06it/s] 69%|██████▉   | 302/437 [00:06<00:02, 45.05it/s] 70%|███████   | 307/437 [00:06<00:02, 45.07it/s] 71%|███████▏  | 312/437 [00:06<00:02, 44.87it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.94it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.81it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.78it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.90it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.98it/s] 78%|███████▊  | 342/437 [00:07<00:02, 45.08it/s] 79%|███████▉  | 347/437 [00:07<00:01, 45.05it/s] 81%|████████  | 352/437 [00:07<00:01, 45.06it/s] 82%|████████▏ | 357/437 [00:07<00:01, 44.91it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.93it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.90it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.89it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.85it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.86it/s] 89%|████████▊ | 387/437 [00:08<00:01, 45.03it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.98it/s] 91%|█████████ | 397/437 [00:08<00:00, 45.04it/s] 92%|█████████▏| 402/437 [00:08<00:00, 44.92it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.85it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.89it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.93it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.90it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.87it/s] 99%|█████████▉| 432/437 [00:09<00:00, 45.13it/s]100%|██████████| 437/437 [00:09<00:00, 45.20it/s]100%|██████████| 437/437 [00:09<00:00, 45.08it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:34:39,313 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:39,313 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:39,313 >>   eval_loss               =     1.0843
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:39,313 >>   eval_runtime            = 0:00:09.70
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:39,313 >>   eval_samples            =       3493
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:39,313 >>   eval_samples_per_second =    359.759
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:39,313 >>   eval_steps_per_second   =     45.008
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:34:39,313 >>   perplexity              =     2.9574
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:46,308 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:46,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:46,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:46,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:46,323 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:34:47,050 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:34:47,050 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:34:47,639 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:34:48,673 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:34:48,673 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:51,516 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:51,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:51,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:51,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:34:51,520 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:34:52,212 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:34:52,213 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:34:52,769 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:34:52,933 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:34:52,933 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/generator/iter5/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12885
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12985, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 3it [00:02,  1.36it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.51it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:06,  1.55it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:09,  1.61it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:10,  1.60it/s]Extractor Predicting: 17it [00:11,  1.61it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:13,  1.54it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:16,  1.61it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:18,  1.55it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.61it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.62it/s]Extractor Predicting: 33it [00:21,  1.49it/s]Extractor Predicting: 34it [00:21,  1.50it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:23,  1.55it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.59it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.61it/s]Extractor Predicting: 42it [00:26,  1.61it/s]Extractor Predicting: 43it [00:27,  1.58it/s]Extractor Predicting: 44it [00:28,  1.61it/s]Extractor Predicting: 45it [00:28,  1.59it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:30,  1.58it/s]Extractor Predicting: 48it [00:30,  1.63it/s]Extractor Predicting: 49it [00:31,  1.65it/s]Extractor Predicting: 50it [00:31,  1.61it/s]Extractor Predicting: 51it [00:32,  1.60it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:33,  1.65it/s]Extractor Predicting: 54it [00:34,  1.65it/s]Extractor Predicting: 55it [00:34,  1.65it/s]Extractor Predicting: 56it [00:35,  1.61it/s]Extractor Predicting: 57it [00:36,  1.63it/s]Extractor Predicting: 58it [00:36,  1.61it/s]Extractor Predicting: 59it [00:37,  1.59it/s]Extractor Predicting: 60it [00:38,  1.56it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.56it/s]Extractor Predicting: 63it [00:40,  1.57it/s]Extractor Predicting: 64it [00:40,  1.57it/s]Extractor Predicting: 65it [00:41,  1.56it/s]Extractor Predicting: 66it [00:41,  1.55it/s]Extractor Predicting: 67it [00:42,  1.53it/s]Extractor Predicting: 68it [00:43,  1.51it/s]Extractor Predicting: 69it [00:43,  1.53it/s]Extractor Predicting: 70it [00:44,  1.53it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:45,  1.59it/s]Extractor Predicting: 73it [00:46,  1.58it/s]Extractor Predicting: 74it [00:47,  1.55it/s]Extractor Predicting: 75it [00:47,  1.58it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:49,  1.55it/s]Extractor Predicting: 78it [00:49,  1.53it/s]Extractor Predicting: 79it [00:50,  1.54it/s]Extractor Predicting: 80it [00:51,  1.54it/s]Extractor Predicting: 81it [00:51,  1.53it/s]Extractor Predicting: 82it [00:52,  1.54it/s]Extractor Predicting: 83it [00:52,  1.58it/s]Extractor Predicting: 84it [00:53,  1.59it/s]Extractor Predicting: 85it [00:54,  1.58it/s]Extractor Predicting: 86it [00:54,  1.52it/s]Extractor Predicting: 87it [00:55,  1.57it/s]Extractor Predicting: 88it [00:56,  1.56it/s]Extractor Predicting: 89it [00:56,  1.56it/s]Extractor Predicting: 90it [00:57,  1.52it/s]Extractor Predicting: 91it [00:58,  1.51it/s]Extractor Predicting: 92it [00:58,  1.53it/s]Extractor Predicting: 93it [00:59,  1.53it/s]Extractor Predicting: 94it [01:00,  1.56it/s]Extractor Predicting: 95it [01:00,  1.60it/s]Extractor Predicting: 96it [01:01,  1.61it/s]Extractor Predicting: 97it [01:01,  1.58it/s]Extractor Predicting: 98it [01:02,  1.59it/s]Extractor Predicting: 99it [01:03,  1.45it/s]Extractor Predicting: 100it [01:04,  1.50it/s]Extractor Predicting: 101it [01:04,  1.51it/s]Extractor Predicting: 102it [01:05,  1.51it/s]Extractor Predicting: 103it [01:05,  1.52it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:07,  1.54it/s]Extractor Predicting: 106it [01:07,  1.53it/s]Extractor Predicting: 107it [01:08,  1.53it/s]Extractor Predicting: 108it [01:09,  1.52it/s]Extractor Predicting: 109it [01:09,  1.53it/s]Extractor Predicting: 110it [01:10,  1.52it/s]Extractor Predicting: 111it [01:11,  1.52it/s]Extractor Predicting: 112it [01:11,  1.52it/s]Extractor Predicting: 113it [01:12,  1.50it/s]Extractor Predicting: 114it [01:13,  1.49it/s]Extractor Predicting: 115it [01:13,  1.48it/s]Extractor Predicting: 116it [01:14,  1.49it/s]Extractor Predicting: 117it [01:15,  1.48it/s]Extractor Predicting: 118it [01:15,  1.49it/s]Extractor Predicting: 119it [01:16,  1.49it/s]Extractor Predicting: 120it [01:17,  1.51it/s]Extractor Predicting: 121it [01:17,  1.49it/s]Extractor Predicting: 122it [01:18,  1.49it/s]Extractor Predicting: 123it [01:19,  1.54it/s]Extractor Predicting: 124it [01:19,  1.54it/s]Extractor Predicting: 125it [01:20,  1.51it/s]Extractor Predicting: 126it [01:21,  1.51it/s]Extractor Predicting: 127it [01:21,  1.52it/s]Extractor Predicting: 128it [01:22,  1.52it/s]Extractor Predicting: 129it [01:23,  1.50it/s]Extractor Predicting: 130it [01:23,  1.50it/s]Extractor Predicting: 131it [01:24,  1.48it/s]Extractor Predicting: 132it [01:25,  1.46it/s]Extractor Predicting: 133it [01:25,  1.46it/s]Extractor Predicting: 134it [01:26,  1.44it/s]Extractor Predicting: 135it [01:27,  1.46it/s]Extractor Predicting: 136it [01:27,  1.79it/s]Extractor Predicting: 136it [01:27,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:29,040 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:29,046 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:29,046 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:29,046 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:29,046 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:36:29,695 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:36:29,696 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:36:30,299 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:36:31,326 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:36:31,327 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:34,303 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:34,307 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:34,308 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:34,308 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:36:34,308 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:36:34,936 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:36:34,937 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:36:35,520 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:36:35,672 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:36:35,672 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.44,
  "recall": 0.1259662181505869,
  "score": 0.1958602270198086,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20625
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20725, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:06,  1.58it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.65it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:17,  1.68it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.65it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:20,  1.73it/s]Extractor Predicting: 34it [00:20,  1.71it/s]Extractor Predicting: 35it [00:21,  1.68it/s]Extractor Predicting: 36it [00:21,  1.65it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:24,  1.69it/s]Extractor Predicting: 41it [00:25,  1.68it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:26,  1.71it/s]Extractor Predicting: 44it [00:26,  1.69it/s]Extractor Predicting: 45it [00:27,  1.66it/s]Extractor Predicting: 46it [00:27,  1.68it/s]Extractor Predicting: 47it [00:28,  1.69it/s]Extractor Predicting: 48it [00:29,  1.69it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:30,  1.75it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:32,  1.71it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.70it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.65it/s]Extractor Predicting: 58it [00:35,  1.67it/s]Extractor Predicting: 59it [00:35,  1.65it/s]Extractor Predicting: 60it [00:36,  1.65it/s]Extractor Predicting: 61it [00:36,  1.67it/s]Extractor Predicting: 62it [00:37,  1.67it/s]Extractor Predicting: 63it [00:38,  1.64it/s]Extractor Predicting: 64it [00:38,  1.70it/s]Extractor Predicting: 65it [00:39,  1.68it/s]Extractor Predicting: 66it [00:39,  1.66it/s]Extractor Predicting: 67it [00:40,  1.67it/s]Extractor Predicting: 68it [00:41,  1.72it/s]Extractor Predicting: 69it [00:41,  1.73it/s]Extractor Predicting: 70it [00:42,  1.72it/s]Extractor Predicting: 71it [00:42,  1.72it/s]Extractor Predicting: 72it [00:43,  1.68it/s]Extractor Predicting: 73it [00:43,  1.71it/s]Extractor Predicting: 74it [00:44,  1.66it/s]Extractor Predicting: 75it [00:45,  1.66it/s]Extractor Predicting: 76it [00:45,  1.67it/s]Extractor Predicting: 77it [00:46,  1.66it/s]Extractor Predicting: 78it [00:46,  1.67it/s]Extractor Predicting: 79it [00:47,  1.67it/s]Extractor Predicting: 80it [00:48,  1.69it/s]Extractor Predicting: 81it [00:48,  1.68it/s]Extractor Predicting: 82it [00:49,  1.70it/s]Extractor Predicting: 83it [00:49,  1.65it/s]Extractor Predicting: 84it [00:50,  1.66it/s]Extractor Predicting: 85it [00:51,  1.63it/s]Extractor Predicting: 86it [00:51,  1.60it/s]Extractor Predicting: 87it [00:52,  1.66it/s]Extractor Predicting: 88it [00:53,  1.61it/s]Extractor Predicting: 89it [00:53,  1.59it/s]Extractor Predicting: 90it [00:54,  1.57it/s]Extractor Predicting: 91it [00:54,  1.58it/s]Extractor Predicting: 92it [00:55,  1.56it/s]Extractor Predicting: 93it [00:56,  1.54it/s]Extractor Predicting: 94it [00:56,  1.55it/s]Extractor Predicting: 95it [00:57,  1.59it/s]Extractor Predicting: 96it [00:58,  1.58it/s]Extractor Predicting: 97it [00:58,  1.56it/s]Extractor Predicting: 98it [00:59,  1.56it/s]Extractor Predicting: 99it [01:00,  1.38it/s]Extractor Predicting: 100it [01:01,  1.41it/s]Extractor Predicting: 101it [01:01,  1.44it/s]Extractor Predicting: 102it [01:02,  1.45it/s]Extractor Predicting: 103it [01:03,  1.50it/s]Extractor Predicting: 104it [01:03,  1.51it/s]Extractor Predicting: 105it [01:04,  1.53it/s]Extractor Predicting: 106it [01:05,  1.50it/s]Extractor Predicting: 107it [01:05,  1.51it/s]Extractor Predicting: 108it [01:06,  1.48it/s]Extractor Predicting: 109it [01:07,  1.48it/s]Extractor Predicting: 110it [01:07,  1.48it/s]Extractor Predicting: 111it [01:08,  1.50it/s]Extractor Predicting: 112it [01:09,  1.49it/s]Extractor Predicting: 113it [01:09,  1.50it/s]Extractor Predicting: 114it [01:10,  1.50it/s]Extractor Predicting: 115it [01:11,  1.49it/s]Extractor Predicting: 116it [01:11,  1.50it/s]Extractor Predicting: 117it [01:12,  1.50it/s]Extractor Predicting: 118it [01:13,  1.52it/s]Extractor Predicting: 119it [01:13,  1.54it/s]Extractor Predicting: 120it [01:14,  1.59it/s]Extractor Predicting: 121it [01:14,  1.57it/s]Extractor Predicting: 122it [01:15,  1.56it/s]Extractor Predicting: 123it [01:16,  1.56it/s]Extractor Predicting: 124it [01:16,  1.55it/s]Extractor Predicting: 125it [01:17,  1.57it/s]Extractor Predicting: 126it [01:18,  1.60it/s]Extractor Predicting: 127it [01:18,  1.60it/s]Extractor Predicting: 128it [01:19,  1.63it/s]Extractor Predicting: 129it [01:19,  1.60it/s]Extractor Predicting: 130it [01:20,  1.56it/s]Extractor Predicting: 131it [01:21,  1.57it/s]Extractor Predicting: 132it [01:21,  1.57it/s]Extractor Predicting: 133it [01:22,  1.60it/s]Extractor Predicting: 134it [01:23,  1.63it/s]Extractor Predicting: 135it [01:23,  1.60it/s]Extractor Predicting: 136it [01:24,  1.61it/s]Extractor Predicting: 137it [01:24,  1.63it/s]Extractor Predicting: 138it [01:25,  1.60it/s]Extractor Predicting: 139it [01:26,  1.61it/s]Extractor Predicting: 140it [01:26,  1.61it/s]Extractor Predicting: 141it [01:27,  1.56it/s]Extractor Predicting: 142it [01:28,  1.55it/s]Extractor Predicting: 143it [01:28,  1.56it/s]Extractor Predicting: 144it [01:29,  1.56it/s]Extractor Predicting: 145it [01:30,  1.55it/s]Extractor Predicting: 146it [01:30,  1.59it/s]Extractor Predicting: 147it [01:31,  1.60it/s]Extractor Predicting: 148it [01:31,  1.58it/s]Extractor Predicting: 149it [01:32,  1.59it/s]Extractor Predicting: 150it [01:33,  1.58it/s]Extractor Predicting: 151it [01:33,  1.54it/s]Extractor Predicting: 152it [01:34,  1.55it/s]Extractor Predicting: 153it [01:35,  1.55it/s]Extractor Predicting: 154it [01:35,  1.60it/s]Extractor Predicting: 155it [01:36,  1.56it/s]Extractor Predicting: 156it [01:37,  1.56it/s]Extractor Predicting: 157it [01:37,  1.54it/s]Extractor Predicting: 158it [01:38,  1.56it/s]Extractor Predicting: 159it [01:38,  1.55it/s]Extractor Predicting: 160it [01:39,  1.53it/s]Extractor Predicting: 161it [01:40,  1.55it/s]Extractor Predicting: 162it [01:40,  1.54it/s]Extractor Predicting: 163it [01:41,  1.54it/s]Extractor Predicting: 164it [01:42,  1.55it/s]Extractor Predicting: 165it [01:42,  1.55it/s]Extractor Predicting: 166it [01:43,  1.56it/s]Extractor Predicting: 167it [01:44,  1.53it/s]Extractor Predicting: 168it [01:44,  1.53it/s]Extractor Predicting: 169it [01:45,  1.53it/s]Extractor Predicting: 170it [01:46,  1.52it/s]Extractor Predicting: 171it [01:46,  1.52it/s]Extractor Predicting: 172it [01:47,  1.53it/s]Extractor Predicting: 173it [01:48,  1.52it/s]Extractor Predicting: 174it [01:48,  1.52it/s]Extractor Predicting: 175it [01:49,  1.58it/s]Extractor Predicting: 176it [01:50,  1.57it/s]Extractor Predicting: 177it [01:50,  1.59it/s]Extractor Predicting: 178it [01:51,  1.60it/s]Extractor Predicting: 179it [01:51,  1.62it/s]Extractor Predicting: 180it [01:52,  1.61it/s]Extractor Predicting: 181it [01:53,  1.62it/s]Extractor Predicting: 182it [01:53,  1.64it/s]Extractor Predicting: 183it [01:54,  1.58it/s]Extractor Predicting: 184it [01:54,  1.64it/s]Extractor Predicting: 185it [01:55,  1.63it/s]Extractor Predicting: 186it [01:56,  1.64it/s]Extractor Predicting: 187it [01:56,  1.64it/s]Extractor Predicting: 188it [01:57,  1.59it/s]Extractor Predicting: 189it [01:58,  1.58it/s]Extractor Predicting: 190it [01:58,  1.41it/s]Extractor Predicting: 191it [01:59,  1.44it/s]Extractor Predicting: 192it [02:00,  1.47it/s]Extractor Predicting: 193it [02:00,  1.53it/s]Extractor Predicting: 194it [02:01,  1.54it/s]Extractor Predicting: 195it [02:02,  1.53it/s]Extractor Predicting: 196it [02:02,  1.50it/s]Extractor Predicting: 197it [02:03,  1.53it/s]Extractor Predicting: 198it [02:04,  1.52it/s]Extractor Predicting: 199it [02:04,  1.55it/s]Extractor Predicting: 200it [02:05,  1.54it/s]Extractor Predicting: 201it [02:06,  1.57it/s]Extractor Predicting: 202it [02:06,  1.57it/s]Extractor Predicting: 203it [02:07,  1.58it/s]Extractor Predicting: 204it [02:07,  1.61it/s]Extractor Predicting: 205it [02:08,  1.59it/s]Extractor Predicting: 206it [02:09,  1.62it/s]Extractor Predicting: 207it [02:09,  1.60it/s]Extractor Predicting: 208it [02:10,  1.58it/s]Extractor Predicting: 209it [02:11,  1.59it/s]Extractor Predicting: 210it [02:11,  1.59it/s]Extractor Predicting: 211it [02:12,  1.58it/s]Extractor Predicting: 212it [02:12,  1.59it/s]Extractor Predicting: 213it [02:13,  1.60it/s]Extractor Predicting: 214it [02:14,  1.59it/s]Extractor Predicting: 215it [02:14,  1.59it/s]Extractor Predicting: 216it [02:15,  1.59it/s]Extractor Predicting: 217it [02:16,  1.57it/s]Extractor Predicting: 218it [02:16,  1.58it/s]Extractor Predicting: 219it [02:17,  1.55it/s]Extractor Predicting: 220it [02:17,  1.58it/s]Extractor Predicting: 221it [02:18,  1.61it/s]Extractor Predicting: 222it [02:19,  1.60it/s]Extractor Predicting: 223it [02:19,  1.56it/s]Extractor Predicting: 224it [02:20,  1.61it/s]Extractor Predicting: 225it [02:21,  1.59it/s]Extractor Predicting: 226it [02:21,  1.58it/s]Extractor Predicting: 227it [02:22,  1.58it/s]Extractor Predicting: 228it [02:22,  1.60it/s]Extractor Predicting: 229it [02:23,  1.59it/s]Extractor Predicting: 230it [02:24,  1.61it/s]Extractor Predicting: 231it [02:24,  1.62it/s]Extractor Predicting: 232it [02:25,  1.60it/s]Extractor Predicting: 233it [02:26,  1.61it/s]Extractor Predicting: 234it [02:26,  1.59it/s]Extractor Predicting: 235it [02:27,  1.55it/s]Extractor Predicting: 236it [02:28,  1.56it/s]Extractor Predicting: 237it [02:28,  1.57it/s]Extractor Predicting: 238it [02:29,  1.56it/s]Extractor Predicting: 239it [02:29,  1.55it/s]Extractor Predicting: 240it [02:30,  1.55it/s]Extractor Predicting: 241it [02:31,  1.55it/s]Extractor Predicting: 242it [02:31,  1.56it/s]Extractor Predicting: 243it [02:32,  1.57it/s]Extractor Predicting: 244it [02:33,  1.55it/s]Extractor Predicting: 245it [02:33,  1.57it/s]Extractor Predicting: 246it [02:34,  1.57it/s]Extractor Predicting: 247it [02:35,  1.59it/s]Extractor Predicting: 248it [02:35,  1.58it/s]Extractor Predicting: 249it [02:36,  1.61it/s]Extractor Predicting: 250it [02:36,  1.60it/s]Extractor Predicting: 251it [02:37,  1.64it/s]Extractor Predicting: 252it [02:38,  1.61it/s]Extractor Predicting: 253it [02:38,  1.61it/s]Extractor Predicting: 254it [02:39,  1.61it/s]Extractor Predicting: 255it [02:40,  1.60it/s]Extractor Predicting: 256it [02:40,  1.56it/s]Extractor Predicting: 257it [02:41,  1.57it/s]Extractor Predicting: 258it [02:41,  1.57it/s]Extractor Predicting: 259it [02:42,  1.62it/s]Extractor Predicting: 260it [02:43,  1.60it/s]Extractor Predicting: 261it [02:43,  1.63it/s]Extractor Predicting: 262it [02:44,  1.61it/s]Extractor Predicting: 263it [02:45,  1.60it/s]Extractor Predicting: 264it [02:45,  1.66it/s]Extractor Predicting: 265it [02:46,  1.65it/s]Extractor Predicting: 266it [02:46,  1.66it/s]Extractor Predicting: 267it [02:47,  1.64it/s]Extractor Predicting: 268it [02:48,  1.63it/s]Extractor Predicting: 269it [02:48,  1.62it/s]Extractor Predicting: 270it [02:49,  1.59it/s]Extractor Predicting: 271it [02:49,  1.57it/s]Extractor Predicting: 272it [02:50,  1.58it/s]Extractor Predicting: 273it [02:51,  1.57it/s]Extractor Predicting: 274it [02:51,  1.55it/s]Extractor Predicting: 275it [02:52,  1.53it/s]Extractor Predicting: 276it [02:53,  1.39it/s]Extractor Predicting: 277it [02:54,  1.43it/s]Extractor Predicting: 278it [02:54,  1.50it/s]Extractor Predicting: 279it [02:55,  1.51it/s]Extractor Predicting: 280it [02:56,  1.52it/s]Extractor Predicting: 281it [02:56,  1.55it/s]Extractor Predicting: 282it [02:57,  1.55it/s]Extractor Predicting: 283it [02:57,  1.54it/s]Extractor Predicting: 284it [02:58,  1.55it/s]Extractor Predicting: 285it [02:59,  1.54it/s]Extractor Predicting: 286it [02:59,  1.59it/s]Extractor Predicting: 287it [03:00,  1.54it/s]Extractor Predicting: 288it [03:00,  1.76it/s]Extractor Predicting: 288it [03:00,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:44,452 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:44,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:44,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:44,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:44,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:39:45,051 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:39:45,051 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:39:45,772 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:39:46,818 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:39:46,818 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:49,734 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:49,739 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:49,739 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:49,739 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:39:49,739 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:39:50,391 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:39:50,392 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:39:50,980 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:39:51,154 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:39:51,154 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3673957621326042,
  "recall": 0.15579710144927536,
  "score": 0.21880724608182375,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 618
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 718, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:01,  2.35it/s]Extractor Predicting: 3it [00:01,  2.02it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.42857142857142855,
  "recall": 0.06,
  "score": 0.10526315789473684,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_2', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/1_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/2_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/3_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/4_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/dev.jsonl', 'labels': ['follows', 'instrument', 'member of political party', 'owned by', 'said to be the same as'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/', 'labels': ['after a work by', 'field of work', 'headquarters location', 'location of formation', 'mouth of the watercourse', 'occupant', 'place served by transport hub', 'record label', 'winner', 'work location'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
